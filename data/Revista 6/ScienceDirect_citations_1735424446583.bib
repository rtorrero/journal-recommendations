@article{COUTURIER2021103666,
title = {A review on absolute visual localization for UAV},
journal = {Robotics and Autonomous Systems},
volume = {135},
pages = {103666},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103666},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305066},
author = {Andy Couturier and Moulay A. Akhloufi},
keywords = {Absolute visual localization, UAV, Navigation, Satellite imagery, Computer vision, Deep learning},
abstract = {Research on unmanned aerial vehicles is growing as they are becoming less expensive and more available than before. The applications span a large number of areas and include border security, search and rescue, wildlife surveying, firefighting, precision agriculture, structure inspection, surveying and mapping, aerial photography, and recreative applications. These applications can require autonomous behavior which is only possible with a precise and robust self-localization. Until recently, the favored approach to localization was based on inertial sensors and global navigation satellite systems. However, global navigation satellite systems have multiple shortcomings related to long-distance radio communications (e.g. non-line-of-sight reception, multipath, spoofing). This motivated the development of new approaches to supplement or supplant satellite navigation. Absolute visual localization is one of the two main approaches to vision-based localization. The goal is to locate the current view of the UAV in a reference satellite map or georeferenced imagery from previous flights. Various approaches were proposed in this area and this paper review most of the literature in this field since 2015. The problematic at hand is analyzed and defined. Existing approaches are reviewed in 4 categories: template matching, feature points matching, deep learning and visual odometry.}
}
@article{LUCET2021103706,
title = {Accurate autonomous navigation strategy dedicated to the storage of buses in a bus center},
journal = {Robotics and Autonomous Systems},
volume = {136},
pages = {103706},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103706},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305467},
author = {Eric Lucet and Alain Micaelli and François-Xavier Russotto},
keywords = {Heavy car-like mobile robot, Linear constrained predictive regulator, Curvilinear abscissa dependent sliding estimator, Bus center autonomous storage},
abstract = {This paper deals with an innovative autonomous bus navigation and parking system in a bus depot, in order to optimize their movements in a confined area. The kinematic model of the vehicle is defined. Considering its dimensions and weight as well as the centimetric accuracy required, a predictive controller is designed, based on its model linearized around the changing path curvature value, to perform accurate curved paths tracking with a limited tracking error guaranteed by the consideration of a constraint. This controller and additional sliding observers are designed according to the distance traveled, allowing maneuvers to be performed at any forward or backward speed with constant accuracy. In addition, these observers are not affected by path tracking errors. The implementation on an industrial vehicle, operated under realistic conditions, demonstrates the performance and robustness of this navigation system.}
}
@article{2023104391,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {162},
pages = {104391},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(23)00030-1},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000301}
}
@article{FISHER2021103755,
title = {ColMap: A memory-efficient occupancy grid mapping framework},
journal = {Robotics and Autonomous Systems},
volume = {142},
pages = {103755},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103755},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000403},
author = {Alex Fisher and Ricardo Cannizzaro and Madeleine Cochrane and Chatura Nagahawatte and Jennifer L. Palmer},
keywords = {Mapping, Occupancy grid, Bayesian estimation, Robotics},
abstract = {In order to possess a significant degree of autonomy, a robot must be able to perceive its environment and store a representation of that environment for use in tasks such as localisation, navigation, collision avoidance, and higher decision making. It must do this subject to constraints on memory and processing power typical of the embedded computer systems commonly found on small robotic devices. These constraints are particularly important for flying robots (i.e. unmanned aerial vehicles), for which weight must be minimised. The challenge of storing a detailed map of a large area on a small embedded computer has led to the development of many algorithms that exploit the sparsity of typical maps to create a more memory-efficient representation. In this paper, we demonstrate that the verticality of both natural and man-made structures can be exploited to create a framework that can store occupancy grid maps efficiently, without causing additional computational burden. The new framework achieves an order-of-magnitude reduction in memory footprint relative to widely-used occupancy grid mapping software, while also achieving a slight speed-up in map insertion and access times. We also make available LIDAR scans taken from a hexacopter of an indoor flight arena that can be used to assist in evaluating future mapping and SLAM developments.}
}
@article{MRONGA2021103779,
title = {Learning context-adaptive task constraints for robotic manipulation},
journal = {Robotics and Autonomous Systems},
volume = {141},
pages = {103779},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103779},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000646},
author = {Dennis Mronga and Frank Kirchner},
keywords = {Context-adaptive control, Constraint-based robot control, Programming-by-demonstration, Gaussian mixture regression, Dual-arm manipulation},
abstract = {Constraint-based control approaches offer a flexible way to specify robotic manipulation tasks and execute them on robots with many degrees of freedom. However, the specification of task constraints and their associated priorities usually requires a human-expert and often leads to tailor-made solutions for specific situations. This paper presents our recent efforts to automatically derive task constraints for a constraint-based robot controller from data and adapt them with respect to previously unseen situations (contexts). We use a programming-by-demonstration approach to generate training data in multiple variations (context changes) of a given task. From this data we learn a probabilistic model that maps context variables to task constraints and their respective soft task priorities. We evaluate our approach with 3 different dual-arm manipulation tasks on an industrial robot and show that it performs better than comparable approaches with respect to reproduction accuracy in previously unseen contexts.}
}
@article{DENG2021103727,
title = {An adaptive planning framework for dexterous robotic grasping with grasp type detection},
journal = {Robotics and Autonomous Systems},
volume = {140},
pages = {103727},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103727},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000129},
author = {Zhen Deng and Bin Fang and Bingwei He and Jianwei Zhang},
keywords = {Dexterous grasping, Grasp planning, Grasp type, Grasp quality measure, Grasp optimization},
abstract = {Dexterous grasping is one of the most fundamental abilities of robots to implement various manipulation tasks. Robots should have the same ability as humans to plan various grasp types for dexterous grasping. This paper addresses the problem of the adaptability of grasp planning. A novel adaptive grasp planning framework is designed to adapt to various grasp types rather than a single one. In this framework, six commonly used grasp types are considered. The information of grasp type is extracted from visual data. Then, inspired by the opposition concept, a novel concept of pregrasping opposition is introduced as the pregrasping configuration to encode the information of the grasp type. After that, a two-stage adaptive grasp planning method is proposed, which determines the pregrasping opposition in Stage One and finds a feasible grasp configuration for object grasping in Stage Two. The pregrasping opposition is used as a waypoint for the formation of complex grasps. The effectiveness of the proposed framework was evaluated in simulation and real-world experiments. The experimental results demonstrated that the proposed framework can plan various grasp types for dexterous robotic grasping. Additionally, the use of grasp types helps to reduce the complexity of grasp planning and improve the grasp dexterity of robotic hands.}
}
@article{IKEDA2021103670,
title = {Cooperative step-climbing strategy using an autonomous wheelchair and a robot},
journal = {Robotics and Autonomous Systems},
volume = {135},
pages = {103670},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103670},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305108},
author = {Hidetoshi Ikeda and Takafumi Toyama and Daisuke Maki and Keisuke Sato and Eiji Nakano},
keywords = {Step climbing, Wheelchair, Architectural accessibility, Assistive technology},
abstract = {This report describes an automatic control system that allows an assistive robot pushing a wheelchair to climb steps. The robot is equipped with a wheeled mechanism and dual manipulators. The wheelchair is a commercially available model that has been equipped with sensors, circuits, and batteries. The robot and wheelchair are connected when the vehicles climb a step. In that operation, the front wheels of the wheelchair are lifted and placed on the step using the velocity differences between the wheelchair and the robot. Next, when the rear wheels of the wheelchair ascend the step, the robot imitates the upper arm motions of a human pushing against his/her chest that commonly occurs when maneuvering a wheelchair up a step. Similarly, the front wheels of the robot are lifted and placed on the step using the velocity differences between the vehicles and the robot’s front wheels. After that, with the assistance of the wheelchair, the other wheels of the robot climb onto the step. In an effort to ensure safety, we also performed a theoretical analysis to determine the most suitable distance for lifting the front wheels of the robot when approaching and climbing a step. Our newly developed cooperative step-climbing system makes it possible to eliminate the complicated operations that were required by previous methods and can also prevent collisions between the wheelchair’s front wheels and the step, thus drastically improving the convenience of the operation. The test subject riding the wheelchair was an able-bodied male, and the experiment conducted to evaluate our system was performed on a 120 mm step height that had a friction coefficient of 0.72. This setup was sufficient for demonstrating the overall effectiveness of our system.}
}
@article{VILLAMARGOMEZ2021103763,
title = {Ontology-based knowledge management with verbal interaction for command interpretation and execution by home service robots},
journal = {Robotics and Autonomous Systems},
volume = {140},
pages = {103763},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103763},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000488},
author = {L. {Villamar Gómez} and J. Miura},
keywords = {Knowledge management, Ontology-based, Service robots, Human–robot interaction},
abstract = {This paper describes a system for service robots that combines ontological knowledge reasoning and human–robot interaction to interpret natural language commands and successfully perform household chores, such as finding and delivering objects. Knowledge and context reasoning is essential for providing more efficient service robots, given their diverse and continuously changing environments. Moreover, since they are in contact with humans, robots require such skills as interaction and language. Therefore, we developed a system with specific modules to manage robots’ knowledge and reasoning, command analysis, decision-making, and talking interaction. The system relies on inference methods and verbal interaction to understand commands and clarify uncertain information. We tested our system inside a simulated environment where the robot receives commands with missing or unclear information. The system’s performance was compared with the average performance of human subjects who completed the same commands in the simulation.}
}
@article{KIM2021103710,
title = {From exploration to control: Learning object manipulation skills through novelty search and local adaptation},
journal = {Robotics and Autonomous Systems},
volume = {136},
pages = {103710},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103710},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305509},
author = {Seungsu Kim and Alexandre Coninx and Stephane Doncieux},
keywords = {Quality–diversity search, Evolutionary algorithm, Long-life learning, Robotics, Developmental robotics, Behavior repertoire},
abstract = {Programming a robot to deal with open-ended tasks remains a challenge, in particular if the robot has to manipulate objects. Launching, grasping, pushing or any other object interaction can be simulated but the corresponding models are not reversible and the robot behavior thus cannot be directly deduced. These behaviors are hard to learn without a demonstration as the search space is large and the reward sparse. We propose a method to autonomously generate a diverse repertoire of simple object interaction behaviors in simulation. Our goal is to bootstrap a robot learning and development process with limited information about what the robot has to achieve and how. This repertoire can be exploited to solve different tasks in reality thanks to a proposed adaptation method or could be used as a training set for data-hungry algorithms. The proposed approach relies on the definition of a goal space and generates a repertoire of trajectories to reach attainable goals, thus allowing the robot to control this goal space. The repertoire is built with an off-the-shelf simulation thanks to a quality–diversity algorithm. The result is a set of solutions tested in simulation only. It may result in two different problems: (1) as the repertoire is discrete and finite, it may not contain the trajectory to deal with a given situation or (2) some trajectories may lead to a behavior in reality that differs from simulation because of a reality gap. We propose an approach to deal with both issues by using a local linearization of the mapping between the motion parameters and the observed effects. Furthermore, we present an approach to update the existing solutions repertoire with the tests done on the real robot. The approach has been validated on two different experiments on the Baxter robot: a ball launching and a joystick manipulation tasks.}
}
@article{SALIMILAFMEJANI2021103774,
title = {Nonlinear MPC for collision-free and deadlock-free navigation of multiple nonholonomic mobile robots},
journal = {Robotics and Autonomous Systems},
volume = {141},
pages = {103774},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103774},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000592},
author = {Amir {Salimi Lafmejani} and Spring Berman},
keywords = {Nonlinear model predictive control, Multi-robot systems, Wheeled mobile robots, Nonholonomic constraints, Collision avoidance, Deadlock avoidance},
abstract = {In this paper, we present an online nonlinear Model Predictive Control (MPC) method for collision-free, deadlock-free navigation by multiple autonomous nonholonomic Wheeled Mobile Robots (WMRs). Our proposed method solves a nonlinear constrained optimization problem at each time step over a specified horizon to compute a sequence of optimal control inputs that drive the robots to target poses along collision-free trajectories, where the robots’ future states are predicted according to a unicycle kinematic model. To reduce the computational complexity of the optimization problem, we formulate it without stabilizing terminal constraints or terminal costs. We describe a computationally efficient approach to programming and solving the optimization problem, using open-source software tools for fast nonlinear optimization and applying the multiple-shooting method. We also provide rigorous proofs of the feasibility of the optimization problem and the stability of the proposed method. To validate the performance of our MPC method, we implement it in both 3D robot simulations and experiments with real nonholonomic WMRs for different multi-robot navigation scenarios with up to six robots. In all scenarios, the robots successfully navigate to their goal poses without colliding with one another or becoming trapped in a deadlock.}
}
@article{2023104373,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104373},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(23)00012-X},
url = {https://www.sciencedirect.com/science/article/pii/S092188902300012X}
}
@article{REKABI2021103689,
title = {Distributed output feedback nonlinear H∞ formation control algorithm for heterogeneous aerial robotic teams},
journal = {Robotics and Autonomous Systems},
volume = {136},
pages = {103689},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103689},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305297},
author = {Fatemeh Rekabi and Farzad A. Shirazi and Mohammad Jafar Sadigh and Mahmood Saadat},
keywords = {Multi-agent system, Monte-Carlo simulation, SIL test, Pixhawk FMU, Raspberry-Pi 3, MAVROS},
abstract = {This paper deals with the formation flying control problem for a team of nonlinear uncertain quadrotors in presence of noisy measurements and environmental disturbances. A novel distributed output-feedback nonlinear robust algorithm is proposed to solve the problem. The algorithm leads to a series of combined estimation-control local policies with minimum communicated information by decomposing the global network to local star networks. An analytical study establishes the stability of the closed-loop system and the Monte-Carlo simulation demonstrates the robust performance and boundedness of the outputs numerically. The Software In the Loop (SIL) testing is performed utilizing Pixhawk open source flight management unit, Raspberry-pi 3 and GAZEBO simulation environment to reveal the effectiveness of the proposed distributed control-estimation algorithm for practical implementation.}
}
@article{FONTANA2021103734,
title = {A benchmark for point clouds registration algorithms},
journal = {Robotics and Autonomous Systems},
volume = {140},
pages = {103734},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103734},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000191},
author = {Simone Fontana and Daniele Cattaneo and Augusto L. Ballardini and Matteo Vaghi and Domenico G. Sorrenti},
keywords = {Benchmark, Point clouds registration, Datasets},
abstract = {Point clouds registration is a fundamental step of many point clouds processing pipelines; however, most algorithms are tested on data that are collected ad-hoc and not shared with the research community. These data often cover only a very limited set of use cases; therefore, the results cannot be generalized. Public datasets proposed until now, taken individually, cover only a few kinds of environment and mostly a single sensor. For these reasons, we developed a benchmark, for localization and mapping applications, using multiple publicly available datasets. In this way, we are able to cover many kinds of environment and many kinds of sensor that can produce point clouds. Furthermore, the ground truth has been thoroughly inspected and evaluated to ensure its quality. For some of the datasets, the accuracy of the ground truth measuring system was not reported by the original authors, therefore we estimated it with our own novel method, based on an iterative registration algorithm. Along with the data, we provide a broad set of registration problems, chosen to cover different types of initial misalignment, various degrees of overlap, and different kinds of registration problems. Lastly, we propose a metric to measure the performances of registration algorithms: it combines the commonly used rotation and translation errors together, to allow an objective comparison of the alignments. This work aims at encouraging authors to use a public and shared benchmark, instead of data collected ad-hoc, to ensure objectivity and repeatability, two fundamental characteristics in any scientific field.}
}
@article{NIU2021103705,
title = {Directional optimal reciprocal collision avoidance},
journal = {Robotics and Autonomous Systems},
volume = {136},
pages = {103705},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103705},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305455},
author = {Haotian Niu and Cunbao Ma and Pei Han},
keywords = {Self-separation assurance, Civil aircraft, DORCA, Decentralized collision avoidance, The unified rules, Direction selectivity},
abstract = {A great amount of effort has been devoted to the study on self-separation assurance approach for civil aviation in the airspace with increasing density. In this article, the Optimal Reciprocal Collision Avoidance (ORCA) algorithm is modified to make it work for autonomous and decentralized collision avoidance for civil aircraft. Without considering the direction selectivity of collision-free maneuver, aircraft may select the relative parallel trajectories by deploying the ORCA algorithm in both decentralized and centralized way. As a result, the collision tends to be postponed to the next time horizon because civil aircraft need to return to original trajectories. Simultaneously, the unified rules can hardly be integrated into the approach due to the lack of direction selectivity for collision-free navigation. The process of separation assurance will be disorderly when multiple aircraft are involved. To solve the problem mentioned above, a new algorithm called Directional Optimal Reciprocal Collision Avoidance (DORCA) is proposed. The DORCA algorithm employs a vector rotation mode to construct the forbidden Velocity Obstacle (VO) set in order to improve the computation efficiency. In addition, the direction selectivity of maneuver is achieved through constructing the direction-constrained VO set according to the direction of relative motion in velocity space. Direction selectivity of the algorithm enables the process of collision avoidance to comply with the unified rules. A number of encounter scenarios are conducted to confirm the validity and feasibility of the proposed DORCA algorithm. In all scenarios tested, the direction selectivity of collision-free maneuver can be successfully integrated into the DORCA algorithm, and the algorithm is more efficient than the ORCA algorithm for collision avoidance in decentralized way.}
}
@article{WENGEFELD2021103665,
title = {Real-time person orientation estimation and tracking using colored point clouds},
journal = {Robotics and Autonomous Systems},
volume = {135},
pages = {103665},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103665},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305054},
author = {Tim Wengefeld and Benjamin Lewandowski and Daniel Seichter and Lennard Pfennig and Steffen Müller and Horst-Michael Gross},
keywords = {Person orientation estimation, Person perception, Mobile robotics, Real time},
abstract = {Robustly estimating the orientations of people is a crucial precondition for a wide range of applications. Especially for autonomous systems operating in populated environments, the orientation of a person can give valuable information to increase their acceptance. Given people’s orientations, mobile systems can apply navigation strategies which take people’s proxemics into account or approach them in a human like manner to perform human robot interaction (HRI) tasks. In this paper, we present an approach for person orientation estimation based on computationally efficient features extracted from colored point clouds, formerly used for a two-class person attribute classification. The classification approach has been extended to the continuous domain while treating the problem of orientation estimation in real time. Furthermore, we present an approach for tracking estimated orientations over time using a Bayesian filter. We will show that tracking can increase the accuracy of orientations by up to 3.69° on a dataset recorded with a mobile robot. Best results on this highly challenging dataset are achieved with a regression approach for orientation estimation in combination with tracking. The mean angular error of just 16.49° proofs the applicability in real-world scenarios.}
}
@article{ALCANTARA2021103778,
title = {Optimal trajectory planning for cinematography with multiple Unmanned Aerial Vehicles},
journal = {Robotics and Autonomous Systems},
volume = {140},
pages = {103778},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103778},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000634},
author = {Alfonso Alcántara and Jesús Capitán and Rita Cunha and Aníbal Ollero},
keywords = {Optimal trajectory planning, UAV cinematography, Multi-UAV coordination},
abstract = {This paper presents a method for planning optimal trajectories with a team of Unmanned Aerial Vehicles (UAVs) performing autonomous cinematography. The method is able to plan trajectories online and in a distributed manner, providing coordination between the UAVs. We propose a novel non-linear formulation for this challenging problem of computing multi-UAV optimal trajectories for cinematography; integrating UAVs dynamics and collision avoidance constraints, together with cinematographic aspects like smoothness, gimbal mechanical limits and mutual camera visibility. We integrate our method within a hardware and software architecture for UAV cinematography that was previously developed within the framework of the MultiDrone project; and demonstrate its use with different types of shots filming a moving target outdoors. We provide extensive experimental results both in simulation and field experiments. We analyze the performance of the method and prove that it is able to compute online smooth trajectories, reducing jerky movements and complying with cinematography constraints.}
}
@article{HARRIS2021103726,
title = {Online plan modification in uncertain resource-constrained environments},
journal = {Robotics and Autonomous Systems},
volume = {140},
pages = {103726},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103726},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000117},
author = {Catherine A. Harris and Nick Hawes and Richard Dearden},
keywords = {Automated planning, Decision making, Resource uncertainty, Contingency planning, Plan repair, Online planning},
abstract = {This paper presents an approach to planning under uncertainty in resource-constrained environments. We describe our novel method for online plan modification and execution monitoring, which augments an existing plan with pre-computed plan fragments in response to observed resource availability. Our plan merging algorithm uses causal structure to interleave actions, creating solutions online using observations of the true state without introducing significant computational cost. Our system monitors resource availability, reasoning about the probability of successfully completing the goals. We show that when the probability of completing a plan decreases, by removing low-priority goals our system reduces the risk of plan failure, increasing mission success rate. Conversely, when resource availability allows, by including additional goals our system increases reward without adversely affecting success rate. We evaluate our approach using the example domain of long-range autonomous underwater vehicle (AUV) missions, in which a vehicle spends months at sea with little or no opportunity for intervention. We compare the performance to a state-of-the-art oversubscription planner. Planning within such domains is challenging because significant resource usage uncertainty means it is computationally infeasible to calculate the optimal strategy in advance. We also evaluate the applicability of our plan merging algorithm to existing IPC domains, presenting a discussion of the domain characteristics which favour the use of our approach.}
}
@article{RICCIO2021103693,
title = {LoOP: Iterative learning for optimistic planning on robots},
journal = {Robotics and Autonomous Systems},
volume = {136},
pages = {103693},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103693},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305339},
author = {Francesco Riccio and Roberto Capobianco and Daniele Nardi},
keywords = {Autonomous planning and learning, Monte-Carlo planning, Q-learning, Deep robot reinforcement learning},
abstract = {Efficient robotic behaviors require robustness and adaptation to dynamic changes of the environment, whose characteristics rapidly vary during robot operation. To generate effective robot action policies, planning and learning techniques have shown the most promising results. However, if considered individually, they present different limitations. Planning techniques lack generalization among similar states and require experts to define behavioral routines at different levels of abstraction. Conversely, learning methods usually require a considerable number of training samples and iterations of the algorithm. To overcome these issues, and to efficiently generate robot behaviors, we introduce LoOP, an iterative learning algorithm for optimistic planning that combines state-of-the-art planning and learning techniques to generate action policies. The main contribution of LoOP is the combination of Monte-Carlo Search Planning and Q-learning, which enables focused exploration during policy refinement in different robotic applications. We demonstrate the robustness and flexibility of LoOP in various domains and multiple robotic platforms, by validating the proposed approach with an extensive experimental evaluation.}
}
@article{FERROLHO2021103814,
title = {Residual force polytope: Admissible task-space forces of dynamic trajectories},
journal = {Robotics and Autonomous Systems},
volume = {142},
pages = {103814},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103814},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000993},
author = {Henrique Ferrolho and Wolfgang Merkt and Carlo Tiseo and Sethu Vijayakumar},
keywords = {Robustness, Polytopes, Trajectory optimization, Robotic arms},
abstract = {We propose a representation for the set of forces a robot can counteract using full system dynamics: the residual force polytope. Given the nominal torques required by a dynamic motion, this representation models the forces which can be sustained without interfering with that motion. The residual force polytope can be used to analyze and compare the set of admissible forces of different trajectories, but it can also be used to define metrics for solving optimization problems, such as in trajectory optimization or system design. We demonstrate how such a metric can be applied to trajectory optimization and compare it against other objective functions typically used. Our results show that the trajectories computed by optimizing objectives defined as functions of the residual force polytope are more robust to unknown external disturbances. The computational cost of these metrics is relatively high and not compatible with the short planning times required by online methods, but they are acceptable for planning motions offline.}
}
@article{CHEN2021103762,
title = {Robust gait design for a compass gait biped on slippery surfaces},
journal = {Robotics and Autonomous Systems},
volume = {140},
pages = {103762},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103762},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000476},
author = {Tan Chen and Bill Goodwine},
keywords = {Biped walking, Gait design, Slipping, Low friction, Robustness},
abstract = {Most current bipedal robots were modeled with an assumption that there is no slip between the stance foot and ground. This paper relaxes that assumption and undertakes a comprehensive study of the compass gait biped on slippery ground. It presents in detail the control of a biped that allows for foot slipping, and shows that feasible gaits fail on slippery ground for two causes: falling backward or requiring negative contact force which cannot be provided by the ground. To characterize a robust gait on slippery ground, three safety factors are proposed to measure the robustness: slip friction, falling friction and tolerance ability of slipping without falling. This study thus uses these factors to investigate independent influence of gait speed and step length on the robustness of the gait, and shows that gaits with small step length and moderate speed are robust and preferable on slippery surfaces. In contrast, gaits with large step length generally require large friction to maintain stable walking on slippery surfaces. Moreover, gaits with a backward swing foot velocity relative to the ground just before touch down are generally more robust than ones with a forward velocity. It is further shown that only one parameter in gait design determines the swing-backward feature, which can help design robust gaits. Models with varying physical parameters such as mass, leg length and position of center of mass (CoM) in each leg, are also studied to validate the universality of this result.}
}
@article{ZHAO2021103733,
title = {Fast-moving piezoelectric micro-robotic fish with double caudal fins},
journal = {Robotics and Autonomous Systems},
volume = {140},
pages = {103733},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103733},
url = {https://www.sciencedirect.com/science/article/pii/S092188902100018X},
author = {Quanliang Zhao and Shiqi Liu and Jinghao Chen and Guangping He and Jiejian Di and Lei Zhao and Tingting Su and Mengying Zhang and Zhiling Hou},
keywords = {Fast movement, Micro-robotic fish, Piezoelectric actuator, Caudal fin},
abstract = {Micro-robotic fish (length ≤10 cm) driven by smart materials have remarkable advantages over conventional motors and piston-based robotic fish. In particular, they are highly efficient and compact. One of the key challenges is attaining high mobility with high energy density, low driving voltage and power loss. In this work, a double caudal fin micro-robotic fish actuated by two piezoelectric bimorph cantilevers is proposed and fabricated from rigid carbon fiber/resin composites and flexible polyimide hinges. Its weight is about 1.93 g and the maximum uniform swimming velocity is as high as about 0.75 BL/s (4.5 cm/s), which is much faster than previously reported micro-robotic fish actuated by ionic polymer–metal composites, shape memory alloys and dielectric elastomers. A theoretical model is validated by the experimental results and can be used to design and analyze a variety of piezoelectric robotic fish propelled by caudal fins.}
}
@article{CHEN2021103672,
title = {Multi-fingered grasping force optimization based on generalized penalty-function concepts},
journal = {Robotics and Autonomous Systems},
volume = {135},
pages = {103672},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103672},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305121},
author = {Zhong Chen and Qisen Wu and Cao Hong and Xianmin Zhang},
keywords = {Grasping force optimization, Multi-fingered hand, Second-order cone programming, Generalized penalty function},
abstract = {This paper presents an efficient multi-fingered grasping force optimization (GFO) method based on generalized penalty-function concepts. In view of the fact that the mainstream multi-fingered GFO method often treats the second-order cone programming (SOCP) problem as a semi-definite programming (SDP) problem, whose computational complexity is high, we hereby use the barrier function to construct the regularized optimization problem. The trade-off representation of different dimension objective functions is given, and the penalty factor is introduced to form the augmented optimization objective function. For specific operational tasks, by adjusting the penalty factor, a more compact, stable or slack, flexible grasping scheme could be obtained. Monte Carlo simulation is used to determine the probability of successful grasping when variability is introduced, and the robustness of the proposed method in the change of contact position and the friction coefficient between hand and object is verified. Experimental results and dynamic simulation are given, which show that the proposed algorithm outperforms the mainstream SDP method in execution time and iteration number, and the obtained force distribution has both continuity and distribution. Operational flexibility is instructive for practical applications.}
}
@article{2023104457,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {165},
pages = {104457},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(23)00096-9},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000969}
}
@article{JIN2021103712,
title = {Gaussian process-based nonlinear predictive control for visual servoing of constrained mobile robots with unknown dynamics},
journal = {Robotics and Autonomous Systems},
volume = {136},
pages = {103712},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103712},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305522},
author = {Zhehao Jin and Jinhui Wu and Andong Liu and Wen-An Zhang and Li Yu},
keywords = {Visual servoing, Gaussian process, Nonlinear model predictive control, Mobile robots, Variant iterative linear quadratic regulator},
abstract = {In this paper, a Gaussian process-based nonlinear model predictive control (GP-based NMPC) algorithm is presented to deal with the visual servoing problem for constrained mobile robots. Firstly, a GP-enhanced model is established by incorporating a GP model and a visual servoing kinematic model where the GP-model is used to capture the robot dynamics with on-line updating. Then, a nonlinear model predictive control (NMPC) strategy is proposed to transform the visual servoing task into a nonlinear optimization problem with robot-physical and camera-visibility constraints. Subsequently, a variant iterative linear quadratic regulator algorithm is presented to solve the constrained NMPC problem in real time. Finally, simulations and experiments are conducted to show the effectiveness of the presented method.}
}
@article{2022104174,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {154},
pages = {104174},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(22)00100-2},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001002}
}
@article{2022104275,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {157},
pages = {104275},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(22)00164-6},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001646}
}
@article{WOODFORD2021103708,
title = {Bootstrapped Neuro-Simulation for complex robots},
journal = {Robotics and Autonomous Systems},
volume = {136},
pages = {103708},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103708},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305480},
author = {Grant W. Woodford and Mathys C. {du Plessis}},
keywords = {Evolutionary Robotics, Coevolution, Simulator, Artificial Neural Networks, Snake robot, Hexapod robot, Bootstrapped Neuro-Simulation},
abstract = {Robotic simulators are often used to speed up the Evolutionary Robotics (ER) process. Most simulation approaches are based on physics modelling. However, physics-based simulators can become complex to develop and require prior knowledge of the robotic system. Robotics simulators can be constructed using Machine Learning techniques, such as Artificial Neural Networks (ANNs). ANN-based simulator development usually requires a lengthy behavioural data collection period before the simulator can be trained and used to evaluate controllers during the ER process. The Bootstrapped Neuro-Simulation (BNS) approach can be used to simultaneously collect behavioural data, train an ANN-based simulator and evolve controllers for a particular robotic problem. This paper investigates proposed improvements to the BNS approach and demonstrates the viability of the approach by optimising gait controllers for a Hexapod and Snake robot platform.}
}
@article{ZHAO2021103736,
title = {Hierarchical POMDP planning for object manipulation in clutter},
journal = {Robotics and Autonomous Systems},
volume = {139},
pages = {103736},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103736},
url = {https://www.sciencedirect.com/science/article/pii/S092188902100021X},
author = {Wenrui Zhao and Weidong Chen},
keywords = {Object manipulation, Task planning, Motion planning, POMDP, Clutter},
abstract = {Object manipulation planning in clutter suffers from perception uncertainties due to occlusion, as well as action constraints required by collision avoidance. Partially observable Markov decision process (POMDP) provides a general model for planning under uncertainties. But a manipulation task usually have a large action space, which not only makes task planning intractable but also brings significant motion planning effort to check action feasibility. In this work, a new kind of hierarchical POMDP is presented for object manipulation tasks, in which a brief abstract POMDP is extracted and utilized together with the original POMDP. And a hierarchical belief tree search algorithm is proposed for efficient online planning, which constructs fewer belief nodes by building part of the tree with the abstract POMDP and invokes motion planning fewer times by determining action feasibility with observation function of the abstract POMDP. A learning mechanism is also designed in case there are unknown probabilities in transition and observation functions. This planning framework is demonstrated with an object fetching task and the performance is empirically validated by simulations and experiments.}
}
@article{JAQUIER2021103812,
title = {Tensor-variate mixture of experts for proportional myographic control of a robotic hand},
journal = {Robotics and Autonomous Systems},
volume = {142},
pages = {103812},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103812},
url = {https://www.sciencedirect.com/science/article/pii/S092188902100097X},
author = {Noémie Jaquier and Robert Haschke and Sylvain Calinon},
keywords = {Tensor methods, Mixture of experts, Generalized linear model, Tactile myography},
abstract = {When data are organized in matrices or arrays of higher dimensions (tensors), classical regression methods first transform these data into vectors, therefore ignoring the underlying structure of the data and increasing the dimensionality of the problem. This flattening operation typically leads to overfitting when only few training data is available. In this paper, we present a mixture-of-experts model that exploits tensorial representations for regression of tensor-valued data. The proposed formulation takes into account the underlying structure of the data and remains efficient when few training data are available. Evaluation on artificially generated data, as well as offline and real-time experiments recognizing hand movements from tactile myography prove the effectiveness of the proposed approach.}
}
@article{PETERSON2021103801,
title = {Distributed safe planning for satisfying minimal temporal relaxations of TWTL specifications},
journal = {Robotics and Autonomous Systems},
volume = {142},
pages = {103801},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103801},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000865},
author = {Ryan Peterson and Ali Tevfik Buyukkocak and Derya Aksaray and Yasin Yazıcıoğlu},
keywords = {Multi-agent systems, Distributed planning, Formal methods, Collision avoidance},
abstract = {We investigate a multi-agent planning problem, where each agent aims to achieve an individual task while avoiding collisions with other agents. Each agent’s task is expressed as a Time-Window Temporal Logic (TWTL) specification defined over a discretized environment. We propose a distributed receding horizon algorithm for online planning of agent trajectories. We show that under mild assumptions on the environment, the resulting trajectories are always safe (collision-free) and lead to the satisfaction of the TWTL specifications or a finite temporal relaxation. Accordingly, each agent is guaranteed to safely achieve its task, possibly with some minimal finite delay. Performance of the proposed algorithm is demonstrated via numerical simulations and experiments with quadrotors.}
}
@article{AGUIAR2021103725,
title = {Particle filter refinement based on clustering procedures for high-dimensional localization and mapping systems},
journal = {Robotics and Autonomous Systems},
volume = {137},
pages = {103725},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103725},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000105},
author = {André Silva Aguiar and Filipe Neves {dos Santos} and Héber Sobreira and José Boaventura Cunha and Armando Jorge Sousa},
keywords = {SLAM, Clustering, Agricultural robotics},
abstract = {Developing safe autonomous robotic applications for outdoor agricultural environments is a research field that still presents many challenges. Simultaneous Localization and Mapping can be crucial to endow the robot to localize itself with accuracy and, consequently, perform tasks such as crop monitoring and harvesting autonomously. In these environments, the robotic localization and mapping systems usually benefit from the high density of visual features. When using filter-based solutions to localize the robot, such an environment usually uses a high number of particles to perform accurately. These two facts can lead to computationally expensive localization algorithms that are intended to perform in real-time. This work proposes a refinement step to a standard high-dimensional filter-based localization solution through the novelty of downsampling the filter using an online clustering algorithm and applying a scan-match procedure to each cluster. Thus, this approach allows scan-matchers without high computational cost, even in high dimensional filters. Experiments using real data in an agricultural environment show that this approach improves the Particle Filter performance estimating the robot pose. Additionally, results show that this approach can build a precise 3D reconstruction of agricultural environments using visual scans, i.e., 3D scans with RGB information.}
}
@article{RAMEZANIDOORAKI2021103671,
title = {An innovative bio-inspired flight controller for quad-rotor drones: Quad-rotor drone learning to fly using reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {135},
pages = {103671},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103671},
url = {https://www.sciencedirect.com/science/article/pii/S092188902030511X},
author = {Amir {Ramezani Dooraki} and Deok-Jin Lee},
keywords = {Reinforcement learning, Autonomous system, Bio-inspired artificial intelligence, Policy optimization, Artificial neural network, Bio-inspired controller, Machine learning},
abstract = {Animals learn to master their capabilities by trial and error, and with out having any knowledge about their dynamics model and mathematical or physical rules. They use their maximum capabilities in an optimized way. This is the result of millions of years of evolution where the best of different possibilities are kept, and makes us rethink How does the nature perform things?, particularly when natural systems outperform our rigid systems. In this study, inspired by the nature, we developed an innovative algorithm by enhancing an existing reinforcement learning algorithm (proximal policy optimization (PPO)). Our algorithm is capable of learning to control a quad-rotor drone in order to fly. This new algorithm called Bio-inspired Flight Controller (BFC) does not use any conventional controller such as PID or MPC to control the quad-rotor drone. The goal of BFC is to completely replace the conventional controller with a controller that acts in a similar way to the animals where they learn to control their movements. It is capable of stabilizing a quad-copter in a desired point, and following way points. We implemented our algorithm in an AscTec Hummingbird quad-copter simulated in Gazebo, and tested it using different scenarios to fully measure its capabilities.}
}
@article{KHADIVAR2021103700,
title = {Learning dynamical systems with bifurcations},
journal = {Robotics and Autonomous Systems},
volume = {136},
pages = {103700},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103700},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305406},
author = {Farshad Khadivar and Ilaria Lauzana and Aude Billard},
keywords = {Learning from demonstration, Model learning for control, Motion control, Optimization and optimal control},
abstract = {Trajectory planning through dynamical systems (DS) provides robust control for robots and has found numerous applications from locomotion to manipulation. However, to date, DS for controlling rhythmic patterns are distinct from DS used to control point to point motion and current approaches switch at run time across these to enable multiple behaviors. This switching can be brittle and subject to instabilities. We present an approach to embed cyclic and point to point dynamics in a single DS. We offer a method to learn the parameters of complete DS through a two-step optimization. By exploiting Hopf bifurcations, we can explicitly and smoothly transit across periodic and non-periodic phases, linear and nonlinear limit cycles, and non-periodic phases, in addition to changing the equilibrium’s location and the limit cycle’s amplitude. We use diffeomorphism and learn a mapping to modify the learned limit cycle to generate nonlinear limit cycles. The approach is validated with a real 7 DOF KUKA LWR 4+ manipulator to control wiping and with a humanoid robot in simulation.}
}
@article{KUSE2021103813,
title = {Learning whole-image descriptors for real-time loop detection and kidnap recovery under large viewpoint difference},
journal = {Robotics and Autonomous Systems},
volume = {143},
pages = {103813},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103813},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000981},
author = {Manohar Kuse and Shaojie Shen},
keywords = {Kidnap recovery, Loop closure, VINS, Whole image descriptor},
abstract = {We present a real-time stereo visual-inertial-SLAM system which is able to recover from complicated kidnap scenarios and failures online in realtime. We propose to learn the whole-image-descriptor in a weakly supervised manner based on NetVLAD and decoupled convolutions. We analyze the training difficulties in using standard loss formulations and propose an allpairloss and show its effect through extensive experiments. Compared to standard NetVLAD, our network takes an order of magnitude fewer computations and model parameters, as a result runs about three times faster. We evaluate the representation power of our descriptor on standard datasets with precision–recall. Unlike previous loop detection methods which have been evaluated only on fronto-parallel revisits, we evaluate the performance of our method with competing methods on scenarios involving large viewpoint difference. Finally, we present the fully functional system with relative computation and handling of multiple world co-ordinate system which is able to reduce odometry drift, recover from complicated kidnap scenarios and random odometry failures. We open source our fully functional system as an add-on for the popular VINS-Fusion.}
}
@article{ABUDAKKA2021103761,
title = {A probabilistic framework for learning geometry-based robot manipulation skills},
journal = {Robotics and Autonomous Systems},
volume = {141},
pages = {103761},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103761},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000464},
author = {Fares J. Abu-Dakka and Yanlong Huang and João Silvério and Ville Kyrki},
keywords = {Learning from demonstration, Variable impedance, Robot learning, Manipulability ellipsoids, Riemannian manifolds},
abstract = {Programming robots to perform complex manipulation tasks is difficult because many tasks require sophisticated controllers that may rely on data such as manipulability ellipsoids, stiffness/damping and inertia matrices. Such data are naturally represented as Symmetric Positive Definite (SPD) matrices to capture specific geometric characteristics of the data, which increases the complexity of hard-coding them. To alleviate this difficulty, the Learning from Demonstration (LfD) paradigm can be used in order to learn robot manipulation skills with specific geometric constraints encapsulated in SPD matrices. Learned skills often need to be adapted when they are applied to new situations. While existing techniques can adapt Cartesian and joint space trajectories described by various desired points, the adaptation of motion skills encapsulated in SPD matrices remains an open problem. In this paper, we introduce a new LfD framework that can learn robot manipulation skills encapsulated in SPD matrices from expert demonstrations and adapt them to new situations defined by new start-, via- and end-matrices. The proposed approach leverages Kernelized Movement Primitives (KMPs) to generate SPD-based robot manipulation skills that smoothly adapt the demonstrations to conform to new constraints. We validate the proposed framework using a couple of simulations in addition to a real experiment scenario.}
}
@article{MA2021103744,
title = {A new approach to time-optimal trajectory planning with torque and jerk limits for robot},
journal = {Robotics and Autonomous Systems},
volume = {140},
pages = {103744},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103744},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000294},
author = {Jian-wei Ma and Song Gao and Hui-teng Yan and Qi Lv and Guo-qing Hu},
keywords = {Robotic system, Time-optimal trajectory planning, Convex optimization, Torque limits, Jerk limits},
abstract = {In this study, a new convex optimization (CO) approach to time-optimal trajectory planning (TOTP) is described, which considers both torque and jerk limits. The key insight of the approach is that the non-convex jerk limits are transformed to linear acceleration constraints and indirectly introduced into CO as the linear acceleration constraints. In this way, the convexity of CO will not be destroyed and the number of optimization variables will not increase, which give the approach a fast computation speed. The proposed approach is implemented on random geometric path of a 6-DOF manipulator. Compared with a similar method, the results show that the torque and jerk limits are addressed by a reasonable increase in the computation time. In addition, the maximum value of joint jerk reduces by over 80% and the joint torque curves are smoother in the comparison, which demonstrates that this approach has the ability to effectively restrain acceleration mutation.}
}
@article{2023104357,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {160},
pages = {104357},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(22)00246-9},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002469}
}
@article{ROVEDA2021103711,
title = {Human–robot collaboration in sensorless assembly task learning enhanced by uncertainties adaptation via Bayesian Optimization},
journal = {Robotics and Autonomous Systems},
volume = {136},
pages = {103711},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103711},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305510},
author = {Loris Roveda and Mauro Magni and Martina Cantoni and Dario Piga and Giuseppe Bucca},
abstract = {Robots are increasingly exploited in production plants. Within the Industry 4.0 paradigm, the robot complements the human’s capabilities, learning new tasks and adapting itself to compensate for uncertainties. With this aim, the presented paper focuses on the investigation of machine learning techniques to make a sensorless robot able to learn and optimize an industrial assembly task. Relying on sensorless Cartesian impedance control, two main contributions are defined: (1) a task-trajectory learning algorithm based on a few human’s demonstrations (exploiting Hidden Markov Model approach), and (2) an autonomous optimization procedure of the task execution (exploiting Bayesian Optimization). To validate the proposed methodology, an assembly task has been selected as a reference application. The task consists of mounting a gear into its square-section shaft on a fixed base to simulate the assembly of a gearbox. A Franka EMIKA Panda manipulator has been used as a test platform, implementing the proposed methodology. The experiments, carried out on a population of 15 subjects, show the effectiveness of the proposed strategy, making the robot able to learn and optimize its behavior to accomplish the assembly task, even in the presence of task uncertainties.}
}
@article{ABDENNOUR2021103707,
title = {Driver identification using only the CAN-Bus vehicle data through an RCN deep learning approach},
journal = {Robotics and Autonomous Systems},
volume = {136},
pages = {103707},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103707},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305479},
author = {N. Abdennour and T. Ouni and N. Ben Amor},
keywords = {Driver behavior, Identification, Machine learning, CNN, Residual neural network, Classification},
abstract = {In the recent years, many studies claim that humans have a unique driving behavior style that could be used as a fingerprint in recognizing the identity of the driver. With the rising evolution of Machine Learning (ML), the research efforts aiming to take advantage of the human driving style identifiers have been increasing exponentially. For Advanced Driver Assistance Systems (ADAS), this attribute can be an efficient factor to ensure the security and protection of the vehicle. Additionally, it extends the ADAS capabilities by creating different profiles for the drivers, which helps every driver according to his own driving style and improve the ADAS fidelity. Nonetheless, certain problems in the unpredictability of human behavior and the effectiveness of capturing the temporal features of the signal represented an ongoing challenge to accomplish driver identification. In this paper, we propose a novel deep learning approach to driver identification based on a Residual Convolutional Network (RCN). This approach outperforms the existing state of the art methods in less than two hours of training, while simultaneously achieving 99.3% accuracy. The used data are exclusively provided by the Controller Area Network (CAN-Bus) vehicle data that eliminates any privacy invading concerns from the user.}
}
@article{LE2021103775,
title = {6D pose estimation with combined deep learning and 3D vision techniques for a fast and accurate object grasping},
journal = {Robotics and Autonomous Systems},
volume = {141},
pages = {103775},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103775},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000609},
author = {Tuan-Tang Le and Trung-Son Le and Yu-Ru Chen and Joel Vidal and Chyi-Yeu Lin},
keywords = {6D pose estimation, Random bin-picking, Deep learning, Point pair feature},
abstract = {Real-time robotic grasping, supporting a subsequent precise object-in-hand operation task, is a priority target towards highly advanced autonomous systems. However, such an algorithm which can perform sufficiently-accurate grasping with time efficiency is yet to be found. This paper proposes a novel method with a 2-stage approach that combines a fast 2D object recognition using a deep neural network and a subsequent accurate and fast 6D pose estimation based on Point Pair Feature framework to form a real-time 3D object recognition and grasping solution capable of multi-object class scenes. The proposed solution has a potential to perform robustly on real-time applications, requiring both efficiency and accuracy. In order to validate our method, we conducted extensive and thorough experiments involving laborious preparation of our own dataset. The experiment results show that the proposed method scores 97.37% accuracy in 5cm5deg metric and 99.37% in Average Distance metric. Experiment results have shown an overall 62% relative improvement (5cm5deg metric) and 52.48% (Average Distance metric) by using the proposed method. Moreover, the pose estimation execution also showed an average improvement of 47.6% in running time. Finally, to illustrate the overall efficiency of the system in real-time operations, a pick-and-place robotic experiment is conducted and has shown a convincing success rate with 90% of accuracy. This experiment video is available at https://sites.google.com/view/dl-ppf6dpose/.}
}
@article{XU2021103776,
title = {LiDAR–camera calibration method based on ranging statistical characteristics and improved RANSAC algorithm},
journal = {Robotics and Autonomous Systems},
volume = {141},
pages = {103776},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103776},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000610},
author = {Xiaobin Xu and Lei Zhang and Jian Yang and Cong Liu and Yiyang Xiong and Minzhou Luo and Zhiying Tan and Bo Liu},
keywords = {LiDAR, Color camera, Statistical filtering, Adjacent points, Improved RANSAC method},
abstract = {For sensory data fusion, a calibration method between 3D light detection and ranging (LiDAR) and color camera based on ranging statistical characteristics and improved RANSAC algorithm is proposed. The multi-frame LiDAR point cloud data of the calibration triangular board are recorded. The scanned points with close angles are defined a cluster within same degrees. Furthermore, accurate points are preserved using statistical filtering based on Gaussian distribution. Afterwards, the plane and edge parameters of the triangular board are estimated by the reserved point cloud using improved the random sample consensus (RANSAC) algorithm to obtain the 3D locations of the vertices. Meanwhile, corner points in the image can be extracted manually. Finally, the projection matrix between the camera and the LiDAR is estimated by using the 2D–3D​ correspondences in different positions. The projection errors of different frames and corresponding points are calculated. The results demonstrate that the average error with 300 frames is reduced by 23.05% compared to 1 frame. Moreover, the standard deviation diminishes with the increasing of corresponding points. The reliability and advantage of the method are verified compared with other state-of-art methods. It provides theoretical and technical support for low resolution LiDAR.}
}
@article{2023104329,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104329},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(22)00218-4},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002184}
}
@article{DELAMER2021103800,
title = {Safe path planning for UAV urban operation under GNSS signal occlusion risk},
journal = {Robotics and Autonomous Systems},
volume = {142},
pages = {103800},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103800},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000853},
author = {Jean-Alexis Delamer and Yoko Watanabe and Caroline P.C. Chanel},
keywords = {Navigation, Path planning, POMDP, PO-SSP, UAV, Safety},
abstract = {This paper introduces a concept of safe path planning for UAV’s autonomous operation in an urban environment where GNSS-positioning may become unreliable or even unavailable. If the operation environment is a priori known and geo-localized, it is possible to predict a GNSS satellite constellation and hence to anticipate its signal occlusions at a given point and time. Motivated from this, our main idea is to utilize such sensor availability map in path planning task for ensuring UAV navigation safety. The proposed concept is implemented by a Partially Observable Markov Decision Process (POMDP) model. It incorporates a low-level navigation and guidance module for propagating the UAV state uncertainty in function of the probabilistic sensor availability. A new definition of cost function is introduced in this model such that the resulting optimal policy respects a user-defined safety requirement. A goal-oriented version of Monte-Carlo Tree Search algorithm, called POMCP-GO, is proposed for POMDP solving. The developed safe path planner is evaluated on two simple obstacle benchmark maps as well as on a real elevation map of San Diego downtown, along with GPS availability maps.}
}
@article{BALASKA2021103760,
title = {Enhancing satellite semantic maps with ground-level imagery},
journal = {Robotics and Autonomous Systems},
volume = {139},
pages = {103760},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103760},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000452},
author = {Vasiliki Balaska and Loukas Bampis and Ioannis Kansizoglou and Antonios Gasteratos},
keywords = {Semantic segmentation, Semantic maps, Machine learning, Deep Neural Networks, 3D reconstruction, Street and satellite images},
abstract = {The paper at hand introduces a novel system for producing an enhanced semantic map that leverages a reconstruction approach of street-view scenes using computer vision and machine learning techniques. Focusing on the recognition and localization of objects/entities, the composed map combines semantic information from publicly available, yet of lower accuracy, satellite images, with more detailed data from ground-level camera measurements. This merging is achieved by utilizing odometry information from a street-moving vehicle and the 3D reconstruction of its recorded view. Then, the 3D semantic segmentation results are georeferenced and superimposed on the semantic map from the satellite images. In such a way, areas that require fine semantic accuracy can be improved, while the rest are left with the segmentation results of the satellite information. Every part of the proposed system is individually evaluated. We additionally test the overall approach on a case-study of georeferencing new labels of traffic signs, which are detected through a specifically designed classification network over a publicly available dataset collected around the city of Berlin.}
}
@article{ALNAJJAR2021103784,
title = {CHAD: Compact Hand-Assistive Device for enhancement of function in hand impairments},
journal = {Robotics and Autonomous Systems},
volume = {142},
pages = {103784},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103784},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000695},
author = {Fady Alnajjar and Hassan Umari and Waleed K. Ahmed and Munkhjargal Gochoo and Alistair A. Vogan and Adel Aljumaily and Peer Mohamad and Shingo Shimoda},
keywords = {Assistive wearable robot, Post-stroke, Activities of daily living, Soft robotic glove},
abstract = {Hand-assistive devices are used to help post-stroke victims encumbered with hand impairments perform activities of daily living (ADL). Unlike robotic rehabilitation devices used in restricted medical conditions for designated periods, hand-assistive devices are designed to be portable and to be used for extended periods by individuals engaging in ADL. Several hand-assistive device designs have been proposed. With these, designers have focused on key elements, such as size, weight, motion profile of the fingers, and generated grip/pinch force. In this paper, we propose a unique compact hand-assistive device (CHAD) that incorporates most of these design parameters, but with less trade-offs. CHAD consists of a single unit worn on the patient’s forearm, which includes all necessary components. It is compact and does not compromise functionality. The novelty of this design can be found in the use of a unique cable-driven mechanism. This mechanism uses dual linear actuators to achieve the flexion of both the index and the middle fingers via the pull of tendon-like structures originating in two selected interphalangeal joints. This permits the numerous necessary sequences in the motion profiles of the digits. The thumb is also made able to flex with a single linear actuator. Finger extensions, in contrast, are achieved passively via adjustable flexible rubber cords joined to the dorsal side of the glove. Experimental results demonstrate that CHAD generates sufficient force and motion profiles for the comfortable execution of ADL. Additionally, CHAD produces a grip and pinch motion profile similar to that of a natural hand and does not force unwanted muscle activities.}
}
@article{TAFRISHI2021103732,
title = {Line–Circle–Square (LCS): A multilayered geometric filter for edge-based detection},
journal = {Robotics and Autonomous Systems},
volume = {137},
pages = {103732},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103732},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000178},
author = {Seyed Amir Tafrishi and Xiaotian Dai and Vahid {Esmaeilzadeh Kandjani}},
keywords = {Edge detection, Motion field, Geometric filter, Vision},
abstract = {This paper presents a state-of-the-art filter that reduces the complexity in object detection, tracking and mapping applications. Existing edge detection and tracking methods are proposed to create suitable autonomy for mobile robots, however, many of them face overconfidence and large computations at the entrance to scenarios with an immense number of landmarks. The method in this work, the Line–Circle–Square (LCS) filter, claims that mobile robots without a large database for object recognition and highly advanced prediction methods can deal with incoming objects that the camera captures in real-time. The proposed filter applies detection, tracking and learning to each defined expert to extract higher level information for judging scenes without over-calculation. The interactive learning feed between each expert increases the consistency of detected landmarks that works against overwhelming detected features in crowded scenes. Our experts are dependent on trust factors’ covariance under the geometric definitions to ignore, emerge and compare detected landmarks. The experiment validates the effectiveness of the proposed filter in terms of detection precision and resource usage in both experimental and real-world scenarios.}
}
@article{CHEN2021103674,
title = {Mathematical approach for the design configuration of magnetic system with multiple electromagnets},
journal = {Robotics and Autonomous Systems},
volume = {135},
pages = {103674},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103674},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305145},
author = {Ruipeng Chen and David Folio and Antoine Ferreira},
keywords = {Electromagnetic actuation system, Magnetic microrobot, Design methodology, Robotic magnetic platform},
abstract = {Magnetic actuation techniques and microrobots have attracted great interest since they have potential in biomedicine applications. Interventional techniques have emerged as a tool to handle a wide range of minimally invasive surgeries (MIS). However, current MIS procedures are constrained by the limitation of manual operation by surgeon. Thus, various microrobotic solutions including magnetic navigation systems have been proposed for MIS, which carries many potential benefits such as reduced incision, less intraoperative hemorrhaging and postoperative pain, and faster recovery time. In recent decades, many electromagnetic actuation (EMA) systems have been reported and involved to general surgery. The EMA system allows to generate efficiently magnetic source for microrobot control when its specifications are further investigated and satisfied for the desired application. To precisely manipulate the biomedical microrobot, a key issue still relies on the design of a suitable EMA platform. In this paper, we demonstrate a mathematical approach for the design configuration of magnetic system with multiple electromagnets. Especially, the required magnetic coil number has been investigated where the heading motion control, magnetic force control and their combination control are discussed respectively. The singular cases of control are pre-evaluated by a mathematical analysis of the simulated electromagnetic field. In addition, the placed positions and tilted orientations of the applied electromagnets are investigated for the optimization regarding the six typical configurations of EMA platform with 4, 6 and 8 coils. The various configurations of EMA systems have been comprehensively analyzed. Therefore, with the number of electromagnets and their optimal configuration obtained by the proposed approach, the EMA system can be initially established.}
}
@article{RATO2021103714,
title = {LIDAR based detection of road boundaries using the density of accumulated point clouds and their gradients},
journal = {Robotics and Autonomous Systems},
volume = {138},
pages = {103714},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103714},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305546},
author = {Daniela Rato and Vítor Santos},
keywords = {LIDAR, Road curbs, Point clouds, Occupancy grid, Gradient},
abstract = {Autonomous driving and driver assistance require a continuous and reliable perception of the road boundaries, namely curbs and berms, including also other minor, or not so minor, obstacles in the neighborhood of the car. This paper proposes to use a 4-layer LIDAR placed close to the ground to capture measurements of the road ahead of the car and allow the detection of the boundaries. This setup provides a special point of view that allows the accumulation of points on vertical surfaces on the road as the car moves, which increases the point density in vertical surfaces but keeps it limited in horizontal surfaces. This technique allows to successfully distinguish curbs from the flat parts of the road. However, this approach has some limitations, namely to detect berms, and another approach had to be developed using the gradient of point density, which extends the detection capabilities to berms and negative obstacles. This is achieved by flattening the point clouds to 2D and use traditional computer vision gradient and edge detection techniques, which also improves the processing speed. Results are obtained on the ATLASCAR real system, at different velocities, and a good performance is reached when comparing to a manually created ground truth.}
}
@article{PAHIC2021103690,
title = {Robot skill learning in latent space of a deep autoencoder neural network},
journal = {Robotics and Autonomous Systems},
volume = {135},
pages = {103690},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103690},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305303},
author = {Rok Pahič and Zvezdan Lončarević and Andrej Gams and Aleš Ude},
keywords = {Skill learning, Latent space representations, Deep autoencoder neural networks},
abstract = {Just like humans, robots can improve their performance by practicing, i.e. by performing the desired behavior many times and updating the underlying skill representation using the newly gathered data. In this paper, we propose to implement robot practicing by applying statistical and reinforcement learning (RL) in a latent space of the selected skill representation. The latent space is computed by a deep autoencoder neural network, with the data to train the network generated in simulation. However, we show that the resulting latent space representation is useful also for learning on a real robot. Our simulation and real-world results demonstrate that by exploiting the latent space of the underlying motor skill representation, a significant reduction of the amount of data needed for effective learning by Gaussian Process Regression (GPR) can be achieved. Similarly, the number of RL epochs can be significantly reduced. Finally, it is evident from our results that an autoencoder-based latent space is more effective for these purposes than a latent space computed by principal component analysis.}
}
@article{EBEL2021103686,
title = {A comparative look at two formation control approaches based on optimization and algebraic graph theory},
journal = {Robotics and Autonomous Systems},
volume = {136},
pages = {103686},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103686},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305261},
author = {Henrik Ebel and Peter Eberhard},
keywords = {Formation control, Model predictive control, Distributed control, Algebraic graph theory, Mobile robots, Robotic network, Experiments},
abstract = {This paper takes a novel look at formation control by comparing control setups based on two very different frameworks. These are applied to the distributed control of communicating omnidirectional mobile robots. One framework, which is possibly the most common approach to formation control, is based on algebraic graph theory, whereas the other, namely distributed model predictive control (DMPC), is based on distributed optimization, representing a rather uncommon view on the task. In this study, formation control is understood as the task of attaining and maintaining a specific relative positioning between robotic agents while moving the formation through the environment. While interesting on its own, formation control can serve as the basis for superordinate tasks like cooperative transportation. For an encompassing treatment of the task, two different control goals are considered, resulting in different setups for each control framework. One goal consists of moving the formation’s geometric center to a specific position, whereas the other aims at letting the whole formation move with the desired velocity. In both cases, the involved robots are subject to input constraints. Already during control design, some qualitative differences between the two frameworks become apparent, with the DMPC controller exhibiting characteristic beneficial qualities in exchange for its higher computational demand. Results from various simulation scenarios confirm these observations. Considerations on the practical implementation of the two schemes, as well as hardware experiments with tailor-made mobile robots, provide valuable insight for robotics practitioners, and highlight the applicability of the two frameworks.}
}
@article{ZHU2021103798,
title = {Vision-based manipulation of deformable and rigid objects using subspace projections of 2D contours},
journal = {Robotics and Autonomous Systems},
volume = {142},
pages = {103798},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103798},
url = {https://www.sciencedirect.com/science/article/pii/S092188902100083X},
author = {Jihong Zhu and David Navarro-Alarcon and Robin Passama and Andrea Cherubini},
keywords = {Visual servoing, Sensor-based control, Deformable object manipulation},
abstract = {This paper proposes a unified vision-based manipulation framework using image contours of deformable/rigid objects. Instead of explicitly defining the features by geometries or functions, the robot automatically learns the visual features from processed vision data. Our method simultaneously generates – from the same data – both visual features and the interaction matrix that relates them to the robot control inputs. Extraction of the feature vector and control commands is done online and adaptively, and requires little data for initialization. Our method allows the robot to manipulate an object without knowing whether it is rigid or deformable. To validate our approach, we conduct numerical simulations and experiments with both deformable and rigid objects.}
}
@article{KONTOUDIS2021103811,
title = {Model-based learning of underwater acoustic communication performance for marine robots},
journal = {Robotics and Autonomous Systems},
volume = {142},
pages = {103811},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103811},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000968},
author = {George P. Kontoudis and Stephen Krauss and Daniel J. Stilwell},
keywords = {Model-based learning, Autonomous underwater vehicles, Wireless communications, Spatial statistics, Kriging},
abstract = {Accurate prediction of acoustic communication performance is an important capability for marine robots. In this paper, we propose a model-based learning methodology for the prediction of underwater acoustic communication performance. The learning algorithm consists of two steps: (i) estimation of the covariance matrix by evaluating candidate functions with estimated parameters using detrended measurements;and (ii) prediction of communication performance. Covariance estimation is addressed with a multi-stage iterative training method that produces unbiased and robust results with nested models. The efficiency of the framework is validated with simulations and experimental data from field trials. The field trials involved a manned surface vehicle and an autonomous underwater vehicle.}
}
@article{JAMWAL2021103787,
title = {Intrinsically compliant parallel robot for fractured femur reduction: Mechanism optimization and control},
journal = {Robotics and Autonomous Systems},
volume = {141},
pages = {103787},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103787},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000725},
author = {Prashant K. Jamwal and Shahid Hussain and Mergen H. Ghayesh},
keywords = {Femur, Fracture reduction, Parallel robot, Compliant actuation, Robot control, Optimization},
abstract = {A robotic system for the reduction of fractured femur bone is proposed in this research to help orthopedics during the labor intensive bone reduction procedures and also save them from radiation stimulated environment. Fractured femur reduction is a good candidate for robotics application owing to its elongated anatomy and strong counteracting forces from surrounding muscles. However, the robot forces should be compliant, and motions need to be accurate. Aiming to achieve these two conflicting objective, a parallel robot actuated by six intrinsically compliant actuators is being proposed here. After an initial design analysis, three performance metrics, namely, the conditioning index, actuator force index and interaction compliance index were identified and formulated. An evolutionary algorithm SPEA2 was employed to simultaneously optimize these objectives by varying the key robot design variables. Subsequent to the optimization, an optimal robot design is obtained which provides the best trade-off between the performance measures. Initial proof of concept experiments were carried out whereby the robot was tested for trajectory following accuracies while maneuvering the moving platform about the three axes. A fuzzy based closed loop feedback controller was implemented on the robot. Excellent trajectory tracking results were observed in response to the sinusoidal inputs.}
}
@article{CHEN2021103735,
title = {Accurate and real-time human-joint-position estimation for a patient-transfer robot using a two-level convolutional neutral network},
journal = {Robotics and Autonomous Systems},
volume = {139},
pages = {103735},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103735},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000208},
author = {Mengqian Chen and Jiang Wu and Shunda Li and Jinyue Liu and Hideo Yokota and Shijie Guo},
keywords = {Patient-transfer robot, Human-joint-position estimation, Information fusion, Convolution neural network},
abstract = {Human-joint-position estimation is crucial for patient-transfer robots. However, high accuracy and real-time property are difficult to achieve simultaneously. To tackle the problem, we develop a new convolutional neural network (CNN), containing two levels of subnetworks, to fuse the information in color and depth images. The first-level subnetwork generates two-dimensional (2D) human joint positions from a color image by the part-affinity-fields method. The second-level subnetwork estimates 3D human-joint positions from 2D ones and corresponding depth images. Here, strong feature-extraction function of the CNN may suppress the negative effect caused by invalid information in depth images. Meanwhile, all the estimations are implemented with the 2D CNNs, which may cause higher time-efficiency than 3D ones (mostly used in previous studies). To assess the validity, first we employed the CNN to estimate human joint positions, and obtained the accuracy and speed of respectively 90.3% and 210 ms (implemented with an affordable processing unit). Then we applied the CNN to a dual-arm nursing-care robot and found that the accuracy and processing speed satisfied the requirements in practical usage; these validated the effectiveness of our proposal and provided a new approach to generate 3D-human-joint positions through information fusion of color and depth images.}
}
@article{DEVO2021103799,
title = {Enhancing continuous control of mobile robots for end-to-end visual active tracking},
journal = {Robotics and Autonomous Systems},
volume = {142},
pages = {103799},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103799},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000841},
author = {Alessandro Devo and Alberto Dionigi and Gabriele Costante},
keywords = {Visual active tracking, Deep learning for robotic applications, Reinforcement learning},
abstract = {In the last decades, visual target tracking has been one of the primary research interests of the Robotics research community. The recent advances in Deep Learning technologies have made the exploitation of visual tracking approaches effective and possible in a wide variety of applications, ranging from automotive to surveillance and human assistance. However, the majority of the existing works focus exclusively on passive visual tracking, i.e., tracking elements in sequences of images by assuming that no actions can be taken to adapt the camera position to the motion of the tracked entity. On the contrary, in this work, we address visual active tracking, in which the tracker has to actively search for and track a specified target. Current State-of-the-Art approaches use Deep Reinforcement Learning (DRL) techniques to address the problem in an end-to-end manner. However, two main problems arise: (i) most of the contributions focus only on discrete action spaces, and the ones that consider continuous control do not achieve the same level of performance; and (ii) if not properly tuned, DRL models can be challenging to train, resulting in considerably slow learning progress and poor final performance. To address these challenges, we propose a novel DRL-based visual active tracking system that provides continuous action policies. To accelerate training and improve the overall performance, we introduce additional objective functions and a Heuristic Trajectory Generator (HTG) to facilitate learning. Through extensive experimentation, we show that our method can reach and surpass other State-of-the-Art approaches performances, and demonstrate that, even if trained exclusively in simulation, it can successfully perform visual active tracking even in real scenarios.}
}
@article{2023104421,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {163},
pages = {104421},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(23)00060-X},
url = {https://www.sciencedirect.com/science/article/pii/S092188902300060X}
}
@article{XU2021103783,
title = {Force analysis of the redundantly actuated parallel mechanism 2RP̲R+P considering different control methodologies},
journal = {Robotics and Autonomous Systems},
volume = {142},
pages = {103783},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103783},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000683},
author = {Yundou Xu and Ze Jiang and Zhongjin Ju and Zengzhao Wang and Wenlan Liu and Yongsheng Zhao},
keywords = {Parallel robots, Redundantly actuated, Force–control},
abstract = {In the problem of driving forces/torques distribution of the redundantly actuated parallel mechanisms (PMs), although numerous optimization analysis methods, including the minimum input torque method and minimum energy consumption method have been proposed so far, however, the actual control modes of the actuators were not taken into account among the existing methods for the above problem. Therefore, the present study comprehensively considers both the elastic deformation and actuator’s displacement of each limb, proposes the idea of ”displacement coordination” and establishes the overall displacement coordination equations of the mechanisms. Three different control methodologies of the redundantly actuated PMs, including the full-position methodology, hybrid position–force control methodology and full-force methodology, are studied. For each control methodology, the correlation among the driving forces/torques, actuators’ displacements, external loads and limbs’ stiffness are discussed. An experimental platform of a redundantly actuated PM is built, and the corresponding test investigations for three control methodologies are conducted. In the present study, different control methodologies of the redundantly actuated PMs are considered for the first time, the principle of the dynamic coordination distribution is revealed in different methodologies, which have important reference values for design of coordinated motion control strategy of such kind of mechanisms.}
}
@article{SHANTIA2021103731,
title = {Two-stage visual navigation by deep neural networks and multi-goal reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {138},
pages = {103731},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103731},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000166},
author = {Amirhossein Shantia and Rik Timmers and Yiebo Chong and Cornel Kuiper and Francesco Bidoia and Lambert Schomaker and Marco Wiering},
keywords = {Robotic navigation, Reinforcement learning, Deep neural networks, Localization and mapping, Robot simulation},
abstract = {In this paper, we propose a two-stage learning framework for visual navigation in which the experience of the agent during exploration of one goal is shared to learn to navigate to other goals. We train a deep neural network for estimating the robot’s position in the environment using ground truth information provided by a classical localization and mapping approach. The second simpler multi-goal Q-function learns to traverse the environment by using the provided discretized map. Transfer learning is applied to the multi-goal Q-function from a maze structure to a 2D simulator and is finally deployed in a 3D simulator where the robot uses the estimated locations from the position estimator deep network. In the experiments, we first compare different architectures to select the best deep network for location estimation, and then compare the effects of the multi-goal reinforcement learning method to traditional reinforcement learning. The results show a significant improvement when multi-goal reinforcement learning is used. Furthermore, the results of the location estimator show that a deep network can learn and generalize in different environments using camera images with high accuracy in both position and orientation.}
}
@article{SU2021103759,
title = {GR-LOAM: LiDAR-based sensor fusion SLAM for ground robots on complex terrain},
journal = {Robotics and Autonomous Systems},
volume = {140},
pages = {103759},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103759},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000440},
author = {Yun Su and Ting Wang and Shiliang Shao and Chen Yao and Zhidong Wang},
keywords = {Simultaneous localization and mapping (SLAM), Ground robot, Encoder, Sensor fusion, Tight coupling scheme},
abstract = {Simultaneous localization and mapping is a fundamental process in robot navigation. We focus on LiDAR to complete this process in ground robots traveling on complex terrain by proposing GR-LOAM, a method to estimate robot ego-motion by fusing LiDAR, inertial measurement unit (IMU), and encoder measurements in a tightly coupled scheme. First, we derive a odometer increment model that fuses the IMU and encoder measurements to estimate the robot pose variation on a manifold. Then, we apply point cloud segmentation and feature extraction to obtain distinctive edge and planar features. Moreover, we propose an evaluation algorithm for the sensor measurements to detect abnormal data and reduce their corresponding weight during optimization. By jointly optimizing the cost derived from the LiDAR, IMU, and encoder measurements in a local window, we obtain low-drift odometry even on complex terrain. We use the estimated relative pose in the local window to reevaluate the matching distance across features and remove dynamic objects and outliers, thus refining the features before being fed to a mapping thread and increasing the mapping efficiency. In the back end, GR-LOAM uses the refined point cloud and tightly couples the IMU and encoder measurements with ground constraints to further refine the estimated pose by aligning the features on a global map. Results from extensive experiments performed in indoor and outdoor environments using real ground robot demonstrate the high accuracy and robustness of the proposed GR-LOAM for state estimation of ground robots.}
}
@article{BOULARES2021103673,
title = {A novel UAV path planning algorithm to search for floating objects on the ocean surface based on object’s trajectory prediction by regression},
journal = {Robotics and Autonomous Systems},
volume = {135},
pages = {103673},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103673},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305133},
author = {Mehrez Boulares and Ahmed Barnawi},
keywords = {Dynamic target path prediction, UAV, High dense clustering, Surface Ekman current, Machine Leaning Regression},
abstract = {Search and find mission in ocean environment is a none trivial operation given the amount of random parameters associated with it. The uncertain and dynamic aspects related to ocean current movement make the trajectory prediction of drifting lost object onto sea water a very complicated task. In this work we present a novel lost target searching algorithm based on Recursive Area Clustering and target trajectory predication in ocean environment. Based on the widely known GlobCurrent v2 dataset which model the drifting of ocean surface current using satellite sensory data combined with mathematical and simulation modeling, we propose a regression algorithm based on our Recursive Area Clustering algorithm that we have developed previously to determine the strategic zones (weight centers) characterizing the high density areas extracted from drifting target history. Given those weight centers, we predict the object trajectory through refined regression. The predicted lost object trajectory is used to plan the path of UAV search mission. The model developed has a significant impact as we have tested our strategy in a scenario for searching an area covering 68517 km2, we have shown that 78% of the time, the lost object can be found within 32 km distance of the predicted trajectories limiting the significant search area to be about 5% of the whole searched area.}
}
@article{2022104252,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {156},
pages = {104252},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(22)00147-6},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001476}
}
@article{ZENG2021103668,
title = {Learning compliant robotic movements based on biomimetic motor adaptation},
journal = {Robotics and Autonomous Systems},
volume = {135},
pages = {103668},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103668},
url = {https://www.sciencedirect.com/science/article/pii/S092188902030508X},
author = {Chao Zeng and Xiongjun Chen and Ning Wang and Chenguang Yang},
keywords = {Compliant robotic movements, Biomimetic motor control, Impedance adaptation, Learning from demonstration (LfD), Human–robot interaction and collaboration},
abstract = {It is one of the great challenges for a robot to learn compliant movements in interaction tasks. The robot can easily acquire motion skills from a human tutor by kinematics demonstration, however, this becomes much more difficult when it comes to the compliant skills. This paper aims to provide a possible solution to address this problem by proposing a two-stage approach. In the first stage, the human tutor demonstrates the robot how to perform a task, during which only motion trajectories are recorded without the involvement of force sensing. A dynamical movement primitives (DMPs) model which can generate human-like motion is then used to encode the kinematics data. In the second stage, a biomimetic controller, which is inspired by the neuroscience findings in human motor learning, is employed to obtain the desired robotic compliant behaviors by online adapting the impedance profiles and the feedforward torques simultaneously. Several tests are conducted to validate the effectiveness of the proposed approach.}
}
@article{2022104213,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {155},
pages = {104213},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(22)00123-3},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001233}
}
@article{WOOSLEY2021103713,
title = {Multi-robot goal conflict resolution under communication constraints using spatial approximation and strategic caching},
journal = {Robotics and Autonomous Systems},
volume = {138},
pages = {103713},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103713},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305534},
author = {Bradley Woosley and Prithviraj Dasgupta and John G. Rogers and Jeffery Twigg},
keywords = {Multi-robot, Communications constrained environment, Goal conflict resolution, Alpha Shapes},
abstract = {We consider the problem of distributed goal conflict resolution in multi-robot systems while remaining resilient to intermittent communication losses between robots. Our proposed approach uses a spatial approximation technique called α-shape to represent the regions that have been explored by robots followed by a O(logn) algorithm that incrementally combines and shares the α-shape information between robots along the robots’ communication tree and rapidly checks for conflicts of a robot’s selected location. We provide theoretical guarantees of the time complexity of our proposed algorithm along with experimental results with simulated and physical robots in different environments. The results show that our approach can rapidly determine conflicts between goal locations selected by multiple robots as well as reduce message loss and re-transmissions between robots. These result in more efficient inter-robot communications as well as less extraneous distance traveled by robots, as compared to a flooding-based communications approach.}
}
@article{GHARAJEH2020103669,
title = {Hybrid Global Positioning System-Adaptive Neuro-Fuzzy Inference System based autonomous mobile robot navigation},
journal = {Robotics and Autonomous Systems},
volume = {134},
pages = {103669},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103669},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305091},
author = {Mohammad Samadi Gharajeh and Hossein B. Jond},
keywords = {Adaptive Neuro-Fuzzy Inference System (ANFIS), Autonomous mobile robot, Global Positioning System (GPS), Obstacle avoidance},
abstract = {The collision-free navigation of a mobile robot in clutter environments is challenging. Global Positioning System (GPS) and adaptive neuro-fuzzy inference system (ANFIS) are well-known techniques widely used for navigation and control, respectively. This paper proposes a hybrid GPS-ANFIS based method for collision-free navigation of autonomous mobile robots. The GPS-based controller keeps the navigation direction of the robot toward the static or dynamic target. It uses the coordinates received from the two GPS modules on the edges of the longitudinal axis of the robot all together with the coordinates of the target to divert it from the current path making a certain angle towards the target. The performance of the proposed method in navigating a mobile robot in clutter environments and its effectiveness in comparison with the other collision-free navigation methods has been evaluated through simulations. The evaluation criteria are on the basis of the obstacle avoidance behavior and the length of the discovered collision-free path by the robot. The results have shown that our hybrid GPS-ANFIS method navigates the robot toward the goal via a shorter path while avoiding the obstacles.}
}
@article{2023104475,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {166},
pages = {104475},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(23)00114-8},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023001148}
}
@article{SCHAEFER2021103709,
title = {Long-term vehicle localization in urban environments based on pole landmarks extracted from 3-D lidar scans},
journal = {Robotics and Autonomous Systems},
volume = {136},
pages = {103709},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103709},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305492},
author = {Alexander Schaefer and Daniel Büscher and Johan Vertens and Lukas Luft and Wolfram Burgard},
keywords = {Mapping, Localization, Lidar, Pole, Landmark, Feature extraction, Autonomous driving},
abstract = {Due to their ubiquity and long-term stability, pole-like objects are well suited to serve as landmarks for vehicle localization in urban environments. In this work, we present a complete mapping and long-term localization system based on pole landmarks extracted from 3-D lidar data. Our approach features a novel pole detector, a mapping module, and an online localization module, each of which are described in detail, and for which we provide an open-source implementation (Schaefer and Büscher, 0000). In extensive experiments, we demonstrate that our method improves on the state of the art with respect to long-term reliability and accuracy: First, we prove reliability by tasking the system with localizing a mobile robot over the course of 15 months in an urban area based on an initial map, confronting it with constantly varying routes, differing weather conditions, seasonal changes, and construction sites. Second, we show that the proposed approach clearly outperforms a recently published method in terms of accuracy.}
}
@article{2022104298,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {158},
pages = {104298},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(22)00187-7},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001877}
}
@article{THOMAS2021103786,
title = {MPTP: Motion-planning-aware task planning for navigation in belief space},
journal = {Robotics and Autonomous Systems},
volume = {141},
pages = {103786},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103786},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000713},
author = {Antony Thomas and Fulvio Mastrogiovanni and Marco Baglietto},
keywords = {Task-motion planning, Belief space planning, Autonomous navigation},
abstract = {We present an integrated Task-Motion Planning (TMP) framework for navigation in large-scale environments. Of late, TMP for manipulation has attracted significant interest resulting in a proliferation of different approaches. In contrast, TMP for navigation has received considerably less attention. Autonomous robots operating in real-world complex scenarios require planning in the discrete (task) space and the continuous (motion) space. In knowledge-intensive domains, on the one hand, a robot has to reason at the highest-level, for example, the objects to procure, the regions to navigate to in order to acquire them; on the other hand, the feasibility of the respective navigation tasks have to be checked at the execution level. This presents a need for motion-planning-aware task planners. In this paper, we discuss a probabilistically complete approach that leverages this task-motion interaction for navigating in large knowledge-intensive domains, returning a plan that is optimal at the task-level. The framework is intended for motion planning under motion and sensing uncertainty, which is formally known as belief space planning. The underlying methodology is validated in simulation, in an office environment and its scalability is tested in the larger Willow Garage world. A reasonable comparison with a work that is closest to our approach is also provided. We also demonstrate the adaptability of our approach by considering a building floor navigation domain. Finally, we also discuss the limitations of our approach and put forward suggestions for improvements and future work.}
}
@article{HIETANEN2021103810,
title = {Benchmarking pose estimation for robot manipulation},
journal = {Robotics and Autonomous Systems},
volume = {143},
pages = {103810},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103810},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000956},
author = {Antti Hietanen and Jyrki Latokartano and Alessandro Foi and Roel Pieters and Ville Kyrki and Minna Lanz and Joni-Kristian Kämäräinen},
keywords = {Object pose estimation, Robot manipulation, Evaluation},
abstract = {Robot grasping and manipulation require estimation of 3D object poses. Recently, a number of methods and datasets for vision-based pose estimation have been proposed. However, it is unclear how well the performance measures developed for visual pose estimation predict success in robot manipulation. In this work, we introduce an approach that connects error in pose and success in robot manipulation, and propose a probabilistic performance measure of the task success rate. A physical setup is needed to estimate the probability densities from real world samples, but evaluation of pose estimation methods is offline using captured test images, ground truth poses and the estimated densities. We validate the approach with four industrial manipulation tasks and evaluate a number of publicly available pose estimation methods. The popular pose estimation performance measure, Average Distance of Corresponding model points (ADC), does not offer any quantitatively meaningful indication of the frequency of success in robot manipulation. Our measure is instead quantitatively informative: e.g., a score of 0.24 corresponds to average success probability of 24%.}
}
@article{AGRAWAL2021103758,
title = {Analyzing the effectiveness of rescheduling and Flexible Execution methods to address uncertainty in execution duration for a planetary rover},
journal = {Robotics and Autonomous Systems},
volume = {140},
pages = {103758},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103758},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000439},
author = {Jagriti Agrawal and Wayne Chi and Steve Chien and Gregg Rabideau and Daniel Gaines and Stephen Kuhn},
keywords = {Scheduling, Rescheduling, Flexible Execution},
abstract = {During execution, activity durations may vary from those predicted in the generated schedule. In this article we study (re) scheduling invocation, execution during rescheduling, and flexible execution to enable a high level of responsiveness to uncertainty in activity execution duration. We discuss these methods theoretically in the context of an embedded scheduler and practically in the context of a limited CPU embedded scheduler with a nonzero scheduler runtime intended for a planetary rover. We use the concept of a commit window to enable execution of the previously generated schedule while (re) scheduling. We define Fixed Cadence and Event Driven scheduling as methods to decide when to reinvoke the scheduler. We define and analyze Flexible Execution (FE) as an approach to execute the generated schedule while adapting it to variations in execution. Specifically, FE focuses on (1) how to take advantage of activities ending earlier than expected and (2) how to maintain a consistent schedule if activities take more time than expected. We present a theoretical model and empirical results documenting how these various methods interact and perform on both synthetic data and best available data for NASA’s next planetary rover, the Mars 2020 rover. We then describe how these analyses influenced the onboard software for the Mars 2020 rover.}
}
@article{MOKOGWU2021103781,
title = {A hybrid position–rate teleoperation system},
journal = {Robotics and Autonomous Systems},
volume = {141},
pages = {103781},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103781},
url = {https://www.sciencedirect.com/science/article/pii/S092188902100066X},
author = {Chiedu N. Mokogwu and Keyvan Hashtrudi-Zaad},
keywords = {Position mode, Rate mode, Range sensor, Stability, Bilateral teleoperation, Workspace expansion},
abstract = {Position resolution is a major problem in teleoperation applications with significant disparity between master and slave workspace. While rate mode control is suitable for slave free motion operations, it poses significant stability and performance challenges for task manipulation. In this paper, we propose a hybrid control scheme that offers seamless transition between position and rate control modes based on the environment location information collected from a range sensor. The system incorporates the strengths of position and rate control modes while masking their shortcomings. Experiments to determine the viability of this method are carried out on a single degree-of-freedom teleoperation test-bed.}
}
@article{2022104140,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {153},
pages = {104140},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(22)00080-X},
url = {https://www.sciencedirect.com/science/article/pii/S092188902200080X}
}
@article{DEQUEIROZMENDES2021103701,
title = {On deep learning techniques to boost monocular depth estimation for autonomous navigation},
journal = {Robotics and Autonomous Systems},
volume = {136},
pages = {103701},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103701},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305418},
author = {Raul {de Queiroz Mendes} and Eduardo Godinho Ribeiro and Nicolas {dos Santos Rosa} and Valdir Grassi},
keywords = {SIDE, CNN, Deep learning},
abstract = {Inferring the depth of images is a fundamental inverse problem within the field of Computer Vision since depth information is obtained through 2D images, which can be generated from infinite possibilities of observed real scenes. Benefiting from the progress of Convolutional Neural Networks (CNNs) to explore structural features and spatial image information, Single Image Depth Estimation (SIDE) is often highlighted in scopes of scientific and technological innovation, as this concept provides advantages related to its low implementation cost and robustness to environmental conditions. In the context of autonomous vehicles, state-of-the-art CNNs optimize the SIDE task by producing high-quality depth maps, which are essential during the autonomous navigation process in different locations. However, such networks are usually supervised by sparse and noisy depth data, from Light Detection and Ranging (LiDAR) laser scans, and are carried out at high computational cost, requiring high-performance Graphic Processing Units (GPUs). Therefore, we propose a new lightweight and fast supervised CNN architecture combined with novel feature extraction models which are designed for real-world autonomous navigation. We also introduce an efficient surface normals module, jointly with a simple geometric 2.5D loss function, to solve SIDE problems. We also innovate by incorporating multiple Deep Learning techniques, such as the use of densification algorithms and additional semantic, surface normals and depth information to train our framework. The method introduced in this work focuses on robotic applications in indoor and outdoor environments and its results are evaluated on the competitive and publicly available NYU Depth V2 and KITTI Depth datasets.}
}
@article{TAYLOR2021103754,
title = {The impact of catastrophic collisions and collision avoidance on a swarming behavior},
journal = {Robotics and Autonomous Systems},
volume = {140},
pages = {103754},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103754},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000397},
author = {Chris Taylor and Cameron Nowzari},
abstract = {Swarms of autonomous agents are useful in many applications due to their ability to accomplish tasks in a decentralized manner, making them more robust to failures. Due to the difficulty in running experiments with large numbers of hardware agents, researchers often make simplifying assumptions and remove constraints that might be present in a real swarm deployment. While simplifying away some constraints is tolerable, we feel that two in particular have been overlooked: one, that agents in a swarm take up physical space, and two, that agents might be damaged in collisions. Many existing works assume agents have negligible size or pass through each other with no added penalty. It seems possible to ignore these constraints using collision avoidance, but we show using an illustrative example that this is easier said than done. In particular, we show that collision avoidance can interfere with the intended swarming behavior and significant parameter tuning is necessary to ensure the behavior emerges as best as possible while collisions are avoided. We compare four different collision avoidance algorithms, two of which we consider to be the best decentralized collision avoidance algorithms available. Despite putting significant effort into tuning each algorithm to perform at its best, we believe our results show that further research is necessary to develop swarming behaviors that can achieve their goal while avoiding collisions with agents of non-negligible volume.}
}
@article{TSINTOTAS2021103782,
title = {Modest-vocabulary loop-closure detection with incremental bag of tracked words},
journal = {Robotics and Autonomous Systems},
volume = {141},
pages = {103782},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103782},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000671},
author = {Konstantinos A. Tsintotas and Loukas Bampis and Antonios Gasteratos},
keywords = {Loop-closure detection, Mapping, Recognition, SLAM, Visual-based navigation},
abstract = {A key feature in the context of simultaneous localization and mapping is loop-closure detection, a process determining whether the current robot’s environment perception coincides with previous observation. However, in long-term operations, both computational efficiency and memory requirements involved in an autonomous robot operation in uncontrolled environments, are of particular importance. The majority of approaches scale linearly with the environment’s size in terms of storage and query time. The article at hand presents an efficient appearance-based loop-closure detection pipeline, which encodes the traversed trajectory by a low amount of unique visual words generated on-line through feature tracking. The incrementally constructed visual vocabulary is referred to as the “Bag of Tracked Words.” A nearest-neighbor voting scheme is utilized to query the database and assign probabilistic scores to all visited locations. Exploiting the inherent temporal coherency in the loop-closure task, the produced scores are processed through a Bayesian filter to estimate the belief state about the robot’s location on the map. Also, a geometrical verification step ensures consistency between image matches. Management is also applied to the resulting vocabulary to reduce its growth rate and constraint the system’s computational complexity while improving its voting distinctiveness. The proposed approach’s performance is experimentally evaluated on several publicly available and challenging datasets, including hand-held, car-mounted, aerial, and ground trajectories. Results demonstrate the method’s adaptability, which retains high operational frequency in environments of up to 13 km and high recall rates for perfect precision, outperforming other state-of-the-art techniques. The system’s effectiveness is owed to the reduced vocabulary size, which is at least one order of magnitude smaller than other contemporary approaches. An open research-oriented source code has been made publicly available, which is dubbed as “BoTW-LCD.”}
}
@article{BEJJANI2021103730,
title = {Learning image-based Receding Horizon Planning for manipulation in clutter},
journal = {Robotics and Autonomous Systems},
volume = {138},
pages = {103730},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103730},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000154},
author = {Wissam Bejjani and Matteo Leonetti and Mehmet R. Dogar},
keywords = {Manipulation in clutter, Physics-based manipulation, Heuristic learning, Receding Horizon Planning, Imitation and reinforcement learning, Abstract state representation},
abstract = {The manipulation of an object into a desired location in a cluttered and restricted environment requires reasoning over the long-term consequences of an action while reacting locally to the multiple physics-based interactions. We present Visual Receding Horizon Planning (VisualRHP) in a framework which interleaves real-world execution with look-ahead planning to efficiently solve a short-horizon approximation to a multi-step sequential decision making problem. VisualRHP is guided by a learned heuristic that acts on an abstract colour-labelled image-based representation of the state. With this representation, the robot can generalize its behaviours to different environment setups, that is, different number and shape of objects, while also having transferable manipulation skills that can be applied to a multitude of real-world objects. We train the heuristic with imitation and reinforcement learning in discrete and continuous actions spaces. We detail our heuristic learning process for environments with sparse rewards, and non-linear, non-continuous, dynamics. In particular, we introduce necessary changes for improving the stability of existing reinforcement learning algorithms that use neural networks with shared parameters. In a series of simulation and real-world experiments, we show the robot performing prehensile and non-prehensile actions in synergy to successfully manipulate a variety of real-world objects in real-time.}
}
@article{BA2021103704,
title = {Dynamics compensation of impedance-based motion control for LHDS of legged robot},
journal = {Robotics and Autonomous Systems},
volume = {139},
pages = {103704},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103704},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305443},
author = {Kaixian Ba and Yanhe Song and Bin Yu and Xiaolong He and Zhipeng Huang and Chunhe Li and Lipeng Yuan and Xiangdong Kong},
keywords = {Legged robot, Leg hydraulic drive system (LHDS), Dynamics compensation, Impedance based motion control},
abstract = {Aimed at the negative effect of dynamics characteristics of leg hydraulic drive system (LHDS) on the accuracy of motion control of the hydraulic drive legged robot, a dynamics compensation control method is proposed. First, according to the mechanical structure of LHDS, the kinematics and statics models of LHDS are analyzed and obtained respectively. Based on the principle of force-based impedance control of LHDS, an impedance based motion control simulation model of LHDS is built and analyzed. The simulation results show that the dynamics characteristics have a great influence on the accuracy of impedance based motion control. Then, a dynamics compensation method considering gravity and inertia force is proposed to solve this problem. Finally, the effect of the dynamics compensation method is verified on the robot single leg test platform. The experimental results show that the compensation method reduces the negative effect of the dynamics characteristics of LHDS on impedance based motion control accuracy, and the position tracking accuracy of the robot’s foot end can be improved by more than 65%. The theory proposed in this paper provides a theoretical basis for the motion control of the whole robot prototype.}
}
@article{DERNER2021103676,
title = {Change detection using weighted features for image-based localization},
journal = {Robotics and Autonomous Systems},
volume = {135},
pages = {103676},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103676},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305169},
author = {Erik Derner and Clara Gomez and Alejandra C. Hernandez and Ramon Barber and Robert Babuška},
keywords = {Mobile robotics, Image-based localization, Change detection, Long-term autonomy},
abstract = {Autonomous mobile robots are becoming increasingly important in many industrial and domestic environments. Dealing with unforeseen situations is a difficult problem that must be tackled to achieve long-term robot autonomy. In vision-based localization and navigation methods, one of the major issues is the scene dynamics. The autonomous operation of the robot may become unreliable if the changes occurring in dynamic environments are not detected and managed. Moving chairs, opening and closing doors or windows, replacing objects and other changes make many conventional methods fail. To deal with these challenges, we present a novel method for change detection based on weighted local visual features. The core idea of the algorithm is to distinguish the valuable information in stable regions of the scene from the potentially misleading information in the regions that are changing. We evaluate the change detection algorithm in a visual localization framework based on feature matching by performing a series of long-term localization experiments in various real-world environments. The results show that the change detection method yields an improvement in the localization accuracy, compared to the baseline method without change detection. In addition, an experimental evaluation on a public long-term localization data set with more than 10000 images reveals that the proposed method outperforms two alternative localization methods on images recorded several months after the initial mapping.}
}
@article{2023104437,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {164},
pages = {104437},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(23)00076-3},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000763}
}
@article{PUTZ2021103688,
title = {The Mesh Tools Package – Introducing Annotated 3D Triangle Maps in ROS},
journal = {Robotics and Autonomous Systems},
volume = {138},
pages = {103688},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103688},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305285},
author = {Sebastian Pütz and Thomas Wiemann and Joachim Hertzberg},
keywords = {ROS, Triangle meshes, RViz, 3D mesh navigation},
abstract = {Triangle mesh maps for robotic applications are becoming increasingly popular, but are not yet effectively supported in the Robot Operating System (ROS). We introduce the Mesh Tools package consisting of message definitions, RViz plugins and tools, as well as a persistence layer. These tools make annotated triangle maps available in ROS and allow to publish, edit and inspect such maps within the existing ROS software stack. The persistence layer efficiently loads and stores large mesh maps. The proposed plugins and tools enable the visualization and validation of the complete layered map and associated properties to allow fluid interaction. We demonstrate the seamless integration of our tools in two application areas as a proof-of-concept: Labeling of triangle clusters for semantic mapping and robot navigation on triangle meshes in rough terrain outdoor environments by integrating our tools into an existing navigation stack.}
}
@article{BRUCKSCHEN2020103664,
title = {Predicting human navigation goals based on Bayesian inference and activity regions},
journal = {Robotics and Autonomous Systems},
volume = {134},
pages = {103664},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103664},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305042},
author = {Lilli Bruckschen and Kira Bungert and Nils Dengler and Maren Bennewitz},
keywords = {Anticipating human behavior, Robot path planning, Human-centered systems},
abstract = {Anticipation of human movements is of great importance for service robots, as it is necessary to avoid interferences and predict areas where human–robot collaboration may be needed. In indoor scenarios, human movements often depend on objects with which they interacted before. For example, if a human interacts with a cup the probability that a table or coffee machine might be the next navigation goal is high. Typically, objects are grouped together in regions depending on the related activities so that environments consist of a set of activity regions. For example, a workspace region may contain a PC, a chair, and a table with many smaller objects on top of it. In this article, we present an approach to predict the navigation goal of a moving human in indoor environments. We hereby combine prior knowledge about typical human transitions between activity regions with robot observations about the human’s current pose and the last object interaction to predict the navigation goal using Bayesian inference. In the experimental evaluation in several simulated environments we demonstrate that our approach leads to a significantly more accurate prediction of the navigation goal in comparison to previous work. Furthermore, we show in a real-world experiment how such human motion anticipation can be used to realize foresighted navigation with an assistance robot, i.e. how predicted human movements can be used to increase the time efficiency of the robot’s navigation policy by early anticipating the user’s navigation goal and moving towards it.}
}
@article{WU2021103797,
title = {Peg-in-hole assembly in live-line maintenance based on generative mapping and searching network},
journal = {Robotics and Autonomous Systems},
volume = {143},
pages = {103797},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103797},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000828},
author = {Wei Wu and Hui Zhou and Yu Guo and Yifei Wu and Jian Guo},
keywords = {Peg-in-hole assembly, Live-line maintenance robot, Fusion signal, Semi-supervised learning network},
abstract = {Replacement of lightning arrester is one of the common tasks in live-line maintenance, and peg-in-hole assembly is a very difficult operation for a robot, because there are visual inaccuracy and force model uncertainty in the process of assembly. This paper presents a new implementation approach fusing signals of vision detection and fuzzy force to realize the high efficiency peg-in-hole assembly by a manipulator autonomously. YOLOv3 is applied as the visual detection network for rough alignment. In the phase of precise hole-searching, we establish a two-dimensional hole-searching model by fusing signal of vision detection and fuzzy force as the condition of state transitions, and propose a new semi-supervised learning network to optimize the hole-searching routine. The performance of the approach is verified by experiments in the simulation environment and the laboratory environment.}
}
@article{KRINKIN2021103809,
title = {Correlation filter of 2D laser scans for indoor environment},
journal = {Robotics and Autonomous Systems},
volume = {142},
pages = {103809},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103809},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000944},
author = {Kirill Krinkin and Anton Filatov},
keywords = {SLAM, Laser scan, Filtering, Correlation, Histogram},
abstract = {Modern laser SLAM (simultaneous localization and mapping) and structure from motion algorithms face the problem of processing redundant data. Even if a sensor does not move, it still continues to capture scans that should be processed. This paper presents the novel filter that allows dropping 2D scans that bring no new information to the system. Experiments on MIT and TUM datasets show that it is possible to drop more than half of the scans. Moreover the paper describes the formulas that enable filter adaptation to a particular robot with known speed and characteristics of lidar. In addition, the indoor corridor detector is introduced that also can be applied to any specific shape of a corridor and sensor.}
}
@article{RIBEIRO2021103757,
title = {Real-time deep learning approach to visual servo control and grasp detection for autonomous robotic manipulation},
journal = {Robotics and Autonomous Systems},
volume = {139},
pages = {103757},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103757},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000427},
author = {Eduardo Godinho Ribeiro and Raul {de Queiroz Mendes} and Valdir Grassi},
keywords = {Robotic grasping, Visual servoing, Real-time, Deep learning, 7DoF robot},
abstract = {Robots still cannot perform everyday manipulation tasks, such as grasping, with the same dexterity as humans do. In order to explore the potential of supervised deep learning for robotic grasping in unstructured and dynamic environments, this work addresses the visual perception phase involved in the task. This phase involves the processing of visual data to obtain the location of the object to be grasped, its pose and the points at which the robot’s grippers must make contact to ensure a stable grasp. For this, the Cornell Grasping Dataset (CGD) is used to train a Convolutional Neural Network (CNN) that is able to consider these three stages simultaneously. In other words, having an image of the robot’s workspace, containing a certain object, the network predicts a grasp rectangle that symbolizes the position, orientation and opening of the robot’s parallel grippers the instant before its closing. In addition to this network, which runs in real-time, another network is designed, so that it is possible to deal with situations in which the object moves in the environment. Therefore, the second convolutional network is trained to perform a visual servo control, ensuring that the object remains in the robot’s field of view. This network predicts the proportional values of the linear and angular velocities that the camera must have to ensure the object is in the image processed by the grasp network. The dataset used for training was automatically generated by a Kinova Gen3 robotic manipulator with seven Degrees of Freedom (DoF). The robot is also used to evaluate the applicability in real-time and obtain practical results from the designed algorithms. Moreover, the offline results obtained through test sets are also analyzed and discussed regarding their efficiency and processing speed. The developed controller is able to achieve a millimeter accuracy in the final position considering a target object seen for the first time. To the best of our knowledge, we have not found in the literature other works that achieve such precision with a controller learned from scratch. Thus, this work presents a new system for autonomous robotic manipulation, with the ability to generalize to different objects and with high processing speed, which allows its application in real robotic systems.}
}
@article{LIU2021103785,
title = {Review of snake robots in constrained environments},
journal = {Robotics and Autonomous Systems},
volume = {141},
pages = {103785},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103785},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000701},
author = {Jindong Liu and Yuchuang Tong and Jinguo Liu},
keywords = {Snake robots, Joint structure, Control algorithms, Constrained environment},
abstract = {Snake robots have advantages of terrain adaptability over wheeled mobile robots and traditional articulated robot arms because of their limbless thin body structure and high flexibility. They have extensive applications in tasks such as rescue, disaster recovery, inspection and minimally invasive surgery. Current research on snake robots is mainly focused on snake-like locomotion and the embodiment of these motion gaits for different applications. Modular structure and real-time control algorithms are two key aspects for snake robots operating in constrained environments. This review will attempt to address both. First, a review on the snake motion and the body structure is provided, which outlines the biological foundation of all snake robots. This is followed by the mechanical structure of snake robots, especially the structure of elemental snake modules. Finally, control algorithms for variant terrain contours and obstacle avoidance are discussed. The review also outlines emerging application areas and potential future directions of snake robots.}
}
@article{HUANG2021103692,
title = {Robot gaining accurate pouring skills through self-supervised learning and generalization},
journal = {Robotics and Autonomous Systems},
volume = {136},
pages = {103692},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103692},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305327},
author = {Yongqiang Huang and Juan Wilches and Yu Sun},
keywords = {Sensorimotor learning, Sensor-based control, Generalization},
abstract = {Pouring is one of the most commonly executed tasks in humans’ daily lives, whose accuracy is affected by multiple factors, including the type of material to be poured and the geometry of the source and receiving containers. In this work, we propose a self-supervised learning approach that learns the pouring dynamics, pouring motion, and outcomes from unsupervised demonstrations for accurate pouring. The learned pouring model is then generalized by self-supervised practicing to different conditions such as using unaccustomed pouring cups. We have evaluated the proposed approach first with one container from the training set and four new but similar containers. The proposed approach achieved better pouring accuracy than a regular human with a similar pouring speed for all five cups. Both the accuracy and pouring speed outperform state-of-the-art works. We have also evaluated the proposed self-supervised generalization approach using unaccustomed containers that are far different from the ones in the training set. The self-supervised generalization reduces the pouring error of the unaccustomed containers to the desired accuracy level.}
}
@article{ARIF2021103764,
title = {Adaptive visual servo control law for finite-time tracking to land quadrotor on moving platform using virtual reticle algorithm},
journal = {Robotics and Autonomous Systems},
volume = {141},
pages = {103764},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103764},
url = {https://www.sciencedirect.com/science/article/pii/S092188902100049X},
author = {Adeel Arif and Hesheng Wang and Zhe Liu and Herman Castañeda and Yong Wang},
keywords = {Virtual Reticle Plane Algorithm, Visual servoing, Finite-time control, Adaptive feedback linearization},
abstract = {This paper proposed an image-based visual servoing (IBVS) control law for a quadrotor that is equipped with a single monocular camera attached to its bottom. For control purposes virtual reticle plane (VRP) algorithm is used to track the relative 3D position of the quadrotor to the tilting and moving target landing platform within the range of the camera’s field of view (FOV). In this article, the landing platform’s tilting motion is considered sinusoidal type oscillatory motion for the overhead camera. For control purposes, an adaptive finite-time control (AFTC) is proposed, based on the finite-time control (FTC) and adaptive approximation of uncertainties. A constructive combination of FTC and adaptive approximation inherits benefits of both to overcome each other’s limitations. The task of the controller is to regulate the position error to zero in time calculated by VRP. The experimental results confirmed the effectiveness of the VRP algorithm to track the desired parameters of the moving target. Finally, simulations are performed to illustrate the effectiveness and improved performance of the proposed AFTC in response time, robustness, and tracking accuracy.}
}
@article{DACOSTABARROS2021103729,
title = {Robotic Mobile Fulfillment Systems: A survey on recent developments and research opportunities},
journal = {Robotics and Autonomous Systems},
volume = {137},
pages = {103729},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103729},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000142},
author = {Ítalo Renan {da Costa Barros} and Tiago Pereira Nascimento},
keywords = {Autonomous mobile robots, Robotic Mobile Fulfillment System, AMR, RMFS, Warehouses},
abstract = {With the advancement of the autonomous mobile robots applied to Warehouses and the creation of the Robotic Mobile Fulfillment System after the market implementation of the Kiva Robots, it is necessary to carry out a deeper approach of the researches carried out to this date. The objective of this survey is to provide a unified and accessible presentation of the basic concepts of a warehouse system, such as its types, layouts, systems, and methodologies already applied to improve the activities, thus going to the latest research and methodologies focused on the development of new architectures and algorithms in Robotic Mobile Fulfillment Systems (RMFS). The main contribution of this work is an attempt to present a comprehensive review of recent breakthroughs in the goods-to-person RMFS field, providing links to the most interesting and successful works from the state-of-the-art, but also to provide a presentation and summary of how a Warehouse systems works, in a way that allows future researchers to understand his taxonomies and principles of operation.}
}
@article{BURKS2021103753,
title = {Collaborative human-autonomy semantic sensing through structured POMDP planning},
journal = {Robotics and Autonomous Systems},
volume = {140},
pages = {103753},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103753},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000385},
author = {Luke Burks and Nisar Ahmed and Ian Loefgren and Luke Barbier and Jeremy Muesing and Jamison McGinley and Sousheel Vunnam},
keywords = {Human–robot interaction, Data fusion, Partially observable Markov decision processes, Planning, Bayesian methods, Target search, Target localization, Mobile robots, Autonomy},
abstract = {Autonomous unmanned systems and robots must be able to actively leverage all available information sources — including imprecise but readily available semantic observations provided by human collaborators. This work develops and validates a novel active collaborative human–machine sensing solution for robotic information gathering and optimal decision making problems, with an example implementation of a dynamic target search scenario. Our approach uses continuous partially observable Markov decision process (CPOMDP) planning to generate vehicle trajectories that optimally exploit imperfect detection data from onboard sensors, as well as semantic natural language observations that can be specifically requested from human sensors. The key innovations are a method for the inclusion of a human querying/sensing model in a CPOMDP based autonomous decision making process, as well as a scalable hierarchical Gaussian mixture model formulation for efficiently solving CPOMDPs with semantic observations in continuous dynamic state spaces. Unlike previous state-of-the-art approaches this allows planning in large, complex, highly segmented environments. Our solution is demonstrated and validated with a real human–robot team engaged in dynamic indoor target search and capture scenarios on a custom testbed.}
}
@article{XIA2021103728,
title = {Multi-UAV trajectory planning using gradient-based sequence minimal optimization},
journal = {Robotics and Autonomous Systems},
volume = {137},
pages = {103728},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103728},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000130},
author = {Qiaoyang Xia and Shuang Liu and Mingyang Guo and Hui Wang and Qigao Zhou and Xiancheng Zhang},
keywords = {Multi-UAV trajectory planning, Time segmentation, Sequential minimal optimization, Decoupled mutual collision},
abstract = {Multi-UAV system is widely used in surveillance, search and rescue, and industrial inspection. Multi-UAV trajectory planning is crucial for the multi-UAV system, but multi-UAV trajectory planning often needs to consider many constraints, such as trajectory smoothness, obstacle collisions, mutual collisions, dynamic limits, time-consuming, and trajectory length. It is a challenge to balance these constraints while considering computational performance. This paper proposes a novel multi-UAV trajectory planning method to solve the challenge. This method uses time segmentation instead of traditional waypoint segmentation to establish a trajectory optimization model based on the unified time interval, which simplifies the calculation of cost functions. At the same time, virtual segments are introduced to adapt to the trajectory length of different UAVs to reduce the total arrival time. Nonlinear constraints are cast into cost functions and a gradient-based sequential minimal optimization (GB-SMO) algorithm is proposed to minimize the cost function, which decouples the constraint of the mutual collisions in each iteration to save the planning time. Experiments are performed on a multi-UAV system to prove the effectiveness of the proposed method. Results show that this method has good performance in obstacle-rich environments and is efficient for a large number of UAVs.}
}
@article{WANG2021103675,
title = {Dynamics evaluation of 2UPU/SP parallel mechanism for a 5-DOF hybrid robot considering gravity},
journal = {Robotics and Autonomous Systems},
volume = {135},
pages = {103675},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103675},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305157},
author = {Xiaojian Wang and Jun Wu and Yutian Wang},
keywords = {Parallel mechanism, Dynamics evaluation, Gravity, Placement direction},
abstract = {Dynamics performance is very important for a manipulator used for high-speed machining. In this paper, the dynamic performance evaluation method of the 2UPU/SP parallel mechanism in a hybrid robot for aerospace composite machining is studied. The dynamic model is obtained by the virtual work principle, and a dynamic performance index considering gravity is proposed. Based on the given performance index, the effect of placement direction on dynamic performance of 2UPU/SP mechanism is studied, and the comparison between the dynamic performance of 2UPU/SP and the traditional Tricept mechanism is carried out. The results show that the 2UPU/SP mechanism has better dynamic performance in the vertical placement than the horizontal placement, and 2UPU/SP mechanism has better dynamic performance than Tricept mechanism.}
}
@article{PARK2021103703,
title = {Jumping over obstacles with MIT Cheetah 2},
journal = {Robotics and Autonomous Systems},
volume = {136},
pages = {103703},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103703},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305431},
author = {Hae-Won Park and Patrick M. Wensing and Sangbae Kim},
keywords = {Legged locomotion, Quadruped robots, Sensor-based planning},
abstract = {This paper presents a planning framework for jumping over obstacles with quadruped robots. The framework accomplishes planning via a structured predictive control strategy that combines the use of heterogeneous simplified models over different prediction time scales. A receding multi-horizon predictive controller coordinates the approach before the jump using a kinematic point-mass model. Consideration of the optimal value function over different planning horizons enables the system to select an appropriate number of steps to take before jumping. The jumping motion is then tailored to the sensed obstacle by solving a nonlinear trajectory optimization problem. The solution of this problem online is enabled by exploiting the analyticity of the flow map for a planar bounding template model under polynomial inputs. By planning with this combination of models, MIT Cheetah 2 is shown to autonomously jump over obstacles up to 40 cm in height during high-speed bounding. Untethered results showcase the ability of the method to automatically adapt to obstacles of different heights and placements in a single trial.}
}
@article{BROUWER2021103691,
title = {Informative path planner with exploration–exploitation trade-off for radiological surveys in non-convex scenarios},
journal = {Robotics and Autonomous Systems},
volume = {136},
pages = {103691},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103691},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305315},
author = {Yoeri Brouwer and Alberto Vale and Rodrigo Ventura},
keywords = {Informative path planning, Area coverage, Radiological monitoring, Autonomous vehicles, Gaussian Processes},
abstract = {The risk toward human lives in situations involving chemical, biological, radiological, and nuclear (CBRN) threats can be mitigated or even neutralized by deploying carrying a suite of suitable sensors. Furthermore, mobile robots open up the possibility for automated radiological field surveys and monitoring operations, which have important applications in scenarios with CBRN threats. A path planner is one of the essential tools required for these robots to perform their tasks autonomously. Moreover, sophisticated path planners can greatly increase the efficiency of monitoring tasks by maximizing the information gathered in the minimum amount of time. This work proposes an informative path planner as an instrument to efficiently estimate maps of scalar quantities (e.g., radiation intensity, chemical concentration), motivated by applications in radiological inspection. The proposed path planner models the path with B-splines, enabling planning in continuous space. A Gaussian Process with a squared exponential kernel is used to model the underlying field. A modified form of mutual information, estimated from the Gaussian Process, is maximized to determine the most informative path, additionally rewarding observations made in regions where the field magnitude is large (e.g., near a radioactive source). A maximum likelihood estimator for source parameters is used to demonstrate that the proposed solution increases the accuracy of the estimated source positions. Simulation results show that the informative path planner adapts to non-convex environments and increases the number of observations made close to radioactive sources while avoiding obstacles.}
}
@article{BROUGHTON2021103687,
title = {Learning to see through the haze: Multi-sensor learning-fusion System for Vulnerable Traffic Participant Detection in Fog},
journal = {Robotics and Autonomous Systems},
volume = {136},
pages = {103687},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103687},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305273},
author = {George Broughton and Filip Majer and Tomáš Rouček and Yassine Ruichek and Zhi Yan and Tomáš Krajník},
keywords = {Mobile robotics, 3D radar, 2D lidar, Machine learning, Pointnet, SVM, Learning fusion, Sensor fusion, On-line learning},
abstract = {We present an experimental investigation of a multi-sensor fusion-learning system for detecting pedestrians in foggy weather conditions. The method combines two pipelines for people detection running on two different sensors commonly found on moving vehicles: lidar and radar. The two pipelines are not only combined by sensor fusion, but information from one pipeline is used to train the other. We build upon our previous work, where we showed that a lidar pipeline can be used to train a Support Vector Machine (SVM)-based pipeline to interpret radar data, which is useful when conditions then become unfavourable to the original lidar pipeline. In this paper, we test the method on a wider range of conditions, such as from a moving vehicle, and with multiple people present. Additionally, we also compare how the traditional SVM performs interpreting the radar data versus a modern deep neural network on these experiments. Our experiments indicate that either of the approaches results in progressive improvement in the performance during normal operation. Further, our experiments indicate that in the event of the loss of information from a sensor, pedestrian detection and position estimation is still effective.}
}
@article{TETI2021103780,
title = {A controlled investigation of behaviorally-cloned deep neural network behaviors in an autonomous steering task},
journal = {Robotics and Autonomous Systems},
volume = {142},
pages = {103780},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103780},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000658},
author = {Michael Teti and William Edward Hahn and Shawn Martin and Christopher Teti and Elan Barenholtz},
keywords = {End-to-end control systems, Deep neural networks, Autonomous vehicles, Imitation learning, Behavioral cloning},
abstract = {Imitation learning (IL) is a popular method used to train machine learning models that are capable of acting on their environment based on expert examples. Two types of IL models are inverse reinforcement learning (IRL) and behavioral cloning (BC). Models trained under IRL traditionally perform better than those trained under BC due to compounding covariate shift associated with the latter, which typically requires algorithms such as DAGGer to help compensate for this. More recently, however, deep learning architectures with increased generalization performance have been developed, which may help to alleviate the problem of compounding covariate shift and allow researchers to take advantage of the simplicity of BC. Despite these developments, recent studies on BC in sub-scale autonomous robots employ relatively primitive convolutional networks without such tools as batch normalization and skip connections, and it is difficult to judge their networks’ performance relative to others due to drastically different training and testing conditions. Here, we examine how an array of artificial neural networks, chosen to reflect more recent architectural choices available, behave in a highly controlled IL task – navigating around a small, indoor racetrack – upon being embedded in a sub-scale RC vehicle as an end-to-end steering system. For our main findings, we report the lap completion rate and path smoothness of each network under the exact same conditions as it controls the vehicle on the track. To supplement these findings, we also measure each network’s bias toward the distribution of the training actions and develop a method to highlight regions of a given input image that are deemed ‘important’ to a given network. We observe that most of the more recent neural networks perform reasonably well during testing, as opposed to the more primitive networks which did not perform as well. For these reasons and others, we identify VGG-16 and AlexNet – out of the networks tested here – as attractive candidate architectures for such tasks.}
}
@article{MCCOOL2023104380,
title = {Special Issue on the 10th European Conference on Mobile Robots (ECMR 2021)},
journal = {Robotics and Autonomous Systems},
volume = {163},
pages = {104380},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104380},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000192},
author = {Chris McCool and Emanuele Menegatti and Sven Behnke}
}
@article{TOFIGH2021103756,
title = {Fractional sliding mode control for an autonomous two-wheeled vehicle equipped with an innovative gyroscopic actuator},
journal = {Robotics and Autonomous Systems},
volume = {140},
pages = {103756},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103756},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000415},
author = {M.A. Tofigh and M.J. Mahjoob and M.R. Hanachi and M. Ayati},
keywords = {Gyrostabilizer, Fractional sliding mode, Robust control, Two-wheeled autonomous vehicle, Gyroscopic actuator},
abstract = {Balancing two-wheeled autonomous vehicles at low forward speeds is one of the primary challenges in the development of such vehicles. Gyrostabilizers can be used as actuators to make the balance; however, conventional gyros are not typically able to maintain constant moments and directions to stabilize against constant ‘heel’. In this paper, we present an innovative gyrostabilizer including a twin-flywheel arrangement that can provide any desired gyroscopic roll moment. The dynamical model of a bicycle together with the gyrostabilizer is derived using Newton–Euler method. The actuator dynamics is included when designing the control system. A robust non-integer sliding mode controller is then developed to guarantee perfect trajectory tracking in the presence of roll disturbance. Extensive comparative simulations (based on the experimentally measured parameters of a typical bike) are conducted to evaluate the method and to show the impact of introducing the novel actuator. Results demonstrate that the proposed system offers superior performance while the control effort also remains within the capacity of normal actuators.}
}
@article{PASKARBEIT2021103715,
title = {Ourobot—A sensorized closed-kinematic-chain robot for shape-adaptive rolling in rough terrain},
journal = {Robotics and Autonomous Systems},
volume = {140},
pages = {103715},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103715},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305558},
author = {Jan Paskarbeit and Simon Beyer and Matthäus Engel and Adrian Gucze and Johann Schröder and Axel Schneider},
keywords = {Closed-kinematic-chain, Mobile robotics, Bioinspired, Tactile sensors, Terrain adaptation, Obstacle evasion, High DoF, Online optimization},
abstract = {Inspired by the abilities of amoeba to alter their shape, a continuous-track robot called Ourobot has been developed that is able to adapt its shape to the environment. Using tactile sensors at the outer hull of the robot, the outline of the terrain and collisions with obstacles can be detected. Thus, the robot is able to locomote in uneven terrain and climb steep slopes. Since the shape adaption is based on run-time optimization, the quality function can be easily expanded to consider additional side conditions. The functionality of the proposed approach is demonstrated both in simulation and hardware.}
}
@article{RIZZO2021103702,
title = {An alternative approach for robot localization inside pipes using RF spatial fadings},
journal = {Robotics and Autonomous Systems},
volume = {136},
pages = {103702},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103702},
url = {https://www.sciencedirect.com/science/article/pii/S092188902030542X},
author = {Carlos Rizzo and Teresa Seco and Jesús Espelosín and Francisco Lera and José Luis Villarroel},
keywords = {Robotics, Pipes, Tunnels, Propagation, RF fadings, Localization, Navigation, Inspection, Maintenance},
abstract = {Accurate robot localization represents a challenge inside pipes due to the particular conditions that characterize this type of environment. Outdoor techniques (GPS in particular) do not work at all inside metal pipes, while traditional indoor localization methods based on camera or laser sensors do not perform well mainly due to a lack of external illumination and distinctive features along pipes. Moreover, humidity and slippery surfaces make wheel odometry unreliable. In this paper, we estimate the localization of a robot along a pipe with an alternative Radio Frequency (RF) approach. We first analyze wireless propagation in metallic pipes and propose a series of setups that allow us to obtain periodic RF spatial fadings (a sort of standing wave periodic pattern), together with the influence of the antenna position and orientation over these fadings. Subsequently, we propose a discrete RF odometry-like method, by means of counting the fadings while traversing them. The transversal fading analysis (number of antennas and cross-section position) makes it possible to increase the resolution of this method. Lastly, the model of the signal is used in a continuous approach serving as an RF map. The proposed localization methods outperform our previous contributions in terms of resolution, accuracy, reliability and robustness. Experimental results demonstrate the effectiveness of the RF-based strategy without the need for a previously known map of the scenario or any substantial modification of the existing infrastructure.}
}