@article{ZHANG2025110985,
title = {Dynamic convolutional time series forecasting based on adaptive temporal bilateral filtering},
journal = {Pattern Recognition},
volume = {158},
pages = {110985},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110985},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007362},
author = {Dandan Zhang and Zhiqiang Zhang and Nanguang Chen and Yun Wang},
keywords = {Nonlinear feature, Adaptive temporal bilateral filtering, Gated deformable convolution, Time series forecasting},
abstract = {Time series data typically contain complex dynamic patterns, which not only include linear trends and seasonal variations but also significant nonlinear changes and complex dependencies. Currently, feature extraction methods for time series data primarily employ mixed-mode extraction within the temporal domain, neglecting the effective extraction and analysis of nonlinear characteristics between different observation points and the interdependencies between variables. To address these issues, we designed an adaptive temporal bilateral filtering module that effectively preserves and highlights the nonlinear features and patterns in time series while filtering out noise and redundant information. We also designed a nonlinear feature adaptive extraction module that integrates a gating mechanism and deformable convolutions. This design allows the model to adaptively adjust the shape of the convolutional kernels according to different time steps and conditions. This enables accurate capture and extraction of nonlinear features between various time observations and dependencies between different variables. Additionally, we use stacked convolutional layers to extract local contextual features, addressing fluctuations in local features caused by changes in data distribution in real-world scenarios. In summary, we propose DCNet, a dynamic convolutional network based on an adaptive temporal bilateral filter, and evaluated its performance on twelve real-world datasets. The results indicate that DCNet consistently achieves state-of-the-art performance in both short-term and long-term forecasting tasks, with favorable runtime efficiency.}
}
@article{YAO2025111070,
title = {SNN using color-opponent and attention mechanisms for object recognition},
journal = {Pattern Recognition},
volume = {158},
pages = {111070},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111070},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008215},
author = {Zhiwei Yao and Shaobing Gao and Wenjuan Li},
keywords = {Color-opponency, Attention mechanism, STDP, SNN, Unsupervised learning},
abstract = {The current spiking neural network (SNN) relies on spike-timing-dependent plasticity (STDP) primarily for shape learning in object recognition tasks, overlooking the equally critical aspect of color information. To address this gap, our study introduces an unsupervised variant of STDP that incorporates principles from color-opponency mechanisms (COM) and classical receptive fields (CRF) found in the biological visual system, facilitating the integration of color information during parameter updates within the SNN architecture. Our approach initially preprocesses images into two distinct feature maps: one for shape and another for color. Then, signals derived from COM and intensity concurrently drive the STDP process, thereby updating parameters associated with both color and shape feature maps. Furthermore, we propose a channel-wise attention mechanism to enhance differentiation among objects sharing similar shapes or colors. Specifically, this mechanism utilizes convolution to generate an output spike-wave, identifying a winner based on earliest spike timing and maximal potential. The winning kernel computes attention, which is then applied via convolution to each input image feature map, generating post-feature maps. A STDP-like normalization rule compares firing times between pre- and post-feature maps, dynamically adjusting channel weights to optimize object recognition during the training phase. We assessed the proposed algorithm using SNN with both single-layer and multi-layer architectures across three datasets. Experimental findings highlight its efficacy and superiority in complex object recognition tasks compared to state-of-the-art (SOTA) algorithms. Notably, our approach achieved a significant 20% performance improvement over the SOTA on the Caltech-101 dataset. Moreover, the algorithm is well-suited for hardware implementation and energy efficiency, leveraging a winner-selection mechanism based on the earliest spike time.}
}
@article{MEI2025111081,
title = {Self-supervised learning from images: No negative pairs, no cluster-balancing},
journal = {Pattern Recognition},
volume = {159},
pages = {111081},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111081},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400832X},
author = {Jian-Ping Mei and Shixiang Wang and Miaoqi Yu},
keywords = {Self-supervised learning, Non-contrastive, Image classification, Self-derived targets, Unsupervised representation learning},
abstract = {Learning with self-derived targets provides a non-contrastive method for unsupervised image representation learning, where the variety in targets is crucial. Recent work has achieved good performance by learning with targets obtained via cluster-balancing. However, the equal-cluster-size constraint becomes too restrictive for handling data with imbalanced categories or coming in small batches. In this paper, we propose a new clustering-based approach for non-contrastive image representation learning with no need for a particular architecture design or extra memory bank and no explicit constraints on cluster size. A key formulation is to learn embedding consistency and variable decorrelation in the cluster space by tweaking the batch-wise cross-correlation matrix towards an identity one. With this identitization loss incorporated, predicted cluster assignments of two randomly augmented views of the same image serve as targets for each other. We carried out comprehensive experimental studies of linear classification with learned representations of benchmark image datasets. Our results show that the proposed approach significantly outperforms state-of-the-art approaches and is more robust to class imbalance than those with cluster balancing.}
}
@article{LI2025111040,
title = {Self-supervised fusion network for RGB-D interest point detection and description},
journal = {Pattern Recognition},
volume = {158},
pages = {111040},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111040},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400791X},
author = {Ningning Li and Xiaomin Wang and Zhou Zheng and Zhendong Sun},
keywords = {Interest point extraction in challenging indoor environment, RGB-D cross-modal fusion network (RDFNet), Self-supervised, Two-stage cross-modal reweighted feature fusion},
abstract = {Interest point detection and description are highly challenging in indoor environments with repeated and sparse textures and heavy illumination changes (noted as challenging indoor environments, CIE). In such environments, it is a severe problem of mismatched or misaligned feature points, often resulting in unsatisfactory accuracy in indoor applications, such as SLAM. To deal with the issue, we propose a self-supervised RGB-D cross-modal fusion network (RDFNet) for feature extraction. In the RDFNet, a dual-stream structure is introduced to build a pseudo-Siamese network for simultaneously processing color and depth images, while a new two-stage cross-modal reweighted fusion method (TCRF) is developed to fuse RGB and depth features. The TCRF achieves effective fusion in two steps: (1) introducing the reweighting idea and compositely enhancing RGB features by the depth features at both low-level and high-level stages; (2) concatenating the enhanced RGB and depth features together. In addition, we add a uniform distribution loss function to encourage the uniform extraction of feature points. To verify the proposed model performance, a new test dataset of specific indoor scenes is created to evaluate it and compare it to other state-of-the-art methods. Experimental results demonstrate its excellent performance in challenging indoor scenarios.}
}
@article{ZHENG2025111029,
title = {Distilling efficient Vision Transformers from CNNs for semantic segmentation},
journal = {Pattern Recognition},
volume = {158},
pages = {111029},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111029},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007805},
author = {Xu Zheng and Yunhao Luo and Pengyuan Zhou and Lin Wang},
keywords = {Knowledge distillation, Vision transformer, Convolutional neural networks, Semantic segmentation},
abstract = {In this paper, we tackle the problem of how to transfer knowledge from a pre-trained, yet well-performing CNN-based model to train a compact Vision Transformer (ViT)-based model while maintaining the CNN learning capacity? Due to the completely different characteristics of ViT and CNN and the long-existing capacity gap between teacher and student models in Knowledge Distillation (KD), directly transferring the cross-model knowledge is non-trivial. To this end, we subtly leverage the visual and linguistic-compatible feature character of ViT (i.e., student), and its capacity gap with the CNN (i.e., teacher) and propose a novel CNN-to-ViT KD framework, dubbed C2VKD. Importantly, as the teacher’s features are heterogeneous to those of the student, we first propose a novel visual-linguistic feature distillation (VLFD) module that explores efficient KD among the aligned visual and linguistic-compatible representations. Moreover, due to the large capacity gap between the teacher and student and the inevitable prediction errors of the teacher, we then propose a pixel-wise decoupled distillation (PDD) module to supervise the student under the combination of labels and teacher’s predictions from the decoupled target and non-target classes. Experiments on three semantic segmentation benchmark datasets consistently show that the increment of mIoU of our method is over 200% of the SoTA KD methods. 11Project Page: https://vlislab22.github.io/C2VKD/.}
}
@article{CHEN2025111013,
title = {Scene Chinese Recognition with Local and Global Attention},
journal = {Pattern Recognition},
volume = {158},
pages = {111013},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111013},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007647},
author = {Zhao Chen and Yaohua Yi and Chaohua Gan and Ziwei Tang and Dezhu Kong},
keywords = {Scene Chinese recognition, Adaptive Position Encoding, Local Attention Module, Global Attention Module},
abstract = {Recently, scene Chinese recognition has attracted increasing attention. While mainstream scene text recognition methods exhibit outstanding performance in English recognition, they are considerably limited in Chinese recognition, due to inter-class similarity, intra-class variability, and complex combination of components in scene Chinese text. In this paper, we design Adaptive Position Encoding(APE) to enhance the model’s ability to perceive spatial information. Based on APE, we have innovatively designed Local Attention Module (LAM) and Global Attention Module (GAM). Specifically, LAM captures local features to identify common characteristics among characters of the same category, addressing the issue of intra-class variability. Meanwhile, LAM captures global features to identify the subordination relationships of Chinese character components. By integrating LAM and GAM, combining both local and global features, it is possible to find differences in the details among features that are fundamentally similar, thus solving the problem of inter-class similarity. Further, we contrive the transformer encoder–decoder structure to identify the vast variety of Chinese characters. Based on the Local/Global Attention Module and transformer encoder–decoder framework, we devise the novel sequence-to-sequence Local and Global Attention Network(LGANet), where both the backbone and the encoder/decoder are composed of attention mechanisms. Subsequent experiments on the Chinese scene dataset show that the recognition accuracy of our proposed LGANet is 77.3% and the normalized editing distance is 88.6%, both of which achieve the SOTA results in Fig. 1.}
}
@article{WANG2025111041,
title = {MMAE: A universal image fusion method via mask attention mechanism},
journal = {Pattern Recognition},
volume = {158},
pages = {111041},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111041},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007921},
author = {Xiangxiang Wang and Lixing Fang and Junli Zhao and Zhenkuan Pan and Hui Li and Yi Li},
keywords = {Deep learning, Image fusion, Computer vision, Universal, Mask attention mechanism},
abstract = {As an important carrier of data, images contain a huge amount of information. The purpose of image fusion is to integrate the information from source images into a single image. Since the source images are from the same scene, there is much redundant information between them. Common fusion methods do not filter this information and the fusion process is often disturbed. This leads to degradation in the quality of the reconstructed fused image. To solve this problem, this paper explores the strategies of information filtering and fusion control, and proposes a universal image fusion method based on the mask attention mechanism. It can be divided into pre-training stage and formal fusion stage. In the pre-training stage, coarse-grained mask maps are generated which are employed to improve the mask autoencoder and the mask attention mechanism. In the image formal fusion stage, with the help of coarse-grained mask maps, the mask autoencoder changes the process of random masking and discards redundant features between source images. Meanwhile the mask attention mechanism focuses on the distinctions between various source images and retains effective complementary information. Qualitative and quantitative extension experiments on different modal datasets validate the applicability of the model in multi-focus image fusion, infrared and visible image fusion, and medical image fusion. Our method achieves excellent performance in all these tasks and performs better than existing fusion methods. Our code is publicly available at https://github.com/xiangxiang-wang/MMAE.}
}
@article{ZHU2025111066,
title = {Diffusion process with structural changes for subspace clustering},
journal = {Pattern Recognition},
volume = {158},
pages = {111066},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111066},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008173},
author = {Yanjiao Zhu and Qilin Li and Wanquan Liu and Chuancun Yin},
keywords = {Subspace clustering, Diffusion process, Affinity learning, Dropout, Structural change},
abstract = {Spectral clustering-based methods have gained significant popularity in subspace clustering due to their ability to capture the underlying data structure effectively. Standard spectral clustering focuses on only pairwise relationships between data points, neglecting interactions among high-order neighboring points. Integrating the diffusion process can address this limitation by leveraging a Markov random walk. However, ensuring that diffusion methods capture sufficient information while maintaining stability against noise remains challenging. In this paper, we propose the Diffusion Process with Structural Changes (DPSC) method, a novel affinity learning framework that enhances the robustness of the diffusion process. Our approach broadens the scope of nearest neighbors and leverages the dropout idea to generate random transition matrices. Furthermore, inspired by the structural changes model, we use two transition matrices to optimize the iteration rule. The resulting affinity matrix undergoes self-supervised learning and is subsequently integrated back into the diffusion process for refinement. Notably, the convergence of the proposed DPSC is theoretically proven. Extensive experiments on benchmark datasets demonstrate that the proposed method outperforms existing subspace clustering methods. The code of our proposed DPSC is available at https://github.com/zhudafa/DPSC.}
}
@article{JIANG2025110997,
title = {Revisiting 3D point cloud analysis with Markov process},
journal = {Pattern Recognition},
volume = {158},
pages = {110997},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110997},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007489},
author = {Chenru Jiang and Wuwei Ma and Kaizhu Huang and Qiufeng Wang and Xi Yang and Weiguang Zhao and Junwei Wu and Xinheng Wang and Jimin Xiao and Zhenxing Niu},
keywords = {Point cloud, Markov model, Set abstraction},
abstract = {3D point cloud analysis has recently garnered significant attention due to its capacity to provide more comprehensive information compared to 2D images. To confront the inherent irregular and unstructured properties of point clouds, recent research efforts have introduced numerous well-designed set abstraction blocks. However, few of them address the issues of information loss and feature mismatch during the sampling process. To address these problems, we have explored the Markov process to revisit point clouds analysis, wherein different-scale point sets are treated as states, and information updating between these point sets is modeled as the probability transition. In the framework of Markov analysis, our encoder can be shown to effectively mitigate information loss in downsampled point sets, while our decoder can accurately recover corresponding features for the upsampled point sets. Furthermore, we introduce a difference-wise attention mechanism to specifically extract discriminative point features, focusing on informative point feature distillation within the states. Extensive experiments demonstrate that our method equipped with Markov process consistently achieves superior performance across a range of tasks including object classification, pose estimation, shape completion, part segmentation, and semantic segmentation. The code is publicly available at https://github.com/ssr0512/Markov-Process-Analysis-on-Point-Cloud.git.}
}
@article{PAN2025111025,
title = {Overcoming learning bias via Prototypical Feature Compensation for source-free domain adaptation},
journal = {Pattern Recognition},
volume = {158},
pages = {111025},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111025},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007763},
author = {Zicheng Pan and Xiaohan Yu and Weichuan Zhang and Yongsheng Gao},
keywords = {Source-free domain adaptation, Learning bias, Feature compensation},
abstract = {The focus of Source-free Unsupervised Domain Adaptation (SFUDA) is to effectively transfer a well-trained model from the source domain to an unlabelled target domain. During the target domain adaptation, the source domain data is no longer accessible. Prevalent methodologies attempt to synchronize the data distributions between the source and target domains, utilizing pseudo-labels to impart categorical information, which has made some progress in improving the model’s performance. However, performance impairments persist due to the introduction of learning bias from the source model and the impact of noisy pseudo-labels generated for the target domain. In this research, we reveal that the central cause for feature misalignment during domain transition is the learning bias, which is generated by the discrepancy of information between source and target domain data. The source domain data may contain distinguishable features that do not appear on the target domain, which causes the pre-trained source model to fail to work during domain adaptation. To overcome the information discrepancy, we propose a Prototypical Feature Compensation (PFC) Network. The network extracts representative feature maps of the source domain. Then use them to minimize the discrepancy information in the target domain feature maps. This mechanism facilitates feature alignment across different domains, allowing the model to generate more accurate categorical data through pseudo-labelling. The experimental results and ablation studies demonstrate exceptional performance on three SFUDA datasets and provide evidence of the proposed PFC method’s ability to adjust the feature distribution of both source and target domain data, ensuring their overlap in the latent space.}
}
@article{ZHANG2025111001,
title = {Efficient large-scale scene representation with a hybrid of high-resolution grid and plane features},
journal = {Pattern Recognition},
volume = {158},
pages = {111001},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111001},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007520},
author = {Yuqi Zhang and Guanying Chen and Shuguang Cui},
keywords = {Neural radiance field, Large-scale scene reconstruction, Multi-view images reconstruction},
abstract = {Existing neural radiance fields (NeRF) methods for large-scale scene modeling require days of training using multiple GPUs, hindering their applications in scenarios with limited computing resources. Radiance field models based on explicit dense or hash grid features have been proposed for fast optimization, but their effectiveness has mainly been demonstrated in the context of object-scale scene representation. In this paper, we point out that the low feature resolution in explicit representation is the bottleneck for large-scale unbounded scene representation. To address this problem, we introduce a new and efficient hybrid feature representation for NeRF that fuses the 3D hash-grids and high-resolution 2D dense plane features. Compared with the dense-grid representation, the resolution of a dense 2D plane can be scaled up more efficiently. Based on this hybrid representation, we propose a fast optimization NeRF variant, called GP-NeRF, that achieves better rendering results while maintaining a compact model size. Extensive experiments on multiple large-scale unbounded scene datasets show that our model can converge in 1.5 h using a single GPU while achieving results comparable to or even better than the existing method that requires about one day’s training with 8 GPUs. Our code can be found at https://zyqz97.github.io/GP_NeRF/.}
}
@article{LIN2025111054,
title = {SLAM2: Simultaneous Localization and Multimode Mapping for indoor dynamic environments},
journal = {Pattern Recognition},
volume = {158},
pages = {111054},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111054},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008057},
author = {Zhihao Lin and Qi Zhang and Zhen Tian and Peizhuo Yu and Ziyang Ye and Hanyang Zhuang and Jianglin Lan},
keywords = {Deep learning, Dynamic environment, Pose estimation, Semantic mapping, SLAM},
abstract = {Traditional visual Simultaneous Localization and Mapping (SLAM) methods based on point features are often limited by strong static assumptions and texture information, resulting in inaccurate camera pose estimation and object localization. To address these challenges, we present SLAM2, a novel semantic RGB-D SLAM system that can obtain accurate estimation of the camera pose and the 6DOF pose of other objects, resulting in complete and clean static 3D model mapping in dynamic environments. Our system makes full use of the point, line, and plane features in space to enhance the camera pose estimation accuracy. It combines the traditional geometric method with a deep learning method to detect both known and unknown dynamic objects in the scene. Moreover, our system is designed with a three-mode mapping method, including dense, semi-dense, and sparse, where the mode can be selected according to the needs of different tasks. This makes our visual SLAM system applicable to diverse application areas. Evaluation in the TUM RGB-D and Bonn RGB-D datasets demonstrates that our SLAM system achieves the most advanced localization accuracy and the cleanest static 3D mapping of the scene in dynamic environments, compared to state-of-the-art methods. Specifically, our system achieves a root mean square error (RMSE) of 0.018 m in the highly dynamic TUM w/half sequence, outperforming ORB-SLAM3 (0.231 m) and DRG-SLAM (0.025 m). In the Bonn dataset, our system demonstrates superior performance in 14 out of 18 sequences, with an average RMSE reduction of 27.3% compared to the next best method.}
}
@article{ZHANG2025111064,
title = {CRCGAN: Toward robust feature extraction in finger vein recognition},
journal = {Pattern Recognition},
volume = {158},
pages = {111064},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111064},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400815X},
author = {Zhongxia Zhang and Zhengchun Zhou and Zhiyi Tian and Shui Yu},
keywords = {Finger vein recognition, Cycle generative adversarial network, Essential features, Noise-resistant},
abstract = {Deep convolutional neural networks (CNNs) have produced remarkable outcomes in finger vein recognition. However, these networks often overfit label information, losing essential image features, and are sensitive to noise, with minor input changes leading to incorrect recognition. To address above problems, this paper presents a new classification reconstruction cycle generative adversarial network (CRCGAN) for finger vein recognition. CRCGAN comprises a feature generator, a feature discriminator, an image generator, and an image discriminator, which are designed for robust feature extraction. Concretely, the feature generator extracts features for classification, while the image generator reconstructs images from these features. Two discriminators provide feedback, guiding the generators to improve the quality of generated data. With this design of bi-directional image-to-feature mapping and cyclic adversarial training, CRCGAN achieves the extraction of essential features and minimizes overfitting. Additionally, precisely due to the extraction of essential features, CRCGAN is not sensitive to noise. Experimental results on three public databases, including THU-FVFDT2, HKPU, and USM, demonstrate CRCGAN’s competitive performance and strong noise resistance, achieving recognition accuracies of 98.36%, 99.17% and 99.49% respectively, with less than 0.5% degradation on HKPU and USM databases under noisy conditions.}
}
@article{HASSAN2025111069,
title = {A wrapper feature selection approach using Markov blankets},
journal = {Pattern Recognition},
volume = {158},
pages = {111069},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111069},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008203},
author = {Atif Hassan and Jiaul Hoque Paik and Swanand Ravindra Khare and Syed Asif Hassan},
keywords = {Feature selection, Markov blanket, Conditional independence test, Classification, Regression},
abstract = {In feature selection, Markov Blanket (MB) based approaches have attracted considerable attention with most MB discovery algorithms being categorized as filter based techniques. Typically, the Conditional Independence (CI) test employed by such methods is different for different data types. In this article, we propose a novel Markov Blanket based wrapper feature selection method. The proposed approach employs Predictive Permutation Independence (PPI), a novel Conditional Independence (CI) test that allows it to work out-of-the-box for both classification and regression tasks on mixed data. PPI can work with any supervised algorithm to estimate the association of a feature with the target variable while also providing a measure of feature importance. The proposed approach also includes an optional MB aggregation step that can be used to find the optimal MB under non-faithful conditions. Our method11Implementation and experimental results are available at https://github.com/AnonymousMLSubmissions/PatternRecognitionSubmission. outperforms other MB discovery methods, in terms of F1-score, by 7% on average, over 3 large-scale BN datasets. It also outperforms state-of-the-art feature selection techniques on 13 real-world datasets.}
}
@article{LI2025111008,
title = {Residual network with self-adaptive time step size},
journal = {Pattern Recognition},
volume = {158},
pages = {111008},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111008},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007593},
author = {Xiyuan Li and Xin Zou and Weiwei Liu},
keywords = {ResNet, ODE, Residual function, Self-adaptive time step},
abstract = {Residual Networks (ResNet) are pivotal in machine learning. The connection between ResNets and ordinary differential equations (ODEs) has inspired enhancements of ResNets using sophisticated numerical methods for ODE systems. Recent advancements in numerical self-adaptive schemes, which adjust time step sizes based on the feature maps or parameters of residual blocks, have demonstrated promising results in enhancing ResNet performance, surpassing those achieved with fixed time step methods. However, these self-adaptive time step constructions lack theoretical support and can limit performance improvements since the self-adaptive time steps should theoretically depend on both the feature maps and the parameters of the residual blocks. In this study, we conduct a rigorous theoretical analysis of the residual functions associated with fixed or self-adaptive time step methods to demonstrate the advantages and rational designs of the self-adaptive approach. Subsequently, we introduce a novel self-adaptive ResNet, AdaTS-ResNet, which effectively incorporates both feature maps and parameters in its time step adjustments. Experimental results reveal that AdaTS-ResNet surpasses TSCLSTM-ResNet (Yang et al., 2020) in prediction accuracy and computational efficiency, where TSCLSTM represents the latest advancements of the methods designed based on the self-adaptive scheme of ODE. Our findings highlight the potential of improving ResNet architectures through adaptive techniques of dynamical systems, offering insights for future enhancements in deep learning models.}
}
@article{ZHANG2025110984,
title = {UniRTL: A universal RGBT and low-light benchmark for object tracking},
journal = {Pattern Recognition},
volume = {158},
pages = {110984},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110984},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007350},
author = {Lian Zhang and Lingxue Wang and Yuzhen Wu and Mingkun Chen and Dezhi Zheng and Liangcai Cao and Bangze Zeng and Yi Cai},
keywords = {RGBT and low-light benchmark, Multitask benchmark, Unified object tracking, RGBT and low-light image},
abstract = {Solving single- and multiple-object tracking problems with a single network is challenging in the RGBT tracking. We present a universal RGBT and low-light benchmark (UniRTL), which contains 3 × 626 videos for SOT and 3 × 50 videos for MOT, totally with more than 158K frame triplet. The dataset is divided into low-, middle-, and high-illuminance categories based on the measurement of the scene illuminance. We also propose a SOT and MOT unified tracking-with-detection tracker (Unismot) that comprises a detector, first-frame target prior (FTP), and data associator. SOT and MOT are unified by feeding FTP into the detector and data associator. Re-ID long-term matching module and reusing low-score bounding boxes are proposed to augment SOT and MOT performance, respectively. Experiments demonstrate that Unismot performs as well as or better than its counterparts on established RGBT tracking datasets. This work promotes a universal multimodal tracking throughout day and night.}
}
@article{SUI2025110968,
title = {ISDAT: An image-semantic dual adversarial training framework for robust image classification},
journal = {Pattern Recognition},
volume = {158},
pages = {110968},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110968},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007192},
author = {Chenhong Sui and Ao Wang and Haipeng Wang and Hao Liu and Qingtao Gong and Jing Yao and Danfeng Hong},
keywords = {Adversarial training, Adversarial example, Robust learning},
abstract = {Adversarial training is known as one of the most effective heuristic defense methods. Unfortunately, most existing work focuses solely on image-space adversarial training, regardless of the exploration of complementary semantic space. Note that semantic space adversarial training is conducive to compensating for the deficiency of insufficient diversity of adversarial examples in pure image-space one, thereby facilitating the improvement of model robustness. On this account, it is sensible to learn from both adversarial images and features. Therefore, this paper proposes an image-semantic dual adversarial training framework (ISDAT) for the robustness enhancement of the classification model against multi-attacks. In the inner loop of ISDAT, to craft adversarial images as well as adversarial features, both the benign images and semantic features are perturbed through the image space path and semantic space path, respectively. Concerning attacking which intermediate layer of semantic features contributes most to improving the model’s anti-attack capability, we provide theoretical analysis for guidance, avoiding invalid neuron importance predictions and excessive computation. To ensure their respective contributions of adversarial images and features to model robustness, we advocate forging them with diverse loss views. In specific, we develop a C2 loss for adversarial feature generation involving semantic variance, aggressiveness, and high confidence. In the outer loop of ISDAT, to promote the model’s comprehensive understanding of both adversarial images and adversarial features, we give a joint image-semantic-guided model defense method. In specific, we develop an adversarial image-semantic perception loss (IS). Then, driven by this loss, we further establish an image-semantic end-to-end optimization process, which allows dual learning from both adversarial images and features. Experimental results on the CIFAR-10, CIFAR-100, and SVHN datasets demonstrate the effectiveness of our ISDAT in terms of defending against multiple both white-box and black-box attacks. The code will be available at https://github.com/flower6top.}
}
@article{YU2025111000,
title = {Low-rank sparse fully-connected tensor network for tensor completion},
journal = {Pattern Recognition},
volume = {158},
pages = {111000},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111000},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007519},
author = {Jinshi Yu and Zhifu Li and Ge Ma and Jingwen Wang and Tao Zou and Guoxu Zhou},
keywords = {Tensor completion, Tensor network, Low-rank, Sparsity, Image/video recovery},
abstract = {Fully-connected tensor network (FCTN) has recently drawn lots of attention in tensor completion due to its full description of all correlations between any two modes. However, the FCTN model has multiple ranks, and existing methods often ignore the difficulty brought about by rank selection, especially when the model is rank-sensitive. To overcome this drawback, this paper proposed a low-rank sparse FCTN for tensor completion. Specifically, we theoretically show that, for a tensor with FCTN structure, its subtensor can be represented as a coefficient sum of basic tensors, where the coefficients are remained in the FCTN’s factors. This means that factors’ sparsity can improve the robustness of FCTN to larger rank selection. Moreover, according to the relation between the target tensor and its FCTN’s factors, low-rank constrain is used to enhance rank robustness. Lastly, we optimize the proposed model by the alternating direction method of multipliers (ADMM) algorithm. Experimental results show that the low-rank sparse constraint effectively improves the rank robustness of the FCTN model, and achieves excellent results compared with other state-of-the-art completion methods.}
}
@article{MUKHRIYA2025111023,
title = {Iterative target updation based boosting ensembles for outlier detection},
journal = {Pattern Recognition},
volume = {158},
pages = {111023},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111023},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400774X},
author = {Akanksha Mukhriya and Rajeev Kumar},
keywords = {Outlier detection, Ensembles, Target formation, Boosting},
abstract = {In unsupervised settings, the goodness of the alleged outlier set, the target, is crucial. The entire ensemble design is centered around this target. In this work, we propose an unsupervised outlier ensemble, iBoost, which employs iterative target updation with a boosting-based selection. The proposed iBoost sequentially revises the target by applying multiple selection rounds. We use agreement rates with averages to estimate the errors of detectors by which inaccurate detectors are restricted. Multiple rounds of a boosting-based selection are then conducted, where the target for each round is calculated using the outcome of the previous round. Our modified boosting ensemble, BoostSel+, implements an add-on boosting step for the initial selection and uses weighted aggregation for the target. These weights are derived from detectors’ errors. Such continuous reformations lead to a robust target with diverse quality outliers. Experimental results on benchmark datasets suggest that iBoost offers up to 12% improvement in detection performance. It also reveals up to 80% more non-average outliers in its exclusive set to the best competitor.}
}
@article{WAN2025110931,
title = {Target recognition via discriminant information and geometrical structure co-learning using radar sensor network},
journal = {Pattern Recognition},
volume = {157},
pages = {110931},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110931},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006824},
author = {Hao Wan and Xu Si and Peikun Zhu and Jing Liang},
keywords = {High-resolution range profile, Cooperative target recognition, Feature extraction, Radar sensor network},
abstract = {The target recognition system based on radar sensor network (RSN) has recently been widely studied in radar automatic target recognition (RATR). The system can observe the target from multiple perspectives to achieve more robust target recognition. However, the target feature information observed by different radar sensors may be interrelated, complementary, or even contradictory. Therefore, a feature extractor is needed to capture information with discriminant consistency in RSN data. In this work, we propose a new RSN cooperative target recognition method, namely global discriminant information and local geometrical structure co-learning (GILSC). Specifically, the proposed GILSC performs feature extraction by projecting global discriminant information and local neighborhood information into the common feature space. In this feature subspace, the discriminant information carried by the high-resolution range profile (HRRP) from different radars is uniformized, and the geometrical structure of the original HRRP is maintained. The experimental results on the measured HRRP dataset and public simulation dataset obtained by the Air Force Research Laboratory (AFRL) prove the effectiveness of GILSC. Compared with other RSN cooperative target recognition methods, GILSC has the highest recognition rate under the condition of low SNR, and the recognition time is greatly shortened. Compared with a single radar, the target recognition rate can be fully improved by two or three radars’ cooperative target recognition. In addition, experiments on feature subspace show that GILSC has better feature cohesion.}
}
@article{CAO2025111036,
title = {NeRF-based Polarimetric Multi-view Stereo},
journal = {Pattern Recognition},
volume = {158},
pages = {111036},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111036},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007878},
author = {Jiakai Cao and Zhenlong Yuan and Tianlu Mao and Zhaoqi Wang and Zhaoxin Li},
keywords = {Multi-view stereo, Neural radiance fields, Shape-from-polarization, 3D reconstruction},
abstract = {In this paper, we introduce NeRF-based Polarimetric Multi-view Stereo (NPMVS), a novel 3D reconstruction method that combines the advantages of neural radiance field (NeRF) and shape-from-polarization (SfP) to address the challenge posed by textureless areas while preserving the fine-scale geometric details. Our method first leverages neural rendering to yield depth priors for each input view, subsequently estimates more accurate depths and normals using polarimetric refinement. We further introduce a pixel-wise depth rectification process to address the scaling problem inherent to the polarimetric refinement procedure. In addition, we contribute a new realistic pBRDF-based multi-view synthetic dataset, comprised of RGB and polarization images rendered under real-world lighting conditions, which will serve as a valuable resource for future research in this field. Experimental evaluations on both synthetic and real-world datasets validate the superiority of NPMVS, demonstrating its advantage over other state-of-the-art multi-view stereo and shape-from-polarization methods.}
}
@article{WANG2025111065,
title = {Understanding amorphous gorge scenes based on the projection of spatial textures},
journal = {Pattern Recognition},
volume = {158},
pages = {111065},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111065},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008161},
author = {Luping Wang and Hui Wei},
keywords = {Amorphous gorge, Scene understanding, Spatial textures, Mountainous terrain modeling, Topographic map},
abstract = {Geographic information and topographical understanding are crucial for effective field environmental monitoring and autonomous navigation. Unfortunately, implementing this knowledge is often difficult due to unsolved problems such as understanding disorganized and unstructured mountainous regions. Traditional three-dimensional (3D) scene understanding methods from 3D point clouds or fused data are energy-consuming and unsuitable for a resource-constrained unmanned aerial vehicle (UAV) with limited computation, memory, and energy. This study proposes a methodology to understand unstructured gorges and reconstruct them in 3D environments from a low-cost monocular camera. The new approach assigns mountain texture projections to clusters and reshapes surfaces of unstructured mountains to follow geometric constraints regarding position and orientation. The relative relationships between surfaces and vanishing points are then analyzed to understand and reconstruct amorphous gorge scenes. This method is practical and efficient for navigating amorphous gorges based on understanding unstructured mountains. Fortunately, this method requires no prior training and is robust to color and illumination. We compared the percentage of incorrectly classified pixels with the ground truth. Experimental results have demonstrated that our approach can successfully understand amorphous gorges, meeting the requirements for autonomous flying in mountainous environments.}
}
@article{SHEN2025111012,
title = {Asymmetric patch sampling for contrastive learning},
journal = {Pattern Recognition},
volume = {158},
pages = {111012},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111012},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007635},
author = {Chengchao Shen and Jianzhong Chen and Shu Wang and Hulin Kuang and Jin Liu and Jianxin Wang},
keywords = {Contrastive learning, Unsupervised learning, Self-supervised learning, Representation learning, Deep learning},
abstract = {Asymmetric appearance between positive pair effectively reduces the risk of representation degradation in contrastive learning. However, there are still a mass of appearance similarities between positive pair constructed by the existing methods, thus inhibiting the further representation improvement. To address the above issue, we propose a novel asymmetric patch sampling strategy, which significantly reduces the appearance similarities but retains the image semantics. Specifically, dual patch sampling strategies are respectively applied to the given image. First, sparse patch sampling is conducted to obtain the first view, which reduces spatial redundancy of image and allows a more asymmetric view. Second, a selective patch sampling is proposed to construct another view with large appearance discrepancy relative to the first one. Due to the inappreciable appearance similarities between positive pair, the trained model is encouraged to capture the similarities on semantics, instead of low-level ones. Experimental results demonstrate that our method significantly outperforms the existing self-supervised learning methods on ImageNet-1K and CIFAR datasets, e.g., 2.5% finetuning accuracy improvement on CIFAR100. Furthermore, our method achieves state-of-the-art performance on downstream tasks, object detection and instance segmentation on COCO. Additionally, compared to other self-supervised methods, our method is more efficient on both memory and computation during pretraining. The source code and the trained weights are available at https://github.com/visresearch/aps.}
}
@article{ZHANG2025110960,
title = {ZooKT: Task-adaptive knowledge transfer of Model Zoo for few-shot learning},
journal = {Pattern Recognition},
volume = {158},
pages = {110960},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110960},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007118},
author = {Baoquan Zhang and Bingqi Shan and Aoxue Li and Chuyao Luo and Yunming Ye and Zhenguo Li},
keywords = {Few-shot learning, Meta-learning, Model ZOO},
abstract = {Few-shot learning (FSL) aims to recognize novel classes with few examples. It is challenging since it suffers from a data scarcity issue. Although existing methods have shown superior performance, they neglect some pretrained model priors from various datasets/tasks, which are usually free or cheap to collect from some open source platforms (e.g., Github). Inspired by this, in this paper, we focus on a new FSL setting, namely, Model-Zoo-based FSL (MZFSL), which transfers knowledge learned from not only base classes but also a zoo of deep models (called Model Zoo) pretrained on various tasks/datasets. To fully exploit the Model Zoo avoiding the risk of overfitting, we propose a novel knowledge transfer framework, called ZooKT, which amalgamates knowledge of model zoo, instead of finetuning the model zoo, to learn to extract transferable visual features for novel classes. Specifically, we first regard the model zoo as a prior of feature extractor. Then a meta weight prediction network is designed to leverage the prior to predict the weights of target feature extractor for novel classes. Finally, we integrate the target feature extractor to existing FSL methods for performing novel class prediction. Experimental results on few-shot classification and detection scenarios show that our ZooKT outperforms not only single-model finetuning methods but also state-of-the-art multiple-model transfer learning methods, with comparable inference time with a single model.}
}
@article{HU2025111048,
title = {Federated Incremental Learning algorithm based on Topological Data Analysis},
journal = {Pattern Recognition},
volume = {158},
pages = {111048},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111048},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007994},
author = {Kai Hu and Sheng Gong and Lingxiao Li and Yuantu Luo and YaoGen Li and Shanshan Jiang},
keywords = {Federated learning, Incremental learning, Topological Data Analysis},
abstract = {Federated learning is a distributed learning approach aimed at preserving user’s data privacy, while incremental learning is an adaptive machine learning method that enables continuous learning of new data. The combination of these two approaches into Federated Incremental Learning (FIL) algorithms brings together their respective advantages. However, the existing federated incremental learning still faces two main challenges: (1) catastrophic forgetting of previous knowledge by local models when adapting to incremental tasks, and (2) limited capability of the server to capture critical features during federated aggregation. To address these challenges, this paper proposes a federated incremental learning algorithm based on Topological Data Analysis (TDA). Firstly, the algorithm extracts topological features from the input information and designs a Topological Stability Loss (TSL) to mitigate catastrophic forgetting of previous knowledge by local models. Secondly, a feature attention mechanism is utilized to select appropriate attention weights for each local model, enhancing the recognition performance of the global model. The experimental results on publicly available datasets, namely CIFAR10, CIFAR100, and ImageNet, demonstrate that the proposed approach in this paper achieves global accuracies of 67.23%, 65.75%, and 62.41% respectively for the federated incremental learning model. These accuracies surpass the existing Icarl incremental algorithm by improvements of 3.21%, 2.87%, and 1.34% respectively. Therefore, the algorithm presented in this paper achieves better performance, indicating its superiority over existing methods.}
}
@article{LI2025110996,
title = {Reference-then-supervision framework for infrared and visible image fusion},
journal = {Pattern Recognition},
volume = {158},
pages = {110996},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110996},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007477},
author = {Guihui Li and Zhensheng Shi and Zhaorui Gu and Bing Zheng and Haiyong Zheng},
keywords = {Image fusion, Infrared and visible images, Reference-then-supervision, Non-ideal supervision, Deep learning},
abstract = {Infrared and visible image fusion has drawn increasing attention of researchers in recent years, wherein the complementary information between two source images is extracted to synthesize a new fusion image with richer information. Deep neural network is the latest technique to tackle various image fusion problems. However, the natural absence of ground truth makes it very challenging to optimize deep learning models. While the current methods mainly consider to uses the two source images themselves or their visual features, to provide supervision for learning, which is easy to result in the imbalance between detail preservation and brightness distribution. To boost the performance of the fusion models, it is meaningful and necessary to establish a flexible framework that can combine the advantages of existing models to produce reliable supervision for model training. In this work, we propose a novel reference-then-supervision framework, which aims to fully leverage and exploit the available favorable reference information based on the performance of existing methods and then construct high-quality reliable supervision to assist in model building. For this purpose, we design an automatic filter to produce favorable reference and devise an adaptive enhancement method to construct reliable supervision, which helps to aggregate the advantages of various existing fusion methods for yielding visually pleasing results adapting to different complex scenarios. Extensive experiments on two commonly used datasets and our built challenging test set demonstrate that our framework can greatly improve the performance of existing fusion methods. Ablation study and empirical analysis also present the efficacy of our framework design. Furthermore, the applications on downstream pedestrian detection and object tracking tasks indicate the great potential of our framework. Our code and data are publicly available at https://github.com/zhenglab/ReferenceSupervisionIVIF.}
}
@article{YING2025111053,
title = {Temporal adaptive bidirectional bridging for RGB-D tracking},
journal = {Pattern Recognition},
volume = {158},
pages = {111053},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111053},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008045},
author = {Ge Ying and Dawei Zhang and Zhou Ou and Xiao Wang and Zhonglong Zheng},
keywords = {RGB-D object tracking, Multimodal fusion, Temporal adaptive, Bidirectional bridging},
abstract = {RGB-T tracking has been widely applied in various fields such as robotics, surveillance processing, and autonomous driving. In contrast, the development of RGB-D tracking remains relatively slow. Existing RGB-D trackers still adopt the paradigm of extracting spatial information between the target template and the search region for appearance feature matching. However, this paradigm cannot capture target appearance variations in traditional RGB tracking, let alone in more complex multimodal object tracking datasets. To enhance the performance of RGB-D trackers, we propose a novel temporal adaptive bidirectional bridging framework for RGB-D tracking named TABBTrack. TABBTrack employs a three-stream architecture with temporal features and uses a bidirectional bridging module to iteratively bridge RGB and Depth modality information, fully achieving cross-modal feature interaction. In addition, we also achieve temporal information update with low-complexity. Extensive experiments on four popular RGB-D tracking benchmarks demonstrate that our method achieves state-of-the-art performance while running at real-time speeds.}
}
@article{YANG2025111024,
title = {Versatile Teacher: A class-aware teacher–student framework for cross-domain adaptation},
journal = {Pattern Recognition},
volume = {158},
pages = {111024},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111024},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007751},
author = {Runou Yang and Tian Tian and Jinwen Tian},
keywords = {Domain adaptation, Object detection, Mean teacher},
abstract = {Addressing the challenge of domain shift between datasets is vital in maintaining model performance. In the context of cross-domain object detection, the teacher–student framework, a widely-used semi-supervised model, has shown significant accuracy improvements. However, existing methods often overlook class differences, treating all classes equally, resulting in suboptimal results. Furthermore, the integration of instance-level alignment with a one-stage detector, essential due to the absence of a Region Proposal Network (RPN), remains unexplored in this framework. In response to these shortcomings, we introduce a novel teacher–student model named Versatile Teacher (VT). VT differs from previous works by considering class-specific detection difficulty and employing a two-step pseudo-label selection mechanism, referred to as Class-aware Pseudo-label Adaptive Selection (CAPS), to generate more reliable pseudo labels. These labels are leveraged as saliency matrices to guide the discriminator for targeted instance-level alignment. Our method demonstrates promising results on three benchmark datasets, and extends the alignment methods for widely-used one-stage detectors, presenting significant potential for practical applications. Code is available at https://github.com/RicardooYoung/VersatileTeacher.}
}
@article{LIU2025111077,
title = {Graph Laplacian regularization for fast infrared small target detection},
journal = {Pattern Recognition},
volume = {158},
pages = {111077},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111077},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008288},
author = {Ting Liu and Yongxian Liu and Jungang Yang and Boyang Li and Yingqian Wang and Wei An},
keywords = {Graph Laplacian regularization, Minimax concave penalty function, Infrared small target detection, Low-rank},
abstract = {Existing low-rank methods usually introduce manifold learning to achieve good detection performance in complex scenes. However, these methods suffer from high computational complexity because they construct graph on each pixel or each patch of infrared image. To solve this problem, our graph is constructed from each frame of infrared sequence, which helps reduce the number of vertices in the graph. Moreover, our graph Laplacian regularization can describe the low-rank information across each frame of infrared sequence images. Therefore, we use graph Laplacian regularization instead of nuclear norm to describe the low-rank properties, which avoids singular value decomposition computation and reduces computational complexity. Then, we use Minimax Concave penalty function instead of l1 norm to describe the sparsity of targets. Finally, the proposed method is solved by alternating direction multiplier method. Experimental results demonstrate that the proposed method outperforms state-of-the-art methods in terms of detection ability and detection efficiency.}
}
@article{ROY2025111015,
title = {A novel domain independent scene text localizer},
journal = {Pattern Recognition},
volume = {158},
pages = {111015},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111015},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007660},
author = {Ayush Roy and Shivakumara Palaiahnakote and Umapada Pal and Cheng-Lin Liu},
keywords = {Scene text detection, Transformer, Attention module, Drone images, Underwater images},
abstract = {Text localization across multiple domains is crucial for applications like autonomous driving and tracking marathon runners. This work introduces DIPCYT, a novel model that utilizes Domain Independent Partial Convolution and a Yolov5-based Transformer for text localization in scene images from various domains, including natural scenes, underwater, and drone images. Each domain presents unique challenges: underwater images suffer from poor quality and degradation, drone images suffer from tiny text and loss of shapes, and scene images suffer from arbitrarily oriented, shaped text. Additionally, license plates in drone images may not provide rich semantic information compared to other text types due to loss of contextual information between characters. To tackle these challenges, DIPCYT employs new partial convolution layers within Yolov5 and integrates Transformer detection heads with a novel Fourier Positional Convolutional Block Attention Module (FPCBAM). This approach leverages common text properties across domains, such as contextual (global) and spatial (local) relationships. Experimental results demonstrate that DIPCYT outperforms existing methods, achieving F-scores of 0.90, 0.90, 0.77, 0.85, 0.85, and 0.88 on Total-Text, ICDAR 2015, ICDAR 2019 MLT, CTW1500, Drone, and Underwater datasets, respectively.}
}
@article{ZHANG2025111072,
title = {CEDNet: A cascade encoder–decoder network for dense prediction},
journal = {Pattern Recognition},
volume = {158},
pages = {111072},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111072},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008239},
author = {Gang Zhang and Ziyi Li and Chufeng Tang and Jianmin Li and Xiaolin Hu},
keywords = {Dense prediction, Object detection, Instance segmentation, Semantic segmentation, Cascade encoder–decoder, Multi-scale feature fusion},
abstract = {The prevailing methods for dense prediction tasks typically utilize a heavy classification backbone to extract multi-scale features and then fuse these features using a lightweight module. However, these methods allocate most computational resources to the classification backbone, which delays the multi-scale feature fusion and potentially leads to inadequate feature fusion. Although some methods perform feature fusion from early stages, they either fail to fully leverage high-level features to guide low-level feature learning or have complex structures, resulting in sub-optimal performance. We propose a streamlined cascade encoder–decoder network, named CEDNet, tailored for dense prediction tasks. All stages in CEDNet share the same encoder–decoder structure and perform multi-scale feature fusion within each decoder, thereby enhancing the effectiveness of multi-scale feature fusion. We explored three well-known encoder–decoder structures: Hourglass, UNet, and FPN, all of which yielded promising results. Experiments on various dense prediction tasks demonstrated the effectiveness of our method.11Code: https://github.com/zhanggang001/CEDNet.}
}
@article{ZHANG2025111031,
title = {Constrained multi-scale dense connections for biomedical image segmentation},
journal = {Pattern Recognition},
volume = {158},
pages = {111031},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111031},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007829},
author = {Jiawei Zhang and Yanchun Zhang and Hailong Qiu and Tianchen Wang and Xiaomeng Li and Shanfeng Zhu and Meiping Huang and Jian Zhuang and Yiyu Shi and Xiaowei Xu},
keywords = {Multi-scale dense connections, Image segmentation, Network architecture search, Feature fusion},
abstract = {Multi-scale dense connection has been widely used in the biomedical image community to enhance the segmentation performance. In this way, features from all or most scales are aggregated or iteratively fused. However, by analyzing the details, we discover that some connections involving distant scales may not contribute to, or even harm, the performance, while they always introduce a noticeable increase in computational cost. In this paper, we propose constrained multi-scale dense connections (CMDC) for biomedical image segmentation. In contrast to current general lightweight approaches, we first introduce two methods, a naive method and a network architecture search (NAS)-based method, to remove redundant connections and verify the optimal connection configuration, thereby improving overall efficiency and accuracy. The results demonstrate that the two approaches obtain a similar optimal configuration in which most features at the adjacent scales are connected. Then, we applied the optimal configuration to various backbone networks to build constrained multi-scale dense networks (CMD-Net). Experimental results evaluated on eight image segmentation datasets covering biomedical images and natural images demonstrate the effectiveness of CMD-Net across a variety of backbone networks (FCN, U-Net, DeepLabV3, SegNet, FCNsa, ConvUNeXt) with a much lower increase in computational cost. Furthermore, CMD-Net achieves state-of-the-art performance on four publicly available datasets. We believe that the CMDC method can offer valuable insight for ways to engage in dense connectivity at multiple scales within communities. The source code has been made available at https://github.com/JerRuy/CMD-Net.}
}
@article{REN2025111056,
title = {IIS-FVIQA: Finger Vein Image Quality Assessment with intra-class and inter-class similarity},
journal = {Pattern Recognition},
volume = {158},
pages = {111056},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111056},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008070},
author = {Hengyi Ren and Lijuan Sun and Xijian Fan and Ying Cao and Qiaolin Ye},
keywords = {Biometrics, Finger vein, Quality assessment, Automatic labeling, Intra-class similarity, Inter-class similarity},
abstract = {In recent years, Finger Vein Image Quality Assessment (FVIQA) has been recognized as an effective solution to the problem of erroneous recognition resulting from low image quality due to false and missing information in finger vein images, and has become an important part of finger vein recognition systems. Compared to traditional FVIQA methods that rely on domain knowledge, newer methods that reject low-quality images have been favored for their independence from human interference. However, these methods only consider intra-class similarity information and ignore valuable information from inter-class distribution, which is also an important factor in evaluating the performance of recognition systems. In this work, we propose a novel FVIQA approach, named IIS-FVIQA, which concurrently takes into account the intra-class similarity density and inter-class similarity distribution distance within recognition systems. Specifically, our method generates quality scores for finger vein images by combining the information entropy of intra-class similarity distribution and Wasserstein distance of inter-class distribution. Then, we train a regression network for quality prediction using training images and corresponding quality scores. When a new image enters the recognition system, the trained regression network directly predicts the quality score of the image, making it easier for the system to select the corresponding operation based on the quality score of the image. Extensive experiments conducted on benchmark datasets demonstrate that the IIS-FVIQA method proposed in this paper consistently achieves top performance across multiple public datasets. After filtering out 10% of low-quality images predicted by the quality regression network, the recognition system’s performance improves by 43.96% (SDUMLA), 32.23% (MMCBNU_6000), and 21.20% (FV-USM), respectively. Furthermore, the method exhibits strong generalizability across different recognition algorithms (e.g., LBP, MC, and Inception V3) and datasets (e.g., SDUMLA, MMCBNU_6000, and FV-USM).}
}
@article{ZHU2025111043,
title = {Saliency detection for underwater moving object with sonar based on motion estimation and multi-trajectory analysis},
journal = {Pattern Recognition},
volume = {158},
pages = {111043},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111043},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007945},
author = {Jifeng Zhu and Wenyu Cai and Meiyan Zhang and Yuxin Lin and Mingming Liu},
keywords = {Underwater moving object, Forward looking sonar, Trajectory analysis, Salient object detection, Optical flow field},
abstract = {Due to the limited detection range of optical sensors in the ocean, sonar stands out as the predominant method for detecting dynamic objects in the deep sea. Since there is stronger reflection underwater, the objects are more salient in sonar images compared to the background. In this paper, we propose a saliency detection framework for underwater moving object, which consists of three stages. In the first stage, we use optical flow to get rough global motion cues under background interference in unstable sonar platforms. In the second stage, we propose a trajectory analysis paradigm, which converts the motion cues into isolated connected domains for tracking, and then evaluates the trajectories to get the path of real target. In the third stage, we propose a salient object detection (SOD) model that predicts local saliency maps through feature fusion and multi-level supervision. The global saliency map is then obtained by remapping. Finally, extensive experiments conducted on eight sonar videos demonstrate that our proposed methodology outperforms fifteen other saliency object detection approaches.}
}
@article{NIE2025111027,
title = {Fast adaptively balanced min-cut clustering},
journal = {Pattern Recognition},
volume = {158},
pages = {111027},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111027},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007787},
author = {Feiping Nie and Fangyuan Xie and Jingyu Wang and Xuelong Li},
keywords = {Fast clustering, Bipartite graph, Balanced min-cut clustering, Coordinate descent method},
abstract = {Min-cut clustering is a typical graph clustering method, which has been extensively applied for pattern recognition, data analysis and image processing. However, min-cut has trivial solutions, which leads to skewed clustering performance. Spectral clustering (SC) alleviates this by relaxing indicator matrix into continuous embedding and then discretizes the embedding. However, there are two primary challenges in SC, which are high computational complexity and two-stage process. To resolve these issues, a fast adaptively balanced min-cut clustering model (FBMC) is proposed, which directly solves the discrete indicator matrix without any post-processing. We utilize the bipartite graph to accelerate the construction of affinity graph and process of optimization. Besides, the balance factors are added in the model, which could alleviate skewed clustering results. Two specific methods are proposed, in which one adds a balance factor to all clusters while the other assigns balance factors to each cluster separately, referred to as FBMC1 and FBMC2. What is more, a one-step optimization is proposed to solve FBMC1 and FBMC2, the complexity of which is linear of time. Finally, comprehensive experiments results highlight the superior performance of our proposed method.}
}
@article{IBRAHEM2025111068,
title = {Pixel shuffling is all you need: spatially aware convmixer for dense prediction tasks},
journal = {Pattern Recognition},
volume = {158},
pages = {111068},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111068},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008197},
author = {Hatem Ibrahem and Ahmed Salem and Hyun-Soo Kang},
keywords = {Image classification, Semantic segmentation, Depth estimation, Convolutional neural networks},
abstract = {ConvMixer is an extremely simple model that could perform better than the state-of-the-art convolutional-based and vision transformer-based methods thanks to mixing the input image patches using a standard convolution. The global mixing process of the patches is only valid for the classification tasks, but it cannot be used for dense prediction tasks as the spatial information of the image is lost in the mixing process. We propose a more efficient technique for image patching, known as pixel shuffling, as it can preserve spatial information. We downsample the input image using the pixel shuffle downsampling in the same form of image patches so that the ConvMixer can be extended for the dense prediction tasks. This paper proves that pixel shuffle downsampling is more efficient than the standard image patching as it outperforms the original ConvMixer architecture in the CIFAR10 and ImageNet-1k classification tasks. We also suggest spatially-aware ConvMixer architectures based on efficient pixel shuffle downsampling and upsampling operations for semantic segmentation and monocular depth estimation. We performed extensive experiments to test the proposed architectures on several datasets; Pascal VOC2012, Cityscapes, and ADE20k for semantic segmentation, NYU-depthV2, and Cityscapes for depth estimation. We show that SA-ConvMixer is efficient enough to get relatively high accuracy at many tasks in a few training epochs (150∼400). The proposed SA-ConvMixer could achieve an ImageNet-1K Top-1 classification accuracy of 87.02%, mean intersection over union (mIOU) of 87.1% in the PASCAL VOC2012 semantic segmentation task, and absolute relative error of 0.096 in the NYU depthv2 depth estimation task. The implementation code of the proposed method is available at: https://github.com/HatemHosam/SA-ConvMixer/.}
}
@article{LUO2025111003,
title = {5-D spatial–temporal information-based infrared small target detection in complex environments},
journal = {Pattern Recognition},
volume = {158},
pages = {111003},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111003},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007544},
author = {Yuan Luo and Xiaorun Li and Shuhan Chen},
keywords = {Infrared small target detection, Spatial-temporal factor, Moreau envelope, Completion model, Multi-block alternating direction method of multipliers},
abstract = {Recently, infrared (IR) small target detection problem has attracted increasing attention. Tensor component analysis-based techniques have been widely utilized, while they are faced with challenges such as tensor structures, background and target estimation, and real-time performance. In this paper, we propose a 5-D spatial–temporal factor-based completion model (5D-STFC) for IR small target detection. Specifically, a 5-D whitened spatial–temporal patch-tensor is constructed. Then, we devise a spatial–temporal factor-based low-rank background estimation norm and a Moreau envelope-derived sparsity estimation norm based on joint spatial–temporal knowledge. Furthermore, we establish a comprehensive completion model for component analysis. To efficiently solve this model, we design a multi-block alternating direction method of multipliers (multi-block ADMM)-based optimization scheme. Extensive experiments conducted on five real IR sequences demonstrate the superiority of 5D-STFC over nine state-of-the-art competitive methods. It can be concluded that 5D-STFC is excellent and practical in target detectability, background suppressibility, overall performance, and real-time performance.}
}
@article{LIU2025110979,
title = {Global attention module and cascade fusion network for steel surface defect detection},
journal = {Pattern Recognition},
volume = {158},
pages = {110979},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110979},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007301},
author = {Guanghu Liu and Maoxiang Chu and Rongfen Gong and Zehao Zheng},
keywords = {Surface defect detection, Deep learning, Object detection, Global attention module, Cascade fusion network},
abstract = {Steel surface defect detection plays a pivotal role in contemporary society, ensuring quality and safety in construction and manufacturing, reducing production costs, improving efficiency, and driving technological innovation. However, this task encounters challenges, including addressing unstructured features, multi-scale issues, and a scarcity of available data. To overcome these challenges, this paper proposes a global attention module and cascade fusion network for steel surface defect detection, called GC-Net. In this network, the global attention module is proposed to enhance the capability of the model to handle unstructured defects. Subsequently, a cascade fusion network is designed for multi-scale feature fusion, thereby improving detection accuracy for defects of varying scales. Following this, soft non-maximum suppression is applied in the post-processing stage to eliminate redundant detection boxes, further enhancing the detection performance of the network. Finally, a series of data augmentation techniques, including oversampling and small object augmentation, are employed in the experimental sessions to mitigate the issue of data scarcity. The experimental results on two datasets for steel surface defect detection demonstrate that the proposed method outperforms state-of-the-art methods in terms of mAP50 metric (NEU-DET: 0.771, GC10-DET: 0.635). The code is released at https://github.com/Ghlerrix/GC-Net.}
}
@article{VAGENA2025111060,
title = {Semantic aware representation learning for optimizing image retrieval systems in radiology},
journal = {Pattern Recognition},
volume = {158},
pages = {111060},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111060},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008112},
author = {Zografoula Vagena and Xiaoyang Wei and Camille Kurtz and Florence Cloppet},
keywords = {Image retrieval, Representation learning, Knowledge graphs, Semantics, Radiology},
abstract = {Content-based image retrieval (CBIR), which consists of ranking a set of images with respect to a query image based on visual similarity, can assist diagnostic radiologists in assessing medical images, by identifying similar digital images in large image databases. Despite the many recent advances and innovations in CBIR for general images, their adoption in radiology has been slow and limited. In the current paper we attempt to close the gap between the two domains and wisely adapt modern CBIR techniques to radiology images: by extending the latest representation learning techniques in a way that can overcome the unique challenges and at the same time take advantage of the specific opportunities that are present in radiology we were able to come up with novel and effective medical image retrieval methods. Our method achieves the highest CUI@5 scores (18.48, 15.95) on two widely used datasets (ROCO and MEDICAT respectively), showcasing the superiority of the proposed method in comparison with state-of-the-art relevant alternatives.}
}
@article{WANG2025111055,
title = {Motion-guided semantic alignment for line art animation colorization},
journal = {Pattern Recognition},
volume = {158},
pages = {111055},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111055},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008069},
author = {Ning Wang and Wei Wang and Hairui Yang and Hong Zhang and Zhiyong Wang and Zhihui Wang and Haojie Li},
keywords = {Animation colorization, Motion, Line arts, Video colorization, Media art},
abstract = {Line art animation colorization aims to colorize a sequence of line art frames with a reference while ensuring semantic alignment across frames. Recent methods have utilized similarity metrics to maintain inter-frame semantic alignment. However, these approaches typically struggle with precise pixel-to-pixel motion alignment, resulting in issues like semantic inconsistencies and color blurring. To address these problems, we introduce an innovative Motion-Guided Colorization Network(MGC-Net), which incorporates line art motions into account for achieving coarse to fine semantic alignment. At the coarse level, our approach includes a Hierarchical Cost-feature Distillation (HCD) module. This module is designed to establish accurate pixel-to-pixel motion alignment between frames. It employs a motion distillation loss, which effectively transfers accurate color motion representation to line arts at varying hierarchical levels. Specifically, we utilize the cost features in inter-frames as a representation of motion, processed through our self-supervision operator. At the finer level, we propose the Correlation Refinement (CR) module. This module enhances semantic alignment by integrating semantic contextual dependencies between the color reference and the motion-based reference obtained from our motion-based confidence estimation block. CR module refines the final colorization process, improving clear color detail and semantic consistency across frames. Quantitative and qualitative experiments on animation datasets show that MGC-Net outperforms the state-of-the-art methods.}
}
@article{LEE2025111038,
title = {A two-layer regression network for robust and accurate domain adaptation},
journal = {Pattern Recognition},
volume = {158},
pages = {111038},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111038},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007891},
author = {Geonseok Lee and Kichun Lee},
keywords = {Transfer learning, Domain adaptation, Feature representation, Alternating direction method of multipliers (ADMM)},
abstract = {Naturally, classification models suffer from severe performance degradation when they are tested on datasets different from the ones used in training. Unsupervised domain adaptation helps to improve the generalizability of a pre-trained model by transferring knowledge from the labeled source domain (i.e., training dataset) to the unlabeled target domain (i.e., test dataset). The typical way of domain adaptation is to align the data distributions in embedding spaces between source and target domains. However, most existing works only enhance cross-domain consistency in the latent space disregarding the impact of source samples on the target classification task. Besides, discovering a subspace shared by two domains first and then training a transfer classifier separately hardly ensures that the obtained target features are suited for the classification model. To address these issues, in this work, we propose a two-layer regression network for domain adaptation (TRN-DA). TRN-DA learns class-wise domain invariant feature representations by jointly optimizing three tasks: supervised classification of source domain, unsupervised reconstruction of target domain, and alignment of cross-domain distributions. Furthermore, we introduce the concept of weight (importance) for source instances so that the resulting classifier can adapt well to the target domain. We formulate the domain adaptation problem as a unified optimization problem and solve it in an iterative way. Experimental results on different cross-domain image classification tasks demonstrate the encouraging performance of our TRN-DA compared to several recent state-of-the-art methods.}
}
@article{GUO2025110986,
title = {Quaternion Nuclear Norm Minus Frobenius Norm Minimization for color image reconstruction},
journal = {Pattern Recognition},
volume = {158},
pages = {110986},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110986},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007374},
author = {Yu Guo and Guoqing Chen and Tieyong Zeng and Qiyu Jin and Michael Kwok-Po Ng},
keywords = {Quaternion, Low rank, Color image, Low-level vision, Nuclear norm, Frobenius norm},
abstract = {Color image restoration methods typically represent images as vectors in Euclidean space or combinations of three monochrome channels. However, they often overlook the correlation between these channels, leading to color distortion and artifacts in the reconstructed image. To address this, we present Quaternion Nuclear Norm Minus Frobenius Norm Minimization (QNMF), a novel approach for color image reconstruction. QNMF utilizes quaternion algebra to capture the relationships among RGB channels comprehensively. By employing a regularization technique that involves nuclear norm minus Frobenius norm, QNMF approximates the underlying low-rank structure of quaternion-encoded color images. Theoretical proofs are provided to ensure the method’s mathematical integrity. Demonstrating versatility and efficacy, the QNMF regularizer excels in various color low-level vision tasks, including denoising, deblurring, inpainting, and random impulse noise removal, achieving state-of-the-art results.}
}
@article{XIAO2025111071,
title = {Understanding adversarial robustness against on-manifold adversarial examples},
journal = {Pattern Recognition},
volume = {159},
pages = {111071},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111071},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008227},
author = {Jiancong Xiao and Liusha Yang and Yanbo Fan and Jue Wang and Zhi-Quan Luo},
keywords = {Adversarial robustness, On-manifold adversarial examples},
abstract = {Deep neural networks (DNNs) are shown to be vulnerable to adversarial examples. A well-trained model can be easily attacked by adding small perturbations to the original data. One of the hypotheses of the existence of the adversarial examples is the off-manifold assumption: adversarial examples lie off the data manifold. However, recent researches showed that on-manifold adversarial examples also exist. In this paper, we revisit the off-manifold assumption and study a question: at what level is the poor adversarial robustness of neural networks due to on-manifold adversarial examples? Since the true data manifold is unknown in practice, we consider two approximated on-manifold adversarial examples on both real and synthesis datasets. On real datasets, we show that on-manifold adversarial examples have greater attack rates than off-manifold adversarial examples on both standard-trained and adversarially-trained models. On synthetic datasets, theoretically, we prove that on-manifold adversarial examples are powerful, yet adversarial training focuses on off-manifold directions and ignores the on-manifold adversarial examples. Furthermore, we provide analysis to show that the properties derived theoretically can also be observed in practice. Our analysis suggests that on-manifold adversarial examples are important. We should pay more attention to on-manifold adversarial examples to train robust models.}
}
@article{LONG2025110998,
title = {MNN: Mixed nearest-neighbors for self-supervised learning},
journal = {Pattern Recognition},
volume = {158},
pages = {110998},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110998},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007490},
author = {Xianzhong Long and Chen Peng and Yun Li},
keywords = {Self-supervised learning, K-nearest neighbors, Contrastive learning, Image mixture, Momentum encoder},
abstract = {In contrastive self-supervised learning, positive samples are typically drawn from the same image but in different augmented views, resulting in a limited source of positive samples. An effective way to alleviate this problem is to incorporate the relationship between samples, which involves including the top-K nearest neighbors of positive samples. However, the issue of false neighbors (i.e., neighbors that do not belong to the same category as the positive sample) is a significant yet often overlooked challenge. In this paper, we present a simple self-supervised learning framework called Mixed Nearest-Neighbors (MNN) for Self-Supervised Learning. The primary objective of the MNN is to enhance the robustness and accuracy of the model by strategically incorporating synthesized neighboring samples. Specifically, our proposed method utilizes image mixture to mitigate the effects of noise introduced by false neighbors, while adopting an intuitive weighting strategy that effectively integrates these synthesized neighbors into the model. The linear evaluation of MNN on CIFAR-10, CIFAR,100, STL-10, and Tiny ImageNet shows an improvement of 0.41% to 1.96% over the previous state-of-the-art (SOTA) methods and 1.82% to 8.8% over the Mean Shift (MSF) method. Furthermore, the proposed components can be seamlessly integrated into other self-supervised learning algorithms to enhance their performance. Code is available at https://github.com/pc-cp/MNN.}
}
@article{LU2025111030,
title = {Geometry-semantic aware for monocular 3D Semantic Scene Completion},
journal = {Pattern Recognition},
volume = {158},
pages = {111030},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111030},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007817},
author = {Zonghao Lu and Bing Cao and Shuyin Xia and Qinghua Hu},
keywords = {Semantic scene completion, Depth estimation, Semantic segmentation, Contextual information},
abstract = {Monocular Semantic Scene Completion (SSC) empowers intelligent devices to comprehend voxel occupancy (geometry) and semantics in 3D scenes, attracting significant attention in indoor and autonomous driving scenarios. However, existing monocular SSC models primarily map 2D images into 3D space, neglecting the potential benefits of leveraging semantic and geometric understanding in 2D. To address this, we propose the Proxy-embedding Parallel Multi-task Network (PPMNet), which aims to perceive the geometry and semantics of 3D space through depth estimation and semantic segmentation proxy tasks on the 2D perspective plane. Moreover, 2D plane features can be inversely projected into 3D space and subsequently processed using the 3D network. In addition, we enhance contextual awareness in both perspective planes and voxel grids through parallel 2D and 3D decoders. Furthermore, we employ Dual-Head Pyramid Pooling (DHPP) to aggregate information from these two representations. Finally, considering the class imbalance and label incompleteness in practical data, we design a local-to-global loss to prioritize challenging categories. Extensive experiments validate our superiority over state-of-the-art methods on the NYUv2 and SemanticKITTI datasets. The code is available at: https://github.com/luzonghao1/PPMNet.}
}
@article{LIN2025111014,
title = {VPA-Net: A visual perception assistance network for 3d lidar semantic segmentation},
journal = {Pattern Recognition},
volume = {158},
pages = {111014},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111014},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007659},
author = {Fangfang Lin and Tianliang Lin and Yu Yao and Haoling Ren and Jiangdong Wu and Qipeng Cai},
keywords = {Multi-sensor fusion, Semantic segmentation, 3D point cloud, Autonomous driving, Intelligent perception, Dataset},
abstract = {The semantic segmentation of 3D point clouds holds paramount importance in visual perception tasks of automatic driving, including obstacle avoidance, decision control, path planning, and map construction, etc. Multi-sensor fusion is a rational and pivotal technique for implementing LiDAR semantic segmentation. However, effectively fusing and utilizing multi-source data remains a challenging task. In this work, we introduce a novel visual perception-assisted point cloud segmentation network, termed VPA-Net. This network architecture employs a dual-branch design to effectively combine spatial information from point clouds and visual cues from images, thereby bolstering the performance of 3D LiDAR semantic segmentation. More specifically, the dual-branch network structure processes and fuses multi-modal data from both point clouds and RGB images. Subsequently, the intermediate features from the two branches are merged via the proposed attention-based feature fusion module. Furthermore, to address the challenge of precise boundary prediction in large-scale point cloud scene segmentation, we introduce a refinement module based on 3D sparse convolution to enhance the spatial information of the LiDAR point clouds. The effectiveness of our method is validated on SemanticKITTI and a more challenging 3D semantic segmentation dataset. Experimental results underscore significant improvement on SemanticKITTI, with our approach surpassing the state-of-the-art method, achieving a 7.1% higher mIoU than SalsaNext.}
}
@article{FENG2025111042,
title = {Hyperbolic prototype rectification for few-shot 3D point cloud classification},
journal = {Pattern Recognition},
volume = {158},
pages = {111042},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111042},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007933},
author = {Yuan-Zhi Feng and Shing-Ho J. Lin and Xuan Tang and Mu-Yu Wang and Jian-Zhang Zheng and Zi-Yao He and Zi-Yi Pang and Jian Yang and Ming-Song Chen and Xian Wei},
keywords = {Hyperbolic geometry, Few-shot learning, Point cloud classification, Prototype rectification, Feature enhancement},
abstract = {Few-shot point cloud classification is a challenging task in 3D computer vision and has received widespread attention from researchers. Most of the works on deep learning models rely heavily on Euclidean spatial metrics. However, point cloud objects often have complex non-Euclidean geometric structures, with underlying inter/intra-class hierarchical structures, which are difficult to capture by current Euclidean-based deep learning models. Moreover, due to the lack of training samples, many few-shot learning methods often suffer from the overfitting problem. Given the Hyperbolic metric of non-Euclidean geometry offering hierarchical structural prior, as we assume to be able to assist FSL task, we propose Hyperbolic Prototype Rectification (HPR) for few-shot point cloud classification, without requiring extra learnable parameter. Firstly, point clouds are embedded into hyperbolic space to better describe hierarchical similarity relationships in data. Secondly, the HPR utilizes hyperbolic spatial and distributional information to enhance the feature representation and improve the generalization capability, with more appropriate hyperbolic prototypes. The few-shot classification experiments and further ablation studies conducted on widely used point cloud datasets demonstrate the effectiveness of our method. On the real-world ScanObjectNN(-PB) datasets, the average classification accuracy outperforms the SOTA method by 2.08%(0.66%), respectively, indicating that the proposed HPR has great generalization capability and strong robustness against perturbed data. Our code is available at: https://github.com/Jonathan-UCAS/HPR.}
}
@article{HOU2025111037,
title = {Parallel multi-scale dynamic graph neural network for multivariate time series forecasting},
journal = {Pattern Recognition},
volume = {158},
pages = {111037},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111037},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400788X},
author = {Mingjie Hou and Zhenyu Liu and Guodong Sa and Yueyang Wang and Jiacheng Sun and Zhinan Li and Jianrong Tan},
keywords = {Multivariate time series forecasting, Parallel multi-scale dynamic modeling, Parallel multi-scale information fusion, Graph neural network, Graph structure learning},
abstract = {Accurately modeling and predicting multivariate time series (MTS) are crucial in real-world scenarios, where precise predictions aid decision-making. MTS exhibit complex variable relationships and dynamic evolution patterns across various time scales, posing significant modeling challenges. To address these, a parallel multi-scale dynamic graph neural network (PMEDGN) is proposed for parallel modeling of dynamic information across multiple time scales. Spatial-temporal embedding module based on spatial-temporal attention mechanisms and multi-scale dynamic graph generation module are designed. These modules capture implicit spatial-temporal dependencies and self-learn to generate multi-scale dynamic graph structures from MTS without predefined graphs, automatically obtaining the importance of different time scale patterns. Additionally, a global graph convolution module is developed to integrate parallel multi-scale information, enhancing collaboration across time scales for final predictions. Comprehensive experiments on real-world datasets demonstrate the effectiveness and superiority of PMEDGN over state-of-the-art methods, underscoring its potential for practical applications.}
}
@article{WU2025111067,
title = {Class agnostic and specific consistency learning for weakly-supervised point cloud semantic segmentation},
journal = {Pattern Recognition},
volume = {158},
pages = {111067},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111067},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008185},
author = {Junwei Wu and Mingjie Sun and Haotian Xu and Chenru Jiang and Wuwei Ma and Quan Zhang},
keywords = {3d point cloud, Weakly-supervised learning, Consistency learning},
abstract = {This paper focuses on Weakly Supervised 3D Point Cloud Semantic Segmentation (WS3DSS), which involves annotating only a few points while leaving a large number of points unlabeled in the training sample. Existing methods roughly force point-to-point predictions across different augmented versions of inputs close to each other. While this paper introduces a carefully-designed approach for learning class agnostic and specific consistency, based on the teacher–student framework. The proposed class-agnostic consistency learning, to bring the features of student and teacher models closer together, enhances the model robustness by replacing the traditional point-to-point prediction consistency with the group-to-group consistency based on the perturbed local neighboring points’ features. Furthermore, to facilitate learning under class-wise supervisions, we propose a class-specific consistency learning method, pulling the feature of the unlabeled point towards its corresponding class-specific memory bank feature. Such a class of the unlabeled point is determined as the one with the highest probability predicted by the classifier. Extensive experimental results demonstrate that our proposed method surpasses the SOTA method SQN (Huet al., 2022) by 2.5% and 8.3% on S3DIS dataset, and 4.4% and 13.9% on ScanNetV2 dataset, on the 0.1% and 0.01% settings, respectively. Code is available at https://github.com/jasonwjw/CASC.}
}
@article{CHEN2025111026,
title = {SiGNN: A spike-induced graph neural network for dynamic graph representation learning},
journal = {Pattern Recognition},
volume = {158},
pages = {111026},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111026},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007775},
author = {Dong Chen and Shuai Zheng and Muhao Xu and Zhenfeng Zhu and Yao Zhao},
keywords = {Dynamic graph representation learning, Spiking neural networks, Graph neural networks, Multiple time granularities},
abstract = {In the domain of dynamic graph representation learning (DGRL), capturing the temporal evolution within real-world networks is of paramount importance. Spiking Neural Networks (SNNs), renowned for their temporal dynamics and low-power characteristics, provide an efficient solution for temporal processing in DGRL tasks. However, due to the spike-based information encoding mechanism of SNNs, existing DGRL methods that employ SNNs face significant limitations in their representational capacity. To address this issue, we propose an innovative framework named Spike-induced Graph Neural Network (SiGNN) for learning enhanced spatiotemporal representations on dynamic graphs. Specifically, we achieve a harmonious integration of SNNs and GNNs through a novel Temporal Activation (TA) mechanism. This mechanism enables SiGNN to effectively harness the temporal dynamics of SNNs while circumventing the representational constraints imposed by the binary nature of spikes. Furthermore, leveraging the inherent adaptability of SNNs, SiGNN incorporates an in-depth analysis of evolutionary patterns within graphs across multiple time granularities, thereby facilitating the acquisition of multiscale temporal node representations. Extensive experiments on various real-world dynamic graph datasets demonstrate the superior performance of SiGNN in the node classification task. Our code is publicly available at https://github.com/CharliedoD/SiGNN.}
}
@article{LI2025111002,
title = {DCFusion: Difference correlation-driven fusion mechanism of infrared and visible images},
journal = {Pattern Recognition},
volume = {158},
pages = {111002},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111002},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007532},
author = {Min Li and Enguang Zuo and Feng Li and Cheng Chen and Chaoxun Guo and Pei Liu and Yunling Wang and Xiaoyi Lv and Chen Chen},
keywords = {Difference correlation, Infrared and visible image, Salient target mask},
abstract = {In end-to-end image fusion models, the loss function significantly impacts performance. However, most loss functions treat salient and background regions in source images equally, failing to distinguish complementary areas in multimodal images. This limits the model’s ability to effectively integrate information from these regions. Therefore, we propose difference correlation-driven fusion mechanism of infrared and visible images, which called DCFusion. Specifically, the model utilizes a dual-branch interactive network that dynamically fuses cross-modal multi-scale complementary information through element-wise multiplication, effectively integrating region-specific information. We introduce a two-stage method for generating salient target masks that adaptively focus on high-contrast regions in infrared images by analyzing pixel contrasts in local areas. Furthermore, we utilize the salient target masks to create heterogeneous images and design the LSCD loss function to minimize the information gap between the heterogeneous images and the fused image, thereby enhancing the model’s interpretability. Experiments on the RoadScene and TNO datasets show that DCFusion surpasses with existing representativity fusion approaches, achieving state-of-the-art performance in both subjective visual and objective evaluations. Our code will be publicly available at https://github.com/MinLila/DCFusion.}
}
@article{LIU2025111078,
title = {Feature-matching method based on keypoint response constraint using binary encoding of phase congruency},
journal = {Pattern Recognition},
volume = {159},
pages = {111078},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111078},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400829X},
author = {Xiaomin Liu and Qiqi Li and Yuzhe Hu and Jeng-Shyang Pan and Huaqi Zhao and Donghua Yuan and Jun-Bao Li},
keywords = {Cross-view geo-localization, Object matching, Phase congruency, Autoencoders, Property optimization point matching},
abstract = {At present, the cross-view geo-localization (CGL) task is still far from practical. This is mainly because of the intensity differences between the two images from different sensors. In this study, we propose a learning feature-matching framework with binary encoding of phase congruency to solve the problem of intensity differences between the two images. First, the autoencoder-weighted fusion method is used to obtain an intensity alignment image that would make the two images from different sensors comparable. Second, the keypoint responses of the two images are calculated using the binary encoding of the phase congruency theory, which is employed to construct the feature-matching method. This method considers the invariance of the phase information in weak-texture images and uses the phase information to compute the keypoint response with higher distinguishability and matchability. Finally, using the two intensity-aligned images, a method for computing the binary encoding of the phase congruency keypoint response loss function is employed to optimize the keypoint detector and feature descriptor and obtain the corresponding keypoint set of the two images. The experimental results show that the improved feature matching is superior to existing methods and solves the problem of view differences in object matching. The code can be found at https://github.com/lqq-dot/FMPCKR.}
}
@article{ZENG2025111059,
title = {An interpretable unsupervised capsule network via comprehensive contrastive learning and two-stage training},
journal = {Pattern Recognition},
volume = {158},
pages = {111059},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111059},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008100},
author = {Ru Zeng and Yan Song and Yanjiu Zhong},
keywords = {Unsupervised capsule network, Comprehensive contrastive loss, Routing algorithm, Interpretability, Primary capsules},
abstract = {Limited attention has been given to unsupervised capsule networks (CapsNets) with contrastive learning due to the challenge of harmoniously learning interpretable primary and high-level capsules. To address this issue, we focus on three aspects: loss function, routing algorithm, and training strategy. First, we propose a comprehensive contrastive loss to ensure consistency in learning both high-level and primary capsules across different objects. Next, we introduce an agreement-based routing mechanism for the activation of high-level capsules. Finally, we present a two-stage training strategy to resolve conflicts between multiple losses. Ablation experiments show that these methods all improve model performance. Results from linear evaluation and semi-supervised learning demonstrate that our model outperforms other CapsNets and convolutional neural networks in learning high-level capsules. Additionally, visualizing capsules provides insights into the primary capsules, which remain consistent across images and align with human vision.}
}
@article{ZHANG2025111083,
title = {Piecewise convolutional neural network relation extraction with self-attention mechanism},
journal = {Pattern Recognition},
volume = {159},
pages = {111083},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111083},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008343},
author = {Bo Zhang and Li Xu and Ke-Hao Liu and Ru Yang and Mao-Zhen Li and Xiao-Yang Guo},
keywords = {Relation extraction, Multi-head attention, PCNN, Variational autoencoder},
abstract = {The task of relation extraction in natural language processing is to identify the relation between two specified entities in a sentence. However, the existing model methods do not fully utilize the word feature information and pay little attention to the influence degree of the relative relation extraction results of each word. In order to address the aforementioned issues, we propose a relation extraction method based on self-attention mechanism (SPCNN-VAE) to solve the above problems. First, we use a multi-head self-attention mechanism to process word vectors and generate sentence feature vector representations, which can be used to extract semantic dependencies between words in sentences. Then, we introduce the word position to combine the sentence feature representation with the position feature representation of words to form the input representation of piecewise convolutional neural network (PCNN). Furthermore, to identify the word feature information that is most useful for relation extraction, an attention-based pooling operation is employed to capture key convolutional features and classify the feature vectors. Finally, regularization is performed by a variational autoencoder (VAE) to enhance the encoding ability of model word information features. The performance analysis is performed on SemEval 2010 task 8, and the experimental results show that the proposed relation extraction model is effective and outperforms some competitive baselines.}
}
@article{ZHANG2025110993,
title = {A novel multi-modal fusion method based on uncertainty-guided meta-learning},
journal = {Pattern Recognition},
volume = {158},
pages = {110993},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110993},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007441},
author = {Duoyi Zhang and Md Abul Bashar and Richi Nayak},
keywords = {Multi-modal fusion, Neural networks, Feature-level bias, Uncertainty estimation, Meta-learning framework},
abstract = {Multi-modal data fusion for effective feature representation in machine learning is challenging due to intrinsic biases present within and across different modalities. Existing multi-modal data fusion methods often face difficulties in learning generic features due to diverse noise patterns and variations in feature dynamics across different modalities. In this paper, we present a novel method called Uncertainty-guided Meta-Learning Multi-modal Fusion and Classification (UMLMC) to address these challenges. UMLMC dynamically transforms multi-modal feature spaces at both the pre- and post-fusion levels by incorporating uncertainty estimates from an auxiliary network. Our model is optimized using a meta-learning algorithm to enhance its generalization capabilities. Extensive experiments on multi-modal data from diverse domains, along with comparisons to state-of-the-art methods, demonstrate the effectiveness of UMLMC in improving classification performance. These results confirm that UMLMC, with its innovative uncertainty estimation and meta-learning framework, effectively learns informative intra- and inter-modal features, leading to superior classification outcomes.}
}
@article{WANG2025111062,
title = {Intuitive-K-prototypes: A mixed data clustering algorithm with intuitionistic distribution centroid},
journal = {Pattern Recognition},
volume = {158},
pages = {111062},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111062},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008136},
author = {Hongli Wang and Jusheng Mi},
keywords = {Clustering, Mixed data, Distribution centroid, Intuitionistic fuzzy sets},
abstract = {Data sets are usually mixed with numerical and categorical attributes in the real world. Data mining of mixed data makes a lot of sense. This paper proposes an Intuitive-K-prototypes clustering algorithm with improved prototype representation and attribute weights. The proposed algorithm defines intuitionistic distribution centroid for categorical attributes. In our approach, a heuristic search for initial prototypes is performed. Then, we combine the mean of numerical attributes and intuitionistic distribution centroid to represent the cluster prototype. In addition, intra-cluster complexity and inter-cluster similarity are used to adjust attribute weights, with higher priority given to those with lower complexity and similarity. The membership and non-membership distance are calculated using the intuitionistic distribution centroid. These distances are then combined parametrically to obtain the composite distance. The algorithm is judged for its clustering effectiveness on the real UCI data set, and the results show that the proposed algorithm outperforms the traditional clustering algorithm in most cases.}
}
@article{LUO2025110977,
title = {Group link prediction in bipartite graphs with graph neural networks},
journal = {Pattern Recognition},
volume = {158},
pages = {110977},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110977},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007283},
author = {Shijie Luo and He Li and Jianbin Huang and Xiaoke Ma and Jiangtao Cui and Shaojie Qiao and Jaesoo Yoo},
keywords = {Bipartite graphs, Link prediction, Group link prediction, Graph machine learning, Graph neural networks},
abstract = {Group link prediction is of both theoretical and practical significance since it can be used to analyze relationships between individuals and groups. However, obeying the homophily assumption, most of previous group link prediction methods suffer from missing information and weak generalization. To this end, we propose BiGLP, a novel group link prediction method based on graph neural networks (GNNs), to infer links between individuals and groups in bipartite graphs. To model intra-group relationships, we first design a new GNN with sampling strategy to learn representations of individuals by capturing neighborhood information. Moreover, we extract features from neighborhood of groups to accurately model inter-group relationships. From a new perspective that combining intra-group and inter-group relationships, BiGLP finally obtains representations of groups and predicts the targets based on group vectors. Experimental results on four datasets show that, in three evaluation metrics, BiGLP obtains average gains of 2.8%, 8.2% and 3.9%.}
}
@article{ZHUANG2025111045,
title = {SemiGMMPoint: Semi-supervised point cloud segmentation based on Gaussian mixture models},
journal = {Pattern Recognition},
volume = {158},
pages = {111045},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111045},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007969},
author = {Xianwei Zhuang and Hualiang Wang and Xiaoxuan He and Siming Fu and Haoji Hu},
keywords = {3D point cloud segmentation, Semi-supervised learning, Generative classifier, Gaussian mixture models, Distribution similarity minimization, Contrastive learning},
abstract = {Existing semi-supervised point cloud segmentation methods emphasize on discriminative learning, which overlooks the underlying class-conditional distributions and distribution similarities. In this paper, we propose SemiGMMPoint, the first generative framework for semi-supervised 3D point cloud segmentation in real-world and large-scale settings. Specifically, we propose a point dense generative classifier based on Gaussian mixture models (GMMs) to explicitly estimate class-conditional distributions. On top of it, we incorporate a novel similarity-minimization algorithm into the Expectation–Maximization (EM) based GMM parameter estimation, which minimizes the inter-class distribution similarity in the representation space. Moreover, we utilize the well-calibrated posterior to develop a modified point contrastive loss to mitigate sampling bias in semi-supervised settings. Extensive experiments show that SemiGMMPoint significantly boosts performance for semi-supervised point cloud segmentation on many state-of-the-art backbones without requiring architectural changes. Codes are available at https://github.com/jojodidli/SemiGMMPoint.}
}
@article{YANG2025111021,
title = {Video Anomaly Detection via self-supervised and spatio-temporal proxy tasks learning},
journal = {Pattern Recognition},
volume = {158},
pages = {111021},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111021},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007726},
author = {Qingyang Yang and Chuanxu Wang and Peng Liu and Zitai Jiang and Jiajiong Li},
keywords = {Video Anomaly Detection, Self-supervised learning model, Spatio-temporal decoupling, Proxy tasks},
abstract = {Video Anomaly Detection (VAD) aims to identify events in videos that deviate from typical patterns. Given the scarcity of anomalous samples, previous research has primarily focused on learning regular patterns from datasets exclusively containing normal behaviors, and treating deviations from these patterns as anomalies. However, most of these methods are constrained by coarse-grained modeling approaches that renders them incapable of learning highly-discriminative features, which are necessary to effectively distinguish between the subtle differences between normal and abnormal behaviors. To better capture these features, we propose an innovative method. Initially, pseudo-anomalous samples for appearance and motion are generated through geometric transformations (2D rotations) and the scrambling of video sequences. Subsequently, a dual-branch network featuring spatio-temporal decoupling is proposed, in which the spatial and temporal branches each handle a specific proxy task. These tasks are designed to distinguish between normal and pseudo-anomalous samples, involving operations such as predicting patch-based 2D rotation angles and classifying video frame triplets as total-anomaly, left-anomaly, right-anomaly, and non-anomaly. Our approach employs an end-to-end training methodology, without relying on pre-trained models (except for the object detector). Evaluations on the UCSD Ped2, Avenue, and ShanghaiTech datasets show that our method achieved AUC scores of 99.1%, 91.9%, and 81.1%, respectively, demonstrating its effectiveness. The code is publicly accessible at the following link: https://spatio-temporal-tasks.}
}
@article{WANG2025111058,
title = {Efficient time series adaptive representation learning via Dynamic Routing Sparse Attention},
journal = {Pattern Recognition},
volume = {158},
pages = {111058},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111058},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008094},
author = {Wenyan Wang and Enguang Zuo and Chen Chen and Cheng Chen and Jie Zhong and Ziwei Yan and Xiaoyi Lv},
keywords = {Multivariate time series, Representation learning, Forecasting, Dimension},
abstract = {Time series prediction plays a crucial role in various fields but also faces significant challenges. Converting original 1D time series data into 2D data through dimension transformation allows capturing more hidden features but incurs high memory consumption and low time efficiency. We have designed a sparse attention mechanism with dynamic routing perception called Dynamic Routing Sparse Attention (DRSA) to address these issues. Specifically, DRSA can effectively handle variations of complex time series data. Meanwhile, under memory constraints, the Dynamic Routing Filter (DRF) module further refines it by filtering the blocked 2D time series data to identify the most relevant feature vectors in the local context. We conducted predictive experiments on six real-world time series datasets with fine granularity and long sequence dependencies. Compared to eight state-of-the-art (SOTA) models, DRSA demonstrated relative improvements ranging from 4.18% to 81.02%. Furthermore, its time efficiency is 2 to 5 times higher than the baseline. Our code and dataset will be available at https://github.com/wwy8/DRSA_main.}
}
@article{ZHANG2025111005,
title = {PSSS-EEG: A Probabilistic-masking Self-Supervised Swin-transformer model for EEG-based drowsiness recognition},
journal = {Pattern Recognition},
volume = {158},
pages = {111005},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111005},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007568},
author = {Jiaming Zhang and Fangzuo Zhang and Hongtao Wei},
keywords = {Electroencephalography (EEG), Drowsiness recognition, Data augmentation, Transformer, Self-supervised learning},
abstract = {Drowsiness recognition based on Electroencephalography (EEG) signals is a critical task with wide-ranging applications in areas such as transportation safety. However, existing methods for EEG-based drowsiness recognition have demonstrated limited performance in data-limited scenarios and struggle with capturing the diverse patterns among different subjects. In this paper, we propose a novel approach, the Probabilistic-masking Self-Supervised Swin-transformer model (PSSS-EEG), for accurate and robust EEG-based drowsiness recognition. The PSSS-EEG model integrates data augmentation and self-supervised learning techniques to improve the recognition accuracy and generalization ability of drowsiness recognition. Specifically, we introduce the incomplete probabilistic-masking operation to the data augmentation. Moreover, the swin-transformer architecture is employed as the backbone of the self-supervised network to capture long-range dependencies at different scales in EEG data. The retention weights are introduced into the loss function as a linkage between data augmentation and self-supervised network. Experimental results demonstrate that our model outperforms existing methods in EEG-based drowsiness recognition accuracy, achieving an accuracy exceeding 80%. In addition, this work highlights the importance of advanced data augmentation techniques and self-supervised learning models. It also contributes to filling a crucial gap in the existing literature about the integration of data augmentation and self-supervised learning, providing a more effective and adaptable solution for EEG-based recognition.}
}
@article{WEN2025111033,
title = {An illumination-guided dual attention vision transformer for low-light image enhancement},
journal = {Pattern Recognition},
volume = {158},
pages = {111033},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111033},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007842},
author = {Yanjie Wen and Ping Xu and Zhihong Li and Wangtu Xu(ATO)},
keywords = {Low-light image enhancement, One-stage Retinex Theory, Transformer, Multi-head Self Attention, Channel-wise attention, Window attention},
abstract = {Existing Retinex-based low-light image enhancement methods often overlook corruptions hidden in darkness or pattern collapse caused by the lit-up process. Recent deep learning approaches suggest the use of U-shaped networks with Vision in Transformer (VIT) to address these issues. However, most VIT-based methods focus on channel modeling to reduce expensive computational costs, but in which the restored images suffer from spatial illumination inconsistencies, artifacts, and blurriness. To end for this, we propose a novel one-stage Retinex-based Illumination-Guided Dual transformer model (IGDFormer) to lit up low-light images. The model consists of an estimator and a restorer. The estimator generates a light-up feature map and a lit-up image through pure Convolutional Neural Networks (CNNs). The restorer denoises the lit-up image with a U-shaped network equipped with an Illumination-Guided Dual Attention Block (IGDAB). Specifically, IGDAB consists of cascaded channel attention and window attention that achieves cross-channel/spatial modeling. Channel attention alleviates inductive bias through the CNN-Transformer collaborative layer, and window attention introduces spatial domains knowledge by partitioning and shifting. In addition, the light-up features act as values guide the interaction modeling of non-local illumination intensities in both the channel and spatial domains. Extensive experiments were conducted on 5 low-light image enhancement benchmarks and 1 dark object detection benchmark, which demonstrate that the efficacy of our IGDFormer and its superiority in restoring spatial details compared to other state-of-the-art methods. The code is available at https://github.com/YanJieWen/IGDFormer-light-up-dark.}
}
@article{FENG2025110981,
title = {Homogeneous and heterogeneous relational graph for visible-infrared person re-identification},
journal = {Pattern Recognition},
volume = {158},
pages = {110981},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110981},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007325},
author = {Yujian Feng and Feng Chen and Jian Yu and Yimu Ji and Fei Wu and Shangdon Liu and Xiao-Yuan Jing},
keywords = {Visible-infrared person re-identification, Heterogeneous structured graph, Homogeneous structured graph, Mutual information},
abstract = {Visible-infrared person re-identification (VI Re-ID) aims to match person images between the visible and infrared modalities. Existing VI Re-ID methods mainly focus on extracting homogeneous structural relationships in an image, i.e., the relations between local features, while ignoring the heterogeneous correlation of local features in different modalities. The heterogeneous structured relationship is crucial to learn effective identity representations and perform cross-modality matching. In this paper, we introduce the Homogeneous–Heterogeneous Graph Framework (HHGF) including the Homogeneous Structured Graph (HOSG), Heterogeneous Graph Alignment Module (HGAM), and cross-modality cross-correlation (CMCC) loss function, which models the homogeneous structural relationships within individual modalities and mine the heterogeneous structural correlations between the visible and infrared modalities. First, the HOSG mines one-vs.-rest relation between an arbitrary node (local feature) and all the rest nodes within a visible or infrared image to learn effective identity representation. Second, to find cross-modality identity-consistent correspondence, the HGAM further measures the relational edge strength between local node features of two modalities with routing search way. Third, we propose the CMCC loss function to extract the modality invariance of feature representations of visible and infrared graphs. CMCC computes the mutual information between modalities and expels semantic redundancy. Extensive experiments on SYSU-MM01 and RegDB datasets demonstrate that our method outperforms state-of-the-art methods. The code is available at https://github.com/fegnyujian/Homogeneous-and-Heterogeneous-Relational-Graph.}
}
@article{CUI2025111074,
title = {EENet: An effective and efficient network for single image dehazing},
journal = {Pattern Recognition},
volume = {158},
pages = {111074},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111074},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008252},
author = {Yuning Cui and Qiang Wang and Chaopeng Li and Wenqi Ren and Alois Knoll},
keywords = {Image dehazing, Frequency separation, Multi-scale representation learning, Efficient network},
abstract = {While numerous solutions leveraging convolutional neural networks and Transformers have been proposed for image dehazing, there remains significant potential to improve the balance between efficiency and reconstruction performance. In this paper, we introduce an efficient and effective network named EENet, designed for image dehazing through enhanced spatial–spectral learning. EENet comprises three primary modules: the frequency processing module, the spatial processing module, and the dual-domain interaction module. Specifically, the frequency processing module handles Fourier components individually based on their distinct properties for image dehazing while also modeling global dependencies according to the convolution theorem. Additionally, the spatial processing module is designed to enable multi-scale learning. Finally, the dual-domain interaction module promotes information exchange between the frequency and spatial domains. Extensive experiments demonstrate that EENet achieves state-of-the-art performance on seven synthetic and real-world datasets for image dehazing. Moreover, the network’s generalization ability is validated by extending it to image desnowing, image defocus deblurring, and low-light image enhancement.}
}
@article{ZHONG2025111061,
title = {MBQuant: A novel multi-branch topology method for arbitrary bit-width network quantization},
journal = {Pattern Recognition},
volume = {158},
pages = {111061},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111061},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008124},
author = {Yunshan Zhong and Yuyao Zhou and Fei Chao and Rongrong Ji},
keywords = {Network quantization, Quantization-aware training, Arbitrary bit-width, Multi-branch topology},
abstract = {Arbitrary bit-width network quantization has received significant attention due to its high adaptability to various bit-width requirements during runtime. However, in this paper, we investigate existing methods and observe a significant accumulation of quantization errors caused by switching weight and activations bit-widths, leading to limited performance. To address this issue, we propose MBQuant, a novel method that utilizes a multi-branch topology for arbitrary bit-width quantization. MBQuant duplicates the network body into multiple independent branches, where the weights of each branch are quantized to a fixed 2-bit and the activations remain in the input bit-width. For completing the computation of a desired bit-width, MBQuant selects multiple branches, ensuring that the computational costs match those of the desired bit-width, to carry out forward propagation. By fixing the weight bit-width, MBQuant substantially reduces quantization errors caused by switching weight bit-widths. Additionally, we observe that the first branch suffers from quantization errors caused by all bit-widths, leading to performance degradation. Thus, we introduce an amortization branch selection strategy that amortizes the errors. Specifically, the first branch is selected only for certain bit-widths, rather than universally, thereby the errors are distributed among the branches more evenly. Finally, we adopt an in-place distillation strategy that uses the largest bit-width to guide the other bit-widths to further enhance MBQuant’s performance. Extensive experiments demonstrate that MBQuant achieves significant performance gains compared to existing arbitrary bit-width quantization methods. Code is made publicly available at https://github.com/zysxmu/MBQuant.}
}
@article{FOJCIK2025111016,
title = {Extremely compact video representation for efficient near-duplicates detection},
journal = {Pattern Recognition},
volume = {158},
pages = {111016},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111016},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007672},
author = {Katarzyna Fojcik and Piotr Syga and Marek Klonowski},
keywords = {Video copy detection, Near-duplicate video, Video dataset, Feature representation, Frame matching},
abstract = {With the influx of online video uploads, a robust method for detecting video copies under various distortions is essential for copyright protection and search efficiency. We propose a compact video representation that uses a Siamese Neural Network for near-duplicate detection. This approach achieves up to 0.998 recall and 0.853 precision, even with distorted 56x56px miniatures. Consequently, we derive extremely compact video descriptors, facilitating video fragment retrieval in large datasets. Our method pre-selects frames from a video by identifying local maximums from the interframe differences curve, preserving characteristic sequence patterns even after extreme compression. Utilizing a simpler convolution-based model in the Siamese Neural Network, we improve results by up to 8% over VGG-16, with a 1.7-fold reduction in inference time. With compact descriptors of 1.875 kB, our models outperform others by more than two times. Additionally, we introduce a new dataset with dynamically changing scenes, enhancing its suitability for near-duplicate video detection.}
}
@article{JIAO2025111049,
title = {DA-GAN: Dual-attention generative adversarial networks for real-world exquisite makeup transfer},
journal = {Pattern Recognition},
volume = {158},
pages = {111049},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111049},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008008},
author = {Qianfen Jiao and Zhen Xu and Si Wu and Hau-San Wong},
keywords = {Facial makeup transfer, Attention mechanism, Image translation},
abstract = {Existing makeup transfer models do not in general perform satisfactorily when applied to real-world unseen face images. Additionally, most existing models primarily focus on color distribution and struggle to transfer accurate details, particularly for elaborate artistic makeup styles. Recognizing these limitations, we propose the dual-attention generative adversarial networks, which is referred to as DA-GAN. Our work addresses the challenge of wide-ranging and exquisite makeup transfer in real-world settings. DA-GAN mainly incorporates a global attention module, a local attention module, and a makeup rendering module. The global attention module builds a coarse mapping from the reference makeup image to the source image. Based on the coarse mapping, the local attention module is designed to build a more precise mapping and preserve makeup details. The local attention module divides the entire face into a set of local patches. Based on these patches, the makeup rendering module applies an exquisite makeup style to the source image which benefits from a set of local discriminators. Extensive experiments demonstrate that DA-GAN achieves state-of-the-art results in transferring exquisite details for both ordinary and artistic makeup styles in real-world scenarios.}
}
@article{KAEWKORN2025111044,
title = {RP-Net: A Robust Polar Transformation Network for rotation-invariant face detection},
journal = {Pattern Recognition},
volume = {158},
pages = {111044},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111044},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007957},
author = {Hathai Kaewkorn and Lifang Zhou and Weisheng Li},
keywords = {Rotation-invariant face detection, Polar transformation, Face landmark localization},
abstract = {Face detection is challenging in unconstrained environments, where it encounters various challenges such as orientation, pose, and occlusion. Deep convolutional neural networks, particularly cascaded ones, have greatly improved detection performance but still struggle with rotating objects due to limitations in the Cartesian coordinate system. Although data augmentation can mitigate this issue, it also increases computational demands. This paper introduces the Robust Polar Transformation Network (RP-Net) for rotation-invariant face detection. RP-Net converts the complex rotational problem into a simpler translational one to enhance feature extraction and computational efficiency. Additionally, the Advanced Spatial-Channel Restoration (ASCR) module optimizes facial landmark detection within polar domains and restores critical details lost during transformation. Experimental results on benchmark datasets show that RP-Net significantly improves rotation invariance over traditional CNNs and surpasses several state-of-the-art rotation-invariant face detection methods.}
}
@article{GAO2025111057,
title = {Frequency domain task-adaptive network for restoring images with combined degradations},
journal = {Pattern Recognition},
volume = {158},
pages = {111057},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111057},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008082},
author = {Hu Gao and Bowen Ma and Ying Zhang and Jingfan Yang and Jing Yang and Depeng Dang},
keywords = {Image restoration, Frequency domain-based gate, Task-adaptive, Combined degradations},
abstract = {Image restoration seeks to obtain a high-quality image by eliminating degradations. While existing methods have shown remarkable performance in image restoration, the majority are designed for single degradation. However, in the real world, environmental factors are complex and variable, leading to images with combined degradation factors—like the simultaneous presence of rain, noise, and haze in a single image. This complexity poses a challenge for existing methods to be effectively applied in real-world scenes. In this paper, we propose a frequency domain-based network for adaptively restoring images with various combinations of degradation factors. Specifically, we design a frequency domain-based gate block (FDGB) to selectively determine which low and high-frequency information should be preserved, choosing the most informative components for recovery. Additionally, we develop a task adaptive block (TAB) composed of FDGBs and frequency domain-based re-weight blocks (FDRBs) to adaptively restore various combined degraded images. FDRB ensures that the TAB can explore various combinations of FDGBs by utilizing a gating mechanism to re-weight the output features of FDGB based on the input signals. Finally, we introduce a Fast Fourier Block (FFB) to enrich the feature set and provide collaborative refinement for the FDGB. To facilitate the training of our proposed method, we create a dataset with various combinations of degradation factors. The resulting tightly interlinked architecture, named as FDTANet, extensive experiments demonstrate that our approach excels not only in restoring images afflicted with combined degradations but also demonstrates competitive performance when compared to state-of-the-art models for single-degradation restoration. The code and the pre-trained models are released at https://github.com/Tombs98/FDTANet/.}
}
@article{GUO2025111020,
title = {Collaborative graph neural networks for augmented graphs: A local-to-global perspective},
journal = {Pattern Recognition},
volume = {158},
pages = {111020},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111020},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007714},
author = {Qihang Guo and Xibei Yang and Ming Li and Yuhua Qian},
keywords = {Graph neural networks, Multi-perspective learning, Embedding fusion, Complementary learning},
abstract = {In the field of graph neural networks (GNNs) for representation learning, a noteworthy highlight is the potential of embedding fusion architectures for augmented graphs. However, prevalent GNN embedding fusion architectures mainly focus on handling graph combinations from a global perspective, often ignoring their collaboration with the information of local graph combinations. This inherent limitation constrains the ability of the constructed models to handle multiple input graphs, particularly when dealing with noisy input graphs collected from error-prone sources or those resulting from deficiencies in graph augmentation methods. In this paper, we propose an effective and robust embedding fusion architecture from a local-to-global perspective termed collaborative graph neural networks for augmented graphs (LoGo-GNN). Essentially, LoGo-GNN leverages a pairwise graph combination scheme to generate local perspective inputs. Together with the global graph combination, this serves as the basis to generate a local-to-global perspective. Specifically, LoGo-GNN employs a perturbation augmentation strategy to generate multiple augmentation graphs, thereby facilitating collaboration and embedding fusion from a local-to-global perspective through the use of graph combinations. In addition, LoGo-GNN incorporates a novel loss function for learning complementary information between different perspectives. We also conduct theoretical analysis to assess its expressive power under ideal conditions, demonstrating the effectiveness of LoGo-GNN. Our experiments, focusing on node classification and clustering tasks, highlight the superior performance of LoGo-GNN compared to state-of-the-art methods. Additionally, robustness analysis further confirms its effectiveness in addressing uncertainty challenges.}
}
@article{IQBAL2025111028,
title = {TBConvL-Net: A hybrid deep learning architecture for robust medical image segmentation},
journal = {Pattern Recognition},
volume = {158},
pages = {111028},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111028},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007799},
author = {Shahzaib Iqbal and Tariq M. Khan and Syed S. Naqvi and Asim Naveed and Erik Meijering},
keywords = {Medical image segmentation, CNN, LSTM, Vision transformers},
abstract = {Deep learning has shown great potential for automated medical image segmentation to improve the precision and speed of disease diagnostics. However, the task presents significant difficulties due to variations in the scale, shape, texture, and contrast of the pathologies. Traditional convolutional neural network (CNN) models have certain limitations when it comes to effectively modelling multiscale context information and facilitating information interaction between skip connections across levels. To overcome these limitations, a novel deep learning architecture is introduced for medical image segmentation, which takes advantage of CNNs and vision transformers. Our proposed model, named TBConvL-Net, involves a hybrid network that combines the local features of a CNN encoder–decoder architecture with long-range and temporal dependencies using biconvolutional long-short-term memory (LSTM) networks and vision transformers (ViT). This enables the model to capture contextual channel relationships in the data and account for the uncertainty of the segmentation over time. Additionally, we introduce a novel composite loss function that considers both segmentation robustness and boundary agreement of the predicted output with the gold standard. Our proposed model shows consistent improvement over the state of the art on ten publicly available datasets of seven different medical imaging modalities.}
}
@article{QIAN2025111073,
title = {Perspective-assisted prototype-based learning for semi-supervised crowd counting},
journal = {Pattern Recognition},
volume = {158},
pages = {111073},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111073},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008240},
author = {Yifei Qian and Liangfei Zhang and Zhongliang Guo and Xiaopeng Hong and Ognjen Arandjelović and Carl R. Donovan},
keywords = {Perspective analysis, Representation learning, Task analysis, Consistency regularization},
abstract = {To alleviate the burden of labeling data to train crowd counting models, we propose a prototype-based learning approach for semi-supervised crowd counting with an embeded understanding of perspective. Our key idea is that image patches with the same density of people are likely to exhibit coherent appearance changes under similar perspective distortion, but differ significantly under varying distortions. Motivated by this observation, we construct multiple prototypes for each density level to capture variations in perspective. For labeled data, the prototype-based learning assists the regression task by regularizing the feature space and modeling the relationships within and across different density levels. For unlabeled data, the learnt perspective-embedded prototypes enhance differentiation between samples of the same density levels, allowing for a more nuanced assessment of the predictions. By incorporating regression results, we categorize unlabeled samples as reliable or unreliable, applying tailored consistency learning strategies to enhance model accuracy and generalization. Since the perspective information is often unavailable, we propose a novel pseudo-label assigner based on perspective self-organization which requires no additional annotations and assigns image regions to distinct spatial density groups, which mainly reflect the differences in average density among regions. Extensive experiments on four crowd counting benchmarks demonstrate the effectiveness of our approach.}
}
@article{JIANG2025111004,
title = {DRNet: Learning a dynamic recursion network for chaotic rain streak removal},
journal = {Pattern Recognition},
volume = {158},
pages = {111004},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111004},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007556},
author = {Zhiying Jiang and Risheng Liu and Shuzhou Yang and Zengxi Zhang and Xin Fan},
keywords = {Single image deraining, Image restoration, Neural architecture search},
abstract = {Image deraining refers to removing the visible rain streaks to restore the rain-free scenes. Existing methods rely on manually crafted networks to model the distribution of rain streaks. However, complex scenes disrupt the uniformity of rain streak characteristics assumed in ideal conditions, resulting in rain streaks of varying directions, intensities, and brightness intersecting within the same scene, challenging the deep learning based deraining performance. To address the chaotic rain streak removal, we handle the rain streaks with similar distribution characteristics in the same layer and employ a dynamic recursive mechanism to extract and unveil them progressively. Specifically, we employ neural architecture search to determine the models of different rain streaks. To avoid the loss of texture details associated with overly deep structures, we integrate multi-scale modeling and cross-scale recruitment within the dynamic structure. Considering the application of real-world scenes, we incorporate contrastive training to improve the generalization. Experimental results indicate superior performance in rain streak depiction compared to existing methods. Practical evaluation confirms its effectiveness in object detection and semantic segmentation tasks. Code is available at https://github.com/Jzy2017/DRNet.}
}
@article{PANG2025111032,
title = {QFAE: Q-Function guided Action Exploration for offline deep reinforcement learning},
journal = {Pattern Recognition},
volume = {158},
pages = {111032},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111032},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007830},
author = {Teng Pang and Guoqiang Wu and Yan Zhang and Bingzheng Wang and Yilong Yin},
keywords = {Deep reinforcement learning, Offline reinforcement learning, Policy constraints, Action exploration, D4RL},
abstract = {Offline reinforcement learning (RL) expects to get an optimal policy by utilizing offline data. During policy learning, one typical method often constrains the target policy by offline data to reduce extrapolation errors. However, it can impede the learning ability of the target policy when the provided data is suboptimal. To solve this issue, we analyze the impact of action exploration on policy learning, which implies that it can improve policy learning under a suitable action perturbation. Inspired by the theoretical analysis, we propose a simple yet effective method named Q-Function guided Action Exploration (QFAE), which solves offline RL by strengthening the exploration of behavior policy with constraint perturbation action. Moreover, it can be viewed as a plug-in-play framework that can be embedded into existing policy constraint methods to improve performance. Experimental results on the D4RL illustrate the effectiveness of our method embedded into existing approaches.}
}
@article{BAO2025110980,
title = {Preterm infant limb movement recognition with graph and convolution fusion network},
journal = {Pattern Recognition},
volume = {158},
pages = {110980},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110980},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007313},
author = {Xianfu Bao and Xiaofeng Guo and Peng Lin and Huafei Huang and Jiuwen Cao},
keywords = {Limb movement recognition, Model fusion, Graph convolution network, Multi-label classification, Premature infant},
abstract = {A continuous real-time video monitoring for preterm infants that suffer from fragile condition attracts increasing attention due to its significance in the early screening of diseases. Specially, the limb movement recognition of preterm infants in neonatal intensive care units (NICUs) plays an important role for assessing the health status of preterm infants. The main challenges of the preterm infant limb movement recognition are arisen from the irregular movement speed and the multi-limb classification. To address these issues, a novel convolution graph recognition network (CGRN) is proposed by integrating three-dimensional (3D) convolution neural networks (CNNs) with graph convolution networks (GCNs). Particularly, the spatial–temporal frequency characteristics of different channels in 3D CNN generally have poor adaptability to characterize fast limb movement. Thus, GCN is merged to enhance the channel connection and improve the capabilities of representation learning. The node filtering and topological connection are learned by GCN to establish dependencies among different channels. A fusion multi-label loss function consisting of the Binary Cross Entropy (BCE) and the Multi-label Soft-Margin (MSM) loss is further developed to train the proposed CGRN. Experiments on the preterm Infant Activity Dataset (IAD) are conducted to demonstrate the effectiveness of the proposed CGRN algorithm. The ablations studies indicate that the structure optimization process and the auxiliary performance of Laplace matrices are worked effectively, and the proposed CGRN obtains the average accuracy of 94.1%, the CF1 value of 88.4%, and the OF1 score of 91.7%.}
}
@article{GE2025111007,
title = {Rethinking the validity of perturbation in single-step adversarial training},
journal = {Pattern Recognition},
volume = {158},
pages = {111007},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111007},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007581},
author = {Yao Ge and Yun Li and Keji Han},
keywords = {Adversarial robustness, Single-step adversarial training, Trade-off, Catastrophic overfitting, Robust overfitting},
abstract = {The neural network model has the drawback of making incorrect predictions under the influence of slight adversarial perturbations. Single-step adversarial training (AT) is an effective tool to bring adversarial robustness for the model to resist such attack. From the perspective of perturbation setting in training, we identify a conflict between the pursuit of greater robustness and the need to prevent catastrophic overfitting within the AT framework. To get out of this dilemma, we delve into the impact of perturbations on human visual perception. Our analysis reveals that examples containing more misleading features should be assigned a smaller perturbation magnitude to preserve subtle yet significant features. Conversely, examples encompassing more relevant features should be assigned a larger perturbation magnitude, enabling the model to adapt to stronger attacks effectively. Motivated by these insights, we propose a concise refinement to the AT framework to unleash its full potential for single-step AT. Instead of employing a fixed perturbation magnitude, we introduce a “band” of magnitudes, allowing each example to select an appropriate magnitude based on its visual characteristic. Through extensive experiments conducted on three datasets, we demonstrate the efficacy of our proposed strategy. Our approach not only improves the model’s robustness and prevents catastrophic overfitting but also effectively mitigates robust overfitting—an issue that has remained unresolved in the context of single-step AT, marking a significant advancement in the field.}
}
@article{LI2025111080,
title = {Semantic-aware frame-event fusion based pattern recognition via large vision–language models},
journal = {Pattern Recognition},
volume = {158},
pages = {111080},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111080},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008318},
author = {Dong Li and Jiandong Jin and Yuhao Zhang and Yanlin Zhong and Yaoyang Wu and Lan Chen and Xiao Wang and Bin Luo},
keywords = {RGB-event fusion, Large vision–language models, Semantic information, Pattern recognition},
abstract = {Pattern recognition through the fusion of RGB frames and Event streams has emerged as a novel research area in recent years. Current methods typically employ backbone networks to individually extract the features of RGB frames and event streams, and subsequently fuse these features for pattern recognition. However, we posit that these methods may suffer from two key issues: (1). They attempt to directly learn a mapping from the input vision modality to the semantic labels. This approach often leads to sub-optimal results due to the disparity between the input and semantic labels; (2). They utilize small-scale backbone networks for the extraction of RGB and Event input features, thus these models fail to harness the recent performance advancements of large-scale visual-language models. In this study, we introduce a novel pattern recognition framework that consolidates the semantic labels, RGB frames, and event streams, leveraging pre-trained large-scale vision–language models. Specifically, given the input RGB frames, event streams, and all the predefined semantic labels, we employ a pre-trained large-scale vision model (CLIP vision encoder) to extract the RGB and event features. To handle the semantic labels, we initially convert them into language descriptions through prompt engineering and polish using ChatGPT, and then obtain the semantic features using the pre-trained large-scale language model (CLIP text encoder). Subsequently, we integrate the RGB/Event features and semantic features using multimodal Transformer networks. The resulting frame and event tokens are further amplified using self-attention layers. Concurrently, we propose to enhance the interactions between text tokens and RGB/Event tokens via cross-attention. Finally, we consolidate all three modalities using self-attention and feed-forward layers for recognition. Comprehensive experiments on the HARDVS and PokerEvent datasets fully substantiate the efficacy of our proposed SAFE model. The source code has been released at https://github.com/Event-AHU/SAFE_LargeVLM.}
}
@article{ZHANG2025111039,
title = {Attention redirection transformer with semantic oriented learning for unbiased scene graph generation},
journal = {Pattern Recognition},
volume = {158},
pages = {111039},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111039},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007908},
author = {Ruonan Zhang and Gaoyun An and Yigang Cen and Qiuqi Ruan},
keywords = {Scene graph generation, Transformer, Attention redirection, Translation embedding, Scene understanding},
abstract = {Scene Graph Generation (SGG) plays an important role in scene understanding because all of the objects and relations in an image can be abstracted into a concise topological graph. Due to the complexity of visual scenes, including mutual occlusion between objects and semantic ambiguity, SGG is still a challenging task. Most of the existing models only focused on the context of a single object while contexts provided by paired objects are ignored. In this paper, we propose an Attention Redirection Transformer (ART) to extract pair-level contexts specifically, which is divided into an attention distraction stage and an attention integration stage. In this way, the attention of the model is forced to be redirected, which explores the implicit information in the background. In addition, to incorporate the semantic information of predicates, a Semantic Oriented Learning Module (SOL) is designed, which may assist in getting better textual semantics and also prompts cross-modal information fusion. At last, a self-diversity driven Dual Translation Embedding Module (DTM) is designed, which refines representations of subject and object and makes them distinct. Experimental results on the Visual Genome dataset demonstrate the effectiveness of our proposed method. Moreover, our method outperforms state-of-the-art methods on the mR@K metric. The source codes are released on Github: https://github.com/Nora-Zhang98/ART-SOL.}
}
@article{ZHENG2025110999,
title = {Estimation of video sequence homography based on a first-order estimation method},
journal = {Pattern Recognition},
volume = {158},
pages = {110999},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110999},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007507},
author = {Qixuan Zheng and Muyu Li and Hong Yan},
keywords = {Homography estimation, First-order estimation method, Video analysis, Camera motion compensation, Video stabilization},
abstract = {As it is a pre-processing task, estimation of video-sequence-based homography requires low computational costs and fast evaluation. However, current algorithms for video sequence tasks are commonly based on image-pair homography and do not consider the inner properties of the video sequences. Therefore they take unnecessary computational resources. In this work, we propose a novel algorithm with a first-order estimation method to fill the gap between estimation of image-pairs and video sequence homography. By considering the continuous movement of the camera, the proposed algorithm adopts a first-order estimation to accelerate the estimation process while maintaining its robustness. Instead of extracting many image features from every frame, we demonstrate that estimating a homography matrix with pixel-based texture patterns is effective and sufficient for video sequences. Experiments show that homography estimation with simple one-dimensional texture vectors, as used in our algorithm, can surpass state-of-the-art feature-based algorithms and deep-learning-based methods. This first-order estimation method was more than 40 times faster and real-time estimation used only the CPU.}
}
@article{ZHONG2025111035,
title = {A benchmark dataset and semantics-guided detection network for spatial–temporal human actions in urban driving scenes},
journal = {Pattern Recognition},
volume = {158},
pages = {111035},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111035},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007866},
author = {Fujin Zhong and Yini Wu and Hong Yu and Guoyin Wang and Zhantao Lu},
keywords = {Spatial–temporal action detection, Urban driving scenes, Benchmark dataset, Semantic inference},
abstract = {In real urban driving scenes, human actions are very complex and have the characteristic of multiple concurrent actions. It has a great significance to detect human actions in urban traffic scenes for auxiliary or autonomous driving systems. In this view, we introduce the TITAN-Human Action dataset for the task of multi-person spatial–temporal action detection in urban driving scenes. TITAN-Human Action provides the fine-grained action labels and location coordinates for 17,574 persons in the processed frames from the TITAN dataset. Furthermore, we propose a semantics-guided detection network (SGDNet) based on a semantic inference module (SIM) for spatial–temporal human action detection in urban driving scenes. SIM encodes the category labels into sentence vectors at the semantic level with prompting and embedding, utilizes graphs to represent the directed co-occurrence relations between categories, and adopts the graph convolutional network for semantic inference. SGDNet exploits the inference results of SIM to guide the visual branch in better performing human action detection, thereby achieving the integration of visual and linguistic information. We conducted experiments to evaluate SGDNet and several baseline methods on the TITAN-Human Action dataset, and reveal the generalizability of SIM in spatial–temporal human action detection. The source code and annotation files will be available at https://github.com/yyhbswyn/SGDNet.}
}
@article{GAO2025110983,
title = {MSNet: Multi-Scale Network for Object Detection in Remote Sensing Images},
journal = {Pattern Recognition},
volume = {158},
pages = {110983},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110983},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007349},
author = {Tao Gao and Shilin Xia and Mengkun Liu and Jing Zhang and Ting Chen and Ziqi Li},
keywords = {Small object detection, Multi-scale object detection, Feature representation, Deep feature fusion},
abstract = {Remote sensing object detection (RSOD) encounters challenges in effectively extracting features of small objects in remote sensing images (RSIs). To alleviate these problems, we proposed a Multi-Scale Network for Object Detection in Remote Sensing Images (MSNet) with multi-dimension feature information. Firstly, we design a Partial and Pointwise Convolution Extraction Module (P2CEM) to capture feature of object in spatial and channel dimension simultaneously. Secondly, we design a Local and Global Information Fusion Module (LGIFM), designed local information stack and context modeling module to capture texture information and semantic information within the multi-scale feature maps respectively. Moreover, the LGIFM enhances the ability of representing features for small objects and objects within complex backgrounds by allocating weights between local and global information. Finally, we introduce Local and Global Information Fusion Pyramid (LGIFP). With the aid of the LGIFM, the LGIFP enhances the feature representation of small object information, which contributes to dense connection across the multi-scale feature maps. Extensive experiments validate that our proposed method outperforms state-of-the-art performance. Specifically, MSNet achieves mean average precision (mAP) scores of 75.3%, 93.39%, 96.00%, and 95.62% on the DIOR, HRRSD, NWPU VHR-10, and RSOD datasets, respectively.The source code and pre-trained models are currently available at https://github.com/ShailinXia/MSNet.}
}
@article{YANG2025111011,
title = {Luminance decomposition and reconstruction for high dynamic range Video Quality Assessment},
journal = {Pattern Recognition},
volume = {158},
pages = {111011},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111011},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007623},
author = {Jifan Yang and Zhongyuan Wang and Baojin Huang and Jiaxin Ai and Yuhong Yang and Jing Xiao and Zixiang Xiong},
keywords = {High dynamic range, Video quality assessment, Luminance decomposition, Feature reorganisation},
abstract = {High dynamic range (HDR) video represents a wider range of brightness, detail and colour than standard dynamic range (SDR) video. However, SDR-based VQA (Video Quality Assessment) models struggle to capture HDR distortions. In addition, some of the existing methods designed for HDR video focus on emphasising the distortion of local areas of the video frame, ignoring the distortion of the video frame as a whole. Therefore, we propose a no reference VQA model based on luminance decomposition and recombination that provides excellent performance for HDR videos, called HDR-DRVQA. Specifically, HDR-DRVQA utilises a luminance decomposition strategy to decompose video frames into different regions for explicit extraction of perceptual features in different regions of the high dynamic range. We then further propose a residual aggregation module for recombining multi-region features to extract static spatial distortion representations and dynamic motion perception (captured by feature differences). Taking advantage of the Transformer network in remote dependency modelling, this information is fed into the Transformer network for interactive learning of motion perception and adaptively constructs a stream of spatial distortion information from shallow to deep layers during temporal aggregation. We validate that our model significantly outperforms SDR VQA and existing HDR VQA methods on the publicly available HDR databases.}
}
@article{WANG2025111006,
title = {Residual k-Nearest Neighbors Label Distribution Learning},
journal = {Pattern Recognition},
volume = {158},
pages = {111006},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111006},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400757X},
author = {Jing Wang and Fu Feng and Jianhui Lv and Xin Geng},
keywords = {Label Distribution Learning (LDL), Label ambiguity, -Nearest Neighbors (NN), Generalization, Manifold, Neighborhood},
abstract = {Label Distribution Learning (LDL) is a novel learning paradigm that assigns label distribution to each instance. It aims to learn the label distribution of training instances and predict unknown ones. Algorithm Adaptation (AA)-kNN is one of the most representative LDL baselines that adapts the kNN algorithm to LDL. Its generalization risk has been proven to approach zero if given infinite training data. Despite its theoretical advantage, AA-kNN generally performs poorly because real-world LDL problems only have finite or small training data. In this paper, we improve AA-kNN and propose a novel method called Residual k-Nearest Neighbors Label Distribution Learning (RkNN-LDL). First, RkNN-LDL introduces residual label distribution learning. Second, RkNN-LDL exploits the neighborhood structure of label distribution. In theoretical analysis, we prove that RkNN-LDL has a tighter generalization bound than AA-kNN. Besides, extensive experiments validate that RkNN-LDL beats several state-of-the-art LDL methods and statistically outperforms AA-kNN.}
}
@article{ZHANG2025111052,
title = {IMCSN: An improved neighborhood aggregation interaction strategy for multi-scale contrastive Siamese networks},
journal = {Pattern Recognition},
volume = {158},
pages = {111052},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111052},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008033},
author = {Haoyu Zhang and Daoli Wang and Wangshu Zhao and Zitong Lu and Xuchu Jiang},
keywords = {Graph representation learning, Multiscale contrastive learning, Data augmentation, Neighborhood aggregation interaction, Pseudo Siamese network},
abstract = {Graph contrastive learning is an effective method for enhancing graph representation by maximizing the similarity of representations between analogous graphs while minimizing the similarity between dissimilar ones. A common approach to improve the representation capabilities of graphs involves data augmentation, which generates additional training samples through transformations applied to the original graphs. However, traditional data augmentation techniques, such as random deletion of nodes or edges, often compromise the structural integrity of graphs, result in loss of crucial information, and lead to representation instability. To overcome these drawbacks, this study introduces a novel data augmentation paradigm, the integration of directed random noise (IDR), which incorporates controlled random noise to optimize the augmentation objectives. IDR enhances the diversity and robustness of representations without sacrificing the structural integrity of the graphs. This approach significantly improves the performance of graph contrastive learning, avoiding the common issues of graph structure damage and information loss associated with conventional methods. To further refine the model, this paper proposes an improved multiscale contrastive Siamese network framework that employs three Siamese networks to process different views of input graphs. It utilizes cross-network and cross-view contrastive learning objectives to optimize graph representations, leveraging the complementary information between different views to maximize the consistency of representations among similar graphs. This enhancement improves the quality and generalizability of graph representations. Additionally, the introduction of a self-supervised loss function based on graph reconstruction within the loss framework capitalizes on the structural similarity between the original and reconstructed graphs to further refine graph representations. This loss function ensures that the global view retains more structural information from the original graph, thus enhancing the complementarity between the global and local views. The effectiveness and stability of this research framework are demonstrated through node classification tasks and visualizations on six real-world datasets, comparing favorably against existing methods such as MERIT and GraphVAT. The proposed model achieves accuracy improvements of 3.21 % and 3.71 % on the Cora dataset and 1.12 % and 1.42 % on the CiteSeer dataset. Additionally, it ranks first in an average accuracy comparison across six datasets, with scores of 80.6 % and 52.67 %. These results underscore the robustness and efficacy of the proposed model in self-supervised graph representation learning, offering substantial advancements over existing techniques.}
}
@article{XU2025111076,
title = {UPT-Flow: Multi-scale transformer-guided normalizing flow for low-light image enhancement},
journal = {Pattern Recognition},
volume = {158},
pages = {111076},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111076},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008276},
author = {Lintao Xu and Changhui Hu and Yin Hu and Xiaoyuan Jing and Ziyun Cai and Xiaobo Lu},
keywords = {Low-light image enhancement, Normalizing flow, Transformer, RGB value unbalance},
abstract = {Low-light images often suffer from information loss and RGB value degradation due to extremely low or nonuniform lighting conditions. Many existing methods primarily focus on optimizing the appearance distance between the enhanced image and the normal-light image, while neglecting the explicit modeling of information loss regions or incorrect information points in low-light images. To address this, this paper proposes an Unbalanced Points-guided multi-scale Transformer-based conditional normalizing Flow (UPT-Flow) for low-light image enhancement. We design an unbalanced point map prior based on the differences in the proportion of RGB values for each pixel in the image, which is used to modify traditional self-attention and mitigate the negative effects of areas with information distortion in the attention calculation. The Multi-Scale Transformer (MSFormer) is composed of several global-local transformer blocks, which encode rich global contextual information and local fine-grained details for conditional normalizing flow. In the invertible network of flow, we design cross-coupling conditional affine layers based on channel and spatial attention, enhancing the expressive power of a single flow step. Without bells and whistles, extensive experiments on low-light image enhancement, night traffic monitoring enhancement, low-light object detection, and nighttime image segmentation have demonstrated that our proposed method achieves state-of-the-art performance across a variety of real-world scenes. The code and pre-trained models will be available at https://github.com/NJUPT-IPR-XuLintao/UPT-Flow.}
}
@article{CHEN2025111047,
title = {GEXMERT: Geometrically enhanced cross-modality encoder representations from transformers inspired by higher-order visual percepts},
journal = {Pattern Recognition},
volume = {158},
pages = {111047},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111047},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007982},
author = {Feiyang Chen and Xue-song Tang and Kuangrong Hao},
keywords = {Bio-inspiration, Multi-modal, Visual question answering, Visual reasoning},
abstract = {Owing to the rapid progress made in transformer-based models and multi-modal fusion strategies, the performance of multi-modal networks has remarkably improved in recent years. However, intrinsic connections among individual modalities are crucial in multi-modal fusion strategies, presenting substantial opportunities for further enhancement of multi-modal perception in this field. Taking inspiration from the biological mechanisms of the human multi-modal cognitive system, we proposed a novel cross-modality fusion network. This network leverages the lateral occipital complex, which is a region responsible for object-shape perception in human vision and integrates an auxiliary modality derived using the conventional multi-stream model. This integration for learning complementary information across image and language modalities effectively captures the biological mechanisms of human processing of higher-order visual percepts through parameter updates of the intermediate modality to optimize the suitability for further fusion. To evaluate the effectiveness of our approach, we conducted experiments under various settings. Our statistical analyses demonstrate a considerable enhancement in the performance of vision-language tasks, thereby validating biological plausibility of our model.}
}
@article{TONG2025110995,
title = {Revisiting trace norm minimization for tensor Tucker completion: A direct multilinear rank learning approach},
journal = {Pattern Recognition},
volume = {158},
pages = {110995},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110995},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007465},
author = {Xueke Tong and Hancheng Zhu and Lei Cheng and Yik-Chung Wu},
keywords = {Tensor decomposition, Tucker model, Multilinear rank, Trace norm minimization},
abstract = {To efficiently express tensor data using the Tucker format, a critical task is to minimize the multilinear rank such that the model would not be over-flexible and lead to overfitting. Due to the lack of rank minimization tools in tensor, existing works connect Tucker multilinear rank minimization to trace norm minimization of matrices unfolded from the tensor data. While these formulations try to exploit the common aim of identifying the low-dimensional structure of the tensor and matrix, this paper reveals that existing trace norm-based formulations in Tucker completion are inefficient in multilinear rank minimization. We further propose a new interpretation of Tucker format such that trace norm minimization is applied to the factor matrices of the equivalent representation, rather than some matrices unfolded from tensor data. Based on the newly established problem formulation, a fixed point iteration algorithm is proposed, and its convergence is proved. Numerical results are presented to show that the proposed algorithm exhibits significant improved performance in terms of multilinear rank learning and consequently tensor signal recovery accuracy, compared to existing trace norm based Tucker completion methods.}
}
@article{ZHANG2025110970,
title = {ERAT:Eyeglasses removal with attention},
journal = {Pattern Recognition},
volume = {158},
pages = {110970},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110970},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007210},
author = {Haitao Zhang and Jingtao Guo},
keywords = {Eyeglasses removal, Facial attribute editing, Attention mechanisms},
abstract = {Eyeglasses removal has witnessed substantial progress in recent years, the key to eyeglasses removal is to accurately detect the eyeglasses and generate visually realistic new content in the eyeglasses removal region. However, current methods either cannot remove eyeglasses cleanly due to inability to accurately detect eyeglasses or fail to synthesize visually realistic eye information in the eyeglasses removal region. In this paper, we propose a new solution for high-quality eyeglasses removal that employs the attention mechanism to focus on solving two key challenges of the eyeglasses removal task. Specifically, our proposed method divides the eyeglasses removal task into two stages. The first stage learns the eyeglasses attention map by simultaneously imposing label filtering strategy and attention mask filtering strategy on both the latent feature space and image space, which mainly solves the challenges of eyeglass detection. The second stage fuses the attention map into a novel three-path network to remove eyeglasses and synthesize visually realistic content in the eyeglasses removal region. Experiments show that our method owns superior performance than almost all existing techniques on the task of eyeglasses removal.}
}
@article{YANG2025111063,
title = {Dual-perspective multi-instance embedding learning with adaptive density distribution mining},
journal = {Pattern Recognition},
volume = {158},
pages = {111063},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111063},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008148},
author = {Mei Yang and Tian-Lin Chen and Wei-Zhi Wu and Wen-Xi Zeng and Jing-Yu Zhang and Fan Min},
keywords = {Adaptive density, Dual-perspective embedding, Discriminative information, Ensemble learning, Multi-instance learning (MIL)},
abstract = {Multi-instance learning (MIL) is a potent framework for solving weakly supervised problems, with bags containing multiple instances. Various embedding methods convert each bag into a vector in the new feature space based on a representative bag or instance, aiming to extract useful information from the bag. However, since the distribution of instances is related to labels, these methods rely solely on the overall perspective embedding without considering the different distribution characteristics, which will conflate the varied distributions of instances and thus lead to poor classification performance. In this paper, we propose the dual-perspective multi-instance embedding learning with adaptive density distribution mining (DPMIL) algorithm with three new techniques. First, the mutual instance selection technique consists of adaptive density distribution mining and discriminative evaluation. The distribution characteristics of negative instances and heterogeneous instance dissimilarity are effectively exploited to obtain instances with strong representativeness. Second, the embedding technique mines two crucial information of the bag simultaneously. Bags are converted into sequence invariant vectors according to the dual-perspective such that the distinguishability is maintained. Finally, the ensemble technique trains a batch of classifiers. The final model is obtained by weighted voting with the contribution of the dual-perspective embedding information. The experimental results demonstrate that the DPMIL algorithm has higher average accuracy than other compared algorithms, especially on web datasets.}
}
@article{LIU2025111050,
title = {AdvCloak: Customized adversarial cloak for privacy protection},
journal = {Pattern Recognition},
volume = {158},
pages = {111050},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111050},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400801X},
author = {Xuannan Liu and Yaoyao Zhong and Xing Cui and Yuhang Zhang and Peipei Li and Weihong Deng},
keywords = {Facial privacy enhancement, Class-wise adversarial example, Two-stage training strategy, Identity feature subspace analysis},
abstract = {With extensive face images being shared on social media, there has been a notable escalation in privacy concerns. In this paper, we propose AdvCloak, an innovative framework for privacy protection using generative models. AdvCloak is designed to automatically customize class-wise adversarial masks that can maintain superior image-level naturalness while providing enhanced feature-level generalization ability. Specifically, AdvCloak sequentially optimizes the generative adversarial networks by employing a two-stage training strategy. This strategy initially focuses on adapting the masks to the unique individual faces and then enhances their feature-level generalization ability to diverse facial variations of individuals. To fully utilize the limited training data, we combine AdvCloak with several general geometric modeling methods, to better describe the feature subspace of source identities. Extensive quantitative and qualitative evaluations on both common and celebrity datasets demonstrate that AdvCloak outperforms existing state-of-the-art methods in terms of efficiency and effectiveness. The code is available at https://github.com/liuxuannan/AdvCloak.}
}
@article{CHEN2025111079,
title = {Fine-grained Automatic Augmentation for handwritten character recognition},
journal = {Pattern Recognition},
volume = {159},
pages = {111079},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111079},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008306},
author = {Wei Chen and Xiangdong Su and Hongxu Hou},
keywords = {Data automatic augmentation, Handwritten character recognition, Bézier curve, Bayesian optimization},
abstract = {With the advancement of deep learning-based character recognition models, the training data size has become a crucial factor in improving the performance of handwritten text recognition. For languages with low-resource handwriting samples, data augmentation methods can effectively scale up the data size and improve the performance of handwriting recognition models. However, existing data augmentation methods for handwritten text face two limitations: (1) Methods based on global spatial transformations typically augment the training data by transforming each word sample as a whole but ignore the potential to generate fine-grained transformation from local word areas, limiting the diversity of the generated samples; (2) It is challenging to adaptively choose a reasonable augmentation parameter when applying these methods to different language datasets. To address these issues, this paper proposes Fine-grained Automatic Augmentation (FgAA) for handwritten character recognition. Specifically, FgAA views each word sample as composed of multiple strokes and achieves data augmentation by performing fine-grained transformations on the strokes. Each word is automatically segmented into various strokes, and each stroke is fitted with a Bézier curve. On such a basis, we define the augmentation policy related to the fine-grained transformation and use Bayesian optimization to select the optimal augmentation policy automatically, thereby achieving the automatic augmentation of handwriting samples. Experiments on seven handwriting datasets of different languages demonstrate that FgAA achieves the best augmentation effect for handwritten character recognition. Our code is available at https://github.com/IMU-MachineLearningSXD/Fine-grained-Automatic-Augmentation}
}
@article{ALAM2025111009,
title = {Online kernel-based clustering},
journal = {Pattern Recognition},
volume = {158},
pages = {111009},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111009},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400760X},
author = {Abrar Alam and Akshay Malhotra and Ioannis D. Schizas},
keywords = {Kernel learning, Online algorithms, Clustering, Optimization},
abstract = {A novel online joint kernel learning and clustering (OKC) framework is derived which is capable of determining time-varying clustering configurations without the need for training data. To facilitate clustering via sparse kernel factorization, a novel time-dependent metric is devised to quantify closeness of candidate kernel similarity matrices to a block diagonal structure. The task is later performed online as newly acquired data are processed making it appropriate for non-stationary settings. The necessary minimization to carry out kernel learning and clustering is performed via an effective interplay among block coordinate descent, difference of convex functions minimization and projected subgradient descent which results in an online iterative algorithm that updates online the kernel covariance matrix, as well as the associated clustering configuration. The resulting recursive kernel updates are proved to converge to a bounded limit. Detailed numerical examples utilizing both synthetic and real data show the superior clustering accuracy achieved by the novel approach over existing alternatives while requiring considerably less running time.}
}
@article{PERNUS2025111022,
title = {FICE: Text-conditioned fashion-image editing with guided GAN inversion},
journal = {Pattern Recognition},
volume = {158},
pages = {111022},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111022},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007738},
author = {Martin Pernuš and Clinton Fookes and Vitomir Štruc and Simon Dobrišek},
keywords = {Generative adversarial networks, Image editing, Deep learning, Multimodality},
abstract = {Fashion-image editing is a challenging computer-vision task where the goal is to incorporate selected apparel into a given input image. Most existing techniques, known as Virtual Try-On methods, deal with this task by first selecting an example image of the desired apparel and then transferring the clothing onto the target person. Conversely, in this paper, we consider editing fashion images with text descriptions. Such an approach has several advantages over example-based virtual try-on techniques: (i) it does not require an image of the target fashion item, and (ii) it allows the expression of a wide variety of visual concepts through the use of natural language. Existing image-editing methods that work with language inputs are heavily constrained by their requirement for training sets with rich attribute annotations or they are only able to handle simple text descriptions. We address these constraints by proposing a novel text-conditioned editing model called FICE (Fashion Image CLIP Editing) that is capable of handling a wide variety of diverse text descriptions to guide the editing procedure. Specifically, with FICE, we extend the common GAN-inversion process by including semantic, pose-related, and image-level constraints when generating images. We leverage the capabilities of the CLIP model to enforce the text-provided semantics, due to its impressive image–text association capabilities. We furthermore propose a latent-code regularization technique that provides the means to better control the fidelity of the synthesized images. We validate the FICE through rigorous experiments on a combination of VITON images and Fashion-Gen text descriptions and in comparison with several state-of-the-art, text-conditioned, image-editing approaches. Experimental results demonstrate that the FICE generates very realistic fashion images and leads to better editing than existing, competing approaches. The source code is publicly available from: https://github.com/MartinPernus/FICE.}
}
@article{PAN2025111034,
title = {Three-way decision-based label integration for crowdsourcing},
journal = {Pattern Recognition},
volume = {158},
pages = {111034},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111034},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007854},
author = {Can Pan and Liangxiao Jiang and Chaoqun Li},
keywords = {Crowdsourcing, Label integration, Three-way decision},
abstract = {In crowdsourcing learning, label integration is often used to infer instances’ integrated labels from their multiple noisy labels. However, almost all existing label integration algorithms apply the same strategy to infer different instances’ integrated labels, which limits their performance. This paper argues that different instances should enjoy different label integration strategies alone. Thanks to the three-way decision theory, a three-way decision-based label integration (TDLI) algorithm is proposed. In TDLI, we at first evaluate the label qualities of each instance and its K-nearest neighbors, and then utilize them to divide the whole crowdsourced datasets into three disjoint subsets, called positive set, boundary set and negative set, respectively. For each instance in the positive set, we directly apply the simplest majority voting (MV) to infer its integrated label. For each instance in the boundary set, we absorb its K-nearest neighbors’ multiple noisy labels to infer its integrated label by the weighted MV. For each instance in the negative set, we merge the positive and boundary sets to train a classifier to infer its integrated label by fusing its own multiple noisy label distribution and the predicted label distribution. Extensive experiments demonstrate that TDLI distinctly outperforms all the other existing label integration algorithms used to compare.}
}
@article{YANG2025110982,
title = {Webly supervised 3D shape recognition},
journal = {Pattern Recognition},
volume = {158},
pages = {110982},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110982},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007337},
author = {Xizhong Yang and Qi Guo and Wenbin Chen and Mofei Song},
keywords = {3D shape dataset, 3D shape recognition, Webly supervised learning},
abstract = {Recently, deep learning has become the most promising method for 3D shape recognition. However, it is expensive to obtain large-scale accurately labeled 3D datasets required for deep learning. To reduce the cost, this paper presents a study on learning 3D recognition models from 3D noisy web data. To realize the method, we crawl 3D data from the Internet to construct a new 3D shape dataset called 3DWebNet40, and propose a novel webly supervised 3D shape recognition method handling label noise and background noise simultaneously. To alleviate the downside of background noise, we utilize the region-attention module to enhance the representative local feature of the view. For label noise, we combine label selection and feature selection to jointly select training data, and train the 3D shape classifier on the selected subsets with different regularization strategies to reduce the interference of label noise. Extensive experiments on 3DWebNet40 demonstrate the effectiveness of our method11Code and datasets are publicly available online at https://github.com/yxizhong...}
}
@article{CUI2025111010,
title = {Layer-wise normalized deep incomplete multiview nonnegative matrix factorization},
journal = {Pattern Recognition},
volume = {158},
pages = {111010},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111010},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007611},
author = {Guosheng Cui and Dan Wu and Ye Li and Jianzhong Li},
keywords = {Layer-wise normalization, Model stableness, Incomplete multiview clustering, Hypergraph regularization},
abstract = {Deep (semi-)nonnegative matrix factorization based models have demonstrated promising performance in the multiview clustering tasks. However, they are all developed upon the assumption that the multiview datasets are complete in all views, and need to be pre-trained layer-by-layer to get reasonable results. Addressing these issues, an algorithm named layer-wise normalized deep incomplete multiview nonnegative matrix factorization (LWNdimNMF) is presented in this paper to tackle the multiview clustering problem with arbitrary missing views. Specifically, a novel layer-wise normalization strategy is developed to constrain the value distribution of the factor matrix in each layer to ensure the numerical stability of the model and make it robust to the initialization. Another benefit of this strategy is that it can effectively depress the objective function value and make the model fit data well. An instance selection alignment is adopted to fuse the incomplete multiple views by aligning the paired instances. To discover the high-order geometrical information in each incomplete view, a hypergraph regularization is incorporated into the proposed model. An effective iterative optimization algorithm based on the multiplicative updating rules is designed to solve the minimizing problem. Extensive experiments are conducted to evaluate the effectiveness of the proposed model by comparing with several state-of-the-art (SOTA) incomplete multiview clustering (IMC) methods on ten real-world multiview datasets. The source code is available at: https://github.com/GuoshengCui/LWNdimNMF.}
}
@article{LIANG2025111051,
title = {Image dehazing via self-supervised depth guidance},
journal = {Pattern Recognition},
volume = {158},
pages = {111051},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111051},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008021},
author = {Yudong Liang and Shaoji Li and De Cheng and Wenjian Wang and Deyu Li and Jiye Liang},
keywords = {Image dehazing, Self-supervised, Depth guidance, Transformer, Hybrid attention},
abstract = {Self-supervised learning methods have demonstrated promising benefits to feature representation learning for image dehazing tasks, especially for avoiding the laborious work of collecting hazy-clean image pairs, while also enabling better generalization abilities of the model. Despite the long-standing interests in depth estimation for image dehazing tasks, few works have fully explored the interactions between depth and dehazing tasks in an unsupervised manner. In this paper, a self-supervised image dehazing framework under the guidance of self-supervised depth estimation has been proposed, to fully exploit the interactions between depth and hazes for image dehazing. Specifically, the hazy image and the corresponding depth estimation are generated and optimized from the clear image in a dual-network self-supervised manner. The correlations between depth and hazy images are exploited in depth-guided hybrid attention Transformer blocks, which adaptively leverage both the cross-attention and self-attention to effectively model hazy densities via cross-modality fusion and capture global context information for better feature representations. In addition, the depth estimations of hazy images are further explored for the detection tasks on hazy images. Extensive experiments demonstrate that the depth estimation not only enhances the model generalization ability across different dehazing datasets, leading to state-of-the-art self-supervised dehazing performance, but also benefits downstream detection tasks on hazy images. Our code is available at https://github.com/DongLiangSXU/Depth-Guidance-dehazing.git.}
}
@article{SUN2025111075,
title = {CHA: Conditional Hyper-Adapter method for detecting human–object interaction},
journal = {Pattern Recognition},
volume = {159},
pages = {111075},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111075},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008264},
author = {Mengyang Sun and Wei Suo and Ji Wang and Peng Wang and Yanning Zhang},
keywords = {Human–object interaction detection, Hypernetwork, Meta learning},
abstract = {Human–object interactions (HOI) detection aims at capturing human–object pairs in images and predicting their actions. It is an essential step for many visual reasoning tasks, such as VQA, image retrieval and surveillance event detection. The challenge of this task is to tackle the compositional learning problem, especially in a few-shot setting. A straightforward approach is designing a group of dedicated models for each specific pair. However, the maintenance of these independent models is unrealistic due to combinatorial explosion. To address the above problems, we propose a new Conditional Hyper-Adapter (CHA) method based on meta-learning. Different from previous works, our approach regards each <verb, object> as an independent sub-task. Meanwhile, we design two kinds of Hyper-Adapter structures to guide the model to learn “how to address the HOI detection”. By combining the different conditions and hypernetwork, the CHA can adaptively generate partial parameters and improve the representation and generalization ability of the model. Finally, our proposed method can be viewed as a plug-and-play module to boost existing HOI detection models on the widely used HOI benchmarks.}
}
@article{WANG2025111046,
title = {Marginal debiased network for fair visual recognition},
journal = {Pattern Recognition},
volume = {158},
pages = {111046},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111046},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007970},
author = {Mei Wang and Weihong Deng and Jiani Hu and Sen Su},
keywords = {Bias, Fairness, Margin penalty, Meta learning},
abstract = {Deep neural networks (DNNs) are often prone to learn the spurious correlations between target classes and bias attributes, like gender and race, inherent in a major portion of training data (bias-aligned samples), thus showing unfair behavior and arising controversy in the modern pluralistic and egalitarian society. In this paper, we propose a novel marginal debiased network (MDN) to learn debiased representations. More specifically, a marginal softmax loss (MSL) is designed by introducing the idea of margin penalty into the fairness problem, which assigns a larger margin for bias-conflicting samples (data without spurious correlations) than for bias-aligned ones, so as to deemphasize the spurious correlations and improve generalization on unbiased test criteria. To determine the margins, our MDN is optimized through a meta learning framework. We propose a meta equalized loss (MEL) to perceive the model fairness, and adaptively update the margin parameters by meta-optimization which requires the trained model guided by the optimal margins should minimize MEL computed on an unbiased meta-validation set. Extensive experiments on BiasedMNIST, Corrupted CIFAR-10, CelebA and UTK-Face datasets demonstrate that our MDN can achieve a remarkable performance on under-represented samples and obtain superior debiased results against the previous approaches.}
}
@article{ZHAO2025110994,
title = {TFformer: A time–frequency domain bidirectional sequence-level attention based transformer for interpretable long-term sequence forecasting},
journal = {Pattern Recognition},
volume = {158},
pages = {110994},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110994},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007453},
author = {Tianlong Zhao and Lexin Fang and Xiang Ma and Xuemei Li and Caiming Zhang},
keywords = {Long-term forecasting, Interpretable forecasting, Frequency decomposition, Sequential attention, Periodic extension},
abstract = {Transformer methods have shown strong predictive performance in long-term time series prediction. However, its attention mechanism destroys temporal dependence and has quadratic complexity. This makes prediction processes difficult to interpret, limiting their application in tasks requiring interpretability. To address this issue, this paper proposes a highly interpretable long-term sequence forecasting model, TFformer. TFformer decomposes time series into low frequency trend component and high frequency period component by frequency decomposition, and forecasts them respectively. The periodic information in high-frequency component is enhanced with the sequential frequency attention, and then the temporal patterns of the two components are obtained by feature extraction. According to the period property in time domain, TFformer through periodic extension to predict the future period patterns using sequential periodic matching attention. Finally, the predicted future period pattern and the extracted trend pattern are reconstructed to future series. TFformer provides an interpretable forecasting process with low time complexity, as it retains temporal dependence using sequence-level attentions. TFformer achieves significant prediction performance in both univariate and multivariate forecasting across six datasets. Detailed experimental results and analyses verify the effectiveness and generalization of TFformer.}
}