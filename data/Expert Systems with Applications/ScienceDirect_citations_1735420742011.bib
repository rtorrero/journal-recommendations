@article{LIU2024110658,
title = {DH-GAN: Image manipulation localization via a dual homology-aware generative adversarial network},
journal = {Pattern Recognition},
volume = {155},
pages = {110658},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110658},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004096},
author = {Weihuang Liu and Xiaodong Cun and Chi-Man Pun},
keywords = {Image manipulation localization, Selective atrous pyramid, Adversarial learning, Masked convolution},
abstract = {Image manipulation localization is a binary segmentation task that sensitive to the tampered artifacts other than awareness of the object. Thus, both traditional and learning-based methods highly rely on hand-crafted features. However, these specifically-defined features limit the ability of the network for general scenes. To tackle this problem, we propose a dual homology-aware generative adversarial network (DH-GAN), a novel GAN-based framework to localize the manipulated region. Firstly, we localize the forgery region via re-calibrating the multi-scale encoded features with a selective pyramid generator. Then, we perform the homology identification in the discriminator. The proposed homology-aware discriminators contain a stack of masked convolution (MConv) layers and learn to identify the real/fake of the segmented pixels on the predicted/target masked image in a hard-gating manner. Overall, the networks are optimized under a standard GAN. Experiments show that the proposed method outperforms other state-of-the-art algorithms on four popular image manipulation datasets.}
}
@article{YAN2024110755,
title = {FeverNet: Enabling accurate and robust remote fever screening},
journal = {Pattern Recognition},
volume = {156},
pages = {110755},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110755},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005065},
author = {Mengkai Yan and Jianjun Qian and Hang Shao and Lei Luo and Jian Yang},
keywords = {Fever screening, Thermal infrared image, Neural network, Public health surveillance},
abstract = {Remote human fever screening via thermal infrared imaging helps reduce the risk of respiratory disease transmission and plays an important role in public health monitoring. However, the accuracy of such systems often falls prey to variations in measurement distance and environment temperature. Most previous methods tend to employ sensors to overcome these variations, which are expensive schemes and have limited performance improvement. To address above problems, this paper presents a novel and robust remote fever screening framework named FeverNet. Specifically, FeverNet introduces depth estimation network and temperature distribution constraints across time periods to reduce the influence of distance variations and environment temperature changes. The fever attention module is thus proposed to enhance feature representation and expand the difference between fever faces and normal ones. In addition, we provide the Extended Thermal Infrared Face dataset (ETIF), which further gives visible images (paired with thermal infrared images) for depth estimation and improve the fever face generated method based on the maximum temperature of the face. Extensive experiments on ETIF demonstrate the advantages of our FeverNet over the state-of-the-art methods.}
}
@article{PAN2024110686,
title = {Pseudo-set Frequency Refinement architecture for fine-grained few-shot class-incremental learning},
journal = {Pattern Recognition},
volume = {155},
pages = {110686},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110686},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004370},
author = {Zicheng Pan and Weichuan Zhang and Xiaohan Yu and Miaohua Zhang and Yongsheng Gao},
keywords = {Few-shot class-incremental learning, Fine-grained classification, Frequency analysis, Feature space optimization},
abstract = {Few-shot class-incremental learning was introduced to solve the model adaptation problem for new incremental classes with only a few examples while still remaining effective for old data. Although recent state-of-the-art methods make some progress in improving system robustness on common datasets, they fail to work on fine-grained datasets where inter-class differences are small. The problem is mainly caused by: (1) the overlapping of new data and old data in the feature space during incremental learning, which means old samples can be falsely classified as newly introduced classes and induce catastrophic forgetting phenomena; (2) lacking discriminative feature learning ability to identify fine-grained objects. In this paper, a novel Pseudo-set Frequency Refinement (PFR) architecture is proposed to tackle these problems. We design a pseudo-set training strategy to mimic the incremental learning scenarios so that the model can better adapt to novel data in future incremental sessions. Furthermore, separate adaptation tasks are developed by utilizing frequency-based information to refine the original features and address the above challenging problems. More specifically, the high and low-frequency components of the images are employed to enrich the discriminative feature analysis ability and incremental learning ability of the model respectively. The refined features are used to perform inter-class and inter-set analyses. Extensive experiments show that the proposed method consistently outperforms the state-of-the-art methods on four fine-grained datasets.}
}
@article{YANG2024110741,
title = {CL-TransFER: Collaborative learning based transformer for facial expression recognition with masked reconstruction},
journal = {Pattern Recognition},
volume = {156},
pages = {110741},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110741},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004928},
author = {Yujie Yang and Lin Hu and Chen Zu and Jianjia Zhang and Yun Hou and Ying Chen and Jiliu Zhou and Luping Zhou and Yan Wang},
keywords = {Facial expression recognition, Representation learning, Collaborative learning, Noisy annotations},
abstract = {Facial expression recognition (FER) has attracted intensive attention due to its critical role in various computer vision tasks. However, existing FER approaches suffer from either noisy annotations or expression ambiguity (high inter-class and low intra-class similarity), limiting the FER performance. To this end, we propose a robust end-to-end collaborative learning based transformer for FER (CL-TransFER) in this paper. Specifically, CL-TransFER co-trains a CNN feature extractor and a transformer feature extractor jointly to extract both rich local semantic features as well as global structural information from facial images. By enforcing the consensus between the predictions of two extractors, the CL-TransFER could suppress the influence of noisy annotations. To further tackle the expression ambiguity problem, we design a simple yet efficient self-supervised masked reconstruction (SSMR) task to pre-train the transformer feature extractor of CL-TransFER. This enhances the model's capability of learning fine-grained discriminative representations. Extensive experiments on three popular benchmarks have demonstrated the effectiveness and superiority of our method.}
}
@article{ZHAO2024110702,
title = {M3Net: Movement Enhancement with Multi-Relation toward Multi-Scale video representation for Temporal Action Detection},
journal = {Pattern Recognition},
volume = {155},
pages = {110702},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110702},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004539},
author = {Zixuan Zhao and Dongqi Wang and Xu Zhao},
keywords = {Temporal action detection, Temporal action proposal, Video understanding, Feature representation, Multi-scale detection},
abstract = {Locating boundary is very important for Temporal Action Detection (TAD) and is a key factor affecting the performance of TAD. However, two factors lead to inaccurate boundary localization: the movement feature submergence and the existence of multi-scale actions. In this work, to address the submergence of movement feature, we design the Movement Enhance Module (MEM), in which the Movement Feature Extractor (MFE) and Multi-Relation Module (MRM) are used to highlight short-term and long-term movement information respectively. To address the characteristic of multi-scale actions, we propose a Scale Feature Pyramid Network (SFPN) to detect multi-scale actions and design a two-stage training strategy that makes each layer focus on a specific scale action. These tow modules are integrated as M3Net, and extensive experiments demonstrate its effectiveness. M3Net outperforms other representative TAD methods on ActivityNet-1.3 and THUMOS-14.}
}
@article{SONG2024110725,
title = {Semantic-embedded similarity prototype for scene recognition},
journal = {Pattern Recognition},
volume = {155},
pages = {110725},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110725},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400476X},
author = {Chuanxin Song and Hanbo Wu and Xin Ma and Yibin Li},
keywords = {Scene recognition, Similarity prototype, Semantic knowledge, Label softening, Contrastive loss},
abstract = {Due to the high inter-class similarity caused by the complex composition and the co-existing objects across scenes, numerous studies have explored object semantic knowledge within scenes to improve scene recognition. However, a resulting challenge emerges as object information extraction techniques require heavy computational costs, thereby burdening the network considerably. This limitation often renders object-assisted approaches incompatible with edge devices in practical deployment. In contrast, this paper proposes a semantic knowledge-based similarity prototype, which can help the scene recognition network achieve superior accuracy without increasing the computational cost in practice. It is simple and can be plug-and-played into existing pipelines. More specifically, a statistical strategy is introduced to depict semantic knowledge in scenes as class-level semantic representations. These representations are used to explore correlations between scene classes, ultimately constructing a similarity prototype. Furthermore, we propose to leverage the similarity prototype to support network training from the perspective of Gradient Label Softening and Batch-level Contrastive Loss, respectively. Comprehensive evaluations on multiple benchmarks show that our similarity prototype enhances the performance of existing networks, all while avoiding any additional computational burden in practical deployments. Code and the statistical similarity prototype will be available at https://github.com/ChuanxinSong/SimilarityPrototype.}
}
@article{HO2024110730,
title = {GLNAS: Greedy Layer-wise Network Architecture Search for low cost and fast network generation},
journal = {Pattern Recognition},
volume = {155},
pages = {110730},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110730},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004813},
author = {Jiacang Ho and Kyongseok Park and Dae-Ki Kang},
keywords = {Automated machine learning, Greedy layer-wise, Network architecture search, Greedy Layer-wise Network Architecture Search, Image classification},
abstract = {The process of applying machine learning algorithms to practical problems can be a challenging and tedious task for non-experts. Previous research has sought to alleviate this burden by introducing automated machine learning techniques, including Network Architecture Search (NAS) and Differentiable Architecture Search (DARTS). However, these methods use a fixed number of layers and predefined skip connections which impose limitations on the generation of an optimal network architecture. In this paper, we propose a novel approach called Greedy Layer-wise Network Architecture Search (GLNAS), which trains network layers one after another and evaluates the network’s performance after each layer is added. GLNAS also assesses the effectiveness of skip connections between layers by testing various outputs of previous layers as an input to the current layer. Our experiment results demonstrate that the network generated by GLNAS requires fewer parameters (i.e., 3.5 millions in both CIFAR-10 and CIFAR-100 datasets) and GPU resources during the searching phase (i.e., 0.17 and 0.24 GPU days in CIFAR-10 and CIFAR-100 datasets respectively) than many existing methods.}
}
@article{WANG2024110738,
title = {Heterophily-aware graph attention network},
journal = {Pattern Recognition},
volume = {156},
pages = {110738},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110738},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004898},
author = {Junfu Wang and Yuanfang Guo and Liang Yang and Yunhong Wang},
keywords = {Graph neural networks, Graph attention networks, Network with heterophily},
abstract = {Graph Neural Networks (GNNs) have shown remarkable success in graph representation learning. Unfortunately, current weight assignment schemes in standard GNNs, such as the calculation based on node degrees or pair-wise representations, can hardly be effective in processing the networks with heterophily, in which the connected nodes usually possess different labels or features. Existing heterophilic GNNs tend to ignore the modeling of heterophily of each edge, which is also a vital part in tackling the heterophily problem. In this paper, we first propose a heterophily-aware attention scheme and reveal the benefits of modeling the edge heterophily, i.e., if a GNN assigns different weights to edges according to different heterophilic types, it can learn effective local attention patterns, enabling nodes to acquire appropriate information from distinct neighbors. Then, we propose a novel Heterophily-Aware Graph Attention Network (HA-GAT) by fully exploring and utilizing the local distribution as the underlying heterophily, to handle the networks with different homophily ratios. To demonstrate the effectiveness of the proposed HA-GAT, we analyze the proposed heterophily-aware attention scheme and local distribution exploration, by seeking an interpretation from their mechanism. Extensive results demonstrate that our HA-GAT achieves state-of-the-art performances on eight datasets with different homophily ratios in both the supervised and semi-supervised node classification tasks.}
}
@article{LI2024110690,
title = {Graph Attentive Dual Ensemble learning for Unsupervised Domain Adaptation on point clouds},
journal = {Pattern Recognition},
volume = {155},
pages = {110690},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110690},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004412},
author = {Qing Li and Chuan Yan and Qi Hao and Xiaojiang Peng and Li Liu},
keywords = {Point clouds, Unsupervised Domain Adaptation, Dual ensemble learning, Graph attentive module, Pseudo labels},
abstract = {Due to the annotation difficulty of point clouds, Unsupervised Domain Adaptation (UDA) is a promising direction to address unlabeled point cloud classification and segmentation. Recent works show that adding a self-supervised learning branch for target domain training consistently boosts UDA point cloud tasks. However, most of these works simply resort to geometric deformation, which ignores semantic information and is hard to bridge the domain gap. In this paper, we propose a novel self-learning strategy for UDA on point clouds, termed as Graph Attentive Dual Ensemble learning (GRADE), which delivers semantic information directly. Specifically, with a pre-training process on the source domain, GRADE further builds dual collaborative training branches on the target domain, where each of them constructs a temporal average teacher model and distills its pseudo labels to the other branch. To achieve faithful labels from each teacher model, we improve the popular DGCNN architecture by introducing a dynamic graph attentive module to mine the relation between local neighborhood points. We conduct extensive experiments on several UDA point cloud benchmarks, and the results demonstrate that our GRADE method outperforms the state-of-the-art methods on both classification and segmentation tasks with clear margins.}
}
@article{YU2024110714,
title = {YOLO-FaceV2: A scale and occlusion aware face detector},
journal = {Pattern Recognition},
volume = {155},
pages = {110714},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110714},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004655},
author = {Ziping Yu and Hongbo Huang and Weijun Chen and Yongxin Su and Yahui Liu and Xiuying Wang},
keywords = {Face detection, YOLO, Scale-aware, Occlusion, Imbalance problem},
abstract = {In recent years, face detection algorithms based on deep learning have made great progress. Nevertheless, the effective utilization of face detectors for small and occlusion faces remains challenging, primarily stemming from the limitations in pixel information and the presence of missing features. In this paper, we propose a novel real-time face detector, YOLO-FaceV2, built upon the YOLOv5 architecture. Our approach introduces a Receptive Field Enhancement (RFE) module designed to extract multi-scale pixel information and augment the receptive field for accurately detecting small faces. To address issues related to face occlusion, we introduce an attention mechanism termed the Separated and Enhancement Attention Module (SEAM), which effectively focuses on the regions affected by occlusion. Furthermore, we propose a Slide Weight Function (SWF) to mitigate the imbalance between easy and hard samples. The experiments demonstrate that our YOLO-FaceV2 achieves performance exceeding the state-of-the-art on the WiderFace validation dataset. Source code and pre-trained model are available at https://github.com/Krasjet-Yu/YOLO-FaceV2.}
}
@article{JI2024110726,
title = {A novel hybrid decoding neural network for EEG signal representation},
journal = {Pattern Recognition},
volume = {155},
pages = {110726},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110726},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004771},
author = {Youshuo Ji and Fu Li and Boxun Fu and Yijin Zhou and Hao Wu and Yang Li and Xiaoli Li and Guangming Shi},
keywords = {EEG representation, Hybrid decoding network, Depthwise separable convolution, Multi-head attention, Spatial-temporal information},
abstract = {In this paper, we proposed a novel hybrid decoding model that combines the superiority of CNNs and multi-head self-attention mechanisms, called HCANN, to finely characterizing EEG features. Depthwise separable convolution with multi-scale factors efficiently decouples temporal relevant information between brain-computer interface (BCI) tasks and EEG signals. Multi-head mechanism adaptively modified for EEG focuses on brain spatial activation patterns and extracts complementary spatial representation information from multiple subspaces. The proposed HCANN decodes the intent information of EEG recorded by three BCI paradigms, including one active and two passive BCI paradigms: rapid serial visual presentation, motor imagery, and imagined speech. We evaluated HCANN by comparing with the current state-of-the-art methods. The experimental results demonstrated that HCANN can effectively decode EEG and improves classification performance for all three BCI tasks. In addition, the visualization of spatial-temporal features at different decoding stages demonstrated that the proposed HCANN gradually extracts effective features related to the BCI tasks. The code of HCANN is publicly available at https://github.com/youshuoji/HCANN.}
}
@article{KAR2024110674,
title = {EDMD: An Entropy based Dissimilarity measure to cluster Mixed-categorical Data},
journal = {Pattern Recognition},
volume = {155},
pages = {110674},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110674},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004254},
author = {Amit Kumar Kar and Mohammad Maksood Akhter and Amaresh Chandra Mishra and Sraban Kumar Mohanty},
keywords = {Proximity measure, Mixed categorical data, Ordinal attributes, Nominal attributes, Entropy, Dissimilarity measure},
abstract = {The effectiveness of clustering techniques is significantly influenced by proximity measures irrespective of type of data and categorical data is no exception. Most of the existing proximity measures for categorical data assume that all attributes contribute equally to the distance measurement which is not true. Usually, frequency or probability-based approaches are better equipped in principle to counter this issue by appropriately weighting the attributes based on the intra-attribute statistical information. However, owing to the qualitative nature of categorical features, the intra-attribute disorder is not captured effectively by the popularly used continuum form of entropy known as Shannon or information entropy. If the categorical data contains ordinal features, then the problem multiplies because the existing measures treat all attributes as nominal. To address these issues, we propose a new Entropy-based Dissimilarity measure for Mixed categorical Data (EDMD) composed of both nominal and ordinal attributes. EDMD treats both nominal and ordinal attributes separately to capture the intrinsic information from the values of two different attribute types. We apply Boltzmann’s definition of entropy, which is based on the principle of counting microstates, to exploit the intra-attribute statistical information of nominal attributes while preserving the order relationships among ordinal values in distance formulation. Additionally, the statistical significance of different attributes of the data towards dissimilarity computation is taken care of through attribute weighting. The proposed measure is free from any user-defined or domain-specific parameters and there is no prior assumption about the distribution of the data sets. Experimental results demonstrate the efficacy of EDMD in terms of cluster quality, accuracy, cluster discrimination ability, and execution time to handle mixed categorical data sets of different characteristics.}
}
@article{ZHANG2024110698,
title = {FMGNet: An efficient feature-multiplex group network for real-time vision task},
journal = {Pattern Recognition},
volume = {156},
pages = {110698},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110698},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004497},
author = {Hao Zhang and Yongqiang Ma and Kaipeng Zhang and Nanning Zheng and Shenqi Lai},
keywords = {Lightweight networks, Image classification, Object detection, Human pose estimation, Person re-identification, Semantic segmentation},
abstract = {Lightweight network design is crucial for optimizing speed and accuracy in computer vision tasks on mobile platforms with limited resources. Widely adopted models, such as EfficientNet and RegNet have achieved significant success by integrating key elements like Pointwise Convolutions (PWConvs) and Squeeze-and-Excitation (SE) blocks. However, a notable observation is that the output feature of the PWConv closely resembles its input, particularly in the absence of an activation function. This similarity and redundancy lead to wasted computational complexity and adversely affect the inference speed. To address these issues, we propose an efficient lightweight network called Efficient Feature-Multiplex Group Network (FMGNet). FMGNet is composed of two key components: the Cross-layer Feature-multiplex Group (CFG) block and the CFG-aligned Cross-layer Attention (CCA) block. The CFG block enables more compact feature learning with fewer parameters by multiplexing the input features of the PWConv. Meanwhile, the CCA block leverages the pre-modified features derived from the CFG block’s PWConv, allowing for simultaneous and parallel channel attention modeling. Our extensive experiments across various tasks, including image classification (ImageNet), object detection (PASCAL VOC), human pose estimation (MPII), person re-identification (Market-1501, DukeMTMC-ReID, CUHK03-NP), and semantic segmentation (Cityscapes), indicate that FMGNet achieves comparable performance to state-of-the-art lightweight convolutional neural networks, offering faster inference times. Remarkably, FMGNet even surpasses recent transformer-based models, such as SwiftFormer and EfficientFormerV2, achieving superior results with lower inference latency.}
}
@article{MA2024110657,
title = {GoogLeNet-AL: A fully automated adaptive model for lung cancer detection},
journal = {Pattern Recognition},
volume = {155},
pages = {110657},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110657},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004084},
author = {Lei Ma and Huiqun Wu and P. Samundeeswari},
keywords = {Lung cancer classification, Deep learning, Convolutional neural networks, GoogLeNet-AL, Adaptive layers, Medical image analysis},
abstract = {As lung cancer has emerged as the top contributor to cancer-related fatalities, efficient and precise diagnostic methods are essential for efficient diagnosis. This research introduces a novel CNN architecture GoogLeNet with Adaptive Layers (GoogLeNet-AL) for lung cancer detection. The GoogLeNet-AL architecture integrates innovative features such as squeeze-and-excitation blocks, dilated convolutions, depthwise separable convolutions, group convolutions, non-local blocks, octave convolutions, inverted Residuals, and ghost convolutions in the inception layers to boost the potential of GoogLeNet-AL to capture multi-scale features efficiently. The GoogLeNet-AL model has been implemented in the PyTorch 1.8.1 platform and trained using publicly accessible datasets IQ-OTH/NCCD and Chest CT-Scan for comprehensive performance evaluation. Additionally, we employ data augmentation, stratified sampling, and fairness-aware training to enhance robustness and mitigate biases. The experimental assessment demonstrate that the GoogLeNet-AL method achieves an accuracy of 98.74 %, an F1-score of 98.96 %, and a precision of 99.74 % in lung cancer detection and also demonstrates its superior performance by outperforming traditional GoogLeNet and other baseline models. Overall, the proposed architecture enhanced the detection and categorization of lung nodules by reducing false positives and negatives, thus offering a valuable tool for combating lung cancer.}
}
@article{MIAO2024110729,
title = {CTNeRF: Cross-time Transformer for dynamic neural radiance field from monocular video},
journal = {Pattern Recognition},
volume = {156},
pages = {110729},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110729},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004801},
author = {Xingyu Miao and Yang Bai and Haoran Duan and Fan Wan and Yawen Huang and Yang Long and Yefeng Zheng},
keywords = {Dynamic neural radiance field, Monocular video, Scene flow, Transformer},
abstract = {The goal of our work is to generate high-quality novel views from monocular videos of complex and dynamic scenes. Prior methods, such as DynamicNeRF, have shown impressive performance by leveraging time-varying dynamic radiation fields. However, these methods have limitations when it comes to accurately modeling the motion of complex objects, which can lead to inaccurate and blurry renderings of details. To address this limitation, we propose a novel approach that builds upon a recent generalization NeRF, which aggregates nearby views onto new viewpoints. However, such methods are typically only effective for static scenes. To overcome this challenge, we introduce a module that operates in both the time and frequency domains to aggregate the features of object motion. This allows us to learn the relationship between frames and generate higher-quality images. Our experiments demonstrate significant improvements over state-of-the-art methods on dynamic scene datasets. Specifically, our approach outperforms existing methods in terms of both the accuracy and visual quality of the synthesized views. Our code is available on https://github.com/xingy038/CTNeRF.}
}
@article{FU2024110632,
title = {Hybrid-context-based multi-prior entropy modeling for learned lossless image compression},
journal = {Pattern Recognition},
volume = {155},
pages = {110632},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110632},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003832},
author = {Chuan Fu and Bo Du and Liangpei Zhang},
keywords = {Lossless image compression, Hybrid-context, Multi-prior-based entropy model},
abstract = {Lossless image compression is an essential aspect of image processing, particularly in many fields that require high information fidelity. In recent years, learned lossless image compression methods have shown promising results. However, many of these methods do not make optimal use of available information, leading to sub-optimal performance. This paper proposes a multi-prior entropy model for lossless image compression, which effectively leverages available information to achieve better compression performance. The proposed multi-prior comprises a cross-channel prior, hybrid local context, and hyperprior, allowing it to effectively utilize all available information. To remove redundancy across color channels, the original image is first losslessly transformed into YUV color space. The network then learns priors from the original image, the prior-coding channels, and the local context, which are fused to form the multi-prior used for GMM parameters estimation. Moreover, to capture the features of different images, a hybrid local context is abstracted using different kernel sizes of mask convolutions in a local context. The experimental results on several datasets demonstrate that our algorithm outperforms several existing learning-based image compression methods and traditional methods, such as JPEG2000, WebP, and FLIF.}
}
@article{LI2024110701,
title = {Class-imbalanced semi-supervised learning for large-scale point cloud semantic segmentation via decoupling optimization},
journal = {Pattern Recognition},
volume = {156},
pages = {110701},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110701},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004527},
author = {Mengtian Li and Shaohui Lin and Zihan Wang and Yunhang Shen and Baochang Zhang and Lizhuang Ma},
keywords = {3D point cloud, Class-imbalanced learning, Semi-supervised learning, Semantic segmentation},
abstract = {Semi-supervised learning (SSL), thanks to the significant reduction of data annotation costs, has been an active research topic for large-scale 3D scene understanding. However, the existing SSL-based methods suffer from severe training bias, mainly due to class imbalance and long-tail distributions of the point cloud data. As a result, they lead to a biased prediction for the tail class segmentation. In this paper, we introduce a new decoupling optimization framework, which disentangles feature representation learning and classifier in an alternative optimization manner to shift the bias decision boundary effectively. In particular, we first employ two-round pseudo-label generation to select unlabeled points across head-to-tail classes. We further introduce multi-class imbalanced focus loss to adaptively pay more attention to feature learning across head-to-tail classes. We fix the backbone parameters after feature learning and retrain the classifier using ground-truth points to update its parameters. Extensive experiments demonstrate the effectiveness of our method outperforming previous state-of-the-art methods on both indoor and outdoor 3D point cloud datasets (i.e., S3DIS, ScanNet-V2, Semantic3D, and SemanticKITTI) using 1% and 1pt evaluation.}
}
@article{BI2024110713,
title = {Motion-guided spatiotemporal multitask feature discrimination for self-supervised video representation learning},
journal = {Pattern Recognition},
volume = {155},
pages = {110713},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110713},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004643},
author = {Shuai Bi and Zhengping Hu and Hehao Zhang and Jirui Di and Zhe Sun},
keywords = {Unsupervised learning, Self-supervised learning, Cross-view learning, Multitask discrimination, Video action understanding},
abstract = {Powerful self-supervised representation models are able to step out of the traditional supervised paradigm and rely merely on unlabeled data to achieve a deep understanding of visual semantic features. However, previous approaches may suffer from background scene bias, making it difficult to perform a comprehensive exploration of video spatiotemporal structure. To address this challenge, this paper proposes a self-supervised video representation learning framework of motion-guided spatiotemporal multitask feature discrimination (MSMFD). The method mainly utilizes the consistency of motion cues between different views to guide the model for spatial and temporal feature similarity discrimination. Specifically, the model first selects video clips with large motion amplitudes based on the collected optical flow maps. Subsequently, the model introduces an instance discrimination task for overall spatiotemporal structure perception of the video, while a shuffled triplet and an augmented quadruple task are created to further enhance the exploration of intraframe sequence order and local spatial fine-grained. Furthermore, we propose joint motion alignment of spatial, temporal, and spatiotemporal dimensions under different views as a powerful compensation for acquiring motion features. Experimental results demonstrate that our self-supervised method is effective for learning video representations and achieves competitive performance in action recognition and video retrieval tasks compared to other state-of-the-art methods.}
}
@article{LUO2024110616,
title = {AMANet: An Adaptive Memory Attention Network for video cloud detection},
journal = {Pattern Recognition},
volume = {155},
pages = {110616},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110616},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003674},
author = {Chen Luo and Shanshan Feng and YingLing Quan and Yunming Ye and Yong Xu and Xutao Li and Baoquan Zhang},
keywords = {Lightweight network, Efficient video cloud detection, FY-4A},
abstract = {According to their orbits, meteorological satellites can be divided into polar-orbiting satellites and geostationary satellites. Existing cloud detection methods mainly focus on polar-orbiting satellite datasets. The geostationary satellite datasets contain, in contrast, time-continuous frames of particular locations. The temporal consistent information in these consecutive frames aid to increase the detection accuracy, but is challenging to be exploited. Besides, powered by the advanced technology of satellites, the onboard cloud detection application becomes a trend. Considering that satellites have resource limitations on energy and storage, applications deployed on them should be lightweight enough. However, the existing cloud detection models never concentrated on this lightweight video cloud detection task before. In this task, the temporal consistent features provided by time-continuous frames should be exploited for accuracy enhancement with low resource consumption. To tackle this problem, we design a lightweight deep learning video cloud detection model: Adaptive Memory Attention Network (AMANet). The proposed network is based on the encoder–decoder structure. The encoder consists of two branches. In the main branch, spatial and semantic features of the current frame are extracted. In the TemporalAttentionFlow branch, the proposed PyramidEncodingModule adaptively extracts context information from frames in sequence based on their distance to the current frame. In addition, in the proposed AdaptiveMemoryAttentionModule, the temporal relation among frames is extracted and propagated adaptively. The lightweight decoder is designed to gradually recover the cloud masks to the same scale as the input image. Experiments on a Video Cloud Detection dataset based on the dataset Fengyun4aCloud demonstrate that the designed AMANet achieves a remarkable balance between accuracy and resource consumption in comparison with current cloud detection methods, lightweight semantic segmentation methods, and video semantic segmentation methods.}
}
@article{WU2024110737,
title = {FairScene: Learning unbiased object interactions for indoor scene synthesis},
journal = {Pattern Recognition},
volume = {156},
pages = {110737},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110737},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004886},
author = {Zhenyu Wu and Ziwei Wang and Shengyu Liu and Hao Luo and Jiwen Lu and Haibin Yan},
keywords = {Indoor scene synthesis, Graph neural networks, Causal inference},
abstract = {In this paper, we propose an unbiased graph neural network learning method called FairScene for indoor scene synthesis. Conventional methods directly apply graphical models to represent the correlation of objects for subsequent furniture insertion. However, due to the object category imbalance in dataset collection and complex object entanglement with implicit confounders, these methods usually generate significantly biased scenes. Moreover, the performance of these methods varies greatly for different indoor scenes. To address this, we propose a framework named FairScene which can fully exploit unbiased object interactions through causal reasoning, so that fair scene synthesis is achieved by calibrating the long-tailed category distribution and mitigating the confounder effects. Specifically, we remove the long-tailed object priors subtract the counterfactual prediction obtained from default input, and intervene in the input feature by cutting off the causal link to confounders based on the causal graph. Extensive experiments on the 3D-FRONT dataset show that our proposed method outperforms the state-of-the-art indoor scene generation methods and enhances vanilla models on a wide variety of vision tasks including scene completion and object recognition.}
}
@article{WANG2024110685,
title = {An empirical study on the robustness of the segment anything model (SAM)},
journal = {Pattern Recognition},
volume = {155},
pages = {110685},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110685},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004369},
author = {Yuqing Wang and Yun Zhao and Linda Petzold},
keywords = {Segment anything model, Model robustness, Prompting techniques},
abstract = {The Segment Anything Model (SAM) is a foundation model for general image segmentation. Although it exhibits impressive performance predominantly on natural images, understanding its robustness against various image perturbations and domains is critical for real-world applications where such challenges frequently arise. In this study we conduct a comprehensive robustness investigation of SAM under diverse real-world conditions. Our experiments encompass a wide range of image perturbations. Our experimental results demonstrate that SAM’s performance generally declines under perturbed images, with varying degrees of vulnerability across different perturbations. By customizing prompting techniques and leveraging domain knowledge based on the unique characteristics of each dataset, the model’s resilience to these perturbations can be enhanced, addressing dataset-specific challenges. This work sheds light on the limitations and strengths of SAM in real-world applications, promoting the development of more robust and versatile image segmentation solutions. Our code is available at https://github.com/EternityYW/SAM-Robustness/.}
}
@article{ZHU2024110708,
title = {Local context attention learning for fine-grained scene graph generation},
journal = {Pattern Recognition},
volume = {156},
pages = {110708},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110708},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400459X},
author = {Xuhan Zhu and Ruiping Wang and Xiangyuan Lan and Yaowei Wang},
keywords = {Fine-grained scene graph generation, Local context, Location attention network, Local context-consistent label transfer},
abstract = {Fine-grained scene graph generation aims to parse the objects and their fine-grained relationships within scenes. Despite the significant progress in recent years, their performance is still limited by two major issues: (1) ambiguous perception under a global view; (2) the lack of reliable, fine-grained annotations. We argue that understanding the local context is important in addressing the two issues. However, previous works often overlook it, which limits their effectiveness in fine-grained scene graph generation. To tackle this challenge, we introduce a Local-context Attention Learning method that concentrates on local context and can generate high-reliability, fine-grained annotations. It comprises two components: (1) The Fine-grained Location Attention Network (FLAN), a multi-branch network that encompasses global and local branches, can attend to local informative context and perceive granularity levels in different regions, thereby adaptively enhancing the learning of fine-grained locations. (2) The Fine-grained Location Label Transfer (FLLT) method identifies coarse-grained labels inconsistent with the local context and determines which labels should be transferred through the global confidence thresholding strategy, finally transferring them to reliable local context-consistent fine-grained ones. Experiments conducted on the Visual Genome, OpenImage, and GQA-200 datasets show that the proposed methods achieve significant improvements on the fine-grained scene graph generation task. By addressing the challenge mentioned above, our method also achieves state-of-the-art performances on the three datasets.}
}
@article{FANG2024110689,
title = {Dynamic and static fusion mechanisms of infrared and visible images},
journal = {Pattern Recognition},
volume = {155},
pages = {110689},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110689},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004400},
author = {Aiqing Fang and Ying Li},
keywords = {Image fusion, Dynamic fusion, Static fusion, Deep learning},
abstract = {This paper propose a dynamic fusion mechanism of infrared and visible images, named DIFM, capable of solving the static fusion optimization problem. The DIFM correlates the image fusion quality with the image restoration quality to construct a unified optimization loss function. According to the DIFM, a dynamic image fusion network of infrared and visible images is constructed and is therefore denoted with DF-Net. Specifically, the DF-Net comprises two modules, i.e., the dynamic fusion module (DFM) and the self-learning dynamic restoration module (SLDRM). In order to solve the static fusion problem of existing methods, the DFM is proposed to learn the fusion weight dynamically. Specifically, the DFM comprises a classification module (CM) and an image fusion module (IFM), which determine whether and how to fuse source images. In addition, a unified fusion loss function is introduced to obtain more hidden features of infrared and visible images in complex environments. Therefore, the stumbling block of deep learning in image fusion, i.e., static fusion, is significantly mitigated. Extensive experiments demonstrate that the dynamic fusion optimization method neatly outperforms the state-of-the-art methods in most metrics.}
}
@article{XU2024110697,
title = {Deep graph matching meets mixed-integer linear programming: Relax or not ?},
journal = {Pattern Recognition},
volume = {155},
pages = {110697},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110697},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004485},
author = {Zhoubo Xu and Puqing Chen and Romain Raveaux and Xin Yang and Huadong Liu},
keywords = {, Feature points correspondence, Graph-based representation, Combinatorial optimization},
abstract = {Graph matching is an important problem that has received widespread attention, especially in the field of computer vision. Recently, state-of-the-art methods seek to incorporate graph matching with deep learning. However, there is no research to explain what role the graph matching algorithm plays in the model. Therefore, we propose an approach integrating a MILP formulation of the graph matching problem. This formulation is solved to optimal and it provides inherent baseline. Meanwhile, similar approaches are derived by releasing the optimal guarantee of the graph matching solver and by introducing a quality level. This quality level controls the quality of the solutions provided by the graph matching solver. In addition, several relaxations of the graph matching problem are put to the test. Our experimental evaluation gives several theoretical insights and guides the direction of deep graph matching methods.}
}
@article{XUE2024110673,
title = {A novel active contour model based on features for image segmentation},
journal = {Pattern Recognition},
volume = {155},
pages = {110673},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110673},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004242},
author = {Peng Xue and Sijie Niu},
keywords = {Active contour model, Energy functional, Feature energy function, Complex natural image},
abstract = {Active contour model is an extraordinarily valuable technique in image segmentation, which is essential for image analysis and understanding. Active contour model has been widely studied because it delineates closed and smooth contours or surfaces of target objects. However, traditional active contour models underperform on complex natural images. To tackle this problem, we propose a novel active contour model framework, called FeaACM. We introduce the feature energy function into the conventional energy functional to minimize the energy functional to maintain the consistency of the object region and account for different distributions of objects and backgrounds in the feature space. To demonstrate the advantages of our method, we compare our method with the state-of-the-art methods, and show that our method achieves competitive performance. In addition, we utilize AutoEncoder technology to extract the feature of the image verifying the generality of our framework. Extensive and numerous experiments indicate that our method can segment complex natural images effectively. Our code is available at https://github.com/xuepeng1234/FeaACM.}
}
@article{KANG2024110688,
title = {Discrete online cross-modal hashing with consistency preservation},
journal = {Pattern Recognition},
volume = {155},
pages = {110688},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110688},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004394},
author = {Xiao Kang and Xingbo Liu and Wen Xue and Xuening Zhang and Xiushan Nie and Yilong Yin},
keywords = {Cross-modal retrieval, Supervised online hashing, Continuous semantic embedding, Fine-grained similarity preserving, Modality deviation calibration},
abstract = {Online cross-modal hashing has attracted widespread attention with the rapid expansion of large-scale streaming data, which can reduce storage requirements and enhance efficiency for online cross-modal retrieval. However, despite promising progress, existing methods still suffer from defective accuracy in a way, primarily attributed to two issues: insufficient semantic information exploitation and mismatched training-retrieval process. To address these challenges, we propose a novel supervised hashing method with dual consistency preservation, called Discrete Online Cross-Modal Hashing (DOCMH). On the one hand, we design more informative continuous semantic labels and fine-grained similarity graphs to preserve semantic consistency across different streaming data chunks and modality representations. On the other hand, we propose an effective modality deviation calibration mechanism for preserving learning process consistency between the training and retrieval phases. Extensive experiments on three widely used benchmark datasets demonstrate the superior performance of the proposed DOCMH under various scenarios.}
}
@article{ZHANG2024110719,
title = {Token-word mixer meets object-aware transformer for referring image segmentation},
journal = {Pattern Recognition},
volume = {155},
pages = {110719},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110719},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004709},
author = {Zhenliang Zhang and Zhu Teng and Jack Fan and Baopeng Zhang and Jianping Fan},
keywords = {Transformer, Multi-modal fusion, Referring image segmentation},
abstract = {Referring image segmentation aims to generate a binary mask of the target object according to a referring expression. Some recent works argue that post-fusion paradigm may result in inconsistency and insufficiency issue and propose to integrate textual features during the visual encoding process. Although effective, they do fusion in a single way at each stage of encoder, e.g. utilizing cross attention mechanisms. This single fusion method ignores local and detailed image information correlated with language due to the incapability of attention in capturing high-frequencies information. To address this issue, we propose a Token-Word Mixer, which takes into consideration the characteristics of convolution and attention, and achieves more comprehensive interactions and alignments of multi-modal features through a mix operation. Furthermore, existing methods that rely solely on grid features lack perception of the target object and inference of relationships between objects, making it difficult to associate and align semantic information of target objects during multi-modal fusion when referring expressions or image scenes are complex. Therefore, we propose to incorporates object-level information by exploiting a DETR-based detector to provide region features, and the Object-Aware Transformer encoder with an additional learnable token is proposed to perceive effective information associated with the target object. Based on the enhanced cross-modal features and the aggregated token, we adopt query-based mask generation method instead of pixel classification framework for referring image segmentation. Extensive experiments and ablation studies indicate the effectiveness of our proposed methods.}
}
@article{MOTALLEBI2024110635,
title = {Efficient and robust clustering based on backbone identification},
journal = {Pattern Recognition},
volume = {155},
pages = {110635},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110635},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003868},
author = {Hassan Motallebi},
keywords = {Clustering algorithm, Data backbone, Popularity, Popularity-based clustering, Arbitrary-shaped clusters},
abstract = {Clustering is the process of grouping similar data objects into different subsets based on their similarities. Inspired by the concept of the popularity of individuals in a community, we rate the popularity of each sample which reflects the centrality of that sample in the dataset. With the aim of identifying clusters with arbitrary shapes and varying densities, we propose a clustering approach that divides samples into separate population groups. This approach is based on identifying the backbone of data, characterized by a set of popular points surrounded by less popular points. To distinguish poorly separated clusters, a proximity measure is defined based on the popularity of samples. We also use the popularity of samples to assign halo points to clusters and calculate cohesion between clusters. The proposed clustering method can detect arbitrary-shaped clusters with varying densities without requiring to specify the number of clusters. Outliers are also identified according to popularity. We demonstrate the effectiveness of the approach on synthetic and real-world datasets.}
}
@article{BRONIK2024110732,
title = {Conditional advancement of machine learning algorithm via fuzzy neural network},
journal = {Pattern Recognition},
volume = {155},
pages = {110732},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110732},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004837},
author = {Kevin Bronik and Le Zhang},
keywords = {Machine learning, Convolutional neural network, Fuzzy neural network, Adaptive neuro-fuzzy inference, Segmentation, Validation metrics},
abstract = {Improving overall performance is the ultimate goal of any machine learning (ML) algorithm. While it is a trivial task to explore multiple individual validation measurements, evaluating and monitoring overall performance can be complicated due to the highly nonlinear nature of the functions describing the relationships among different validation metrics, such as the Dice Similarity Coefficient (DSC) and Jaccard Index (JI). Therefore, it is naturally desirable to have a reliable validation algorithm or model that can integrate all existing validation metrics into a single value. This consolidated metric would enable straightforward assessment of an ML algorithm’s performance and identify areas for improvement. To deal with such a complex nonlinear problem, this study suggests a novel parameterized model named Adaptive Neuro-Fuzzy Inference Systems (ANFIS), which takes any set of input–output precise-imprecise data and uses a neuro-adaptive learning strategy to tune the parameters of the pre-defined membership functions. Our method can be accepted as an elegant and the state-of-the-art method for the nonlinear function approximation, which could be added directly to any convolutional neural networks (CNN) loss functions as the regularization term to generate a constrained-CNN-FUZZY model optimization. To demonstrate the ability of the purposed method and provide a practical explanation of the capability of ANFIS, we use deep CNN as a testing platform to consider the fact that one of the biggest challenges CNN-developers faced today is to reduce the mismatching between the provided input data and the predicted results monitored by different validation metrics. We first create a toy dataset using MNIST and investigate the properties of the proposed model. We then use a medical dataset to demonstrate our method’s efficacy on brain lesion segmentation. In both datasets, our method shows reliable validation results to guide researchers towards choosing performance metrics in a problem-aware manner, especially when the results of different validation metrics are too similar among models to determine the best one.}
}
@article{TIAN2024110744,
title = {Rethink video retrieval representation for video captioning},
journal = {Pattern Recognition},
volume = {156},
pages = {110744},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110744},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004953},
author = {Mingkai Tian and Guorong Li and Yuankai Qi and Shuhui Wang and Quan Z. Sheng and Qingming Huang},
keywords = {Video captioning, Video-text retrieval, Token shift, Cross-attention},
abstract = {Video captioning, a challenging task targeting the automatic generation of accurate and comprehensive descriptions based on video content, has witnessed substantial success recently driven by bridging video representations and textual semantics. Inspired by the nature of the video retrieval task, which learns visual features strongly related to text queries, we propose to take advantage of visual representation learning from the video retrieval framework to tackle video captioning tasks and construct adequate multi-grained cross-modal matching while extracting visual features. However, a simple direct application of recent video retrieval models fails to capture sufficient temporal details and the rich visual features of local patch tokens of video frames lack semantic information essential for captioning tasks. These deficiencies are primarily due to these models lack fine-grained interactions between video frames and offer only weak textual supervision over frame patch tokens. To increase the attention on temporal details, we propose a learnable token shift module, which flexibly captures subtle movements in local regions across the temporal sequence. Furthermore, we devise a Refineformer, which learns to integrate local video patch tokens strongly related to desired captions via a cross-attention mechanism. Extensive experiments on MSVD, MSR-VTT and VATEX demonstrate the favorable performance of our method. Code will be available at https://github.com/tiesanguaixia/IVRC.}
}
@article{JIANGZHOU2024110692,
title = {DGRM: Diffusion-GAN recommendation model to alleviate the mode collapse problem in sparse environments},
journal = {Pattern Recognition},
volume = {155},
pages = {110692},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110692},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004436},
author = {Deng Jiangzhou and Wang Songli and Ye Jianmei and Ji Lianghao and Wang Yong},
keywords = {Diffusion model, Generative adversarial network, Mode collapse, Data sparsity, Recommender systems},
abstract = {Generative adversarial network (GAN) has been widely adopted in recommender systems (RSs) to improve the recommendation accuracy. However, existing GAN-based models often suffer from the mode collapse problem in sparse environments and fail to adequately capture the complexity of user preferences and behaviors, which affects recommendation performance. To address these issues, we introduce a diffusion model (DM) into the GAN framework, proposing an efficient Diffusion-GAN recommendation model (DGRM) to achieve mutual enhancement between the two generative models. This model first utilizes the forward process of DM to generate conditional vectors that guide the training of the GAN generator. Subsequently, the backward process of DM assists the GAN discriminator using Wasserstein distance during adversarial training. The Wasserstein distance is adopted to solve the asymmetry of Kullback-Leibler (KL) divergence as a loss function in traditional GANs. Experiments on multiple datasets demonstrate that the proposed model effectively alleviates mode collapse and surpasses other state-of-the-art (SOTA) methods in various evaluation metrics.}
}
@article{YANG2024110716,
title = {Large-scale multi-view clustering via matrix factorization of consensus graph},
journal = {Pattern Recognition},
volume = {155},
pages = {110716},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110716},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004679},
author = {Zengbiao Yang and Yihua Tan and Tao Yang},
keywords = {Multi-view clustering, Consensus graph, Matrix factorization, Anchors},
abstract = {Recently, anchors-based multi-view clustering methods have been widely concerned for they can not only significantly reduce the time complexity but also have good interpretability. However, the time consumption of optimization and spectral embedding with Singular Value Decomposition (SVD) is expensive for large-scale multi-view clustering due to the large-scale consensus graph. This paper proposes to factorize the consensus graph to directly obtain the low-dimension consensus embedding matrix by optimizing the objective function. Specifically, the consensus graph is factorized into a transition matrix and a low-dimension embedding matrix. Among them, the transition matrix is used to prevent the clustering time consumption from increasing significantly with the number of anchors, and the low-dimension embedding matrix is used to mine the low-dimension consensus information of each view. The proposed method demonstrates its superiority by outperforming eight multi-view clustering algorithms on nine datasets, as evidenced by the clustering results.}
}
@article{ZHAN2024110720,
title = {A novel weighted approach for time series forecasting based on visibility graph},
journal = {Pattern Recognition},
volume = {155},
pages = {110720},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110720},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004710},
author = {Tianxiang Zhan and Fuyuan Xiao},
keywords = {Time series, Complex network, Visibility graph, Link forecasting, Pattern recognition},
abstract = {Time series has attracted a lot of attention in many fields today. Time series forecasting algorithm based on complex network analysis is a research hotspot. How to use time series information to achieve more accurate forecasting is a problem. To solve this problem, this paper proposes a weighted network forecasting method to improve the forecasting accuracy. Firstly, the time series will be transformed into a complex network, and the similarity between nodes will be found. Then, the similarity will be used as a weight to make weighted forecasting on the predicted values produced by different nodes. Compared with the previous method, the proposed method is more accurate. In order to verify the effect of the proposed method, the experimental part is tested on M1, M3 datasets and Construction Cost Index (CCI) dataset, which shows that the proposed method has more accurate forecasting performance.}
}
@article{CHEN2024110749,
title = {DACBN: Dual attention convolutional broad network for fine-grained visual recognition},
journal = {Pattern Recognition},
volume = {156},
pages = {110749},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110749},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005004},
author = {Tao Chen and Lijie Wang and Yang Liu and Haisheng Yu},
keywords = {Fine-grained visual classification, Dual attention mechanism, Broad learning system, Discriminative features},
abstract = {Fine-grained visual classification (FGVC) is a challenging task due to its small inter-class differences and large intra-class differences. Most existing methods rely on manual labeling of key identification areas, which requires high labor costs. In addition, existing methods also tend to ignore the differences effect of different feature channels in the feature map, which has a certain impact on the model classification accuracy. To solve the above problems, this paper proposes a dual attention convolutional broad network. Firstly, a new dual attention mechanism is designed to suppress the background noise of fine-grained images and give greater weight to the discriminative feature regions and channels. Secondly, the ensemble broad learning system framework is used to further enhance the dual attention features, so that the discriminative features can further improve the recognition ability of the model. Finally, by multiple comparative experiments, it is reported that the method proposed in this article has achieved excellent recognition results on three commonly used datasets.}
}
@article{RUAN2024110728,
title = {Semantic attention-based heterogeneous feature aggregation network for image fusion},
journal = {Pattern Recognition},
volume = {155},
pages = {110728},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110728},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004795},
author = {Zhiqiang Ruan and Jie Wan and Guobao Xiao and Zhimin Tang and Jiayi Ma},
keywords = {Image fusion, High-level vision tasks, Attention mechanism, Semantic prior},
abstract = {Infrared and visible image fusion aims to generate a comprehensive image that retains both salient targets of the infrared image and texture details of the visible image. However, existing methods overlook the differences in attention to semantic information among different fused images. To address this issue, we propose a semantic attention-based heterogeneous feature aggregation network for image fusion. The key component of our network is the semantic attention-based fusion module, which leverages the weights derived from semantic feature maps to dynamically adjust the significance of various semantic objects within the fusion feature maps. By using semantic weights as guidance, our fusion process concentrates on regions with crucial semantics, resulting in a more focused fusion that preserves rich semantic information. Moreover, we propose an innovative component called the attentive dense block. This block effectively filters out irrelevant features during extraction, accentuates essential features to their maximum potential, and enhances the visual quality of the fused images. Importantly, our network demonstrates strong generalization capabilities. Extensive experiments validate the superiority of our proposed network over current state-of-the-art techniques in terms of both visual appeal and semantics-driven evaluation.}
}
@article{DUTTA2024110676,
title = {DIVA: Deep unfolded network from quantum interactive patches for image restoration},
journal = {Pattern Recognition},
volume = {155},
pages = {110676},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110676},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004278},
author = {Sayantan Dutta and Adrian Basarab and Bertrand Georgeot and Denis Kouamé},
keywords = {Quantum many-body interaction, Schrödinger equation, Unfolding, Deep learning, Image restoration, Quantum image processing},
abstract = {This paper presents a deep neural network called DIVA unfolding a baseline adaptive denoising algorithm (DeQuIP), relying on the theory of quantum many-body physics. Furthermore, it is shown that with very slight modifications, this network can be enhanced to solve more challenging image restoration tasks such as image deblurring, super-resolution and inpainting. Despite a compact and interpretable (from a physical perspective) architecture, the proposed deep learning network outperforms several recent algorithms from the literature, designed specifically for each task. The key ingredients of the proposed method are on one hand, its ability to handle non-local image structures through the patch-interaction term and the quantum-based Hamiltonian operator, and, on the other hand, its flexibility to adapt the hyperparameters patch-wisely, due to the training process.}
}
@article{LEE2024110704,
title = {Discriminative action tubelet detector for weakly-supervised action detection},
journal = {Pattern Recognition},
volume = {155},
pages = {110704},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110704},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004552},
author = {Jiyoung Lee and Seungryong Kim and Sunok Kim and Kwanghoon Sohn},
keywords = {Spatiotemporal action detection, Action proposal, Weakly-supervised learning},
abstract = {We propose a novel framework for spatiotemporal action detection using only video-level class labels as weak supervision. Traditional fully-supervised approaches rely on labor-intensive manual annotation of bounding boxes for each frame. In contrast, collecting video-level class labels is significantly less tedious and more feasible compared to annotating frame-level sequences with bounding boxes. To address this challenge, we propose a discriminative action tubelet detector, called DAT-detector, designed to discern discriminative tubelets from action tubelet proposals (ATPs). Whereas the previous approaches have only focused on tubelet selection among the predefined object proposals, our DAT-detector prioritizes the generation of more precise action tubelets using regression and attention modules. Moreover, we introduce an ATP generation method that enhances the quality of tubelet proposals. Our approach achieves state-of-the-art performance on several benchmarks, and also demonstrates competitive performance even with fully-supervised approaches.}
}
@article{ZHAO2024110652,
title = {Consistency approximation: Incremental feature selection based on fuzzy rough set theory},
journal = {Pattern Recognition},
volume = {155},
pages = {110652},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110652},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004035},
author = {Jie Zhao and Daiyang Wu and JiaXin Wu and Wenhao Ye and Faliang Huang and Jiahai Wang and Eric W.K. See-To},
keywords = {Fuzzy rough set, Incremental feature selection, Consistency approximation},
abstract = {Fuzzy Rough Set Theory (FRST)-based feature selection has been widely used as a preprocessing step to handle dynamic and large datasets. However, large-scale or high-dimensional datasets remain intractable for FRST-based feature selection approaches due to high space complexity and unsatisfactory classification performance. To overcome these challenges, we propose a Consistency Approximation (CA)-based framework for incremental feature selection. By exploring CA, we introduce a novel significance measure and a tri-accelerator. The CA-based significance measure provides a mechanism for each sample in the universe to keep members with different class labels within its fuzzy neighbourhood as far as possible, while keeping members with the same label as close as possible. Furthermore, our tri-accelerator reduces the search space and decreases the computational space with a theoretical lower bound. The experimental results demonstrate the superiority of our proposed algorithm compared to state-of-the-art methods on efficiency and classification accuracy, especially for large-scale and high-dimensional datasets.}
}
@article{COSTA2024110687,
title = {Evaluating brain group structure methods using hierarchical dynamic models},
journal = {Pattern Recognition},
volume = {155},
pages = {110687},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110687},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004382},
author = {Lilia Costa and Osvaldo Anacleto and Diego C. Nascimento and James Q. Smith and Catriona M. Queen and Francisco Louzada and Thomas Nichols},
keywords = {Multiregression dynamic model, Bayesian network, Group analysis, Cluster analysis, Hierarchical models},
abstract = {In graph theory, complex structures are studied, as well as the dynamics of the connectivity strength of this structure. However, in the estimation procedure, particular characteristics need to be considered at some level as informative when estimating the characteristics of a group. This work proposes a model that provides dynamic estimation of the network structure based on a model that makes it possible to incorporate hierarchy (individual information) in the process. In addition, we show the feasibility of modeling a complex structure by levels, exemplifying this by cluster analysis as a visualization of the embedding projection reduction space. Our case study is a neuroscience experiment, which needs to estimate the brain connectivity map, that is, to study the information flow of the brain in resting-stage subjects. Methods for estimating group networks can be grouped into the following 4 categories: group-structure (GS), virtual-typical-subject (VTS), common-structure (CS), and individual-structure (IS). These four group-structure estimation methods were compared in the context of the Multiregression Dynamic Models. Results showed that the proposed Bayesian Network Structure Dynamic estimation, using GS and hierarchical dynamic models, accommodates the latent/personal information in the estimation process by extracting the pattern shared between them. Moreover, the cluster analysis estimation corroborates the empirical results and expert judgments.}
}
@article{LI2024110742,
title = {Self-contrastive Feature Guidance Based Multidimensional Collaborative Network of metadata and image features for skin disease classification},
journal = {Pattern Recognition},
volume = {156},
pages = {110742},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110742},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400493X},
author = {Feng Li and Min Li and Enguang Zuo and Chen Chen and Cheng Chen and Xiaoyi Lv},
keywords = {Multimodal fusion, Multidimensional synergy, Contrast learning, Feature refinement},
abstract = {Both clinical images and metadata are the foundation of clinical diagnosis, effectively fusing these two resources is a major difficulty in the detection of skin cancer. Even though existing fusion methods produced better fusion outcomes, they only carried out single-level fusion prior to making decisions and used distinct feature extraction for each modal data. The ability of inter-modal synergy is diminished by this fusion strategy, resulting in coarse fusion features. To enhance the multidimensional representation of images, we suggest a Self-contrastive Feature Guidance Based Multidimensional Collaborative Network (SGMC Net). Specifically, we split the fusion method into three steps: spatial dimension fusion, channel dimension fusion, and adaptive corrective outputting to establish multidimensional collaboration between metadata and image features in the feature extraction process. Accordingly, we build three blocks: channel fusion block, spatial fusion block, and feature rectification block. On this basis, we propose a Self-contrastive Feature Guidance method that utilizes the contrast loss between shallow and deep features of the image as a supervisory signal in a non-enhanced manner to optimize shallow features. Finally, extensive experiments were conducted on PAD-UFES-20 and Der7pt dataset, our method achieved an accuracy of 83.3% beyond other state-of-the-art models. We further validated the effectiveness of the feature guidance method, showing a 5.2% improvement in accuracy for SGMC18.}
}
@article{AN2024110731,
title = {Dynamic weighted knowledge distillation for brain tumor segmentation},
journal = {Pattern Recognition},
volume = {155},
pages = {110731},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110731},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004825},
author = {Dianlong An and Panpan Liu and Yan Feng and Pengju Ding and Weifeng Zhou and Bin Yu},
keywords = {Brain tumor segmentation, MRI, Static knowledge distillation, Dynamic weighted knowledge distillation, Interpretability},
abstract = {Automatic 3D MRI brain tumor segmentation holds a crucial position in the field of medical image analysis, contributing significantly to the clinical diagnosis and treatment of brain tumors. However, traditional 3D brain tumor segmentation methods often entail extensive parameters and computational demands, posing substantial challenges in model training and deployment. To overcome these challenges, this paper introduces a brain tumor segmentation framework based on knowledge distillation. This framework includes training a lightweight network by extracting knowledge from a well-established brain tumor segmentation network. Firstly, this framework replaces the conventional static knowledge distillation (SKD) with the proposed dynamic weighted knowledge distillation (DWKD). DWKD dynamically adjusts the distillation loss weights for each pixel based on the learning state of the student network. Secondly, to enhance the student network's generalization capability, this paper customizes a loss function for DWKD, known as regularized cross-entropy (RCE). RCE introduces controlled noise into the model, enhancing its robustness and diminishing the risk of overfitting. This controlled injection of noise aids in fortifying the model's robustness. Lastly, Empirical validation of the proposed methodology is conducted using two distinct backbone networks, namely Attention U-Net and Residual U-Net. Rigorous experimentation is executed across the BraTS 2019, BraTS 2020, and BraTS 2021 datasets. Experimental results demonstrate that DWKD exhibits significant advantages over SKD in enhancing the segmentation performance of the student network. Furthermore, when dealing with limited training data, the RCE method can further improve the student network's segmentation performance. Additionally, this paper quantitatively analyzes the number of concept detectors identified in network dissection. It assesses the impact of DWKD on model interpretability and finds that compared to SKD, DWKD can more effectively enhance model interpretability. The source code is available at https://github.com/YuBinLab-QUST/DWKD/.}
}
@article{PENG2024110679,
title = {Global self-sustaining and local inheritance for source-free unsupervised domain adaptation},
journal = {Pattern Recognition},
volume = {155},
pages = {110679},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110679},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004308},
author = {Lin Peng and Yuhang He and Shaokun Wang and Xiang Song and Songlin Dong and Xing Wei and Yihong Gong},
keywords = {Source-free, Domain adaptation, Global consistency, Local topology},
abstract = {In this paper, we investigate a practical problem called source-free unsupervised domain adaptation, which adapts a source-trained model to the target domain with unlabeled target data. To address this problem, we propose a novel GlObal self-sustAining and Local inheritance (GOAL) method. GOAL contains three components. (1) A backbone follows a mean teacher scheme. The teacher model serves as a smoothing functionality, facilitating a more consistent convergence of the student model. This capability alleviates the student model’s sensitivity to minor input data variations and enhances the overall robustness of the model. Additionally, disparities in predictions between the student and teacher models can be leveraged to identify potential noise in the data. (2) A Global Consistency Self-Sustaining mechanism for learning a stable, discriminative, and diverse prediction space. On the one hand, we employ neighbor samples and mean-teacher schemes to enhance the discriminability and stability of model predictions. On the other hand, non-neighbor samples are leveraged to augment the diversity of model predictions. Furthermore, to mitigate the impact of potential negative neighbors, we derive a weighting factor by incorporating both neighbor entropy and the top-nd similarity of features. (3) A Local Topology Inheritance mechanism to improve the semantic structure of the feature space. We construct a semantic topology graph based on the output predictions of the teacher model and subsequently transmit the teacher topology to the feature space of the student utilizing a local topology inheritance loss. Combining these three components, GOAL can effectively solve the source-free unsupervised domain adaptation. To the best of our knowledge, GOAL is the first attempt to perform topology inheritance for global consistency domain adaptation. Comprehensive experiments illustrate the effectiveness and superiority of GOAL in addressing source-free unsupervised domain adaptation.}
}
@article{ZHAO2024110715,
title = {Multi-view clustering via dynamic unified bipartite graph learning},
journal = {Pattern Recognition},
volume = {156},
pages = {110715},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110715},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004667},
author = {Xingwang Zhao and Shujun Wang and Xiaolin Liu and Jiye Liang},
keywords = {Multi-view clustering, Unified bipartite graph, Multi-granular structural information, Dynamic filter},
abstract = {Multi-view clustering algorithms based on graph learning have the ability to extract the potential association between data samples, which has been a concern of many researchers in recent years. However, existing algorithms have two limitations: (1) they directly learn from the raw graph, which includes noise and outliers, and they construct the graph filter statically, biasing the clustering results; (2) during graph construction, they mainly use the information of a single structure and fail to fully extract the multi-granular structural information among the data. To address these issues, this paper proposes a novel multi-view clustering method via dynamic unified bipartite graph learning. Specifically, a learnable graph filter is first refined to dynamically filter the original data feature space, gradually filtering out the undesirable high-frequency noise and achieving a clustering-friendly smooth representation. Second, a unified bipartite graph is constructed by combining the multi-granular structural information of different views to better explore the distinct and common information of each view. In one framework, the dynamic filter and multi-granular structure information are combined to iteratively learn the unified bipartite graph. An efficient iterative algorithm is designed to decompose the objective function into small-scale subproblems for solving. Extensive experiments on benchmark datasets show the superiority of the proposed algorithm over several existing state-of-the-art multi-view clustering algorithms.}
}
@article{WEN2024110743,
title = {Restoring vision in rain-by-snow weather with simple attention-based sampling cross-hierarchy Transformer},
journal = {Pattern Recognition},
volume = {156},
pages = {110743},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110743},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004941},
author = {Yuanbo Wen and Tao Gao and Kaihao Zhang and Peng Cheng and Ting Chen},
keywords = {Computer vision, Image restoration, Rain-by-snow weather removal, Vision Transformer, Attention sampling},
abstract = {As an unnoticed specialized task in image restoration, rain-by-snow weather removal aims to eliminate the complicated coexisting rain streaks and snow particles. In this work, we propose a simple attention-based sampling cross-hierarchy Transformer (SASCFormer). Initially, we explore the proximity of convolution network and Transformer in hierarchical architectures and experimentally find they perform approximately for intra-stage feature representation. On this basis, we utilize a Transformer-like convolution block (TCB) to replace the computation-heavy self-attention while preserving the attention characteristics for adapting to the input content. Meanwhile, we demonstrate that cross-stage sampling progression is critical for the performance improvement in rain-by-snow weather removal, and propose a global–local self-attention sampling mechanism (GLSASM) that samples the features while preserving both the global and local dependencies. Finally, we synthesize two novel rain-by-snow weather-degraded benchmarks, RSCityscapes and RS100K datasets. Extensive experiments verify that our proposed SASCFormer achieves the best trade-off between the performance and inference time. In particular, our approach advances existing methods by 1.14dB∼4.89dB in peak signal-to-noise ratio. Related resources are available at https://github.com/chdwyb/Rain-by-snow.}
}
@article{ZENG2024110691,
title = {Robust image hiding network with Frequency and Spatial Attentions},
journal = {Pattern Recognition},
volume = {155},
pages = {110691},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110691},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004424},
author = {Xiaobin Zeng and Bingwen Feng and Zhihua Xia and Zecheng Peng and Tiewei Qin and Wei Lu},
keywords = {Convert Image Communication (CIC), Image hiding, Robustness, JPEG compression, Attention mask, Frequency},
abstract = {Convert Image Communication (CIC) is a promising technology to protect the privacy of images. Recently, the emergence of robust CIC resistant to JPEG compression has gained due to the widespread use of JPEG compression in image communication. This paper introduces a R̲obust image hiding network with F̲requency and S̲patial A̲ttentions (RFSA) to implement robust CIC. RFSA can hide an image within another image with high robust. It incorporates multiple image attentions corresponding to imperceptibility, recovered image quality, and resistance to JPEG compression, which ensure that secret images are hidden within regions that cause little distortion and can well withstand JPEG compression. Additionally, two encoders, that is, a frequency encoder and a spatial encoder, are mixed to adaptively embed secret images across both frequency and spatial domains. Experimental results demonstrate that the proposed scheme not only maintains high image quality and capacity but also exhibits exceptional resistance to JPEG compression compared to other state-of-the-art image hiding methods. The average Peak Signal-to-Noise Ratio (PSNR) of the recovered image remains at 24.96 dB even under JPEG compression with a quality factor of 55.}
}
@article{ZHANG2024110703,
title = {Hunt-inspired Transformer for visual object tracking},
journal = {Pattern Recognition},
volume = {156},
pages = {110703},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110703},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004540},
author = {Zhibin Zhang and Wanli Xue and Yuxi Zhou and Kaihua Zhang and Shengyong Chen},
keywords = {Visual object tracking, Particle filter, Uncertainty estimation, Transformer},
abstract = {This paper presents a hunt-inspired Transformer for visual object tracking, dubbed as HuntFormer. The HuntFormer focuses on robust target detection and identification, simulating natural hunting processes. Specifically, the HuntFormer comprises two essential module designs including a predictor for detection and a verifier for identification. The predictor emulates the detection stage by designing a motion trajectory guided particle filter, which identifies potential target locations by predicting the motion state within a particle filtering framework. The predictor utilizes spatio-temporal correlation scores between dynamic target templates and the search region to guide the learning process to generate a set of reliable particles. This enables the base tracker to narrow its search range to focus on the target, and swiftly re-detect the target in case of model drift. Once the target is re-detected, the verifier assesses the detection result as a reliable tracked item. The verifier initially maintains a dynamic memory that stores reliable target templates and their corresponding locations in the motion trajectory. It then models the uncertainty of appearance information within this memory probabilistically. The output uncertainty score determines whether the memory gets updated or not. Ultimately, the predictor and the verifier collaborate, ensuring a robust tracking outcome. Extensive evaluations on six challenging benchmark datasets demonstrate HuntFormer’s favorable performance against various state-of-the-art trackers. Notably, in the VOT-LT2022 tracking challenge, the HuntFormer won the third place with an F-score of 0.598, closely competing for the second place with an F-score of 0.600.}
}
@article{KUMAR2024110651,
title = {MOVES: Movable and moving LiDAR scene segmentation in label-free settings using static reconstruction},
journal = {Pattern Recognition},
volume = {155},
pages = {110651},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110651},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004023},
author = {Prashant Kumar and Dhruv Makwana and Onkar Susladkar and Anurag Mittal and Prem Kumar Kalra},
keywords = {Label-free LiDAR segmentation, Static LiDAR reconstruction, Dynamic environments},
abstract = {Accurate static structure reconstruction and segmentation of non-stationary objects is of vital importance for autonomous navigation applications. These applications assume a LiDAR scan to consist of only static structures. In the real world however, LiDAR scans consist of non-stationary dynamic structures — moving and movable objects. Current solutions use segmentation information to isolate and remove moving structures from LiDAR scan. This strategy fails in several important use-cases where segmentation information is not available. In such scenarios, moving objects and objects with high uncertainty in their motion i.e. movable objects, may escape detection. This violates the above assumption. We present MOVES, a novel GAN based adversarial model that segments out moving as well as movable objects in the absence of segmentation information. We achieve this by accurately transforming a dynamic LiDAR scan to its corresponding static scan. This is obtained by replacing dynamic objects and corresponding occlusions with static structures which were occluded by dynamic objects. We leverage corresponding static-dynamic LiDAR pairs. We design a novel discriminator, coupled with a contrastive loss on a smartly selected LiDAR scan triplet. For datasets lacking paired information, we propose MOVES-MMD that integrates Unsupervised Domain Adaptation into the network. We perform rigorous experiments to demonstrate state of the art dynamic to static translation performance on a sparse real world industrial dataset, an urban and a simulated dataset. MOVES also segments out movable and moving objects without using segmentation information. Without utilizing segmentation labels, MOVES performs better than segmentation based navigation baseline in highly dynamic and long LiDAR sequences. The code is available here.}
}
@article{YANG2024110727,
title = {D-Net: A dual-encoder network for image splicing forgery detection and localization},
journal = {Pattern Recognition},
volume = {155},
pages = {110727},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110727},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004783},
author = {Zonglin Yang and Bo Liu and Xiuli Bi and Bin Xiao and Weisheng Li and Guoyin Wang and Xinbo Gao},
keywords = {Image splicing forgery detection, Tampered region localization, Convolutional neural network, Dual-encoder, Image fingerprints, Global perspective},
abstract = {Many detection methods based on convolutional neural networks (CNNs) have been proposed for image splicing forgery detection. Most of these methods focus on validating local patches or local objects. We regard image splicing forgery detection as a binary classification task that distinguishes tampered and non-tampered regions by forensic fingerprints rather than semantic features. As the network goes deep, its representation ability becomes strong. However, the non-semantic forensic fingerprints can hardly be retained by normal CNNs in deep layers. We proposed a novel dual-encoder network (D-Net) for image splicing forgery detection to resolve these issues, employing an unfixed and a fixed encoder. The unfixed encoder autonomously learns the image fingerprints that differentiate between the tampered and non-tampered regions, whereas the fixed encoder intentionally provides structural information that assists the learning and detection of the forgeries. This dual-encoder is followed by a spatial pyramid global-feature extraction module that expands the global insight of D-Net for classifying the tampered and non-tampered regions more accurately. In an experimental comparison study of D-Net and state-of-the-art methods, D-Net, without pre-training or training on a large number of forgery images, outperformed the other methods in pixel-level forgery detection. Moreover, it is stably robust to different anti-forensic attacks.}
}
@article{ZHANG2024110675,
title = {Multi-view reduced dimensionality K-means clustering with σ-norm and Schatten p-norm},
journal = {Pattern Recognition},
volume = {155},
pages = {110675},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110675},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004266},
author = {Xiangdong Zhang and Fangfang Li and Zhaoyang Shi and Ming Yang},
keywords = {Multi-view, Dimensionality reduction, Matrix σ-norm, Schatten -norm},
abstract = {Recently, multi-view high dimensional data obtained from diverse domains or various feature extractors has drawn great attention due to its reflection of different properties or distributions. In this paper, we propose a novel unsupervised multi-view clustering method, which is called Multi-View Reduced Dimensionality K-means clustering (MRDKM) and integrates the dimension reduction mechanism, σ-norm, Schatten p-norm, and multi-view K-means clustering. Moreover, an unsupervised optimization scheme was proposed to solve the minimization problem with good convergence properties. Comprehensive evaluations of five benchmark datasets and comparisons with several multi-view clustering algorithms demonstrate the superiority of the proposed work.}
}
@article{PAN2024110597,
title = {A unified framework for convolution-based graph neural networks},
journal = {Pattern Recognition},
volume = {155},
pages = {110597},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110597},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003480},
author = {Xuran Pan and Xiaoyan Han and Chaofei Wang and Zhuo Li and Shiji Song and Gao Huang and Cheng Wu},
keywords = {Laplacian optimization, Graph convolution network, Graph neural networks, Graph Fourier space, Oversmoothing},
abstract = {Graph Convolutional Networks (GCNs) have attracted a lot of research interest in machine learning, and many variants have been proposed recently. In this paper, we take a step forward to establish a unified framework for convolution-based graph neural networks, aiming to provide a systematic view of different GCN variants and deep understanding of the relations among them. Our key idea is formulating the basic graph convolution operation as an optimization problem in the graph Fourier space. Under this framework, a variety of popular GCN models, including vanilla-GCNs, attention-based GCNs and topology-based GCNs, can be interpreted as a similar optimization problem but with different regularizers. This novel perspective enables a better understanding of the similarities and differences among many widely used GCNs, and may inspire new model designs. As a showcase, we present a novel regularization technique under the proposed framework to tackle the oversmoothing problem in graph convolution. The effectiveness of newly designed model is validated empirically.}
}
@article{YUAN2024110678,
title = {A generalizable framework for low-rank tensor completion with numerical priors},
journal = {Pattern Recognition},
volume = {155},
pages = {110678},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110678},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004291},
author = {Shiran Yuan and Kaizhu Huang},
keywords = {Tensorial representations, Prior distributions, CP decomposition, Block coordinate descent, Missing data},
abstract = {Low-Rank Tensor Completion, a method which exploits the inherent structure of tensors, has been studied extensively as an effective approach to tensor completion. Whilst such methods attained great success, none have systematically considered exploiting the numerical priors of tensor elements. Ignoring numerical priors causes loss of important information regarding the data, and therefore prevents the algorithms from reaching optimal accuracy. Despite the existence of some individual works which consider ad hoc numerical priors for specific tasks, no generalizable frameworks for incorporating numerical priors have appeared. We present the Generalized CP Decomposition Tensor Completion (GCDTC) framework, the first generalizable framework for low-rank tensor completion that takes numerical priors of the data into account. We test GCDTC by further proposing the Smooth Poisson Tensor Completion (SPTC) algorithm, an instantiation of the GCDTC framework, whose performance exceeds current state-of-the-arts by considerable margins in the task of non-negative tensor completion, exemplifying GCDTC’s effectiveness. Our code is open-source.}
}
@article{MA2024110710,
title = {Momentum recursive DARTS},
journal = {Pattern Recognition},
volume = {156},
pages = {110710},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110710},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004618},
author = {Benteng Ma and Yanning Zhang and Yong Xia},
keywords = {Neural architecture search, Gradient estimation, Image recognition},
abstract = {DARTS has emerged as a popular method for neural architecture search (NAS) owing to its efficiency and simplicity. It employs gradient-based bi-level optimization to iteratively optimize the upper-level architecture parameters and lower-level super-network weights. The key challenge in DARTS is the accurate estimation of gradients for two-level object functions, leading to significant errors in gradient approximation. To address this issue, we propose a new approach, MR-DARTS, that incorporates a momentum term and a recursive scheme to improve gradient estimation. Specifically, we leverage historical information by using a running average of past observed gradients to enhance the quality of current gradient estimation in both upper-level and lower-level functions. Our theoretical analysis shows that the variance of our estimated gradient decreases with each iteration. By utilizing momentum and a recursive scheme, MR-DARTS effectively controls the error in stochastic gradient updates that result from inaccurate gradient estimation. Furthermore, we utilize the Neumann series approximation and Hessian Vector Product scheme to reduce computational requirements and memory usage. We evaluate our proposed method on several benchmarks and demonstrate its effectiveness through comprehensive experiments.}
}
@article{GUO2024110746,
title = {A restarted large-scale spectral clustering with self-guiding and block diagonal representation},
journal = {Pattern Recognition},
volume = {156},
pages = {110746},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110746},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004977},
author = {Yongyan Guo and Gang Wu},
keywords = {Spectral clustering, Restarting, Self-guiding, Block diagonal representation, Kernel trick, Nyström approximation},
abstract = {Spectral clustering, a prominent unsupervised machine learning method, involves a critical task of constructing a similarity matrix. In existing approaches, this matrix is either computed once for all or updated alternatively. However, the former struggles to capture comprehensive relationships among data points, and the latter is often impractical for large-scale problems. This study introduces a new clustering framework with self-guidance and a block diagonal representation, retaining valuable information from previous cycles. To our knowledge, this is the first application of such a framework to spectral clustering, with a key distinction being the reclassification of samples in each cycle. To reduce computational overhead, we employ a block diagonal representation with Nyström approximation for constructing the similarity matrix. Theoretical results justify the rationality of approximate computations in spectral clustering. Comprehensive experiments on benchmark datasets demonstrate the superiority of our proposed algorithms over state-of-the-art methods for large-scale clustering. Notably, our framework has the potential to enhance clustering algorithms, performing well even with a randomly chosen initial guess.}
}
@article{YAN2024110722,
title = {LM-Metric: Learned pair weighting and contextual memory for deep metric learning},
journal = {Pattern Recognition},
volume = {155},
pages = {110722},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110722},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004734},
author = {Shiyang Yan and Lin Xu and Xinyao Shu and Zhenyu Lu and Jialie Shen},
keywords = {Image retrieval, Metric learning, Normalizing flow, Policy gradient, Meta-learning},
abstract = {Deep Metric Learning (DML) is a crucial machine learning method in computer vision, and constructing an effective loss function. Average Precision (AP) is a well-known evaluation metric for image retrieval tasks. However, AP’s non-differentiable and non-decomposable nature constrains its potential for adoption as an optimization goal. We propose a deep metric learning method with learned parametric metric learning loss and a contextual memory block (LM-Metric) for large-scale image retrieval tasks, which overcome AP’s drawbacks and integrate AP within DML loss. We first introduce a parametric pairwise weighting scheme via policy gradient optimization and model the batch-wise inter-sample relationship via a Gated Recurrent Unit (GRU). Furthermore, a conditional Normalizing Flow-based contextual memory feature block to learn a compact single embedding for each image containing the contextual information during retrieval. We perform experiments on retrieval benchmark datasets and improve performance over the state-of-the-arts.}
}
@article{ZHOU2024110670,
title = {IRCNN: A novel signal decomposition approach based on iterative residue convolutional neural network},
journal = {Pattern Recognition},
volume = {155},
pages = {110670},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110670},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004217},
author = {Feng Zhou and Antonio Cicone and Haomin Zhou},
keywords = {Empirical mode decomposition, Adaptive signal decomposition, Signal local average, Convolutional neural network, Residual network},
abstract = {The decomposition of non-stationary signals is an important and challenging task in the field of signal time–frequency analysis. In the recent two decades, many decomposition methods have been proposed, inspired by the empirical mode decomposition method, first published by Huang et al. in 1998. However, they still have some limitations. For example, they are generally prone to boundary and mode mixing effects and are not very robust to noise. Inspired by the successful applications of deep learning, and given the lack in the literature of works in which deep learning techniques are used directly to decompose non-stationary signals into simple oscillatory components, we use the convolutional neural network, residual structure and nonlinear activation function to compute in an innovative way the local average of the signal, and study a new non-stationary signal decomposition method under the framework of deep learning. We discuss the training process of the proposed model and study the convergence analysis of the learning algorithm. In the experiments, we evaluate the performance of the proposed model from two points of view: the calculation of the local average and the signal decomposition. Furthermore, we study the mode mixing, noise interference, and orthogonality properties of the decomposed components produced by the proposed method, and compare it with the state-of-the-art ones. All results show that the proposed model allows for better handling boundary effect, mode mixing effect, robustness, and the orthogonality of the decomposed components than existing methods.}
}
@article{LONG2024110751,
title = {Semi-supervised clustering guided by pairwise constraints and local density structures},
journal = {Pattern Recognition},
volume = {156},
pages = {110751},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110751},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005028},
author = {Zhiguo Long and Yang Gao and Hua Meng and Yuxu Chen and Hui Kou},
keywords = {Semi-supervised clustering, Local density peaks, Pairwise constraint propagation, Inter-cluster conflict resolution},
abstract = {Clustering based on local density peaks and graph cut (LDP-SC) is one of the state-of-the-art algorithms in unsupervised clustering, which first divides the data set to be multiple local trees, and then aggregates these local trees to obtain the final clustering result. However, for complex data sets, there might exist data points from different classes in the same local tree. In this article, we use pairwise constraint information to resolve this issue and propose a semi-supervised local density peaks and graph cut based clustering algorithm (SLDPC). In particular, SLDPC proposes intra-cluster conflict resolution and inter-cluster conflict resolution steps to split the local trees which are inconsistent with the provided pairwise constraint information. Theoretically, we show that the two steps will finish in a finite number of operations and the split local trees will be consistent with the pairwise constraint information. Subsequently, root node redirection and noise filtering steps are designed to avoid the local trees becoming too fragmented. Finally, we exploit the E2CP algorithm to further improve the similarity matrix between local trees using the pairwise constraint information, and the spectral clustering algorithm is adopted to obtain the clustering result. Experiments on multiple widely used synthetic and real-world data sets show that SLDPC is superior to LDP-SC and several other semi-supervised prominent clustering algorithms for most of the cases.}
}
@article{TCHUITCHEU2024110734,
title = {Table representation learning using heterogeneous graph embedding},
journal = {Pattern Recognition},
volume = {156},
pages = {110734},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110734},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004850},
author = {Willy Carlos Tchuitcheu and Tan Lu and Ann Dooms},
keywords = {Document parsing, Table representation learning, Semantic meta-path, Heterogeneous graph embedding, Permutation invariance},
abstract = {Tables, especially when having complex layouts, contain rich semantic information. However, effectively learning from tables to uncover such semantic information remains challenging. The rapid progress in natural language processing does not necessarily correspond to equivalent advancements in table parsing, which often requires joint visual and language modeling. Indeed, humans can quickly derive semantic meaning from table entries by associating them with corresponding column and/or row headers. Motivated by this observation, we propose a new heterogeneous Graph-based Table Representation Learning (GTRL) framework. GTRL combines graph-based visual modeling with sequence-based language modeling to learn granular per-cell embeddings that are sensitive to the semantic meaning of cells within their corresponding table context. We systematically evaluate the proposed GTRL framework using two datasets: a new adhesive table benchmark comprising complex tables extracted from industrial documents for learning per-entry semantics, and a publicly available large-scale dataset that enables learning header semantics from column tables. Experimental results demonstrate the competitive performance of the proposed GTRL, which often exhibits reduced computational complexity compared to state-of-the-art table representation learning models.}
}
@article{DENG2024110682,
title = {G2-SCANN: Gaussian-kernel graph-based SLD clustering algorithm with natural neighbourhood},
journal = {Pattern Recognition},
volume = {155},
pages = {110682},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110682},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004333},
author = {Zhidong Deng and Jingyi Wang},
keywords = {Tree-like clustering algorithm, Shortest path length (SPL), Graph-based SLD, Natural neighbourhood, Clustering accuracy},
abstract = {For most clustering methods, not only the number of clusters must be set in advance, but also various hyperparameters such as initial centroids, number of nearest neighbours, the minimum number of points, neighbourhood radius, and cutoff distance all require pre-specification. As one of the most promising unsupervised learning methods in machine intelligence, existing clustering methods cannot simultaneously handle datasets with arbitrary shapes, different densities, distinct sizes, and overlapping. Background outliers and high dimensionality make clustering problems more challenging. In this paper, we propose a novel universal clustering methodology, called G2-SCANN, which yields the best clustering performance for all 30 synthetic and real datasets without any hyperparameter tuning if the exact number of clusters is known. Firstly, the shortest path length (SPL) in complex network or graph-based geodesic distance is used to give a locally backbone-structured description of graph vertex similarity. Accordingly, SPL-weighted local degree (SLD) is defined as vertex attributes of a SPL-weighted graph expressed by G2-SPL adjacency matrix with ε-natural neighbourhood. Secondly, the process of calculating SLD for every data point in a bottom-up way directly leads to division from a complete graph constituted by all data points to a group of SLD trees. This brings the interpretability and the elimination of lone trees. Thirdly, contrastive learning of largest SLD values for finding root vertices of each divisive tree is conducted and top-down category message is then transmitted from the root vertices to all the leaf ones of a SLD tree. It eventually produces tree-like clusters. Totally, the proposed G2-SCANN method leverages both local neighbouring similarity of data points and global information about data distribution and makes it perform better than other methods.}
}
@article{CAO2024110706,
title = {View-unaligned clustering with graph regularization},
journal = {Pattern Recognition},
volume = {155},
pages = {110706},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110706},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004576},
author = {Junfeng Cao and Wenhua Dong and Jing Chen},
keywords = {Multi-view clustering, View-unaligned problem, Latent embedding learning, Latent embedding alignment, Matrix factorization},
abstract = {In current multi-view clustering modeling scenarios, the cross-view correspondence of the data is generally presumed in advance. However, this assumption is inevitably violated in practical applications as each view is independently processed during data collection and transmission, thus resulting in the view-unaligned problem (VuP). The absence of cross-view correspondence between the data renders most existing multi-view clustering methods ineffective in addressing the VuP. To address this problem, we propose a novel view-unaligned clustering method, termed View-unaligned Clustering with Graph regularization (VuCG), which performs latent embedding learning on manifold, latent embedding alignment and partition generation in a one-stage manner. Specifically, we implement the latent embedding learning on manifold by seeking a matrix factorization respecting the graph structure, and perform latent embedding alignment by identifying the counterpart from a selected template of the shuffled embedding using global and local information of the data in the latent space. Additionally, we obtain the partition of the aligned data by applying the relaxed k-means algorithm to the aligned embeddings. Extensive experimental results on four practical datasets demonstrate the effectiveness of the proposed method.}
}
@article{DU2024110654,
title = {AnatPose: Bidirectionally learning anatomy-aware heatmaps for human pose estimation},
journal = {Pattern Recognition},
volume = {155},
pages = {110654},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110654},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004059},
author = {Songlin Du and Zhiwen Zhang and Takeshi Ikenaga},
keywords = {Human pose estimation, Human body structure modeling, Anatomical features, Bidirectional recurrent neural networks, Heatmap regression},
abstract = {Estimating human pose from images is the key to enabling machines to understand human actions. Existing works on human pose estimation mainly focus on designing more resultful deep neural networks to regress the locations of human joints. Although the human pose is obedient to anatomy and shows rich anatomical features, reasoning human body structure by machine in a complex environment is still an open problem. This paper proposes AnatPose which can effectively capture the structural dependency among human body parts by both deep neural network architecture and learning objectives: (1) For the deep neural network architecture, a bidirectional learning paradigm is proposed to learn body-part proportions and dependencies by organizing human body parts as sequential data. This innovation enables the messages to pass in a bidirectional way and makes the human body exchange information about each part deeper during training. (2) For the learning objective, the proposed AnatPose learns a probabilistic representation of multi-scale anatomical features, including keypoint heatmaps, bone heatmaps, and symmetry heatmaps. This innovation enables the multi-scale anatomical features to successfully capture the structural dependency at both low-level joints and high-level associations from the anatomical priors of the human body. Extensive experimental results demonstrate that the proposed AnatPose shows state-of-the-art performance on three challenging datasets. It achieves a PCK@0.2 detection rate of 95.2% on the LSP dataset, a PCKh@0.5 detection rate of 92.9% on the MPII dataset, and an mAP of 76.6% on the Microsoft COCO dataset. Benefiting from its state-of-the-art accuracy, the proposed approach is expected to be widely used in various human pose estimation-driven applications.}
}
@article{ZHOU2024110677,
title = {Linear Gaussian bounding box representation and ring-shaped rotated convolution for oriented object detection},
journal = {Pattern Recognition},
volume = {155},
pages = {110677},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110677},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400428X},
author = {Zhen Zhou and Yunkai Ma and Junfeng Fan and Zhaoyang Liu and Fengshui Jing and Min Tan},
keywords = {Oriented object detection, Oriented bounding box representation, Rotation-sensitive feature extraction, Gaussian distribution modeling, Rotated convolution},
abstract = {In oriented object detection, current representations of oriented bounding boxes (OBBs) often suffer from the boundary discontinuity problem. Methods of designing continuous regression losses do not essentially solve this problem. Although Gaussian bounding box (GBB) representation avoids this problem, directly regressing GBB is susceptible to numerical instability. We propose linear GBB (LGBB), a novel OBB representation. By linearly transforming the elements of GBB, LGBB avoids the boundary discontinuity problem and has high numerical stability. In addition, existing convolution-based rotation-sensitive feature extraction methods only have local receptive fields, resulting in slow feature aggregation. We propose ring-shaped rotated convolution (RRC), which adaptively rotates feature maps to arbitrary orientations to extract rotation-sensitive features under a ring-shaped receptive field, rapidly aggregating features and contextual information. Experimental results demonstrate that LGBB and RRC achieve state-of-the-art performance. Furthermore, integrating LGBB and RRC into various models effectively improves detection accuracy.}
}
@article{ZHANG2024110718,
title = {A cross-network node classification method in open-set scenario},
journal = {Pattern Recognition},
volume = {155},
pages = {110718},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110718},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004692},
author = {Yuhong Zhang and Yunlong Ji and Kui Yu and Xuegang Hu and Xindong Wu},
keywords = {Cross-network, Node classification, Open-set, Domain adaptation, Graph neural network},
abstract = {Cross-network node classification aims to classify the nodes of unlabeled target network using a labeled source network. Existing methods introduce domain adaptation to address representation discrepancy in closed-set scenario. However, the open-set scenario is widespread in applications, in which, the coexistence and interaction of representation discrepancy and label discrepancy pose a great challenge. To this end, we make the first attempt for cross-network node classification in open-set scenario and propose a novel method based on reconstruction. Firstly, the pseudo unknown class nodes from target network are reconstructed into source network, which addresses label discrepancy by transforming open-set into closed-set with K+1 classes. Secondly, the contrastive-center loss is introduced to enhance the node representations, which aims to identify the unknown nodes from known nodes in networks. And then the invariant representations are learned better to address representation discrepancy. Extensive experiments demonstrate the effectiveness of our method.}
}
@article{YONG2024110717,
title = {Two-stage zero-shot sparse hashing with missing labels for cross-modal retrieval},
journal = {Pattern Recognition},
volume = {155},
pages = {110717},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110717},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004680},
author = {Kailing Yong and Zhenqiu Shu and Hongbin Wang and Zhengtao Yu},
keywords = {Missing labels, Zero-shot sparse hashing, Cross-modal retrieval, Joint semantic similarity, Class attribute, Sample-wise fine-grained similarity, Clustering-wise similarity},
abstract = {Recently, zero-shot cross-modal hashing has gained significant popularity due to its ability to effectively realize the retrieval of emerging concepts within multimedia data. Although the existing approaches have shown impressive results, the following limitations still need to be solved: (1) Labels in large-scale multimodal datasets in real scenes are usually incomplete or partially missing. (2) The existing methods ignore the influence of features-wise low-level similarity and label distribution on retrieval performance. (3) The representation ability of dense hash codes limits its discriminative potential. To solve these issues, we introduce an effective cross-modal retrieval framework called two-stage zero-shot sparse hashing with missing labels (TZSHML). Specifically, we learn a classifier through the partially known labeled samples to predict the labels of unlabeled data. Then, we use the reliable information in the correctly marked labels to recover the missing labels. The predicted and recovered labels are combined to obtain more accurate labels for the samples with missing labels. In addition, we employ sample-wise fine-grained similarity and cluster-wise similarity to learn hash codes. Therefore, TZSHML ensures that more samples with similar semantics are clustered together. Besides, we apply high-dimensional sparse hash codes to explore richer semantic information. Finally, the drift and interaction terms are introduced into the learning of the hash function to further narrow the gap between different modalities. Extensive experimental results demonstrate the competitiveness of our approach over other state-of-the-art methods in zero-shot retrieval scenarios with missing labels. The source code of this paper can be obtained from https://github.com/szq0816/TZSHML.}
}
@article{MOCAER2024110733,
title = {Early gesture detection in untrimmed streams: A controlled CTC approach for reliable decision-making},
journal = {Pattern Recognition},
volume = {156},
pages = {110733},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110733},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004849},
author = {William Mocaër and Eric Anquetil and Richard Kulpa},
keywords = {Online action detection, Gesture recognition, Early recognition, 3D CNN, CTC},
abstract = {This paper focuses on the problem of online action detection for interactive systems, with a special emphasis on earliness. Online Action Detection (OAD) refers to the challenging task of recognizing gestures in untrimmed, streaming videos where the actions occur in unpredictable orders and durations. To address these challenges, we present a skeleton-based system for OAD incorporating a decision mechanism to accurately detect ongoing gestures. This allows us to provide instance-level output, achieving a high level of stream understanding. This mechanism relies on a novel Connectionist Temporal Classification (CTC) loss design that restricts the path possibilities according to the action boundaries. We also present a mechanism to tune the trade-off between accuracy and earliness according to the needs of the interactive system using a weighted label prior. This system includes a 3D CNN network, referred to as DOLT-C3D, exploiting the spatial–temporal information provided by the euclidean skeleton representation. We extensively evaluate our approach on eight publicly available datasets, demonstrating its superior performance compared to state-of-the-art methods in terms of both accuracy and earliness. We also successfully applied our approach to early 2D gestures detection. Furthermore, our system shows real-time performance, making it a suitable choice for interactive systems.}
}
@article{ZHAO2024110709,
title = {Cross-lingual font style transfer with full-domain convolutional attention},
journal = {Pattern Recognition},
volume = {155},
pages = {110709},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110709},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004606},
author = {Hui-huang Zhao and Tian-le Ji and Paul L. Rosin and Yu-Kun Lai and Wei-liang Meng and Yao-nan Wang},
keywords = {Cross-lingual, Full-domain convolutional attention, Multi-layer perceptual discriminator, Font style transfer},
abstract = {In this paper, we propose a new cross-lingual font style transfer model, FCAGAN, which enables font style transfer between different languages by observing a small number of samples. Most previous work has been on style transfer of different fonts for single language content, but in our task we can learn the font style of one language and migrate it to another. We investigated the drawbacks of related studies and found that existing cross-lingual approaches cannot perfectly learn styles from other languages and maintain the integrity of their own content. Therefore, we designed a new full-domain convolutional attention (FCA) module in combination with other modules to better learn font styles, and a multi-layer perceptual discriminator to ensure character integrity. Experiments show that using this model provides more satisfying results than the current cross-lingual font style transfer methods. Code can be found at https://github.com/jtlxlf/FCAGAN.}
}
@article{WU2024110680,
title = {AMMD: Attentive maximum mean discrepancy for few-shot image classification},
journal = {Pattern Recognition},
volume = {155},
pages = {110680},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110680},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400431X},
author = {Ji Wu and Shipeng Wang and Jian Sun},
keywords = {Few-shot learning, Metric learning, Attentive MMD},
abstract = {Metric-based methods have attained promising performance for few-shot image classification. Maximum Mean Discrepancy (MMD) is a typical distance between distributions, requiring to compute expectations w.r.t. data distributions. In this paper, we propose Attentive Maximum Mean Discrepancy (AMMD) to measure the distances between query images and support classes for few-shot classification. Each query image is classified as the support class with minimal AMMD distance. The proposed AMMD assists MMD with distributions adaptively estimated by an Attention-based Distribution Generation Module (ADGM). ADGM is learned to put more mass on more discriminative features, which makes the proposed AMMD distance emphasize discriminative features and overlook spurious features. Extensive experiments show that our AMMD achieves competitive or state-of-the-art performance on multiple few-shot classification benchmark datasets. Code is available at https://github.com/WuJi1/AMMD.}
}
@article{SONG2024110648,
title = {Prompt-guided DETR with RoI-pruned masked attention for open-vocabulary object detection},
journal = {Pattern Recognition},
volume = {155},
pages = {110648},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110648},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003996},
author = {Hwanjun Song and Jihwan Bang},
keywords = {Object detection, Open-vocabulary detection, OVD, Transformer},
abstract = {Prompt-OVD is an efficient and effective DETR-based framework for open-vocabulary object detection that utilizes class embeddings from CLIP as prompts, guiding the Transformer decoder to detect objects in base and novel classes. Additionally, our RoI-pruned masked attention helps leverage the zero-shot classification ability of the Vision Transformer-based CLIP, resulting in improved detection performance at a minimal computational cost. Our experiments on the OV-COCO and OV-LVIS datasets demonstrate that Prompt-OVD achieves an impressive 21.2 times faster inference speed than the first end-to-end open-vocabulary detection method (OV-DETR), while also achieving higher APs than four two-stage methods operating within similar inference time ranges. We release the code at https://github.com/DISL-Lab/Prompt-OVD.}
}
@article{KUMAR2024110721,
title = {TSANet: Forecasting traffic congestion patterns from aerial videos using graphs and transformers},
journal = {Pattern Recognition},
volume = {155},
pages = {110721},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110721},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004722},
author = {K. Naveen Kumar and Debaditya Roy and Thakur Ashutosh Suman and Chalavadi Vishnu and C. Krishna Mohan},
keywords = {Spatio-temporal graphs, Transformers, Sequence modelling, Sequence estimation},
abstract = {Forecasting traffic congestion patterns in lane-less traffic scenarios is a complex task because of the combination of high & irregular vehicle densities, fluctuating speeds, and the presence of environmental obstacles. Existing techniques like vehicle counting and density prediction, which successfully estimate congestion in lane-based traffic, are unsuitable for lane-less traffic scenarios due to the irregular and unpredictable nature of traffic density patterns. To overcome these challenges, we propose traffic states to measure congestion patterns in lane-less traffic scenarios. Each traffic state is characterized by the spatio-temporal distribution of neighbouring road users, including vehicles and motorcyclists. We employ traffic graphs to capture the spatial distribution of neighbouring road users. Also, we propose a novel method for the automated construction of traffic graphs by leveraging the detection and tracking of individual road users in aerial videos. Further, in order to incorporate the temporal distribution, we utilize a transformer model to capture the evolution of spatial traffic graphs over time. This enables us to forecast future spatio-temporal distributions and their associated traffic states. Our proposed model, named Traffic State Anticipation Network (TSANet), can effectively forecast future traffic states by analysing sequences of current traffic graphs, thereby enhancing our understanding of evolving traffic patterns in lane-less scenarios. Also, to address the lack of publicly available lane-less traffic datasets, we introduce EyeonTraffic (EoT), a large-scale lane-less traffic dataset containing three hours of aerial videos captured at three busy intersections in Ahmedabad city, India. Experimental results on the EoT dataset demonstrate the efficacy of our proposed TSANet in effectively anticipating traffic states across diverse spatial regions within an intersection. In addition, we also show that TSANet generalizes well for previously unseen intersections, making it suitable for analysing various traffic scenarios without the need for explicit training, thereby enhancing its practical applicability.}
}
@article{QUAN2024110745,
title = {Global contrast-masked autoencoders are powerful pathological representation learners},
journal = {Pattern Recognition},
volume = {156},
pages = {110745},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110745},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004965},
author = {Hao Quan and Xingyu Li and Weixing Chen and Qun Bai and Mingchen Zou and Ruijie Yang and Tingting Zheng and Ruiqun Qi and Xinghua Gao and Xiaoyu Cui},
keywords = {Self-supervised learning, Representation learning, Pathological image},
abstract = {Using digital pathology slide scanning technology, artificial intelligence algorithms, particularly deep learning, have achieved significant results in the field of computational pathology. Compared to other medical images, pathology images are more difficult to annotate, and thus, there is an extreme lack of available datasets for conducting supervised learning to train robust deep learning models. In this paper, we introduce a self-supervised learning (SSL) model, the Global Contrast-masked Autoencoder (GCMAE), designed to train encoders to capture both local and global features of pathological images and significantly enhance the performance of transfer learning across datasets. Our study demonstrates the capability of the GCMAE to learn transferable representations through extensive experiments on three distinct disease-specific hematoxylin and eosin (H&E)-stained pathology datasets: Camelyon16, NCT-CRC, and BreakHis. Moreover, we propose an effective automated pathology diagnosis process based on the GCMAE for clinical applications. The source code of this paper is publicly available at https://github.com/StarUniversus/gcmae.}
}
@article{ZHU2024110693,
title = {CMIGNet: Cross-Modal Inverse Guidance Network for RGB-Depth salient object detection},
journal = {Pattern Recognition},
volume = {155},
pages = {110693},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110693},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004448},
author = {Hegui Zhu and Jia Ni and Xi Yang and Libo Zhang},
keywords = {Cross-modal, RGB-depth, Salient object detection, Feature guidance, Transformer},
abstract = {Currently, the majority of RGB-Depth salient object detection (SOD) methods utilize the encoder–decoder architecture. However, they often fail to utilize the encoding and decoding features fully. This paper rethinks the differences and correlations between them and proposes the Cross-Modal Inverse Guidance Network (CMIGNet) for SOD. Specifically, a Multi-level Feature Guidance Enhancement (MFGE) module is integrated into every layer of the foundational network. It employs a high-level decoding feature to guide the low-level RGB and depth encoding features, facilitating the rapid identification of salient regions and noise removal. The dual-stream encoding features guided by the MFGE module are combined using the proposed Dual-Stream Interactive Fusion (DSIF) module. It could simultaneously reduce dependence on two modal features during the fusion process. Thus, the impact on the results can be reduced in complex scenes when one modality is absent or confusing. Finally, the edge information is supplemented using the proposed Edge Refinement Awareness (ERA) module to generate the final salient map. Comparisons on seven widely used and one latest challenging RGB-D datasets show that the performance of the proposed CMIGNet is highly competitive with the state-of-the-art RGB-Depth SOD models. Additionally, our model is lighter and faster.}
}
@article{LI2024110705,
title = {A Transformer-based visual object tracker via learning immediate appearance change},
journal = {Pattern Recognition},
volume = {155},
pages = {110705},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110705},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004564},
author = {Yifan Li and Xiaotao Liu and Dian Yuan and Jiaoying Wang and Peng Wu and Jing Liu},
keywords = {Transformer, Attention mechanism, Visual object tracking, Appearance change},
abstract = {Transformer has shown its great strength in visual object tracking due to its effective attention mechanism, but most prevailing transformer-based trackers only explore temporal information frame by frame, thus overlooking the rich context information inherent in videos. To alleviate this problem, we propose a transformer-based tracker via learning immediate appearance change information in videos, called IAC-tracker. The proposed tracker enhances the perception of the immediate motion state to improve the performance of single target tracking. IAC-tracker contains three key components: a spatial information extractor (SIE) with a superior attention mechanism to progressively extract spatial information, a temporal information extractor (TIE) with a designed temporal attention mechanism to progressively learn target immediate appearance change, and a novel spatial–temporal context enhanced fusion module integrating the information from SIE and TIE to prepare for the final prediction head. Comparison experiments with state-of-the-art trackers on six challenging datasets demonstrate the superior performance of IAC-tracker with real-time running speed.}
}
@article{FANG2024110653,
title = {Prototype learning for adversarial domain adaptation},
journal = {Pattern Recognition},
volume = {155},
pages = {110653},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110653},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004047},
author = {Yuchun Fang and Chen Chen and Wei Zhang and Jiahua Wu and Zhaoxiang Zhang and Shaorong Xie},
keywords = {Domain adaptation, Transfer learning, Unsupervised learning, Deep learning},
abstract = {Adversarial learning has been widely used in recent years to address the issue of domain shift in domain adaptation. However, this approach focuses on global cross-domain alignment and overlooks the alignment of class boundaries. To tackle this limitation, we introduce a novel method called PLADA. PLADA leverages prototype learning to align category distributions across domains. The prototypes in PLADA represent the source category distribution, which is constructed using labelled data and transferred to the target domain. In addition to adversarial learning for global domain-invariant feature learning, we propose the weighted prototype loss (WPL) to embed prototype information. WPL transforms the local category distribution alignment problem into a distance measurement between the prediction and prototypes, resulting in a more discriminative representation. Experimental results demonstrate that our proposed model performs comparably well on multiple classic domain adaptation tasks, showcasing the potential of PLADA.}
}
@article{YI2024110750,
title = {Prototype rectification for zero-shot learning},
journal = {Pattern Recognition},
volume = {156},
pages = {110750},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110750},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005016},
author = {Yuanyuan Yi and Guolei Zeng and Bocheng Ren and Laurence T. Yang and Bin Chai and Yuxin Li},
keywords = {Zero-shot learning, Image recognition, Meta-learning, Contrastive learning, Manifold alignment},
abstract = {Learning to recognize unseen objects is the goal of zero-shot learning (ZSL), building on transferring the class-level semantic descriptions. Previous methods devote to bridging the instance-level objects with class-level semantics through feature generation or co-embedding, neglecting prototype-level and distribution-level associations, which is not conducive to narrowing the visual-semantic gap. This paper yields a novel prototype rectification framework for ZSL, termed PRZSL, which is dedicated to learning and calibrating the dual prototype distributions in a meta-domain. We first propose a contrastive embedding module with a compatibility loss and an angular loss to make inter-class prototypes well-separated. We further collaboratively rectify the dual prototypes by injecting the prototype distribution information of another modality, boosting the visual-semantic alignment at the distribution level. Unlike previous methods that anchor the semantic position, semantic prototypes also participate in collaborative updates, thereby promoting alignment from semantic to vision. Comprehensive experimental results on five zero-shot benchmarks demonstrate that our proposed method can achieve competitive performance compared with the state-of-the-art methods.}
}
@article{SUN2024110681,
title = {A lie group semi-supervised FCM clustering method for image segmentation},
journal = {Pattern Recognition},
volume = {155},
pages = {110681},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110681},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004321},
author = {Haocheng Sun and Li Liu and Fanzhang Li},
keywords = {Image segmentation, Semi-supervised learning, Lie group manifold, Lie group machine learning, Fuzzy clustering},
abstract = {As an unsupervised clustering method with low overhead, Fuzzy C-means (FCM) clustering has been widely used in a variety of image segmentation tasks. However, existing FCM clustering methods are sensitive to image noises and are either suffer from losing of image detail or falling into local optima in identifying cluster centers. Aiming at these problems, this paper proposes a Lie group semi-supervised FCM (LieSSFCM) clustering method for image segmentation. The method maps the input image from Euclidean space to Lie group manifold by representing each image pixel as a matrix Lie group and calculates geodesic distances between group elements and cluster centers on Lie group manifold. Prior information of the image and neighborhood relationships of pixels are used to guide the initialization and constrain the update of cluster centers and the corresponding fuzzy membership matrix. The proposed LieSSFCM has been validated against two medical image datasets and was compared with seven FCM clustering methods. Experimental results along with a systematic evaluation demonstrated that the method was superior in segmentation accuracy both visually and statistically, robustness to noises, adaptability to different tasks, and stability while maintaining a moderate computational complexity.}
}
@article{MA2024110656,
title = {Dual feature disentanglement for face anti-spoofing},
journal = {Pattern Recognition},
volume = {155},
pages = {110656},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110656},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004072},
author = {Yimei Ma and Jianjun Qian and Jun Li and Jian Yang},
keywords = {Face anti-spoofing, Feature disentangle, Liveness feature, Spoof pattern, Feature projection},
abstract = {Domain generalization for Face Anti-Spoofing (FAS) is increasingly crucial with face attacks across unseen domains. Some existing face anti-spoofing methods aim to disentangle spoof feature for both seen and unseen scenarios. However, it is still a challenging problem to capture spoof feature from facial image, because of spoof pattern are often mixed with various facial attributes such as identity, expression, age and gender. To solve the above problem, we propose a Dual Feature Disentanglement Network (DFDN), which leverages feature projection scheme based on domain-invariant feature in conjunction with rearranging the facial structure to learn spoof feature jointly. Specifically, DFDN introduces the local mask module and domain discriminator to enhance the domain-invariant feature. Based on this, we employ the geometry projection relations to achieve the refined spoof pattern via the feature projection. Meanwhile, we disrupt facial structure to weaken face attributes feature learning and guide CNNs to learn spoof feature. Subsequently, consistent regularization is developed to reduce the gap between the above two kinds of spoof pattern by introducing optimal transport and cosine similarity. Extensive experiments and visualizations demonstrate the advantages of our DFDN over the state-of-the-art methods.}
}
@article{ALY2024110740,
title = {Boosted multilayer feedforward neural network with multiple output layers},
journal = {Pattern Recognition},
volume = {156},
pages = {110740},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110740},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004916},
author = {Hussein Aly and Abdulaziz K. Al-Ali and Ponnuthurai Nagaratnam Suganthan},
keywords = {Multiple output layers, Layer-wise training, Boosting, Ensemble classifier, Tabular data classification},
abstract = {This research introduces the Boosted Ensemble deep Multi-Layer Layer Perceptron (EdMLP) architecture with multiple output layers, a novel enhancement for the traditional Multi-Layer Perceptron (MLP). By adopting a layer-wise training approach, EdMLP enables the integration of boosting techniques within a single model, treating each layer as a weak learner, resulting in substantial performance gains. Additionally, the inclusion of layer-wise hyperparameter tuning allows optimization of individual layers thereby reducing the tuning time. Furthermore, the ensemble deep architecture’s versatility can be extended to other neural network-based models, such as the Self Normalized Network (SNN) where experiments demonstrate substantial performance enhancements yielded by the EdSNN compared to the standard original SNN model. This research underscores the potential of the EdMLP, and the Ed architecture in general as a powerful tool for improving the performance of various multilayer feedforward neural network models. The source code of this work is publicly accessible from the authors GitHub.}
}
@article{YANG2024110711,
title = {Multi-threshold deep metric learning for facial expression recognition},
journal = {Pattern Recognition},
volume = {156},
pages = {110711},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110711},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400462X},
author = {Wenwu Yang and Jinyi Yu and Tuo Chen and Zhenguang Liu and Xun Wang and Jianbing Shen},
keywords = {Facial expression recognition, Triplet loss learning, Multiple thresholds},
abstract = {Feature representations generated through triplet-based deep metric learning offer significant advantages for facial expression recognition (FER). Each threshold in triplet loss inherently shapes a distinct distribution of inter-class variations, leading to unique representations of expression features. Nonetheless, pinpointing the optimal threshold for triplet loss presents a formidable challenge, as the ideal threshold varies not only across different datasets but also among classes within the same dataset. In this paper, we propose a novel multi-threshold deep metric learning approach that bypasses the complex process of threshold validation and markedly improves the effectiveness in creating expression feature representations. Instead of choosing a single optimal threshold from a valid range, we comprehensively sample thresholds throughout this range, which ensures that the representation characteristics exhibited by the thresholds within this spectrum are fully captured and utilized for enhancing FER. Specifically, we segment the embedding layer of the deep metric learning network into multiple slices, with each slice representing a specific threshold sample. We subsequently train these embedding slices in an end-to-end fashion, applying triplet loss at its associated threshold to each slice, which results in a collection of unique expression features corresponding to each embedding slice. Moreover, we identify the issue that the traditional triplet loss may struggle to converge when employing the widely-used Batch Hard strategy for mining informative triplets, and introduce a novel loss termed dual triplet loss to address it. Extensive evaluations demonstrate the superior performance of the proposed approach on both posed and spontaneous facial expression datasets.}
}
@article{KANWAL2024110700,
title = {Incomplete RGB-D salient object detection: Conceal, correlate and fuse},
journal = {Pattern Recognition},
volume = {155},
pages = {110700},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110700},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004515},
author = {Samra Kanwal and Imtiaz Ahmad Taj},
keywords = {Salient object detection, Incomplete-modality learning problem, Latent correlation representation, Depth quality assessment},
abstract = {Integrating RGB and depth information has advanced salient object detection but low-quality depth maps lead to inaccurate results. The current methods address this issue by employing either a weighting approach or by estimating depth images directly from RGB images. However, these methods face limitations in low-contrast RGB scenarios and fluctuating illumination conditions. To overcome these limitations, a new model has been proposed that discards low-quality depth images and formulates an incomplete multi-modality salient object detection learning. To the best of our knowledge, this is the first incomplete multi-modality salient object detection model that is capable of describing the common latent multi-modality correlation representation between RGB and depth modalities. The model acquires a resilient representation of multiple modalities even when some depth samples are missing due to noise or data scarcity. The proposed approach follows a three-step process: concealing modality-specific representation, correlating common latent representation, and fusing multilevel representation. We processed shallow and deep features separately in Shallow Common Latent Representation (SCLR) block and Deep Common Latent Representation (DCLR) block, respectively. The model outperforms 14 state-of-the-art saliency detectors on 6 benchmark datasets.}
}
@article{HEDEGAARD2024110724,
title = {Structured pruning adapters},
journal = {Pattern Recognition},
volume = {156},
pages = {110724},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110724},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004758},
author = {Lukas Hedegaard and Aman Alok and Juby Jose and Alexandros Iosifidis},
keywords = {Transfer learning, Adapters, Pruning, Structured pruning, Parameter efficient, Image classification, Convolutional neural network, Vision transformer},
abstract = {Adapters are a parameter-efficient alternative to fine-tuning, which augment a frozen base network to learn new tasks. Yet, the inference of the adapted model is often slower than the corresponding fine-tuned model. To improve on this, we introduce the concept of Structured Pruning Adapters (SPAs), a family of compressing, task-switching network adapters, that accelerate and specialize networks using tiny parameter sets and structured pruning. Specifically, we propose the Structured Pruning Low-rank Adapter (SPLoRA) and the Structured Pruning Residual Adapter (SPPaRA) and evaluate them on a suite of pruning methods, architectures, and image recognition benchmarks. Compared to regular structured pruning with fine-tuning, SPLoRA improves image recognition accuracy by 6.9% on average for ResNet50 while using half the parameters at 90% pruned weights. Alternatively, a SPLoRA augmented model can learn adaptations with 17× fewer parameters at 70% pruning with 1.6% lower accuracy. For ViT-b/16 models, SPLoRA improves accuracy by an average of 43%-points at 75% pruned weights while learning 6.8× fewer parameters. Our experimental code and Python library of adapters are available at www.github.com/lukashedegaard/structured-pruning-adapters.}
}
@article{LI2024110736,
title = {A simple scheme to amplify inter-class discrepancy for improving few-shot fine-grained image classification},
journal = {Pattern Recognition},
volume = {156},
pages = {110736},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110736},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004874},
author = {Xiaoxu Li and Zijie Guo and Rui Zhu and Zhanyu Ma and Jun Guo and Jing-Hao Xue},
keywords = {Few-shot learning, Fine-grained image classification, Metric-based methods},
abstract = {Few-shot image classification is a challenging topic in pattern recognition and computer vision. Few-shot fine-grained image classification is even more challenging, due to not only the few shots of labelled samples but also the subtle differences to distinguish subcategories in fine-grained images. A recent method called task discrepancy maximisation (TDM) can be embedded into the feature map reconstruction network (FRN) to generate discriminative features, by preserving the appearance details through reconstructing the query image and then assigning higher weights to more discriminative channels, producing the state-of-the-art performance for few-shot fine-grained image classification. However, due to the small inter-class discrepancy in fine-grained images and the small training set in few-shot learning, the training of FRN+TDM can result in excessively flexible boundaries between subcategories and hence overfitting. To resolve this problem, we propose a simple scheme to amplify inter-class discrepancy and thus improve FRN+TDM. To achieve this aim, instead of developing new modules, our scheme only involves two simple amendments to FRN+TDM: relaxing the inter-class score in TDM, and adding a centre loss to FRN. Extensive experiments on five benchmark datasets showcase that, although embarrassingly simple, our scheme is quite effective to improve the performance of few-shot fine-grained image classification. The code is available at https://github.com/Airgods/AFRN.git.}
}
@article{JIANG2024110684,
title = {Multi-label feature selection using self-information in divergence-based fuzzy neighborhood rough sets},
journal = {Pattern Recognition},
volume = {155},
pages = {110684},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110684},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004357},
author = {Jiefang Jiang and Xianyong Zhang and Zhong Yuan},
keywords = {Divergence-based fuzzy neighborhood rough sets, Multi-label learning, Feature selection, Self-information, Feature significance},
abstract = {Multi-label feature selection corresponds to pattern recognition and knowledge mining, and its application has been expanded to different scenarios. As an excellent processing platform for uncertain and ambiguous information, divergence-based fuzzy rough sets (Div-FRSs) have been proposed and applied to feature selection. However, there are three critical problems to be solved when applying Div-FRSs to multi-label learning. The first is how to effectively dispose the noise produced by features in multi-label data. The second is how to synthetically consider the relevance among all labels. The last is how to thoroughly mine the uncertainty brought by upper approximations neglected in Div-FRSs existing researches. To address these issues, this study presents a new divergence-based fuzzy neighborhood rough set model (Div-FNRSs) for multi-label learning using self-information. First, the divergence-based fuzzy neighborhood relation and class are gradually raised to manage the noise in multi-label data, and fuzzy decision is introduced to dispose all labels as a whole. Combining them together, a new model Div-FNRSs is constructed. Then, divergence-based fuzzy neighborhood self-information containing upper approximations and lower approximations is designed to depict distinguishing ability of features through three-level uncertainty measure establishment and granulation property exploration. Furthermore, feature significance for choosing the optimal features is given and it motivates a heuristic feature-selection algorithm DivFNSI-FS. Finally, data experiments are completed to validate DivFNSI-FS effectiveness with six state-of-the-art multi-label feature selection approaches on fourteen multi-label datasets. A conclusion can be drawn that DivFNSI-FS outperforms existing algorithms to obtain better performance on eight commonly-used evaluation indexes.}
}
@article{LIN2024110752,
title = {Efficient and lightweight convolutional neural network architecture search methods for object classification},
journal = {Pattern Recognition},
volume = {156},
pages = {110752},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110752},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400503X},
author = {Chuen-Horng Lin and Tsung-Yi Chen and Huan-Yu Chen and Yung-Kuan Chan},
keywords = {Convolutional neural network (CNN), Architecture search, Lightweight, Long short-term memory (LSTM) network},
abstract = {Determining the architecture of deep learning models is a complex task. Several automated search techniques have been proposed, but these methods typically require high-performance graphics processing units (GPUs), manual parameter adjustments, and specific training approaches. This study introduces an efficient, lightweight convolutional neural network architecture search approach tailored for object classification. It features an optimized search space design and a novel controller design. This study introduces a refined search space design incorporating optimizations in both spatial and operational aspects. The focus is on the synergistic integration of convolutional units, dimension reduction units, and the stacking of Convolutional Neural Network (CNN) architectures. To enhance the search space, ShuffleNet modules are integrated, reducing the number of parameters and training time. Additionally, BlurPool is implemented in the dimension reduction unit operation to achieve translational invariance, alleviate the gradient vanishing problem, and optimize unit compositions. Moreover, an innovative controller model, Stage LSTM, is proposed based on Long Short-Term Memory (LSTM) to generate lightweight architectural sequences. In conclusion, the refined search space design and the Stage LSTM controller model are synergistically combined to establish an efficient and lightweight architecture search technique termed Stage and Lightweight Network Architecture Search (SLNAS). The experimental results highlight the superior performance of the optimized search space design, primarily when implemented with the Stage LSTM controller model. This approach shows significantly improved accuracy and stability compared to random, traditional LSTM, and Genetic Algorithm (GA) controller models, with statistically significant differences. Notably, the Stage LSTM controller excels in accuracy while producing models with fewer parameters within the expanded architecture search space. The study adopts the Stage LSTM controller model due to its ability to approximate optimal sequence structures, particularly when combined with the optimized search space design, referred to as SLNAS. SLNAS's performance is evaluated through experiments and comparisons with other Neural Architecture Search (NAS) and object classification methods from different researchers. These experiments consider model parameters, hardware resources, model stability, and multiple datasets. The results show that SLNAS achieves a low error rate of 2.86 % on the CIFAR-10 dataset after just 0.2 days of architecture search, matching the performance of manually designed models but using only 2 % of the parameters. SLNAS consistently demonstrates robust performance across various image classification domains, with an approximate parameter count 700,000. To summarize, SLNAS emerges as a highly effective automated network architecture search method tailored for image classification. It streamlines the model design process, making it accessible to researchers without specialized knowledge in deep learning. Optimizing this method unlocks the full potential of deep learning across diverse research areas. Interested parties can publicly access the source code and pre-trained models through the following link: https://github.com/huanyu-chen/LNASG-and-SLNAS-model.}
}
@article{KHAN2024110712,
title = {Beyond local patches: Preserving global–local interactions by enhancing self-attention via 3D point cloud tokenization},
journal = {Pattern Recognition},
volume = {155},
pages = {110712},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110712},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004631},
author = {M.Q. Khan and M. Shahzad and S.A. Khan and M.M. Fraz and X.X. Zhu},
keywords = {3D point cloud, Transformer, Self-attention, Segmentation, Classification},
abstract = {Transformer-based architectures have recently shown impressive performance on various point cloud understanding tasks such as 3D object shape classification and semantic segmentation. Particularly, this can be attributed to their self-attention mechanism, which has the ability to capture long-range dependencies. However, current methods have constrained it to operate in local patches due to its quadratic memory constraints. This hinders their generalization ability and scaling capacity due to the loss of non-locality in early layers. To tackle this issue, we propose a window-based transformer architecture that captures long-range dependencies while aggregating information in the local patches. We do this by interacting each window with a set of global point cloud tokens — a representative subset of the entire scene — and augmenting the local geometry through a 3D Histogram of Oriented Gradients (HOG) descriptor. Through a series of experiments on segmentation and classification tasks, we show that our model exceeds the state-of-the-art on S3DIS semantic segmentation (+1.67% mIoU), ShapeNetPart part segmentation (+1.03% instance mIoU) and performs competitively on ScanObjectNN 3D object classification.11The code and trained models shall be made publicly available.}
}
@article{SONG2024110672,
title = {The spiking neural network based on fMRI for speech recognition},
journal = {Pattern Recognition},
volume = {155},
pages = {110672},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110672},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004230},
author = {Yihua Song and Lei Guo and Menghua Man and Youxi Wu},
keywords = {functional Magnetic Resonance Imaging, Functional brain network, Spiking neural network, Speech recognition, Neuronal firing activity, Neural information transmission},
abstract = {The structure of the human brain has evolved to achieve extraordinary computing power through continuous refinement by natural selection. At present, the topology of brain-like model lacks biological plausibility. In this paper, a new brain-like model is proposed, called fMRI-SNN, which is a spiking neural network (SNN) constrained by the topology of a functional brain network from human functional Magnetic Resonance Imaging (fMRI) data. To verify its performance, this fMRI-SNN is applied to speech recognition. Our results indicate that the recognition accuracy of fMRI-SNN is superior to that of other SNNs and reported methods, and exhibits stronger performance on more difficult speech recognition tasks. Our discussion on recognition mechanism finds the advantage of fMRI-SNN is that the differences in its neuronal firing patterns are greater than those of other SNNs, since it has better information transmission ability.}
}
@article{LEI2024110748,
title = {CORE: Learning consistent ordinal representations with convex optimization for image ordinal estimation},
journal = {Pattern Recognition},
volume = {156},
pages = {110748},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110748},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004990},
author = {Yiming Lei and Zilong Li and Yangyang Li and Junping Zhang and Hongming Shan},
keywords = {Ordinal regression, Image ordinal estimation, Convex optimization, Dual decomposition},
abstract = {Image ordinal estimation is to estimate the ordinal label of a given image. Existing methods primarily rely on ordinal regression, mapping feature representations directly to ordinal labels. However, these methods often struggle to preserve the inherent order within the learned feature representations. To this end, this paper proposes learning intrinsic Consistent Ordinal REpresentations (CORE), a novel approach that learns intrinsic ordinal relationships directly from ground-truth labels. First, it constructs an ordinal manifold using an ordinal totally ordered set (toset) distribution (OTD), capturing the inherent order of labels while regularizing feature embeddings. Second, the CORE leverages the toset distribution to convert both feature representations and labels into a unified embedding space, enabling consistent manifold alignment. Third, CORE employs an ordinal prototype-constrained convex programming formulation with dual decomposition, minimizing the Kullback–Leibler (KL) divergence between the toset distributions of labels and feature representations. Extensive experiments demonstrate that CORE, when combined with existing deep ordinal regression methods, significantly improves their performance in preserving ordinal relationships and achieves superior quantitative results across four real-world scenarios.}
}
@article{GUAN2024110696,
title = {Global–local consistent semi-supervised segmentation of histopathological image with different perturbations},
journal = {Pattern Recognition},
volume = {155},
pages = {110696},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110696},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004473},
author = {Xi Guan and Qi Zhu and Liang Sun and Junyong Zhao and Daoqiang Zhang and Peng Wan and Wei Shao},
keywords = {Pathological image, Semi-supervised segmentation, Generative adversarial learning, Global–local consistent},
abstract = {A histopathological image is a microscopic image applied to examine cellular and tissue structures and identify any abnormalities or disease processes. Histopathological image segmentation is a prerequisite step for analyzing histopathological images that can divide an image into meaningful regions or objects to accurately classify and analyze tissue structures, cellular regions, or particular histological entities. However, the existing deep learning based pathological image segmentation methods require huge annotation efforts from the pathologists, which is labor-intensive and time-consuming. In this scenario, it has become a hotspot to leverage abundantly available unlabeled data to help learn segmentation models given limited labeled data. In this paper, we propose a global–local consistent semi-supervised segmentation (GLCS) model that enforce the consistency of the segmentation results with weak and strong perturbations on unlabeled data. In GLCS, we firstly generate different weak perturbations for each unlabeled sample, and then add a regularization term to ensure the segmentation consistency among different weak perturbations. Next, different from the existing studies applying the regression methods to match the segmentation results among different perturbations, our methods are based on the generative adversarial learning that can keep the global structure consistency among unlabeled data with different strength of perturbations. Finally, we also add a patch-correlation based regularization term to preserve the local structure similarity among different perturbations images. We validate our GLCS on three datasets, i.e. Glas, Crag and MoNuSeg. The experimental results show that our method can achieve to the dice ratio of 90.35, 82.61 and 81.60 with 1:1 proportion of labeled data, which are significantly superior to the state-of-the-art semi-supervised histopathological image segmentation methods. Our code is public available at https://github.com/ISBELLAG/GLCS.}
}
@article{YAN2024110655,
title = {Toward comprehensive and effective palmprint reconstruction attack},
journal = {Pattern Recognition},
volume = {155},
pages = {110655},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110655},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004060},
author = {Licheng Yan and Fei Wang and Lu Leng and Andrew Beng Jin Teoh},
keywords = {Palmprint recognition, Reconstruction attack, Double reuse training strategy, Scale-adaptive multi-texture complementarity, Generative adversarial network},
abstract = {The challenge posed by the template-based reconstruction attack significantly impacts the security and privacy of biometric systems. Current reconstruction techniques rely on extensive training data or encounter limitations in adaptability, resulting in subpar reconstruction performance. In this paper, we propose a black-box palmprint template reconstruction method based on the modified Progressive GAN (ProGAN), which achieves a substantial success rate in attacking deep-learning-based and hand-crafted-based templates. Our approach incorporates the dropout mechanism into the generator of ProGAN and introduces a Double Reuse Training Strategy to enable effective training of the reconstruction network despite limited data. Furthermore, we devise a novel Scale-Adaptive Multi-Texture Complementarity loss, enhancing the texture quality of reconstructed images. We conduct extensive experiments on diverse palmprint recognition techniques. The resulting reconstructed images exhibit exceptional image quality. Additionally, we thoroughly examine the security and privacy aspects of the palmprint recognition algorithm based on the insights gained from the reconstruction attacks.}
}
@article{ZHANG2024110739,
title = {Robust multilayer bootstrap networks in ensemble for unsupervised representation learning and clustering},
journal = {Pattern Recognition},
volume = {156},
pages = {110739},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110739},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004904},
author = {Xiao-Lei Zhang and Xuelong Li},
keywords = {Ensemble selection, Cluster ensemble, Multilayer bootstrap networks, Unsupervised learning},
abstract = {It is known that unsupervised nonlinear learning is sensitive to the selection of hyperparameters, which hinders its practical use. How to determine the optimal hyperparameter setting that may be dramatically different across applications is a hard issue. In this paper, we aim to address this issue for multilayer bootstrap networks (MBN), a recent unsupervised model, in a way as simple as possible. Specifically, we first propose an MBN ensemble (MBN-E) algorithm which concatenates the sparse outputs of a set of MBN base models with different network structures into a new representation. Then, we take the new representation produced by MBN-E as a reference for selecting the optimal MBN base models. Moreover, we propose a fast version of MBN-E (fMBN-E), which is not only theoretically even faster than a single standard MBN but also does not increase the estimation error of MBN-E. Empirically, comparing to a number of advanced clustering methods, the proposed methods reach reasonable performance in their default settings. fMBN-E is empirically hundreds of times faster than MBN-E without suffering performance degradation. The applications to image segmentation and graph data mining further demonstrate the advantage of the proposed methods.}
}
@article{CHEN2024110642,
title = {Compressing spectral kernels in Gaussian Process: Enhanced generalization and interpretability},
journal = {Pattern Recognition},
volume = {155},
pages = {110642},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110642},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003935},
author = {Kai Chen and Twan {van Laarhoven} and Elena Marchiori},
keywords = {Gaussian Process, Compression, Spectral mixture kernel, Component pruning, Component merging},
abstract = {The modeling capabilities of a Gaussian Process (GP), such as generalization, nonlinearity, and smoothness, are largely determined by the choice of its kernel. A popular family of kernels for GPs, the spectral mixture (SM) kernels, have the desirable property that with a large number of spectral components they can approximate any stationary kernel. However, using a large number of SM components increases the risk of overfitting and hinders interpretability. To overcome these challenges, we propose a compression algorithm incorporating component pruning and component merging for GPs. Here SM components with small signal variance are removed, and a moment-matching merge method is proposed to further reduce the number of SM components. The main novelty of the proposed method is a similarity measure between SM components based on their normalized cross-correlation, which is related to the Bhattacharyya coefficient. We derive a greedy GP compression algorithm and perform a comparative evaluation over various learning tasks in terms of forecasting performance and compression capability. Results substantiate the beneficial effect of the method, both in terms of generalization and interpretability.11Source code: https://github.com/ck2019ML/Gaussian-Process-Model-Compression.}
}
@article{QIU2024110723,
title = {Guided contrastive boundary learning for semantic segmentation},
journal = {Pattern Recognition},
volume = {155},
pages = {110723},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110723},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004746},
author = {Shoumeng Qiu and Jie Chen and Haiqiang Zhang and Ru Wan and Xiangyang Xue and Jian Pu},
keywords = {Semantic segmentation, Contrastive learning, Boundary optimization},
abstract = {Semantic segmentation, a fundamental task in environmental understanding, aims to assign each image pixel to a specific class. Despite recent progress, segmentation accuracy in boundary regions remains suboptimal. This paper introduces Guided Contrastive Boundary Learning (GCBL), a novel framework designed to enhance feature representation learning, thereby improving boundary segmentation performance. Unlike conventional contrastive learning, GCBL guides inter-class representation learning by weighting pixel contributions based on their estimated probabilities. For intra-class learning, it leverages the neural collapse phenomenon, encouraging representations to align with last-layer classifier weights. Additionally, an asymmetric distance boundary pixel search strategy ensures a more reasonable selection of contrastive pairs. To prevent weight collapse in learning, a regularization term is applied to the last-layer classifier’s weights. The GCBL method is readily integrable into existing and future segmentation frameworks. Extensive experiments on the Cityscapes, ADE20K, and S3DIS datasets demonstrate the effectiveness and generalizability of our approach. Code is available at https://github.com/skyshoumeng/GCBL.}
}
@article{JAYASIMHAN2024110671,
title = {ResPrune: An energy-efficient restorative filter pruning method using stochastic optimization for accelerating CNN},
journal = {Pattern Recognition},
volume = {155},
pages = {110671},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110671},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004229},
author = {Anusha Jayasimhan and Pabitha P.},
keywords = {Model compression, Image classification, Neural networks, Deep learning, Pruning},
abstract = {Convolutional Neural Networks (CNNs) are frequently employed for image pattern recognition and other computer vision tasks. When over-parameterized deep learning models are used for inference, resource-constrained edge devices may struggle. As a result, model compression, particularly filter pruning, has become critical. A reduction in model size might result in less calculation, resulting in faster hardware execution and lower energy consumption. One of the drawbacks of current pruning strategies is that once the filters are pruned, their weights are permanently lost. To address this constraint, we propose a unique two-phase pruning technique in which the filters to be pruned are selected using two criteria: l2-norm and redundancy. Second, rather than omitting the selected filters for all future epochs, we restore them to their original value with some stochasticity. Retaining the most optimal filter weights in earlier epochs enables the survival of the fittest filters, resulting in higher model convergence. Experiments on three benchmark datasets, CIFAR-10, CIFAR-100, and ILSVRC-2012, reveal that our strategy outperforms other state-of-the-art pruning methods by a minimum reduction of 57% FLOPs with an accuracy loss as minimal as 0.08 %.}
}
@article{CAI2024110626,
title = {Multi-modal interaction with token division strategy for RGB-T tracking},
journal = {Pattern Recognition},
volume = {155},
pages = {110626},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110626},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003777},
author = {Yujue Cai and Xiubao Sui and Guohua Gu and Qian Chen},
keywords = {, Multi-modal fusion, Vision transformer, Cross-modal interaction, Attention masking strategy},
abstract = {RGB-T tracking takes visible and infrared images as inputs, which is an extended application of multi-modal fusion in the field of visual object tracking. The complementarity between visible and infrared modalities can enhance the robustness of tracker in complex scenes. Cross-modal interaction can facilitate the fusion and synergy of different modalities, but most previous methods lack clear target information in multi-modal fusion, leading to some undesired cross-relation in interaction. To reduce these undesired cross-relations, we propose a Multi-modal Interaction scheme Guided by Token Division strategy (MIGTD). This scheme divides the input multi-modal tokens into several categories and restricts the interaction between tokens by setting different rules. The above operation is implemented in parallel through an attention masking strategy. To accurately classify search tokens, an instance segmentation task with box-supervised loss is employed. We conduct extensive experiments on three popular benchmark datasets, RGBT234, LasHeR and VTUAV. The experimental results indicate that the tracker proposed in this article reach the world’s advanced level in performance.}
}
@article{YI2024110747,
title = {Self-Training-Transductive-Learning Broad Learning System (STTL-BLS): A model for effective and efficient image classification},
journal = {Pattern Recognition},
volume = {156},
pages = {110747},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110747},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004989},
author = {Lin Yi and Di Lv and Dinghao Liu and Suhuan Li and Ran Liu},
keywords = {Broad learning system, Feature extraction, Convolutional neural network, Self training, Image classification},
abstract = {A novel model called Self-Training-Transductive-Learning Broad Learning System (STTL-BLS) is proposed for image classification. The model consists of two key blocks: Feature Block (FB) and Enhancement Block (EB). The FB utilizes the Proportion of Large Values Attention (PLVA) technique and an Encoder for feature extraction. Multiple FBs are cascaded in the model to learn discriminative features. The Enhancement Block (EB) enhances feature learning and prevents under-fitting on complex datasets. Additionally, an architecture that combines characteristics of Broad Learning System (BLS) and gradient descent is designed for STTL-BLS, enabling the model to leverage the advantages of both BLS and Convolutional Neural Networks (CNNs). Moreover, a training algorithm (STTL) that combines self-training and transductive learning is presented for the model to improve its generalization ability. Experimental results demonstrate that the accuracy of the proposed model surpasses all compared BLS variants and performs comparably or even superior to deep networks: on small-scale datasets, STTL-BLS has an average accuracy improvement of 14.82 percentage points compared to other models; on large-scale datasets, 12.95 percentage points. Notably, the proposed model exhibits low time complexity, particularly with the shortest testing time on the small-scale datasets among all compared models: it has an average testing time of 46.4 s less than other models. It proves to be an additional valuable solution for image classification tasks on both small- and large-scale datasets. The source code for this paper can be accessed at https://github.com/threedteam/sttl_bls.}
}
@article{LIU2024110694,
title = {GLAN: A graph-based linear assignment network},
journal = {Pattern Recognition},
volume = {155},
pages = {110694},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110694},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400445X},
author = {He Liu and Tao Wang and Congyan Lang and Songhe Feng and Yi Jin and Yidong Li},
keywords = {Linear assignment, Graph networks, Learning-based solver, Multi-object tracking},
abstract = {Differentiable solvers for the linear assignment problem (LAP) have attracted much research attention in recent years, which are usually embedded into learning frameworks as components. However, previous algorithms, with or without learning strategies, usually suffer from the degradation of the optimality with the increment of the problem size. In this paper, we propose a learnable linear assignment solver based on deep graph networks. Specifically, we first transform the cost matrix to a bipartite graph and convert the assignment task to the problem of selecting reliable edges from the constructed graph. Subsequently, a deep graph network is developed to aggregate and update the features of nodes and edges. Finally, the network predicts a label for each edge that indicates the assignment relationship. The experimental results on a synthetic dataset reveal that our method outperforms state-of-the-art baselines and achieves consistently high accuracy with the increment of the problem size. Furthermore, we also embed the proposed solver, in comparison with state-of-the-art baseline solvers, into a popular multi-object tracking (MOT) framework to train the tracker in an end-to-end manner. The experimental results on MOT benchmarks illustrate that the proposed LAP solver improves the tracker by the largest margin.}
}
@article{JING2024110699,
title = {Boosting edge detection via Fusing Spatial and Frequency Domains},
journal = {Pattern Recognition},
volume = {155},
pages = {110699},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110699},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004503},
author = {Dongdong Jing and Huikai Shao and Dexing Zhong},
keywords = {Edge detection, Frequency perception, Multi-scale enhancement, Spatial-Frequency Fusion},
abstract = {Deep learning-based edge detection methods have shown great advantages and obtained promising performance. However, most of the current methods only extract features from the spatial (RGB) domain for edge detection and the information that can be mined is limited. As a result, they could not work well for the scenarios where the object is similar in color to the background. To combat this challenge, we propose a novel edge detection method by incorporating the features of both spatial and frequency domains, named Fusing Spatial and Frequency Domains (FSFD). A Frequency Perception (FP) module is constructed to extract the edges of objects in the frequency domain, which can avoid the indistinguishable situation in the spatial domain due to similar colors. A Multi-Scale Enhancement (MSE) module is designed to learn multi-scale feature in spatial domain, enabling the model to perceive edges of small objects. Spatial-Frequency Fusion (S2F) module is further introduced to fuse the features of spatial and frequency domains using an online learning manner. Adequate experiments are conducted on popular BSDS500, NYUDV2, and Multicue datasets. The results show that our method can outperform other methods as the state-of-the-art when dealing with the problem of edge detection. The codes will be released on https://github.com/JingDongdong/FSFD.}
}
@article{ZHANG2024110707,
title = {Exploring target-related information with reliable global pixel relationships for robust RGB-T tracking},
journal = {Pattern Recognition},
volume = {155},
pages = {110707},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110707},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004588},
author = {Tianlu Zhang and Xiaoyi He and Yongjiang Luo and Qiang Zhang and Jungong Han},
keywords = {RGB-T tracking, Siamese network, Transformer, Target-related feature enhancement, Intra- and mutual–modal attention based multi-modal feature fusion, Hard-focused online classifier},
abstract = {RGB-T Siamese trackers have drawn continuous interest in recent years due to their proper trade-off between accuracy and speed. However, they are sensitive to the background distractors in some challenging cases, thereby inducing unreliable response positions. To overcome such drawbacks, we advance a new RGB-T Siamese tracker, named SiamTIH, which will advance the RGB-T Siamese trackers’ discriminability against distractors by exploiting target-related information and reliable global pixel relationships within multi-modal data. Specifically, we propose a target-related feature enhancement module (TFE) to highlight such areas in the detection branch that are similar to the templates and suppress those background distractor regions that are significantly different from the templates but are greatly informative. Then, we propose an intra- and mutual-modal attention based multi-modal feature fusion module (IMA-MF) to capture the reliable global pixel relationships within multi-modal data. Especially, the intra-modal attention is used to capture the global pixel relationships within each single modality data, and the mutual-modal attention is utilized to enhance the feature representation of the current modality by overall pixel relationships as well as modality-specific relationships. Finally, we propose a hard-focused online classifier (HFOC) that combines an offline classifier and an online classifier to further improve the robustness of our tracker. Besides, the proposed framework is further extended to a Transformer based tracker to verify its generality. Extensive experiments on three RGB-T benchmarks demonstrate that our new RGB-T tracker outperforms the existing ones and maintains real-time performance, exceeding on average 30 frames per second (FPS). The code will be available at https://github.com/Tianlu-Zhang/SiamTIH.}
}
@article{LIU2024110659,
title = {AdaptBIR: Adaptive Blind Image Restoration with latent diffusion prior for higher fidelity},
journal = {Pattern Recognition},
volume = {155},
pages = {110659},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110659},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004102},
author = {Yingqi Liu and Jingwen He and Yihao Liu and Xinqi Lin and Fanghua Yu and Jinfan Hu and Yu Qiao and Chao Dong},
keywords = {Image restoration, Diffusion model, Adaptive adjustment},
abstract = {This work aims to help diffusion models get their footing in the low-level vision field, solving the pain point of insufficient fidelity. Specifically, we propose an Adaptive Blind Image Restoration framework with latent diffusion prior — AdaptBIR, which can adaptively distinguish and address various ranges of degradations. First, we quantitatively categorize images through an Image Quality Assessment (IQA) method. Then, a dual-encoder degradation removal module is employed with the guidance of IQA scores to reach better information preservation. Lastly, we utilize a two-phase controller to handle the reconstruction process in an organized manner. Extensive experiments show that applying such an adaptive framework achieves better performance on both fidelity and perceptual metrics. In this way, AdaptBIR represents more than just a novel framework, it paves the way for a broader application of the diffusion model in blind image restoration tasks.}
}
@article{FENG2024110735,
title = {Poisson tensor completion with transformed correlated total variation regularization},
journal = {Pattern Recognition},
volume = {156},
pages = {110735},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110735},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004862},
author = {Qingrong Feng and Jingyao Hou and Weichao Kong and Chen Xu and Jianjun Wang},
keywords = {Tensor completion, Low rankness and local smoothness, Transformed correlated total variation, Maximum likelihood estimate, Poisson observations},
abstract = {Tensor completion involves recovering the underlying tensor from partial observations, and in this paper we focus on the point that these observations obey the Poisson distribution. To contend with this problem, we adopt a popular method that minimizes the sum of the data-fitting term and the regularization term under a uniform sampling mechanism. Specifically, we consider the negative logarithmic maximum likelihood estimate of the Poisson distribution as the data-fitting term. To effectively characterize the intrinsic structure of the tensor data, we propose a parameter-free regularization term that can simultaneously capture the low rankness and local smoothness of the underlying tensor. Here, the transformed tensor nuclear norm is used to explore the low rankness under suitable unitary transformations. We present theoretical derivations to demonstrate the feasibility of the proposed model. Furthermore, we develop an algorithm based on the alternating direction multiplier method (ADMM) to efficiently solve the proposed optimization problem, with its overall convergence being established. A series of numerical experiments show that proposed model yields a pleasing accuracy over several state-of-the-art models.}
}
@article{LI2024110683,
title = {Dual space-based fuzzy graphs and orthogonal basis clustering for unsupervised feature selection},
journal = {Pattern Recognition},
volume = {155},
pages = {110683},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110683},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004345},
author = {Duanzhang Li and Hongmei Chen and Yong Mi and Chuan Luo and Shi-Jinn Horng and Tianrui Li},
keywords = {Unsupervised feature selection, Dual-graph, Orthogonal basis clustering, Sparse learning},
abstract = {Unsupervised feature selection (UFS) takes an important position because gaining the class labels is laborious or even impossible. In the domain of UFS, clustering is a major means to exploit label information. The existing methods either could not model a clear clustering structure or could not utilize the local structure of data. Consequently, a new clustering method in combination with graph learning is proposed in this work. Specifically, for clustering, orthogonal basis clustering is introduced, where orthogonal constraints are imposed on the cluster center matrix and the clustering matrix. The clustering indicator matrix is also imposed by a non-negative constraint. A clearer clustering structure and more independent clustering centers are obtained through these constraints. For local preservation, given that traditional graphs for keeping the local manifold are faced with the problem of imbalanced neighbors, the fuzzy graph is introduced to acquire a robust structure, which is applied to both data space and feature space. The topological structure in these spaces can be well maintained. For the choice of salient features, ℓ2,0-norm regularization is imposed on the projection matrix. The object function is solved alternately. Then, a feature selection algorithm is designed. Experiments are designed and performed on nine real-world data sets. The results attest to the effectiveness of the proposed algorithm compared with other relevant algorithms.}
}
@article{WANG2024110695,
title = {Robust Self-expression Learning with Adaptive Noise Perception},
journal = {Pattern Recognition},
volume = {155},
pages = {110695},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110695},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004461},
author = {Yangbo Wang and Jie Zhou and Jianglin Lu and Jun Wan and Can Gao and Qingshui Lin},
keywords = {Self-expression, Noise perception, Representation learning, Subspace clustering},
abstract = {Self-expression learning methods often obtain a coefficient matrix to measure the similarity between pairs of samples. However, directly using the raw data to represent each sample under the self-expression framework may not be ideal, as noise points are inevitably involved in the process of representing clean samples. To address this issue, this work proposes a novel self-expression model called robust Self-Expression learning with adaptive Noise Perception (SENP). SENP decomposes each sample into a clean part and a noisy part, and samples with large self-expression losses can be recognized as the noise points. A reliable coefficient matrix can then be learned by using only the clean points to reconstruct the clean part of each sample. By simultaneously detecting the noisy part of each sample and noise points, and adaptively mitigating their negative impacts, the representative ability of the generated coefficient matrix is improved. Moreover, inspired by the solution of non-negative matrix factorization (NMF), an effective algorithm is formed to optimize SENP. Extensive experiments on well-known benchmark datasets demonstrate the superiority of SENP compared to several state-of-the-art methods.}
}