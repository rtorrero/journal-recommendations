@article{KIM2024109908,
title = {MHCanonNet: Multi-Hypothesis Canonical lifting Network for self-supervised 3D human pose estimation in the wild video},
journal = {Pattern Recognition},
volume = {145},
pages = {109908},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109908},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006064},
author = {Hyun-Woo Kim and Gun-Hee Lee and Woo-Jeoung Nam and Kyung-Min Jin and Tae-Kyung Kang and Geon-Jun Yang and Seong-Whan Lee},
keywords = {3D human pose, Self-supervised learning, Multi-view geometry},
abstract = {Recent advancements in 3D Human Pose Estimation using fully-supervised learning approach have shown impressive results; however, these methods heavily rely on large amounts of annotated 3D data, which are challenging to obtain outside controlled laboratory environments. Therefore, in this study, we propose a new self-supervised training method designed to train a 3D human pose estimation network using unlabeled multi-view images. The model trains relative depths between joints without any 3D annotation by satisfying multi-view consistency constraints from unlabeled multi-view videos without camera calibration, while simultaneously learning representations of multiple plausible pose hypotheses. For this reason, we call our proposed network a Multi-Hypothesis Canonical Lifting Network (MHCanonNet). By enriching the diversity of extracted features and keeping various possibilities open, our network accurately estimates the final 3D pose. The key idea lies in the design of a novel and unbiased reconstruction objective function that combines multiple hypotheses from different viewpoints. The proposed approach demonstrates state-of-the-art results not only on two popular benchmark datasets, Human3.6M and MPI-INF-3DHP but also on an in-the-wild dataset, Ski-Pose, surpassing existing self-supervised training methods.}
}
@article{MESGARAN2024109960,
title = {Graph fairing convolutional networks for anomaly detection},
journal = {Pattern Recognition},
volume = {145},
pages = {109960},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109960},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006581},
author = {Mahsa Mesgaran and A. Ben Hamza},
keywords = {Anomaly detection, Graph convolutional network, Skip connection, Implicit fairing, Jacobi method},
abstract = {Graph convolution is a fundamental building block for many deep neural networks on graph-structured data. In this paper, we introduce a simple, yet very effective graph convolutional network with skip connections for semi-supervised anomaly detection. The proposed layerwise propagation rule of our model is theoretically motivated by the concept of implicit fairing in geometry processing, and comprises a graph convolution module for aggregating information from immediate node neighbors and a skip connection module for combining layer-wise neighborhood representations. This propagation rule is derived from the iterative solution of the implicit fairing equation via the Jacobi method. In addition to capturing information from distant graph nodes through skip connections between the network’s layers, our approach exploits both the graph structure and node features for learning discriminative node representations. These skip connections are integrated by design in our proposed network architecture. The effectiveness of our model is demonstrated through extensive experiments on five benchmark datasets, achieving better or comparable anomaly detection results against strong baseline methods. We also demonstrate through an ablation study that skip connection helps improve the model performance.}
}
@article{DAS2023109879,
title = {Estimation of interlayer textural relationships to discriminate the benignancy/malignancy of brain tumors},
journal = {Pattern Recognition},
volume = {144},
pages = {109879},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109879},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005770},
author = {Poulomi Das and Arpita Das},
keywords = {Advanced PCNN module, Classification, FCM clustering algorithm, Interlayer feature quantifiers, NSST based decomposition},
abstract = {A computer-aided diagnosis system is a popular tool to predict the risk factors of brain tumors. However, the existing techniques are unable to provide high detection accuracy due to the inability of capturing the hidden features of brain tumors. In this view, the present work decomposes the MR modality images into different layers using non-sub-sampled shearlet transformation (NSST). Following this, the proposed detection pipeline employs adaptive pulse-coupled neural network (A-PCNN) module and a fuzzy c-means (FCM) clustering algorithm for enhancement and segmentation of suspicious regions respectively. Interlayer textural relationships of tumors are estimated in terms of the layer-wise difference of the gray-level cooccurrence matrix (GLCM), similarity indices, and entropy distributions. Thenceforth, those feature coefficients are coded with binary sequences to express the textural homogeneity/randomness of tumors. Finally, these handcrafted multi-scale feature quantifiers are fed to some standard classifiers such as the k-nearest neighbor (kNN) technique, the linear square support vector machine (LS-SVM), and the decision tree (DT) for discriminating the state of benignancy/malignancy of tumors. Experimental results ensure that the proposed detection model shows superior performance compared to existing methods.}
}
@article{JIANG2024109920,
title = {A novel interval dual convolutional neural network method for interval-valued stock price prediction},
journal = {Pattern Recognition},
volume = {145},
pages = {109920},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109920},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006180},
author = {Manrui Jiang and Wei Chen and Huilin Xu and Yanxin Liu},
keywords = {Interval-valued time series, Interval-valued stock price, Stock price prediction, Relevant stock information, Convolutional neural network},
abstract = {Accurate interval-valued stock price prediction is challenging and of great interest to investors and for-profit organizations. In this study, by considering individual stock information and relevant stock information simultaneously, we propose a novel interval dual convolutional neural network (Dual-CNNI) model based method to predict interval-valued stock prices. First, the individual and relevant stock information are collected and transformed into images. Then, the Dual-CNNI model is proposed to predict interval-valued stock prices. Specifically, two convolutional neural network (CNN) models with different structures are constructed to respectively extract individual stock features and relevant stock features, and then an interval multilayer perceptron (MLPI) model is used for final interval-valued stock price prediction. Finally, extensive experiments are conducted based on six randomly selected stocks, with comparison to several popular machine learning model based methods and interval-valued time series (ITS) prediction methods. The experimental results indicate that the proposed Dual-CNNI based method has superior predictive ability.}
}
@article{TAN2023109883,
title = {Semantic Similarity Distance: Towards better text-image consistency metric in text-to-image generation},
journal = {Pattern Recognition},
volume = {144},
pages = {109883},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109883},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005812},
author = {Zhaorui Tan and Xi Yang and Zihan Ye and Qiufeng Wang and Yuyao Yan and Anh Nguyen and Kaizhu Huang},
keywords = {Text-to-image, Image generation, Generative adversarial networks, Semantic consistency},
abstract = {Generating high-quality images from text remains a challenge in visual-language understanding, with text-image consistency being a major concern. Particularly, the most popular metric R-precision may not accurately reflect the text-image consistency, leading to misleading semantics in generated images. Albeit its significance, designing a better text-image consistency metric surprisingly remains under-explored in the community. In this paper, we make a further step forward to develop a novel CLIP-based metric, Semantic Similarity Distance (SSD), which is both theoretically founded from a distributional viewpoint and empirically verified on benchmark datasets. We also introduce Parallel Deep Fusion Generative Adversarial Networks (PDF-GAN), which use two novel components to mitigate inconsistent semantics and bridge the text-image semantic gap. A series of experiments indicate that, under the guidance of SSD, our developed PDF-GAN can induce remarkable enhancements in the consistency between texts and images while preserving acceptable image quality over the CUB and COCO datasets.}
}
@article{ZHANG2024109952,
title = {A Cross-Lingual Summarization method based on cross-lingual Fact-relationship Graph Generation},
journal = {Pattern Recognition},
volume = {146},
pages = {109952},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109952},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006507},
author = {Yongbing Zhang and Shengxiang Gao and Yuxin Huang and Kaiwen Tan and Zhengtao Yu},
keywords = {Cross-lingual summarization, Fact-relationship graph, Deliberation network, Graph generation, Factual inconsistency},
abstract = {The aim of cross-lingual summarization (CLS) is to condense the content of a document in one language into a summary in another language. In essence, a CLS model requires both translation and summarization capabilities, which presents a unique challenge, as the model must effectively tackle the difficulties associated with both tasks simultaneously (e.g., semantic alignment, information compression and factual inconsistency). Graph-based semantic representation can model important text information in a structured manner, which may alleviate these challenges. Therefore, in this paper, we propose a Cross-Lingual Summarization method based on cross-lingual Fact-relationship Graph Generation (FGGCLS). Specifically, we first construct fact-relationship graphs for source language documents and target language summaries. Then, we introduce a cross-lingual fact-relationship graph generation method, which converts the CLS problem into a cross-lingual fact-relationship graph generation problem. This approach simplifies semantic alignment and information compression through the generation of graphs and leads to improved fact consistency. Finally, the generated fact-relationship graph of the target language summary serves as a draft for generating the summary, which enhances the quality of the generated summary. We conduct systematic experiments on the Zh2EnSum and En2ZhSum datasets, and the results demonstrate that our method can effectively improve the performance of CLS and alleviate factual inconsistency.}
}
@article{ZHAO2024109842,
title = {Hierarchical long-tailed classification based on multi-granularity knowledge transfer driven by multi-scale feature fusion},
journal = {Pattern Recognition},
volume = {145},
pages = {109842},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109842},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300540X},
author = {Wei Zhao and Hong Zhao},
keywords = {Long-tailed learning, Hierarchical classification, Multi-granularity, Multi-scale feature fusion, Knowledge transfer},
abstract = {Long-tailed learning is attracting increasing attention due to the unbalanced distributions of real-world data. The aim is to train well-performing depth models. Traditional knowledge transfer methods for long-tailed learning are classified into feature-based horizontal knowledge transfer (HKT) and class-based vertical knowledge transfer (VKT). HKT transfers head-to-tail feature knowledge from different classes to improve classification performance when there are few tail classes. However, HKT easily leads to invalid transfer due to the deviation caused by the difference between the knowledge of head and tail classes. Fortunately, the class space has a multi-grained relationship and can form a multi-granularity knowledge graph (MGKG), which can be recast as coarse-grained and fine-grained losses to guide VKT. In this paper, we propose a hierarchical long-tailed classification method based on multi-granularity knowledge transfer (MGKT), which vertically transfers knowledge from coarse- to fine-grained classes. First, we exploit the semantic information of classes to construct an MGKG, which forms an affiliation of fine- and coarse-grained classes. Fine-grained knowledge can inherit coarse-grained knowledge to reduce transfer bias with the help of MGKG. We then propose a multi-scale feature fusion network, which aims to fully mine the rich information of the features to drive MGKT. Experiments show that the proposed model outperforms several state-of-the-art models in classifying long-tailed data. For example, our model performed 4.46% better than the next-best model on the SUN-LT dataset.}
}
@article{MI2024109895,
title = {Fast Multi-view Subspace Clustering with Balance Anchors Guidance},
journal = {Pattern Recognition},
volume = {145},
pages = {109895},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109895},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005939},
author = {Yong Mi and Hongmei Chen and Zhong Yuan and Chuan Luo and Shi-Jinn Horng and Tianrui Li},
keywords = {Multi-view subspace clustering, Anchor-based MVSC methods, Balance structure, Anchor graph},
abstract = {Multi-view subspace clustering (MVSC) has acquired satisfactory clustering performance since it effectively integrates the information from multiple views. However, existing MVSC methods often suffer from high time costs and are difficult to be used in real-life large-scale data. Anchor-based MVSC methods have been presented to select crucial landmarks to reduce time-consuming effectively. In addition, the processes of anchor selection of existing methods are performed in the raw space, in which the high-dimensional data often involve lots of noise information and outliers that inevitably lead to the degradation of clustering performance. Moreover, these methods also ignore the balance structure of data, such that the selected anchors cannot fully characterize the intrinsic structure information of the original data. To tackle the aforementioned issues, we present a novel MVSC method named Fast Multi-view Subspace Clustering with Balance Anchors Guidance (FMVSC-BAG). Specifically, FMVSC-BAG integrates the learning processes of anchors, anchor graphs, and labels into a united framework in embedding space seamlessly. This way, they can reinforce each other to improve final clustering performance while eliminating noise and outliers hidden in the original data. Furthermore, FMVSC-BAG constrains the learned labels to preserve the balance structure by a novel balance strategy to promote further that the intrinsic balance structure information of original data can be reserved in the learned anchors and anchor graph. Finally, extensive experiments on eight real-life large-scale datasets prove its efficiency and superiority compared to some advanced clustering methods.}
}
@article{FUMANALIDOCIN2024109924,
title = {Supervised penalty-based aggregation applied to motor-imagery based brain-computer-interface},
journal = {Pattern Recognition},
volume = {145},
pages = {109924},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109924},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006222},
author = {J. Fumanal-Idocin and C. Vidaurre and J. Fernandez and M. Gómez and J. Andreu-Perez and M. Prasad and H. Bustince},
keywords = {Brain–computer interface, Motor imagery, Penalty function, Aggregation functions, Classification, Signal processing},
abstract = {In this paper we propose a new version of penalty-based aggregation functions, the Multi Cost Aggregation choosing functions (MCAs), in which the function to minimize is constructed using a convex combination of two relaxed versions of restricted equivalence and dissimilarity functions instead of a penalty function. We additionally suggest two different alternatives to train a MCA in a supervised classification task in order to adapt the aggregation to each vector of inputs. We apply the proposed MCA in a Motor Imagery-based Brain–Computer Interface (MI-BCI) system to improve its decision making phase. We also evaluate the classical aggregation with our new aggregation procedure in two publicly available datasets. We obtain an accuracy of 82.31% for a left vs. right hand in the Clinical BCI challenge (CBCIC) dataset, and a performance of 62.43% for the four-class case in the BCI Competition IV 2a dataset compared to a 82.15% and 60.56% using the arithmetic mean. Finally, we have also tested the goodness of our proposal against other MI-BCI systems, obtaining better results than those using other decision making schemes and Deep Learning on the same datasets.}
}
@article{ZHANG2023109851,
title = {Graph matching for knowledge graph alignment using edge-coloring propagation},
journal = {Pattern Recognition},
volume = {144},
pages = {109851},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109851},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005496},
author = {Yuxuan Zhang and Yuanxiang Li and Xian Wei and Yongsheng Yang and Lei Liu and Yi Lu Murphey},
keywords = {Knowledge graph, Entity alignment, Relation alignment, Quadratic assignment problem},
abstract = {Knowledge graph (KG) is a kind of structured human knowledge of modeling the relationships between real-world entities. High quality KG is of crucial importance for many knowledge-based applications, e.g., question answering, recommender systems, etc. This paper studies the problem of entity alignment in KGs to promote knowledge fusion. Existing methods model the semantic representation of entities by using graph structural information or attribute information of the KG and then align the entities across different domains by calculating the distances between entities’ embeddings. However, these methods only consider the node-to-node similarity in the alignment procedure while the edge-to-edge similarity is ignored. Our research hypothesis is that the graph edge alignment information is critical in entity alignment. We reformulate the knowledge entity alignment as a quadratic assignment problem (QAP) by adding relation alignment under the one-to-one mapping constraint. To solve the notorious QAP in a large-scale heterogeneous graph like KG, we propose a model, dual neighborhood consensus network (DNCN), which approximately decomposes the QAP into two small-scale linear assignment problems, i.e., entity alignment and relation alignment. After that, an edge-coloring propagation method is proposed to refine the coarse entity alignment result using the relation correspondence. Theoretical proof shows that this method can guarantee the isomorphism between local sub-graphs. The performance of DNCN is evaluated using the DBP15K and DWY100K benchmarks. Experimental results show that DNCN achieves the best performance on the DBP15K benchmark, and is computationally efficient. Ablation studies verify the importance of graph edge alignment information.}
}
@article{APICELLA2023109867,
title = {Adaptive filters in Graph Convolutional Neural Networks},
journal = {Pattern Recognition},
volume = {144},
pages = {109867},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109867},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005654},
author = {Andrea Apicella and Francesco Isgrò and Andrea Pollastro and Roberto Prevete},
keywords = {Graph Convolutional Neural Networks, Deep learning, Dynamic neural networks, Programmable ANNs, Graph structure learning},
abstract = {Over the last few years, the availability of an increasing data generated from non-Euclidean domains, which are usually represented as graphs with complex relationships, and Graph Neural Networks (GNN) have gained a high interest because of their potential in processing graph-structured data. In particular, there is a strong interest in performing convolution on graphs using an extension of the GNN architecture, generally referred to as Graph Convolutional Neural Networks (ConvGNN). Convolution on graphs has been achieved mainly in two forms: spectral and spatial convolutions. Due to the higher flexibility in exploring and exploiting the graph structure of data, there is recently an increasing interest in investigating the possibilities that the spatial approach can offer. The idea of finding a way to adapt the network behaviour to the inputs they process to maximize the total performances has aroused much interest in the neural networks literature over the years. This paper presents a novel method to adapt the behaviour of a ConvGNN to the input performing spatial convolution on graphs using input-specific filters, which are dynamically generated from nodes feature vectors. The experimental assessment confirms the capabilities of the proposed approach, achieving satisfying results using a low number of filters.}
}
@article{CHEN2024109959,
title = {Enhancing texture representation with deep tracing pattern encoding},
journal = {Pattern Recognition},
volume = {146},
pages = {109959},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109959},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300657X},
author = {Zhile Chen and Yuhui Quan and Ruotao Xu and Lianwen Jin and Yong Xu},
keywords = {Texture recognition, Deep learning, Feature encoding},
abstract = {Texture representation is a challenging problem due to the complex underlying physics of texture as well as the variations caused by changes in viewpoint. Recent progress in texture analysis has been made by the power of convolutional neural networks (CNNs) in feature learning. However, most current methods aggregate the features from the last convolutional layer of the CNN to obtain a global feature vector, which fails to leverage shallow low-level visual cues and cross-layer feature patterns, limiting their performance. In this paper, we propose to trace the features generated along the convolutional layers via a histogram of local 3D invariant binary patterns, called deep tracing patterns. This leads to a highly discriminative yet robust global feature representation module. Building such a module into a CNN backbone, we develop an effective approach for texture recognition. Extensive experiments on six benchmark datasets show that the proposed approach provides a discriminative and robust texture descriptor, with state-of-the-art performance achieved.}
}
@article{LI2024109912,
title = {PSCFormer: A lightweight hybrid network for gas identification in electronic nose system},
journal = {Pattern Recognition},
volume = {145},
pages = {109912},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109912},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006106},
author = {Ziyang Li and Siyuan Kang and Ninghui Feng and Chongbo Yin and Yan Shi},
keywords = {Transformer, Convolutional neural network, Electronic nose, Gas identification, Engineering application},
abstract = {Based on their powerful feature extraction capability, a convolutional neural network (CNN) has been gradually applied to gas identification in the electronic nose (e-nose) system. The responses of different intensities in the e-nose system are significantly correlated, and CNN extracts the local gas features by convolution while ignoring their global correlation. Transformer combines different responses and obtains the correlation between global features by self-attention. This paper proposes a lightweight hybrid network called Peak Search-based Convolutional Transformers (PSCFormer). First, combining the data characteristics of gas information, the Local Peak Search and Feature Fusion (LPSF) module is proposed to focus on the key gas features. Second, Transformer Encoder (TE) is proposed to obtain the global correlation between global features, and the parallel Convolution Encoder (CE) is proposed to capture the local dependence. Finally, a reasonable feature complementation mechanism is presented, and the preference of TE is alleviated for the slow-down response while solving the receptive field limitation of CE. This paper has evaluated three different datasets to validate the effectiveness of PSCFormer, all of which show stable and excellent performance with a good tradeoff between efficiency and complexity. The results prove that PSCFormer is an efficient and lightweight gas identification network, which provides a method to promote the engineering application of the e-nose system.}
}
@article{LING2023109891,
title = {Motional foreground attention-based video crowd counting},
journal = {Pattern Recognition},
volume = {144},
pages = {109891},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109891},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005897},
author = {Miaogen Ling and Tianhang Pan and Yi Ren and Ke Wang and Xin Geng},
keywords = {Video crowd counting, Frame difference, Attention mechanism},
abstract = {In this paper, we tackle the problem of video crowd counting. Compared with single image crowd counting, video provides gradual spatial and temporal variation information that would help to strengthen the robustness of crowd counting. Therefore, it is critical to make full use of neighboring frames both in feature extraction and final prediction for current frame’s estimation. Based on the above observations, we propose a motional foreground attention-based video crowd counting method. Specifically, we first leverage an foreground estimation module based on ConvNeXt to extract the motional features from bidirectional frame differences and output a foreground estimation map. Then the motional features combined with the static features of current frame are sent into feature fusion network, where foreground estimation map is transformed as attention weights for crowd number estimation. Three new indoor video datasets are manually annotated. The proposed method achieves state-of-the-art performance on all indoor and outdoor video datasets.}
}
@article{XU2024109936,
title = {Depth Map Denoising Network and Lightweight Fusion Network for Enhanced 3D Face Recognition},
journal = {Pattern Recognition},
volume = {145},
pages = {109936},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109936},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006349},
author = {Ruizhuo Xu and Ke Wang and Chao Deng and Mei Wang and Xi Chen and Wenhui Huang and Junlan Feng and Weihong Deng},
keywords = {Depth map denoising, Implicit neural representations, Low-quality 3D face recognition, Lightweight network, Deep learning},
abstract = {With the increasing availability of consumer depth sensors, 3D face recognition (FR) has attracted more and more attention. However, the data acquired by these sensors are often coarse and noisy, making them impractical to use directly. In this paper, we introduce an innovative Depth map denoising network (DMDNet) based on the Denoising Implicit Image Function (DIIF) to reduce noise and enhance the quality of facial depth images for low-quality 3D FR. After generating clean depth faces using DMDNet, we further design a powerful recognition network called Lightweight Depth and Normal Fusion network (LDNFNet), which incorporates a multi-branch fusion block to learn unique and complementary features between different modalities such as depth and normal images. Comprehensive experiments conducted on four distinct low-quality databases demonstrate the effectiveness and robustness of our proposed methods. Furthermore, when combining DMDNet and LDNFNet, we achieve state-of-the-art results on the Lock3DFace database.}
}
@article{WU2024109961,
title = {MISL: Multi-grained image-text semantic learning for text-guided image inpainting},
journal = {Pattern Recognition},
volume = {145},
pages = {109961},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109961},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006593},
author = {Xingcai Wu and Kejun Zhao and Qianding Huang and Qi Wang and Zhenguo Yang and Gefei Hao},
keywords = {Text-guided image inpainting, Textual and visual semantics, Hierarchical learning},
abstract = {Text-guided image inpainting aims to generate corrupted image patches and obtain a plausible image based on textual descriptions, considering the relationship between textual and visual semantics. Existing works focus on predicting missing patches from the residual pixels of corrupted images, ignoring the visual semantics of the objects of interest in the images corresponding to the textual descriptions. In this paper, we propose a text-guided image inpainting method with multi-grained image-text semantic learning (MISL), consisting of global-local generators and discriminators. More specifically, we devise hierarchical learning (HL) with global-coarse-grained, object-fine-grained, and global-fine-grained learning stages in the global-local generators to refine the corrupted images from the global to local. In particular, the object-fine-grained learning stage focuses on the visual semantics of objects of interest in corrupted images by using an encoder-decoder network with self-attention blocks. Not only that, we design a mask reconstruction (MR) module to further act on the restoration of the objects of interest corresponding to the given textual descriptions. To inject the textual semantics into the global-local generators, we implement a multi-attention (MA) module that incorporates the word-level and sentence-level textual features to generate three different-grained images. For training, we exploit a global discriminator and a flexible discriminator to penalize the whole image and the corrupted region, respectively. Extensive experiments conducted on four datasets show the outperformance of the proposed MISL.}
}
@article{YUE2024109937,
title = {A zero-shot learning boosting framework via concept-constrained clustering},
journal = {Pattern Recognition},
volume = {145},
pages = {109937},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109937},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006350},
author = {Qin Yue and Junbiao Cui and Liang Bai and Jianqing Liang and Jiye Liang},
keywords = {Zero-shot learning, Concept-constrained clustering, Co-training, Domain shift},
abstract = {Zero-shot learning (ZSL) aims to recognize novel classes that have no labeled samples during the training phase, which leads to the domain shift problem. In reality, there exists a large number of compounded unlabeled samples. Therefore, it is crucial to accurately estimate the data distribution of these compounded unlabeled samples and improve the performance of ZSL. This paper proposes a zero-shot learning boosting framework. Specifically, ZSL is transformed into a co-training problem between the data distribution estimation of the unlabeled samples and ZSL. The data distribution estimation is modeled as concept-constrained clustering. Furthermore, we design an alternative optimization strategy to realize mutual guidance between the two processes. Finally, systematic experiments verify the effectiveness of the proposed concept-constrained clustering for alleviating the domain shift problem in ZSL and the universality of the proposed framework for boosting different base ZSL models.}
}
@article{SUN2024109909,
title = {Semantic augmentation by mixing contents for semi-supervised learning},
journal = {Pattern Recognition},
volume = {145},
pages = {109909},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109909},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006076},
author = {Rémy Sun and Clément Masson and Gilles Hénaff and Nicolas Thome and Matthieu Cord},
keywords = {Deep-learning, Semi-supervised learning, Data augmentation, Mixing augmentation},
abstract = {Leveraging unlabeled examples is a crucial issue for boosting performances in semi-supervised learning. In this work, we introduce the SAMOSA framework based on semantic augmentation for mixing semantic components from labeled examples and non semantic characteristics from unlabeled data. Our approach is based on a novel reconstruction module that can be grafted onto most state of the art networks. The proposed approach leans on two main aspects: an architectural component optimized to disentangle semantic and auxiliary non semantic representations using an unsupervised loss, and a semantic augmentation scheme that leverages this disentangling module to generate artificially labeled examples preserving known class information while controlling auxiliary variations. We demonstrate the ability of our method to improve the performance of models trained according to standard semi-supervised procedures Mean Teacher (Tarvainen and Valpola, 2017) MixMatch (Berthelot et al., 2019) and FixMatch (Sohn et al., 2020).}
}
@article{GONG2024109884,
title = {Personalized recommendation via inductive spatiotemporal graph neural network},
journal = {Pattern Recognition},
volume = {145},
pages = {109884},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109884},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005824},
author = {Jibing Gong and Yi Zhao and Jinye Zhao and Jin Zhang and Guixiang Ma and Shaojie Zheng and Shuying Du and Jie Tang},
keywords = {Recommendation system, Inductive learning, Spatiotemporal graph neural network},
abstract = {Graph neural network-based collaborative filtering methods have achieved excellent performance in recommender systems. However, previous works have primarily focused on representation using the entire graph or a single sampled subgraph, which fails to address the issue of noise introduced by prolonged ineffective interactions. Moreover, these approaches lack the capacity to model temporal information in an inductive manner, thereby limiting their ability to fully capture the evolving dynamic interests of users over time in a real world scenario. To address the aforementioned challenges, we propose a novel personalized inductive spatiotemporal graph neural network-based framework PistGNN. PistGNN extracts multiple temporal-aware subgraphs from user–item pairs to avoid long-time gap meaningless interactions, which are then fed into a spatiotemporal graph neural network module. It is worth noting that PistGNN trains the model only by relying on subgraphs extracted from bipartite graphs, without depending on global information, and is therefore capable of predicting for new users or items. Finally, we propose an attention-based meta-learning method to personalize the aggregation of subgraphs’ embeddings. Extensive experiments conducted on four real-world datasets demonstrate the superiority of PistGNN over both inductive and transductive baseline methods.}
}
@article{PAN2023109832,
title = {Hyperspectral image destriping and denoising from a task decomposition view},
journal = {Pattern Recognition},
volume = {144},
pages = {109832},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109832},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005307},
author = {Erting Pan and Yong Ma and Xiaoguang Mei and Jun Huang and Qihai Chen and Jiayi Ma},
keywords = {Image restoration, Hyperspectral images, Denoising, Destriping, Multi-task learning},
abstract = {The generalized mathematical model for HSI denoising or destriping lacks stability and uniqueness properties, failing to accurately portray the distribution and effects of stripes. Solutions following such a model would inevitably result in excessive destriping of strip-free areas, leading to the loss of texture detail. To remedy the above deficiencies, we reformulate the destriping task and introduce a novel solution from the task decomposition view. It is broken down into auxiliary sub-tasks involving stripe mask detection, stripe intensity estimation, and HSI restoration, which greatly reduces the difficulty of solving such an ill-posed problem. Based on this, we adopt a sequential multi-task learning framework and propose a stripes location-dependent restoration network, termed SLDR, which integrates the distribution and intensity features of stripes to achieve accurate destriping and high-fidelity restoration. Furthermore, we design a stripe attribute-aware estimator and a weighted total variation loss function to capture the unique properties of stripes and adaptively adjust the restoration weights of striped and non-striped regions. Extensive evaluation and comprehensive ablation studies on synthetic and practical scenes show the effectiveness and superiority of our model and architecture.}
}
@article{TENG2024109953,
title = {Multi-label borderline oversampling technique},
journal = {Pattern Recognition},
volume = {145},
pages = {109953},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109953},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006519},
author = {Zeyu Teng and Peng Cao and Min Huang and Zheming Gao and Xingwei Wang},
keywords = {Multi-label learning, Class imbalance, Borderline sample, Oversampling},
abstract = {Class imbalance problem commonly exists in multi-label classification (MLC) tasks. It has non-negligible impacts on the classifier performance and has drawn extensive attention in recent years. Borderline oversampling has been widely used in single-label learning as a competitive technique in dealing with class imbalance. Nevertheless, the borderline samples in multi-label data sets (MLDs) have not been studied. Hence, this paper deeply discussed the borderline samples in MLDs and found they have different neighboring relationships with class borders, which makes their roles different in the classifier training. For that, they are divided into two types named the self-borderline samples and the cross-borderline samples. Further, a novel MLDs resampling approach called Multi-Label Borderline Oversampling Technique (MLBOTE) is proposed for multi-label imbalanced learning. MLBOTE identifies three types of seed samples, including interior, self-borderline, and cross-borderline samples, and different oversampling mechanisms are designed for them, respectively. Meanwhile, it regards not only the minority classes but also the classes suffering from one-vs-rest imbalance as those in need of oversampling. Experiments on eight data sets with nine MLC algorithms and three base classifiers are carried out to compare MLBOTE with some state-of-art MLDs resampling techniques. The results show MLBOTE outperforms other methods in various scenarios.}
}
@article{XI2023109872,
title = {Online portfolio selection with predictive instantaneous risk assessment},
journal = {Pattern Recognition},
volume = {144},
pages = {109872},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109872},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005708},
author = {Wenzhi Xi and Zhanfeng Li and Xinyuan Song and Hanwen Ning},
keywords = {Portfolio optimization, Online learning, High-dimensional covariance matrix, Ensemble learning, High-dimensional short-term data},
abstract = {Online portfolio selection (OPS) has received increasing attention from machine learning and quantitative finance communities. Despite their effectiveness, the pioneering OPS methods have several key limitations. First, price predictions are usually based on predetermined trends, which is inadequate for a fast-changing market patterns. Second, each asset is treated individually, ignoring the pervading relevance among the assets. Third, the risk terms are usually missing or inappropriate in optimizations. This paper proposes a novel OPS method, namely, the online low-dimension ensemble method, to overcome the limitations. Motivated by the stylized facts for the co-movements of assets, the financial market is regarded as a high-dimensional dynamical system (HDS), and a large number of low-dimensional subsystems (LDSs) are randomly generated from the HDS to extract the correlation information among the assets. The assets’ price predictions are first made using these LDSs and then aggregated to formulate the final prediction using ensemble learning techniques. Thanks to the particular merits brought by our predicting scheme, we also develop a novel high-dimensional covariance matrix estimation/prediction method for short-term data, efficiently assessing the instantaneous risk of the projected portfolios. Compared with state-of-the-art methods, our approach obtains more accurate predictions as the correlation information is fully exploited. With the predictive instantaneous risk assessment, a more appropriate optimization problem is proposed, substantially improving the OPS setting and leading to significantly better investment performance. Therefore, this study develops a flexible and promising approach to learning fast-changing market patterns and demonstrates that the high-dimensional feature of the market is a crucial information source for financial modeling with short-term data rather than a barrier in the conventional sense. Extensive experiments on real-world datasets are conducted to illustrate our method further.}
}
@article{MA2023109880,
title = {Federated adaptive reweighting for medical image classification},
journal = {Pattern Recognition},
volume = {144},
pages = {109880},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109880},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005782},
author = {Benteng Ma and Yu Feng and Geng Chen and Changyang Li and Yong Xia},
keywords = {Medical image classification, Federated learning, Deep learning},
abstract = {Medical data sharing across institutes is crucial to large-scale multi-center studies and the development of real-world AI applications but suffers from serious privacy issues. A promising solution to address this challenge is federated learning, which typically aggregates a global model from heterogeneous data spread across numerous clients without exchanging data. However, the traditional federated learning algorithm (i.e., FedAvg) merely aggregates the locally distributed models according to the amount of data on each client and lacks the consideration of data heterogeneity. In this paper, we propose a novel Federated Adaptive Reweighting (FedAR) algorithm for medical image classification. FedAR employs a flexible re-weighting scheme that can balance adaptively the contributions of the amount of data and the performance of the local model on each client to the weight of that client. Specifically, we allow the amount of local data to contribute more to the weight of each client in the early training stage and let the performance of the local model play a more important role in the late stage. We have evaluated the proposed FedAR algorithm against the locally trained model, globally trained baseline, and two existing federated learning algorithms on the ISIC2018 dataset and Chest X-ray14 dataset under the settings with a variable number of clients. Our results suggest that FedAR is an effective federated learning algorithm that substantially outperforms existing federated learning approaches.}
}
@article{PENG2024109925,
title = {A multi-center study of ultrasound images using a fully automated segmentation architecture},
journal = {Pattern Recognition},
volume = {145},
pages = {109925},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109925},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006234},
author = {Tao Peng and Caishan Wang and Caiyin Tang and Yidong Gu and Jing Zhao and Quan Li and Jing Cai},
keywords = {Medical image processing, Segmentation, Polygon searching method, Quantum evolution network, Mathematical mapping formula},
abstract = {Accurate organ segmentation in ultrasound (US) images remains challenging because such images have inhomogeneous intensity distributions in their regions of interest (ROIs) and speckle and imaging artifacts. We address this problem by developing a coarse-to-refinement architecture for the segmentation of multiple organs (i.e., the prostate and kidney) in US image datasets from multiple centers. Our proposed architecture has the following four advantages: (1) it inherits the ability of the deep learning models to locate an ROI automatically while also using a principal curve approach to automatically fit a dataset center; (2) it takes advantage of a principal curve-based enhanced polygon searching method, which inherits the principal curve's characteristic to automatically approach the center of the dataset; (3) it incorporates quantum characteristics into a storage-based evolution network together to improve the global search performance of our method, which includes several improvements, such as a new quantum mutation module, a cuckoo search method, and global optimum schemes; (4) it incorporates a suitable mathematical model to smooth the contour of ROIs, which is explained by the parameters of a neural network model. Application of our method to US image datasets of multiple organs and from multiple centers demonstrates that it achieves satisfactory segmentation performance.}
}
@article{CARICHON2023109839,
title = {Unsupervised update summarization of news events},
journal = {Pattern Recognition},
volume = {144},
pages = {109839},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109839},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300537X},
author = {Florian Carichon and Florent Fettu and Gilles Caporossi},
keywords = {Natural language processing, Neural network, Automatic document summarization, Unsupervised approach, Update sentence compression, Information novelty},
abstract = {A long-running event represents a continuous stream of information on a given topic, such as natural disasters, stock market updates, or even ongoing customer relationship. These news stories include hundreds of individual, time-dependent texts. Simultaneously, new technologies have profoundly transformed the way we consume information. The need to obtain quick, relevant, and digest updates continuously has become a crucial issue and creates new challenges for the task of automatic document summarization. To that end, we introduce an innovative unsupervised method based on two competing sequence-to-sequence models to produce short updated summaries. The proposed architecture relies on several parameters to balance the outputs from the two autoencoders. This relation enables the overall model to correlate generated summaries with relevant information coming from both current and previous news iterations. Depending on the model configuration, we are then able to control the novelty or the consistency of terms included in generated summaries. We evaluate our method on a modified version of the TREC 2013, 2014, and 2015 datasets to track continuous events from a single source. We not only achieve state-of-the-art performance similar to other more complex unsupervised sentence compression approaches, but also influence the information included in the model in the summaries.}
}
@article{ZOU2024109896,
title = {Learning geometric consistency and discrepancy for category-level 6D object pose estimation from point clouds},
journal = {Pattern Recognition},
volume = {145},
pages = {109896},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109896},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005940},
author = {Lu Zou and Zhangjin Huang and Naijie Gu and Guoping Wang},
keywords = {6D object pose estimation, 3D object detection, Point cloud processing, Shape recovery},
abstract = {Category-level 6D object pose estimation aims to predict the position and orientation of unseen object instances, which is a fundamental problem in robotic applications. Previous works mainly focused on exploiting visual cues from RGB images, while depth images received less attention. However, depth images contain rich geometric attributes about the object’s shape, which are crucial for inferring the object’s pose. This work achieves category-level 6D object pose estimation by performing sufficient geometric learning from depth images represented by point clouds. Specifically, we present a novel geometric consistency and geometric discrepancy learning framework called CD-Pose to resolve the intra-category variation, inter-category similarity, and objects with complex structures. Our network consists of a Pose-Consistent Module and a Pose-Discrepant Module. First, a simple MLP-based Pose-Consistent Module is utilized to extract geometrically consistent pose features of objects from the pre-computed object shape priors for each category. Then, the Pose-Discrepant Module, designed as a multi-scale region-guided transformer network, is dedicated to exploring each instance’s geometrically discrepant features. Next, the NOCS model of the object is reconstructed according to the integration of consistent and discrepant geometric representations. Finally, 6D object poses are obtained by solving the similarity transformation between the reconstruction and the observed point cloud. Experiments on the benchmark datasets show that our CD-Pose produces superior results to state-of-the-art competitors.}
}
@article{ABU2023109868,
title = {Underwater object classification combining SAS and transferred optical-to-SAS Imagery},
journal = {Pattern Recognition},
volume = {144},
pages = {109868},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109868},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005666},
author = {Avi Abu and Roee Diamant},
keywords = {Feature extraction, Shape descriptors, Self-similarity, Fourier descriptor, Region-based feature, Contour-based features},
abstract = {Combining synthetic aperture sonar (SAS) imagery with optical images for underwater object classification has the potential to overcome challenges such as water clarity, the stability of the optical image analysis platform, and strong reflections from the seabed for sonar-based classification. In this work, we propose this type of multi-modal combination to discriminate between man-made targets and objects such as rocks or litter. We offer a novel classification algorithm that overcomes the problem of intensity and object formation differences between the two modalities. To this end, we develop a novel set of geometrical shape descriptors that takes into account the geometrical relation between the object’s shadow and highlight. Results from 7,052 pairs of SAS and optical images collected during several sea experiments show improved classification performance compared to the state-of-the-art for better discrimination between different types of underwater objects. For reproducability, we share our database.}
}
@article{SHEN2024109913,
title = {ICAFusion: Iterative cross-attention guided feature fusion for multispectral object detection},
journal = {Pattern Recognition},
volume = {145},
pages = {109913},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109913},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006118},
author = {Jifeng Shen and Yifei Chen and Yue Liu and Xin Zuo and Heng Fan and Wankou Yang},
keywords = {Multispectral object detection, Cross-attention, Transformer, Iterative feature fusion},
abstract = {Effective feature fusion of multispectral images plays a crucial role in multispectral object detection. Previous studies have demonstrated the effectiveness of feature fusion using convolutional neural networks, but these methods are sensitive to image misalignment due to the inherent deficiency in local-range feature interaction resulting in the performance degradation. To address this issue, a novel feature fusion framework of dual cross-attention transformers is proposed to model global feature interaction and capture complementary information across modalities simultaneously. This framework enhances the discriminability of object features through the query-guided cross-attention mechanism, leading to improved performance. However, stacking multiple transformer blocks for feature enhancement incurs a large number of parameters and high spatial complexity. To handle this, inspired by the human process of reviewing knowledge, an iterative interaction mechanism is proposed to share parameters among block-wise multimodal transformers, reducing model complexity and computation cost. The proposed method is general and effective to be integrated into different detection frameworks and used with different backbones. Experimental results on KAIST, FLIR, and VEDAI datasets show that the proposed method achieves superior performance and faster inference, making it suitable for various practical scenarios. Code will be available at https://github.com/chanchanchan97/ICAFusion.}
}
@article{LUO2024109906,
title = {Global semantic enhancement network for video captioning},
journal = {Pattern Recognition},
volume = {145},
pages = {109906},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109906},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006040},
author = {Xuemei Luo and Xiaotong Luo and Di Wang and Jinhui Liu and Bo Wan and Lin Zhao},
keywords = {Video captioning, Feature aggregation, Semantic enhancement},
abstract = {Video captioning aims to briefly describe the content of a video in accurate and fluent natural language, which is a hot research topic in multimedia processing. As a bridge between video and natural language, video captioning is a challenging task that requires a deep understanding of video content and effective utilization of diverse video multimodal information. Existing video captioning methods usually ignore the relative importance between different frames when aggregating frame-level video features and neglect the global semantic correlations between videos and texts in learning visual representations, resulting in the learned representations less effective. To address these problems, we propose a novel framework, namely Global Semantic Enhancement Network (GSEN) to generate high-quality captions for videos. Specifically, a feature aggregation module based on a lightweight attention mechanism is designed to aggregate frame-level video features, which highlights features of informative frames in video representations. In addition, a global semantic enhancement module is proposed to enhance semantic correlations for video and language representations in order to generate semantically more accurate captions. Extensive qualitative and quantitative experiments on two public benchmark datasets MSVD and MSR-VTT demonstrate that the proposed GSEN can achieve superior performance than state-of-the-art methods.}
}
@article{ZIELINSKI2024109946,
title = {A network classification method based on density time evolution patterns extracted from network automata},
journal = {Pattern Recognition},
volume = {146},
pages = {109946},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109946},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006441},
author = {Kallil M.C. Zielinski and Lucas C. Ribas and Jeaneth Machicao and Odemir M. Bruno},
keywords = {Complex networks, Cellular automata, Network automata, Pattern recognition},
abstract = {Network modeling has proven to be an efficient tool for many interdisciplinary areas, including social, biological, transportation, and various other complex real-world systems. In addition, cellular automata (CA) are a formalism that has received significant attention in recent decades as a model for investigating patterns in the dynamic spatio-temporal behavior of these systems, based on local rules. Some studies investigate the use of cellular automata to analyze the dynamic behavior of networks and refer to them as network automata (NA). Recently, it has been demonstrated that NA is effective for network classification, as it employs a Time-Evolution Pattern (TEP) for feature extraction. However, the TEPs investigated in previous studies consist of binary values (states) that do not capture the intrinsic details of the analyzed network. Therefore, in this work, we propose alternative sources of information that can be used as descriptors for the classification task, which we refer as Density Time-Evolution Pattern (D-TEP) and State Density Time-Evolution Pattern (SD-TEP). We examine the density of alive neighbors of each node, which is a continuous value, and compute feature vectors based on histograms of TEPs. Our results demonstrate significant improvement over previous studies on five synthetic network datasets, as well as seven real datasets. Our proposed method is not only a promising approach for pattern recognition in networks, but also shows considerable potential for other types of data that can be transformed into network.}
}
@article{XIE2023109821,
title = {ANAS: Asymptotic NAS for large-scale proxyless search and multi-task transfer learning},
journal = {Pattern Recognition},
volume = {144},
pages = {109821},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109821},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005198},
author = {Bangquan Xie and Zongming Yang and Liang Yang and Ruifa Luo and Jun Lu and Ailin Wei and Xiaoxiong Weng and Bing Li},
keywords = {Neural architecture search, Memory consumption, Proxyless search, Multi-task transfer, Classification and segmentation},
abstract = {Neural Architecture Search (NAS) is an emerging solution to design a lightweight network for researchers to obtain a trade-off between accuracy and speed, releasing researchers from tedious repeated trials. However, the main shortcoming of NAS is its high and unstable memory consumption of the search work, especially for large-scale tasks. In this study, the proposed Asymptotic Neural Architecture Search network (ANAS) achieved a proxyless search for large-scale tasks with economic and stable memory consumption. Instead of proxy search like other NAS algorithms, ANAS achieved the large-scale proxyless search that directly learns deep neural network architecture for target task. ANAS reduced the peak value of memory consumption by an asymptotic method, and kept the memory consumption stable by the linkage change of a series of key indexes. A pruning operation and efficient candidate operations decreased the total memory consumption. Finally, ANAS achieved a good trade-off between accuracy and speed for classification tasks on CIFAR-10, CIFAR-100, and ImageNet datasets. Besides, except for the classification task, it achieved excellent multi-task transfer learning ability for implementing the segmentation task on CamVid and Cityscapes. ANAS reached 22.8% test errs with 5 M parameter on ImageNet, and 72.9 mIoU (mean Intersection over Union) with 119.9 FPS (Frames Per Second) on Cityscapes dataset.}
}
@article{CUI2024109877,
title = {QBER: Quantum-based Entropic Representations for un-attributed graphs},
journal = {Pattern Recognition},
volume = {145},
pages = {109877},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109877},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005757},
author = {Lixin Cui and Ming Li and Lu Bai and Yue Wang and Jing Li and Yanchao Wang and Zhao Li and Yunwen Chen and Edwin R. Hancock},
keywords = {Graph embedding, Graph entropy, Quantum walks, Entropic representations},
abstract = {In this paper, we propose a novel framework of computing the Quantum-based Entropic Representations (QBER) for un-attributed graphs, through the Continuous-time Quantum Walk (CTQW). To achieve this, we commence by transforming each original graph into a family of k-level neighborhood graphs, where each k-level neighborhood graph encapsulates the connected information between each vertex and its k-hop neighbor vertices, providing a fine representation to reflect the multi-level topological information for the original global graph structure. To further capture the complicated structural characteristics of the original graph through its neighborhood graphs, we propose to characterize the structure of each neighborhood graph with the Average Mixing Matrix (AMM) of the CTQW, that encapsulates the time-averaged behavior of the CTQW evolved on the neighborhood graph. More specifically, we show how the AMM matrix allows us to compute a Quantum Shannon Entropy for each vertex, and thus compute an entropic signature for each neighborhood graph by measuring the averaged value or the Jensen–Shannon Divergence between the entropies of its vertices. For each original graph, the resulting QBER is defined by gauging how the entropic signat ures vary on its k-level neighborhood graphs with increasing k, reflecting the multi-dimensional entropy-based structure information of the original graph. Experiments on standard graph datasets demonstrate the effectiveness of the proposed QBER approach in terms of the classification accuracies. The proposed approach can significantly outperform state-of-the-art entropic complexity measuring methods, graph kernel methods, as well as graph deep learning methods.}
}
@article{LI2024109893,
title = {Learning consensus-aware semantic knowledge for remote sensing image captioning},
journal = {Pattern Recognition},
volume = {145},
pages = {109893},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109893},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005915},
author = {Yunpeng Li and Xiangrong Zhang and Xina Cheng and Xu Tang and Licheng Jiao},
keywords = {Cross-modal understanding, Visual-semantic interaction, Remote sensing image captioning, Graph convolutional network},
abstract = {Tremendous progresses have been made in remote sensing image captioning (RSIC) task in recent years, yet there still some unresolved problems: (1) facing the gap between the visual features and semantic concepts, (2) reasoning the higher-level relationships between semantic concepts. In this work, we focus on injecting high-level visual-semantic interaction into RSIC model. Firstly, the semantic concept extractor (SCE), end-to-end trainable, precisely captures the semantic concepts contained in the RSIs. In particular, the visual-semantic co-attention (VSCA) is designed to grain coarse concept-related regions and region-related concepts for multi-modal interaction. Furthermore, we incorporate the two types of attentive vectors with semantic-level relational features into a consensus exploitation (CE) block for learning cross-modal consensus-aware knowledge. The experiments on three benchmark data sets show the superiority of our approach compared with the reference methods.}
}
@article{XIANG2024109918,
title = {InvFlow: Involution and multi-scale interaction for unsupervised learning of optical flow},
journal = {Pattern Recognition},
volume = {145},
pages = {109918},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109918},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006167},
author = {Xuezhi Xiang and Rokia Abdein and Ning Lv and Abdulmotaleb El Saddik},
keywords = {Unsupervised optical flow estimation, Involution, Feature interaction, Self-attention, Deformable convolution},
abstract = {The convolution neural network is still the main tool for extracting the image features and the motion features for most of the optical flow models. The convolution neural networks cannot model the long-range dependencies, and more details are lost in deeper layers. All the deficiencies in the extracted features affect the estimated flow. Therefore, in this work, we concentrated on optimizing the convolution neural network in both the encoder and decoder parts to improve the image and motion features. To enhance the image features, we utilize the involution to provide rich features and model the long-range dependencies. In addition, we propose a Multi-Scale-Interaction module which utilizes the self-attention to make an interaction between the feature scales to avoid detail loss. Additionally, we propose a Motion-Features-Optimization block that utilizes the deformable convolution to enhance the motion features. Our model achieves the state-of-the-art performance on Sintel and KITTI 2015 benchmarks.}
}
@article{LO2024109970,
title = {A quantitative method for the assessment of facial attractiveness based on transfer learning with fine-grained image classification},
journal = {Pattern Recognition},
volume = {145},
pages = {109970},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109970},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006684},
author = {Lun-Jou Lo and Chao-Tung Yang and Wen-Chung Chiang and Hsiu-Hsia Lin},
keywords = {Facial attractiveness, Transfer learning, Fine-grained image classification, Facial surface images},
abstract = {In this paper, we investigate a new approach based on a combination of three-dimensional (3D) facial images and deep transfer learning (TL) with fine-grained image classification (FGIC) for quantitative evaluation of facial attractiveness. The 3D facial surface images of patients with and without filtering and the publicly available SCUT-FBP5500 dataset was used for transfer training and model pre-training, respectively. Experimental results show that a bilinear CNN model with a Gaussian filter freezing 80 % of the weights exhibit the strongest performance and lowest average error as a deep learning prediction model; the model was subsequently adopted for automatic assessment of facial attractiveness in clinical application. This is the first TL model with FGIC using 3D facial images for automatic quantitative evaluation of facial attractiveness in patients undergoing Orthognathic surgery (OGS). The developed web browser–based user interface enables effective and rapid assessment, thus contributing to effective patient–clinician communication and decision-making.}
}
@article{WU2023109865,
title = {Audio-driven talking face generation with diverse yet realistic facial animations},
journal = {Pattern Recognition},
volume = {144},
pages = {109865},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109865},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005630},
author = {Rongliang Wu and Yingchen Yu and Fangneng Zhan and Jiahui Zhang and Xiaoqin Zhang and Shijian Lu},
keywords = {Audio-driven talking face generation, Face, Face animation, Audio-to-visual mapping, Image synthesis},
abstract = {Audio-driven talking face generation, which aims to synthesize talking faces with realistic facial animations (including accurate lip movements, vivid facial expression details and natural head poses) corresponding to the audio, has achieved rapid progress in recent years. However, most existing work focuses on generating lip movements only without handling the closely correlated facial expressions, which degrades the realism of the generated faces greatly. This paper presents DIRFA, a novel method that can generate talking faces with diverse yet realistic facial animations from the same driving audio. To accommodate fair variation of plausible facial animations for the same audio, we design a transformer-based probabilistic mapping network that can model the variational facial animation distribution conditioned upon the input audio and autoregressively convert the audio signals into a facial animation sequence. In addition, we introduce a temporally-biased mask into the mapping network, which allows to model the temporal dependency of facial animations and produce temporally smooth facial animation sequence. With the generated facial animation sequence and a source image, photo-realistic talking faces can be synthesized with a generic generation network. Extensive experiments show that DIRFA can generate talking faces with realistic facial animations effectively.}
}
@article{MAHON2024109889,
title = {Minimum description length clustering to measure meaningful image complexity},
journal = {Pattern Recognition},
volume = {145},
pages = {109889},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109889},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005873},
author = {Louis Mahon and Thomas Lukasiewicz},
keywords = {Meaningful complexity, Clustering, Image complexity, Minimum description length, Machine learning, Information theory},
abstract = {We present a new image complexity metric. Existing complexity metrics cannot distinguish meaningful content from noise, and give a high score to white noise images, which contain no meaningful information. We use the minimum description length principle to determine the number of clusters and designate certain points as outliers and, hence, correctly assign white noise a low score. The presented method is a step towards humans’ ability to detect when data contain a meaningful pattern. It also has similarities to theoretical ideas for measuring meaningful complexity. We conduct experiments on seven different sets of images, which show that our method assigns the most accurate scores to all images considered. Additionally, comparing the different levels of the hierarchy of clusters can reveal how complexity manifests at different scales, from local detail to global structure. We then present ablation studies showing the contribution of the components of our method, and that it continues to assign reasonable scores when the inputs are modified in certain ways, including the addition of Gaussian noise and the lowering of the resolution. Code is available at https://github.com/Lou1sM/meaningful_image_complexity.}
}
@article{LIAO2024109958,
title = {Sequence-level affective level estimation based on pyramidal facial expression features},
journal = {Pattern Recognition},
volume = {145},
pages = {109958},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109958},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006568},
author = {Jiacheng Liao and Yan Hao and Zhuoyi Zhou and Jiahui Pan and Yan Liang},
keywords = {Sequence-level affective level estimation, Facial expression features pyramid network, Temporal transformer encoder},
abstract = {People tend to focus on changes in a certain complex human affect in the majority of practical applications of affective computing. Facial expression classification models are unable to represent all human affects through a limited number of expression categories. In this backdrop, this paper studies the Sequence-level affective level estimation (S-ALE), which is more relevant to real scenarios and can depict individual affective level in continuous manner. A spatio-temporal framework applied to S-ALE is proposed, which consists of a Facial Expression Features Pyramid Network (FEFPN) and a Temporal Transformer Encoder (TTE). FEFPN is capable of extracting pyramidal facial expression features, while TTE can effectively capture coarse-grained and fine-grained temporal variations of facial sequences. The proposed model is evaluated on six public datasets across three typical S-ALE tasks (engagement prediction, fatigue detection, and pain assessment), and experimental results show that our method is comparable to or outperforms the state-of-the-art algorithms.}
}
@article{LU2023109861,
title = {A weakly supervised inpainting-based learning method for lung CT image segmentation},
journal = {Pattern Recognition},
volume = {144},
pages = {109861},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109861},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005599},
author = {Fangfang Lu and Zhihao Zhang and Tianxiang Liu and Chi Tang and Hualin Bai and Guangtao Zhai and Jingjing Chen and Xiaoxin Wu},
keywords = {COVID-19, Weakly supervised, Lesion segmentation, Image inpainting},
abstract = {Recently, various fully supervised learning methods are successfully applied for lung CT image segmentation. However, pixel-wise annotations are extremely expert-demanding and labor-intensive, but the performance of unsupervised learning methods are failed to meet the demands of practical applications. To achieve a reasonable trade-off between the performance and label dependency, a novel weakly supervised inpainting-based learning method is introduced, in which only bounding box labels are required for accurate segmentation. Specifically, lesion regions are first detected by an object detection network, then we crop them out of the input image and recover the missing holes to normal regions using a progressive CT inpainting network (PCIN). Finally, a post-processing method is designed to get the accurate segmentation mask from the difference image of input and recovered images. In addition, real information (i.e., number, location and size) of the bounding boxes of lesions from the dataset guides us to make the training dataset for PCIN. We apply a multi-scale supervised strategy to train PCIN for a progressive and stable inpainting. Moreover, to remove the visual artifacts resulted from the invalid features of missing holes, an initial patch generation network (IPGN) is proposed for holes initialization with generated pseudo healthy image patches. Experiments on the public COVID-19 dataset demonstrate that PCIN is outstanding in lung CT images inpainting, and the performance of our proposed weakly supervised method is comparable to fully supervised methods.}
}
@article{YAO2024109934,
title = {Efficient Supervised Graph Embedding Hashing for large-scale cross-media retrieval},
journal = {Pattern Recognition},
volume = {145},
pages = {109934},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109934},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006325},
author = {Tao Yao and Ruxin Wang and Jintao Wang and Ying Li and Jun Yue and Lianshan Yan and Qi Tian},
keywords = {Cross-media retrieval, Supervised graph embedding, Hashing, Discrete optimization},
abstract = {Recently, graph based hashing has gained much attention due to its effectiveness in multi-media retrieval. Although several graph embedding based works have been designed and achieved promising performance, there are still some issues that need to be further studied, including, (1) one significant drawback of graph embedding is its expensive memory and computation cost caused by the graph Laplacian matrix; (2) most pioneer works fail to fully explore the available class labels in training procedure, which generally makes them suffer from unsatisfactory retrieval performance. To overcome these drawbacks, we propose a simple yet effective supervised cross-media hashing scheme, termed Efficient Supervised Graph Embedding Hashing (ESGEH), which can simultaneously learn hash functions and discrete binary codes efficiently. Specifically, ESGEH leverages both class label based semantic embedding and graph embedding to generate a sharing semantic subspace, and class labels are also incorporated to minimize the quantization error for better approximating the generated binary codes. In order to reduce the computational sources, a well-designed intermediate terms decomposition is proposed to avoid explicitly computing the graph Laplacian matrix. Finally, an iterative discrete optimal algorithm is derived to solve above problem, and each subproblem can yield a closed-form solution. Extensive experimental results on four public datasets demonstrate the superiority of the proposed approach over several existing cross-media hashing methods in terms of both accuracy and efficiency.}
}
@article{DAI2024109945,
title = {Multi-label feature selection by strongly relevant label gain and label mutual aid},
journal = {Pattern Recognition},
volume = {145},
pages = {109945},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109945},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300643X},
author = {Jianhua Dai and Weiyi Huang and Chucai Zhang and Jie Liu},
keywords = {Fuzzy rough set, Fuzzy conditional mutual information, Multi-label feature selection, Strongly relevant label gain, Label mutual aid},
abstract = {Multi-label feature selection, which addresses the challenge of high dimensionality in multi-label learning, has wide applicability in pattern recognition, machine learning, and related domains. Most existing studies on multi-label feature selection assume that all labels have the same importance with respect to features, however, they overlook the differences between labels and candidate features relative to selected features and the internal influence of the label space. To address this issue, we propose a novel method for multi-label feature selection that accounts for both the strongly relevant label gain and the label mutual aid. Firstly, we advance two new potential relationships between labels and candidate features relative to selected features, and the label discriminant function is introduced. Secondly, the mutual aid information between labels is presented to describe the internal correlation of the label space. Thirdly, the concept of strongly relevant label gain is defined based on the label discriminant function, which allows better exploration of positive correlation between features. Finally, the experimental results on sixteen multi-label benchmark datasets indicate that the proposed method outperforms other compared representative multi-label feature selection methods.}
}
@article{LIU2024109907,
title = {Cross-scale contrastive triplet networks for graph representation learning},
journal = {Pattern Recognition},
volume = {145},
pages = {109907},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109907},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006052},
author = {Yanbei Liu and Wanjin Shan and Xiao Wang and Zhitao Xiao and Lei Geng and Fang Zhang and Dongdong Du and Yanwei Pang},
keywords = {Graph contrastive learning, Contextual contrastive network, Intrinsic contrastive network},
abstract = {Graph representation learning aims to learn low-dimensional representation for the graph, which has played a vital role in real-world applications. Without requiring additional labeled data, contrastive learning based graph representation learning (or graph contrastive learning) has attracted considerable attention. Recently, one of the most exciting advancement in graph contrastive learning is Deep Graph Infomax (DGI), which maximizes the Mutual Information (MI) between the node and graph representations. However, DGI only considers the contextual node information, ignoring the intrinsic node information (i.e., the similarity between node representations in different views). In this paper, we propose a novel Cross-scale Contrastive Triplet Networks (CCTN) framework, which captures both contextual and intrinsic node information for graph representation learning. Specifically, to obtain the contextual node information, we utilize an infomax contrastive network to maximize the MI between node and graph representations. For acquiring the intrinsic node information, we present a Siamese contrastive network by maximizing the similarity between node representations in different augmented views. Two contrastive networks learn together through a shared graph convolution network to form our cross-scale contrastive triplet networks. Finally, we evaluate CCTN on six real-world datasets. Extensive experimental results demonstrate that CCTN achieves state-of-the-art performance on node classification and clustering tasks.}
}
@article{MESHUWELDE2023109850,
title = {Counting-based visual question answering with serial cascaded attention deep learning},
journal = {Pattern Recognition},
volume = {144},
pages = {109850},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109850},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005484},
author = {Tesfayee MeshuWelde and Lejian Liao},
keywords = {Counting-based visual question answering, Visual geometry group16, Text convolutional neural network, Optimal weighted fused features, Improved tuna swarm optimization, Serial cascaded recurrent neural network with attention mechanism-based long short-term memory},
abstract = {The counting-based questions play a major part in Visual Question Answering (VQA), the most challenging factor is counting the different objects present in the images. Recently more attention is paid to design a model of count-aided VQA. Based on the questions, the VQA system responds with appropriate answers. Yet, the complex questions are necessitating in the system with answers. The earlier models are still facing the challenging problems of counting the various objects within the images as the models become futile to select the features and lack fine-grained representation. In order to sustain the image representation, this paper proposes a new model for VQA using the heuristic approach of serial cascaded deep learning methods. Initially, the standard data regarding images and text data are gathered and fed to the pre-processing process. Consequently, the feature extraction is done on both the image and the text data. Here, the deep features from images are taken using Visual Geometry Group 16 (VGG16) and the text features are extracted using Text Convolutional Neural Network (TCNN). Then, the optimal weighted fused features are obtained, where the weights used for getting the necessary features are tuned via the Improved Tuna Swarm Optimization (ITSO) algorithm. Finally, the counting answers are retrieved based on the given queries, which is carried out via Serial Cascaded Recurrent Neural Network with Attention Mechanism-based Long Short-Term Memory (SCRAM-LSTM). The performance is examined with divergent metrics compared with conventional models. Hence, the findings reveal that it offers superior performance in estimating the appropriate answers. Therefore, the proposed work is widely used for such potential applications as helping blind or visually impaired people to get information, integrating with image retrieval systems, and also for search engines. Especially, it is utilized for the vision and language systems.}
}
@article{LIN2024109878,
title = {Feature disentanglement in one-stage object detection},
journal = {Pattern Recognition},
volume = {145},
pages = {109878},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109878},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005769},
author = {Wenjie Lin and Jun Chu and Lu Leng and Jun Miao and Lingfeng Wang},
keywords = {Object detection, Feature misalignment, Response alignment, Feature disentanglement, Soft sampling},
abstract = {In this paper, an enhanced disentanglement module is proposed to address feature misalignment caused by inherently irreconcilable conflicts between classification and regression tasks in Convolutional Neural Network-based object detectors. The proposed method disentangles features in the feature pyramid network (FPN) at the neck of the architecture. In addition, a response alignment strategy is proposed to reduce inconsistent responses and suppress inferior predictions. Extensive experiments are performed on the MS COCO and PASCAL VOC datasets with different backbones, confirming that the proposed method improves performance significantly. The proposed method exhibits two main advantages over existing solutions—features are disentangled at the neck instead of at the head, enabling comprehensive resolution of feature misalignment, and independent outputs of the two tasks after feature disentanglement are avoided, thereby preventing response inconsistencies.}
}
@article{SUN2024109962,
title = {Reparameterizing and dynamically quantizing image features for image generation},
journal = {Pattern Recognition},
volume = {146},
pages = {109962},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109962},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300660X},
author = {Mingzhen Sun and Weining Wang and Xinxin Zhu and Jing Liu},
keywords = {Vector quantization, Variational auto-encoder, Unconditional image generation, Text-to-image generation, Autoregressive generation},
abstract = {For autoregressive image generation, vector-quantized VAEs (VQ-VAEs) quantize image features with discrete codebook entries and reconstruct images from quantized features. However, they treat each codebook entry separately, which causes losses of image details. In this paper, we propose to reparameterize image features with weight vectors to treat all codebook entries as an entity, and present a novel dynamically vector quantized VAE (DVQ-VAE) to quantize reparameterized image features. Specifically, each image feature corresponds to a weight vector and we sum weighted codebook entries to obtain values of image features. In this way, image features can incorporate information from different codebook entries. Additionally, a novel continuous weight regularization loss is proposed to improve the reconstruction of image details. Our method achieves competitive results with prior state-of-the-art works for image generation and extensive experiments are conducted to take a deep insight into our DVQ-VAE.}
}
@article{2024110909,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {156},
pages = {110909},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(24)00660-5},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006605}
}
@article{BAO2024109894,
title = {Robust embedding regression for semi-supervised learning},
journal = {Pattern Recognition},
volume = {145},
pages = {109894},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109894},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005927},
author = {Jiaqi Bao and Mineichi Kudo and Keigo Kimura and Lu Sun},
keywords = {Feature selection, Semi-supervised learning, Ridge regression, Nuclear norm},
abstract = {To utilize both labeled data and unlabeled data in real-world applications, semi-supervised learning is widely used as an effective technique. However, most semi-supervised methods do not perform well when there are many noises and redundant information in the original data. To address these issues, in this paper, we proposed a novel approach called robust embedding regression (RER) for semi-supervised learning by inheriting the advantages of the existing semi-supervised learning, robust linear regression, and low-rank representation techniques. Specifically, RER constructs a more robust and accurate graph by adaptively arranging the weight coefficient for each data point. Furthermore, the low-rank representation is introduced to reduce the negative influence of the redundant features and noises residing in the original data while the graph construction. More importantly, the proper norms are imposed on both the reconstruction and regularization terms to further improve the robustness and earn feature/sample selection. We designed an effective iterative algorithm to optimize the problem of RER. Comprehensive experimental results conducted on both synthetic and real-world datasets indicate that RER is superior in classification and clustering performance and robust to different types of noise compared with the existing semi-supervised methods.}
}
@article{PANG2024109947,
title = {MCNet: Magnitude consistency network for domain adaptive object detection under inclement environments},
journal = {Pattern Recognition},
volume = {145},
pages = {109947},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109947},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006453},
author = {Jian Pang and Weifeng Liu and Bingfeng Zhang and Xinghao Yang and Baodi Liu and Dapeng Tao},
keywords = {Object detection, Inclement environments, Frequency domain, Magnitude spectrum},
abstract = {Deep learning-based object detection methods have achieved promising results in normal scenarios, while such methods often fail to locate objects from the disturbed images captured in inclement environments. Most existing methods utilized denoise modules to assist the detection network or exploit prior knowledge to reduce the environment interference remaining in the features, which ignores the essential role of the frequency element. From a novel perspective, we observe that the inclement environment alters the frequency content in the features, which in turn crushes the detection. To tickle this problem, we present the Magnitude Consistency Network (MCNet) to distill the irrelevant contents in the frequency domain. The MCNet is composed of the detection network and the magnitude corrector. The detection network is able to locate and classify objects. However, in inclement environments, the crucial information about the objects of interest is disrupted by the nuisance noise introduced from the inclement environments. The magnitude corrector can recover relevant information about the objects in the frequency domain by distilling the irrelevant factors and refining the affected features. Alternately optimizing the magnitude corrector and the detection network gradually makes the frequency content between the disturbed image and the clear image to be consistent. By distilling the irrelevant noise in the feature, the detection network can learn domain-invariant representations. Extensive experiments prove that the proposed method is effective and outperforms existing methods by a clear margin on four datasets.}
}
@article{PARK2024109919,
title = {Mutual Domain Adaptation},
journal = {Pattern Recognition},
volume = {145},
pages = {109919},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109919},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006179},
author = {Sunghong Park and Myung Jun Kim and Kanghee Park and Hyunjung Shin},
keywords = {Domain adaptation, Semi-supervised learning, Label propagation, Pseudo-labeling},
abstract = {To solve the label sparsity problem, domain adaptation has been well-established, suggesting various methods such as finding a common feature space of different domains using projection matrices or neural networks. Despite recent advances, domain adaptation is still limited and is not yet practical. The most pronouncing problem is that the existing approaches assume source-target relationship between domains, which implies one domain supplies label information to another domain. However, the amount of label is only marginal in real-world domains, so it is unrealistic to find source domains having sufficient labels. Motivated by this, we propose a method that allows domains to mutually share label information. The proposed method finds a projection matrix that matches the respective distributions of different domains, preserves their respective geometries, and aligns their respective class boundaries. The experiments on benchmark datasets show that the proposed method outperforms relevant baselines. In particular, the results on varying proportions of labels present that the fewer labels the better improvement.}
}
@article{UMIRZAKOVA2023109866,
title = {Deep learning-driven diagnosis: A multi-task approach for segmenting stroke and Bell's palsy},
journal = {Pattern Recognition},
volume = {144},
pages = {109866},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109866},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005642},
author = {Sabina Umirzakova and Shabir Ahmad and Sevara Mardieva and Shakhnoza Muksimova and Taeg Keun Whangbo},
keywords = {Segmentation, Face parsing, Early stroke detection, Bell's palsy detection},
abstract = {Strong efforts have been undertaken to enhance the diagnosis and identification of diseases that cause facial paralysis, such as Bell's palsy and stroke, because of their detrimental social effects. Stroke is one of the most serious and potentially fatal conditions among the major cardiovascular disorders. We are introducing a deep-learning-based method for early diagnosis of facial paralysis diseases such as stroke and Bell's palsy. Recognizing the costs associated with traditional diagnostic techniques like magnetic resonance tomography (MRI) and computed tomography (CT) scan images, our model employs a multi-task network, integrating face parsing, facial asymmetry parsing, and category enhancement. Spatial inconsistencies are addressed via a depth-map estimation module that leverages an instance-specific kernel approach. To clarify the boundaries of facial components, we use category edge detection with a foreground attention module, generating generic geometric structures and detailed semantic cues. Our model is trained on two datasets, comprising individuals with regular smiles and those with one-sided facial weakness. This cost-effective, easily accessible solution can streamline the diagnostic process, minimizing data gaps, and reducing needless rescreening and intervention costs.}
}
@article{JOSHI2024109935,
title = {A novel minutiae-oriented approach for partial fingerprint-based MasterPrint mitigation},
journal = {Pattern Recognition},
volume = {145},
pages = {109935},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109935},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006337},
author = {Mahesh Joshi and Bodhisatwa Mazumdar and Somnath Dey},
keywords = {Partial fingerprint identification, Biometric security, User recognition, MasterPrint vulnerability, MasterPrint mitigation},
abstract = {Partial fingerprint identification systems recognise an individual when the sensor size has a small form factor in accepting a full fingerprint. However, the distinctive features within a partial fingerprint are significantly less. Hence, the uniqueness of a partial fingerprint cannot be assured, leading to the possibility of identifying multiple users. A MasterPrint is a partial fingerprint identifying at least 4% distinct individuals in a partial fingerprint identification system. This work addresses the MasterPrint vulnerability by proposing a novel partial fingerprint identification scheme that extracts minutiae-oriented local features from binarized and thinned partial fingerprint images over eight axes emerging from a reference minutia. It also introduces a metric to compute the similarity score between two partial fingerprint templates. The results are compared with the baseline minutiae matching (BMM) method, a modified Speeded-Up Robust Features (SURF) based approach, VeriFinger 12.1 SDK, standard NIST NBIS, and Ridge Shape Feature (RSF) scheme. The experiments employing partial fingerprint datasets cropped from standard FVC2002 DB1_A, FVC2002 DB2_A, NIST Special Databases (sd302b and sd302d), and CrossMatch VeriFinger dataset have demonstrated that the proposed method generates the lowest MasterPrints with the highest identification accuracy.}
}
@article{WANG2023109890,
title = {Crisis event summary generative model based on hierarchical multimodal fusion},
journal = {Pattern Recognition},
volume = {144},
pages = {109890},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109890},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005885},
author = {Jing Wang and Shuo Yang and Hui Zhao},
keywords = {Multimodal summary generation, Unimodal bias, Crisis event, Hierarchy, Dynamic selection},
abstract = {How to quickly obtain information about crisis events on social media such as Twitter and Weibo is crucial for follow-up rescue work and the promotion of postdisaster reconstruction. Therefore, it is very important to obtain useful information through multimodal summary generation technology. The current technology for generating crisis event summaries is mainly affected by unimodal bias and disregards the diversity of information in text and images. To solve these problems, this paper proposes a hierarchical multimodal crisis event summary generation model based on the modal alignment premise and hierarchical thinking. First, the visual context vector and text context vector are obtained, and then the hierarchical multimodal pointer model is employed to generate the text summary. Thus, the modal deviation is solved. Second, to select high-quality images, this paper proposes a dynamic selection strategy, which to some extent considers the requirements of the high correlation between text and images and the diversity of crisis information. Last, the experimental results based on the crisis event data in the MSMO dataset show that the proposed model achieves good performance in the summary generation and image selection of crisis events.}
}
@article{ZUO2024109963,
title = {Auto-adjustable hypergraph regularized non-negative matrix factorization for image clustering},
journal = {Pattern Recognition},
volume = {145},
pages = {109963},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109963},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006611},
author = {Hongliang Zuo and Shuo Li and Cong Liang and Juntao Li},
keywords = {Non-negative matrix factorization, Hypergraph regularization, Robustness, Outlier},
abstract = {Non-negative matrix factorization (NMF) is an effective method for image clustering. However, relatively fixed graph regularization terms and loss functions have been adopted by recently proposed variants of NMF, and their clustering performance can be improved by incorporating configurable parameters. In this paper, an auto-adjustable hypergraph regularized non-negative matrix factorization (AHRNMF) algorithm was proposed. In the AHRNMF framework, we proposed a piecewise loss function and an innovative auto-adjustable hypergraph. The loss function incorporates two adaptive parameters, harmonizing reconstruction error and anti-outlier efficacy. Hypergraph construction relies on the calculation of two k-nearest neighbors (KNN) with different scales. Furthermore, an KNN-based algorithm was developed to assist AHRNMF in achieving auto-adjustment, which can automatically detect outliers without determining the number of clusters in advance. It was demonstrated by extensive experiments that the proposed AHRNMF outperforms other state-of-the-art methods.}
}
@article{LIANG2024109901,
title = {Mask-guided multiscale feature aggregation network for hand gesture recognition},
journal = {Pattern Recognition},
volume = {145},
pages = {109901},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109901},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300599X},
author = {Hao Liang and Lunke Fei and Shuping Zhao and Jie Wen and Shaohua Teng and Yong Xu},
keywords = {Hand gesture recognition, Attention mechanism, Multiscale, Feature aggregation, Mask branch},
abstract = {Hand gesture recognition from images is a longstanding computer vision task that can be used to build a potential bridge for human-computer interaction and sign language translation. For number of methods proposed for hand gesture recognition (HGR); however, difficult scenarios such as different scales of hand gestures and complex backgrounds exist, making them less effective. In this paper, we propose an end-to-end multiscale feature learning network for HGR, which consists of a CNN-based backbone network, a feature aggregation pyramid network (FAPN) embedded with a two-stage expansion-squeeze-aggregation (ESA) module, and three task-specific prediction branches. First, the backbone network extracts multiscale features from the original hand gesture images. Furthermore, the FAPN embedded with two-stage ESA extensively exploits multiscale feature information and learns hand gesture-specific features at different scales. Then, the mask loss guides the network to locate hand-specific regions during the training stage, and finally, the classification and regression branches output the category and location of a hand gesture during the model training and prediction. The experimental results on two publicly available datasets show that the proposed method outperforms most state-of-the-art HGR methods.}
}
@article{LI2024109882,
title = {TransOSV: Offline Signature Verification with Transformers},
journal = {Pattern Recognition},
volume = {145},
pages = {109882},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109882},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005800},
author = {Huan Li and Ping Wei and Zeyu Ma and Changkai Li and Nanning Zheng},
keywords = {Signature verification, Transformer, Holistic encoder, Part decoder},
abstract = {Signature verification is a frequently-used forensics technology in numerous safety-critical situations. Although convolutional neural networks (CNNs) have made significant advancements in the field of signature verification, their reliance on local neighborhood operations poses limitations in capturing the global contextual relationships among signature strokes. To overcome this weakness, in this paper, we propose a novel holistic-part unified model named TransOSV based on the vision transformer framework to solve offline signature verification problem. The signature images are first encoded into patch sequences by the proposed transformer-based holistic encoder to learn the global signature representation. Second, considering the subtle local difference between the genuine signature and forged signature, we design a contrast based part decoder along with a sparsity loss, which are utilized to learn the discriminative part features. With the learned holistic features and part features, the proposed model is optimized by the contrast loss function. To reduce the influence of sample imbalance, we also formulate a new focal contrast loss function. Furthermore, we conduct the proposed model to learn signature representations for writer-dependent signature verification task. The experimental results demonstrate the potential of the proposed TransOSV model for both writer-independent and writer-dependent signature verification tasks, achieving remarkable performance improvements and competitive results on four widely-used offline signature datasets.}
}
@article{WANG2024109911,
title = {Exploring global information for session-based recommendation},
journal = {Pattern Recognition},
volume = {145},
pages = {109911},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109911},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300609X},
author = {Ziyang Wang and Wei Wei and Ding Zou and Yifan Liu and Xiao-Li Li and Xian-Ling Mao and Minghui Qiu},
keywords = {Session-based recommendation, Graph neural network, Graph contrastive learning},
abstract = {Session-based recommendation (SBR) aims to recommend items based on anonymous behavior sequences. However, most existing SBR approaches focus solely on the current session while neglecting the item-transition information from other sessions, which suffer from the inability of modeling the complicated item-transition. To address the limitations, we introduce global item-transition information to augment the modeling of item-transitions. Specifically, we first propose a basic GNN-based framework (BGNN), which solely uses session-level item-transition information. Based on BGNN, we propose a novel approach, called Session-based Recommendation with Global Information (SRGI), which infers the user preferences via fully exploring item-transitions over all sessions from two different perspectives: (i) Fusion-based Model (SRGI-FM), which recursively incorporates the neighbor embeddings of each node on global graph into the learning process of item representation; and (ii) Constrained-based Model (SRGI-CM), which treats the global-level information as a constraint to ensure the learned item embeddings are consistent with the global item-transition. Extensive experiments conducted on three popular benchmark datasets demonstrate that both SRGI-FM and SRGI-CM outperform the state-of-the-art methods.}
}
@article{NELLAS2023109871,
title = {Two phase cooperative learning for supervised dimensionality reduction},
journal = {Pattern Recognition},
volume = {144},
pages = {109871},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109871},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005691},
author = {Ioannis A. Nellas and Sotiris K. Tasoulis and Spiros V. Georgakopoulos and Vassilis P. Plagianakos},
keywords = {Artificial neural networks, Deep learning, Dimensionality reduction, Autoencoders, Image classification},
abstract = {The simultaneous minimization of the reconstruction and classification error is a hard non convex problem, especially when a non-linear mapping is utilized. To overcome this obstacle, motivated by the widespread success of Cooperative Neural Networks, an innovative supervised dimensionality reduction framework is proposed, based on a cooperative two phase optimization strategy. Specifically, the proposed framework that requires minimal parameter adjustment consists of an autoencoder for dimensionality reduction and a separator network for separability assessment of the embedding. This scheme results in meaningful and discriminable codes, which are optimized for the classification task and are exploitable by any trainable classifier. The experimental results showed that the proposed methodology achieved competitive results against the state-of-the-art competing methods, while being much more efficient in terms of parameter count. Finally, it was empirically justified that the proposed methodology introduces advanced behavioural explainability, while enabling applicability for image generation tasks.}
}
@article{PANDA2024109916,
title = {Compositional Zero-Shot Learning using Multi-Branch Graph Convolution and Cross-layer Knowledge Sharing},
journal = {Pattern Recognition},
volume = {145},
pages = {109916},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109916},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006143},
author = {Aditya Panda and Dipti Prasad Mukherjee},
keywords = {Compositional zero shot, Knowledge sharing, State-object composition, Feasibility assessment, Graph convolution},
abstract = {The purpose of the Compositional Zero-Shot Learning (CZSL) is to recognize new state-object compositions of known objects and known states. For example, the CZSL model should recognize young cat when the model has seen images of a few state-object compositions like young tiger, old tiger and old cat. The visual features of a state may have significant variation across different compositions of the state with different objects. For example, in the compositions peeled apple and peeled orange, the state peeled has different visual features. This context dependency of state features is difficult to learn from the annotated images of different compositions. We propose a Graph Convolutional Network (GCN) with two distinct branches for object and state recognition. GCN utilizes its ability to aggregate features from the non-Euclidean neighbourhood. This aggregation ability of GCN can help our model to capture the intricate dependencies between visual features of state and object. We also propose a novel cross-layer knowledge sharing strategy for the purpose of reducing ambiguity in learning state features due to context dependency. The proposed cross-layer knowledge sharing helps in identifying a set of objects having feasible compositions with a particular state and thereby reducing the ambiguity in the state features. Finally, we propose a feasibility based penalization to better regularize the joint prediction from the two branches of the network. The proposed algorithm is evaluated on the challenging benchmarks and competitive results in comparison to state-of-the-art algorithms have been achieved.}
}
@article{YANG2024109944,
title = {Efficient disentangled representation learning for multi-modal finger biometrics},
journal = {Pattern Recognition},
volume = {145},
pages = {109944},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109944},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006428},
author = {Weili Yang and Junduan Huang and Dacan Luo and Wenxiong Kang},
keywords = {Finger multi-modal, Disentangled representation learning, Multi-task distillation, Heterogeneous recognition, Fusion recognition},
abstract = {Most multi-modal biometric systems use multiple devices to capture different traits and directly fuse multi-modal data while ignoring correlation information between modalities. In this paper, finger skin and finger vein images are acquired from the same region of the finger and therefore have a higher correlation. To represent data efficiently, we propose a novel Finger Disentangled Representation Learning Framework (FDRL-Net) that is based on a factorization concept, which disentangles each modality into shared and private features, thereby improving complementarity for better fusion and extracting modality-invariant features for heterogeneous recognition. Besides, to capture as much finger texture as possible, we utilize three-view finger images to reconstruct full-view multi-spectral finger traits, which increases the identity information and the robustness to finger posture variation. Finally, a Boat-Trackers-based multi-task distillation method is proposed to migrate the feature representation ability to a lightweight multi-task network. Extensive experiments on six single-view multi-spectral finger datasets and two full-view multi-spectral finger datasets demonstrate the effectiveness of our approach.}
}
@article{QIU2023109863,
title = {Few-shot forgery detection via Guided Adversarial Interpolation},
journal = {Pattern Recognition},
volume = {144},
pages = {109863},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109863},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005617},
author = {Haonan Qiu and Siyu Chen and Bei Gan and Kun Wang and Huafeng Shi and Jing Shao and Ziwei Liu},
keywords = {Forgery detection, DeepFake, Few-shot, Face manipulation},
abstract = {The increase in face manipulation models has led to a critical issue in society—the synthesis of realistic visual media. With the emergence of new forgery approaches at an unprecedented rate, existing forgery detection methods suffer from significant performance drops when applied to unseen novel forgery approaches. In this work, we address the few-shot forgery detection problem by (1) designing a comprehensive benchmark based on coverage analysis among various forgery approaches, and (2) proposing Guided Adversarial Interpolation (GAI). Our key insight is that there exist transferable distribution characteristics between majority and minority forgery classes.11Majority class: class with abundant samples; minority class: class with scarce samples. Specifically, we enhance the discriminative ability against novel forgery approaches via adversarially interpolating the forgery artifacts of the minority samples to the majority samples under the guidance of a teacher network. Unlike the standard re-balancing method which usually results in over-fitting to minority classes, our method simultaneously takes account of the diversity of majority information as well as the significance of minority information. Extensive experiments demonstrate that our GAI achieves state-of-the-art performances on the established few-shot forgery detection benchmark. Notably, our method is also validated to be robust to choices of majority and minority forgery approaches.}
}
@article{LI2023109835,
title = {Dynamics-aware loss for learning with label noise},
journal = {Pattern Recognition},
volume = {144},
pages = {109835},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109835},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005332},
author = {Xiu-Chuan Li and Xiaobo Xia and Fei Zhu and Tongliang Liu and Xu-Yao Zhang and Cheng-Lin Liu},
keywords = {Label noise, Dynamics, Robust loss function},
abstract = {Label noise poses a serious threat to deep neural networks (DNNs). Employing robust loss functions which reconcile fitting ability with robustness is a simple but effective strategy to handle this problem. However, the widely-used static trade-off between these two factors contradicts the dynamics of DNNs learning with label noise, leading to inferior performance. Therefore, we propose a dynamics-aware loss (DAL) to solve this problem. Considering that DNNs tend to first learn beneficial patterns, then gradually overfit harmful label noise, DAL strengthens the fitting ability initially, then gradually improves robustness. Moreover, at the later stage, to further reduce the negative impact of label noise and combat underfitting simultaneously, we let DNNs put more emphasis on easy examples than hard ones and introduce a bootstrapping term. Both the detailed theoretical analyses and extensive experimental results demonstrate the superiority of our method.}
}
@article{GILLIOZ2023109859,
title = {Graph-based pattern recognition on spectral reduced graphs},
journal = {Pattern Recognition},
volume = {144},
pages = {109859},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109859},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005575},
author = {Anthony Gillioz and Kaspar Riesen},
keywords = {Graph matching, Graph classification, Graph reduction},
abstract = {Graph-based pattern recognition – in particular in conjunction with large graphs – is often computationally expensive. This hampers, or makes it at least challenging, to employ graph-based representations for real-world data. To address this issue, we propose a method for reducing the size of the underlying graphs to their most important substructures using spectral graph clustering. The proposed method partitions the nodes of the graphs into clusters and then merges each cluster into supernodes. The motivation of this procedure is to reduce the computational cost of any graph comparison algorithm while maintaining the accuracy of the final classification. To assess the benefits and limitations of our method, we conduct thorough experiments on nine real-world datasets with different levels of graph reductions. The classification is obtained by four different graph classifiers (viz. a KNN based on graph edit distance, two SVMs based on a shortest path graph and a Weisfeiler–Lehman graph kernel, as well as a graph neural network). The results indicate that we can reduce computation time by up to two orders of magnitude without substantially degrading the classification accuracy.}
}
@article{LIU2024109904,
title = {Few-shot classification guided by generalization error bound},
journal = {Pattern Recognition},
volume = {145},
pages = {109904},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109904},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006027},
author = {Fan Liu and Sai Yang and Delong Chen and Huaxi Huang and Jun Zhou},
keywords = {Few-shot classification, Generalization error bound, Self-supervised learning, Knowledge distillation},
abstract = {Recently, transfer learning has generated promising performance in few-shot classification by pre-training a backbone network on base classes and then applying it to novel classes. Nevertheless, there lacks a theoretical analysis on how to reduce the generalization error during the learning process. To fill this gap, we prove that the classification error bound on novel classes is mainly determined by the base-class generalization error, given the base-novel domain divergence and the novel-class generalization error produced by an incremental learner using novel samples. The novel-class generalization error is further decided by the base-class empirical error and the VC-dimension of the hypothesis space. Based on this theoretical analysis, we propose a Born-Again Networks under Self-supervised Label Augmentation (BANs-SLA) method to improve the generalization capability of classifiers. In this method, cross-entropy and supervised contrastive losses are simultaneously used to minimize the base-class empirical error in the expanded space with SLA. Afterward, BANs are adopted to transfer the knowledge sequentially across generations, which acts as an effective regularizer to trade-off the VC-dimension. Extensive experimental results have verified the effectiveness of our method, which establishes the new state-of-the-art performance on popular few-shot classification benchmark datasets.}
}
@article{SUN2023109870,
title = {Heterogeneous network representation learning based on role feature extraction},
journal = {Pattern Recognition},
volume = {144},
pages = {109870},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109870},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300568X},
author = {Yueheng Sun and Mengyu Jia and Chang Liu and Minglai Shao},
keywords = {Representation learning, Role discovery, Heterogeneous network, Matrix factorization},
abstract = {Since most of the real-world networks are heterogeneous, existing methods cannot characterize the roles of nodes in heterogeneous networks. The neighborhood structure of nodes in heterogeneous networks largely determines the node roles, and the basic statistical features of nodes describe the topology of nodes to some extent, so extracting structural features from the adjacency matrix of networks is crucial for role-oriented network representation learning(structural equivalence). Therefore, in this paper, we propose a heterogeneous network representation learning model based on role feature extraction, called HRFE(Heterogeneous Network Representation Learning for Role Feature Extraction). Firstly, we perform feature extraction for each node in the heterogeneous network to obtain a high-dimensional feature matrix, then perform role discovery using non-negative matrix decomposition techniques to obtain a role-based node representation, and finally verify the effectiveness of the model HRFE through experiments on a large number of real datasets.}
}
@article{LI2023109875,
title = {Memory efficient data-free distillation for continual learning},
journal = {Pattern Recognition},
volume = {144},
pages = {109875},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109875},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005733},
author = {Xiaorong Li and Shipeng Wang and Jian Sun and Zongben Xu},
keywords = {Continual learning, Catastrophic forgetting, Knowledge distillation},
abstract = {Deep neural networks suffer from the catastrophic forgetting phenomenon when trained on sequential tasks in continual learning, especially when data from previous tasks are unavailable. To mitigate catastrophic forgetting, various methods either store data from previous tasks, which may raise privacy concerns, or require large memory storage. Particularly, the distillation-based methods mitigate catastrophic forgetting by using proxy datasets. However, proxy datasets may not match the distributions of the original datasets of previous tasks. To address these problems in a setting where the full training data of previous tasks are unavailable and memory resources are limited, we propose a novel data-free distillation method. Our method encodes knowledge of previous tasks into network parameter gradients by Taylor expansion, deducing a regularizer relying on gradients in network training loss. To improve memory efficiency, we design an approach to compressing the gradients in the regularizer. Moreover, we theoretically analyze the approximation error of our method. Experimental results on multiple datasets demonstrate that our proposed method outperforms the existing approaches in continual learning.}
}
@article{FAN2024109899,
title = {Learning correlation information for multi-label feature selection},
journal = {Pattern Recognition},
volume = {145},
pages = {109899},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109899},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005976},
author = {Yuling Fan and Jinghua Liu and Jianeng Tang and Peizhong Liu and Yaojin Lin and Yongzhao Du},
keywords = {Multi-label feature selection, Label correlations, Feature redundancy, Manifold framework, Adaptive spectral graph},
abstract = {In many real-world multi-label applications, the content of multi-label data is usually characterized by high dimensional features, which contains complex correlation information, i.e., label correlations and redundant features. To alleviate the problem, we present a novel scheme, called learning correlation information for multi-label feature selection (LCIFS) method, by jointly digging up label correlations and controlling feature redundancy. To be specific, the regression model via manifold framework is presented to fit the relationship between feature space and label distribution, during which adaptive spectral graph is leveraged to learn more precise structural correlations of labels simultaneously. Besides, we utilize the relevance of features to constrain the redundancy of the generated feature subset, and a general ℓ2,p-norm regularized model is employed to fulfill more robust feature selection. The proposed method is transformed into an explicit optimization function, which is conquered by an efficient iterative optimization algorithm. Finally, we conduct comprehensive experiments on twelve realistic multi-label datasets, including text domain, image domain, and audio domain. The statistic results demonstrate the effectiveness and superiority of the proposed method among nine competition methods.}
}
@article{XU2024109969,
title = {Shadow-aware dynamic convolution for shadow removal},
journal = {Pattern Recognition},
volume = {146},
pages = {109969},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109969},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006672},
author = {Yimin Xu and Mingbao Lin and Hong Yang and Fei Chao and Rongrong Ji},
keywords = {Dynamic convolution, Image processing},
abstract = {With a wide range of shadows in many collected images, shadow removal has aroused increasing attention since uncontaminated images are of vital importance for many computer vision tasks. Current methods consider the same convolution operations for both shadow and non-shadow regions while ignoring the large gap between the color mappings for the shadow region and the non-shadow region, leading to poor quality of reconstructed images and a heavy computation burden. To solve this problem, this paper introduces a novel plug-and-play Shadow-Aware Dynamic Convolution (SADC) module to decouple the interdependence between the shadow region and the non-shadow region. Inspired by the fact that the color mapping of the non-shadow region is easier to learn, our SADC processes the non-shadow region with a lightweight convolution module in a computationally cheap manner and recovers the shadow region with a more complicated convolution module to ensure the quality of image reconstruction. Given that the non-shadow region often contains more background color information, we further develop a novel intra-convolution distillation loss to strengthen the information flow from the non-shadow region to the shadow region. Extensive experiments on the ISTD and SRD datasets show our method achieves better performance in shadow removal over many state-of-the-art methods. Codes have been made available at https://github.com/xuyimin0926/SADC.}
}
@article{DU2024109917,
title = {Robust multi-agent reinforcement learning via Bayesian distributional value estimation},
journal = {Pattern Recognition},
volume = {145},
pages = {109917},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109917},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006155},
author = {Xinqi Du and Hechang Chen and Che Wang and Yongheng Xing and Jielong Yang and Philip S. Yu and Yi Chang and Lifang He},
keywords = {Multi-agent reinforcement learning, Bayesian inference, Distributional value function, Deep reinforcement learning},
abstract = {Reinforcement learning in multi-agent scenarios is essential for real-world applications as it can vividly depict agents’ collaborative and competitive behaviors from a perspective closer to reality. However, most existing studies suffer from poor robustness, preventing multi-agent reinforcement learning from practical applications where robustness is the core indicator of system security and stability. In view of this, we propose a novel Bayesian Multi-Agent Reinforcement Learning method, named BMARL, which leverages the distributional value function calculated by Bayesian inference to improve the robustness of the model. Specifically, Bayesian linear regression is adopted to estimate a posterior distribution concerning value function parameters, rather than approximating an expectation value for Q-value by point estimation. In this way, the value function is more generalized than previously obtained by point estimation, which is beneficial to the robustness of our model. Meanwhile, we utilize the Gaussian prior knowledge to integrate more prior knowledge while estimating the value function, which improves learning efficiency. Extensive experimental results on three benchmark multi-agent environments comparing with seven state-of-the-art methods demonstrate the superiority of BMARL in terms of both robustness and efficiency.}
}
@article{ZOU2024109900,
title = {Gradient-based multi-label feature selection considering three-way variable interaction},
journal = {Pattern Recognition},
volume = {145},
pages = {109900},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109900},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005988},
author = {Yizhang Zou and Xuegang Hu and Peipei Li},
keywords = {Multi-label learning, Feature selection, Gradient-based methods, Three-way interaction, Information theory},
abstract = {Nowadays, Multi-Label Feature Selection (MLFS) attracts more and more attention to tackle the high-dimensional problem in multi-label data. A key characteristic of existing gradient-based MLFS methods is that they typically consider two-way variable correlations between features and labels, including feature-feature and label-label correlations. However, two-way correlations are not sufficient to steer feature selection since such correlations vary given different additional variables in practical scenarios, which leads to the selected features with relatively-poor classification performance. Motivated by this, we capture three-way variable interactions including feature-feature-label and feature-label-label interactions to further characterize the fluctuating correlations in the context of another variable, and propose a new gradient-based MLFS approach incorporating the above three-way variable interactions into a global optimization objective. Specifically, based on information theory, we develop second-order regularization penalty terms to regard three-way interactions while jointly combining with the main loss term in regard to feature relevance. Then the objective function can be efficiently optimized via a block-coordinate gradient descent schema. Meanwhile, we provide a theoretical analysis demonstrating the effectiveness of the regularization terms in exploiting three-way interaction. In addition, experiments conducted on a series of benchmark data sets also verify the validity of the proposed method on multiple evaluation metrics.}
}
@article{IGARCIA2023109887,
title = {A Gaussian kernel for Kendall’s space of m-D shapes},
journal = {Pattern Recognition},
volume = {144},
pages = {109887},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109887},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300585X},
author = {Vicent Gimeno {i Garcia} and Ximo Gual-Arnau and M. Victoria Ibáñez and Amelia Simó},
keywords = {Riemannian manifold, Kendall shape space, Embedding, Reproducible Kernel Hilbert Space},
abstract = {In this paper, we develop an approach to exploit kernel methods with data lying on the m-D Kendall shape space. When data arise in a finite-dimensional curved Riemannian manifold, as in this case, the usual Euclidean computer vision and machine learning algorithms must be treated carefully. A good approach is to use positive definite kernels on manifolds to embed the manifold with its corresponding metric in a high-dimensional reproducing kernel Hilbert space, where it is possible to utilize algorithms developed for linear spaces. Different Gaussian kernels can be found in the literature on the 2-D Kendall shape space to perform this embedding. The main novelty of this work is to provide a Gaussian kernel for the m-D Kendall shape space. This new Kernel coincides in the case m=2 with the Gaussian kernels most widely used in the Kendall planar shape space and allows to define an embedding of the m-D Kendall shape space into a reproducible kernel Hilbert space. As far as we know, the complexity of the m-D Kendall shape space has meant that this embedding has not been addressed in the literature until now. This methodology will be tested on a machine learning problem with a simulated and a real data set.}
}
@article{ZHANG2023109892,
title = {Expansion window local alignment weighted network for fine-grained sketch-based image retrieval},
journal = {Pattern Recognition},
volume = {144},
pages = {109892},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109892},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005903},
author = {Zi-Chao Zhang and Zhen-Yu Xie and Zhen-Duo Chen and Yu-Wei Zhan and Xin Luo and Xin-Shun Xu},
keywords = {Fine-grained sketch-based image retrieval, Local feature, Expansion window, Attention mechanism},
abstract = {Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) is a worthwhile task, which can be useful in many scenarios like recommendation systems, receiving a great deal of attention. In this study, we analyze challenges faced in FG-SBIR and propose a novel Expansion Window Local Alignment Weighted Network (EWLAW-Net). Specifically, it contains two main components: the Expansion Window Local Alignment module (EWLA) and the Local Weighted Fusion module (LWF). The EWLA module adopts an expansion window mechanism to align local features extracted from the backbone with the same semantic meaning between photos and sketches. The LWF module assigns weights to each local feature of the sketch after evaluating their importance and fuses them to calculate the similarity between the sketch and photos for retrieval. Experiments are conducted on five datasets and the results demonstrate the effectiveness of the proposed method.}
}
@article{MAO2023109864,
title = {A novel method of human identification based on dental impression image},
journal = {Pattern Recognition},
volume = {144},
pages = {109864},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109864},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005629},
author = {Jiafa Mao and Lixin Wang and Ning Wang and Yahong Hu and Weigou Sheng},
keywords = {Dental impression image, Tooth print detection, Multi-scale feature extraction, Feature aggregation, Human identification},
abstract = {In large-scale natural disasters and special criminal cases, surface features of bodies, such as faces and fingerprints, are easily destroyed. Teeth possess strong high-temperature resistance, corrosion resistance, and high hardness, which can compensate for the shortcomings of the aforementioned situations. This paper proposes an identification method based on the aggregated features of multi-scale dental impression images. Firstly, a method exploiting the adaptive object detection method based on YOLOv8 is proposed to segment toothprints. Next, a novel geometric feature named calibrated offset distance is extracted, combined with the SIFT feature method, to extract multi-scale and multi-dimensional features from the global toothprint, local toothprints, and single-tooth prints. Finally, all features are aggregated to enhance the descriptive ability and robustness. Experimental results indicate that the method proposed in this paper demonstrates good identification performance.}
}
@article{YEBIN2024109888,
title = {ENInst: Enhancing weakly-supervised low-shot instance segmentation},
journal = {Pattern Recognition},
volume = {145},
pages = {109888},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109888},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005861},
author = {Moon Ye-Bin and Dongmin Choi and Yongjin Kwon and Junsik Kim and Tae-Hyun Oh},
keywords = {Low-shot learning, Weakly-supervised learning, Instance segmentation, Sub-task analysis, Enhancement methods},
abstract = {We address a weakly-supervised low-shot instance segmentation, an annotation-efficient training method to deal with novel classes effectively. Since it is an under-explored problem, we first investigate the difficulty of the problem and identify the performance bottleneck by conducting systematic analyses of model components and individual sub-tasks with a simple baseline model. Based on the analyses, we propose ENInst with sub-task enhancement methods: instance-wise mask refinement for enhancing pixel localization quality and novel classifier composition for improving classification accuracy. Our proposed method lifts the overall performance by enhancing the performance of each sub-task. We demonstrate that our ENInst is 7.5 times more efficient in achieving comparable performance to the existing fully-supervised few-shot models and even outperforms them at times.}
}
@article{YU2023109860,
title = {Multi-view clustering via efficient representation learning with anchors},
journal = {Pattern Recognition},
volume = {144},
pages = {109860},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109860},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005587},
author = {Xiao Yu and Hui Liu and Yan Zhang and Shanbao Sun and Caiming Zhang},
keywords = {Multi-view clustering, Large-scale, Anchor, Representation learning},
abstract = {Multi-view spectral clustering has gained considerable attention due to its potential to enhance clustering performance. Although many methods have shown promising results, they often suffer from high time complexity and are not suitable for large-scale datasets. On the other hand, anchor-based methods are well-known for their efficiency. These methods typically learn the similarity relationship between instances and anchors and then convert it into the similarity relationship between instances, involving a considerable number of calculations. To address this issue, we propose a novel method called Multi-view clustering via Efficient Representation LearnIng with aNchors (MERLIN) in this paper. Instead of learning the instance–instance relationship, MERLIN approaches the clustering problem from the perspective of representation learning. Specifically, MERLIN selects the same anchors for different views and utilizes these anchors to learn a consensus representation that integrates information from all views. Additionally, MERLIN adaptively learns weights for different views to fully exploit the complementary information among multiple views. In comparison with seven state-of-the-art baseline methods across five datasets, MERLIN demonstrates both efficiency and effectiveness in handling multi-view datasets and is suitable for handling large-scale datasets.}
}
@article{MA2024109905,
title = {Relative-position embedding based spatially and temporally decoupled Transformer for action recognition},
journal = {Pattern Recognition},
volume = {145},
pages = {109905},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109905},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006039},
author = {Yujun Ma and Ruili Wang},
keywords = {Transformer, Relative-position embedding, Spatial–temporal features, Subsampling},
abstract = {Recognition of human actions is to classify actions in a video. Recently, Vision Transformer (ViT) has been applied to action recognition. However, the Vision Transformer is unsuitable for high-resolution input videos due to the constraint of computing power since ViT splits frames into fixed-size patches embedded (i.e., tokens) with absolute-position information and adopts a pure Transformer encoder to model the relationships among these tokens. To address this issue, we propose a relative-position embedding based spatially and temporally decoupled Transformer (RPE-STDT) for action recognition, which can capture spatial–temporal information by stacked self-attention layers. The proposed RPE-STDT model consists of two separate series of Transformer encoders. The first series of encoders is the spatial Transformer encoders, which model interactions between tokens extracted from the same temporal index. The second series of encoders is the temporal Transformer encoders, which model interactions across time dimensions with a subsampling strategy. Furthermore, we replace the absolute-position embeddings in the Vision Transformer encoders with the proposed relative-position embeddings to capture the order of the embedded tokens to reduce computational costs. Finally, we conduct thorough ablation studies. Our RPE-STDT achieves state-of-the-art results on multiple action recognition datasets, exceeding prior convolution and Transformer-based networks.}
}
@article{XIE2024109957,
title = {An adaptive error-correcting output codes algorithm based on gene expression programming and similarity measurement matrix},
journal = {Pattern Recognition},
volume = {145},
pages = {109957},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109957},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006556},
author = {Shutong Xie and Zongbao He and Lifang Pan and Kunhong Liu and Shubin Su},
keywords = {Multi-class classification, Error-correcting output codes, Gene expression programming, Similarity measurement matrix, Adaptive adjusting},
abstract = {The multi-class classification task is one of the most common tasks in machine learning. As a typical solution based on a partitioning strategy, Error-Correcting Output Codes (ECOC) can transform a multi-class classification problem into multiple binary classification problems. The key of ECOC is to construct an effective codematrix to represent a set of class decomposition schemes, which transforms a multiclass problem into a group of binary class problems. Consequently, the design of a fast and effective ECOC codematrix generation method is of great research significance and value for solving multi-class classification problems. In ECOC algorithms, the design of codematrix is treated as a combination problem between different code columns, in which the evolutionary algorithm shows a great advantage. Based on this consideration, the Gene Expression Programming (GEP) is applied to search for the codematrix with high performance because its expressive tree structure makes it well represent codematrcies for subsequent optimization operations. This paper proposes an adaptive ECOC algorithm based on Gene Expression Programming (GEP) and similarity measurement matrix, named GEP-ECOC. In our GEP, each individual represents a set of columns to form a random ECOC codematrix, which is optimized in the evolutionary process. Meanwhile, the crossover and mutation operations are modified to include a legality checking process to ensure that the generated codematrix satisfies the ECOC constraints. The GEP-based ECOC codematrix generation algorithm can quickly produce a codematrix with better performance, which ensures the efficiency of the algorithm to a certain extent. In addition, an adaptive algorithm based on a similarity measurement matrix is proposed to add new columns to the current codematrix, aiming to better handle hard classes. Our algorithm is compared with other algorithms on various data sets, and the experimental results confirm that our GEP-ECOC can balance the efficiency and performance of the algorithm and achieve higher performance.}
}
@article{ZHAO2023109876,
title = {Patch-guided point matching for point cloud registration with low overlap},
journal = {Pattern Recognition},
volume = {144},
pages = {109876},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109876},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005745},
author = {Tianming Zhao and Linfeng Li and Tian Tian and Jiayi Ma and Jinwen Tian},
keywords = {Point cloud registration, Low overlap, Matching pyramid, Cross-level fusion},
abstract = {Point cloud registration is a classic and fundamental problem. Existing point cloud registration methods obtain correspondence point pairs by calculating the correlation between point features. However, the instability of point features makes the outlier rate of corresponding point pairs high, resulting in poor matching results, especially when facing low overlap point clouds. An obvious fact is that patch matching has higher reliability than point matching. To this end, we propose a patch-guided point cloud registration network. Specifically, we perform fusion on patches and points at both the feature and result levels to achieve the guidance of patch to point matching and improve the accuracy of predicted point pairs. At the feature level, we propose a Matching Pyramid Network (MPN) for multi-level patch/point matching. The core of the MPN is an attention-based cross-layer context aggregation (CCA) module, which is used for the fusion of matching features between upper and lower layers. At the result level, we design a matching consistency judgment module to ensure that the point pairs are consistent in the matching of each layer, which greatly reduces the outlier ratio. Based on the above design, the corresponding point pairs predicted by our network have a high inlier ratio, which makes our method perform well in the face of low overlapping point clouds. Extensive experimental results show that our method outperforms other existing methods for indoor and outdoor datasets.}
}
@article{WIROONSRI2024109910,
title = {Clustering performance analysis using a new correlation-based cluster validity index},
journal = {Pattern Recognition},
volume = {145},
pages = {109910},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109910},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006088},
author = {Nathakhun Wiroonsri},
keywords = {Clustering algorithm, Cluster validity measure, Data partitions, Correlation, Pattern recognition, Marketing},
abstract = {There are various cluster validity indices used for evaluating clustering results. One of the main objectives of using these indices is to seek the optimal unknown number of clusters. Some indices work well for clusters with different densities, sizes, and shapes. Yet, one shared weakness of those validity indices is that they often provide only one optimal number of clusters. That number is unknown in real-world problems, and there might be more than one possible option. We develop a new cluster validity index based on a correlation between an actual distance between a pair of data points and a centroid distance of clusters that the two points occupy. Our proposed index constantly yields several local peaks and overcomes the previously stated weakness. Several experiments in different scenarios, including UCI real-world data sets, have been conducted to compare the proposed validity index with several well-known ones.}
}
@article{ZHANG2023109885,
title = {ARAI-MVSNet: A multi-view stereo depth estimation network with adaptive depth range and depth interval},
journal = {Pattern Recognition},
volume = {144},
pages = {109885},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109885},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005836},
author = {Song Zhang and Wenjia Xu and Zhiwei Wei and Lili Zhang and Yang Wang and Junyi Liu},
keywords = {Multi-view stereo, Depth estimation, Adaptive range, Adaptive interval},
abstract = {Multi-View Stereo (MVS) is a fundamental problem in geometric computer vision which aims to reconstruct a scene using multi-view images with known camera parameters. However, the mainstream approaches represent the scene with a fixed all-pixel depth range and equal depth interval partition, which will result in inadequate utilization of depth planes and imprecise depth estimation. In this paper, we present a novel multi-stage coarse-to-fine framework to achieve adaptive all-pixel depth range and depth interval. We predict a coarse depth map in the first stage, then an Adaptive Depth Range Prediction module is proposed in the second stage to zoom in the scene by leveraging the reference image and the obtained depth map in the first stage and predict a more accurate all-pixel depth range for the following stages. In the third and fourth stages, we propose an Adaptive Depth Interval Adjustment module to achieve adaptive variable interval partition for pixel-wise depth range. The depth interval distribution in this module is normalized by Z-score, which can allocate dense depth hypothesis planes around the potential ground truth depth value and vice versa to achieve more accurate depth estimation. Extensive experiments on four widely used benchmark datasets (DTU, TnT, BlendedMVS, ETH 3D) demonstrate that our model achieves state-of-the-art performance and yields competitive generalization ability. Particularly, our method achieves the highest Acc and Overall on the DTU dataset, while attaining the highest Recall and F1-score on the Tanks and Temples intermediate and advanced dataset. Moreover, our method also achieves the lowest e1 and e3 on the BlendedMVS dataset and the highest Acc and F1-score on the ETH 3D dataset, surpassing all listed methods. Project website: https://github.com/zs670980918/ARAI-MVSNet}
}
@article{WANG2024109954,
title = {Training feedforward neural nets in Hopfield-energy-based configuration: A two-step approach},
journal = {Pattern Recognition},
volume = {145},
pages = {109954},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109954},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006520},
author = {Jing Wang and Jiahong Chen and Kuangen Zhang and Leonid Sigal},
keywords = {Hopfield-based energy, Feedforward neural nets, Learning algorithm, Supervised learning, Unsupervised domain adaptation, Computer vision},
abstract = {We introduce Hopfield-Energy-Based Learning, a general learning framework that is inspired by energy-based models, to train feedforward neural nets. Our approach includes two training phases applied iteratively: first, the minimization of the internal energy, which captures dependencies between input samples and network parameters, is carried out in an unsupervised manner; second, the problem-dependent supervised external energy (e.g., cross-entropy loss) combined with partially reversed internal energy gradients are back-propagated in a standard manner. The intuition is that the first stage helps parameters to settle into the state that simply partitions data into clusters; while in the second stage, the network is allowed to deviate from that clustering a bit (hence gradient reversal) in order to converge to parameters that ultimately perform well on the task at hand. Notably, the data used for the two steps might not be one and the same (e.g., can come from different domains) and the approach naturally tailors itself to solve unsupervised domain adaptation problems without adopting any distribution alignment techniques. We also show that the proposed training strategy substantially improves the performance of several ConvNets on standard supervised classification tasks; showing improvements of at least 1.2% (2.64% on CIFAR-10, 4.5% on CIFAR-100, and 1.35% on ImageNet). Our formulation is general, performs well in practice, and holds promise for scenarios where labeled data is limited.}
}
@article{LIU2024109902,
title = {Jacobian norm with Selective Input Gradient Regularization for interpretable adversarial defense},
journal = {Pattern Recognition},
volume = {145},
pages = {109902},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109902},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006003},
author = {Deyin Liu and Lin Yuanbo Wu and Bo Li and Farid Boussaid and Mohammed Bennamoun and Xianghua Xie and Chengwu Liang},
keywords = {Selective input gradient regularization, Jacobian normalization, Adversarial robustness},
abstract = {Deep neural networks (DNNs) can be easily deceived by imperceptible alterations known as adversarial examples. These examples can lead to misclassification, posing a significant threat to the reliability of deep learning systems in real-world applications. Adversarial training (AT) is a popular technique used to enhance robustness by training models on a combination of corrupted and clean data. However, existing AT-based methods often struggle to handle transferred adversarial examples that can fool multiple defense models, thereby falling short of meeting the generalization requirements for real-world scenarios. Furthermore, AT typically fails to provide interpretable predictions, which are crucial for domain experts seeking to understand the behavior of DNNs. To overcome these challenges, we present a novel approach called Jacobian norm and Selective Input Gradient Regularization (J-SIGR). Our method leverages Jacobian normalization to improve robustness and introduces regularization of perturbation-based saliency maps, enabling interpretable predictions. By adopting J-SIGR, we achieve enhanced defense capabilities and promote high interpretability of DNNs. We evaluate the effectiveness of J-SIGR across various architectures by subjecting it to powerful adversarial attacks. Our experimental evaluations provide compelling evidence of the efficacy of J-SIGR against transferred adversarial attacks, while preserving interpretability. The project code can be found at https://github.com/Lywu-github/jJ-SIGR.git.}
}
@article{CHEN2024109881,
title = {Dynamic contrastive learning guided by class confidence and confusion degree for medical image segmentation},
journal = {Pattern Recognition},
volume = {145},
pages = {109881},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109881},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005794},
author = {Jingkun Chen and Changrui Chen and Wenjian Huang and Jianguo Zhang and Kurt Debattista and Jungong Han},
keywords = {Class confusion degree, Dynamic contrastive learning, Medical image segmentation},
abstract = {This work proposes an intra-Class-confidence and inter-Class-confusion guided Dynamic Contrastive (CCDC) learning framework for medical image segmentation. A core contribution is to dynamically select the most expressive pixels to build positive and negative pairs for contrastive learning at different training phases. For the positive pairs, dynamically adaptive sampling strategies are introduced for sampling different sets of pixels based on their hardness (namely the easiest, easy, and hard pixels). For the negative pairs, to efficiently learn from the classes with high confusion degree w.r.t a query class (i.e., a class containing the query pixels), a new hard class mining strategy is presented. Furthermore, pixel-level representations are extended to the neighbourhood region to leverage the spatial consistency of adjacent pixels. Extensive experiments on the three public datasets demonstrate that the proposed method significantly surpasses the state-of-the-art.}
}
@article{FU2024109926,
title = {Robust implementation of foreground extraction and vessel segmentation for X-ray coronary angiography image sequence},
journal = {Pattern Recognition},
volume = {145},
pages = {109926},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109926},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006246},
author = {Zeyu Fu and Zhuang Fu and Chenzhuo Lu and Jun Yan and Jian Fei and Hui Han},
keywords = {X-ray coronary angiography, Tensor RPCA, TV regularization, Two-stage region growing, Foreground extraction, Vessel segmentation},
abstract = {The extraction of contrast-filled vessels from X-ray coronary angiography (XCA) image sequence has important clinical significance for intuitively diagnosis and therapy. In this study, the XCA image sequence is regarded as a 3D tensor input, the vessel layer is regarded as a sparse tensor, and the background layer is regarded as a low-rank tensor. Using tensor nuclear norm (TNN) minimization, a novel method for vessel layer extraction based on tensor robust principal component analysis (TRPCA) is proposed. Furthermore, considering the irregular movement of vessels and the low-frequency dynamic disturbance of surrounding irrelevant tissues, the total variation (TV) regularized spatial–temporal constraint is introduced to smooth the foreground layer. Subsequently, for vessel layer images with uneven contrast distribution, a two-stage region growing (TSRG) method is utilized for vessel enhancement and segmentation. A global threshold method is used as the preprocessing to obtain main branches, and the Radon-Like features (RLF) filter is used to enhance and connect broken minor segments. The final binary vessel mask is constructed by combining the two intermediate results. The visibility of TV-TRPCA algorithm for foreground extraction is evaluated on clinical XCA image sequences and third-party dataset, which can effectively improve the performance of commonly used vessel segmentation algorithms. Based on TV-TRPCA, the accuracy of TSRG algorithm for vessel segmentation is further evaluated. Both qualitative and quantitative results validate the superiority of the proposed method over existing state-of-the-art approaches.}
}
@article{HUANG2024109897,
title = {Sparse self-attention transformer for image inpainting},
journal = {Pattern Recognition},
volume = {145},
pages = {109897},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109897},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005952},
author = {Wenli Huang and Ye Deng and Siqi Hui and Yang Wu and Sanping Zhou and Jinjun Wang},
keywords = {Image inpainting, Transformer, Sparse attention, Channel attention},
abstract = {Learning-based image inpainting methods have made remarkable progress in recent years. Nevertheless, these methods still suffer from issues such as blurring, artifacts, and inconsistent contents. The use of vanilla convolution kernels, which have limited perceptual fields and spatially invariant kernel coefficients, is one of the main causes for these problems. In contrast, the multi-headed attention in the transformer can effectively model non-local relations among input features by generating adaptive attention scores. Therefore, this paper explores the feasibility of employing the transformer model for the image inpainting task. However, the multi-headed attention transformer blocks pose a significant challenge due to their overwhelming computational cost. To address this issue, we propose a novel U-Net style transformer-based network for the inpainting task, called the sparse self-attention transformer (Spa-former). The Spa-former retains the long-range modeling capacity of transformer blocks while reducing the computational burden. It incorporates a new channel attention approximation algorithm that reduces attention calculation to linear complexity. Additionally, it replaces the canonical softmax function with the ReLU function to generate a sparse attention map that effectively excludes irrelevant features. As a result, the Spa-former achieves effective long-range feature modeling with fewer parameters and lower computational resources. Our empirical results on challenging benchmarks demonstrate the superior performance of our proposed Spa-former over state-of-the-art approaches.}
}
@article{LI2023109845,
title = {Self-imitation guided goal-conditioned reinforcement learning},
journal = {Pattern Recognition},
volume = {144},
pages = {109845},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109845},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005435},
author = {Yao Li and YuHui Wang and XiaoYang Tan},
keywords = {Goal-conditioned reinforcement learning, Self-imitation learning, Deterministic policy gradient, Behavior cloning},
abstract = {Goal-conditioned reinforcement learning (GCRL) aims to control agents to reach desired goals, which poses a significant challenge due to task-specific variations in configurations. However, current GCRL methods suffer from limitations in sample efficiency and the need for substantial training data. While existing self-imitation-based GCRL approaches can improve sample efficiency, their scalability to large-scale tasks is limited. In this paper, we propose integrating self-imitation learning with goal-conditioned RL methods into a compatible and reasonable framework. Specifically, we introduce a novel target action value function to aggregate self-imitation learning and goal-conditioned reinforcement learning. The designed target value effectively combines these two policy training mechanisms to accomplish specific tasks. Moreover, we theoretically demonstrate that our approach can learn a superior policy compared to both self-imitation learning and goal-conditioned reinforcement learning. Additionally, experimental results showcase the stability and effectiveness of our method compared to existing approaches in various challenging robotic control tasks.}
}
@article{YANG2024109966,
title = {Spatial transcriptomics analysis of gene expression prediction using exemplar guided graph neural network},
journal = {Pattern Recognition},
volume = {145},
pages = {109966},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109966},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006647},
author = {Yan Yang and Md Zakir Hossain and Eric Stone and Shafin Rahman},
keywords = {Spatial transcriptomics, Gene expression prediction, Deep learning, Graph convolution, Tissue slide image},
abstract = {Spatial transcriptomics (ST) is essential for understanding diseases and developing novel treatments. It measures the gene expression of each fine-grained area (i.e., different windows) in the tissue slide with low throughput. This paper proposes an exemplar guided graph network dubbed EGGN to accurately and efficiently predict gene expression from each window of a tissue slide image. We apply exemplar learning to dynamically boost gene expression prediction from nearest/similar exemplars of a given tissue slide image window. Our framework has three main components connected in a sequence: (i) an extractor to structure a feature space for exemplar retrievals; (ii) a graph construction strategy to connect windows and exemplars as a graph; (iii) a graph convolutional network backbone to process window and exemplar features, and a graph exemplar bridging block to adaptively revise the window features using its exemplars. Finally, we complete the gene expression prediction task with a simple attention-based prediction block. Experiments on standard benchmark datasets indicate the superiority of our approach when compared with past state-of-the-art methods. We release our code at https://github.com/Yan98/EGN.}
}
@article{ZHOU2023109869,
title = {Attribute subspaces for zero-shot learning},
journal = {Pattern Recognition},
volume = {144},
pages = {109869},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109869},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005678},
author = {Lei Zhou and Yang Liu and Xiao Bai and Na Li and Xiaohan Yu and Jun Zhou and Edwin R. Hancock},
keywords = {Zero-shot learning, Attribute localization, Subspace representation, Attribute subspaces, Self-supervised learning},
abstract = {Zero-shot learning (ZSL) aims to recognize unseen categories without corresponding training samples, which is a practical yet challenging task in computer vision and pattern recognition community. Current state-of-the-art locality-based ZSL methods aim to learn the explicit locality of discriminative attributes, which may suffer from insufficient class-level attribute supervision. In this paper, we introduce an Attribute Subspace learning method for ZSL (AS-ZSL) to learn implicit attribute composition, which is more general than attribute localization with only class-level attribute supervision. AS-ZSL exploits subspace representations that can effectively capture the intrinsic composition of high-dimensional image features and the diversity within attribute appearance. Furthermore, we develop a subspace distance based triplet loss to improve the distinguishability of the attribute subspace representation. Attribute subspace learning module is only needed for the training phase to jointly learn discriminative global features. This leads to a compact inference phase. Furthermore, the proposed AS-ZSL can be naturally extended to adapt to the transductive ZSL setting using a novel self-supervised training strategy. Extensive experimental results on several widely used ZSL datasets, i.e., CUB, AwA2, and SUN, demonstrate the advantage of AS-ZSL compared with the state-of-the-art under different ZSL settings.}
}
@article{PARK2024109942,
title = {Understanding open-set recognition by Jacobian norm and inter-class separation},
journal = {Pattern Recognition},
volume = {145},
pages = {109942},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109942},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006404},
author = {Jaewoo Park and Hojin Park and Eunju Jeong and Andrew Beng Jin Teoh},
keywords = {Open-set recognition, Representation learning, Metric-learning, Object classification},
abstract = {The findings on open-set recognition (OSR) show that models trained on classification datasets are capable of detecting unknown classes not encountered during the training process. Specifically, after trainig, the learned representations of known classes dissociate from the representations of the unknown class, facilitating OSR. In this paper, we investigate this emergent phenomenon by examining the relationship between the Jacobian norm of representations and the inter/intra-class learning dynamics. We provide a theoretical analysis, demonstrating that intra-class learning reduces the Jacobian norm for known class samples, while inter-class learning increases the Jacobian norm for unknown samples, even in the absence of direct exposure to any unknown sample. Overall, the discrepancy in the Jacobian norm between the known and unknown classes enables OSR. Based on this insight, which highlights the pivotal role of inter-class learning, we devise a marginal one-vs-rest (m-OvR) loss function that promotes strong inter-class separation. To further improve OSR performance, we integrate the m-OvR loss with additional strategies that maximize the Jacobian norm disparity. We present comprehensive experimental results that support our theoretical observations and demonstrate the efficacy of our proposed OSR approach.}
}
@article{ZHOU2024109951,
title = {Benchmarking deep models on salient object detection},
journal = {Pattern Recognition},
volume = {145},
pages = {109951},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109951},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006490},
author = {Huajun Zhou and Yang Lin and Lingxiao Yang and Jianhuang Lai and Xiaohua Xie},
keywords = {Benchmark, Salient object detection, Loss function, Objectness},
abstract = {The performance discrepancies caused by different implementation details may obscure the actual progress of the Salient Object Detection (SOD) task. In this paper, we construct a SALient Object Detection (SALOD) benchmark to ensure a fair and comprehensive evaluation by unifying the implementation details of SOD methods. By doing so, we can reveal the reasons behind recent progress by analyzing the impact of network structure and optimization strategy. Based on the experimental results, we first find that U-shaped networks, both older and more recent variants, achieve better performance than other structures. Second, optimization strategy, e.g., training strategy and loss function, significantly impacts SOD accuracy. Finally, we provide a new perspective to validate the generalizability of SOD methods on objectness shifting. Code is available at https://github.com/moothes/SALOD.}
}
@article{YE2024109915,
title = {Self-supervised cross-modal visual retrieval from brain activities},
journal = {Pattern Recognition},
volume = {145},
pages = {109915},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109915},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006131},
author = {Zesheng Ye and Lina Yao and Yu Zhang and Sylvia Gustin},
keywords = {Visual stimuli recovery, Cross-modal retrieval, Self-supervised learning, Brain-Computer Interface},
abstract = {We study the problem of restoring visual stimuli from visually-evoked electroencephalography (EEG) signals. Using a supervised classification-then-generation framework, the reconstruction-based approaches learn the mapping between distributions of two modalities but fail to reproduce the exact visual stimulus. Instead, we propose a self-supervised cross-modal retrieval paradigm that seeks instance-level alignment by maximizing the mutual information between the EEG encoding and associated visual stimulus. We demonstrate the threefold advantages of the self-supervised retrieval over supervised reconstruction on the largest visual-evoked EEG dataset with two evaluation protocols. First, it restores the exact visual stimulus without accessing the image class information, which was not possible with previous approaches. Second, it produces more recognizable results than generated ones and bypasses the challenge of training an image generator. Finally, it illustrates the benefits of self-supervision over supervised models in handling open-set data.}
}
@article{RU2023109862,
title = {Cross-Modal Transformer for RGB-D semantic segmentation of production workshop objects},
journal = {Pattern Recognition},
volume = {144},
pages = {109862},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109862},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005605},
author = {Qingjun Ru and Guangzhu Chen and Tingyu Zuo and Xiaojuan Liao},
keywords = {Cross-Modal, Production workshop object, RGB-D, Semantic segmentation, Transformer},
abstract = {Scene understanding in a production workshop is an important technology to improve its intelligence level, semantic segmentation of production workshop objects is an effective method for realizing scene understanding. Since the varieties of information of production workshop, making full use of the complementary information of RGB image and depth image can effectively improve the semantic segmentation accuracy of production workshop objects. Aiming at solving the multi-scale and real-time problems of segmenting the production workshop objects, this paper proposes Cross-Modal Transformer (CMFormer), a Transformer-based cross-modal semantic segmentation model. Its key feature correction and feature fusion parts are composed of the Multi-Scale Channel Attention Correction(MS-CAC) module and the Global Feature Aggregation(GFA) module. By improving Multi-Head Self-Attention(MHSA) in Transformer, we design Cross-Modal Multi-Head Self-Attention(CM-MHSA) to build long-range interaction between RGB image and depth image, and further design the MS-CAC module and the GFA module on the basis of the CM-MHSA module, to achieve cross-modal information interaction in the channel and spatial dimensions. Among them, the MS-CAC module enriches the multi-scale features of each channel and achieve more accurate channel attention correction between the two modals; the GFA module interacts with RGB feature and depth feature in the spatial dimension and fuses global and local features at the same time. In the experiments on the NYU Depth v2 dataset, the CMFormer reached 68.00% MPA(Mean Pixel Accuracy) and 55.75% mIoU(Mean Intersection over Union), achieves the state-of-the-art results. While in the experiments on the Scene Objects for Production workshop dataset(SOP), the CMFormer achieves 96.74% MPA, 92.98% mIoU and 43 FPS(Frames Per Second), which has high precision and good real-time performance. Code is available at: https://github.com/FutureIAI/CMFormer}
}
@article{HOU2024109886,
title = {Network pruning via resource reallocation},
journal = {Pattern Recognition},
volume = {145},
pages = {109886},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109886},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005848},
author = {Yuenan Hou and Zheng Ma and Chunxiao Liu and Zhe Wang and Chen Change Loy},
keywords = {Network pruning, Resource reallocation, Searching cost},
abstract = {Channel pruning is broadly recognized as an effective approach to obtain a small compact model through eliminating unimportant channels from a large cumbersome network. Contemporary methods typically perform iterative pruning procedure from the original over-parameterized model, which is both tedious and expensive especially when the pruning is aggressive. In this paper, we propose a simple yet effective channel pruning technique, termed network Pruning via rEsource rEalLocation (PEEL), to quickly produce a desired slim model with negligible cost. Specifically, PEEL first constructs a predefined backbone and then conducts resource reallocation on it to shift parameters from less informative layers to more important layers in one round, thus amplifying the positive effect of these informative layers. To demonstrate the effectiveness of PEEL , we perform extensive experiments on ImageNet with ResNet-18, ResNet-50, MobileNetV2, MobileNetV3-small and EfficientNet-B0. Experimental results show that structures uncovered by PEEL exhibit competitive performance with state-of-the-art pruning algorithms under various pruning settings. Encouraging results are also observed when applying PEEL to compress the semantic segmentation model. Our code is available at https://github.com/cardwing/Codes-for-PEEL.}
}
@article{GUAN2023109873,
title = {Sparse kernel k-means for high-dimensional data},
journal = {Pattern Recognition},
volume = {144},
pages = {109873},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109873},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300571X},
author = {Xin Guan and Yoshikazu Terada},
keywords = {Clustering, Feature selection, Kernel method},
abstract = {The kernel k-means method usually loses its power when clustering high-dimensional data, due to a large number of irrelevant features. We propose a novel sparse kernel k-means clustering (SKKM) to extend the advantages of kernel k-means to the high-dimensional cases. We assign each feature a 0-1 indicator and optimize an equivalent kernel k-means loss function while penalizing the sum of the indicators. An alternating minimization algorithm is proposed to estimate both the class labels and the feature indicators. We prove the consistency of both clustering and feature selection of the proposed method. In addition, we apply the proposed framework to the normalized cut. In the numerical experiments, we demonstrate that the proposed method provides better/comparable performance compared to the existing high-dimensional clustering methods.}
}
@article{2024110832,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {155},
pages = {110832},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(24)00583-1},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005831}
}
@article{HU2024109903,
title = {Scalable frame resolution for efficient continuous sign language recognition},
journal = {Pattern Recognition},
volume = {145},
pages = {109903},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109903},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006015},
author = {Lianyu Hu and Liqing Gao and Zekang Liu and Wei Feng},
keywords = {Continuous sign language recognition, Efficient inference, Scalable frame resolution, Adaptive inference},
abstract = {In this paper, we explore the spatial redundancy in continuous sign language recognition (CSLR), aiming to improve its efficiency. Despite recent advances in accuracy in CSLR, state-of-the-art CSLR methods typically require large amounts of computations and memory occupation, which are not friendly towards fast inference under limited computation/memory budgets. Based on a simple observation that not all frames are equally important for CSLR, we propose AdaSize to handle this problem by modeling the frame resolution decision as an end-to-end learnable task to save unnecessary computations. Specifically, a lightweight 2D convolutional neural network (CNN) is first used to quickly browse input frames under a low resolution (e.g., 112 × 112). These extracted coarse and cheap features are sent into a recurrent policy network to dynamically determine the desired resolution for each frame. Once the optimal resolution for each frame is decided, frames with different resolutions are fed into the following backbones to extract representative features. Finally, these features pass through a sequence of temporal modules and a classifier to predict sentences. Extensive experiments on four large-scale datasets, including PHOENIX14, PHOENIX14-T, CSL-Daily and CSL, demonstrate the effectiveness of AdaSize. AdaSize could consistently achieve comparable accuracy with state-of-the-art CSLR methods, with only 0.38× computations, 0.41× memory usage and 1.25× throughput. Comparisons with commonly-used lightweight backbones and other efficient methods verify the superiority of AdaSize under similar computational/memory budgets. We finally plot the frame resolution decisions for AdaSize, hoping to provide insightful analysis of the inherent spatial redundancy in videos.}
}
@article{WANG2024109914,
title = {Visual camera relocalization using both hand-crafted and learned features},
journal = {Pattern Recognition},
volume = {145},
pages = {109914},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109914},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300612X},
author = {Junyi Wang and Yue Qi},
keywords = {Visual relocalization, Weighted point cloud generation, Hand-crafted feature refinement, Dynamic environment},
abstract = {The localization of the camera is essential in AR, MR, and robotics. Diverse pipelines employ a hand-crafted or learning based way to predict the camera pose as per the task. In the localization process, both weaknesses and strengths are maintained. However, few current frameworks consider these two features simultaneously. In this study, a novel relocalization pipeline for RGB or RGB-D input is proposed, including a coarse stage with learned features, further refinement with hand-crafted features, and a stable process to measure the confidence of both stages for improving localization robustness. Instead of directly regressing the camera pose, the coarse procedure uses registration to the known source and predicted weighted target point cloud to obtain the initial result. Therefore, we design a deep network called PGNet to construct the weighted target point cloud with the image and previous poses as inputs. Moreover, in consideration of dynamic surroundings, we add a segmentation branch distinguishing each point as either fixed or dynamic with the purpose of promoting dynamic perception. Correspondingly, the segmentation-extended Chamfer Distance is added to optimize PGNet. During the pose refinement, the feature space is established via hand-crafted feature extraction and matching on the training set. Based on the coarse pose, we obtain the accurate pose by applying Kabsch or Perspective-n-Point (PnP) algorithm to point-to-point correspondences built through searching the space and matching Oriented Fast and Rotated Brief (ORB) features. Furthermore, an additional process is presented by defining coarse and refinement metrics to gain a more stable performance. Finally, experiments on both static and dynamic scenes are conducted. On the one side, the results demonstrate the state-of-the-art performance over other existing methods on 7 Scenes, INDOOR-6, Cambridge Landmarks and TUM RGB-D. On the other side, the positive effects of the pose learning part, dynamic branch, confidence regression and hand-crafted feature based refinement are also provided.}
}
@article{LI2023109874,
title = {BLoG: Bootstrapped graph representation learning with local and global regularization for recommendation},
journal = {Pattern Recognition},
volume = {144},
pages = {109874},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109874},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005721},
author = {Ming Li and Lin Zhang and Lixin Cui and Lu Bai and Zhao Li and Xindong Wu},
keywords = {Graph neural networks (GNN), Graph representation learning, Graph contrastive learning, GNN-based recommender systems},
abstract = {With the explosive growth of online information, the significant application value of recommender systems has received considerable attention. Since user–item interactions can naturally fit into graph structure data, graph neural networks (GNNs), by virtue of their strong ability in graph representation learning, have become the new state-of-the-art approach to recommender systems. Recently, GNN-based contrastive self-supervised learning (SSL) methods have received careful attention due to their superiority over graph-based recommendation under the typical supervised learning paradigm. However, to achieve state-of-the-art performance, GNN-based recommendation with SSL often needs a huge amount of negative examples and the model’s performance is heavily dependent on complex data augmentations. Also, the information interaction among various augmented views is often performed under a single perspective (e.g., structure/feature space or node/graph level). In this paper, we propose a novel bootstrapped graph representation learning with local and global regularization for recommendation, i.e., BLoG, which constructs positive/negative pairs based on the aggregated node features by referring to two alternate views of the original user–item graph structure. In particular, BLoG learns user–item representations by encoding two augmented versions of a user–item bipartite graph using two separate encoders: an online encoder and a target encoder. To facilitate the information interaction between these two distinct graph encoders, we introduce local and global regularization for recommendation, where a graph structural contrastive loss and a node-level semantic loss are defined for local regularization while a graph-level contrastive loss is used for global regularization. An alternative optimization approach is used to train the online encoder and the target encoder. Experimental studies on three benchmark datasets demonstrate that BLoG achieves better recommendation accuracy than several existing baselines.}
}
@article{CHEN2024109943,
title = {Multi-scale self-supervised representation learning with temporal alignment for multi-rate time series modeling},
journal = {Pattern Recognition},
volume = {145},
pages = {109943},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109943},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006416},
author = {Jiawei Chen and Pengyu Song and Chunhui Zhao},
keywords = {Multi-rate time series, Multi-scale temporal dynamics, Label scarcity, Temporal alignment, Self-supervised learning, Segment-wise mask autoencoding},
abstract = {Deep sequential networks have shown great power in time series regression and classification. So far, most approaches naturally assume that the time sequential data are uniformly sampled. In practice, however, different variables usually have different sampling rates, thereby forming multi-rate time series (MR-TS). Particularly, the target variable (i.e., label) to be predicted usually has a lower sampling frequency due to the difficulty of manual annotations. The multi-rate problem poses two challenges. One is the diverse dynamics at different sampling rates, which is defined as multi-scale dynamics. The other is label scarcity. To tackle the above obstacles, this paper developed a Multi-scale Self-supervised Representation Learning technique with a Temporal Alignment mechanism (MSRL-TA) as a coherent framework. Concretely, a probabilistic masked autoencoding approach is pertinently developed, in which segment-wise masking schemes and rate-aware positional encodings are devised to enable the characterization of multi-scale temporal dynamics. In the course of pre-training, the encoder networks are able to generate rich and holistic representations of multi-rate data, thereby alleviating the label scarcity issue for supervised fine-tuning. Furthermore, a Temporal Alignment mechanism is devised to refine synthesized features for dynamic predictive modeling through feature block division and block-wise convolution. With empirical evaluation through extensive experiments, our proposed MSRL-TA achieved consistent state-of-the-art in both multi-rate time series regression and classification tasks on five real-world datasets, including air quality prediction, industrial soft sensing, human activity recognition, and speech voice classification.}
}
@article{SHI2024109898,
title = {PAMI: Partition Input and Aggregate Outputs for Model Interpretation},
journal = {Pattern Recognition},
volume = {145},
pages = {109898},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109898},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005964},
author = {Wei Shi and Wentao Zhang and Wei-shi Zheng and Ruixuan Wang},
keywords = {Interpretation, Visualization, Post-hoc},
abstract = {There is an increasing demand for interpretation of model predictions especially in high-risk applications. Various visualization approaches have been proposed to estimate the part of input which is relevant to a specific model prediction. However, most approaches require model structure and parameter details in order to obtain the visualization results, and in general much effort is required to adapt each approach to multiple types of tasks particularly when model backbone and input format change over tasks. In this study, a simple yet effective visualization framework called PAMI is proposed based on the observation that deep learning models often aggregate features from local regions for model predictions. The basic idea is to mask majority of the input and use the corresponding model output as the relative contribution of the preserved input part to the original model prediction. For each input, since only a set of model outputs are collected and aggregated, PAMI does not require any model detail and can be applied to various prediction tasks with different model backbones and input formats. Extensive experiments on multiple tasks confirm the proposed method performs better than existing visualization approaches in more precisely finding class-specific input regions, and when applied to different model backbones and input formats. The source code is available at https://github.com/fuermowei/PAMI.}
}
@article{JIAO2023109846,
title = {DTEC: Decision tree-based evidential clustering for interpretable partition of uncertain data},
journal = {Pattern Recognition},
volume = {144},
pages = {109846},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109846},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005447},
author = {Lianmeng Jiao and Haoyu Yang and Feng Wang and Zhun-ga Liu and Quan Pan},
keywords = {Evidential clustering, Interpretable clustering, Unsupervised decision tree, Belief function theory},
abstract = {Recently, the evidential clustering has been developed as a promising clustering framework for uncertain data, which generalizes those hard, fuzzy, possibilistic and rough clustering. However, the resulting cluster assignments are less interpretable in terms of human cognition, which limits its applications in those security, privacy or ethic related fields. In this study, the unsupervised decision tree model is introduced into the evidential clustering framework to improve the interpretability of the evidential partition. A Decision Tree-based Evidential Clustering (DTEC) algorithm is developed to build an unsupervised evidential decision tree, which uses the paths from the root node to leaf nodes to achieve the interpretability of each cluster. The proposed algorithm is composed of three procedures, i.e., cutting-point selection, node evidential splitting, and cluster adjustment, in which the first two procedures are carried out iteratively to build a preliminary unsupervised decision tree and the last procedure is designed to adjust the preliminary decision tree if the number of clusters is available. Both synthetic and real datasets are used to evaluate the performance of the proposed algorithm, and the experimental results demonstrate the good performance of the proposal compared with some representative fuzzy, evidential or decision tree-based clustering algorithms.}
}