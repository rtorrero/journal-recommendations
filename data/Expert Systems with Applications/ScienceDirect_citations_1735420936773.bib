@article{ZHANG2022108709,
title = {FocusNet: Classifying better by focusing on confusing classes},
journal = {Pattern Recognition},
volume = {129},
pages = {108709},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108709},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200190X},
author = {Xue Zhang and Zehua Sheng and Hui-Liang Shen},
keywords = {Image classification, Inter-class correlations, Confusing classes},
abstract = {Nowadays, most classification networks use one-hot encoding to represent categorical data because of its simplicity. However, one-hot encoding may affect the generalization ability as it neglects inter-class correlations. We observe that, even when a neural network trained with one-hot labels produces incorrect predictions, it still pays attention to the target image region and reveals which classes confuse the network. Inspired by this observation, we propose a confusion-focusing mechanism to address the class-confusion issue. Our confusion-focusing mechanism is implemented by a two-branch network architecture. Its baseline branch generates confusing classes, and its FocusNet branch, whose architecture is flexible, discriminates correct labels from these confusing classes. We also introduce a novel focus-picking loss function to improve classification accuracy by encouraging FocusNet to focus on the most confusing classes. The experimental results validate that our FocusNet is effective for image classification on common datasets, and that our focus-picking loss function can also benefit the current neural networks in improving their classification accuracy.}
}
@article{SHAN2022108748,
title = {Self-Attention based fine-grained cross-media hybrid network},
journal = {Pattern Recognition},
volume = {130},
pages = {108748},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108748},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002291},
author = {Wei Shan and Dan Huang and Jiangtao Wang and Feng Zou and Suwen Li},
keywords = {Fine-Grained, Cross-Media, Retrieval, Attention},
abstract = {Due to the heterogeneity gap, the data representations of different types of media are inconsistent. It is challenging to measure the fine-grained gap between different media. To this end, we propose a self-attention-based hybrid network to learn the common representations of different media data. Specifically, we first utilize a local self-attention layer to learn the common attention space between different media data. Then we propose a similarity concatenation method to understand the content relationship between features. To further improve the robustness of the model, we also learn a local position encoding to capture the spatial relationships between features. Therefore, our proposed approach can effectively reduce the gap between different feature distributions on cross-media retrieval tasks. Extensive experiments and ablation studies demonstrate that our proposed method achieves state-of-the-art performance. The source code and models are publicly available at: https://github.com/NUST-Machine-Intelligence-Laboratory/SAFGCMHN.}
}
@article{HUANG2022108736,
title = {AVPL: Augmented visual perception learning for person Re-identification and beyond},
journal = {Pattern Recognition},
volume = {129},
pages = {108736},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108736},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002175},
author = {Yewen Huang and Sicheng Lian and Haifeng Hu},
keywords = {Person Re-identification, Augmented visual perception learning, Batch attention, Two-stream hypothesis},
abstract = {In this work, we propose an Augmented Visual Perception Learning (AVPL) method for Person Re-identification (ReID) which is inspired by the two-stream hypothesis theory of Human Visual System (HVS). Deep learning methods dominate ReID and many state-of-the-art performances are achieved from the perspective of optimizing the model of ’what’ visual pathway. It does not blend ’what’ and ’where’ well. Our AVPL method uses the essential mechanism of the ventro-dorsal stream of the ’where’ visual pathway to expand the perception field of the model, and integrates with the ’what’ to complete the information of the visually salient regions. A novel Batch Attention (BA), the key component of our Augmented Visual Perception (AVP) module, is proposed to apply fusion and augmentation into all input feature maps within each batch. Through AVP module, the improved attention-based model attaches more importance to enhancement of salient features, therefore acquiring better perceptual ability of salient regions which provide the most distinguishably distinctions for ReID. Extensive experiments have been carried out on four main stream ReID datasets and two recognition datasets. In terms of ReID, our method achieves rank-1 accuracy of 95.2% on CUHK03-NP, 98.7% on Market-1501, 96.0% on DukeMTMC-reID and 88.8% on MSMT17-V1, outperforming the state-of-the-art methods by a large margin. Besides, it has been experimentally proven to be applicable and effective in other recognition tasks including facial expression recognition and action recognition with an improved accuracy.}
}
@article{SUN2022108788,
title = {Novel hyperbolic clustering-based band hierarchy (HCBH) for effective unsupervised band selection of hyperspectral images},
journal = {Pattern Recognition},
volume = {130},
pages = {108788},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108788},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002692},
author = {He Sun and Lei Zhang and Jinchang Ren and Hua Huang},
keywords = {Hyperspectral image, Unsupervised band selection, Hyperbolic space clustering, Hierarchical clustering},
abstract = {For dimensionality reduction of HSI, many clustering-based unsupervised band selection (UBS) methods have been proposed due to their superiority of reducing the high redundancy between selected bands. However, most of these methods fail to reflect the data structure of HSI, leading to inconsistent results of band selection. To tackle this particular issue, we have proposed a novel hyperbolic clustering-based band hierarchy (HCBH) to fully represent the underlying spectral structure and obtain a more consistent band selection. With the proposed adaptive hyperbolic clustering, the performance can be effectively improved with the aid of geometrical information. By introducing a cluster-centre based ranking metric, the desired band subset can be naturally obtained during the clustering process. Experimental results on three popularly used datasets have validated the superior performance of the proposed approach, which outperforms a few state-of-the-art (SOTA) UBS approaches.}
}
@article{BARISIN2022108747,
title = {Methods for segmenting cracks in 3d images of concrete: A comparison based on semi-synthetic images},
journal = {Pattern Recognition},
volume = {129},
pages = {108747},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108747},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200228X},
author = {Tin Barisin and Christian Jung and Franziska Müsebeck and Claudia Redenbach and Katja Schladitz},
keywords = {Computed tomography, Fractional Brownian surface, 3d segmentation, Crack detection, Machine learning, Deep learning},
abstract = {Concrete is the standard construction material for buildings, bridges, and roads. As safety plays a central role in the design, monitoring, and maintenance of such constructions, it is important to understand the cracking behavior of concrete. Computed tomography captures the microstructure of building materials and allows to study crack initiation and propagation. Manual segmentation of crack surfaces in large 3d images is not feasible. In this paper, automatic crack segmentation methods for 3d images are reviewed and compared. Classical image processing methods (edge detection filters, template matching, minimal path and region growing algorithms) and learning methods (convolutional neural networks, random forests) are considered and tested on semi-synthetic 3d images. Their performance strongly depends on parameter selection which should be adapted to the grayvalue distribution of the images and the geometric properties of the concrete. In general, the learning methods perform best, in particular for thin cracks and low grayvalue contrast.}
}
@article{ULICNY2022108707,
title = {Harmonic convolutional networks based on discrete cosine transform},
journal = {Pattern Recognition},
volume = {129},
pages = {108707},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108707},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001881},
author = {Matej Ulicny and Vladimir A. Krylov and Rozenn Dahyot},
keywords = {Harmonic network, Convolutional neural network, Discrete cosine transform, Image classification, Object detection, Semantic segmentation},
abstract = {Convolutional neural networks (CNNs) learn filters in order to capture local correlation patterns in feature space. We propose to learn these filters as combinations of preset spectral filters defined by the Discrete Cosine Transform (DCT). Our proposed DCT-based harmonic blocks replace conventional convolutional layers to produce partially or fully harmonic versions of new or existing CNN architectures. Using DCT energy compaction properties, we demonstrate how the harmonic networks can be efficiently compressed by truncating high-frequency information in harmonic blocks thanks to the redundancies in the spectral domain. We report extensive experimental validation demonstrating benefits of the introduction of harmonic blocks into state-of-the-art CNN models in image classification, object detection and semantic segmentation applications.}
}
@article{GAMMELLI2022108752,
title = {Recurrent flow networks: A recurrent latent variable model for density estimation of urban mobility},
journal = {Pattern Recognition},
volume = {129},
pages = {108752},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108752},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002333},
author = {Daniele Gammelli and Filipe Rodrigues},
keywords = {Urban mobility, Latent variable models, Normalizing flows, Variational inference},
abstract = {Mobility-on-demand (MoD) systems represent a rapidly developing mode of transportation wherein travel requests are dynamically handled by a coordinated fleet of vehicles. Crucially, the efficiency of an MoD system highly depends on how well supply and demand distributions are aligned in spatio-temporal space (i.e., to satisfy user demand, cars have to be available in the correct place and at the desired time). To do so, we argue that predictive models should aim to explicitly disentangle between temporal and spatial variability in the evolution of urban mobility demand. However, current approaches typically ignore this distinction by either treating both sources of variability jointly, or completely ignoring their presence in the first place. In this paper, we propose recurrent flow networks11Code available at https://www.github.com/DanieleGammelli/recurrent-flow-nets (RFN), where we explore the inclusion of (i) latent random variables in the hidden state of recurrent neural networks to model temporal variability, and (ii) normalizing flows to model the spatial distribution of mobility demand. We demonstrate how predictive models explicitly disentangling between spatial and temporal variability exhibit several desirable properties, and empirically show how this enables the generation of distributions matching potentially complex urban topologies.}
}
@article{NAKAMURA2022108776,
title = {Stochastic batch size for adaptive regularization in deep network optimization},
journal = {Pattern Recognition},
volume = {129},
pages = {108776},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108776},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002576},
author = {Kensuke Nakamura and Stefano Soatto and Byung-Woo Hong},
keywords = {Deep network optimization, Adaptive regularization, Stochastic gradient descent, Adaptive mini-batch size},
abstract = {We propose a first-order stochastic optimization algorithm incorporating adaptive regularization for pattern recognition problems in deep learning framework. The adaptive regularization is imposed by stochastic process in determining batch size for each model parameter at each optimization iteration. The stochastic batch size is determined by the update probability of each parameter following a distribution of gradient norms in consideration of their local and global properties in the neural network architecture where the range of gradient norms may vary within and across layers. We empirically demonstrate the effectiveness of our algorithm using an image classification task based on conventional network models applied to commonly used benchmark datasets. The quantitative evaluation indicates that our algorithm outperforms the state-of-the-art optimization algorithms in generalization while providing less sensitivity to the selection of batch size which often plays a critical role in optimization, thus achieving more robustness to the selection of regularity.}
}
@article{ZHENG2022108724,
title = {High-resolution rectified gradient-based visual explanations for weakly supervised segmentation},
journal = {Pattern Recognition},
volume = {129},
pages = {108724},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108724},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002059},
author = {Tianyou Zheng and Qiang Wang and Yue Shen and Xiang Ma and Xiaotian Lin},
abstract = {Visual explanations for convolutional neural networks (CNNs) act as the backbone for weakly supervised segmentation with image-level labels. This paper proposes a high-resolution rectified gradient-based class activation mapping with bounding box annotations (bbox) to improve the initial seed for weakly supervised segmentation (WSS) tasks. HRCAM extends Grad-CAM by separating the gradient maps from the class activation maps from the shallow layer for higher resolution. Gradient rectified methods are proposed to improve the visualization and WSS score. Experiments and evaluations are conducted to verify the performance of HRCAM-BB on Pascal VOC 2012 and COCO datasets. On Pascal VOC 2012 set, our method achieves outstanding performance with a mean intersection over union (mIOU) of 71.6 with image-level labels and 78.2 with bbox on WSSS, and increases the WSIS mIOU (AP50) to 52.1 with image-level labels, and 61.9 with bbox. our method surpasses the previous SOTA approach in the same condition.}
}
@article{TANG2022108792,
title = {Learning attention-guided pyramidal features for few-shot fine-grained recognition},
journal = {Pattern Recognition},
volume = {130},
pages = {108792},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108792},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002734},
author = {Hao Tang and Chengcheng Yuan and Zechao Li and Jinhui Tang},
keywords = {Few-shot learning, Fine-grained recognition, Weakly-supervised learning},
abstract = {Few-shot fine-grained recognition (FS-FGR) aims to distinguish several highly similar objects from different sub-categories with limited supervision. However, traditional few-shot learning solutions typically exploit image-level features and are committed to capturing global silhouettes while accidentally ignore to exploring local details, resulting in an inevitable problem of inconspicuous but distinguishable information loss. Thus, how to effectively address the fine-grained recognition issue given limited samples still remains a major challenging. In this article, we tend to propose an effective bidirectional pyramid architecture to enhance internal representations of features to cater to fine-grained image recognition task in the few-shot learning scenario. Specifically, we deploy a multi-scale feature pyramid and a multi-level attention pyramid on the backbone network, and progressively aggregated features from different granular spaces via both of them. We then further present an attention-guided refinement strategy in collaboration with a multi-level attention pyramid to reduce the uncertainty brought by backgrounds conditioned by limited samples. In addition, the proposed method is trained with the meta-learning framework in an end-to-end fashion without any extra supervision. Extensive experimental results on four challenging and widely-used fine-grained benchmarks show that the proposed method performs favorably against state-of-the-arts, especially in the one-shot scenarios.}
}
@article{BERENGUELBAETA2022108740,
title = {Atlanta scaled layouts from non-central panoramas},
journal = {Pattern Recognition},
volume = {129},
pages = {108740},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108740},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002217},
author = {Bruno Berenguel-Baeta and Jesus Bermudez-Cameo and Jose J. Guerrero},
keywords = {Omnidirectional vision, 3D vision, Non-central cameras, Layout recovery, Scene understanding},
abstract = {In this work we present a novel approach for 3D layout recovery of indoor environments using a non-central acquisition system. From a single non-central panorama, full and scaled 3D lines can be independently recovered by geometry reasoning without additional nor scale assumptions. However, their sensitivity to noise and complex geometric modeling has led these panoramas and required algorithms being little investigated. Our new pipeline aims to extract the boundaries of the structural lines of an indoor environment with a neural network and exploit the properties of non-central projection systems in a new geometrical processing to recover scaled 3D layouts. The results of our experiments show that we improve state-of-the-art methods for layout recovery and line extraction in non-central projection systems. We completely solve the problem both in Manhattan and Atlanta environments, handling occlusions and retrieving the metric scale of the room without extra measurements. As far as the authors’ knowledge goes, our approach is the first work using deep learning on non-central panoramas and recovering scaled layouts from single panoramas.}
}
@article{LIANG2022108706,
title = {Uncertainty-aware twin support vector machines},
journal = {Pattern Recognition},
volume = {129},
pages = {108706},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108706},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200187X},
author = {Zhizheng Liang and Lei Zhang},
keywords = {Uncertain data, Twin support vector machines, Halfspaces, Kernel functions, Data classification},
abstract = {There exist uncertain data in the real world due to some factors such as imprecise measurements and noise. Unlike deterministic data, the features of samples in uncertain data are often described by interval numbers or random vectors with probability density functions. In this paper we propose novel twin support vector machines (TSVMs) to handle uncertain data. In the proposed models which are referred to as uncertainty-aware TSVMs, each uncertain sample is modeled as a random vector with Gaussian distributions. To deal with the multi-dimensional integrals in the original models, we derive an interesting and important theorem which helps us transform the original models into the model involving one-dimensional integrals. The simplification of models makes the optimization problem tractable and the simplified models are solved by using the quasi-Newton optimization algorithm. The proposed decision rule allows us to classify uncertain samples with means and covariance matrices. In addition, we extend the proposed models to their kernel versions to capture the nonlinear structure of uncertain data. Experiments on a series of data sets have been performed to demonstrate that the proposed models gain better classification performance than some existing algorithms, especially for representing uncertain cross-plane problems.}
}
@article{FU2022108711,
title = {Fovea localization by blood vessel vector in abnormal fundus images},
journal = {Pattern Recognition},
volume = {129},
pages = {108711},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108711},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001923},
author = {Yinghua Fu and Ge Zhang and Jiang Li and Dongyan Pan and Yongxiong Wang and Dawei Zhang},
keywords = {Blood vessel vector (BVV), Fovea localization, Retinal raphe, Probability bubble, Region search},
abstract = {In human eyes, macula is responsible for sharp central vision with fovea in the center. The location of fovea becomes an important landmark in diagnosing the retinal diseases. As macula doesn’t have the clear boundary and obvious shape, deep learning techniques to locate the fovea often fail in complicated lesions and insufficient training samples, and the unsupervised method is incapable for illumination variations. In this paper, a new unsupervised fovea localization method using the retinal raphe and region searching is proposed, and the blood vessel vector (BVV) model is developed. After detecting blood vessels and OD by U-net and probability bubbles, the BVVs are conceived and the retinal raphe is obtained by summating all the BVVs, then the fovea is estimated through the local region searching. Compared with the parabola model, the BVV model does not involve the coordinate transformation and reduces the complexity to the linear time cost O(N). Two other unsupervised techniques the parabola model and intensity searching and five supervised techniques cGAN, U-net, DRNet, MedTnet and EANet are included and compared. The global feature of retinal vessels is utilized which makes the proposed method more robust to the lesions than the other localization methods. The experiments on public datasets Kaggle, MESSIDOR and IDRiD validate the effectiveness of the proposed method by the student’s t-test, and our method obtains the least average Euclidean distance to the groundtruth on Kaggle and almost least on Base 33 of MESSIDOR.}
}
@article{HADI2022108780,
title = {A new distance between multivariate clusters of varying locations, elliptical shapes, and directions},
journal = {Pattern Recognition},
volume = {129},
pages = {108780},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108780},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002618},
author = {Ali S. Hadi},
keywords = {Clustering methods, Complete linkage, Elliptical distance, Euclidean distance, Hamming distance, Hierarchical clustering, Iris data, -Means clustering, Manhattan distance, Single linkage, Robust estimation, Ward method},
abstract = {Clustering methods are based on the computations of both the distances between every pair of the n observations in a multivariate dataset as well as the distances between every pair of clusters in the dataset. The clusters can have different locations and varying elliptical shapes and directions. Numerous methods have been proposed in the literature for computing both of these two types of distances. The contributions of this paper are two folds. First, we propose a new elliptical distance between pairs of clusters in a dataset with different cluster centers and elliptical shapes and directions, Second, we proved analytically that the Ward distance and the Euclidean distance are equivalent. We propose a new classical method for computing the distance between a pair of clusters in the dataset. It is the only distance that does not assume spherical clusters. The proposed classical distances could also be made robust by replacing estimates of location and scale by their respective robust estimators. The proposed distance has a number of advantages including simplicity, interpretability, computational efficiency as well as the ability to accurately capture both the variability of the cluster centers as well as the variability of shapes and directions of their respective covariance matrices. The method is also illustrated by several motivating examples that demonstrate the need of the new proposed distance. The superiority of the proposed method is also demonstrated by application to real-life as well as challenging synthetic data.}
}
@article{TANG2022108787,
title = {Contrastive author-aware text clustering},
journal = {Pattern Recognition},
volume = {130},
pages = {108787},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108787},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002680},
author = {Xudong Tang and Chao Dong and Wei Zhang},
keywords = {Text clustering, Contrastive learning, Representation learning},
abstract = {In the era of User Generated Content (UGC), authors (IDs) of texts widely exist and play a key role in determining the topic categories of texts. Existing text clustering efforts are mainly attributed to utilizing textual information, but the effect of authors on text clustering remains largely underexplored. To mitigate this issue, we propose a novel Contrastive Author-aware Text clustering approach, dubbed as CAT. CAT injects author information not only in characterizing texts through representations but also in pushing or pulling text representations of different authors through contrastive learning, which is rarely adopted by text clustering. Specifically, the developed contrastive learning method conducts both cluster-instance contrast by the text representation augmentation and instance-instance contrast by the multi-view representations. We perform comprehensive experiments on three public datasets, demonstrating that CAT largely outperforms strong competitive text clustering baselines and validating the effectiveness of the CAT’s main components.}
}
@article{GAMMELLI2022108751,
title = {Generalized multi-output Gaussian process censored regression},
journal = {Pattern Recognition},
volume = {129},
pages = {108751},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108751},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002321},
author = {Daniele Gammelli and Kasper Pryds Rolsted and Dario Pacino and Filipe Rodrigues},
keywords = {Censored data, Gaussian processes, Variational inference},
abstract = {When modelling censored observations (i.e. data in which the value of a measurement or observation is un-observable beyond a given threshold), a typical approach in current regression methods is to use a censored-Gaussian (i.e. Tobit) model to describe the conditional output distribution. In this paper, as in the case of missing data, we argue that exploiting correlations between multiple outputs can enable models to better address the bias introduced by censored data. To do so, we introduce a heteroscedastic multi-output Gaussian process model which combines the non-parametric flexibility of GPs with the ability to leverage information from correlated outputs under input-dependent noise conditions. To address the resulting inference intractability, we further devise a variational bound to the marginal log-likelihood suitable for stochastic optimization. We empirically evaluate our model against other generative models for censored data on both synthetic and real world tasks and further show how it can be generalized to deal with arbitrary likelihood functions. Results show how the added flexibility allows our model to better estimate the underlying non-censored (i.e. true) process under potentially complex censoring dynamics.}
}
@article{NAI2022108775,
title = {Dynamic feature fusion with spatial-temporal context for robust object tracking},
journal = {Pattern Recognition},
volume = {130},
pages = {108775},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108775},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002564},
author = {Ke Nai and Zhiyong Li and Haidong Wang},
keywords = {Object tracking, Dynamic feature fusion, Spatial-temporal context, Correlation filters framework},
abstract = {Feature fusion has been widely used for improving the tracking performance. However, how to effectively analyze the characteristics of different visual features to realize dynamical feature fusion is still a challenging task. In this paper, we propose a spatial-temporal context-based dynamic feature fusion method (STCDFF) with the correlation filters framework for object tracking. The proposed STCDFF method exploits spatial-temporal context to deeply analyze the characteristics of multiple visual features (e.g., HOG, Color-Names and CNN features) to perform feature fusion. On the one hand, spatial context is employed to evaluate the discriminative ability of different features to distinguish the target object from the background. On the other hand, temporal context is utilized to consider the representative ability of different features to capture significant appearance changes of the target object. The weight of a feature is decided by both its discriminative ability and representative ability. By exploring spatial-temporal context for feature fusion, the STCDFF method can fully utilize the strengths of different features to handle complex appearance changes and background clutters to achieve better performance. Extensive experiments on multiple object tracking datasets prove that our STCDFF method performs competitively against several popular tracking methods.}
}
@article{ZHOU2022108723,
title = {Three-dimensional affinity learning based multi-branch ensemble network for breast tumor segmentation in MRI},
journal = {Pattern Recognition},
volume = {129},
pages = {108723},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108723},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002047},
author = {Lei Zhou and Shuai Wang and Kun Sun and Tao Zhou and Fuhua Yan and Dinggang Shen},
keywords = {Breast tumor segmentation, Three-dimensional affinity learning based refinement, Multi-branch ensemble network},
abstract = {Accurate and automatic breast tumor segmentation based on dynamic contrast-enhancement magnetic resonance imaging (DCE-MRI) plays an important role in breast cancer analysis. However, the background parenchymal enhancement and large variations in tumor size, shape or appearance make the task very challenging, and also the segmentation performance is still not satisfactory, especially for non-mass enhancement (NME) and small size tumors (≤2 cm). To address these challenges, we propose a novel 3D affinity learning based multi-branch ensemble network for accurate breast tumor segmentation. Specifically, two different types of subnetworks are built to form a multi-branch network. The two subnetworks are equipped with effective operation components, i.e., residual connection and channel-wise attention or making use of dense connectivity patterns, which can process the input images in parallel. Second, we propose an end-to-end trainable 3D affinity learning based refinement module by calculating the similarities between features of voxels, which is useful in discovering more pixels belonging to breast tumors. Third, two local affinity matrices are constructed by 3D affinity learning, which are used to refine the segmentation outputs of two subnetworks, respectively. Finally, a novel ensemble module is proposed to combine the information derived from the subnetworks, which can hierarchically merge the local and global affinity matrices derived from subnetworks. A large-scale breast DCE-MR images dataset with 420 subjects are built for evaluation, and comprehensive experiments have been conducted to demonstrate that our proposed method achieves superior performance over state-of-the-art medical image segmentation methods.}
}
@article{CHEN2022108739,
title = {Few-shot Website Fingerprinting attack with Meta-Bias Learning},
journal = {Pattern Recognition},
volume = {130},
pages = {108739},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108739},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002205},
author = {Mantun Chen and Yongjun Wang and Xiatian Zhu},
keywords = {User privacy, Internet anonymity, Data traffic, Website fingerprinting, Deep learning, Neural network, Few-shot learning, Meta-learning, Parameter factorization},
abstract = {Website fingerprinting (WF) attack aims to identify which website a user is visiting from the traffic data patterns. Whilst existing methods assume many training samples, we investigate a more realistic and scalable few-shot WF attack with only a few labeled training samples per website. To solve this problem, we introduce a novel Meta-Bias Learning (MBL) method for few-shot WF learning. Taking the meta-learning strategy, MBL simulates and optimizes the target tasks. Moreover, a new model parameter factorization idea is introduced for facilitating meta-training with superior task adaptation. Expensive experiments show that our MBL outperforms significantly existing hand-crafted feature and deep learning based alternatives in both closed-world and open-world attack scenarios, at the absence and presence of defense.}
}
@article{CHENG2022108718,
title = {Entropy guided attention network for weakly-supervised action localization},
journal = {Pattern Recognition},
volume = {129},
pages = {108718},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108718},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001996},
author = {Yi Cheng and Ying Sun and Hehe Fan and Tao Zhuo and Joo-Hwee Lim and Mohan Kankanhalli},
keywords = {Temporal action localization, Weakly-supervised learning, Entropy guided loss, Global similarity loss},
abstract = {One major challenge of Weakly-supervised Temporal Action Localization (WTAL) is to handle diverse backgrounds in videos. To model background frames, most existing methods treat them as an additional action class. However, because background frames usually do not share common semantics, squeezing all the different background frames into a single class hinders network optimization. Moreover, the network would be confused and tends to fail when tested on videos with unseen background frames. To address this problem, we propose an Entropy Guided Attention Network (EGA-Net) to treat background frames as out-of-domain samples. Specifically, we design a two-branch module, where a domain branch detects whether a frame is an action by learning a class-agnostic attention map, and an action branch recognizes the action category of the frame by learning a class-specific attention map. By aggregating the two attention maps to model the joint domain-class distribution of frames, our EGA-Net can handle varying backgrounds. To train the class-agnostic attention map with only the video-level class labels, we propose an Entropy Guided Loss (EGL), which employs entropy as the supervision signal to distinguish action and background. Moreover, we propose a Global Similarity Loss (GSL) to enhance the action-specific attention map via action class center. Extensive experiments on THUMOS14, ActivityNet1.2 and ActivityNet1.3 datasets demonstrate the effectiveness of our EGA-Net.}
}
@article{QIN2022108791,
title = {Enforced block diagonal subspace clustering with closed form solution},
journal = {Pattern Recognition},
volume = {130},
pages = {108791},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108791},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002722},
author = {Yalan Qin and Hanzhou Wu and Jian Zhao and Guorui Feng},
keywords = {Subspace clustering, General form, Analytical, Nonnegative, Symmetrical solution},
abstract = {Subspace clustering aims to fit each category of data points by learning an underlying subspace and then conduct clustering according to the learned subspace. Ideally, the learned subspace is expected to be block diagonal such that the similarities between clusters are zeros. In this paper, we provide the explicit theoretical connection between spectral clustering and the subspace clustering based on block diagonal representation. We propose Enforced Block Diagonal Subspace Clustering (EBDSC) and show that the spectral clustering with the Radial Basis Function kernel can be regarded as EBDSC. Compared with the exiting subspace clustering methods, an analytical, nonnegative and symmetrical solution can be obtained by EBDSC. An important difference with respect to the existing ones is that our model is a more general case. EBDSC directly uses the obtained solution as the similarity matrix, which can avoid the complex computation of the optimization program. Then the solution obtained by the proposed method can be used for the final clustering. Finally, we provide the experimental analysis to show the efficiency and effectiveness of our method on the synthetic data and several benchmark data sets in terms of different metrics.}
}
@article{LI2022108763,
title = {The devil in the tail: Cluster consolidation plus cluster adaptive balancing loss for unsupervised person re-identification},
journal = {Pattern Recognition},
volume = {129},
pages = {108763},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108763},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002448},
author = {Mingkun Li and He Sun and Chaoqun Lin and Chun-Guang Li and Jun Guo},
keywords = {Unsupervised person re-identification, Cluster consolidation, Cluster adaptive balancing loss, Long-tail problem},
abstract = {Unsupervised person re-identification (Re-ID) is to retrieve pedestrians from different camera views without supervision information. State-of-the-art methods are usually built upon training a convolution neural network with pseudo labels generated by clustering. Unfortunately, the pseudo labels are highly unbalanced and heavily noisy, carrying ineffective or even erroneous supervision information. To address these deficiencies, we present an effective clustering and reorganization approach, called Cluster Consolidation, which aims to separate a small proportion of unreliable data points from each cluster. This approach benefits to improve the quality of the pseudo labels, but also yields more tiny clusters. Thus, we further propose a Cluster Adaptive Balancing (CAB) loss to effectively train the network with the imbalance pseudo labels, where our CAB loss is able to automatically balance the importance of each cluster. We conduct extensive experiments on widely used person Re-ID benchmark datasets and demonstrate the effectiveness of our proposals.}
}
@article{DONG2022108750,
title = {Negational symmetry of quantum neural networks for binary pattern classification},
journal = {Pattern Recognition},
volume = {129},
pages = {108750},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108750},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200231X},
author = {Nanqing Dong and Michael Kampffmeyer and Irina Voiculescu and Eric Xing},
keywords = {Deep learning, Quantum machine learning, Binary pattern classification, Representation learning, Symmetry},
abstract = {Although quantum neural networks (QNNs) have shown promising results in solving simple machine learning tasks recently, the behavior of QNNs in binary pattern classification is still underexplored. In this work, we find that QNNs have an Achilles’ heel in binary pattern classification. To illustrate this point, we provide a theoretical insight into the properties of QNNs by presenting and analyzing a new form of symmetry embedded in a family of QNNs with full entanglement, which we term negational symmetry. Due to negational symmetry, QNNs can not differentiate between a quantum binary signal and its negational counterpart. We empirically evaluate the negational symmetry of QNNs in binary pattern classification tasks using Google’s quantum computing framework. Both theoretical and experimental results suggest that negational symmetry is a fundamental property of QNNs, which is not shared by classical models. Our findings also imply that negational symmetry is a double-edged sword in practical quantum applications.}
}
@article{YIN2022108710,
title = {MPCCL: Multiview predictive coding with contrastive learning for person re-identification},
journal = {Pattern Recognition},
volume = {129},
pages = {108710},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108710},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001911},
author = {Junhui Yin and Jiyang Xie and Zhanyu Ma and Jun Guo},
keywords = {Person re-identification, Kernel density estimation, Representation construction, Contrastive learning},
abstract = {In this paper, we investigate a new representation learning approach, termed as Multiview Predictive Coding with Contrastive Learning (MPCCL), for person re-identification (re-ID). Different from the conventional re-ID approaches that focus on learning representations from semantic label, our approach learns the identification of invariant information via representation reconstruction, which explores more fine-grained semantic information in representation space. Specifically, given a chosen identity, the learned representation of its single view can be reconstructed by those of other views. Therefore, kernel density estimation (KDE) is firstly introduced for the adaptive reconstruction of the representation. Then, contrastive learning is adopted to increase the distance between the representations of the same views with different identities. Finally, representation reconstruction and contrastive learning jointly supervise the representation learning process, thus obtaining fine-grained semantic information and appearance-free representations. Extensive experiments on several re-ID datasets demonstrate that the proposed approach yields state-of-the-art results.}
}
@article{YAN2022108779,
title = {Robust distance metric optimization driven GEPSVM classifier for pattern classification},
journal = {Pattern Recognition},
volume = {129},
pages = {108779},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108779},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002606},
author = {He Yan and Liyong Fu and Tian'an Zhang and Jun Hu and Qiaolin Ye and Yong Qi and Dong-Jun Yu},
keywords = {Classification problem, Distance metric learning, Outliers and noises, Robust L-GEPSVM method, Squared L-norm distance},
abstract = {Proximal support vector machine via generalized eigenvalues (GEPSVM) is one of the most successful methods for classification problems. However, GEPSVM is vulnerable to outliers since it learns classifiers based on the squared L2-norm distance without a specific strategy to deal with the outliers. Motivated by existing studies that improve the robustness of GEPSVM via the L1-norm distance or not-squared L2-norm distance formulation, a novel GEPSVM formulation that minimizes the p-order of L2-norm distance is proposed, namely, L2,p-GEPSVM. This formulation weakens the negative effects of both light and heavy outliers in the data. An iterative algorithm is designed to solve the general L2,p-norm distance minimization problems and rigorously prove its convergence. In addition, we adjust the parameters of L2,p-GEPSVM to balance the accuracy and training time. This is especially useful for larger datasets. Extensive results indicate that the L2,p-GEPSVM improves the classification performance and robustness in various experimental settings.}
}
@article{GAO2022108789,
title = {Time-varying Group Lasso Granger Causality Graph for High Dimensional Dynamic system},
journal = {Pattern Recognition},
volume = {130},
pages = {108789},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108789},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002709},
author = {Wei Gao and Haizhong Yang},
keywords = {Time-varying Granger causality, Feature selection, Group Lasso, Financial market network},
abstract = {Feature selection is a crucial preprocessing step in data analysis and machine learning. Since causal relationships imply the underlying mechanism of a system, causality-based feature selection methods have gradually attracted great attentions. For a high dimensional system undergoing dynamic transformation, because of the non-stationarity and sample scarcity, modeling the causal structure among these features is difficult. In this paper, we propose a time-varying Granger causal networks to capture the causal relations underlying high dimensional time-varying vector autoregressive models with high order lagged dependence. A kernel reweighted group lasso method is proposed, which overcomes the limitations of sample scarcity and transforms the problem of Granger causal structural learning into a group variable selection problem. The asymptotic consistency of the proposed algorithm is proved. We apply the time-varying Granger causal networks to simulation experiments and real data in the financial market. The study demonstrates that the method provides an efficient tool to detect changes and analysis characters of causal dependency structure in network evolution.}
}
@article{YIN2022108758,
title = {Multilevel wavelet-based hierarchical networks for image compressed sensing},
journal = {Pattern Recognition},
volume = {129},
pages = {108758},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108758},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002394},
author = {Zhu Yin and WuZhen Shi and Zhongcheng Wu and Jun Zhang},
keywords = {Compressed sensing, Hierarchical reconstruction, Sparse signal, Multilevel wavelet transform},
abstract = {Recently, deep learning-based compressed sensing (CS) algorithms have been reported, which remarkably achieve pleasing reconstruction quality with low computational complexity. However, the sampling process of the common deep learning-based CS methods and the conventional ones cannot sufficiently exploit the structured sparsity within image sequences, especially in preserving finer texture details. In this paper, we propose a novel multilevel wavelet-based hierarchical networks for image compressed sensing (dubbed MWHCS-Net). In particular, MWHCS-Net consists of three modules: a sampling module based on a multilevel wavelet transform, a hierarchical initial reconstruction module and a lightweight deep reconstruction module. Motivated by the fact that a sparser signal is easier to reconstruct accurately, we present the sampling module based on multilevel wavelet transform with hierarchical subspace learning for progressive acquisition of measurements to further optimize sampling efficiency and stability. To enhance the finer texture details, the hierarchical initial reconstruction module is designed as a basic initial reconstruction network plus an enhanced initial reconstruction network, which corresponding to the dominant structure component and the texture detail component of the reconstructed image, respectively. At the same time, we also further explore the impact of the hierarchical initial reconstruction module and prove that the texture detail component branch plays an important role in improving the reconstruction quality. Experimental results demonstrate that the proposed MWHCS-Net achieves the state-of-the-art performance while maintaining an efficient running speed. Furthermore, MWHCS-Net outperforms the existing image CS methods based on deep learning in terms of anti-noise performance in most cases.}
}
@article{LIU2022108774,
title = {VFMVAC: View-filtering-based multi-view aggregating convolution for 3D shape recognition and retrieval},
journal = {Pattern Recognition},
volume = {129},
pages = {108774},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108774},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002552},
author = {Zehua Liu and Yuhe Zhang and Jian Gao and Shurui Wang},
keywords = {Multi-view, Channel shuffle, Convolution, Recognition, Retrieval},
abstract = {Multi-view based 3D shape recognition methods have achieved state-of-the-art performance in 3D shape recognition and retrieval. The main focus of multi-view based approaches is determining how to fuse multi-view features into a compact, descriptive, and robust 3D shape descriptor that can then be utilized for 3D shape recognition and retrieval. This paper proposes a novel multi-view aggregating framework, view-filtering-based multi-view aggregating convolution (VFMVAC) to learn global shape descriptors for 3D shape recognition. The proposed VFMVAC applies a voting-based view filtering strategy to select representative views, also introduces a novel multi-view aggregating module to integrate multi-view features; this substantially improves the descriptiveness of the descriptors, and therefore improves the performance of 3D shape recognition and retrieval. Specifically, all views are fed into a voting-based view filtering module to select the top-k representative views. Subsequently, the features of the top-k views are fed into the multi-view aggregating module, which first conducts cross-view channel shuffle for achieving cross-view information flowing, and the resulted reshaped features are then fed into the aggregating convolution module for feature fusion. Experiments on benchmark datasets demonstrate that the proposed VFMVAC is effective and outperforms several recent techniques with respect to the classification and retrieval performance, robustness and efficiency.}
}
@article{LI2022108722,
title = {Paying attention for adjacent areas: Learning discriminative features for large-scale 3D scene segmentation},
journal = {Pattern Recognition},
volume = {129},
pages = {108722},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108722},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002035},
author = {Mengtian Li and Yuan Xie and Lizhuang Ma},
keywords = {Large-scale 3D point clouds, Attention, Long-tailed distribution, Segmentation},
abstract = {Despite recent improvements in analyzing large-scale 3D point clouds, several problems still exist: (a) segmentation models suffer from intra-class inconsistency and inter-class indistinction; (b) the existing methods ignore the inherent long-tailed class distribution of real-world 3D data. These problems result in unsatisfactory semantic segmentation predictions, especially in object adjacent areas. To handle these problems, this paper proposes a novel Adjacent areas Refinement Network (ARNet). Specifically, an Adjacent areas Refinement (AR) module is designed, which consists of two parallel attention blocks. Besides, our proposed attention blocks can process a large number of points (N∼105) with a slight increase in the computational complexity and time cost. Additionally, to deal with the inherent long-tailed class distribution in real-world 3D data, imbalance adjustment loss and occupancy regression loss are introduced. Based on this, the proposed network can handle the classification of both majority and minority classes, which is essential in distinguishing the ambiguous parts in large-scale 3D scenes. The proposed AR module and the loss functions can be easily integrated into the cutting-edge backbone networks, contributing to better performance in modeling semantic inter-dependencies and significantly improving the accuracy of the state-of-the-art semantic segmentation methods on indoor and outdoor scenes.}
}
@article{RIBERO2022108746,
title = {Federating recommendations using differentially private prototypes},
journal = {Pattern Recognition},
volume = {129},
pages = {108746},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108746},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002278},
author = {Mónica Ribero and Jette Henderson and Sinead Williamson and Haris Vikalo},
keywords = {Recommender systems, Differential Privacy, Federated Learning, Cross-Silo Federated Learning, Matrix Factorization},
abstract = {Machine learning methods exploit similarities in users’ activity patterns to provide recommendations in applications across a wide range of fields including entertainment, dating, and commerce. However, in domains that demand protection of personally sensitive data, such as medicine or banking, how can we learn recommendation models without accessing the sensitive data and without inadvertently leaking private information? Many situations in the medical field prohibit centralizing the data from different hospitals and thus require learning from information kept in separate databases. We propose a new federated approach to learning global and local private models for recommendation without collecting raw data, user statistics, or information about personal preferences. Our method produces a set of locally learned prototypes that allow us to infer global behavioral patterns while providing differential privacy guarantees for users in any database of the system. By requiring only two rounds of communication, we both reduce the communication costs and avoid excessive privacy loss associated with typical federated learning iterative procedures. We test our framework on synthetic data, real federated medical data, and a federated version of Movielens ratings. We show that local adaptation of the global model allows the proposed method to outperform centralized matrix-factorization-based recommender system models, both in terms of the accuracy of matrix reconstruction and in terms of the relevance of recommendations, while maintaining provable privacy guarantees. We also show that our method is more robust and has smaller variance than individual models learned by independent entities.}
}
@article{NOVAKOVIC2022108790,
title = {The CP‐ABM approach for modelling COVID‐19 infection dynamics and quantifying the effects of non‐pharmaceutical interventions},
journal = {Pattern Recognition},
volume = {130},
pages = {108790},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108790},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002710},
author = {Aleksandar Novakovic and Adele H. Marshall},
keywords = {COVID-19, Non pharmaceutical interventions, Change point detection, Agent based model, Genetic algorithm},
abstract = {The motivation for this research is to develop an approach that reliably captures the disease dynamics of COVID-19 for an entire population in order to identify the key events driving change in the epidemic through accurate estimation of daily COVID-19 cases. This has been achieved through the new CP-ABM approach which uniquely incorporates Change Point detection into an Agent Based Model taking advantage of genetic algorithms for calibration and an efficient infection centric procedure for computational efficiency. The CP-ABM is applied to the Northern Ireland population where it successfully captures patterns in COVID-19 infection dynamics over both waves of the pandemic and quantifies the significant effects of non-pharmaceutical interventions (NPI) on a national level for lockdowns and mask wearing. To our knowledge, there is no other approach to date that has captured NPI effectiveness and infection spreading dynamics for both waves of the COVID-19 pandemic for an entire country population.}
}
@article{ZHENG2022108717,
title = {HFA-Net: High frequency attention siamese network for building change detection in VHR remote sensing images},
journal = {Pattern Recognition},
volume = {129},
pages = {108717},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108717},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001984},
author = {Hanhong Zheng and Maoguo Gong and Tongfei Liu and Fenlong Jiang and Tao Zhan and Di Lu and Mingyang Zhang},
keywords = {Building change detection, High frequency enhancement, Spatial-wise attention, Convolutional neural network},
abstract = {Building change detection (BCD) recently can be handled well under the booming of deep-learning based computer vision techniques. However, segmentation and recognition for objects with sharper boundaries still suffer from the poorly acquired high frequency information, which can result in the deteriorated annotation of building boundaries in BCD. To better obtain the high frequency pattern under the deep learning pipeline, we propose a high frequency attention-guided Siamese network (HFA-Net) in which a novel built-in high frequency attention block (HFAB) is applied. HFA-Net is designed to enhance high frequency information of buildings via HFAB which is composed of two main stages, i.e., the spatial-wise attention (SA) and the high frequency enhancement (HF). The SA firstly guides the model to search and focus on buildings, and HF is employed afterwards to highlight the high frequency information of the input feature maps. With high frequency information of buildings enhanced by HFAB, HFA-Net is able to better detect the edges of changed buildings, so as to improve the performance of BCD. Our proposed method is evaluated on three widely-used public datasets, i.e., WHU-CD, LEVIR-CD, and Google dataset. Remarkable experimental results on these datasets indicate that our proposed method can better detect edges of changed buildings and shows a better performance. The source code will be released at: https://github.com/HaiXing-1998/HFANet.}
}
@article{QINGYUN2022108786,
title = {Cross-modality attentive feature fusion for object detection in multispectral remote sensing imagery},
journal = {Pattern Recognition},
volume = {130},
pages = {108786},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108786},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002679},
author = {Fang Qingyun and Wang Zhaokui},
keywords = {Cross-modality, Attention, Feature fusion, Object detection, Multispectral remote sensing imagery},
abstract = {Cross-modality fusing complementary information of multispectral remote sensing image pairs can improve the perception ability of detection algorithms, making them more robust and reliable for a wider range of applications, such as nighttime detection. Compared with prior methods, we think different features should be processed specifically, the modality-specific features should be retained and enhanced, while the modality-shared features should be cherry-picked from the RGB and thermal IR modalities. Following this idea, a novel and lightweight multispectral feature fusion approach with joint common-modality and differential-modality attentions are proposed, named Cross-Modality Attentive Feature Fusion (CMAFF). Given the intermediate feature maps of RGB and thermal images, our module parallel infers attention maps from two separate modalities, common- and differential-modality, then the attention maps are multiplied to the input feature map respectively for adaptive feature enhancement or selection. Extensive experiments demonstrate that our proposed approach can achieve the state-of-the-art performance at a low computation cost.}
}
@article{AVOLA2022108762,
title = {3D hand pose and shape estimation from RGB images for keypoint-based hand gesture recognition},
journal = {Pattern Recognition},
volume = {129},
pages = {108762},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108762},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002436},
author = {Danilo Avola and Luigi Cinque and Alessio Fagioli and Gian Luca Foresti and Adriano Fragomeni and Daniele Pannone},
keywords = {Hand pose estimation, Hand shape estimation, Deep learning, Hand gesture recognition},
abstract = {Estimating the 3D pose of a hand from a 2D image is a well-studied problem and a requirement for several real-life applications such as virtual reality, augmented reality, and hand gesture recognition. Currently, reasonable estimations can be computed from single RGB images, especially when a multi-task learning approach is used to force the system to consider the shape of the hand when its pose is determined. However, depending on the method used to represent the hand, the performance can drop considerably in real-life tasks, suggesting that stable descriptions are required to achieve satisfactory results. In this paper, we present a keypoint-based end-to-end framework for 3D hand and pose estimation and successfully apply it to the task of hand gesture recognition as a study case. Specifically, after a pre-processing step in which the images are normalized, the proposed pipeline uses a multi-task semantic feature extractor generating 2D heatmaps and hand silhouettes from RGB images, a viewpoint encoder to predict the hand and camera view parameters, a stable hand estimator to produce the 3D hand pose and shape, and a loss function to guide all of the components jointly during the learning phase. Tests were performed on a 3D pose and shape estimation benchmark dataset to assess the proposed framework, which obtained state-of-the-art performance. Our system was also evaluated on two hand-gesture recognition benchmark datasets and significantly outperformed other keypoint-based approaches, indicating that it is an effective solution that is able to generate stable 3D estimates for hand pose and shape.}
}
@article{WANG2022108814,
title = {YOLO-Anti: YOLO-based counterattack model for unseen congested object detection},
journal = {Pattern Recognition},
volume = {131},
pages = {108814},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108814},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002953},
author = {Kun Wang and Maozhen Liu},
keywords = {Deep learning, Congested and occluded objects, Object detection},
abstract = {Object detection is advancing rapidly with the development of deep learning solutions and big data dimensions. This paper takes the challenging recognition task as the core work and proposes a novel and efficient network framework dedicated to unseen congestion detection. To guarantee the accuracy as well as the speed of inference, the detector utilizes the advanced You Only Look Once v4 (YOLOv4) as the backbone and agglutinates the four proposed strategies, called YOLO-Anti. Our model mainly consists of three modules: First, an adaptive context module similar to valve control is proposed to obtain contextual information that balances foreground and background features. Second, to solve the problem that the imbalance between feature levels weakens the detection performance, a balanced prediction layer method is developed. Finally, we propose an anti-congestion network to selectively expand the local domain to achieve finer-grained detection. Besides, in the training procedure, a designed heterogeneous cross-entropy loss is utilized to strengthen the detector’s discrimination of similar targets in different categories. Extensive experiments were conducted on the PASCAL VOC, COCO, and UA-DETRAC data sets. The state-of-the-art results were achieved on UA-DETRAC and the leading performance on PASCAL VOC and COCO. Also, compared with baseline YOLOv4, the proposed method brings significant accuracy improvement and negligible time consumption.}
}
@article{WAN2022108705,
title = {Multi-level graph learning network for hyperspectral image classification},
journal = {Pattern Recognition},
volume = {129},
pages = {108705},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108705},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001868},
author = {Sheng Wan and Shirui Pan and Shengwei Zhong and Jie Yang and Jian Yang and Yibing Zhan and Chen Gong},
keywords = {Graph convolutional network, Graph-based machine learning, Hyperspectral image classification, Remote sensing, Graph structural learning},
abstract = {Graph Convolutional Network (GCN) has emerged as a new technique for hyperspectral image (HSI) classification. However, in current GCN-based methods, the graphs are usually constructed with manual effort and thus is separate from the classification task, which could limit the representation power of GCN. Moreover, the employed graphs often fail to encode the global contextual information in HSI. Hence, we propose a Multi-level Graph Learning Network (MGLN) for HSI classification, where the graph structural information at both local and global levels can be learned in an end-to-end fashion. First, MGLN employs attention mechanism to adaptively characterize the spatial relevance among image regions. Then localized feature representations can be produced and further used to encode the global contextual information. Finally, prediction can be acquired with the help of both local and global contextual information. Experiments on three real-world hyperspectral datasets reveal the superiority of our MGLN when compared with the state-of-the-art methods.}
}
@article{BAGCHI2022108757,
title = {EEG-ConvTransformer for single-trial EEG-based visual stimulus classification},
journal = {Pattern Recognition},
volume = {129},
pages = {108757},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108757},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002382},
author = {Subhranil Bagchi and Deepti R. Bathula},
keywords = {EEG, Visual stimulus classification, Deep learning, Transformer, Multi-head attention, Inter-region similarity, Temporal convolution, Inter-head diversity, Head representations},
abstract = {Different categories of visual stimuli evoke distinct activation patterns in the human brain. These patterns can be captured with EEG for utilization in application such as Brain-Computer Interface (BCI). However, accurate classification of these patterns acquired using single-trial data is challenging due to the low signal-to-noise ratio of EEG. Recently, deep learning-based transformer models with multi-head self-attention have shown great potential for analyzing variety of data. This work introduces an EEG-ConvTranformer network that is based on both multi-headed self-attention and temporal convolution. The novel architecture incorporates self-attention modules to capture inter-region interaction patterns and convolutional filters to learn temporal patterns in a single module. Experimental results demonstrate that EEG-ConvTransformer achieves improved classification accuracy over state-of-the-art techniques across five different visual stimulus classification tasks. Finally, quantitative analysis of inter-head diversity also shows low similarity in representational space, emphasizing the implicit diversity of multi-head attention.}
}
@article{DONG2022108797,
title = {Identifying the key frames: An attention-aware sampling method for action recognition},
journal = {Pattern Recognition},
volume = {130},
pages = {108797},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108797},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002783},
author = {Wenkai Dong and Zhaoxiang Zhang and Chunfeng Song and Tieniu Tan},
keywords = {Action recognition, Deep learning, Reinforcement learning, Pseudo labels},
abstract = {Deep learning based methods have achieved remarkable progress in action recognition. Existing works mainly focus on designing novel deep architectures to learn video representations for action recognition. Most existing methods treat sampled frames equally and average all the frame-level predictions to generate video-level predictions at the testing stage. However, within a video, discriminative actions may occur sparsely in a few frames whereas most other frames are irrelevant to the ground truth which may even lead to wrong results. As a result, we think that the strategy of selecting relevant frames would be a further important key to enhance the existing deep learning based action recognition. In this paper, we propose an attention-aware sampling method for action recognition, which aims to discard the irrelevant and misleading frames and preserve the most discriminative frames. We formulate the process of mining key frames from videos as a Markov decision process and train the attention agent through deep reinforcement learning without extra labels. The agent takes features and predictions from the baseline model as inputs and generates importance scores for all frames. Moreover, our approach is extensible, which can be applied to different existing deep learning based action recognition models. We achieve very competitive action recognition performance on two widely used action recognition datasets.}
}
@article{CUI2022108773,
title = {Pseudo loss active learning for deep visual tracking},
journal = {Pattern Recognition},
volume = {130},
pages = {108773},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108773},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002540},
author = {Zhiyan Cui and Na Lu and Weifeng Wang},
keywords = {Active learning, Visual tracking, Pseudo loss, Pseudo label},
abstract = {In visual tracking tasks, the training data are commonly composed of a large number of video sequences and each frame in the sequences needs to be labeled manually, which is labor-intensive and time-consuming. In addition, considering the similarity among the consecutive frames in the same sequence, there is significant redundancy in the training data. To address these problems, a novel pseudo loss active learning (PLAL) method is developed in this paper. PLAL aims to select the most informative and least redundant data for training to reduce the cost of labeling and maintain competitive tracking results simultaneously. Firstly, the Gaussian distribution based pseudo label is generated for the unlabeled candidates based on the tracking model which is initially trained on a small amount of training data. Then, the pseudo loss based on cross entropy is designed to compute the difference between the pseudo label and the target response map. The pseudo loss measures the uncertainty of the target spatial context which is used as the informativeness criterion of the image frame for selection. Meanwhile, a sampling interval threshold and a temporal penalty are employed for frame selection to avoid drastic variation in target appearance and reduce the redundancy within the consecutive candidate frames. Only the selected frames are labeled by the oracle (human expert) and then added to the training data. Extensive experiments on public benchmarks (OTB2013, OTB2015, VOT2018, UAV123, GOT-10K, TrackingNet, LaSOT, OxUvA and TLP) demonstrate that PLAL method outperforms the baseline and other recent active learning approaches. With only 3% of labeled data from the training dataset, PLAL reaches competitive performance (98-100%) compared to the model trained on the entire training dataset.}
}
@article{PAGESZAMORA2022108721,
title = {Unsupervised ensemble learning for genome sequencing},
journal = {Pattern Recognition},
volume = {129},
pages = {108721},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108721},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002023},
author = {Alba Pagès-Zamora and Idoia Ochoa and Gonzalo Ruiz Cavero and Pol Villalvilla-Ornat},
keywords = {Expectation maximization algorithm, Variant calling, Genome sequencing, Unsupervised multi-class ensemble classifier, GATK},
abstract = {Unsupervised ensemble learning refers to methods devised for a particular task that combine data provided by decision learners taking into account their reliability, which is usually inferred from the data. Here, the variant calling step of the next generation sequencing technologies is formulated as an unsupervised ensemble classification problem. A variant calling algorithm based on the expectation-maximization algorithm is further proposed that estimates the maximum-a-posteriori decision among a number of classes larger than the number of different labels provided by the learners. Experimental results with real human DNA sequencing data show that the proposed algorithm is competitive compared to state-of-the-art variant callers as GATK, HTSLIB, and Platypus.}
}
@article{QV2022108745,
title = {Clustering by centroid drift and boundary shrinkage},
journal = {Pattern Recognition},
volume = {129},
pages = {108745},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108745},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002266},
author = {Hui Qv and Tao Ma and Xinyi Tong and Xuhui Huang and Zhe Ma and Jiehong Feng},
keywords = {Clustering, Centroid drift, Boundary detection},
abstract = {Locating the centers before assigning clustering labels is a traditional routine of clustering methods, which also limits the development of new clustering ideas. In this paper, we achieve the clustering task by firstly identifying the boundary points in the feature space, and then we shrink the boundary points to allocate the un-clustered points. Concretely, we propose a Centroid Drift (CD) metric and a Boundary Shrinkage (BS) strategy to detect boundary points in the feature space and allocate labels for un-clustered points, respectively. Both the CD and BS are closely related to the pre-computed k-nearest neighbor matrix, contributing to the decrease of algorithm parameters. Moreover, the common problems of noise points and non-uniform density distribution of data points in clustering task can also be alleviated with our proposed large value suppression and normalization of k-nearest neighbor distance techniques. The experiments on synthetic datasets, real-world face image datasets and hyperspectral images demonstrate the superiorities of our proposed clustering framework.}
}
@article{GU2022108716,
title = {Example-based color transfer with Gaussian mixture modeling},
journal = {Pattern Recognition},
volume = {129},
pages = {108716},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108716},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001972},
author = {Chunzhi Gu and Xuequan Lu and Chao Zhang},
keywords = {Color transfer, Gaussian mixture model, EM optimization},
abstract = {Color transfer, which plays a key role in image editing, has attracted noticeable attention recently. It has remained a challenge to date due to various issues such as time-consuming manual adjustments and prior segmentation issues. In this paper, we propose to model color transfer under a probability framework and cast it as a parameter estimation problem. In particular, we relate the transferred image with the example image under the Gaussian Mixture Model (GMM) and regard the transferred image color as the GMM centroids. We employ the Expectation-Maximization (EM) algorithm (E-step and M-step) for optimization. To better preserve gradient information, we introduce a Laplacian based regularization term to the objective function at the M-step which is solved by deriving a gradient descent algorithm. Given the input of a source image and an example image, our method is able to generate multiple color transfer results with increasing EM iterations. Extensive experiments show that our approach generally outperforms other competitive color transfer methods, both visually and quantitatively.}
}
@article{CHANDALIYA2022108761,
title = {ChildGAN: Face aging and rejuvenation to find missing children},
journal = {Pattern Recognition},
volume = {129},
pages = {108761},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108761},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002424},
author = {Praveen Kumar Chandaliya and Neeta Nain},
keywords = {Child face aging and rejuvenation, Child datasets, Face recognition, Age estimation, Gender preservation, Child trafficking},
abstract = {Child-face aging and rejuvenation have amassed considerable active research interest, owing to their immense impact on a broad range of social and security applications, e.g., digital entertainment, fashion and wellness, and searching for long-lost children using childhood photos. All current face aging approaches based on generative adversarial networks (GANs) focus on adult images or long-term aging. We present a new large-scale longitudinal Indian child (ICD) benchmark dataset to facilitate face age progression and regression, cross-age face recognition, age estimation, gender prediction, and kinship face recognition to alleviate these issues. Furthermore, we propose an automatic child-face age progression and regression model, namely, ChildGAN, that generates visually realistic images for enhanced face-identification accuracy while preserving the identity. Consequently, we have trained state-of-the-art (SOTA) face aging models on ICD for comprehensive qualitative and quantitative evaluations. We also present a multi-racial experiments dataset named Multi-Racial Child Dataset (MRCD) containing 64,965 child face images. The images are selected from publicly available datasets and web crawling. Finally, we investigate the generalization of ChildGAN by experimenting with White, Black, Asian, and Indian races. The experimental results suggest that the proposed ChildGAN and SOTA models can aid in reconnecting young children, who were lost at a young age as victims of child trafficking or abduction, with their families. The model and the MRCD web crawled images are available at https://github.com/praveenkumarchandaliya/ChildGAN_Tamp1/.}
}
@article{KADIOGLU2022108688,
title = {Sample complexity of rank regression using pairwise comparisons},
journal = {Pattern Recognition},
volume = {130},
pages = {108688},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108688},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001698},
author = {Berkan Kadıoğlu and Peng Tian and Jennifer Dy and Deniz Erdoğmuş and Stratis Ioannidis},
keywords = {Sample complexity, Rank regression, Pairwise comparisons, Features},
abstract = {We consider a rank regression setting, in which a dataset of N samples with features in Rd is ranked by an oracle via M pairwise comparisons. Specifically, there exists a latent total ordering of the samples; when presented with a pair of samples, a noisy oracle identifies the one ranked higher with respect to the underlying total ordering. A learner observes a dataset of such comparisons and wishes to regress sample ranks from their features. We show that to learn the model parameters with ϵ>0 accuracy, it suffices to conduct M∈Ω(dNlog3N/ϵ2) comparisons uniformly at random when N is Ω(d/ϵ2).}
}
@article{LI2022108785,
title = {HAM: Hybrid attention module in deep convolutional neural networks for image classification},
journal = {Pattern Recognition},
volume = {129},
pages = {108785},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108785},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002667},
author = {Guoqiang Li and Qi Fang and Linlin Zha and Xin Gao and Nenggan Zheng},
keywords = {Hybrid attention module, Channel attention map, Spatial feature descriptor, HAM-integrated networks},
abstract = {Recently, many researches have demonstrated that the attention mechanism has great potential in improving the performance of deep convolutional neural networks (CNNs). However, the existing methods either ignore the importance of using channel attention and spatial attention mechanisms simultaneously or bring much additional model complexity. In order to achieve a balance between performance and model complexity, we propose the Hybrid Attention Module (HAM), a really lightweight yet efficient attention module. Given an intermediate feature map as the input feature, HAM firstly produces one channel attention map and one channel refined feature through the channel submodule, and then based on the channel attention map, the spatial submodule divides the channel refined feature into two groups along the channel axis to generate a pair of spatial attention descriptors. By applying saptial attention descriptors, the spatial submodule generates the final refined feature which can adaptively emphasize the important regions. Besides, HAM is a simple and general module, it can be embedded into various mainstream deep CNN architectures seamlessly and can be trained with base CNNs in the end-to-end way. We evaluate HAM through abundant of experiments on CIFAR-10, CIFAR-100 and STL-10 datasets. The experimental results show that HAM-integrated networks achieve accuracy improvements and further reduce the negative impact of less training data on deeper networks performance than its counterparts, which proves the effectiveness of HAM.}
}
@article{YUAN2022108704,
title = {A novel forget-update module for few-shot domain generalization},
journal = {Pattern Recognition},
volume = {129},
pages = {108704},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108704},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001856},
author = {Minglei Yuan and Chunhao Cai and Tong Lu and Yirui Wu and Qian Xu and Shijie Zhou},
keywords = {Few-shot classification, Domain adaptation, Few-shot domain generalization},
abstract = {Existing Few-Shot Learning (FSL) methods learn and recognize new classes with the help of prior knowledge. However, they cannot handle this task well in a cross-domain scenario when training and testing sets are from different domains, since the fact that prior knowledge in different domains often varies greatly. To solve this problem, in this paper, we propose a few-shot domain generalization method, which is designed to extract relationship embeddings using Forget-Update Modules named FUM. The relationship embedding considers valuable relational information between samples in a specific task, and the forget-update module takes into account differences between domains and adjusts the distribution of relational embeddings through forgetting and updating mechanisms based on specific tasks. To evaluate the few-shot domain generalization ability of FUM, extensive experiments on eight cross-domain scenarios and six same-domain scenarios are conducted, and the results show that FUM achieves superior performances compared to recent few-shot learning methods. Visualization results also show that the distribution of the relationship embeddings extracted by FUM has stronger few-shot domain generalization ability than the feature embeddings used in the existing FSL methods.}
}
@article{KORYCKI2022108749,
title = {Instance exploitation for learning temporary concepts from sparsely labeled drifting data streams},
journal = {Pattern Recognition},
volume = {129},
pages = {108749},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108749},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002308},
author = {Łukasz Korycki and Bartosz Krawczyk},
keywords = {Machine learning, Data stream mining, Concept drift, Sparse labeling, Active learning},
abstract = {Continual learning from streaming data sources becomes more and more popular due to the increasing number of online tools and systems. Dealing with dynamic and everlasting problems poses new challenges for which traditional batch-based offline algorithms turn out to be insufficient in terms of computational time and predictive performance. One of the most crucial limitations is that we cannot assume having an access to a finite and complete data set – we always have to be ready for new data that may complement our model. This poses a critical problem of providing labels for potentially unbounded streams. In real world, we are forced to deal with very strict budget limitations, therefore, we will most likely face the scarcity of annotated instances, which are essential in supervised learning. In our work, we emphasize this problem and propose a novel instance exploitation technique. We show that when: (i) data is characterized by temporary non-stationary concepts, and (ii) there are very few labels spanned across a long time horizon, it is actually better to risk overfitting and adapt models more aggressively by exploiting the only labeled instances we have, instead of sticking to a standard learning mode and suffering from severe underfitting. We present different strategies and configurations for our methods, as well as an ensemble algorithm that attempts to maintain a sweet spot between risky and normal adaptation. Finally, we conduct a complex in-depth comparative analysis of our methods, using state-of-the-art streaming algorithms relevant for the given problem.}
}
@article{WANG2022108811,
title = {Discriminative and regularized echo state network for time series classification},
journal = {Pattern Recognition},
volume = {130},
pages = {108811},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108811},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002928},
author = {Heshan Wang and Yuxi Liu and Dongshu Wang and Yong Luo and Chudong Tong and Zhaomin Lv},
keywords = {Echo state network, Recurrent neural networks, Discriminative feature extraction, Time series classification, Outlier-robust weights},
abstract = {An echo State Network (ESN) is a special structure of a recurrent neural network (RNN) in which the recurrent neurons are randomly connected. ESN models which have achieved a high accuracy on time series prediction tasks can be used as time series prediction models in many domains. Nevertheless, in most ESN models, the input weights are randomly generated and the output weights calculated by the least square method are susceptible to outliers, which cannot guarantee that the ESN models will always be optimal for a given task. In this paper, a novel discriminative and regularized ESN (DR-ESN) combines discriminative feature aggregation (DFA) and outlier-robust weights (ORW) algorithms are proposed for time series classification. DFA is firstly proposed to replace the random input weights of ESN with the constrained weights generated from sample information. In DFA, weight vectors are selected from the vector space spanned by initial input sequence vectors, then the new generated input weights can adequately represent the data features. Secondly, ORW is employed to enhance the robustness of output weights by constraining the weights assigned to samples with large training errors. The weights evaluation and experiments on a massive set of the synthetic time series data, real-world bearing fault data and UCR benchmarks indicate that the proposed DR-ESN can not only considerably improve the original ESN classifier but also effectively suppress the effect of outliers on classification performance.}
}
@article{SUN2022108728,
title = {Query-efficient decision-based attack via sampling distribution reshaping},
journal = {Pattern Recognition},
volume = {129},
pages = {108728},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108728},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002096},
author = {Xuxiang Sun and Gong Cheng and Lei Pei and Junwei Han},
keywords = {Adversarial examples, Decision-based attack, Image classification, Normal vector estimation, Distribution reshaping},
abstract = {With a limited query budget and only the final decision of a target model, how to find adversarial examples with low-magnitude distortion has attracted great attention among researchers. Recent solutions to this issue made use of the estimated normal vector at a boundary data point to search for adversarial examples. However, since the sampling independence between two sampling epochs, they still suffer from a prohibitively high query budget, which will get worse when the dimensionality of the attacked samples get increased. To push for further development, in this paper, we pay attention to a query-efficient method to estimate the normal vector for decision-based attack in high-dimensional space. Specifically, we propose a simple yet effective normal vector estimation framework for high-dimension decision-based attack via Sampling Distribution Reshaping, dubbed SDR. Next, SDR is incorporated into general geometric attack framework. Briefly, SDR leverages all the historically sampled noise to build a guiding vector, which will be used to reshape the next sampling distribution. Besides, we also extend SDR to different ℓp norms for p={2,∞} and deploy low-frequency constraint to enhance the performance of SDR. Compared to peer decision-based attacks, SDR can reach the competitive ℓp norms for p={2,∞}, according to extensive experimental evaluations against both defended and undefended classifiers. Since the simplicity and effectiveness of SDR, we think that reshaping the sampling distribution deserves further research in future works.}
}
@article{LEE2022108720,
title = {Variational cycle-consistent imputation adversarial networks for general missing patterns},
journal = {Pattern Recognition},
volume = {129},
pages = {108720},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108720},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002011},
author = {Woojin Lee and Sungyoon Lee and Junyoung Byun and Hoki Kim and Jaewook Lee},
keywords = {Imputation, Missing data, Cycle-consistent},
abstract = {Imputation of missing data is an important but challenging issue because we do not know the underlying distribution of the missing data. Previous imputation models have addressed this problem by assuming specific kinds of missing distributions. However, in practice, the mechanism of the missing data is unknown, so the most general case of missing pattern needs to be considered for successful imputation. In this paper, we present cycle-consistent imputation adversarial networks to discover the underlying distribution of missing patterns closely under some relaxations. Using adversarial training, our model successfully learns the most general case of missing patterns. Therefore our method can be applied to a wide variety of imputation problems. We empirically evaluated the proposed method with numerical and image data. The result shows that our method yields the state-of-the-art performance quantitatively and qualitatively on standard datasets.}
}
@article{HU2022108744,
title = {Representation learning using deep random vector functional link networks for clustering},
journal = {Pattern Recognition},
volume = {129},
pages = {108744},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108744},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002254},
author = {Minghui Hu and P.N. Suganthan},
keywords = {Random vector functional link, Unsupervised learning, Consensus clustering, Manifold regularization},
abstract = {Random Vector Functional Link (RVFL) Networks have received a lot of attention due to the fast training speed as the non-iterative solution characteristic. Currently, the main research direction of RVFLs has supervised learning, including semi-supervised and multi-label. There are hardly any unsupervised research results for RVFLs. In this paper, we propose the unsupervised RVFL (usRVFL), and the unsupervised framework is generic that can be used with other RVFL variants, thus we extend it to an ensemble deep variant, unsupervised deep RVFL (usdRVFL). The unsupervised method is based on the manifold regularization while the deep variant is related to the consensus clustering method, which can increase the capability and diversity of RVFLs. Our unsupervised approaches also benefit from fast training speed, even the deep variant offers a very competitive computation efficiency. Empirical experiments on several benchmark datasets demonstrated the effectiveness of the proposed algorithms.}
}
@article{QIAN2022108796,
title = {3D Object Detection for Autonomous Driving: A Survey},
journal = {Pattern Recognition},
volume = {130},
pages = {108796},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108796},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002771},
author = {Rui Qian and Xin Lai and Xirong Li},
keywords = {3D object detection, Autonomous driving, Point clouds},
abstract = {Autonomous driving is regarded as one of the most promising remedies to shield human beings from severe crashes. To this end, 3D object detection serves as the core basis of perception stack especially for the sake of path planning, motion prediction, and collision avoidance etc.. Taking a quick glance at the progress we have made, we attribute challenges to visual appearance recovery in the absence of depth information from images, representation learning from partially occluded unstructured point clouds, and semantic alignments over heterogeneous features from cross modalities. Despite existing efforts, 3D object detection for autonomous driving is still in its infancy. Recently, a large body of literature have been investigated to address this 3D vision task. Nevertheless, few investigations have looked into collecting and structuring this growing knowledge. We therefore aim to fill this gap in a comprehensive survey, encompassing all the main concerns including sensors, datasets, performance metrics and the recent state-of-the-art detection methods, together with their pros and cons. Furthermore, we provide quantitative comparisons with the state of the art. A case study on fifteen selected representative methods is presented, involved with runtime analysis, error analysis, and robustness analysis. Finally, we provide concluding remarks after an in-depth analysis of the surveyed works and identify promising directions for future work.}
}
@article{CAO2022108768,
title = {Unsupervised discriminative feature learning via finding a clustering-friendly embedding space},
journal = {Pattern Recognition},
volume = {129},
pages = {108768},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108768},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002497},
author = {Wenming Cao and Zhongfan Zhang and Cheng Liu and Rui Li and Qianfen Jiao and Zhiwen Yu and Hau-San Wong},
keywords = {Deep clustering, Unsupervised learning, Generative adversarial networks, Siamese network},
abstract = {In this paper, we propose an enhanced deep clustering network (EDCN), which is composed of a Feature Extractor, a Conditional Generator, a Discriminator and a Siamese Network. Specifically, we will utilize two kinds of generated data based on adversarial training, as well as the original data, to train the Feature Extractor for learning effective latent representations. In addition, we adopt the Siamese network to find an embedding space, where a better affinity similarity matrix is obtained as the key to success of spectral clustering in providing reliable pseudo-labels. Particularly, the obtained pseudo-labels will be used to generate realistic data by the Generator. Finally, the discriminator is used to model the real joint distribution of data and corresponding latent representations for Feature Extractor enhancement. To evaluate our proposed EDCN, we conduct extensive experiments on multiple data sets including MNIST, USPS, FRGC, CIFAR-10, STL-10, and Fashion-MNIST by comparing our method with a number of state-of-the-art deep clustering methods, and experimental results demonstrate its effectiveness and superiority.}
}
@article{GUO2022108715,
title = {Direction of arrival estimation for indoor environments based on acoustic composition model with a single microphone},
journal = {Pattern Recognition},
volume = {129},
pages = {108715},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108715},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001960},
author = {Xingchen Guo and Xuexin Xu and Xunquan Chen and Jinhui Chen and Rong Jia and Zhihong Zhang and Tetsuya Takiguchi and Edwin R. Hancock},
keywords = {Gaussian mixture model (GMM), Acoustic transfer function (ATF), Talker localization},
abstract = {This paper presents an effective method for multiple talker localisation using only a single microphone in a room. One of the main challenge here is obtaining a model that can be used for estimating the localization parameter. This model must be sensitive to all possible speaker locations and correctly discriminate their positions. The reverberant speech signal in a room environment can be composited by the clean speech and the acoustic transfer function (ATF). The ATF is a useful tool to describe changes of the speech source, and the approaches based on ATF can thus be used to identify talker localizations with a single microphone. This paper presents two methods, referred to as Composite Reverberant Speech (CRS) model and Direct Training Reverberant Speech (DTRS) model, and uses these methods for obtaining the ATF of a room. The approaches based on proposed methods can successfully and accurately process multi-talker localization task with single microphone. Experiments also demonstrate the effectiveness of the proposed methods.}
}
@article{CHEN2022108760,
title = {Discrete curve model for non-elastic shape analysis on shape manifold},
journal = {Pattern Recognition},
volume = {130},
pages = {108760},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108760},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002412},
author = {Peng Chen and Xutao Li and Changxing Ding and Jianxing Liu and Ligang Wu},
keywords = {Discrete curve model, Non-elastic shape analysis, Shape manifold, Shape synthesis, Shape retrieval, Shape arithmetics},
abstract = {In this paper, we construct a novel finite dimensional shape manifold for shape analyses. Elements of the shape manifold are a set of discrete, planar, and closed curves, which stand for object boundaries and are represented by direction function. On this manifold, we use a set of N-dimensional Fourier basis to construct the tangent space of the shape manifold as a finite dimensional space. Furthermore, we construct the shape manifold as a Riemannian manifold, in which the Riemannian metric is interpreted as an l2 metric. Our method improves the performance of bending-only models in the issues of shape analysis including the shape synthesis, comparison, and statistic analysis. We evaluate the performance of the manifold via the following applications: 1) shape interpolation and extrapolation between curves, 2) shape retrieval on the Flavia leaf database, 3) shape synthesis using an estimated probability distribution on the manifold, and 4) a novel application named shape arithmetic. All the above experiments clearly demonstrate our approach achieves superior performance to state-of-the-art methods.}
}
@article{ZHANG2022108784,
title = {Self-supervised rigid transformation equivariance for accurate 3D point cloud registration},
journal = {Pattern Recognition},
volume = {130},
pages = {108784},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108784},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002655},
author = {Zhiyuan Zhang and Jiadai Sun and Yuchao Dai and Dingfu Zhou and Xibin Song and Mingyi He},
keywords = {Point cloud, Rigid transformation equivariance, Learned cost volume},
abstract = {Transformation equivariance has been widely investigated in 3D point cloud representation learning for more informative descriptors, which formulates the change of the representation with respect to the transformation of the input point clouds explicitly. In this paper, we extend this property to the task of 3D point cloud registration and propose a rigid transformation equivariance (RTE) for accurate 3D point cloud registration. Specifically, RTE formulates the change of the relative pose explicitly with respect to the rigid transformation of the input point clouds. To exploit RTE, we adopt a Siamese structure network with two shared registration branches. One focuses on the input pair of point clouds, and the other one focuses on the new pair achieved by applying two random rigid transformations to the input point clouds respectively. Since the change of the two output relative poses has been predicted according to RTE, a new additional self-supervised loss is obtained to supervise the training. This general network structure can be integrated with most learning-based point cloud registration frameworks easily to improve the performance. Our method adopts the state-of-the-art virtual point-based pipelines as our shared branches, in which we propose a data-driven matching based on learned cost volume (LCV) rather than traditional hand-crafted matching strategies. Experimental evaluations on both synthetic datasets and real datasets validate the effectiveness of our proposed framework. The source code will be made public.}
}
@article{STRAZZERI2022108687,
title = {Possibility results for graph clustering: A novel consistency axiom},
journal = {Pattern Recognition},
volume = {128},
pages = {108687},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108687},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001686},
author = {Fabio Strazzeri and Rubén J. Sánchez-García},
keywords = {Data clustering, Graph clustering, Axiomatic clustering, Morse theory, Morse flow},
abstract = {Kleinberg introduced three natural clustering properties, or axioms, and showed they cannot be simultaneously satisfied by any clustering algorithm. We present a new clustering property, Monotonic Consistency, which avoids the well-known problematic behaviour of Kleinberg’s Consistency axiom, and the impossibility result. Namely, we describe a clustering algorithm, Morse Clustering, inspired by Morse Theory in Differential Topology, which satisfies Kleinberg’s original axioms with Consistency replaced by Monotonic Consistency. Morse clustering uncovers the underlying flow structure on a set or graph and returns a partition into trees representing basins of attraction of critical vertices. We also generalise Kleinberg’s axiomatic approach to sparse graphs, showing an impossibility result for Consistency, and a possibility result for Monotonic Consistency and Morse clustering.}
}
@article{LIU2022108808,
title = {FastOPM—A practical method for partial match of time series},
journal = {Pattern Recognition},
volume = {130},
pages = {108808},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108808},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002898},
author = {Jixue Liu and Jiuyong Li and Lin Liu},
keywords = {Time series, Query processing, Global optimization, Partial match},
abstract = {In applications like stock markets, engineering, medicine, etc., a large amount of time series data has been collected. Interrogating the data for patterns is important for analysis like event prediction and event investigation. A fundamental operation to support such analysis is query processing. In this paper, we aim to efficiently find the optimal match of a query in a timeseries when the match is calculated based on the trend and allows points to be skipped from the middle and ends of the sequences. This problem requires global optimization. The solutions in the literature have prohibitively high time complexities and are not practical for long timeseries. Our method consists of three parts. The first part is an efficiency improvement algorithm called FastOPM which applies the Dijkstra algorithm to get the optimal solution in an efficient manner. The second part derives bounds for optimal solutions. The third part is an algorithm for efficiently searching the target timeseries for the best optimal match of a query. Our experiments show that our method is faster than the baseline method, the bounds are effective, and the search algorithm can identify the best optimal match efficiently. Overall, our algorithm effectively outperforms the state-of-the-art algorithms DTW and MASS in retrieving target segments.}
}
@article{SULTANA2022108719,
title = {Unsupervised moving object segmentation using background subtraction and optimal adversarial noise sample search},
journal = {Pattern Recognition},
volume = {129},
pages = {108719},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108719},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200200X},
author = {Maryam Sultana and Arif Mahmood and Soon Ki Jung},
keywords = {Moving objects segmentation, Generative adversarial network, Background subtraction},
abstract = {Moving Objects Segmentation (MOS) is a fundamental task in many computer vision applications such as human activity analysis, visual object tracking, content based video search, traffic monitoring, surveillance, and security. MOS becomes challenging due to abrupt illumination variations, dynamic backgrounds, camouflage and scenes with bootstrapping. To address these challenges we propose a MOS algorithm exploiting multiple adversarial regularizations including conventional as well as least squares losses. More specifically, our model is trained on scene background images with the help of cross-entropy loss, least squares adversarial loss and ℓ1 loss in image space working jointly to learn the dynamic background changes. During testing, our proposed method aims to generate test image background scenes by searching optimal noise samples using joint minimization of ℓ1 loss in image space, ℓ1 loss in feature space, and discriminator least squares loss. These loss functions force the generator to synthesize dynamic backgrounds similar to the test sequences which upon subtraction results in moving objects segmentation. Experimental evaluations on five benchmark datasets have shown excellent performance of the proposed algorithm compared to the twenty one existing state-of-the-art methods.}
}
@article{CHO2022108703,
title = {Unsupervised video anomaly detection via normalizing flows with implicit latent features},
journal = {Pattern Recognition},
volume = {129},
pages = {108703},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108703},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001844},
author = {MyeongAh Cho and Taeoh Kim and Woo Jin Kim and Suhwan Cho and Sangyoun Lee},
keywords = {Video anomaly detection, Surveillance system, AutoEncoder, Normalizing flow},
abstract = {In contemporary society, surveillance anomaly detection, i.e., spotting anomalous events such as crimes or accidents in surveillance videos, is a critical task. As anomalies occur rarely, most training data consists of unlabeled videos without anomalous events, which makes the task challenging. Most existing methods use an autoencoder (AE) to learn to reconstruct normal videos; they then detect anomalies based on their failure to reconstruct the appearance of abnormal scenes. However, because anomalies are distinguished by appearance as well as motion, many previous approaches have explicitly separated appearance and motion informationfor example, using a pre-trained optical flow model. This explicit separation restricts reciprocal representation capabilities between two types of information. In contrast, we propose an implicit two-path AE (ITAE), a structure in which two encoders implicitly model appearance and motion features, along with a single decoder that combines them to learn normal video patterns. For the complex distribution of normal scenes, we suggest normal density estimation of ITAE features through normalizing flow (NF)-based generative models to learn the tractable likelihoods and identify anomalies using out-of-distribution detection. NF models intensify ITAE performance by learning normality through implicitly learned features. Finally, we demonstrate the effectiveness of ITAE and its feature distribution modeling on six benchmarks, including databases that contain various anomalies in real-world scenarios.}
}
@article{TABAK2022108795,
title = {Distributional barycenter problem through data-driven flows},
journal = {Pattern Recognition},
volume = {130},
pages = {108795},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108795},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200276X},
author = {Esteban G. Tabak and Giulio Trigila and Wenjun Zhao},
keywords = {Optimal transport, Barycenter problem, Pattern visualization, Simulation, Generative models},
abstract = {A new method is proposed for the solution of the data-driven optimal transport barycenter problem and of the more general distributional barycenter problem that the article introduces. The distributional barycenter problem provides a conceptual and computational toolbox for central problems in pattern recognition, such as the simulation of conditional distributions, the construction of a representative for a family of distributions indexed by a covariate and a new class of data-based generative models. The method proposed improves on previous approaches based on adversarial games, by slaving the discriminator to the generator and minimizing the need for parameterizations. It applies not only to a discrete family of distributions, but to more general distributions conditioned to factors z of any cardinality and type. The methodology is applied to numerical examples, including an analysis of the MNIST data set with a new cost function that penalizes non-isometric maps.}
}
@article{YU2022108772,
title = {Auto-weighted sample-level fusion with anchors for incomplete multi-view clustering},
journal = {Pattern Recognition},
volume = {130},
pages = {108772},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108772},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002539},
author = {Xiao Yu and Hui Liu and Yuxiu Lin and Yan Wu and Caiming Zhang},
keywords = {Incomplete data, Multi-view clustering, Anchor, Auto-weighted, Large-scale},
abstract = {Aiming at solving the problem of clustering in the multi-view datasets which include samples with information missing in one or more views, incomplete multi-view clustering has received considerable attention. However, most studies can not get satisfying accuracy and efficiency when dealing with datasets in which a considerable number of instances are missing in partial views. To address this problem, a method named Auto-weighted Sample-level Fusion with Anchors for Incomplete Multi-view Clustering (ASA-IC) is proposed in this paper. It designs an auto-weighted sample-level fusion strategy, which realizes the optimized conversion from the individual instance-to-anchor similarity learning to the concensus instance-to-anchor similarity matrix construction. ASA-IC can not only handle incomplete samples and effectively explore the relationship between each instance and anchors, but also deal with various incomplete clustering situations and be applied in large-scale datasets as well. Besides, experiments on 5 complete datasets and 27 incomplete ones illustrate its effectiveness quantitatively and qualitatively.}
}
@article{YU2022108691,
title = {SPARE: Self-supervised part erasing for ultra-fine-grained visual categorization},
journal = {Pattern Recognition},
volume = {128},
pages = {108691},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108691},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001728},
author = {Xiaohan Yu and Yang Zhao and Yongsheng Gao},
keywords = {Self-Supervised part erasing, Ultra-fine-grained visual categorization, Fine-grained visual categorization, Random part erasing, Weakly-supervised part segmentation},
abstract = {This paper presents SPARE, a self-supervised part erasing framework for ultra-fine-grained visual categorization. The key insight of our model is to learn discriminative representations by encoding a self-supervised module that performs random part erasing and prediction on the contextual position of the erased parts. This drives the network to exploit intrinsic structure of data, i.e., understanding and recognizing the contextual information of the objects, thus facilitating more discriminative part-level representation. This also enhances the learning capability of the model by introducing more diversified training part segments with semantic meaning. We demonstrate that our approach is able to achieve strong performance on seven publicly available datasets covering ultra-fine-grained visual categorization and fine-grained visual categorization tasks.}
}
@article{HUO2022108727,
title = {Attention regularized semi-supervised learning with class-ambiguous data for image classification},
journal = {Pattern Recognition},
volume = {129},
pages = {108727},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108727},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002084},
author = {Xiaoyang Huo and Xiangping Zeng and Si Wu and Hau-San Wong},
keywords = {Semi-supervised learning, Image classification, Attention regularization, Class-ambiguous data},
abstract = {Data augmentation via randomly combining training instances and interpolating the corresponding labels has shown impressive gains in image classification. However, model attention regions are not necessarily meaningful in class semantics, especially for the case of limited supervision. In this paper, we present a semi-supervised classification model based on Class-Ambiguous Data with Attention Regularization, which is referred to as CADAR. Specifically, we adopt a Random Regional Interpolation (RRI) module to construct complex and effective class-ambiguous data, such that the model behavior can be regularized around decision boundaries. By aggregating the parameters of a classification network over training epochs to produce more reliable predictions on unlabeled data, RRI can also be applied to them as well as labeled data. Further, the classifier is enforced to apply consistent attention on the original and constructed data. This is important for inducing the model to learn discriminative features from the class-related regions. The experiment results demonstrate that CADAR significantly benefits from the constructed data and attention regularization, and thus achieves superior performance across multiple standard benchmarks and different amounts of labeled data.}
}
@article{COOPER2022108743,
title = {Believe the HiPe: Hierarchical perturbation for fast, robust, and model-agnostic saliency mapping},
journal = {Pattern Recognition},
volume = {129},
pages = {108743},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108743},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002242},
author = {Jessica Cooper and Ognjen Arandjelović and David J Harrison},
keywords = {XAI, AI safety, Saliency mapping, Deep learning explanation, Interpretability, Prediction attribution},
abstract = {Understanding the predictions made by Artificial Intelligence (AI) systems is becoming more and more important as deep learning models are used for increasingly complex and high-stakes tasks. Saliency mapping – a popular visual attribution method – is one important tool for this, but existing formulations are limited by either computational cost or architectural constraints. We therefore propose Hierarchical Perturbation, a very fast and completely model-agnostic method for interpreting model predictions with robust saliency maps. Using standard benchmarks and datasets, we show that our saliency maps are of competitive or superior quality to those generated by existing model-agnostic methods – and are over 20× faster to compute.}
}
@article{LIU2022108767,
title = {SELF-LLP: Self-supervised learning from label proportions with self-ensemble},
journal = {Pattern Recognition},
volume = {129},
pages = {108767},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108767},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002485},
author = {Jiabin Liu and Zhiquan Qi and Bo Wang and YingJie Tian and Yong Shi},
keywords = {Learning from label proportion, Self-supervised learning, Self-ensemble strategy, Multi-task learning},
abstract = {In this paper, we tackle the problem called learning from label proportions (LLP), where the training data is arranged into various bags, with only the proportions of different categories in each bag available. Existing efforts mainly focus on training a model with only the limited proportion information in a weakly supervised manner, thus result in apparent performance gap to supervised learning, as well as computational inefficiency. In this work, we propose a multi-task pipeline called SELF-LLP to make full use of the information contained in the data and model themselves. Specifically, to intensively learn representation from the data, we leverage the self-supervised learning as a plug-in auxiliary task to learn better transferable visual representation. The main insight is to benefit from the self-supervised representation learning with deep model, as well as improving classification performance by a large margin. Meanwhile, in order to better leverage the implicit benefits from the model itself, we incorporate the self-ensemble strategy to guide the training process with an auxiliary supervision information, which is constructed by aggregating multiple previous network predictions. Furthermore, a ramp-up mechanism is further employed to stabilize the training process. In the extensive experiments, our method demonstrates compelling advantages in both accuracy and efficiency over several state-of-the-art LLP approaches.}
}
@article{CHANG2022108778,
title = {Self-weighted learning framework for adaptive locality discriminant analysis},
journal = {Pattern Recognition},
volume = {129},
pages = {108778},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108778},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200259X},
author = {Wei Chang and Feiping Nie and Zheng Wang and Rong Wang and Xuelong Li},
keywords = {Supervised dimensionality reduction, Linear discriminant analysis, Re-weighted method},
abstract = {Linear discriminant analysis (LDA) is one of the most important dimensionality reduction techniques and applied in many areas. However, traditional LDA algorithms aim to capture the global structure from data and ignore the local information. That may lead to the failure of LDA in some real-world datasets which have a complex geometry distribution. Although there are many previous works that focus on preserving the local information, they are all stuck in the same problem that the neighbor relationships of pairwise data points obtained from the original space may not be reliable, especially in the case of heavy noise. Therefore, we proposed a novel self-weighted learning framework, named Self-Weighted Adaptive Locality Discriminant Analysis (SALDA), for locality-aware based dimensionality reduction. The proposed framework can adaptively learn an intrinsic low-dimensional subspace, so that we can explore the better neighbor relationships for samples under the ideal subspace. In addition, our model can automatically learn to assign the weights to data pairwise points within the same class and takes no extra parameters compared to other classical locality-aware methods. At last, the experimental results on both synthetic and real-world benchmark datasets demonstrate the effectiveness and superiority of the proposed algorithm.}
}
@article{HUUSARI2022108759,
title = {Cross-View kernel transfer},
journal = {Pattern Recognition},
volume = {129},
pages = {108759},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108759},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002400},
author = {Riikka Huusari and Cécile Capponi and Paul Villoutreix and Hachem Kadri},
keywords = {Multi-view learning, Cross-view transfer, Kernel completion, Kernel learning},
abstract = {We consider the kernel completion problem with the presence of multiple views in the data. In this context the data samples can be fully missing in some views, creating missing columns and rows to the kernel matrices that are calculated individually for each view. We propose to solve the problem of completing the kernel matrices with Cross-View Kernel Transfer (CVKT) procedure, in which the features of the other views are transformed to represent the view under consideration. The transformations are learned with kernel alignment to the known part of the kernel matrix, allowing for finding generalizable structures in the kernel matrix under completion. Its missing values can then be predicted with the data available in other views. We illustrate the benefits of our approach with simulated data, multivariate digits dataset and multi-view dataset on gesture classification, as well as with real biological datasets from studies of pattern formation in early Drosophila melanogaster embryogenesis.}
}
@article{CHIANG2022108807,
title = {A multi-embedding neural model for incident video retrieval},
journal = {Pattern Recognition},
volume = {130},
pages = {108807},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108807},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002886},
author = {Ting-Hui Chiang and Yi-Chun Tseng and Yu-Chee Tseng},
keywords = {Artificial intelligence, Computer vision, Deep metric learning, Incident video retrieval},
abstract = {Many internet search engines have been developed, however, the retrieval of video clips remains a challenge. This paper considers the retrieval of incident videos, which may contain more spatial and temporal semantics. We propose an encoder-decoder ConvLSTM model that explores multiple embeddings of a video to facilitate comparison of similarity between a pair of videos. The model is able to encode a video into an embedding that integrates both its spatial information and temporal semantics. Multiple video embeddings are then generated from coarse- and fine-grained features of a video to capture high- and low-level meanings. Subsequently, a learning-based comparative model is proposed to compare the similarity of two videos based on their embeddings. Extensive evaluations are presented and show that our model outperforms state-of-the-art methods for several video retrieval tasks on the FIVR-200K, CC_WEB_VIDEO, and EVVE datasets.}
}
@article{DINESH2022108783,
title = {Fully convolutional Deep Stacked Denoising Sparse Auto encoder network for partial face reconstruction},
journal = {Pattern Recognition},
volume = {130},
pages = {108783},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108783},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002643},
author = {P.S. Dinesh and M. Manikandan},
keywords = {Partial face recognition, Deep learning algorithm, Fully convolutional network, Autoencoder},
abstract = {Face recognition is one of the most successful applications of image analysis. Since 1960s, automatic face recognition research has been carried out, but the problem is still unresolved. Therefore, in this manuscript, a novel Partial face reconstruction (PFR) algorithm called Self- motivated feature mapping (SMFM) combining a Fully Convolutional Network (FCN) and Deep Stacked Denoising Sparse Autoencoders (DS-DSA) algorithm is proposed to overcome the challenges. The proposed approach focuses on the generation of feature maps from the Fully Convolutional Network and it is used Deep Stacked Denoising Sparse Autoencoders to perform the partial face reconstruction. The spatial maps are generated by extracting the features from Fully Convolutional Network and it is supplied as the input for partial reconstruction and re-identification to the Deep Stacked Denoising Sparse Autoencoders network. The main aim of the proposed work is “to enhance the accuracy during facial reconstruction”. The proposed approach is implemented in MATLAB platform. The performance of the proposed approach attains 23.45% and 20.41% accuracy,25.93`% and 19.43% sensitivity, 22.21% and 24.41% precision and20.21% and 23.41% Specificity greater than the existing approaches, like Partial Face Reconstruction using generative adversarial networks (GANs), Partial Face Reconstruction using Deep Recurrent neural network (DRNN).}
}
@article{COSKUN2022108702,
title = {An adaptive estimation method with exploration and exploitation modes for non-stationary environments},
journal = {Pattern Recognition},
volume = {129},
pages = {108702},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108702},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001832},
author = {Kutalmış Coşkun and Borahan Tümer},
keywords = {Stochastic learning, Concept drift, Change detection, Parameter estimation, Dynamic learning rate},
abstract = {Dynamic systems are highly complex and hard to deal with due to their subject- and time-varying nature. The fact that most of the real world systems/events are of dynamic character makes modeling and analysis of such systems inevitable and charmingly useful. One promising estimation method that is capable of unlearning past information to deal with non-stationarity is Stochastic Learning Weak Estimator (SLWE) by Oommen and Rueda (2006). However, due to using a constant learning rate, it faces a trade-off between plasticity and stability. In this paper, we model SLWE as a random walk and provide rigorous theoretical analysis of asymptotic behavior of estimates to obtain a statistical model. Utilizing this model, we detect changes in stationarity to switch between exploratory and exploitative learning modes. Experimental evaluations on both synthetic and real world data show that the proposed method outperforms related algorithms in different types of drifts.}
}
@article{NGUYENMEIDINE2022108771,
title = {Incremental multi-target domain adaptation for object detection with efficient domain transfer},
journal = {Pattern Recognition},
volume = {129},
pages = {108771},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108771},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002527},
author = {Le Thanh Nguyen-Meidine and Madhu Kiran and Marco Pedersoli and Jose Dolz and Louis-Antoine Blais-Morin and Eric Granger},
keywords = {Deep learning, Convolutional NNs, Object detection, Unsupervised domain adaptation, Multi-Target domain adaptation, Incremental learning},
abstract = {Recent advances in unsupervised domain adaptation have significantly improved the recognition accuracy of CNNs by alleviating the domain shift between (labeled) source and (unlabeled) target data distributions. While the problem of single-target domain adaptation (STDA) for object detection has recently received much attention, multi-target domain adaptation (MTDA) remains largely unexplored, despite its practical relevance in several real-world applications, such as multi-camera video surveillance. Compared to the STDA problem that may involve large domain shifts between complex source and target distributions, MTDA faces additional challenges, most notably the computational requirements and catastrophic forgetting of previously-learned targets, which can depend on the order of target adaptations. STDA for detection can be applied to MTDA by adapting one model per target, or one common model with a mixture of data from target domains. However, these approaches are either costly or inaccurate. The only state-of-art MTDA method specialized for detection learns targets incrementally, one target at a time, and mitigates the loss of knowledge by using a duplicated detection model for knowledge distillation, which is computationally expensive and does not scale well to many domains. In this paper, we introduce an efficient approach for incremental learning that generalizes well to multiple target domains. Our MTDA approach is more suitable for real-world applications since it allows updating the detection model incrementally, without storing data from previous-learned target domains, nor retraining when a new target domain becomes available. Our approach leverages domain discriminators to train a novel Domain Transfer Module (DTM), which only incurs a modest overhead. The DTM transforms source images according to diverse target domains, allowing the model to access a joint representation of previously-learned target domains, and to effectively limit catastrophic forgetting. Our proposed method – called MTDA with DTM (MTDA-DTM) – is compared against state-of-the-art approaches on several MTDA detection benchmarks and Wildtrack, a benchmark for multi-camera pedestrian detection. Results indicate that MTDA-DTM achieves the highest level of detection accuracy across multiple target domains, yet requires significantly fewer computational resources. Our code is available.11https://github.com/Natlem/M-HTCN.}
}
@article{HE2022108714,
title = {Multi-manifold discriminant local spline embedding},
journal = {Pattern Recognition},
volume = {129},
pages = {108714},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108714},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001959},
author = {Ping He and Xiaohua Xu and Xincheng Chang and Jie Ding and Suquan Chen},
keywords = {Manifold learning, Dimension reduction, Classification, Thin plate spline, Multiple manifolds},
abstract = {Manifold learning reveals the intrinsic low-dimensional manifold structure of high-dimensional data and has achieved great success in a wide spectrum of applications. However, traditional manifold learning methods assume that all the data lie on a common manifold, hence fail to capture the complicated geometry structure of the real-world data lying on multiple manifolds. This paper proposes a novel Multi-manifold Discriminant Local Spline Embedding (MDLSE) algorithm for high-dimensional classification, which considers a more realistic scenario where data of the same class lies on the same manifold. On the basis of this assumption, MDLSE seeks to reconstruct multiple manifolds for different classes of data in the embedding and separate them as apart as possible. In order to preserve the geometry structure of all the manifolds, MDLSE employs thin plate splines to align the local patches within each manifold compatibly in the global embedding. Meanwhile, to separate the different manifolds, MDLSE utilizes discriminative information to ensure the neighboring data from different manifolds to be mapped far from each other. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of MDLSE over the other representative manifold learning algorithms. The advantage of MDLSE is often more obvious on smaller size of training data and in lower embedding dimensions.}
}
@article{WANG2022108726,
title = {Pose error analysis method based on a single circular feature},
journal = {Pattern Recognition},
volume = {129},
pages = {108726},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108726},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002072},
author = {Zepeng Wang and Derong Chen and Jiulu Gong},
keywords = {Pose measurement, Monocular vision, Geometrical analysis, Outlier analysis, Optimisation},
abstract = {The measurement accuracy of pose parameters based on a single circular feature depends not only on the accuracy of camera calibration and feature extraction but also on the relative pose of the feature and camera—different poses correspond to different error transmission coefficients. To obtain the relationship between measurement errors and pose parameters, we propose an error analysis method based on geometric interpretation. The method characterises measurement error by the sensitivity the imaging feature has to the variation of pose parameters. In addition, the method can be extended to the error analysis work of other coplanar features' pose measurement algorithms. We conducted simulations on measurement errors of pose parameters under different poses, and the results show that the error distribution of pose parameters is in good agreement with the theoretical analysis. Moreover, we propose a method for judging and optimising outliers, and experimental results show the feasibility of this method.}
}
@article{ZHU2022108742,
title = {Representation learning with deep sparse auto-encoder for multi-task learning},
journal = {Pattern Recognition},
volume = {129},
pages = {108742},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108742},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002230},
author = {Yi Zhu and Xindong Wu and Jipeng Qiang and Xuegang Hu and Yuhong Zhang and Peipei Li},
keywords = {Deep sparse auto-encoder, Multi-task learning, RICA, Labeled and unlabeled data},
abstract = {We demonstrate an effective framework to achieve a better performance based on Deep Sparse auto-encoder for Multi-task Learning, called DSML for short. To learn the reconstructed and higher-level features on cross-domain instances for multiple tasks, we combine the labeled and unlabeled data from all tasks to reconstruct the feature representations. Furthermore, we propose the model of Stacked Reconstruction Independence Component Analysis (SRICA for short) for the optimization of feature representations with a large amount of unlabeled data, which can effectively address the redundancy of image data. Our proposed SRICA model is developed from RICA and is based on deep sparse auto-encoder. In addition, we adopt a Semi-Supervised Learning method (SSL for short) based on model parameter regularization to build a unified model for multi-task learning. There are several advantages in our proposed framework as follows: 1) The proposed SRICA makes full use of a large amount of unlabeled data from all tasks. It is used to pursue an optimal sparsity feature representation, which can overcome the over-fitting problem effectively. 2) The deep architecture used in our SRICA model is applied for higher-level and better representation learning, which is designed to train on patches for sphering the input data. 3) Training parameters in our proposed framework has lower computational cost compared to other common deep learning methods such as stacked denoising auto-encoders. Extensive experiments on several real image datasets demonstrate our proposed framework outperforms the state-of-the-art methods.}
}
@article{KANG2022108766,
title = {Pay attention to what you read: Non-recurrent handwritten text-Line recognition},
journal = {Pattern Recognition},
volume = {129},
pages = {108766},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108766},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002473},
author = {Lei Kang and Pau Riba and Marçal Rusiñol and Alicia Fornés and Mauricio Villegas},
keywords = {Handwriting text recognition, Transformers, Self-Attention, Implicit language model},
abstract = {The advent of recurrent neural networks for handwriting recognition marked an important milestone reaching impressive recognition accuracies despite the great variability that we observe across different writing styles. Sequential architectures are a perfect fit to model text lines, not only because of the inherent temporal aspect of text, but also to learn probability distributions over sequences of characters and words. However, using such recurrent paradigms comes at a cost at training stage, since their sequential pipelines prevent parallelization. In this work, we introduce a novel method that bypasses any recurrence during the training process with the use of transformer models. By using multi-head self-attention layers both at the visual and textual stages, we are able to tackle character recognition as well as to learn language-related dependencies of the character sequences to be decoded. Our model is unconstrained to any predefined vocabulary, being able to recognize out-of-vocabulary words, i.e. words that do not appear in the training vocabulary. We significantly advance over prior art and demonstrate that satisfactory recognition accuracies are yielded even in few-shot learning scenarios.}
}
@article{LI2022108738,
title = {An end-to-end identity association network based on geometry refinement for multi-object tracking},
journal = {Pattern Recognition},
volume = {129},
pages = {108738},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108738},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002199},
author = {Rui Li and Baopeng Zhang and Zhu Teng and Jianping Fan},
keywords = {Multi-object tracking, Interactions, Occlusions, Data association, Identity verification},
abstract = {In multi-target tracking, object interactions and occlusions are two significant factors that affect tracking performance. To settle this, we propose an identity association network (IANet) that integrates the geometry refinement network (GRNet) and the identity verification (IV) module to perform data association and reason the mapping between the detections and tracklets. In our data association process, the object drifts caused by object interactions are suppressed effectively by encoding the direction and velocity of objects to refine the geometric position of tracklets. The tracklets with refined geometric information are further utilized in the IV module to achieve a sufficient encoding of multivariate spatial cues including both appearance and geometry information, which defeats the misleading impacts of interactions and occlusions dramatically in multi-object tracking. The extensive experiments and comparative evaluations have demonstrated that our proposed method can significantly outperform many state-of-the-art methods on benchmarks of 2D MOT2015, MOT16, MOT17, MOT20, and KITTI by using public detection and online settings.}
}
@article{WANG2022108782,
title = {Salient object detection with image-level binary supervision},
journal = {Pattern Recognition},
volume = {129},
pages = {108782},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108782},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002631},
author = {Pengjie Wang and Yuxuan Liu and Ying Cao and Xin Yang and Yu Luo and Huchuan Lu and Zijian Liang and Rynson W.H. Lau},
keywords = {Weak supervision, Salient object detection, Binary labels},
abstract = {Recent deep learning based salient object detection (SOD) methods have achieved impressive performance. However, while fully-supervised methods require a large amount of labeled data, weakly-supervised methods still require a considerable human effort. To address this problem, we propose a novel weakly-supervised method for salient object detection based on only binary image tags, which are much cheaper to collect. Our basic idea is to construct a dataset of images that are labeled as either salient (with salient objects) or non-salient (without salient objects), and leverage such binary labels as supervision to learn a salient object detector based on existing unsupervised methods. In particular, we propose a target saliency map hallucinator, which can synthesize pseudo ground truth saliency maps for the salient images in the training data solely from binary labels. We can then use the pseudo ground truth labels to train a salient object detector. Experimental results show that our method performs comparably to the state-of-the-art weakly-supervised methods, but requires considerably less human supervision.}
}
@article{ZENG2022108754,
title = {Realistic frontal face reconstruction using coupled complementarity of far-near-sighted face images},
journal = {Pattern Recognition},
volume = {129},
pages = {108754},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108754},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002357},
author = {Kangli Zeng and Zhongyuan Wang and Tao Lu and Jianyu Chen and Baojin Huang and Zhen Han and Xin Tian},
keywords = {Face frontalization, Super-resolution, Information compensation, Far-near faces},
abstract = {There is still a huge gap in the accuracy of face recognition in public video surveillance scenarios. The far-sighted low-resolution (LR) frontal faces have holistic facial profiles but lack sufficient clearness, while the near-sighted high-resolution (HR) tilted faces show rich facial details yet incomplete facial structure suffering from the overhead self-occlusion of the head blocking the face. Following this observation, this paper proposes a dual-branch HR frontal face reconstruction network to explicitly exploit such coupled complementarity hidden in the far-near face images of the same subject, where one branch performs super-resolution (SR) of the LR frontal face and the other branch performs detail fusion and holistic compensation between multiple HR tilted faces as well as the super-resolved frontal result. In particular, we propose a secondary relevance attention mechanism to enhance the embedding of key features, which sequentially performs rough and precise feature matching and embedding, thus enabling coarse-to-fine progressive compensation. Further, scale-entangled densely connected blocks (SEDCB) are used to gradually integrate the relevance information at different scales (due to the different sighting distances) to promote the information interaction between the features of tilted faces. Besides, we also propose a ternary coupled sample pair (LR far-sighted frontal face, HR near-sighted tilted face, normal ground truth clear face) training scheme to supervise the network optimization. Extensive experimental results on two real-world tilt-view face datasets show that our method can not only reconstruct more realistic HR frontal faces but also facilitate the down-stream face identification task compared with the competing counterparts.}
}
@article{XING2022108806,
title = {Joint prediction of monocular depth and structure using planar and parallax geometry},
journal = {Pattern Recognition},
volume = {130},
pages = {108806},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108806},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002874},
author = {Hao Xing and Yifan Cao and Maximilian Biber and Mingchuan Zhou and Darius Burschka},
keywords = {Monocular depth estimation, Plane and parallax geometry, Structure information, Joint prediction model},
abstract = {Supervised learning depth estimation methods can achieve good performance when trained on high-quality ground-truth, like LiDAR data. However, LiDAR can only generate sparse 3D maps which causes losing information. Obtaining high-quality ground-truth depth data per pixel is difficult to acquire. In order to overcome this limitation, we propose a novel approach combining structure information from a promising Plane and Parallax geometry pipeline with depth information into a U-Net supervised learning network, which results in quantitative and qualitative improvement compared to existing popular learning-based methods. In particular, the model is evaluated on two large-scale and challenging datasets: KITTI Vision Benchmark and Cityscapes dataset and achieve the best performance in terms of relative error. Compared with pure depth supervision models, our model has impressive performance on depth prediction of thin objects and edges, and compared to structure prediction baseline, our model performs more robustly.}
}
@article{WANG2022108794,
title = {The iterative convolution–thresholding method (ICTM) for image segmentation},
journal = {Pattern Recognition},
volume = {130},
pages = {108794},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108794},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002758},
author = {Dong Wang and Xiao-Ping Wang},
keywords = {Convolution, Thresholding, Image segmentation, Heat kernel},
abstract = {Variational methods, which have been tremendously successful in image segmentation, work by minimizing a given objective functional. The objective functional usually consists of a fidelity term and a regularization term. Because objective functionals may vary from different types of images, developing an efficient, simple, and general numerical method to minimize them has become increasingly vital. However, many existing methods are model-based, converge relatively slowly, or involve complicated techniques. In this paper, we develop a novel iterative convolution–thresholding method (ICTM) that is simple, efficient, and applicable to a wide range of variational models for image segmentation. In ICTM, the interface between two different segment domains is implicitly represented by the characteristic functions of domains. The fidelity term is usually written into a linear functional of the characteristic functions, and the regularization term is approximated by a functional of characteristic functions in terms of heat kernel convolution. This allows us to design an iterative convolution–thresholding method to minimize the approximate energy. The method has the energy-decaying property, and thus the unconditional stability is theoretically guaranteed. Numerical experiments show that the method is simple, easy to implement, robust, and applicable to various image segmentation models.}
}
@article{ALAVI2022108770,
title = {A bi-level formulation for multiple kernel learning via self-paced training},
journal = {Pattern Recognition},
volume = {129},
pages = {108770},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108770},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002515},
author = {Fatemeh Alavi and Sattar Hashemi},
keywords = {Multiple kernel learning, Self-paced learning, Bi-level optimization, Local kernel alignment, Global kernel alignment},
abstract = {Multiple kernel learning (MKL) is a crucial issue which has been widely researched over the last two decades. Although existing MKL algorithms have achieved satisfactory performance in a broad range of applications, these methods do not adequately consider the adverse effects of unreliable or less reliable instances. To handle this shortcoming, we formulate multiple kernel learning in a bi-level learning paradigm consisting of the kernel combination weight learning (KWL) stage and the self-paced learning (SPL) stage, which alternatively negotiate with each other. The KWL stage dynamically absorbs reliable instances into model learning to accurately capture neighborhood relationships and obtains kernel coefficients via maximizing both global and local kernel alignment in a common schema. The SPL stage automatically evaluates the reliability of training samples via self-paced training. The extensive experiments indicate the robustness and superiority of the presented approach in comparison with existing MKL methods.}
}
@article{CHEN2022108781,
title = {Residual objectness for imbalance reduction},
journal = {Pattern Recognition},
volume = {130},
pages = {108781},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108781},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200262X},
author = {Joya Chen and Dong Liu and Bin Luo and Xuezheng Peng and Tong Xu and Enhong Chen},
keywords = {Object detection, Class imbalance, Residual objectness},
abstract = {As most object detectors rely on dense candidate samples to cover objects, they have always suffered from the extreme imbalance between very few foreground samples and numerous background samples during training, i.e., the foreground-background imbalance. Although several resampling and reweighting schemes (e.g., OHEM, Focal Loss, GHM) have been proposed to alleviate the imbalance, they are usually heuristic with multiple hyper-parameters, which is difficult to generalize on different object detectors and datasets. In this paper, we propose a novel Residual Objectness (ResObj) mechanism that adaptively learns how to address the foreground-background imbalance problem in object detection. Specifically, we first formulate the imbalance problems on all object classes as an imbalance problem on an “objectness” class. Then, we design multiple cascaded objectness estimators with residual connections for that objectness class to progressively distinguish the foreground samples from background samples. With our residual objectness mechanism, object detectors can learn how to address the foreground-background problem in an end-to-end way, rather than rely on hand-crafted resampling or reweighting schemes. Extensive experiments on the COCO benchmark demonstrate the effectiveness and compatibility of our method for various object detectors: the RetinaNet-ResObj, YOLOv3-ResObj and FasterRCNN-ResObj achieve relative 3%∼4% Average Precision (AP) improvements compared with their vanilla models, respectively.}
}
@article{2024110100,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {146},
pages = {110100},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(23)00797-5},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007975}
}
@article{SUH2022108810,
title = {Two-stage generative adversarial networks for binarization of color document images},
journal = {Pattern Recognition},
volume = {130},
pages = {108810},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108810},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002916},
author = {Sungho Suh and Jihun Kim and Paul Lukowicz and Yong Oh Lee},
keywords = {Document image binarization, Generative adversarial networks, Optical character recognition, Color document image enhancement},
abstract = {Document image enhancement and binarization methods are often used to improve the accuracy and efficiency of document image analysis tasks such as text recognition. Traditional non-machine-learning methods are constructed on low-level features in an unsupervised manner but have difficulty with binarization on documents with severely degraded backgrounds. Convolutional neural network (CNN)based methods focus only on grayscale images and on local textual features. In this paper, we propose a two-stage color document image enhancement and binarization method using generative adversarial neural networks. In the first stage, four color-independent adversarial networks are trained to extract color foreground information from an input image for document image enhancement. In the second stage, two independent adversarial networks with global and local features are trained for image binarization of documents of variable size. For the adversarial neural networks, we formulate loss functions between a discriminator and generators having an encoder–decoder structure. Experimental results show that the proposed method achieves better performance than many classical and state-of-the-art algorithms over the Document Image Binarization Contest (DIBCO) datasets, the LRDE Document Binarization Dataset (LRDE DBD), and our shipping label image dataset. We plan to release the shipping label dataset as well as our implementation code at github.com/opensuh/DocumentBinarization/.}
}
@article{LUO2022108713,
title = {ECDNet: A bilateral lightweight cloud detection network for remote sensing images},
journal = {Pattern Recognition},
volume = {129},
pages = {108713},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108713},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001947},
author = {Chen Luo and Shanshan Feng and Xutao Li and Yunming Ye and Baoquan Zhang and Zhihao Chen and YingLing Quan},
keywords = {Lightweight network, Efficient cloud detection, Dual-branch architecture},
abstract = {Cloud detection is one of the critical tasks in remote sensing image pre-processing and it has attracted extensive research interest. In recent years, deep neural networks based cloud detection methods have surpassed the traditional methods (threshold-based methods and conventional machine learning-based methods). However, current approaches mainly focus on improving detection accuracy. The computation complexity and large model size are ignored. To tackle this problem, we propose a lightweight deep learning cloud detection model: Efficient Cloud Detection Network (ECDNet). This model is based on the encoder-decoder structure. In the encoder, a two-path architecture is proposed to extract the spatial and semantic information concurrently. One pathway is the detail branch. It is designed to capture low-level detail spatial features with only a few parameters. The other pathway is the semantic branch, which is mainly for capturing context features. In the semantic branch, a proposed dense pyramid module (DPM) is designed for multi-scale contextual information extraction. The number of parameters and calculations in DPM is greatly reduced by features reusing. Besides, a FusionBlock is developed to merge these two kinds of information. Then the extreme lightweight decoder recovers the cloud mask to the same scale as the input image step by step. To improve performance, boost loss is introduced without inference cost increment. We evaluate the proposed method on two public datasets: LandSat8 and MODIS. Extensive experiments demonstrate that the proposed ECDNet achieves comparable accuracy as the state-of-art cloud detection methods, and meantime has a much smaller model size and less computation burden.}
}
@article{XIA2022108725,
title = {Dual relation network for temporal action localization},
journal = {Pattern Recognition},
volume = {129},
pages = {108725},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108725},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002060},
author = {Kun Xia and Le Wang and Sanping Zhou and Gang Hua and Wei Tang},
keywords = {Temporal action localization, Relation reasoning},
abstract = {Temporal action localization is a challenging task for video understanding. Most previous methods process each proposal independently and neglect the reasoning of proposal-proposal and proposal-context relations. We argue that the supplementary information obtained by exploiting these relations can enhance the proposal representation and further boost the action localization. To this end, we propose a dual relation network to model both proposal-proposal and proposal-context relations. Concretely, a proposal-proposal relation module is leveraged to learn discriminative supplementary information from relevant proposals, which allows the network to model their interaction based on appearance and geometric similarities. Meanwhile, a proposal-context relation module is employed to mine contextual clues by adaptively learning from the global context outside of region-based proposals. They effectively leverage the inherent correlation between actions and the long-term dependency with videos for high-quality proposal refinement. As a result, the proposed framework enables the model to distinguish similar action instances and locate temporal boundaries more precisely. Extensive experiments on the THUMOS14 dataset and ActivityNet v1.3 dataset demonstrate that the proposed method significantly outperforms recent state-of-the-art methods.}
}
@article{TRAN2022108765,
title = {Security and privacy enhanced smartphone-based gait authentication with random representation learning and digital lockers},
journal = {Pattern Recognition},
volume = {129},
pages = {108765},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108765},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002461},
author = {Lam Tran and Thuc Nguyen and Hyunil Kim and Deokjai Choi},
keywords = {Gait authentication, Biometric template protection, Biometric cryptosystems, Gait recognition, Key binding scheme, Biometric key generation},
abstract = {Gait data captured by inertial sensors of smartphone have demonstrated promising results on user authentication. However, most existing models stored the enrolled gait pattern in plaintext for matching with the pattern being validated, thus, posed critical security and privacy issues. In this study, we present a gait cryptosystem that generates from gait data captured by smartphone sensors the random keys for user authentication, meanwhile, secures the gait pattern. First, we propose a revocable and random binary string extraction method using deep neural network followed by feature-wise binarization. A novel loss function for network optimization is also designed, to tackle not only the intra-user stability but also the inter-user randomness. Second, we propose a new biometric key generation scheme, namely Irreversible Error Correct and Obfuscate (IECO), improved from the Error Correct and Obfuscate (ECO) scheme, to securely generate from the binary string a random and irreversible key. The model was evaluated with two benchmark datasets as OU-ISIR and whuGAIT. The evaluation showed that our model could generate the key of 139 bits from 5-second data sequence with zero False Acceptance Rate (FAR) and False Rejection Rate (FRR) smaller than 5.441%. In addition, the security and user privacy analyses showed that our model was secure against existing attacks on biometric template protection, and fulfilled the irreversibility and unlinkability requirements.}
}
@article{ZHANG2022108737,
title = {Improving the Facial Expression Recognition and Its Interpretability via Generating Expression Pattern-map},
journal = {Pattern Recognition},
volume = {129},
pages = {108737},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108737},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002187},
author = {Jing Zhang and Huimin Yu},
keywords = {Facial expression recognition, Facial expression visualization, Expression pattern-map generator, Deep neural networks},
abstract = {Facial expression recognition focuses on extracting expression-related features on a face. In this paper, a novel method is proposed for facial expression modeling based on the following two aspects: seeking expression-related regions more accurately, and enhancing expression features more discriminating. To this end, we design a model containing three submodules: the Expression Feature Extractor (EFE), the Expression Mask Refiner (EMR), and the Expression Pattern-Map Generator (EPMG). The EFE module is the backbone that extracts expression features and generates a coarse attention mask which roughly indicates expression-related regions. The EMR module refines the mask to be more precise by modeling the relationship among expression-related regions, and generates the masked features. The EPMG module utilizes the masked features to further model the fusion and extraction process which obtains a compact and discriminating expression-salient embedding for recognition, and generates an expression pattern-map. We propose the concept of the expression pattern-map, which provides a unified visualization of expression features and improves the interpretability of facial expression recognition. Our model is evaluated on four public datasets (CK+, Oulu-CASIA, RAF-DB, AffectNet), and achieves the competitive performance compared with the state-of-the-art.}
}
@article{LI2022108684,
title = {Spatial information enhancement network for 3D object detection from point cloud},
journal = {Pattern Recognition},
volume = {128},
pages = {108684},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108684},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001650},
author = {Ziyu Li and Yuncong Yao and Zhibin Quan and Jin Xie and Wankou Yang},
keywords = {3D object detection, Autonomous vehicles, Point cloud, LiDAR sensor, 3D shape completion},
abstract = {LiDAR-based 3D object detection pushes forward an immense influence on autonomous vehicles. Due to the limitation of the intrinsic properties of LiDAR, fewer points are collected at the objects farther away from the sensor. This imbalanced density of point clouds degrades the detection accuracy but is generally neglected by previous works. To address the challenge, we propose a novel two-stage 3D object detection framework, named SIENet. Specifically, we design the Spatial Information Enhancement (SIE) module to predict the spatial shapes of the foreground points within proposals, and extract the structure information to learn the representative features for further box refinement. The predicted spatial shapes are complete and dense point sets, thus the extracted structure information contains more semantic representation. Besides, we design the Hybrid-Paradigm Region Proposal Network (HP-RPN) which includes multiple branches to learn discriminate features and generate accurate proposals for the SIE module. Extensive experiments on the KITTI 3D object detection benchmark show that our elaborately designed SIENet outperforms the state-of-the-art methods by a large margin. Codes will be publicly available at https://github.com/Liz66666/SIENet.}
}
@article{WANG2022108729,
title = {Versatile, full‐spectrum, and swift network sampling for model generation},
journal = {Pattern Recognition},
volume = {129},
pages = {108729},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108729},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002102},
author = {Huanyu Wang and Yongshun Zhang and Jianxin Wu},
keywords = {Model generation, Convolutional neural networks, Structured pruning, Model compression},
abstract = {Given one task, it is difficult to generate CNN models for many different hardware platforms with extremely diverse computing power for this task. Repeating network pruning or architecture search for each platform is very time-consuming. In this paper, we propose properties that are required for this model generation problem: versatile (fits diverse applications and network structures), full-spectrum (generates models for devices with tiny to gigantic computing power), and swift (total training time for all platforms is short, and generated models have low latency). We show that existing methods do not satisfy these requirements and propose a VFS method (the V/F/S represents Versatile/Full-spectrum/Swift, respectively). VFS uses importance sampling to sample many submodels with versatile structures and with different input image resolutions. We propose new fine-tuning strategies that only need to fine-tune a best candidate submodel for few epochs for each platform. VFS satisfies all three requirements. It generates versatile models with low latency for diverse applications, is suitable for devices with a wide range of computing power differences, and the models which are generated by VFS achieve state-of-the-art accuracy.}
}
@article{YAO2022108708,
title = {A sparse graph wavelet convolution neural network for video-based person re-identification},
journal = {Pattern Recognition},
volume = {129},
pages = {108708},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108708},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001893},
author = {Yingmao Yao and Xiaoyan Jiang and Hamido Fujita and Zhijun Fang},
keywords = {Video-based person re-identification, Weighted sparse graph, Graph wavelet convolution neural network},
abstract = {Video-based person re-identification (Re-ID) aims to match identical person sequences captured across non-overlapping surveillance areas. It is an essential yet challenging task to effectively embed spatial and temporal information into the video feature representation. For one thing, we observe that different frames in the video can provide complementary information for each other. Also, local features which is lost due to target occlusion or visual ambiguity in one frame can be supplemented by the same pedestrian part in other frames. For another thing, graph neural network enables the contextual interactions between relevant regional features. Therefore, we propose a novel sparse graph wavelet convolution neural network (SGWCNN) for video-based person Re-ID. Distinct from previous graph-based Re-ID methods, we exploit the weighted sparse graph to model the semantic relation among the local patches of pedestrians in the video. Each local patch in one frame can extract supplementary information from highly related patches in other frames. Moreover, to effectively solve the problems of short time occlusion and pedestrian misalignment, the graph wavelet convolution neural network is adopted for feature propagation to refine regional features iteratively. Experiments and evaluation on three challenging benchmarks, that is, MARS, DukeMTMC-VideoReID, and iLIDS-VID, show that the proposed SGWCNN effectively improves the performance of video-based person re-identification.}
}
@article{CHEN2022108753,
title = {Orthogonal channel attention-based multi-task learning for multi-view facial expression recognition},
journal = {Pattern Recognition},
volume = {129},
pages = {108753},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108753},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002345},
author = {Jingying Chen and Lei Yang and Lei Tan and Ruyi Xu},
keywords = {Multi-view facial expression recognition, Orthogonal channel attention, Multi-task learning, Siamese convolutional neural network, Separated channel attention module},
abstract = {Multi-view facial expression recognition (FER) is a challenging computer vision task due to the large intra-class difference caused by viewpoint variations. This paper presents a novel orthogonal channel attention-based multi-task learning (OCA-MTL) approach for FER. The proposed OCA-MTL approach adopts a Siamese convolutional neural network (CNN) to force the multi-view expression recognition model to learn the same features as the frontal expression recognition model. To further enhance the recognition accuracy of non-frontal expression, the multi-view expression model adopts a multi-task learning framework that regards head pose estimation (HPE) as an auxiliary task. A separated channel attention (SCA) module is embedded in the multi-task learning framework to generate individual attention for FER and HPE. Furthermore, orthogonal channel attention loss is presented to force the model to employ different feature channels to represent the facial expression and head pose, thereby decoupling them. The proposed approach is performed on two public facial expression datasets to evaluate its effectiveness and achieves an average recognition accuracy rate of 88.41% under 13 viewpoints on Multi-PIE and 89.04% under 5 viewpoints on KDEF, outperforming state-of-the-art methods.}
}
@article{FENG2022108777,
title = {DMT: Dynamic mutual training for semi-supervised learning},
journal = {Pattern Recognition},
volume = {130},
pages = {108777},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108777},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002588},
author = {Zhengyang Feng and Qianyu Zhou and Qiqi Gu and Xin Tan and Guangliang Cheng and Xuequan Lu and Jianping Shi and Lizhuang Ma},
keywords = {Dynamic mutual training, Inter-model disagreement, Noisy pseudo label, Semi-supervised learning},
abstract = {Recent semi-supervised learning methods use pseudo supervision as core idea, especially self-training methods that generate pseudo labels. However, pseudo labels are unreliable. Self-training methods usually rely on single model prediction confidence to filter low-confidence pseudo labels, thus remaining high-confidence errors and wasting many low-confidence correct labels. In this paper, we point out it is difficult for a model to counter its own errors. Instead, leveraging inter-model disagreement between different models is a key to locate pseudo label errors. With this new viewpoint, we propose mutual training between two different models by a dynamically re-weighted loss function, called Dynamic Mutual Training (DMT). We quantify inter-model disagreement by comparing predictions from two different models to dynamically re-weight loss in training, where a larger disagreement indicates a possible error and corresponds to a lower loss value. Extensive experiments show that DMT achieves state-of-the-art performance in both image classification and semantic segmentation. Our codes are released at https://github.com/voldemortX/DST-CBC.}
}
@article{PARK2022108764,
title = {Maximization and restoration: Action segmentation through dilation passing and temporal reconstruction},
journal = {Pattern Recognition},
volume = {129},
pages = {108764},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108764},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200245X},
author = {Junyong Park and Daekyum Kim and Sejoon Huh and Sungho Jo},
keywords = {Action segmentation, Temporal segmentation, Video understanding},
abstract = {Action segmentation aims to split videos into segments of different actions. Recent work focuses on dealing with long-range dependencies of long, untrimmed videos, but still suffers from over-segmentation and performance saturation due to increased model complexity. This paper addresses the aforementioned issues through a divide-and-conquer strategy that first maximizes the frame-wise classification accuracy of the model and then reduces the over-segmentation errors. This strategy is implemented with the Dilation Passing and Reconstruction Network, composed of the Dilation Passing Network, which primarily aims to increase accuracy by propagating information of different dilations, and the Temporal Reconstruction Network, which reduces over-segmentation errors by temporally encoding and decoding the output features from the Dilation Passing Network. We also propose a weighted temporal mean squared error loss that further reduces over-segmentation. Through evaluations on the 50Salads, GTEA, and Breakfast datasets, we show that our model achieves significant results compared to existing state-of-the-art models.}
}
@article{CHEN2022108769,
title = {Symmetric Binary Tree Based Co-occurrence Texture Pattern Mining for Fine-grained Plant Leaf Image Retrieval},
journal = {Pattern Recognition},
volume = {129},
pages = {108769},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108769},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002503},
author = {Xin Chen and Bin Wang and Yongsheng Gao},
keywords = {Leaf image pattern, Species recognition, Fine-grained image recognition, Feature fusion, Image retrieval},
abstract = {Leaf image patterns have been actively researched for plant species recognition. However, as a very challenging fine-grained pattern identification issue, cultivar recognition in which the leaf image patterns usually have very subtle difference among cultivars has not yet received considerable attention in computer vision and pattern recognition community. In this paper, a novel symmetric geometric configuration, named Symmetric Binary Tree (SBT) which has multiple symmetric branch pairs and can change in size, is designed to mine the multiple scale co-occurrence texture patterns. The resulting SBT descriptors encode both shape and texture features which make them more informative than the existing individual descriptors and co-occurrence features. A novel feature fusion scheme, named K-NN Based Handcrafted and Deep Features Fusion (KNN-HDFF) that encodes the neighbouring information of distance measure, is proposed for further boosting the retrieval performance. Extensive experiments conducted on the challenging soybean cultivar leaf image dataset and peanut cultivar leaf image dataset consistently indicate the superiority of the proposed method over the state-of-the-art methods on fine-grained leaf image retrieval. We also conduct extensive experiments of feature fusions using the proposed KNN-HDFF on the benchmark datasets and the experimental results prove its potential for improving the performance of cultivar identification which also indicates that fusing handcrafted and deep features may be the direction to address the challenging fine-grained image recognition problem.}
}
@article{ZHAO2022108741,
title = {Progressive privileged knowledge distillation for online action detection},
journal = {Pattern Recognition},
volume = {129},
pages = {108741},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108741},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002229},
author = {Peisen Zhao and Lingxi Xie and Jiajie Wang and Ya Zhang and Qi Tian},
keywords = {Online action detection, Knowledge distillation, Privileged information, Curriculum learning},
abstract = {Online Action Detection (OAD) in videos addresses the problem of real-time analysis for streaming videos, i.e., only the observed historical video frames are available at prediction time. Considering the future frames observable only at the training stage as a form of privileged information, this paper adopts the Learning Using Privileged Information (LUPI) paradigm. Knowledge distillation (KD) is employed to transfer the privileged information from the offline teacher to the online student. Note that this setting is different from conventional KD because the difference between the teacher and student models mostly lies in the input data rather than the network architecture. To relieves the input information gap for the LUPI, we propose a simple but effective Privileged Knowledge Distillation (PKD) method that enforce KD loss to partial hidden features of the student model. Moreover, we also schedules a curriculum learning procedure to gradually distill the privileged information. This approach is named as Progressive Privileged Knowledge Distillation (PPKD). Compared to some OAD methods that explicitly predict future frames or feature, our approach avoids predicting stage and achieves state-of-the-art accuracy on two popular OAD benchmarks, TVSeries and THUMOS14.}
}
@article{CHAN2022108793,
title = {Online multiple object tracking using joint detection and embedding network},
journal = {Pattern Recognition},
volume = {130},
pages = {108793},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108793},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002746},
author = {Sixian Chan and Yangwei Jia and Xiaolong Zhou and Cong Bai and Shengyong Chen and Xiaoqin Zhang},
keywords = {One-shot MOT, Joint detection and tracking, YOLO tracker},
abstract = {Multiple object tracking (MOT) generally employs the paradigm of tracking-by-detection, where object detection and object tracking are executed conventionally using separate systems. Current progress in MOT has focused on detecting and tracking objects by harnessing the representational power of deep learning. Since existing methods always combine two submodules in the same network, it is particularly important that they must be trained effectively together. Therefore, the development of a suitable network architecture for the end-to-end joint training of detection and tracking submodules remains a challenging issue. The present work addresses this issue by proposing a novel architecture denoted as YOLOTracker that performs online MOT by exploiting a joint detection and embedding network. First, an efficient and powerful joint detection and tracking model is constructed to accomplish instance-level embedded training, which can ensure that the proposed tracker achieves highly accurate MOT results with high efficiency. Then, the Path Aggregation Network is employed to combine low-resolution and high-resolution features for integrating textural features and semantic information and mitigating the misalignment of the re-identification features. Experiments are conducted on three challenging and publicly available benchmark datasets and results demonstrate the proposed tracker outperforms other state-of-the-art MOT trackers in terms of accuracy and efficiency.}
}
@article{OZER2022108712,
title = {SiameseFuse: A computationally efficient and a not-so-deep network to fuse visible and infrared images},
journal = {Pattern Recognition},
volume = {129},
pages = {108712},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108712},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001935},
author = {Sedat Özer and Mert Ege and Mehmet Akif Özkanoglu},
keywords = {Multi-temporal fusion, Efficient learning, Multi-modal fusion},
abstract = {Recent developments in pattern analysis have motivated many researchers to focus on developing deep learning based solutions in various image processing applications. Fusing multi-modal images has been one such application area where the interest is combining different information coming from different modalities in a more visually meaningful and informative way. For that purpose, it is important to first extract salient features from each modality and then fuse them as efficiently and informatively as possible. Recent literature on fusing multi-modal images reports multiple deep solutions that combine both visible (RGB) and infra-red (IR) images. In this paper, we study the performance of various deep solutions available in the literature while seeking an answer to the question: “Do we really need deeper networks to fuse multi-modal images?” To have an answer for that question, we introduce a novel architecture based on Siamese networks to fuse RGB (visible) images with infrared (IR) images and report the state-of-the-art results. We present an extensive analysis on increasing the layer numbers in the architecture with the above-mentioned question in mind to see if using deeper networks (or adding additional layers) adds significant performance in our proposed solution. We report the state-of-the-art results on visually fusing given visible and IR image pairs in multiple performance metrics, while requiring the least number of trainable parameters. Our experimental results suggest that shallow networks (as in our proposed solutions in this paper) can fuse both visible and IR images as well as the deep networks that were previously proposed in the literature (we were able to reduce the total number of trainable parameters up to 96.5%, compare 2,625 trainable parameters to the 74,193 trainable parameters).}
}
@article{WANG2022108809,
title = {Directly solving normalized cut for multi-view data},
journal = {Pattern Recognition},
volume = {130},
pages = {108809},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108809},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002904},
author = {Chen Wang and Xiaojun Chen and Feiping Nie and Joshua Zhexue Huang},
keywords = {Clustering, Graph cut, Multi-view},
abstract = {Graph-based multi-view clustering, which aims to uncover clusters from multi-view data with graph clustering technique, is one of the most important multi-view clustering methods. Such methods usually perform eigen-decomposition first to solve the relaxed problem and then obtain the final cluster indicator matrix from eigenvectors by k-means or spectral rotation. However, such a two-step process may result in undesired clustering result since the two steps aim to solve different problems. In this paper, we propose a k-way normalized cut method for multi-view data, named as the Multi-view Discrete Normalized Cut (MDNC). The new method learns a set of implicit weights for each view to identify its quality, and a novel iterative algorithm is proposed to directly solve the new model without relaxation and post-processing. Moreover, we propose a new method to adjust the distribution of the implicit view weights to obtain better clustering result. Extensive experimental results show that the performance of our approach is superior to the state-of-the-art methods.}
}