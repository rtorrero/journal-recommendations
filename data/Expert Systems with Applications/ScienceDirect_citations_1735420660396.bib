@article{TANG2025111262,
title = {A trigger-perceivable backdoor attack framework driven by image steganography},
journal = {Pattern Recognition},
volume = {161},
pages = {111262},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111262},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010136},
author = {Weixuan Tang and Jiahao Li and Yuan Rao and Zhili Zhou and Fei Peng},
keywords = {Backdoor attack, Image steganography, Data transformation, Perceivability, Robustness},
abstract = {As deep network models have developed rapidly in the past decades, studying their vulnerability with backdoor attacks is essential towards building robust models in practical scenarios. To launch effective backdoor attack, the core is enabling the victim model to perceive the trigger for making wrong predictions. However, in the existing methods, the attackers inject trigger without considering whether such trigger can be perceived by the victim model. Therefore, the potential of backdoor attacks may not be fully exploited. To address the issues, this paper proposes a backdoor attack framework called Steganography-Driven Backdoor Attack (SDriBA). Its key is to formulate trigger injection and perception under backdoor attack into message hiding and extraction under steganography. Specifically, an invertible neural network (INN) equipped with a transformation layer is applied. The forward calculation of INN takes in the clean image and trigger image and then generates the poisoned image, while the reverse calculation receives the poisoned image and the randomly sampled latent noise and outputs the recovered clean image and recovered trigger image. Owing to the optimization paradigm of steganography and the reversibility of INN, the recovered trigger and the original trigger is of high similarity. In such a manner, trigger can be injected in a model-perceivable manner. Experimental results show that the proposed SDriBA can achieve satisfying attack performance under data transformations and against mainstream backdoor defense methods.}
}
@article{NIU2025111203,
title = {SEMACOL: Semantic-enhanced multi-scale approach for text-guided grayscale image colorization},
journal = {Pattern Recognition},
volume = {160},
pages = {111203},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111203},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009543},
author = {Chaochao Niu and Ming Tao and Bing-Kun Bao},
keywords = {Text-guided image colorization, Cross-modal semantic enhancement, Multi-scale features},
abstract = {High-quality colorization of grayscale images using text descriptions presents a significant challenge, especially in accurately coloring small objects. The existing methods have two major flaws. First, text descriptions typically omit size information of objects, resulting in text features that often lack semantic information reflecting object sizes. Second, these methods identify coloring areas by relying solely on low-resolution visual features from the Unet encoder and fail to leverage the fine-grained information provided by high-resolution visual features effectively. To address these issues, we introduce the Semantic-Enhanced Multi-scale Approach for Text-Guided Grayscale Image Colorization (SEMACOL). We first introduce a Cross-Modal Text Augmentation module that incorporates grayscale images into text features, which enables accurate perception of object sizes in text descriptions. Subsequently, we propose a Multi-scale Content Location module, which utilizes multi-scale features to precisely identify coloring areas within grayscale images. Meanwhile, we incorporate a Text-Influenced Colorization Adjustment module to effectively adjust colorization based on text descriptions. Finally, we implement a Dynamic Feature Fusion Strategy, which dynamically refines outputs from both the Multi-scale Content Location and Text-Influenced Colorization Adjustment modules, ensuring a coherent colorization process. SEMACOL demonstrates remarkable performance improvements over existing state-of-the-art methods on public datasets. Specifically, SEMACOL achieves a PSNR of 25.695, SSIM of 0.92240, LPIPS of 0.156, and FID of 17.54, surpassing the previous best results (PSNR: 25.511, SSIM: 0.92104, LPIPS: 0.157, FID: 26.93). The code will be available at https://github.com/ChchNiu/SEMACOL.}
}
@article{LUO2025111286,
title = {Joint radical embedding and detection for zero-shot Chinese character recognition},
journal = {Pattern Recognition},
volume = {161},
pages = {111286},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111286},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010379},
author = {Guo-Feng Luo and Da-Han Wang and Xu-Yao Zhang and Zi-Hao Lin and Shunzhi Zhu},
keywords = {Zero-shot learning, Chinese character recognition, Joint radical embedding and detection},
abstract = {Radical-based Zero-shot Chinese character recognition (ZSCCR) has attracted much attentions in recent years as radicals are common mid-level primitives that can bridge seen and unseen classes. However, while existing radical detection and sequence matching based methods have the difficulty of training a high-performance radical detection model due to the lacking of positional information of radicals in the character images, the attribute embedding based methods directly drop the radical detection procedure and overlooked the rich structural and spatial information of radicals in the visual perspective. In this paper, we proposed a novel joint radical embedding and detection method (JRED) for ZSCCR, which integrates the radical detection into the radical-level attribute embedding to explore and utilize the radicals’ structural and spatial information for ZSCCR. Specifically, we maintain a learnable embedding vector for representing each radical and learn their representations without the positional information of radicals. The learned radical representations are regarded as the radical detectors to obtain the radical-level attributes by sliding the detector on the visual feature maps. The attributes are then used for the character representation by attribute embedding. Comprehensive experiments on various datasets show that the proposed JRED method can achieve the state-of-the-art performance, demonstrating the effectiveness of our proposed approach.}
}
@article{WANG2025111227,
title = {Local and global feature attention fusion network for face recognition},
journal = {Pattern Recognition},
volume = {161},
pages = {111227},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111227},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009786},
author = {Yu Wang and Wei Wei},
keywords = {Low-quality face recognition, Feature fusion},
abstract = {Recognition of low-quality face images remains a challenge due to invisible or deformation in partial facial regions. For low-quality images dominated by missing partial facial regions, local region similarity contributes more to face recognition (FR). Conversely, in cases dominated by local face deformation, excessive attention to local regions may lead to misjudgments, while global features exhibit better robustness. However, most of the existing FR methods neglect the bias in feature quality of low-quality images introduced by different factors. To address this issue, we propose a Local and Global Feature Attention Fusion (LGAF) network based on feature quality. The network adaptively allocates attention between local and global features according to feature quality and obtains more discriminative and high-quality face features through local and global information complementarity. In addition, to effectively obtain fine-grained information at various scales and increase the separability of facial features in high-dimensional space, we introduce a Multi-Head Multi-Scale Local Feature Extraction (MHMS) module. Experimental results demonstrate that the LGAF achieves the best average performance on 4 validation sets (CFP-FP, CPLFW, AgeDB, and CALFW), and the performance on TinyFace and SCFace outperforms the state-of-the-art methods (SoTA).}
}
@article{PENG2025111219,
title = {RSANet: Relative-sequence quality assessment network for gait recognition in the wild},
journal = {Pattern Recognition},
volume = {161},
pages = {111219},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111219},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009701},
author = {Guozhen Peng and Yunhong Wang and Shaoxiong Zhang and Rui Li and Yuwei Zhao and Annan Li},
keywords = {Gait recognition, Wild scenarios, Quality assessment, Relative quality},
abstract = {Gait recognition in the wild has received increasing attention since the gait pattern is hard to disguise and can be captured in a long distance. However, due to occlusions and segmentation errors, low-quality silhouettes are common and inevitable. To mitigate this low-quality problem, some prior arts propose absolute-single quality assessment models. Although these methods obtain a good performance, they only focus on the silhouette quality of a single frame, lacking consideration of the variation state of the entire sequence. In this paper, we propose a Relative-Sequence Quality Assessment Network, named RSANet. It uses the Average Feature Similarity Module (AFSM) to evaluate silhouette quality by calculating the similarity between one silhouette and all other silhouettes in the same silhouette sequence. The silhouette quality is based on the sequence, reflecting a relative quality. Furthermore, RSANet uses Multi-Temporal-Receptive-Field Residual Blocks (MTB) to extend temporal receptive fields without parameter increases. It achieves a Rank-1 accuracy of 75.2% on Gait3D, 81.8% on GREW, and 77.6% on BUAA-Duke-Gait datasets respectively. The code is available at https://github.com/PGZ-Sleepy/RSANet.}
}
@article{LEJEUNE2025111243,
title = {A comparative attention framework for better few-shot object detection on aerial images},
journal = {Pattern Recognition},
volume = {161},
pages = {111243},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111243},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009944},
author = {Pierre {Le Jeune} and Bissmella Bahaduri and Anissa Mokraoui},
keywords = {Few-shot learning, Object detection, Aerial image, Few-shot object detection, Attention mechanisms},
abstract = {Few-Shot Object Detection (FSOD) methods are mainly designed and evaluated on natural image datasets such as Pascal VOC and MS COCO. However, it is not clear whether the best methods for natural images are also the best for aerial images. Furthermore, a direct comparison of performance between FSOD methods is difficult due to the wide variety of detection frameworks and training strategies. To this end, our contributions are twofold. First, we propose a benchmarking framework that provides a flexible environment to implement and compare attention-based FSOD methods. The proposed framework focuses on attention mechanisms and is divided into three modules: spatial alignment, global attention, and fusion layer. To remain competitive with existing methods, which often leverage complex training, we propose new augmentation techniques designed specifically for object detection. Using this framework, several FSOD methods are reimplemented and compared. This comparison highlights two distinct performance regimes on aerial and natural images: FSOD performs worse on aerial images. Our experiments confirm that small objects account for the poor performance. Small objects are difficult to detect, however in the few-shot regime, this challenge is largely reinforced. While the small object detection issue is well-known, to our knowledge this few-shot complication has never been reported in the literature. Second, always within the proposed framework, we develop a novel alignment method called Cross-Scales Query-Support Alignment (XQSA) for FSOD, to improve the detection of small objects. XQSA significantly outperforms the state-of-the-art on DOTA and DIOR, two aerial image datasets.}
}
@article{IMBERT2025111231,
title = {Mixture-of-experts for handwriting trajectory reconstruction from IMU sensors},
journal = {Pattern Recognition},
volume = {161},
pages = {111231},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111231},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009828},
author = {Florent Imbert and Eric Anquetil and Yann Soullard and Romain Tavenard},
keywords = {Online handwriting, Trajectory reconstruction, Digital pen, Inertial Measurement Units, Deep neural network},
abstract = {The use of digital pens for online handwriting trajectory reconstruction is a prevalent method for human–computer interaction. In this study, we focus on a digital pen equipped with sensors where we aim at reconstructing the online handwriting trajectory. This pen enables writing on any surface and preserving the digital trace of handwriting. This type of pen could be used as an aid to learning to write in classroom. In this paper, we propose a new approach learning to finely reconstruct the touching trajectories while precisely analyzing the hovering part in order to position the next touching trace correctly. This relies on a Mixture-Of-Experts (MOE) approach. The first expert is dedicated for the pencil touch, and is named touching expert model. The second one is dedicated for the hovering pen trajectory, and is named hovering expert model. We improve on the learning of each of these experts based on additional context or specific examples. In addition we introduce a novel public benchmark dataset, to enable future research and comparisons in the field of handwriting reconstruction. The results demonstrates a significant enhancement compared to its primary competitors.}
}
@article{LIU2025111298,
title = {MFECNet: Multi-level feature enhancement and correspondence network for few-shot anomaly detection of high-speed trains},
journal = {Pattern Recognition},
volume = {161},
pages = {111298},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111298},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010495},
author = {Wei Liu and Xiaobo Lu and Yun Wei and Zhidan Ran},
keywords = {Anomaly detection, Few-shot learning, MFECNet},
abstract = {Anomaly detection is pivotal to ensuring the normal operation of high-speed trains, yet prevailing deep learning methods typically necessitate substantial data, a luxury not afforded by the scarcity of high-speed railway anomaly data. Therefore, in this paper, we propose a multi-level feature enhancement and correspondence network (MFECNet) based on few-shot learning to diagnose anomalies in high-speed trains. We can detect new anomalies based on a scant number of annotated samples (1-shot, 5-shot) by this method. At its core, the feature similarity computation (FSC) module is proposed to capture the similarities of multi-level features based on support features and query features. The mutual attention feature enhancement (MAFE) module is proposed to reinforce features of different levels based on context aggregation and attention mechanism. The multifarious feature fusion (MFF) module is proposed to facilitate the fusion of multi-level similarities and the integration between the similarities and the query features. Compared to the cutting-edge approaches, our method based on VGG16 and ResNet50 backbone improves by 8.51% and 2.06% in mean intersection over union (IoU), respectively. In addition, we conduct experiments on a public anomaly detection dataset (MVTec dataset) and the proposed network outperforms the state-of-the-art methods by a large margin, which fully demonstrates the superiority and powerful generalization of our method.}
}
@article{HUANG2025111274,
title = {kMaXU: Medical image segmentation U-Net with k-means Mask Transformer and contrastive cluster assignment},
journal = {Pattern Recognition},
volume = {161},
pages = {111274},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111274},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010252},
author = {Chengying Huang and Zhengda Wu and Heran Xi and Jinghua Zhu},
keywords = {U-shaped network, Convolutional neural network, Mask Transformer, Medical image segmentation, Contrastive learning, Cluster assignments},
abstract = {Though U-Net excels in medical image segmentation for disease diagnosis and treatment planning, it cannot model long-range dependencies. Transformer has emerged as an alternative segmentation architecture due to its ability to capture long-range relations. However, the existing CNN and Transformer network segmentation performance deteriorates significantly when applied to class-imbalanced medical data. Furthermore, their simple cascaded up-sampling decoder does not fully exploit Transformer potential. To address these issues, we propose a k-means Mask Transformer enhanced U-Net network (kMaXU) for medical image segmentation. Our innovation lies in three designs: (1) Hybrid encoder and dual-branch decoder architecture, which takes advantage of both the CNN and Transformer; (2) Multi-scale kMaX blocks, which leverages k-Means Mask Transformer to formulate class feature centers for class-imbalance segmentation; and (3) Cross contrastive cluster assignment, which improves segmentation performance through consistency cluster assignment between feature-enhanced views. Extensive experiments on Synapse, ACDC, and ISIC2018 datasets demonstrate that the proposed method achieves consistent performance improvements over state-of-the-art methods in medical image segmentation, with an average improvements of 6.83%, 2.79% and 0.87% in terms of DSC, respectively. Code is available at https://github.com/ChengyingHuang/kMaXU.}
}
@article{WANG2025111289,
title = {Semantics-guided multi-task genetic programming for multi-output regression},
journal = {Pattern Recognition},
volume = {161},
pages = {111289},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111289},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010409},
author = {Chunyu Wang and Qi Chen and Bing Xue and Mengjie Zhang},
keywords = {Multi-output regression, Genetic programming, Evolutionary multi-task optimization},
abstract = {Multi-output regression entails the simultaneous prediction of two or more output variables, presenting greater complexities than single-output regression due to the frequent interdependent relationships of these variables. Such dependencies mean that accurately predicting one variable typically requires careful analysis of its relationships with others. In this paper, multi-output regression problems are treated as multi-task problems, with a prediction of one output variable as a distinct task. A new multi-task multi-population genetic programming method is proposed to solve the problem. The method incorporates a semantics based crossover operator to identify the most informative subtree from a similar task that facilitates positive knowledge transfer. Empirical results indicate that our method significantly improves the training and testing performances of other multi-task GP methods, surpassing standard GP and GP with regressor chain on most examined regression datasets. Further analysis reveals that our proposed method can generate high-quality solutions by knowledge transfer and efficiently evolves similar GP models for analogous output variables, significantly enhancing positive knowledge transfer.}
}
@article{XIAO2025111187,
title = {SC-VAE: Sparse coding-based variational autoencoder with learned ISTA},
journal = {Pattern Recognition},
volume = {161},
pages = {111187},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111187},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009385},
author = {Pan Xiao and Peijie Qiu and Sung Min Ha and Abdalla Bani and Shuang Zhou and Aristeidis Sotiras},
keywords = {Sparse coding, Variational autoencoder, Learned iterative shrinkage thresholding algorithm},
abstract = {Learning rich data representations from unlabeled data is a key challenge towards applying deep learning algorithms in downstream tasks. Several variants of variational autoencoders (VAEs) have been proposed to learn compact data representations by encoding high-dimensional data in a lower dimensional space. Two main classes of VAEs methods may be distinguished depending on the characteristics of the meta-priors that are enforced in the representation learning step. The first class of methods derives a continuous encoding by assuming a static prior distribution in the latent space. The second class of methods learns instead a discrete latent representation using vector quantization (VQ) along with a codebook. However, both classes of methods suffer from certain challenges, which may lead to suboptimal image reconstruction results. The first class suffers from posterior collapse, whereas the second class suffers from codebook collapse. To address these challenges, we introduce a new VAE variant, termed sparse coding-based VAE with learned ISTA (SC-VAE), which integrates sparse coding within variational autoencoder framework. The proposed method learns sparse data representations that consist of a linear combination of a small number of predetermined orthogonal atoms. The sparse coding problem is solved using a learnable version of the iterative shrinkage thresholding algorithm (ISTA). Experiments on two image datasets demonstrate that our model achieves improved image reconstruction results compared to state-of-the-art methods. Moreover, we demonstrate that the use of learned sparse code vectors allows us to perform downstream tasks like image generation and unsupervised image segmentation through clustering image patches. The code is available at https://github.com/sotiraslab/SC-VAE.}
}
@article{SONG2025111198,
title = {Frequency domain-based latent diffusion model for underwater image enhancement},
journal = {Pattern Recognition},
volume = {160},
pages = {111198},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111198},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400949X},
author = {Jingyu Song and Haiyong Xu and Gangyi Jiang and Mei Yu and Yeyao Chen and Ting Luo and Yang Song},
keywords = {Frequency domain, Latent diffusion model, Underwater image enhancement},
abstract = {The degradation of underwater images, due to complex factors, negatively impacts the performance of underwater visual tasks. However, most underwater image enhancement methods (UIE) have been confined to the spatial domain, disregarding the frequency domain. This limitation hampers the full exploitation of the model’s learning and representational capabilities. To address this, a two-stage frequency domain-based latent diffusion model (FD-LDM) is introduced for UIE. Firstly, the model employs a lightweight parameter estimation network (L-PEN) to estimate the degradation parameters of underwater images, thereby mitigating the impact of color bias on the diffusion model. Subsequently, considering the varying degrees of recovery between high and low-frequency images, high and low-frequency priors are extracted in the second stage and integrated with the refined latent diffusion model to enhance the images further. Extensive experiments have confirmed the method’s effectiveness and robustness, particularly under color bias scenes.}
}
@article{ZHU2025111285,
title = {Self-supervised noise2noise method utilizing corrupted images with a modular network for LDCT denoising},
journal = {Pattern Recognition},
volume = {161},
pages = {111285},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111285},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010367},
author = {Yuting Zhu and Qiang He and Yudong Yao and Yueyang Teng},
keywords = {LDCT denoising, Modular network, Noisy-as-clean strategy, Parameter sharing, Self-supervised learning},
abstract = {Deep learning is a very promising technique for low-dose computed tomography (LDCT) image denoising. However, traditional deep learning methods require paired noisy and clean datasets, which are often difficult to obtain. This paper proposes a new method for performing LDCT image denoising with only LDCT data, which eliminates the need for normal-dose CT (NDCT). We adopt a combination of the self-supervised noise2noise model and the noisy-as-clean strategy. First, we add a second yet similar type of noise to the LDCT images multiple times. Note that we use LDCT images instead of NDCT images based on the noisy-as-clean strategy for corruption. Then, the noise2noise model is executed using only the secondary corrupted images for training. We select a modular U-Net structure from several candidates with shared parameters to perform the task, which increases the receptive field without increasing the parameter size. The experimental results obtained on the Mayo LDCT dataset show the effectiveness of the proposed method compared with that of the state-of-the-art deep learning methods. The developed code is available at https://github.com/XYuan01/Self-supervised-Noise2Noise-for-LDCT.}
}
@article{PEI2025111226,
title = {Adaptive Graph K-Means},
journal = {Pattern Recognition},
volume = {161},
pages = {111226},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111226},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009774},
author = {Shenfei Pei and Yuanchen Sun and Feiping Nie and Xudong Jiang and Zengwei Zheng},
keywords = {Machine learning, Clustering, Graph-based, -means, Computational efficiency},
abstract = {Clustering large-scale datasets has received increasing attention recently. However, existing algorithms are still not efficient in scenarios with extremely large number of clusters. To this end, Adaptive Graph K-Means (AGKM) is proposed in this work. Its idea originates from k-means, but it operates on an adaptive k-Nearest Neighbor (k-NN) graph instead of data features. First, AGKM is highly efficient for processing datasets where both the numbers of samples and clusters are very large. Specifically, the time and space complexity are both linear w.r.t the number of samples and, more importantly, independent to the cluster number. Second, AGKM is designed for balanced clusters. This constraint is realized by adding a regularization term in loss function, and a simple modification of the graph in optimization algorithm, which does not increase the computational burden. Last, the indicator and dissimilarity matrices are learned simultaneously, so that the proposed AGKM obtains the final partition directly with higher efficacy and efficiency. Experiments on several datasets validate the advantages of AGKM. In particular, over 29X and 46X speed-ups with respect to k-means are observed on the two large-scale datasets WebFace and CelebA, respectively.}
}
@article{JIAO2025111261,
title = {Deep evidential clustering based on feature representation learning and belief function theory},
journal = {Pattern Recognition},
volume = {161},
pages = {111261},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111261},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010124},
author = {Lianmeng Jiao and Feng Wang and Xiaojiao Geng and Zhun-ga Liu and Feng Yang and Quan Pan},
keywords = {Evidential clustering, Deep clustering, Representation learning, Belief function theory},
abstract = {Evidential clustering is a promising clustering framework based on belief function theory which generalizes hard, fuzzy, possibilistic, and rough clustering. It allows us to gain deeper insight into the structure of the data. However, traditional evidential clustering algorithms cannot deal well with high-dimensional raw data such as text and images by the way of clustering on the original data representations directly. To address this problem, a deep evidential clustering (DEC) algorithm based on feature representation learning and belief function theory is proposed in this study. This algorithm incorporates the evidential clustering loss, the autoencoder reconstruction loss, and the regularization loss of network parameters to construct a combined optimization model. DEC performs deep feature extraction and evidential clustering simultaneously to ensure that the learned deep features are the evidential clustering-friendly representations of the raw data. In addition, DEC introduces evidential partition so that an object can belong to either a single class or any subset of a collection of classes, which provides more powerful expressive ability for uncertain data. Extensive experiments were conducted on real high-dimensional datasets, and the experimental results illustrated the superiority of DEC compared to the state-of-the-art evidential clustering and deep clustering algorithms.}
}
@article{YANG2025111301,
title = {Bidirectional modality information interaction for Visible–Infrared Person Re-identification},
journal = {Pattern Recognition},
volume = {161},
pages = {111301},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111301},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010525},
author = {Xi Yang and Huanling Liu and Nannan Wang and Xinbo Gao},
keywords = {Dynamic aggregation, Feature intersection, Multi-weight loss},
abstract = {Due to the distinct imaging principles of visible and infrared images, significant challenges arise for visible–infrared person re-identification (VI-ReID). Current methods either establish intermediate modalities to reduce image differences or use various techniques to extract better shared features. However, despite the modality differences in VI-ReID, significant similarities in contours and postures exist, so focusing on learning feature representation can fundamentally solve the problem. To this end, this paper proposes a bidirectional modality information interaction network to reduce the difficulty of inter-modality recognition and enhance intra-modality feature extraction. To fully exploit cross-modality information, the Dynamic Aggregation (DA) module iteratively projects features of both modalities onto each other, embedding image attributes from both, enabling mutual learning, improving feature representation, and reducing modality differences. To enhance intra-modality feature representation, the Feature Intersection (FI) module combines two pooling styles in a novel way, ensuring global connection of local features and regional highlights. Lastly, we propose a Multi-Weight (MW) loss to increase the center distance of same identity features within the same modality, helping the model better learn identity attributes. Extensive experiments on SYSU-MM01 and RegDB confirm the superiority of our approach.}
}
@article{YE2025111242,
title = {Stain-adaptive self-supervised learning for histopathology image analysis},
journal = {Pattern Recognition},
volume = {161},
pages = {111242},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111242},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009932},
author = {Haili Ye and Yuan-yuan Yang and Shunzhi Zhu and Da-Han Wang and Xu-Yao Zhang and Xin Yang and Heguang Huang},
keywords = {Histopathology image analysis, Domain adaptation, Stain adaptation, Self-supervised learning, Domain adversarial training},
abstract = {Staining variability is a critical factor affecting the accuracy of histopathological image analysis by reducing the distinguishability of tissue regions. Existing methods employ preprocessing techniques such as color matching and stain transfer for stain normalization, which can compromise data features. We propose a novel Stain-Adaptive Self-Supervised Learning (SASSL) method for histopathological image analysis. Our SASSL integrates a stain domain adversarial training module into the self-supervised learning (SSL) framework, allowing adaptation to staining variations while learning invariant features. SASSL can be viewed as a general invariant representation SSL method, with derived self-supervised weights applicable to various downstream tasks (classification, regression, and segmentation) in histopathological images. We conducted experiments on publicly available histopathological image analysis datasets, including PANDA, BreastPathQ, and CAMELYON16, achieving state-of-the-art performance. Results demonstrate that SASSL enhances feature extraction and mitigates the impact of staining variability, consistently improving performance across tasks. Our code is available at https://github.com/YeahHighly/SASSL_PR_2024.}
}
@article{LIU2025111249,
title = {Human emotion and StO2: Dataset, pattern, and recognition of basic emotions},
journal = {Pattern Recognition},
volume = {161},
pages = {111249},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111249},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010008},
author = {Xinyu Liu and Tong Chen and Ju Zhou and Hanpu Wang and Guangyuan Liu and Xiaolan Fu},
keywords = {Tissue oxygen saturation, Emotion recognition, Emotion pattern, Emotion dataset, Action unit, Handcrafted features, Transformer},
abstract = {Emotions can be recognized through facial expressions, but this becomes challenging when facial expressions are deliberately manipulated. This study introduces tissue oxygen saturation (StO2), a non-contact measured physiological signal of hemoglobin binding to oxygen, as a novel and expression-independent indicator for emotion recognition. Herein, the first StO2-emotion dataset is developed, which encompasses six basic emotions collected from 145 participants via emotional stimulus videos. Pattern analyses indicate that StO2 exhibits distinctive patterns under various emotions, whereas facial expressions have no impact on StO2. Based on this finding, we design the intensity difference pattern-masked feature (IMF) to amplify the changes in StO2 and construct the region of interest embedded Transformer network (ROI-EmbedFormer) to capture the synergistic changes across different facial regions. Finally, an accuracy of 80.31% is achieved for StO2-based emotion recognition. This work shows that StO2 is a promising signal that can identify genuine emotions even without facial expressions.}
}
@article{TAN2025111325,
title = {Clinical-inspired skin lesions recognition based on deep hair removal with multi-level feature fusion},
journal = {Pattern Recognition},
volume = {161},
pages = {111325},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111325},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010768},
author = {Ling Tan and Hui Wu and Jining Zhu and Ying Liang and Jingming Xia},
keywords = {Dermoscopy image, Deep hair removal, Multi-level feature fusion, Global-local discriminator, Clinical-inspired recognition},
abstract = {Hair in dermoscopy images often obscures critical lesion details, reducing the accuracy of automated diagnostic systems. Hair removal techniques are used to eliminate hair, restore hidden lesion information, and enhance diagnostic tasks. However, traditional methods may either inadequately or excessively remove hair, leading to the loss of key lesion features. Additionally, the black-box nature of deep learning models often lacks interpretability, making it challenging to provide clinically relevant diagnostic evidence. To address these challenges, we propose a Clinical-Inspired Recognition Network for Skin Lesions Based on Deep Hair Removal with Multi-level Feature Fusion (MFF-CINet). In the core hair removal module of MFF-CINet, we design a Multi-level Feature Fusion (MFF) submodule for hair segmentation, which combines deep and shallow features through skip connections and upsampling, expanding the receptive field and refining hair feature extraction. To ensure the realism of hair-removed images after inpainting, we propose a Global-Local Discriminator (GLD) that imposes dual constraints on global and local features. The global discriminator assesses overall image coherence, while the local discriminator ensures consistency in lesion detail features. Additionally, inspired by clinical practices, we integrate a physician-inspired diagnostic strategy to guide the automated recognition of skin lesions, enhancing both interpretability and diagnostic accuracy. Based on the ISIC public dataset, we generate paired hair-free and hair-containing images to train the network. Comparative evaluations show that our approach outperforms mainstream methods in hair removal, significantly improves recognition accuracy, and enhances model interpretability.}
}
@article{JIA2025111214,
title = {Enhancing out-of-distribution detection via diversified multi-prototype contrastive learning},
journal = {Pattern Recognition},
volume = {161},
pages = {111214},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111214},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009658},
author = {Yulong Jia and Jiaming Li and Ganlong Zhao and Shuangyin Liu and Weijun Sun and Liang Lin and Guanbin Li},
keywords = {Out-of-distribution detection, Robust AI, Contrastive learning},
abstract = {Detecting out-of-distribution (OOD) inputs is critical for safely deploying deep neural networks in the open world. Recent distance-based contrastive learning methods demonstrated their effectiveness by learning improved feature representations in the embedding space. However, those methods might lead to an incomplete and ambiguous representation of a class, thereby resulting in the loss of intra-class semantic information. In this work, we propose a novel diversified multi-prototype contrastive learning framework, which preserves the semantic knowledge within each class’s embedding space by introducing multiple fine-grained prototypes for each class. This preserves intrinsic features within the in-distribution data, promoting discrimination against OOD samples. We also devise an activation constraints technique to mitigate the impact of extreme activation values on other dimensions and facilitate the computation of distance-based scores. Extensive experiments on several benchmarks show that our proposed method is effective and beneficial for OOD detection, outperforming previous state-of-the-art methods.}
}
@article{SHENG2025111230,
title = {Exploring multi-scale forgery clues for stereo super-resolution image forgery localization},
journal = {Pattern Recognition},
volume = {161},
pages = {111230},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111230},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009816},
author = {Ziqi Sheng and Chengxi Yin and Wei Lu},
keywords = {Image forgery localization, Stereo super-resolution image forensics, Reconstruction artifacts, Bidirectional feature fusion},
abstract = {Existing forgery image localization methods have achieved impressive performance on monocular images but struggle to maintain comparable performance on stereo super-resolution (SR) images. The stereo image SR process usually introduces additional noise, i.e., reconstruction artifacts, which interferes with the extraction of forgery clues and thus undermines the localization performance. To mitigate this issue, we present a multi-scale attention framework named SSR-IFL for stereo SR image forgery localization, which explores multi-scale forgery clues in stereo SR images to minimize disturbances caused by reconstruction artifacts. Specifically, the novel framework integrates two significant components: the multi-scale feature extraction (MSFE) network to thoroughly mine forgery clues against the inference of reconstruction artifacts, and the bidirectional progressive feature fusion (BPFF) network to effectively reason the learned forgery clues and obtain robust forgery feature representation. Under the MSFE network, the multi-dilated self-attention (MDSA) block makes full use of multi-resolution pixel correlation to adapt to changes in pixel correlation distributions induced by the stereo SR reconstruction process, thus extracting more precise multi-scale forgery clues. The BPFF network introduces a bidirectional feature fusion strategy to fuse multi-scale forgery clues, effectively eliminating redundant noise such as reconstruction artifacts and enhancing the robustness of forgery feature representation. Experimental results on public image datasets demonstrate that our method can achieve not only competitive performance on stereo SR images but also on monocular images.}
}
@article{KHAN2025111273,
title = {Optimized cross-module attention network and medium-scale dataset for effective fire detection},
journal = {Pattern Recognition},
volume = {161},
pages = {111273},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111273},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010240},
author = {Zulfiqar Ahmad Khan and Fath U Min Ullah and Hikmat Yar and Waseem Ullah and Noman Khan and Min Je Kim and Sung Wook Baik},
keywords = {Fire detection, Channel attention, Multi-scale feature selection, Image classification and detection},
abstract = {Over a decade, computer vision has shown a keen interest toward vision-based fire detection due to its wide range of applications. Primarily, fire detection relies on color features that inspired recent deep models to achieve reasonable performance. However, a perfect balance between high fire detection rate and computational complexity over mainstream surveillance setups is a challenging task. To establish a better tradeoff between model complexity and fire detection rate, this article develops an efficient and effective Cross Module Attention Network (CANet) for fire detection. CANet is developed from scratch with a squeezing and expansive paths to focus on the fire regions and its location. Next, the channel attention and Multi-Scale Feature Selection (MSFS) modules are integrated to accomplish the most important channels, selectively emphasize the contributions of feature maps, and enhance the discrimination potential of fire and non-fire objects. Furthermore, the CANet is optimized by removing a significant number of parameters for real-world applications. Finally, we introduce a challenging database for fire classification comprised of multiple classes and highly similar fire and non-fire object images. CANet improved accuracy by 2.5 % for the BWF, 2.2 % for the DQFF, 1.42 % for the LSFD, 1.8 % for the DSFD, and 1.14 % for the FG, Additionally, CANet achieved a 3.6 times higher FPS on resource-constrained devices compared to baseline methods.}
}
@article{LEE2025111297,
title = {Extrinsic kernel ridge regression classifier for Kendall’s planar shape space},
journal = {Pattern Recognition},
volume = {161},
pages = {111297},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111297},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010483},
author = {Hwiyoung Lee and Vic Patrangenaru},
keywords = {Kendall’s planar shape space, Riemannian manifold, Kernel ridge regression classifier, Kernel method, Extrinsic data analysis},
abstract = {Kernel methods have had great success in Statistics and Machine Learning. Despite their growing popularity, however, less effort has been drawn towards developing kernel-based classification methods on Riemannian manifolds due to the difficulty in dealing with non-Euclidean geometry. In this work, motivated by the extrinsic framework of manifold-valued data analysis, we introduce a method for constructing kernels on Riemannian manifolds. We show that our approach guarantees the positive definiteness of the kernel across general Riemannian manifolds. We apply the extrinsic approach to the planar Kendall shape space (Σ2k). Furthermore, kernel ridge regression classifier (KRRC) is implemented to address the shape classification problem on Σ2k. Promising performances are demonstrated through both real data analysis and simulation studies.}
}
@article{DECARVALHO2025111158,
title = {A game-inspired algorithm for marginal and global clustering},
journal = {Pattern Recognition},
volume = {160},
pages = {111158},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111158},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009099},
author = {Miguel {de Carvalho} and Gabriel Martos and Andrej Svetlošák},
keywords = {Cluster analysis, Mixture models, Model-based clustering, Similarity-based clustering, Unsupervised learning},
abstract = {An often overlooked pitfall of model-based clustering is that it typically results in the same number of clusters per margin, an assumption that may not be natural in practice. We develop a clustering method that takes advantage of the sturdiness of model-based clustering, while attempting to mitigate this issue. The proposed approach allows each margin to have a varying number of clusters and employs a strategy game-inspired algorithm, named ‘Reign-and-Conquer’, to cluster the data. Since the proposed clustering approach only specifies a model for the margins, but leaves the joint unspecified, it has the advantage of being partially parallelizable; hence, the proposed approach is computationally appealing as well as more tractable for moderate to high dimensions than a ‘full’ (joint) model-based clustering approach. A battery of numerical experiments on simulated data indicates an overall good performance of the proposed methods in a variety of scenarios, and real datasets are used to showcase their usefulness in practice.}
}
@article{GAO2025111202,
title = {Robust and privacy-preserving feature extractor for perturbed images},
journal = {Pattern Recognition},
volume = {161},
pages = {111202},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111202},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009531},
author = {Pengfei Gao and Jiaohua Qin and Xuyu Xiang and Yun Tan},
keywords = {Perturbed images classification, Feature extraction from perturbed images, Privacy protection, Deep leakage from gradients attack, Attention privacy leakage attack},
abstract = {Feature extraction from perturbed images is essential for both privacy protection and pattern recognition. Traditional CNNs struggle with large-scale spatial perturbations due to their translational invariance, and while Vision Transformers handle disruptions well, their shallow layers suffer from limited global attention. We introduce JigFormer, a model with a larger average attention distance and stronger channel information mixing. JigFormer employs a Patch Wise Inception Aggregate Module (PWIA) to aggregate features at multiple scales from the single patch, and innovations like Fourier Shifted Gated Linear Units (FSGLU) and Query and Key Shortcut Connection (QKSC) to improve attention distance and feature mixing. Validated on datasets of various scales, JigFormer demonstrates leading performance. Compared to baseline model, it improves accuracy by 4.11%, 7.2%, and 7.38% on CIFAR-10, CIFAR-100, and TinyImageNet datasets, respectively, while effectively defending against classical gradients attack and attentions attack.}
}
@article{ZHAO2025111292,
title = {The Cascaded Forward algorithm for neural network training},
journal = {Pattern Recognition},
volume = {161},
pages = {111292},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111292},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010434},
author = {Gongpei Zhao and Tao Wang and Yi Jin and Congyan Lang and Yidong Li and Haibin Ling},
keywords = {Deep neural network, Backpropagation algorithm, Forward–Forward algorithm, Image classification},
abstract = {Backpropagation (BP) algorithm has played a significant role in the development of deep learning. However, there exist some limitations associated with this algorithm, such as getting stuck in local minima and experiencing vanishing/exploding gradients, which have led to questions about its biological plausibility. To address these limitations, alternative algorithms to backpropagation have been preliminarily explored, with the Forward–Forward (FF) algorithm being one of the most well-known. In this paper we propose a new learning framework for neural networks, namely Cascaded Forward (CaFo) algorithm, which does not rely on BP optimization as that in FF. Unlike FF, CaFo directly outputs label distributions at each cascaded block and waives the requirement of generating additional negative samples. Consequently, CaFo leads to a more efficient process at both training and testing stages. Moreover, in our CaFo framework each block can be trained in parallel, allowing easy deployment to parallel acceleration systems. The proposed method is evaluated on four public image classification benchmarks, and the experimental results illustrate significant improvement in prediction accuracy in comparison with recently proposed baselines. The code is available at: https://github.com/Graph-ZKY/CaFo.}
}
@article{WANG2025111233,
title = {A dual-domain deep network for high pitch CT reconstruction},
journal = {Pattern Recognition},
volume = {161},
pages = {111233},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111233},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009841},
author = {Wei Wang and Xiang-Gen Xia and Chuanjiang He},
keywords = {Helical CT, High pitch, Deep learning, Katsevich algorithm},
abstract = {In this paper, we propose an end-to-end dual-domain deep network for the high pitch helical CT reconstruction. Our network is composed of a sinogram domain subnetwork and a CT image domain subnetwork, and the two subnetworks are linked by the Katsevich algorithm layer. The sinogram subnetwork is a discrete cosine transform (DCT) 3D U-net and used to complete the missing sinograms due to high pitch while the CT image subnetwork is a standard 3D U-net used to denoise the CT images. By utilizing the periodic properties of parameters of Katsevich algorithm, our network reconstructs CT images rotation turn by rotation turn and so can reduce the GPU memory consumption. To further reduce the GPU memory consumption, we train the sinogram domain subnetwork and the CT image domain subnetwork separately, where the gradient flow through the Katsevich layer is obtained manually. Experiments show that the proposed network trained by our method outperforms the related methods both in subjective and objective evaluations.}
}
@article{SHARMA2025111205,
title = {Predicting pedestrian intentions with multimodal IntentFormer: A Co-learning approach},
journal = {Pattern Recognition},
volume = {161},
pages = {111205},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111205},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009567},
author = {Neha Sharma and Chhavi Dhiman and Sreedevi Indu},
keywords = {Pedestrians intention prediction, Co-learning, Weight sharing intent transformer, Autonomous driving},
abstract = {The prediction of pedestrian crossing intention is a crucial task in the context of autonomous driving to ensure traffic safety and reduce the risk of accidents without human intervention. Nevertheless, the complexity of pedestrian behaviour, which is influenced by numerous contextual factors in conjunction with visual appearance cues and past trajectory, poses a significant challenge. Several state-of-the-art approaches have recently emerged that incorporate multiple modalities. Nonetheless, the suboptimal modality integration techniques in these approaches fail to capture the intricate intermodal relationships and robustly represent pedestrian-environment interactions in challenging scenarios. To address these issues, a novel Multimodal IntentFormer architecture is presented. It works with three transformer encoders {TEI,TEII,TEIII} which learn RGB, segmentation maps, and trajectory paths in a co-learning environment controlled by a Co-learning module. A novel Co-learning Adaptive Composite (CAC) loss function is also proposed, which penalizes different stages of the architecture, regularizes the model, and mitigates the risk of overfitting. Each encoder {TEη} applies the concept of the Multi-Head Shared Weight Attention (MHSWA) mechanism while learning three modalities in the proposed co-learning approach. The proposed architecture outperforms existing state-of-the-art approaches on benchmark datasets, PIE and JAAD, with 93 % and 92 % accuracy, respectively. Furthermore, extensive ablation studies demonstrate the efficiency and robustness of the architecture, even under varying Time-to-event (TTE) and observation lengths. The code is available at https://github.com/neha013/IntentFormer}
}
@article{VOON2024111316,
title = {Trapezoidal Step Scheduler for Model-Agnostic Meta-Learning in Medical Imaging},
journal = {Pattern Recognition},
pages = {111316},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111316},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010677},
author = {Wingates Voon and Yan Chai Hum and Yee Kai Tee and Wun-She Yap and Khin Wee Lai and Humaira Nisar and Hamam Mokayed},
keywords = {Few-shot learning, Medical image classification, Trapezoidal step scheduler, Model-agnostic meta-learning},
abstract = {Model Agnostic Meta-learning (MAML) is a widely adopted few-shot learning (FSL) method designed to mitigate the dependency on large, labeled datasets of deep-learning-based methods in medical imaging analysis. However, MAML's reliance on a fixed number of gradient descent (GD) steps for task adaptation results in computational inefficiency and task-level overfitting. To address these issues, we introduce Tra-MAML, which optimizes the balance between model adaptation capacity and computational efficiency through a trapezoidal step scheduler (TRA). The TRA scheduler dynamically adjusts the number of GD steps in the inner optimization loop: initially increasing the steps uniformly to reduce variance, maintaining the maximum number to enhance adaptation capacity, and finally decreasing the steps uniformly to mitigate overfitting. Our evaluation of Tra-MAML against selected FSL methods across four medical imaging datasets demonstrates its superior performance. Notably, Tra-MAML outperforms MAML by 13.36% on the BreaKHis40X dataset in the 3-way 10-shot scenario.}
}
@article{BELLO2025111221,
title = {The level of strength of an explanation: A quantitative evaluation technique for post-hoc XAI methods},
journal = {Pattern Recognition},
volume = {161},
pages = {111221},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111221},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009725},
author = {Marilyn Bello and Rosalís Amador and María-Matilde García and Javier Del Ser and Pablo Mesejo and Óscar Cordón},
keywords = {Explainable artificial intelligence, Trustworthiness, Feature attribution, Quantitative evaluation, Likelihood ratio},
abstract = {Explainability has become one of the leading research topics within Artificial Intelligence (AI) in the last few years, as it has increased the confidence and credibility of “black box” models, such as deep neural networks. However, the evaluation of the explanations provided by different explainability approaches remains a hot research topic. This evaluation enables the possibility of comparing different existing techniques and would play a crucial role in improving the auditability of AI-based systems. The current literature on the subject considers that the evaluation of explainability methods can be approached in two ways: qualitative and quantitative. While qualitative evaluations are based on assumptions induced by human understanding that are hard to develop and prone to introduce certain cognitive biases, quantitative ones avoid these biases by excluding the human expert from the evaluation process. However, the main challenge in quantitatively evaluating an explanation is the lack of ground truth specifying what defines a correct explanation. In this paper, we propose an evaluation measure that quantifies the Level of Strength of an Explanation (LSE), i.e., the extent to which the explanation produced by a post-hoc explainability method supports the class predicted by a classifier. Our proposal is inspired by the semantics underlying the Likelihood Ratio in evaluating forensic evidence, which is defined as the weight to be attributed to a piece of forensic evidence according to the prosecution and defense propositions. To validate our proposal, nine popular explainability techniques are compared across two deep neural architectures dedicated to image classification and three classifiers for binary classification over tabular datasets. In addition, we use MetaQuantus as a meta-evaluation approach. Results from our experimental study reveal that GradCAM and LRP outperform the other explainability methods in terms of the proposed LSE measure.}
}
@article{XU2025111245,
title = {Deep metric learning in projected-hypersphere space},
journal = {Pattern Recognition},
volume = {161},
pages = {111245},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111245},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009968},
author = {Yunhao Xu and Zhentao Chen and Junlin Hu},
keywords = {Deep metric learning, Projected-hypersphere, Feature embedding, Image retrieval},
abstract = {Distance metric learning is a subfield of machine learning that aims to learn a discriminative space in which samples of the same class are closer and samples of different classes are farther apart. Recent studies have demonstrated that geometric spaces with the constant non-zero curvature, i.e., non-Euclidean spaces, can better capture discriminative information between data points than Euclidean space whose curvature is zero. In this paper, we concentrate on the metric learning task in the positive curvature space, and we present a novel deep projected-hypersphere metric learning (DPHM) method to exploit the intrinsic geometry of data, which maps the features extracted from the deep learning model into the projected-hypersphere space, calculates projected-hypersphere distance between embeddings, and directly optimizes these embeddings using the supervised contrastive loss function. We evaluate our approach using different backbone networks on four image benchmark datasets including CUB-200-2011, Cars-196, Stanford Online Product, and In-shop Clothes Retrieval datasets for image retrieval task. Experimental results show that our proposed DPHM approach achieves the state-of-the-art performance compared to the methods used in zero and positive curvature spaces, and is also competitive to the negative curvature space-based methods, verifying its superiority.}
}
@article{LIU2025111288,
title = {Capped norm based discriminant robust regression learning},
journal = {Pattern Recognition},
volume = {161},
pages = {111288},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111288},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010392},
author = {Ning Liu and Zhihui Lai and Junhong Zhang and Can Gao and Heng Kong},
keywords = {Capped norm, Discriminant robust regression, Joint sparse subspace learning},
abstract = {Exploring underlying correlation structures among data, improving robustness to noise, obtaining discriminant projections, integrating local information of data, and selecting significant features are important objectives in feature extraction methods. To address these objectives, a novel capped norm based discriminant robust regression (CNDRR) method is proposed in this paper. In CNDRR, underlying correlation structures are explored. Besides, the robustness to noise is guaranteed since robust capped norm are used as basic measurement. Moreover, to obtain more discriminant projections and improve the robustness, CNDRR redefines the scatter matrices with a robust L2,1-norm based on the differential criterion, discards the square operation on the Euclidean distance in the locality preserving criterion, and employs a robust capped norm as a basic measurement. In addition, by imposing the L2,1-norm regularization term on the projection matrix, the joint sparsity of the projection can be enhanced, and the most significant features can be selected. Theoretical analyses, including convergence analyses and computational complexity, are presented. The experimental results show that the proposed CNDRR is superior to the other compared methods on eight datasets.}
}
@article{JIANG2025111232,
title = {Online weighted hashing for cross-modal retrieval},
journal = {Pattern Recognition},
volume = {161},
pages = {111232},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111232},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400983X},
author = {Zining Jiang and Zhenyu Weng and Runhao Li and Huiping Zhuang and Zhiping Lin},
keywords = {Cross-modal retrieval, Online hashing, Weighted hashing},
abstract = {In recent times, there has been a growing interest in online hashing for cross-modal retrieval tasks with streaming data. This technique involves encoding streaming data into compact binary codes, which has the potential to reduce data storage requirements and enhance search efficiency. However, it is important to note that as the length of binary codes shortens, the retrieval performance deteriorates rapidly due to an ambiguity issue caused by Hamming distance. To solve this problem, we propose a novel online hashing method, Online Weighted Hashing (OWH), to improve the retrieval performance for short binary codes by learning weights online to replace Hamming distance with weighted Hamming distance. By learning different weights on each bit of binary hash codes, it solves the ambiguity issue associated with Hamming distance as well as preserves more semantic information, and thus improves the accuracy of cross-modal retrieval. The online weight learning problem is formulated by exploiting the similarity between newly coming data and existing binary codes, and an efficient optimization algorithm is designed to solve this problem. Extensive experimental results on three benchmark datasets demonstrate that the proposed method outperforms state-of-the-art online and offline cross-modal hashing methods in terms of retrieval accuracy.}
}
@article{LAZAROU2024111304,
title = {Exploiting unlabeled data in few-shot learning with manifold similarity and label cleaning},
journal = {Pattern Recognition},
pages = {111304},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111304},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010550},
author = {Michalis Lazarou and Tania Stathaki and Yannis Avrithis},
keywords = {Computer vision, Few-shot learning, Semi-supervised learning, Transductive learning},
abstract = {Few-shot learning investigates how to solve novel tasks given limited labeled data. Exploiting unlabeled data along with the limited labeled has shown substantial improvement in performance. In this work we propose a novel algorithm that exploits unlabeled data in order to improve the performance of few-shot learning. We focus on transductive few-shot inference, where the entire test set is available at inference time, and semi-supervised few-shot learning where unlabeled data are available and can be exploited. Our algorithm starts by leveraging the manifold structure of the labeled and unlabeled data in order to assign accurate pseudo-labels to the unlabeled data. Iteratively, it selects the most confident pseudo-labels and treats them as labeled improving the quality of pseudo-labels at every iteration. Our method surpasses or matches the state of the art results on four benchmark datasets, namely miniImageNet, tieredImageNet, CUB and CIFAR-FS, while being robust over feature pre-processing and the quantity of available unlabeled data. Furthermore, we investigate the setting where the unlabeled data contains data from distractor classes and propose ideas to adapt our algorithm achieving new state of the art performance in the process. Specifically, we utilize the unnormalized manifold class similarities obtained from label propagation for pseudo-label cleaning and exploit the uneven pseudo-label distribution between classes to remove noisy data. The publicly available source code can be found at https://github.com/MichalisLazarou/iLPC.}
}
@article{DUAN2025111252,
title = {LCCo: Lending CLIP to co-segmentation},
journal = {Pattern Recognition},
volume = {161},
pages = {111252},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111252},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010033},
author = {Xin Duan and Yan Yang and Liyuan Pan and Xiabi Liu},
keywords = {Co-segmentation, CLIP, Common semantic},
abstract = {This paper studies co-segmenting common semantic objects in a set of images. Existing works either rely on carefully engineered networks to mine implicit semantics in visual features or require extra data (i.e., classification labels) for training. In this paper, we leverage the contrastive language-image pre-training framework (CLIP) for the task. With a backbone segmentation network that processes each image from the set, we introduce semantics from CLIP into the backbone features, refining them in a coarse-to-fine manner with three key modules: (i) an image set feature correspondence module, encoding global consistent semantics of the image set; (ii) a CLIP interaction module, using CLIP-mined common semantics of the image set to refine the backbone feature; (iii) a CLIP regularization module, drawing CLIP towards co-segmentation, identifying and using the best CLIP semantic to regularize the backbone feature. Experiments on four standard co-segmentation benchmark datasets show that our method outperforms state-of-the-art methods.}
}
@article{JEON2025111276,
title = {Mutually-aware feature learning for few-shot object counting},
journal = {Pattern Recognition},
volume = {161},
pages = {111276},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111276},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010276},
author = {Yerim Jeon and Subeen Lee and Jihwan Kim and Jae-Pil Heo},
keywords = {Few-shot object counting, Class-agnostic counting, Object counting, Few-shot learning, Deep learning},
abstract = {Few-shot object counting has garnered significant attention for its practicality as it aims to count target objects in a query image based on given exemplars without additional training. However, the prevailing extract-and-match approach has a shortcoming: query and exemplar features lack interaction during feature extraction since they are extracted independently and later correlated based on similarity. This can lead to insufficient target awareness and confusion in identifying the actual target when multiple class objects coexist. To address this, we propose a novel framework, Mutually-Aware FEAture learning (MAFEA), which encodes query and exemplar features with mutual awareness from the outset. By encouraging interaction throughout the pipeline, we obtain target-aware features robust to a multi-category scenario. Furthermore, we introduce background token to effectively associate the query’s target region with exemplars and decouple its background region. Our extensive experiments demonstrate that our model achieves state-of-the-art performance on FSCD-LVIS and FSC-147 benchmarks with remarkably reduced target confusion.}
}
@article{AN2025111303,
title = {Relative neighborhood rough feature selection and robust classification for multi-density data},
journal = {Pattern Recognition},
volume = {161},
pages = {111303},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111303},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010549},
author = {Shuang An and Yanan Zhang and Changzhong Wang},
keywords = {Relative neighborhood, Multi-density data, Relative neighborhood rough sets, Feature selection, Robust classification},
abstract = {Uncertainty measures based on neighborhood theory are usually sensitive to noise and data distribution, which has attracted extensive attention. To improve the generalization of neighborhood uncertainty measure, a relative neighborhood theory is proposed, and a relative neighborhood rough set (RNRS) model is designed with the theory. The model uses relative neighborhood to measure the classification uncertainty of samples, and its main advantage is that it can effectively handle the uncertainty of data with large density differences using a uniform relative neighborhood radius. This effectively reduces the sensitivity of some neighborhood rough set models to neighborhood radius and multi-density data. Based on the RNRS model, a feature selection algorithm and a robust classification algorithm are designed, and the effectiveness and generalization of the proposed algorithms are verified by experiments. The results show that the RNRS model is effective, robust and applicable in evaluating the uncertainty of data. And the designed algorithms using RNRS model is feasible and efficient.}
}
@article{RAN2025111217,
title = {Camera-aware graph multi-domain adaptive learning for unsupervised person re-identification},
journal = {Pattern Recognition},
volume = {161},
pages = {111217},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111217},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009683},
author = {Zhidan Ran and Xiaobo Lu and Xuan Wei and Wei Liu},
keywords = {Unsupervised person re-identification, Heterogeneous graph learning, Adversarial learning},
abstract = {Recently, unsupervised person re-identification (Re-ID) has gained much attention due to its important practical significance in real-world application scenarios without pairwise labeled data. A key challenge for unsupervised person Re-ID is learning discriminative and robust feature representations under cross-camera scene variation. Contrastive learning approaches treat unsupervised representation learning as a dictionary look-up task. However, existing methods ignore both intra- and inter-camera semantic associations during training. In this paper, we propose a novel unsupervised person Re-ID framework, Camera-Aware Graph Multi-Domain Adaptive Learning (CGMAL), which can conduct multi-domain feature transfer with semantic propagation for learning discriminative domain-invariant representations. Specifically, we treat each camera as a distinct domain and extract image samples from every camera domain to form a mini-batch. A heterogeneous graph is constructed for representing the relationships between all instances in a mini-batch. Then a Graph Convolutional Network (GCN) is employed to fuse the image samples into a unified space and implement promising semantic transfer for providing ideal feature representations. Subsequently, we construct the memory-based non-parametric contrastive loss to train the model. In particular, we design an adversarial training scheme for transferring the knowledge learned by GCN to the feature extractor. Experimental experiments on three benchmarks validate that our proposed approach is superior to the state-of-the-art unsupervised methods.}
}
@article{KONG2025111291,
title = {RestoCrowd: Image restoration coadjutant learning for hazy-weather crowd counting without weather labels},
journal = {Pattern Recognition},
volume = {161},
pages = {111291},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111291},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010422},
author = {Weihang Kong and Jienan Shen and He Li and Yang Li},
keywords = {Hazy-weather crowd counting, Image adaptive restoration and enhancement, Image restoration coadjutant learning, Frequency-channel refined fusion},
abstract = {Despite consideration works on the clear-scene crowd counting and the consequent satisfactory performance, the hazy-weather crowd counting significant to industrial surveillance application has less been studied. Taking into account the issue of unrevealed interrelationship between low-level image restoration and high-level counting from current research, this paper leverages the image restoration coadjutant learning to achieve the weather-label-free hazy-weather crowd counting. Specially, an adaptive image restoration module is designed to realize the hazy-weather restoration and enhancement. Then, the developed dual-branch restoration-coadjutant sub-network receives the restored image and hazy image to realize the mutual feature representation compensation between the low-level restoration and high-level counting, without the need of clear-weather labels. In addition, a frequency-channel refined fusion module is utilized to refine the feature precision to the crowd and the feature robustness to hazy issue. Extensive experiments on benchmarks demonstrate the advantages of our method and its competitiveness to representative methods under hazy-weather scenes.}
}
@article{HU2025111263,
title = {Query-efficient black-box ensemble attack via dynamic surrogate weighting},
journal = {Pattern Recognition},
volume = {161},
pages = {111263},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111263},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010148},
author = {Cong Hu and Zhichao He and Xiaojun Wu},
keywords = {Black-box attack, Ensemble strategies, Deep neural networks, Transferable adversarial example, Image classification},
abstract = {In recent years, deep neural networks (DNNs) have been widely applied across various fields, but the sensitivity of DNNs to adversarial attacks has attracted widespread attention. Existing research has highlighted the potential of ensemble attacks, which blend the strengths of transfer-based and query-based methods, to create highly transferable adversarial examples. It has been noted that simply amalgamating outputs from various models, without considering the gradient variances, can lead to low transferability. Furthermore, employing static model weights or inefficient weight update strategies may contribute to an unnecessary proliferation of query iterations. To address these issues, this paper introduces a novel black-box ensemble attack algorithm (DSWEA) that combines the Ranking Variance Reduced (RVR) ensemble strategy with the Dynamic Surrogate Weighting (DSW) weight update strategy. RVR employs multiple internal iterations within each query to compute and accumulate unbiased gradients, which are then used to update adversarial examples. This optimization of the gradient diminishes the negative impact of excessive gradient discrepancies between models, thereby enhancing the transferability of perturbations. DSW dynamically adjusts the surrogate weights in each query iteration based on model gradient information, guiding the efficient generation of perturbations. We conduct extensive experiments on the ImageNet and CIFAR-10 datasets, involving various models with varying architectures. Our empirical results reveal that our methodology outperforms existing state-of-the-art techniques, showcasing superior efficacy in terms of Attack Success Rate (ASR) and Average Number of Queries (ANQ).}
}
@article{FU2025111204,
title = {UM-CAM: Uncertainty-weighted multi-resolution class activation maps for weakly-supervised segmentation},
journal = {Pattern Recognition},
volume = {160},
pages = {111204},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111204},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009555},
author = {Jia Fu and Guotai Wang and Tao Lu and Qiang Yue and Tom Vercauteren and Sébastien Ourselin and Shaoting Zhang},
keywords = {Segmentation, Brain tumor, Class activation map, Exponential geodesic distance, Noisy label},
abstract = {Weakly-supervised medical image segmentation methods utilizing image-level labels have gained attention for reducing the annotation cost. They typically use Class Activation Maps (CAM) from a classification network but struggle with incomplete activation regions due to low-resolution localization without detailed boundaries. Differently from most of them that only focus on improving the quality of CAMs, we propose a more unified weakly-supervised segmentation framework with image-level supervision. Firstly, an Uncertainty-weighted Multi-resolution Class Activation Map (UM-CAM) is proposed to generate high-quality pixel-level pseudo-labels. Subsequently, a Geodesic distance-based Seed Expansion (GSE) strategy is introduced to rectify ambiguous boundaries in the UM-CAM by leveraging contextual information. To train a final segmentation model from noisy pseudo-labels, we introduce a Random-View Consensus (RVC) training strategy to suppress unreliable pixel/voxels and encourage consistency between random-view predictions. Extensive experiments on 2D fetal brain segmentation and 3D brain tumor segmentation tasks showed that our method significantly outperforms existing weakly-supervised methods. Code is available at: https://github.com/HiLab-git/UM-CAM.}
}
@article{LIU2025111228,
title = {PyramidPCD: A novel pyramid network for point cloud denoising},
journal = {Pattern Recognition},
volume = {161},
pages = {111228},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111228},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009798},
author = {Zheng Liu and Weijie Zhou and Chuchen Guo and Qinjun Qiu and Zhong Xie},
keywords = {Point cloud denoising, Feature pyramid, Transformer},
abstract = {Point cloud denoising, which aims to restore high-quality point clouds from noisy input, is an ingredient in various fields, including 3D mapping, 3D vision, and structured modeling. In this study, we present a feature pyramid network that can effectively remove noise while accurately preserving structural-to-detailed shape characteristics at varying scales. Our method is built upon the key insight that the coarser scale in the feature pyramid naturally contains more primary structures, while the finer scale provides more shape details. This discovery motivates us to progressively upgrade the current-scale feature with coarser-scale features to preserve structures while recovering details from the finer scale. To this end, we present a U-Net-shaped architecture that incorporates structure-aware and detail-preserving units at multiple scales for feature-preserving point cloud denoising. The structure-aware unit can enhance the current-scale feature by applying structural guidance from the coarse level of the feature pyramid, while the detail-preserving unit can learn a more comprehensive representation that incorporates the finer-scale feature in the pyramid. Extensive experiments conducted on publicly available benchmarks demonstrate that the proposed approach achieves state-of-the-art performances and outperforms the competing methods. Our source code and data are available at https://github.com/ForestNobear/PyramidPCD.}
}
@article{LIAN2025111215,
title = {Semi-supervised anomaly traffic detection via multi-frequency reconstruction},
journal = {Pattern Recognition},
volume = {161},
pages = {111215},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111215},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400966X},
author = {Xinglin Lian and Yu Zheng and Zhangxuan Dang and Chunlei Peng and Xinbo Gao},
keywords = {Anomaly traffic detection, Semi-supervised learning, Multi-frequency information extraction, Integrated prediction},
abstract = {Anomaly traffic detection is a crucial issue in cyber-security field. Traditionally, many researchers have approached anomaly traffic detection as a supervised classification problem. However, in real-world scenarios, anomaly network traffic is unpredictable, dynamic, and difficult to collect. To address these challenges, we adopt an anomaly detection setting that trains using only normal traffic data and propose a novel semi-supervised method: Multi-Frequency Reconstruction (MFR). Our approach is driven by a key observation: traffic images often lack explicit patterns and texture features, making accurate modeling of normal traffic difficult. To overcome this limitation, we employ low-pass filters to extract multi-scale low-frequency information, offering a more effective multi-view perspective to identify anomaly patterns. Additionally, we introduce a channel-spatial attention mechanism within reconstruction models to enhance the network’s ability to capture the spatio-temporal features of traffic flows. Ultimately, to adequately leverage multi-view information, we integrate anomaly scores from multi-frequency branches, achieving a more comprehensive anomaly detection. Extensive experiments on three public anomaly traffic detection datasets demonstrate the superior performance of our method, outperforming state-of-the-art approaches by 4.6% and 10.5% in AUROC on the DataCon2020 and CIC-IDS2017 datasets, respectively.}
}
@article{DING2025111279,
title = {DeepUTF: Locating transcription factor binding sites via interpretable dual-channel encoder-decoder structure},
journal = {Pattern Recognition},
volume = {161},
pages = {111279},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111279},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010306},
author = {Pengju Ding and Jianxin Wang and Shiyue He and Xin Gao and Xu Yu and Bin Yu},
keywords = {Transcription factor binding sites, Interpretability, Swin transformer, Improved encoder-decoder},
abstract = {The accurate location of transcription factor binding sites (TFBSs) is important for the design of synthetic biology components and the realization of precision medicine. Despite the growing use of deep learning for TFBSs prediction, model interpretability remains challenging. We introduce DeepUTF, a novel architecture integrating improved encoder-decoder, swin transformer, and parallel Bi-LSTM, which realizes precise localization of TFBSs and prediction of motifs. We elucidate the effectiveness of the swin transformer in capturing a wide range of dependencies and emphasizing the learning of critical features. Meanwhile, interpretability analysis of the model's output and predictions of TF-DNA binding motifs are conducted, providing a thorough exploration of the model's intrinsic mechanisms and feature learning process. Experiments conducted on 53 ChIP-seq datasets illustrate that DeepUTF surpasses several leading algorithms. The trained model accurately predicts direct and indirect TF-DNA binding motifs. Furthermore, comparisons with the PDB database validate the continuity and accuracy of predictions.}
}
@article{HU2025111220,
title = {MSAttnFlow: Normalizing flow for unsupervised anomaly detection with multi-scale attention},
journal = {Pattern Recognition},
volume = {161},
pages = {111220},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111220},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009713},
author = {Zhengnan Hu and Xiangrui Zeng and Yiqun Li and Zhouping Yin and Erli Meng and Ziyu Wei and Leyan Zhu and Zitian Wang},
keywords = {Anomaly detection, Unsupervised learning, Normalizing flows, Distribution modeling},
abstract = {Unsupervised anomaly detection (UAD) aims to locate anomalies in images without using annotated defective data. Normalizing flow is inherently suitable for the UAD task because it can explicitly model the data distribution and perform accurate density estimations. The multi-scale problem of anomalies is one of the key issues that existing flow-based methods focus on. To solve this problem, existing methods either introduce complex cross-scale information exchange operations or use multiple independent parallel flows. In this work, we propose a novel normalizing flow network with multi-scale attention, MSAttnFlow. We construct normalizing flows at different levels of the feature pyramid to handle multi-scale anomalies. Meanwhile, we introduce an autoregressive multi-scale attention module with two directions to fuse features of different scales, which facilitates the sharing of perception capabilities between different scales without introducing large computational overhead. MSAttnFlow achieves state-of-the-art on popular anomaly detection benchmarks MVTec-AD and VisA, with image/pixel level AUROC reaching 99.7/98.9 and 98.1/99.1 respectively. The performance of MSAttnFlow also improves by 2.9/1.3 on VisA compared to the previous SOTA method, which is a significant improvement considering the high performance of existing methods.}
}
@article{XIA2025111244,
title = {Collaborative contrastive learning for cross-domain gaze estimation},
journal = {Pattern Recognition},
volume = {161},
pages = {111244},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111244},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009956},
author = {Lifan Xia and Yong Li and Xin Cai and Zhen Cui and Chunyan Xu and Antoni B. Chan},
keywords = {Gaze estimation, Domain generalization, Domain adaption},
abstract = {Gaze estimation methods commonly rely on single-camera facial appearance analysis to estimate the direction of a person’s gaze. Recently, there has been significant interest in exploring gaze estimation techniques across different domains. Despite the advances, distinguishing authentic gaze-relevant features primarily relies on detecting subtle changes in the eye region. Moreover, non-gaze-related features are intricately entangled with the gaze-relevant components in a nonlinear manner. In response to these challenges, our work addresses the cross-domain gaze estimation problem through a feature decontaminating approach. Specifically, we propose a Cross Gaze Generalization (CGaG) method that explicitly encodes gaze- and domain-dedicated feature and leverages cross-identity features swapping to generate novel images where changes are exclusively confined to either the gaze or domain attributes. To consolidate feature purification and domain generalization, we utilize the equivariance between images and gaze labels to facilitate collaborative contrastive learning (CCL), which faithfully ensures that the generated novel images align with the expected outputs. Extensive experiments are conducted on various cross-domain tasks, demonstrating the effectiveness of CGaG. Meanwhile, CGaG achieves superior or comparable gaze estimation accuracy on both domain generalization and domain adaptation experiments. CGaG also shows promising cross-identity gaze or domain transfer visualizations.}
}
@article{CUI2025111250,
title = {Dynamic multi-scale feature augmentation for inductive network representation learning},
journal = {Pattern Recognition},
volume = {161},
pages = {111250},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111250},
url = {https://www.sciencedirect.com/science/article/pii/S003132032401001X},
author = {Shicheng Cui and Deqiang Li and Jing Zhang},
keywords = {Inductive network representation learning, Graph neural networks, Multi-scale features, Data augmentation},
abstract = {Inductive Network Representation Learning (INRL) has been successfully applied in various graph-based machine learning tasks. Prior arts advance INRL by using Graph Neural Networks (GNNs) on graph-structured data. Most of them follow a topology-static aggregation scheme, where the representation of a node is calculated through a recursive process of aggregating and transforming the fixed local proximity information from its neighborhood. However, this may affect model generalization when facing data noise or data scarcity problems. Therefore, in this paper, we propose a novel INRL framework, dubbed MUFA, to learn meaningful network representations via dynamic MUlti-scale Feature Augmentation based on GNNs. MUFA follows a topology-dynamic aggregation scheme, which incorporates structure-based and attribute-based graph properties as multi-scale features for data augmentation. Precisely, we design two modules to augment the features. One is the randomized combination of ego network structures, which can provide various substructures of graph data and relieve isolation issues caused by arbitrary selection of nodes. The other is the Inductive Masked Message Passing (IMMP), which dynamically masks parts of subregional information in the graph convolutional receptive field. Thus, diverse unmask-to-mask feature pairs are yielded as graph context augmentation for INRL. The pairs are fed to an auto-encoder, by which the encoder encodes the unmasked information and the decoder reconstructs networked relationships and reproduces node attributes in the masked subregions simultaneously. We conduct experiments on link prediction and node classification tasks over several public benchmarks. Experimental results present that the proposed MUFA performs better in model generalization and is able to generate high-quality network representations compared with state-of-the-art neural network baselines.}
}
@article{STRAGAPEDE2024111287,
title = {KVC-onGoing: Keystroke Verification Challenge},
journal = {Pattern Recognition},
pages = {111287},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111287},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010380},
author = {Giuseppe Stragapede and Ruben Vera-Rodriguez and Ruben Tolosana and Aythami Morales and Ivan DeAndres-Tame and Naser Damer and Julian Fierrez and Javier-Ortega Garcia and Alejandro Acien and Nahuel Gonzalez and Andrei Shadrikov and Dmitrii Gordin and Leon Schmitt and Daniel Wimmer and Christoph Großmann and Joerdis Krieger and Florian Heinz and Ron Krestel and Christoffer Mayer and Simon Haberl and Helena Gschrey and Yosuke Yamagishi and Sanjay Saha and Sanka Rasnayaka and Sandareka Wickramanayake and Terence Sim and Weronika Gutfeter and Adam Baran and Mateusz Krzysztoń and Przemysław Jaskóła},
keywords = {Keystroke dynamics, Behavioral biometrics, Biometric verification, KVC-onGoing, Challenge},
abstract = {This article presents the Keystroke Verification Challenge - onGoing (KVC-onGoing)11https://sites.google.com/view/bida-kvc/., on which researchers can easily benchmark their systems in a common platform using large-scale public databases, the Aalto University Keystroke databases, and a standard experimental protocol. The keystroke data consist of tweet-long sequences of variable transcript text from over 185,000 subjects, acquired through desktop and mobile keyboards simulating real-life conditions. The results on the evaluation set of KVC-onGoing have proved the high discriminative power of keystroke dynamics, reaching values as low as 3.33% of Equal Error Rate (EER) and 11.96% of False Non-Match Rate (FNMR) @1% False Match Rate (FMR) in the desktop scenario, and 3.61% of EER and 17.44% of FNMR @1% at FMR in the mobile scenario, significantly improving previous state-of-the-art results. Concerning demographic fairness, the analyzed scores reflect the subjects’ age and gender to various extents, not negligible in a few cases. The framework runs on CodaLab22https://codalab.lisn.upsaclay.fr/competitions/14063..}
}
@article{LV2025111275,
title = {DrKD: Decoupling response-based distillation for object detection},
journal = {Pattern Recognition},
volume = {161},
pages = {111275},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111275},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010264},
author = {Yilong Lv and Yancheng Cai and Yujie He and Min Li},
keywords = {Decoupled response-based distillation, Encoder distillation, Decoder distillation, Object detection, Model compression},
abstract = {Response-based distillation, a key form of knowledge distillation (KD), has been central to KD research. However, the sequential training pipelines inherent to object detection models limit the effectiveness of response-based distillation, constraining student model performance. To address this limitation, we propose a novel parallel distillation framework. Specifically, we decouple response-based knowledge distillation (DrKD) into parallel encoder and decoder distillation. For encoder distillation, feature imitation is employed, but conventional methods are hindered by the semantic gap between features and prediction results. To mitigate this issue, we introduce gap-free feature imitation, which uses a simple yet effective adapter to transform features into the response space, effectively bridging the semantic gap. In addition, we propose autocorrelation imitation to further enhance feature imitation. For decoder distillation, we extend the function approximation principle to KD. By maintaining consistent inputs to the decoder and aligning the student’s output with the teacher’s output, we effectively implement decoder distillation. Extensive experiments across multiple detectors show that DrKD achieves state-of-the-art performance. Specifically, GFL, FCOS, ATSS, TOOD, and DDOD with DrKD based on ResNet-50 achieve 42.6%, 40.1%, 40.9%, 44.8%, and 42.1% AP on COCO, which are 2.4, 1.4, 1.5, 2.4 and 0.9 higher than their baselines. RetinaNet with DrKD based on ResNet-50 achieve 59.1% mAP on VOC, which are 3.7 higher than their baselines.}
}
@article{HU2025111251,
title = {CCDFormer: A dual-backbone complex crack detection network with transformer},
journal = {Pattern Recognition},
volume = {161},
pages = {111251},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111251},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010021},
author = {Xiangkun Hu and Hua Li and Yixiong Feng and Songrong Qian and Jian Li and Shaobo Li},
keywords = {Concrete crack detection, Semantic segmentation, Transformer, Convolution neural network, UNet},
abstract = {Concrete crack detection is a critical aspect of infrastructure maintenance. However, existing methods often fail to deliver satisfactory results in real-world scenarios where various detection challenges coexist. We propose a Transformer-based model to enhance feature extraction for complex crack detection (CCDFormer). CCDFormer employs a dual-backbone U-shaped structure to independently capture crack features from different perspectives, avoiding interference. Deformable linear convolution align with crack structures, while the proposed feature enhancement module enriches semantic features by boosting local features at multiple scales. The pyramid-shaped Transformer models long-range dependencies across different scales. A carefully designed feature fusion module addresses the shortcomings of local and contextual features, generating robust crack features. On a challenging public dataset for concrete crack detection, CCDFormer improves accuracy, recall, F-measure, and IoU by 3.54%, 0.71%, 2.17%, and 1.48%, compared to existing models. CCDFormer demonstrates higher precision and crack detection rates across various challenges, proving practical for real-world crack detection.}
}
@article{CHEN2025111216,
title = {Multi-consistency for semi-supervised medical image segmentation via diffusion models},
journal = {Pattern Recognition},
volume = {161},
pages = {111216},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111216},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009671},
author = {Yunzhu Chen and Yang Liu and Manti Lu and Liyao Fu and Feng Yang},
keywords = {Medical image segmentation, Diffusion models, Semi-supervised learning},
abstract = {Medical image segmentation presents a formidable challenge, compounded by the scarcity of annotated data in numerous datasets. Semi-supervised methods offer viable solutions to mitigate that, while the image generation capability of diffusion models has shown potential in capturing semantically meaningful information. This paper introduces Multi-Consistency for semi-supervised medical image Segmentation via Diffusion Models (MCSD). We propose a Diffusion-based Feature-guided Module (DFM) that extracts features from pre-trained diffusion models and uses multi-scale features to guide multi-consistency segmentation networks. Additionally, we introduce the Dual-branch Image Consistency (DIC) strategy, which performs multi-consistency learning by generating two independent strong augmented images and optimizing the network by encouraging consistency between strong and weak inputs at multiple levels of features and images. Our method outperforms current approaches, demonstrating its effectiveness in semi-supervised medical image segmentation through experimental results for various labeled data ratios. Furthermore, this work points out the potentiality of diffusion models in semi-supervised medical image segmentation and offers suggestions for improving their use in medical imaging tasks. The code is available at https://github.com/yunzhuC/MCSD.}
}
@article{OUYANG2024111306,
title = {PlanePDM: Boundary-aware 3D planar recovery by using parallel dilated mask head},
journal = {Pattern Recognition},
pages = {111306},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111306},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010574},
author = {Wenzhe Ouyang and Zenglin Xu and Qianying Zhu and Bin Shen and Yong Xu},
keywords = {Plane recovery, 3D reconstruction, Plane segmentation},
abstract = {This paper investigates the problem of recovering 3D planar structures from single RGB images, which aims to segment plane instances and predict their corresponding 3D parameters simultaneously. Despite remarkable progress in this area, current mainstream methods still suffer from two shortcomings: (1) incorrect detection of non-plane regions; (2) unsatisfactory plane restoration quality. To tackle these issues, we first propose utilizing a direct segmentation framework to predict plane instances and their corresponding normal vectors. On this basis, we propose PlanePDM to provide lightweight yet effective boundary supervision for high-quality 3D plane recovery. More specifically, the PlanePDM designs a tailored dilated mask head parallel to the conventional plane mask prediction head. Due to such a design, we can generate boundary predictions of planes by performing simple per-pixel minus operations, thereby avoiding complex post-processing techniques typically required by contour regression methods. Comprehensive experiments demonstrate that PlanePDM outperforms existing state-of-the-art techniques with higher margins in terms of plane detection, segmentation, and reconstruction metrics across the ScanNet and NYUv2 datasets.}
}
@article{LUO2025111254,
title = {PA-Net: A hybrid architecture for retinal vessel segmentation},
journal = {Pattern Recognition},
volume = {161},
pages = {111254},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111254},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010057},
author = {Xuebing Luo and Lingxi Peng and Ziyan Ke and Jinhui Lin and Zhiwen Yu},
keywords = {Retinal vessel segmentation, Retinal images, Transformer, Feature fusion, Deep learning},
abstract = {This paper proposes a hybrid architecture, PA-Net, which amalgamates the strengths of convolutional neural networks and the transformer model to enhance the precision of retinal vessel segmentation. We propose a novel component, the Lightweight Parallel Transformer (LPT), to augment the transformer's adaptability for the task of retinal vessel segmentation. This LPT addresses the shortcomings of standard transformer that are highly dependent on large datasets and computing resources, and can capture long-range dependencies to prevent slender vessels from breaking. Furthermore, we introduce an Adaptive Vascular Feature Fusion module to offset the vascular information loss induced by downsampling layers, thereby enhancing microvessel recognition. The effectiveness of PA-Net was assessed across four distinct datasets: DRIVE, CHASE_DB1, STARE, and HRF, with sensitivities of 0.8284, 0.8570, 0.8813, and 0.8497, respectively. The results suggest that the proposed method outperforms other state-of-the-art alternatives.}
}
@article{WANG2025111211,
title = {ClickTrack: Towards real-time interactive single object tracking},
journal = {Pattern Recognition},
volume = {161},
pages = {111211},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111211},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009622},
author = {Kuiran Wang and Xuehui Yu and Wenwen Yu and Guorong Li and Xiangyuan Lan and Qixiang Ye and Jianbin Jiao and Zhenjun Han},
keywords = {Click, Single object tracking, Video object segmentation, Real-time interaction},
abstract = {Single object tracking (SOT) relies on precise object bounding box initialization. In this paper, we reconsidered the deficiencies in the current approaches to initializing single object trackers and propose a new paradigm for single object tracking algorithms, ClickTrack, a new paradigm using clicking interaction for real-time scenarios. Moreover, click as an input type inherently lack hierarchical information. To address ambiguity in certain special scenarios, we designed the Guided Click Refiner (GCR), which accepts point and optional textual information as inputs, transforming the point into the bounding box expected by the operator. The bounding box will be used as input of single object trackers. Experiments on LaSOT and GOT-10k benchmarks show that tracker combined with GCR achieves stable performance in real-time interactive scenarios. Furthermore, we explored the integration of GCR into the Segment Anything model (SAM), significantly reducing ambiguity issues when SAM receives point inputs.}
}
@article{HAN2025111281,
title = {Integrating label confidence-based feature selection for partial multi-label learning},
journal = {Pattern Recognition},
volume = {161},
pages = {111281},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111281},
url = {https://www.sciencedirect.com/science/article/pii/S003132032401032X},
author = {Qingqi Han and Liang Hu and Wanfu Gao},
keywords = {Partial multi-label learning, Feature selection, Label confidence},
abstract = {Partial Multi-Label Learning (PML) is an emerging learning paradigm that deals with candidate label sets containing false positive labels, facing the negative impacts of noisy labels and high-dimensional data. Existing methods evaluate label confidence in the original feature space, neglecting the negative impacts of ambiguous and redundant features. To tackle this issue, we propose a novel feature selection method, Label Confidence Feature Selection-Partial Multi-Label (LCFS-PML). This method establishes a better mapping relationship by simultaneously optimizing both features and labels. First, label confidence is evaluated within the unique feature subspace of each label by combining the average distance between instances sharing the same label and the distance from the instance to the cluster center. Second, the optimized, more reliable labels are used to guide the optimization process of the feature space. During the alternating optimization between the feature and label spaces, LCFS-PML effectively mitigates the negative impacts of noisy labels, ambiguous features, and redundant features, ultimately identifying the optimal feature subset for each label. Comparative experiments on nine benchmark datasets show that the proposed method demonstrates significant superiority.}
}
@article{ZHENG2025111270,
title = {Micro-community domain adaptation},
journal = {Pattern Recognition},
volume = {161},
pages = {111270},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111270},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010215},
author = {Zefeng Zheng and Shaohua Teng and Luyao Teng and Naiqi Wu and Wei Zhang},
keywords = {Domain adaptation, Micro-community, Pairwise Similarity Learning, Semantic guidance},
abstract = {Domain Adaptation (DA) leverages knowledge learned from labeled source domain to classify unlabeled target domain. It is believed that a subcategory usually possesses broader common knowledge than its superclass, and this study demonstrates that Micro-Community exhibits more extensive shared knowledge than its supersets and superclasses. With the above observation, the existing DA methods may encounter three problems: (a) they transfer knowledge either from common features of two domains or individual-specific features within a single domain, while overlooking to exhaustively transfer knowledge from both of them; (b) they neglect to extract knowledge from micro-communities that hold more common features than the class in the source domain; and (c) they overlook to exhaustively transfer multilevel knowledge from individuals, micro-communities, and classes. To address the above issues, a Micro-Community-oriented learning framework called Micro-Community Domain Adaptation (Micro-CDA) is proposed. Micro-CDA consists of two parts: (a) Intra-Micro-Community Learning (IMicro-CL) that divides the intra-class samples in the source domain into nu easily transferable micro-communities, and adaptively explores both the micro-community-shared and individual-specific knowledge of data; and (b) Pairwise Similarity Learning (PSL) that imposes a pairwise similarity constraint between the source and target domains by applying the micro-communities, and jointly learns fused knowledge from individuals and micro-communities of the two domains under semantic guidance. By Micro-CDA, knowledge is conveyed gradually from the source to target domain under semantic guidance, with the knowledge of individuals, micro-communities, and classes being included. Theoretical analysis shows that PSL effectively achieves at most nu subregions during domain adaptation. Extensive experiments on three benchmark datasets and one multi-attribute dataset demonstrate the effectiveness of Micro-CDA.}
}
@article{GAO2025111246,
title = {A large-scale combinatorial benchmark for sign language recognition},
journal = {Pattern Recognition},
volume = {161},
pages = {111246},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111246},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400997X},
author = {Liqing Gao and Liang Wan and Lianyu Hu and Ruize Han and Zekang Liu and Peng Shi and Fanhua Shang and Wei Feng},
keywords = {Sign language recognition, T and E disassemble-and-reassemble strategy, Cost-controllable large-scale dataset, Combinatorial framework},
abstract = {Lacking a large-scale dataset is the major obstacle limiting sign language recognition (SLR) to work well in the real world, because of the huge collection and annotation cost of sign language videos. This paper rethinks a sign language sentence as a combination of a template (T) and an entity (E) and presents a novel T and E Disassemble-and-reAssemble (TEDA) strategy to collect T and E sign videos independently. The proposed TEDA strategy has a theoretical capability of generating T×E effective samples with only T+E collection and annotation cost. With the TEDA strategy, we build a cost-controllable large-scale (CCLS) sign language dataset with 300,400 combinatorial samples, generated from 6,000 T videos and 29,700 E videos. To enable training arbitrary SLR models on combinatorial data, we propose a combinatorial SLR framework. Specifically, we first design a dynamic combination module to dynamically combine independent T and E features to generate combinatorial features. Then, we propose a joint constraint module to ensure that the distribution of the combinatorial features is as close as possible with the complete features. Finally, we develop a multi-stage training strategy to accommodate SLR learning with the combinatorial data. Plentiful experiments demonstrate the rationality of our TEDA strategy in generating large-scale effective combinatorial samples as well as the effectiveness of the combinatorial framework in promoting SLR.}
}
@article{TANG2025111294,
title = {Circle-YOLO: An anchor-free lung nodule detection algorithm using bounding circle representation},
journal = {Pattern Recognition},
volume = {161},
pages = {111294},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111294},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010458},
author = {Chaosheng Tang and Feifei Zhou and Junding Sun and Yudong Zhang},
keywords = {Yolo, Attention mechanism, Bounding circle, Loss function, Lung nodule detection},
abstract = {Objective
Lung nodule detection is crucial for lung cancer prevention. Despite the advanced performance achieved by existing anchor-based detectors, manual pre-design of anchor parameters is required, and fixed anchor box sizes lack robustness when dealing with targets of different shapes. Furthermore, the current research commonly adopts the rectangular bounding box detection method, which does not align with the circular or elliptical characteristics of nodules and clinical diagnostic requirements.
Methods
To address these issues, we propose the anchor-free lung nodule detection algorithm (Circle-YOLO), consisting of two novel components: bounding circle representation and cross-stage partial attention. Firstly, to align with nodule annotations in clinical diagnosis, we use bounding circles to represent the nodule center location and diameter size. Secondly, a Bounding Circle Intersection-over-Union Loss Function (LBCIoU++) is proposed to further stabilize and efficiently train the network. Lastly, to capture the nodule information and reduce false positives and false negatives, we designed Cross-stage Partial Attention (CSPA). Since the Circle-YOLO method is anchor-free, the nodule location and diameter size can be accurately predicted without designing anchor parameters.
Results
Evaluated on multiple datasets, the Accuracy, Precision, Recall, and mAP on LUNA16 are 97.6 %, 96.1 %, 97.2 %, and 97.7 %, respectively, while the Accuracy, Precision, Recall, and mAP on LIDC-IDRI are 95.3 %, 95.6 %, 95.1 %, 95.1 %, and 95.8 %, respectively, surpassing numerous existing state-of-the-art (SOTA) techniques. Moreover, Circle-YOLO inference speeds of 62 FPS and 52 FPS, respectively, meet the real-time requirements (>30 FPS). The number of parameters is 18.9 M (only 0.4 M is introduced on Baseline), which facilitates lightweight deployment.}
}
@article{YANG2025111199,
title = {Dynamic VAEs via semantic-aligned matching for continual zero-shot learning},
journal = {Pattern Recognition},
volume = {160},
pages = {111199},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111199},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009506},
author = {Junbo Yang and Borui Hu and Hanyu Li and Yang Liu and Xinbo Gao and Jungong Han and Fanglin Chen and Xuangou Wu},
keywords = {Continual zero-shot learning, Catastrophic forgetting, Semantic-aligned matching, Variational autoencoders},
abstract = {Continual Zero-shot Learning (CZSL) is capable of classifying unseen categories across a sequence of tasks. However, CZSL is often plagued by the challenge of catastrophic forgetting. While recent studies have shown that preserving past data for experience replay can effectively address this issue, it may be limited to specific scenarios and pose a risk of data leakage. Additionally, many existing CZSL models fail to adequately highlight the correlation between semantic and visual features. To tackle these shortcomings, we introduce dynamic Variational Autoencoders (VAEs) via semantic-aligned matching for CZSL. The proposed model utilizes both semantic and visual VAEs to enhance the transfer capability of knowledge from past tasks. Leveraging generative experience replay, our model effectively combats catastrophic forgetting. Our approach was assessed on five datasets: aPY, AWA1, AWA2, CUB, and SUN, yielding superior performance to baseline models.}
}
@article{WU2025111223,
title = {FrePrompter: Frequency self-prompt for all-in-one image restoration},
journal = {Pattern Recognition},
volume = {161},
pages = {111223},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111223},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009749},
author = {Zhijian Wu and Wenhui Liu and Jingchao Wang and Jun Li and Dingjiang Huang},
keywords = {Image restoration, Prompt learning, Frequency learning},
abstract = {Deep learning-based models have shown unprecedented success in image restoration. However, existing methods are limited to task-specific restoration, where the model performance is undesirable when the type of degradation changes. This is due to the inconsistency between the actual situation and the priori adopted for model construction. In this paper, we propose a novel prompt learning method called Frequency Self-Prompt (FSP) customized for image restoration. Motivated by the frequency properties, FSP utilizes the degradation information of the input image to generate frequency prompts that dynamically guide the restoration network in removing the corresponding corruption. On the one hand, the frequency representation can disentangle image degradation and content components, which makes learning the degradation information more effective. On the other hand, the frequency domain naturally encodes the globally distributed degradation-specific information. We exploit FSP to build a universal model for all-in-one image restoration, called FrePrompter, which can be applied to various restoration tasks without any prior knowledge of degradation. Extensive experiments show that our method establishes new state-of-the-art results for various restoration tasks.}
}
@article{ZHENG2025111266,
title = {Multi-scale hierarchical feature fusion network for change detection},
journal = {Pattern Recognition},
volume = {161},
pages = {111266},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111266},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010173},
author = {Hanhong Zheng and Mingyang Zhang and Maoguo Gong and A.K. Qin and Tongfei Liu and Fenlong Jiang},
keywords = {Change detection, VHR remote sensing imagery, Multi-scale feature fusion, Deep learning},
abstract = {Change Detection (CD) in Very High-Resolution (VHR) images can bring more detailed and valuable geo-information for geographic information system. However, existing CD methods are still limited by the poor recognition of multi-scale land cover objects with varied shapes in VHR CD tasks. Besides, obtaining accurate change information in complex scenes is still challenging since high-frequency component tends to be suppressed in the most deep learning-based CD methods. To alleviate these problems, we propose a novel encoder–decoder network, Multi-scale Hierarchical Feature Fusion Network (MHF2Net), which utilizes all-scale feature fusion and global high-frequency enhancement strategies. The key functional modules are High-frequency Enhancement Blocks (HEBs) and a Layer-wise Multi-scale Feature Fusion Module (LMF2M). HEB provides auxiliary high-frequency information to further finely annotate the changed land cover. LMF2M fully integrates hierarchical features to better fuse semantic and spatial information at each layer, thus better detecting multi-scale changed objects. And in LMF2M, a Self-weighted Attention Block (SAB) is constructed to throttle uninformative features without supervised parameters. As a result, MHF2Net is able to map the changed objects of varied scales and shapes with quite high accuracy. Extensive experiments are conducted over three public VHR change detection data sets. And the corresponding results suggest that the proposed method achieves state-of-the-art change detection performance in comparison with several recently proposed methods.}
}
@article{WANG2025111282,
title = {CHCANet: Two-view Correspondence Pruning with Consensus-guided Hierarchical Context Aggregation},
journal = {Pattern Recognition},
volume = {161},
pages = {111282},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111282},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010331},
author = {Gang Wang and Yufei Chen and Bin Wu},
keywords = {Correspondence pruning, Context aggregation, Geometry pose estimation, Feature matching},
abstract = {Recent advances in two-view correspondence pruning have leveraged techniques such as context normalization, order-aware aggregation, and local consensus, leading to significant improvements. However, designing an effective correspondence pruning algorithm faced several key challenges: accurately capturing both local and global contexts, handling the inherent sparsity and noise in correspondence data, and achieving robust generalization across diverse scenarios. Although order-aware aggregation enhances network performance by focusing on different clustering views of the putative correspondences, existing methods fall short of fully exploring the hierarchical global context necessary for reliable correspondence pruning. To address these challenges, we propose a novel Hierarchical Context Aggregation (HCA) framework, specifically designed to capture and propagate multi-granularity global contexts. The HCA framework integrates clustering modules and propagates multi-level order-aware clusters through graph pooling, effectively aggregating informative contexts and generating robust global features. Additionally, we introduce a consensus block that aligns the global context with the spatial structure of local neighbors. Our Consensus-guided Hierarchical Context Aggregation Network (CHCANet), built upon this HCA framework with consensus guidance that demonstrates superior performance across large-scale indoor and outdoor datasets, surpassing state-of-the-art methods in two-view correspondence pruning.}
}
@article{ZOU2025111207,
title = {Diffusion-based framework for weakly-supervised temporal action localization},
journal = {Pattern Recognition},
volume = {160},
pages = {111207},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111207},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009580},
author = {Yuanbing Zou and Qingjie Zhao and Prodip Kumar Sarker and Shanshan Li and Lei Wang and Wangwang Liu},
keywords = {Temporal action localization, Weakly-supervised learning, Diffusion, Mask learning},
abstract = {Weakly supervised temporal action localization aims to localize action instances with only video-level supervision. Due to the absence of frame-level annotation supervision, how effectively separate action snippets and backgrounds from semantically ambiguous features becomes an arduous challenge for this task. To address this issue from a generative modeling perspective, we propose a novel diffusion-based network with two stages. Firstly, we design a local masking mechanism module to learn the local semantic information and generate binary masks at the early stage, which (1) are used to perform action-background separation and (2) serve as pseudo-ground truth required by the diffusion module. Then, we propose a diffusion module to generate high-quality action predictions under the pseudo-ground truth supervision in the second stage. In addition, we further optimize the new-refining operation in the local masking module to improve the operation efficiency. The experimental results demonstrate that the proposed method achieves a promising performance on the publicly available mainstream datasets THUMOS14 and ActivityNet. The code is available at https://github.com/Rlab123/action_diff.}
}
@article{ZHAO2025111155,
title = {Negatively correlated ensemble against transfer adversarial attacks},
journal = {Pattern Recognition},
volume = {161},
pages = {111155},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111155},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009063},
author = {Yunce Zhao and Wei Huang and Wei Liu and Xin Yao},
keywords = {Ensemble learning, Model robustness, Transfer adversarial attacks, Negative correlation, Ensemble diversity},
abstract = {Deep neural networks (DNNs) have become a popular tool in various fields, but their susceptibility to adversarial attacks poses a significant threat to the security of machine learning systems. Adversarial examples, which are intentionally crafted inputs to deceive a DNN, can easily transfer between different machine learning models, further exacerbating the problem. To address the transfer adversarial vulnerability issue, ensemble methods have been developed to promote diversity among members, thereby impeding the transfer of adversarial examples among them. However, we observed that the diversity among ensemble members tends to diminish rapidly as the magnitude of adversarial perturbations increases. All ensemble members tend to be deceived by the same adversarial example, resulting in a poor performance against slightly larger transfer adversarial perturbations. To overcome this challenge, we introduce NCRE (negative correlation robust ensemble) and NCRE+(negative correlation robust ensemble on adversarial vulnerabilities) in this work. Our approach leverages the explicit maximization of negative correlation among ensemble member outputs to enhance ensemble diversity and robustness against adversarial perturbations, particularly transfer black-box attacks. Extensive experimental studies and comparisons with state-of-the-art algorithms demonstrate the effectiveness of our approach.}
}
@article{DUAN2025111309,
title = {Deep contrastive representation learning for supervised tasks},
journal = {Pattern Recognition},
volume = {161},
pages = {111309},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111309},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010604},
author = {Chenguang Duan and Yuling Jiao and Lican Kang and Jerry Zhijian Yang and Fusheng Zhou},
keywords = {Contrastive learning, Deep neural networks, Error bound, Supervised learning},
abstract = {Representation learning, with its capability to extract and encapsulate the inherent structure and information embedded within raw data, assumes a crucial role in facilitating the establishment of models. In this paper, we adopt a deep contrastive learning paradigm to derive representations of raw data, thereby laying the foundation for an exploration into supervised learning that encompasses both regression and classification problems. This methodological approach is delineated as a two-stage process. In the initial stage, data representations are acquired through contrastive learning, employing deep ReLU neural networks. Subsequently, in the second stage, the obtained representations are harnessed to address regression and classification in downstream tasks, utilizing norm-constrained ReLU neural networks. Theoretically, we present an informative property and rigorously establish error bounds for the excess risk associated with the resulting estimators of the two-stage method by balancing the stochastic and approximation errors. This establishment serves as a guarantee for the efficacy of deep contrastive learning, especially in scenarios where the sample size during the pretraining phase exceeds that allocated for downstream tasks. Numerical experiments corroborate the effectiveness of the proposed two-stage method and demonstrate a notable alignment with the theoretical findings.}
}
@article{ZHANG2025111247,
title = {Local-enhanced representation for text-based person search},
journal = {Pattern Recognition},
volume = {161},
pages = {111247},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111247},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009981},
author = {Guoqing Zhang and Yuhao Chen and Yuhui Zheng and Gaven Martin and Ruili Wang},
keywords = {Person re-identification, Cross-modal retrieval, Local representation},
abstract = {Text-based person search is a critical task in intelligent security, designed to locate a person of interest by text descriptions. The primary challenge in this task is to effectively bridge the significant gap between the text and image domains while simultaneously extracting the discriminative features that are crucial for the accurate identification of individuals. Existing methods have made some effective attempts by conducting cross-modal matching at the fine-grained representation level. However, these approaches frequently overlook two crucial factors: (i) the presence of noise in the local features during information fusion, and (ii) the lack of intra-modal matching when measuring feature similarity. To address the above issues, we propose a novel local-enhanced representation framework in this paper. Specifically, to restrain noises in local features, we design a Relation-based cross-modal local-enhanced fusion module, which can filter out weak related information by relation assessment. In addition, we explore an intra-cross modal projection strategy to overcome the limitations of existing cross-modal projection methods. This strategy jointly applies the intra-modal and cross-modal matching constrains in feature distribution. Finally, experiments on three mainstream datasets verify the performance superiority of our proposed method compared to existing state-of-the-art methods.}
}
@article{LIU2025111264,
title = {Supervised contrastive deep Q-Network for imbalanced radar automatic target recognition},
journal = {Pattern Recognition},
volume = {161},
pages = {111264},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111264},
url = {https://www.sciencedirect.com/science/article/pii/S003132032401015X},
author = {Guanliang Liu and Wenchao Chen and Bo Chen and Bo Feng and Penghui Wang and Hongwei Liu},
keywords = {Imbalanced RATR, Deep learning, Deep reinforcement learning (DRL), Supervised contrastive learning, Priority sampling},
abstract = {In the presence of limited and extremely imbalanced data, deep learning methods for radar automatic target recognition (RATR) often suffer from significant performance degradation and overfitting. To tackle this issue, we propose Supervised Contrastive Deep Q-network (SCDQ), a novel end-to-end reinforcement learning method, for multi-class imbalanced RATR. SCDQ formulates the imbalanced recognition problem as a Markov decision process (MDP) and optimizes the classifier through an enhanced Q-learning paradigm. In order to augment the model’s feature extraction capabilities under the constraint of limited samples, we tightly integrate reinforcement learning (RL) with supervised contrastive learning, introducing an innovative feature enhancement module. To further enhance the model’s adaptability to challenging samples, we integrate a meticulously designed priority sampling into the proposed SCDQ framework, denoted as SCDQ-P. Experimental results on both simulated and real datasets demonstrate the reliability and effectiveness of the proposed method.}
}
@article{SUN2025111210,
title = {Scalable and Adaptive Graph Neural Networks with Self-Label-Enhanced Training},
journal = {Pattern Recognition},
volume = {160},
pages = {111210},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111210},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009610},
author = {Chuxiong Sun and Jie Hu and Hongming Gu and Jinpeng Chen and Wei Liang and Mingchuan Yang},
keywords = {Graph Neural Networks, Deep learning, Representation learning, Semi-supervised learning, Label propagation},
abstract = {Although GNNs have achieved success in semi-supervised graph learning tasks, common GNNs suffer from expensive message passing during each epoch and the exponentially growing receptive field occupying too much memory, especially on large graphs. Neighbor sampling techniques can reduce GNNs’ memory footprints, but they encounter either redundant computation or incomplete edges. Some simplified GNNs decouple graph convolutions and feature transformations to reduce computation in training. However, only a part of them can scale to large graphs without neighbor sampling techniques, which can be concluded as decoupled GNNs. Nevertheless, they either only utilize the last convolution output or simply add multi-hop features with uniform weights, which limits their expressiveness. In this paper, we refine the pipeline of decoupled GNNs and propose Scalable and Adaptive Graph Neural Networks (SAGN), which effectively leverages multi-hop information with a scalable attention mechanism. Moreover, we generalize the input of decoupled GNNs to view another classical technique, label propagation, as a special case of decoupled GNNs and propose decoupled label trick (DecLT) to incorporate label information into decoupled GNNs. Furthermore, by incorporating self-training technique, we further propose the Self-Label-Enhanced (SLE) training framework, leveraging pseudo labels to simultaneously augment the training set and improve label propagation. Extensive experiments show that SAGN outperforms other baselines, and that DecLT and SLE can consistently and significantly improve all types of models on semi-supervised node classification tasks. Many top-ranked models on Open Graph Benchmark (OGB) leaderboard adopt our methods as the main backbone.}
}
@article{YAN2025111277,
title = {Spatial–spectral unfolding network with mutual guidance for multispectral and hyperspectral image fusion},
journal = {Pattern Recognition},
volume = {161},
pages = {111277},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111277},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010288},
author = {Jun Yan and Kai Zhang and Qinzhu Sun and Chiru Ge and Wenbo Wan and Jiande Sun and Huaxiang Zhang},
keywords = {Remote sensing, Unfolding network, Image fusion, Multispectral image, Hyperspectral image},
abstract = {Fusing low spatial resolution hyperspectral (LR HS) and high spatial resolution multispectral (HR MS) images from different modalities aim to obtain high spatial resolution hyperspectral (HR HS) images. However, most deep neural network (DNN)-based methods overlook the correlation between the spatial domain and spectral domain, leading to limited fusion performance. To solve this problem, we propose the spatial–spectral unfolding network with mutual guidance (SMGU-Net). Specifically, the information of different modalities in the source images is treated as mutual complementary components to derive the reconstruction model. Then, the model is optimized using half-quadratic splitting and gradient descent algorithms and is unfolded into a network that leverages the powerful learning capabilities of DNNs to explore more potential information in the deep feature space. In this way, the network achieves the interaction and supplementarity of cross-modality information to generate fused images. Experiments are conducted on four benchmark datasets to demonstrate the effectiveness of SMGU-Net. The code can be downloaded from https://github.com/yansql/SMGU-Net.}
}
@article{DING2025111206,
title = {KSOF: Leveraging kinematics and spatio-temporal optimal fusion for human motion prediction},
journal = {Pattern Recognition},
volume = {161},
pages = {111206},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111206},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009579},
author = {Rui Ding and KeHua Qu and Jin Tang},
keywords = {Human motion prediction, Kinematic constraints, Spatio-temporal optimal fusion},
abstract = {Ignoring the meaningful kinematics law, which generates improbable or impractical predictions, is one of the obstacles to human motion prediction. Current methods attempt to tackle this problem by taking simple kinematics information as auxiliary features to improve predictions. However, it remains challenging to utilize human prior knowledge deeply, such as the trajectory formed by the same joint should be smooth and continuous in this task. In this paper, we advocate explicitly describing kinematics information via velocity and acceleration by proposing a novel loss called joint point smoothness (JPS) loss, which calculates the acceleration of joints to smooth the sudden change in joint velocity. In addition, capturing spatio-temporal dependencies to make feature representations more informative is also one of the obstacles in this task. Therefore, we propose a dual-path network (KSOF) that models the temporal and spatial dependencies from kinematic temporal convolutional network (K-TCN) and spatial graph convolutional networks (S-GCN), respectively. Moreover, we propose a novel multi-scale fusion module named spatio-temporal optimal fusion (SOF) to enhance extraction of the essential correlation and important features at different scales from spatio-temporal coupling features. We evaluate our approach on three standard benchmark datasets, including Human3.6M, CMU-Mocap, and 3DPW datasets. For both short-term and long-term predictions, our method achieves outstanding performance on all these datasets. The code is available at https://github.com/qukehua/KSOF.}
}
@article{HUANG2024111293,
title = {IMWA: Iterative Model Weight Averaging benefits class-imbalanced learning},
journal = {Pattern Recognition},
pages = {111293},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111293},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010446},
author = {Zitong Huang and Ze Chen and Bowen Dong and Chaoqi Liang and Erjin Zhou and Wangmeng Zuo},
keywords = {Model weight averaging, Class-imbalanced learning},
abstract = {Model Weight Averaging (MWA) enhances model performance by averaging weights of multiple trained models. This paper shows that MWA (1) is beneficial for class-imbalanced learning, (2) with early-epoch averaging yielding the most improvement. Building on these insights, we propose Iterative Model Weight Averaging (IMWA) for class-imbalanced learning tasks. IMWA divides training into multiple episodes, within which multiple models are trained from the same initial weights and then averaged into a single model. This averaged model initializes the next episode, creating an iterative approach. IMWA offers higher performance improvements compared to MWA. Notably, several class-imbalanced learning methods use Exponential Moving Average (EMA) to gradually update models weight for improving performance. Our IMWA method synergizes effectively with EMA-based approaches, leading to enhanced overall performance. Extensive experiments validate IMWA’s effectiveness across various class-imbalanced learning tasks, including classification and object detection.}
}
@article{KE2025111222,
title = {Graph-based referring expression comprehension with expression-guided selective filtering and noun-oriented reasoning},
journal = {Pattern Recognition},
volume = {161},
pages = {111222},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111222},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009737},
author = {Jingcheng Ke and Qi Zhang and Jia Wang and Hongqing Ding and Pengfei Zhang and Jie Wen},
keywords = {Referring expression comprehension, Expression-guided selective and filtering module, Language-to-vision mapping, Noun-oriented reasoning},
abstract = {The objective of referring expression comprehension (REC) is to find the common feature domain between language expressions and visual objects. Due to the complex nature of modeling relationships between objects in images, graph-based methods are widely used for the REC task. However, during the process of graph construction, existing graph-based REC methods insufficiently harness the visual information associated with objects in images. Moreover, in modeling the relationships between objects, these methods consider only the relational words of the expression and the positions of the objects, while ignoring the objects themselves. Thus, they are sub-optimal in capturing underlying relationships between the objects and the expression, leading to incorrect predictions when given a complex expression. To address these issues, we propose a plug-and-adapt module called expression-guided selective and filtering module (EGSFM) for graph-based REC methods that constructs an expression-guided filter to adaptively select relevant and important visual features from feature maps of objects. Then, the selected visual object features and the textual features of the expression are jointly used for graph construction. Finally, a noun-oriented reasoning strategy is proposed for graph reasoning and target object matching, with the number of reasoning steps based on the number of nouns or noun phrases in the expression. Extensive experimental results on three challenging public datasets, including RefCOCO, RefCOCO+, and RefCOCOg, show that our method outperforms the compared graph-based methods and is robust to complex language expressions. In addition, our method performs favorably against other state-of-the-art transformer-based methods while consuming much fewer computational resources for training than those methods.}
}
@article{FAN2025111265,
title = {Complementary CatBoost based on residual error for student performance prediction},
journal = {Pattern Recognition},
volume = {161},
pages = {111265},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111265},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010161},
author = {Zongwen Fan and Jin Gou and Shaoyuan Weng},
keywords = {Complementary CatBoost, Residual error, Model integration, Student performance prediction, Educational data mining},
abstract = {Student performance prediction is crucial for early identification of potential students who may fail the final exam. In this paper, we propose a residual error-based Complementary CatBoost approach (C-CatBoost) for student performance prediction. Unlike the conventional CatBoost, our approach incorporates the evaluation of errors between the predicted values and target values. We design the residual error model to complement the performance of CatBoost. Besides, we integrate the CatBoost and the residual error model for the final prediction. The experimental results show that the C-CatBoost outperforms the comparing models in all the evaluated metrics. Specifically, our C-CatBoost achieves the lowest root mean square error of 1.1099 for predicting the final grades in Mathematics, surpassing the comparing models by 8.06% to 17.99% and achieves the value of 1.0246, outperforming the comparing models by 2.18% to 10.41% for the Portuguese language course. These promising results validate the effectiveness of complementary approach in enhancing student performance prediction, indicating the C-CatBoost could be a valuable tool within the educational institutions, contributing to the improvement of education quality.}
}
@article{ZHANG2025111253,
title = {Distilled transformers with locally enhanced global representations for face forgery detection},
journal = {Pattern Recognition},
volume = {161},
pages = {111253},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111253},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010045},
author = {Yaning Zhang and Qiufu Li and Zitong Yu and Linlin Shen},
keywords = {Deepfake detection, Transformer, Knowledge distillation, Mixture of expert, Multi-attention scaling},
abstract = {Face forgery detection (FFD) is devoted to detecting the authenticity of face images. Although current CNN-based works achieve outstanding performance in FFD, they are susceptible to capturing local forgery patterns generated by various manipulation methods. Though transformer-based detectors exhibit improvements in modeling global dependencies, they are not good at exploring local forgery artifacts. Hybrid transformer-based networks are designed to capture local and global manipulated traces, but they tend to suffer from the attention collapse issue as the transformer block goes deeper. Besides, soft labels are rarely available. In this paper, we propose a distilled transformer network (DTN) to capture both rich local and global forgery traces and learn general and common representations for different forgery faces. Specifically, we design a mixture of expert (MoE) module to mine various robust forgery embeddings. Moreover, a locally-enhanced vision transformer (LEVT) module is proposed to learn locally-enhanced global representations. We design a lightweight multi-attention scaling (MAS) module to avoid attention collapse, which can be plugged and played in any transformer-based models with only a slight increase in computational costs. In addition, we propose a deepfake self-distillation (DSD) scheme to provide the model with abundant soft label information. Extensive experiments show that the proposed method surpasses the state of the arts on five deepfake datasets.}
}
@article{MAO2025111280,
title = {Robust multi-view subspace clustering with missing data by aligning nonlinear manifolds},
journal = {Pattern Recognition},
volume = {161},
pages = {111280},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111280},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010318},
author = {Zhan-Wang Mao and Lu Sun and Youlong Wu},
keywords = {Multi-view learning, Subspace clustering, Incomplete view, Nonlinear manifold},
abstract = {We study the clustering of high-dimensional multi-view data with randomly missing features. Most existing methods employ the low-dimensional subspace assumption, which ignore the fact that data may reside close to multiple nonlinear manifolds, let alone nonlinear relationships between multiple views. Usually, they give multiple views equal weights, making them sensitive to redundant and noisy views. In most cases, completion and clustering are treated as separate processes, preventing them from reinforcing each other. To address these problems, we propose a Robust Nonlinear Multi-view Subspace Clustering and Completion (RNMSCC) algorithm, which projects multi-view data to high-dimensional feature spaces and integrates data completion and clustering therein. For data completion, the minimum intrinsic rank of sub-manifold is promoted while for clustering, an adaptive weighting technique is developed to automatically adjust the importance of multiple views in self-expression. Integrated with manifold alignment, redundant and noisy views are selected out, thus the learning process enjoys robust mutual reinforcement. The optimization problem is solved by an alternating algorithm. Experiments on real-world datasets validate its performance advantage over state-of-the-art methods.}
}
@article{CHEN2025111218,
title = {Cross-level interaction fusion network-based RGB-T semantic segmentation for distant targets},
journal = {Pattern Recognition},
volume = {161},
pages = {111218},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111218},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009695},
author = {Yu Chen and Xiang Li and Chao Luan and Weimin Hou and Haochen Liu and Zihui Zhu and Lian Xue and Jianqi Zhang and Delian Liu and Xin Wu and Linfang Wei and Chaochao Jian and Jinze Li},
keywords = {Semantic segmentation, Feature fusion, Cross modality, Multi-scale information, Distant object},
abstract = {RGB-T segmentation represents an innovative approach driven by advancements in multispectral detection and is poised to replace traditional RGB segmentation methods. An effective cross-modality feature fusion module is essential for this technology. The precise segmentation of distant objects is another significant challenge. Focused on these two areas, we propose an end-to-end distant object feature fusion network (DOFFNet) for RGB-T segmentation. Initially, we introduce a cross-level interaction fusion strategy (CLIF) and an inter-correlation fusion method (IFFM) in the encoder to enhance multi-scale feature expression and improve fusion accuracy. Subsequently, we propose a residual dense pixel convolution (R-DPC) in the decoder with a trainable upsampling unit that dynamically reconstructs information lost during encoding, particularly for distant objects whose features may vanish after pooling. Experimental results show that our DOFFNet achieves a top mean pixel accuracy of 75.8% and dramatically improves accuracy for four classes, including objects occupying as little as 0.2%–2% of total pixels. This improvement ensures more reliable and effective performance in practical applications, particularly in scenarios where small object detection is critical. Moreover, it demonstrates potential applicability in other fields like medical imaging and remote sensing.}
}
@article{LIU2025111308,
title = {IW-ViT: Independence-Driven Weighting Vision Transformer for out-of-distribution generalization},
journal = {Pattern Recognition},
volume = {161},
pages = {111308},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111308},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010598},
author = {Weifeng Liu and Haoran Yu and YingJie Wang and Baodi Liu and Dapeng Tao and Honglong Chen},
keywords = {Out-of-distribution generalization, Vision Transformer, Independence sample weighting, Feature decorrelation},
abstract = {Vision Transformer has shown excellent performance in various computer vision applications under the independently and identically distributed assumption. However, if the test distribution differs from the training distribution, the performance of the model drops significantly. To solve this problem, we propose to use independence sample weighting to improve the model’s out-of-distribution generalization ability. It learns a set of sample weights to eliminate the spurious correlation between irrelevant features and labels by eliminating the dependencies between features. Previous work based on independence sample weighting only learned sample weights from the final output of the feature extractor to optimize the model. Different from these works, we consider the difference in spurious correlations between different layers in the feature extraction process. Combining the modular architecture of ViT and independence sample weighting, we propose Independence-Driven Weighting Vision Transformer (IW-ViT) for out-of-distribution generalization. The IW-ViT is constructed by a specialized encoder block, IW-Block, where each IW-Block incorporates the independence sample weighting module. Every IW-Block learns a set of sample weights and generates weighted loss function to differentially eliminate the spurious correlations in different blocks. We conduct detailed verifications on various datasets. Experimental results demonstrate that IW-ViT significantly outperforms previous work in different OOD generalization settings.}
}
@article{SHI2025111197,
title = {Multi-branch feature transformation cross-domain few-shot learning for hyperspectral image classification},
journal = {Pattern Recognition},
volume = {160},
pages = {111197},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111197},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009488},
author = {Meilin Shi and Jiansi Ren},
keywords = {Hyperspectral image classification, Few-shot learning, Meta-learning, Domain adaptation},
abstract = {In the field of hyperspectral image (HSI) classification, a source dataset with ample labeled samples is commonly utilized to enhance the classification performance of a target dataset with few labeled samples. Existing few-shot learning (FSL) methods typically assume identical feature distribution in the source and target domains. However, since the classes of samples collected from different regions may vary considerably, it leads to a disparity in the feature distribution. To address the domain distribution shift between the source and target domains, a cross-domain FSL method based on multi-branch feature transformation (MBFT-CFSL) is proposed for HSI classification. First, the spectral–spatial features of the image are extracted by the multi-branch feature fusion module, and the feature diversity is increased using the featurewise transformation layers to boost the generalization performance of the model. Then, the conditional adversarial domain adaptation technique is employed for model training to lessen the impact of domain shift. Finally, the model is optimized by minimizing the maximum mean difference loss function to further diminish the distribution difference between the source and target domains. Experimental results on three distinct hyperspectral datasets validate the effectiveness of MBFT-CFSL, with the overall classification accuracy improved by 1.73%–5.45% compared to the suboptimal method. The source code is available at https://github.com/Ziyin2/MBFT-CFSL.}
}
@article{WANG2025111184,
title = {Exploring Latent Transferability of feature components},
journal = {Pattern Recognition},
volume = {160},
pages = {111184},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111184},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400935X},
author = {Zhengshan Wang and Long Chen and Juan He and Linyao Yang and Fei-Yue Wang},
keywords = {Unsupervised domain adaptation, Feature disentanglement, Adversarial learning, Dynamic learning},
abstract = {Feature disentanglement techniques have been widely employed to extract transferable (domain-invariant) features from non-transferable (domain-specific) features in Unsupervised Domain Adaptation (UDA). However, due to the complex interplay among high-dimensional features, the separated “non-transferable” features may still be partially informative. Suppressing or disregarding them, as commonly employed in previous methods, can overlook the inherent transferability. In this work, we introduce two concepts: Partially Transferable Class Features and Partially Transferable Domain Features (PTCF and PTDF), and propose a succinct feature disentanglement technique. Different with prior works, we do not seek to thoroughly peel off the non-transferable features, as it is challenging practically. Instead, we take the two-stage strategy consisting of rough feature disentanglement and dynamic adjustment. We name our model as ELT because it can systematically Explore Latent Transferability of feature components. ELT can automatically evaluate the transferability of internal feature components, dynamically giving more attention to features with high transferability and less to features with low transferability, effectively solving the problem of negative transfer. Extensive experimental results have proved its efficiency. The code and supplementary file will be available at https://github.com/njtjmc/ELT.}
}
@article{ZHANG2025111284,
title = {PSSCL: A progressive sample selection framework with contrastive loss designed for noisy labels},
journal = {Pattern Recognition},
volume = {161},
pages = {111284},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111284},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010355},
author = {Qian Zhang and Yi Zhu and Filipe R. Cordeiro and Qiu Chen},
keywords = {Noisy label, Deep neural networks, Semi-supervised learning, Image datasets},
abstract = {Large-scale image datasets frequently contain unavoidable noisy labels, resulting in overfitting in deep neural networks and declining performance. Most existing methods for learning from noisy labels operate as one-stage frameworks, where training data division and semi-supervised learning (SSL) are intertwined for optimization. Accordingly, their effectiveness is significantly influenced by the precision of the separated clean set, prior knowledge of noise, and the robustness of SSL. In this paper, we propose a progressive sample selection framework with contrastive loss for noisy labels named PSSCL. This framework operates in two stages, using robust and contrastive losses to augment the robustness of the model. Stage I focuses on identifying a small clean set through a long-term confidence detection strategy, while stage II aims to enhance performance by expanding this clean set. PSSCL demonstrates significant improvement across various benchmarks when compared with state-of-the-art methods. The code is available at https://github.com/LanXiaoPang613/PSSCL.}
}
@article{XIANG2025111241,
title = {Driver multi-task emotion recognition network based on multi-modal facial video analysis},
journal = {Pattern Recognition},
volume = {161},
pages = {111241},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111241},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009920},
author = {Guoliang Xiang and Song Yao and Xianhui Wu and Hanwen Deng and Guojie Wang and Yu Liu and Fan Li and Yong Peng},
keywords = {Video analysis, Multi-modal information fusion, Multi-task learning, Driver emotion, Remote physiological signal extraction},
abstract = {Driver emotion recognition is crucial for enhancing the safety and user experience in driving scenarios. However, current emotion recognition methods often rely solely on a single modality and a single-task setup, leading to suboptimal performance in driving scenarios. To address this, this paper proposes a driver multitask emotion recognition method based on multimodal facial video analysis (MER-MFVA). This method extracts facial expression features and remote photoplethysmography (rPPG) signals from driver facial videos. Facial expression features include facial action units and eye movement information, representing the driver's external characteristics. rPPG information, representing the driver's internal characteristics, is enhanced through a designed dual-path Transformer network and an introduced focus module. We also propose a cross-modal mutual attention computation mechanism to effectively fuse multimodal features by calculating mutual attention between facial expression features and rPPG information. In the final task output, we employ a multitask learning mechanism, setting discrete emotion recognition as the primary task and emotion valence recognition, emotion arousal recognition, and the previous rPPG information extraction as auxiliary tasks to facilitate effective information sharing across different tasks. Experimental results on the established driver emotion dataset demonstrate that our proposed method significantly improves driver emotion recognition performance, achieving an accuracy of 86.98% and an F1 score of 85.83% in the primary task. This validates the effectiveness of the proposed approach.}
}
@article{V2024111272,
title = {Deep Vision-Based Wildlife Intrusion Detection with Colour Distribution Preserved Generative Adversarial Networks},
journal = {Pattern Recognition},
pages = {111272},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111272},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010239},
author = {Mr. Moorthy V and Dr. Rukkumani V},
keywords = {Color Distribution Preserved Generative Adversarial Networks, Data-adaptive Gaussian Average Filtering, Elephant, Red Panda Optimization Algorithm, Wildlife Intrusion Detection},
abstract = {Animal preservation is a significant issue and the human-elephant collisions (HEC) remains unidentified. Rail track is placed in forested areas and elephants are often struck by trains due to their bulk. These kinds of disasters are common in the southern green belt of India. To overcome these problems, this paper proposes a Deep Vision-Based Wildlife Intrusion Detection with Colour Distribution Preserved Generative Adversarial Networks (DV-WID-CDPGAN). Here, the input images are taken from real time video, which are converted into individual frames. The input data is pre-processed under Data-adaptive Gaussian Average Filtering (DAGAF) to remove noise. Edge, line and rectangle Features are extracted utilizing Dual Tree Complex Discrete Wavelet Transform (DTCDT). Then elephant present and not present is classified using Color Distribution Preserved Generative Adversarial Networks (CDPGAN). The Red Panda Optimization Algorithm (RPOA) optimizes the CDPGAN classifier for precise categorization. The proposed DV-WID-CDPGAN method is evaluated and compared with existing methods.}
}
@article{WANG2025111213,
title = {Multi-level network Lasso for multi-task personalized learning},
journal = {Pattern Recognition},
volume = {161},
pages = {111213},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111213},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009646},
author = {Jiankun Wang and Luhuan Fei and Lu Sun},
keywords = {Multi-level network Lasso, Personalized learning, Multi-task learning},
abstract = {We propose the multi-level network Lasso, which aims to overcome the key limitations of existing personalized learning methods, such as ignoring sample homogeneity or heterogeneity, and over-parametrization. Multi-level network Lasso learns both sample-common model and sample-specific model, that are succinct and interpretable in the sense that model parameters are shared across neighboring samples based on only a subset of relevant features. To apply personalized learning in multi-task scenarios, we further extend the multi-level network Lasso for multi-task personalized learning by learning underlying task groups in the feature subspace. Additionally, we investigate a family of the multi-level network Lasso based on the ℓp quasi-norm (0<p<1), that helps prevent over-penalization on large group outliers. An alternating algorithm is developed to efficiently solve the proposed optimization problem. Experimental results on synthetic and real-world datasets demonstrate the effectiveness of the proposed method.}
}
@article{WANG2025111295,
title = {SoftPatch+: Fully unsupervised anomaly classification and segmentation},
journal = {Pattern Recognition},
volume = {161},
pages = {111295},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111295},
url = {https://www.sciencedirect.com/science/article/pii/S003132032401046X},
author = {Chengjie Wang and Xi Jiang and Bin-Bin Gao and Zhenye Gan and Yong Liu and Feng Zheng and Lizhuang Ma},
keywords = {Anomaly detection, Unsupervised learning, Learn with noise},
abstract = {Although mainstream unsupervised anomaly detection (AD) (including image-level classification and pixel-level segmentation) algorithms perform well in academic datasets, their performance is limited in practical application due to the ideal experimental setting of clean training data. Training with noisy data is an inevitable problem in real-world anomaly detection but is seldom discussed. This paper is the first to consider fully unsupervised industrial anomaly detection (i.e., unsupervised AD with noisy data). To solve this problem, we proposed memory-based unsupervised AD methods, SoftPatch and SoftPatch+, which efficiently denoise the data at the patch level. Noise discriminators are utilized to generate outlier scores for patch-level noise elimination before coreset construction. The scores are then stored in the memory bank to soften the anomaly detection boundary. Compared with existing methods, SoftPatch maintains a strong modeling ability of normal data and alleviates the overconfidence problem in coreset, and SoftPatch+ has more robust performance which is particularly useful in real-world industrial inspection scenarios with high levels of noise (from 10% to 40%). Comprehensive experiments conducted in diverse noise scenarios demonstrate that both SoftPatch and SoftPatch+ outperform the state-of-the-art AD methods on the MVTecAD, ViSA, and BTAD benchmarks. Furthermore, the performance of SoftPatch and SoftPatch+ is comparable to that of the noise-free methods in conventional unsupervised AD setting. The code of the proposed methods can be found at https://github.com/TencentYoutuResearch/AnomalyDetection-SoftPatch.}
}
@article{WANG2024111300,
title = {CTPT: Continual Test-time Prompt Tuning for vision-language models},
journal = {Pattern Recognition},
pages = {111300},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111300},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010513},
author = {Fan Wang and Zhongyi Han and Xingbo Liu and Yilong Yin and Xin Gao},
keywords = {Test-time adaptation, Contrastive Language-Image Pretraining (CLIP), Test-time Prompt Tuning, Stable self-learning},
abstract = {Test-time Prompt Tuning (TPT) aims to further enhance the generalization capabilities of pre-trained vision-language models, e.g., CLIP, on streaming test samples from a new distribution. Current TPT methods primarily utilize self-training techniques in an episodic manner, involving the resetting of prompt parameters to their initial state after each TTA process to maintain the high zero-shot performance of the CLIP model. However, this approach leads to the prompt adapting only to the distribution of the current data batch rather than the entire streaming data, resulting in poor generalization. Therefore, this paper introduces Continual Test-time Prompt Tuning (CTPT), which aims to learn prompts that excel during test time and effectively generalize to all newly learned streaming data. To achieve the objectives of CTPT, we adopt the strategy of iteratively tuning prompts. We observe that existing self-training and TPT methods often experience performance declines due to inaccurate predictions in iterative learning, resulting in ineffective prompts. To address this issue, we propose the prototype-facilitated continuous prompt tuning (PCPT) method, which leverages the robust feature extraction capability of the image encoder in CLIP model to construct stable class prototypes and generate accurate pseudo-labels for iterative prompt tuning. Furthermore, PCPT introduces a diverse prototype loss to prevent model bias towards certain classes, ensuring the robustness of class prototypes and stable pseudo-labels. Experimental results across zero-shot test-time adaptation and test-time generalization demonstrate that PCPT outperforms current TPT methods.}
}
@article{ZHANG2025111229,
title = {MAGIC: Multi-granularity domain adaptation for text recognition},
journal = {Pattern Recognition},
volume = {161},
pages = {111229},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111229},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009804},
author = {Jia-Ying Zhang and Xiao-Qian Liu and Zhi-Yuan Xue and Xin Luo and Xin-Shun Xu},
keywords = {Text recognition, Unsupervised domain adaptation, Entropy minimization, Multi-granularity prediction},
abstract = {Domain gaps between synthetic text and real-world text restrict current text recognition methods. One solution is to align features through Unsupervised Domain Adaptation (UDA). Most existing UDA-based text recognition methods extract global and local features to alleviate domain differences, only focusing on character-level distribution gaps. However, notable distribution gaps in character combinations exert a pivotal influence on diverse text recognition tasks. To this end, we propose a Multi-level And multi-Granularity domain adaptation with entropy loss guIded text reCognition model, named MAGIC. It integrates Global-level Domain Adaptation (GDA) to mitigate image-level domain drift and Local-level Multi-granularity Domain Adaptation (LMDA) for local feature shifts. Particularly, we design a subword-level domain discriminator to align the subword features relating to each character combination. Moreover, multi-granularity entropy minimization is used to optimize the target domain data for better domain adaptation. Experimental results on several types of text datasets demonstrate the effectiveness of MAGIC.}
}
@article{WANG2025111296,
title = {A semantic structure-based emotion-guided model for emotion-cause pair extraction},
journal = {Pattern Recognition},
volume = {161},
pages = {111296},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111296},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010471},
author = {Yuwei Wang and Yuling Li and Kui Yu and Jing Yang},
keywords = {Emotion-cause pair extraction, Emotion type, Semantic structure},
abstract = {Emotion-cause pair extraction (ECPE) task aims to identify both emotion and cause clauses in a given document, which has received growing attention in recent years. Existing ECPE methods primarily focus on learning general clause representations by exploring interactions between words or clauses. Despite the remarkable achievements in these works, they neglect the distinct emotion types of clauses and semantic structures associated with each individual clause, which may affect the model’s accuracy in grasping emotion and cause clues. In this paper, we propose a novel Semantic Structure-based Emotion-Guided (SEG) model, which integrates the information from both emotion types and semantic structures to capture relations between emotion and cause clauses. In specific, we design an emotion detection module to identify the emotion types of clauses and learn the emotion-enhanced clause representations. Furthermore, the module can help match clauses with similar emotion types that are more likely to form an emotion-cause pair, and promote the extraction performance of emotion-cause pairs. To model the complex semantic structures within the documents, we propose a semantic structure-based graph module to learn the relevant relationships of clauses and obtain semantic structure clause representations, which further facilitates the extraction of cause clauses. Experimental results on the public dataset demonstrate the effectiveness of our proposed SEG in the ECPE task when compared with state-of-the-art baselines.}
}
@article{FEI2025111225,
title = {Semantic decomposition and enhancement hashing for deep cross-modal retrieval},
journal = {Pattern Recognition},
volume = {160},
pages = {111225},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111225},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009762},
author = {Lunke Fei and Zhihao He and Wai Keung Wong and Qi Zhu and Shuping Zhao and Jie Wen},
keywords = {Cross-modal retrieval, Semantic decomposition, Deep cross-modal hashing, Multi-label semantic learning},
abstract = {Deep hashing has garnered considerable interest and has shown impressive performance in the domain of retrieval. However, the majority of the current hashing techniques rely solely on binary similarity evaluation criteria to assess the semantic relationships between multi-label instances, which presents a challenge in overcoming the feature gap across various modalities. In this paper, we propose semantic decomposition and enhancement hashing (SDEH) by extensively exploring the multi-label semantic information shared by different modalities for cross-modal retrieval. Specifically, we first introduce two independent attention-based feature learning subnetworks to capture the modality-specific features with both global and local details. Subsequently, we exploit the semantic features from multi-label vectors by decomposing the shared semantic information among multi-modal features such that the associations of different modalities can be established. Finally, we jointly learn the common hash code representations of multimodal information under the guidelines of quadruple losses, making the hash codes informative while simultaneously preserving multilevel semantic relationships and feature distribution consistency. Comprehensive experiments on four commonly used multimodal datasets offer strong support for the exceptional effectiveness of our proposed SDEH.}
}
@article{WANG2025111173,
title = {Distilling heterogeneous knowledge with aligned biological entities for histological image classification},
journal = {Pattern Recognition},
volume = {160},
pages = {111173},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111173},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009245},
author = {Kang Wang and Feiyang Zheng and Dayan Guan and Jia Liu and Jing Qin},
keywords = {Histological image classification, Knowledge distillation, Heterogeneous biological entities, Graph neural network, Biological affiliation recognition},
abstract = {In the task of classifying histological images, prior works widely leverage Graph neural network (GNN) to aggregate histological knowledge from multi-level biological entities (e.g., cell and tissue). However, current GNN-based methods suffer from either inadequate entity representation or intolerable computation burden. To the end, we propose a heterogeneous knowledge distillation (HKD) model to capture and amalgamate the spatial-hierarchical feature of multi-level biological entities. We first design multiple message-passing GNNs with different hidden layers as the teachers for extracting adjacent regions of cells, and leverage a transformer-based GNN as the student to model the global interaction of tissues. Such multi-teacher student architecture enables our HKD to simultaneously obtain topological knowledge at different scales from heterogeneous biological entities. We further propose a biological affiliation recognition module to adaptively align the cell knowledge learned from multi-teacher models with cell-corresponding tissue in the student model, encouraging the student model to attentively amalgamate the semantics of multi-level biological entities for highly accurate classification. Extensive experiments show that our method outperforms the state-of-the-art on three public datasets of histological image classification.}
}
@article{DU2025111283,
title = {Text generation and multi-modal knowledge transfer for few-shot object detection},
journal = {Pattern Recognition},
volume = {161},
pages = {111283},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111283},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010343},
author = {Yaoyang Du and Fang Liu and Licheng Jiao and Shuo Li and Zehua Hao and Pengfang Li and Jiahao Wang and Hao Wang and Xu Liu},
keywords = {Few-shot learning, Few-shot object detection, Multi-modal},
abstract = {The challenge of detecting novel categories with limited annotated samples for learning is referred to as few-shot object detection (FSOD). Due to the scarcity of data, networks struggle to learn robust features that effectively represent object categories from limited samples. Currently, most methods addressing FSOD tasks rely solely on a single modality in network design, overlooking the advanced semantic relationships between categories and their descriptions. We propose a method named Text Generation and Multi-Modal Knowledge Transfer for Few-Shot Object Detection (MMKT). This approach employs a Text Prompt Descriptors Generator (TPDG) to generate prompts tailored to specific categories, reducing the reliance on particular knowledge sources observed in previous methods. To better understand the relationship between text-based descriptions and visual features in a shared space, we develop an Image–Text Matching module to establish correlations between textual and visual features. Experiments demonstrate the effectiveness of MMKT method.}
}
@article{XU2024111307,
title = {Graph neural network based on graph kernel: A survey},
journal = {Pattern Recognition},
pages = {111307},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111307},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010586},
author = {Lixiang Xu and Jiawang Peng and Xiaoyi Jiang and Enhong Chen and Bin Luo},
keywords = {Graph kernel, Graph neural network, Fusion strategy, Graph representation learning, Classification ability},
abstract = {Graph data are pervasive in real-world scenarios, and research on graph data has become a research hotspot. Over the past few decades, significant advancements have been made in the graph domain, particularly in the development of graph kernels and graph neural networks. But they also face challenges, such as graph kernel is difficult to learn complex interactions, and too many parameters of graph neural network lead to poor optimization, etc. Therefore, integrating them has become an important strategy. Existing reviews in the published literature primarily concentrate on either graph kernels or graph neural networks individually, with no mention of the graph neural network methods based on graph kernels. This paper starts from the basic knowledge, presents the challenges they encounter, and analyzes the existence of complementary perspectives between them, thus confirming the feasibility of the integration strategy. Following this, this paper organizes some important methods of graph neural networks based on graph kernels in recent years in terms of expressiveness, performance, and applications. In addition we have substantiated the actual effectiveness with experimental results. Lastly, we explore future research directions. We have also collected papers and open source code resources on graph kernel based graph neural network methods in recent years at https://github.com/bigdata-graph/GNN_GK.}
}
@article{ZHANG2025111212,
title = {Unsupervised evaluation for out-of-distribution detection},
journal = {Pattern Recognition},
volume = {160},
pages = {111212},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111212},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009634},
author = {Yuhang Zhang and Jiani Hu and Dongchao Wen and Weihong Deng},
keywords = {Unsupervised evaluation, Out-of-distribution detection},
abstract = {We need to acquire labels for test sets to evaluate the performance of existing out-of-distribution (OOD) detection methods. In real-world deployment, it is laborious to label each new test set as there are various OOD data with different difficulties. However, we need to use different OOD data to evaluate OOD detection methods as their performance varies widely. Thus, we propose evaluating OOD detection methods on unlabeled test sets, which can free us from labeling each new OOD test set. It is a non-trivial task as we do not know which sample is correctly detected without OOD labels, and the evaluation metric like AUROC cannot be calculated. In this paper, we address this important yet untouched task for the first time. Inspired by the bimodal distribution of OOD detection test sets, we propose an unsupervised indicator named Gscore that has a certain relationship with the OOD detection performance; thus, we could use neural networks to learn that relationship to predict OOD detection performance without OOD labels. Through extensive experiments, we validate that there does exist a strong quantitative correlation, which is almost linear, between Gscore and the OOD detection performance. Additionally, we introduce Gbench, a new benchmark consisting of 200 different real-world OOD datasets, to test the performance of Gscore. Our results show that Gscore achieves state-of-the-art performance compared with other unsupervised evaluation methods and generalizes well with different in-distribution (ID)/OOD datasets, OOD detection methods, backbones, and ID:OOD ratios. Furthermore, we conduct analyses on Gbench to study the effects of backbones and ID/OOD datasets on OOD detection performance. The dataset and code will be available.}
}
@article{SHI2025111299,
title = {Real face foundation representation learning for generalized deepfake detection},
journal = {Pattern Recognition},
volume = {161},
pages = {111299},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111299},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010501},
author = {Liang Shi and Jie Zhang and Zhilong Ji and Jinfeng Bai and Shiguang Shan},
keywords = {Deepfake detection, Masked image modeling, Representation learning, Vision Transformers},
abstract = {The emergence of deepfake technologies has become a matter of social concern as they pose threats to individual privacy and public security. It is now of great significance to develop reliable deepfake detectors. However, with numerous face manipulation algorithms present, existing detectors fail to generalize to all types of manipulated faces. To address this, we propose an alternative method that primarily models the distribution of real faces. Our approach, named Real Face Foundation Representation Learning (RFFR), aims to learn a general representation from large-scale real face datasets and identifies potential artifacts that deviate from the distribution of RFFR. Specifically, we train a model on real face datasets by masked image modeling (MIM). When applying the model on fake samples, we observe clear discrepancies between input faces and the reconstructed ones, which reveals artifacts absent in the RFFR distribution. However, these discrepancies are significantly less evident in real faces, which makes it easier to build a deepfake detector sensitive to a wide range of potential artifacts. Extensive experiments demonstrate that our method achieves better generalization performance, as it significantly outperforms the state-of-the-art methods in cross-manipulation evaluations, and has the potential to further improve by introducing extra real faces for training RFFR.}
}
@article{ZHAO2025111271,
title = {Generalizable 3D Gaussian Splatting for novel view synthesis},
journal = {Pattern Recognition},
volume = {161},
pages = {111271},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111271},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010227},
author = {Chuyue Zhao and Xin Huang and Kun Yang and Xue Wang and Qing Wang},
keywords = {Novel view synthesis, 3D Gaussian Splatting, Generalizable scene representation, Image-based rendering},
abstract = {We present a generalizable 3D Gaussian Splatting (3DGS) method that can synthesize novel views of unseen scenes. Existing methods directly input image features into the parameter regression network without establishing a connection to the 3D representation, leading to inaccurate parameter predictions and artifacts in the rendered views. To address this issue, our method integrates spatial information from multiple source views. Specifically, by leveraging multi-view feature mapping to bridge 2D features with 3D representations, our method directly align the Gaussians with image features. The well-aligned features provide guidance for the accurate prediction of Gaussian parameters, thereby enhancing the ability to represent unseen scenes and alleviating artifacts caused by feature sampling ambiguity. The proposed framework is fully differentiable and allows optimizing Gaussian parameters in a feed-forward manner. After training on a large dataset of real-world scenes, our method enables novel view synthesis of unseen scenes without the need for optimization. Experimental results on real-world datasets demonstrate that our method outperforms recent novel view synthesis methods that also seek to generalize to unseen scenes.}
}
@article{YU2025111200,
title = {Reviving undersampling for long-tailed learning},
journal = {Pattern Recognition},
volume = {161},
pages = {111200},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111200},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009518},
author = {Hao Yu and Yingxiao Du and Jianxin Wu},
keywords = {Image classification, Long-tailed learning, Undersampling},
abstract = {The training datasets used in long-tailed recognition are extremely unbalanced, resulting in significant variation in per-class accuracy across categories. Prior works mostly used average accuracy to evaluate their algorithms, which easily ignores those worst-performing categories. In this paper, we aim to enhance the accuracy of the worst-performing categories and utilize the harmonic mean and geometric mean to assess the model’s performance. We revive the balanced undersampling idea to achieve this goal. In few-shot learning, balanced subsets are few-shot and will surely under-fit, hence it is not used in modern long-tailed learning. But, we find that it produces a more equitable distribution of accuracy across categories with much higher harmonic and geometric mean accuracy, but with lower average accuracy. Moreover, we devise a straightforward model ensemble strategy, which does not result in any additional overhead and achieves improved harmonic and geometric mean while keeping the average accuracy almost intact when compared to state-of-the-art long-tailed learning methods. We validate the effectiveness of our approach on widely utilized benchmark datasets for long-tailed learning. Our code is at https://github.com/yuhao318/BTM/.}
}
@article{YANG2025111278,
title = {Adaptively bypassing vision transformer blocks for efficient visual tracking},
journal = {Pattern Recognition},
volume = {161},
pages = {111278},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111278},
url = {https://www.sciencedirect.com/science/article/pii/S003132032401029X},
author = {Xiangyang Yang and Dan Zeng and Xucheng Wang and You Wu and Hengzhou Ye and Qijun Zhao and Shuiwang Li},
keywords = {Efficient visual tracking, Adaptively bypassing, Pruning},
abstract = {Empowered by transformer-based models, visual tracking has advanced significantly. However, the slow speed of current trackers limits their applicability on devices with constrained computational resources. To address this challenge, we introduce ABTrack, an adaptive computation framework that adaptively bypassing transformer blocks for efficient visual tracking. The rationale behind ABTrack is rooted in the observation that semantic features or relations do not uniformly impact the tracking task across all abstraction levels. Instead, this impact varies based on the characteristics of the target and the scene it occupies. Consequently, disregarding insignificant semantic features or relations at certain abstraction levels may not significantly affect the tracking accuracy. We propose a Bypass Decision Module (BDM) to determine if a transformer block should be bypassed, which adaptively simplifies the architecture of ViTs and thus speeds up the inference process. To counteract the time cost incurred by the BDMs and further enhance the efficiency of ViTs, we introduce a novel ViT pruning method to reduce the dimension of the latent representation of tokens in each transformer block. Extensive experiments on multiple tracking benchmarks validate the effectiveness and generality of the proposed method and show that it achieves state-of-the-art performance. Code is released at: https://github.com/xyyang317/ABTrack.}
}
@article{ZHANG2025111208,
title = {Consistency and label constrained transfer low-rank representation for cross-light finger vein recognition},
journal = {Pattern Recognition},
volume = {161},
pages = {111208},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111208},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009592},
author = {Zhen Zhang and Lu Yang and Kuikui Wang and Xiaoming Xi and Xiushan Nie and Gongping Yang and Yilong Yin},
keywords = {Biometrics, Cross-light finger vein recognition, Low-rank representation, Transfer learning, Consistency constraint},
abstract = {Finger vein sensors are embedded into all kinds of electronic devices for personal identification, and the upgrading of sensors is unavoidable. Therefore, the concern about cross-sensor finger vein recognition is raised recently. However, little attention is paid to cross-sensor finger vein recognition. The imaging light variation is one main difference between different sensors, and it brings large image differences, seriously degrading finger vein recognition performance. This paper focuses on cross-light finger vein recognition problem, in which we assume that the training and testing finger vein images are captured by different near-infrared lights, and proposes a consistency and label constrained transfer low-rank representation (CLTLRR) method for dealing with cross-light finger vein recognition. In the proposed method, we first transfer cross-light finger vein images into a common feature space to narrow the gap between training images and testing images, and achieve the low-rank linear representations of images. Then, we develop a consistency constraint between the low-rank coefficients in the common feature space and the sparse coefficients in the original feature space to enhance the discrimination of linear representation. In addition, we design a class label constraint for the projection matrix to guide image transfer. Finally, the low-rank coefficients and the projected features in the common feature space are integrated for recognition. Experiments are performed on single-light and cross-light finger palmar vein databases and finger dorsal vein databases, and the experimental results prove the effectiveness of our CLTLRR.}
}
@article{SHI2025111224,
title = {LDH-ViT: Fine-grained visual classification through local concealment and feature selection},
journal = {Pattern Recognition},
volume = {161},
pages = {111224},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111224},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009750},
author = {Yanli Shi and Qihua Hong and Yong Yan and Jing Li},
keywords = {Vision transformer, Fine-grained image recognition, Local Patch Concealment, Depth Feature Selection},
abstract = {Many methods based on Vision Transformer (ViT) have been proposed and applied in fine-grained visual classification (FGVC). It is of critical importance to improve the recognition accuracy and generalization ability of computer vision systems. However, the ViT model faces the challenges of increasing the amount of noise calculation, difficulty dealing with small targets with complex, diverse, and subtle characteristics in the images of the dataset, insufficient extraction of fine-grained information, and the impact of a few information patches on accuracy. To solve these problems an LDH-ViT model, which includes three new components. The LPC module randomly overshadows some of the small blocks separated from the ViT by a preset ratio. Subsequently, the local features of these new small blocks are calculated through training, aiming to reduce the influence of noise on the calculation process and further strengthen the difference between the images. The DFS module innovatively introduces a similarity selection layer, which aims to accurately identify and screen tokens with richer target information, thereby optimizing the information processing capability of the model. At the same time, the HMO module successfully constructs a new loss function based on the above improvements through clever fusion and optimization strategy. Comprehensive experiments on four widely used data sets show that LDH-ViT can achieve competitive performance.}
}
@article{HUSSAIN2025111248,
title = {Few-shot based learning recaptured image detection with multi-scale feature fusion and attention},
journal = {Pattern Recognition},
volume = {161},
pages = {111248},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111248},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009993},
author = {Israr Hussain and Shunquan Tan and Jiwu Huang},
keywords = {Recaptured image detection, Few-shot learning, Multi-scale attention, Attention mechanism, Feature aggregation, Efficient-Net-models},
abstract = {Advancements in LCD screen resolution and digital camera technology have significantly improved the quality of recaptured images, making it increasingly challenging to differentiate them from original images. Previous approaches to recaptured image detection have primarily focused on generating large-scale datasets of recaptured images and utilizing deep-learning techniques for detection. However, in rare and emerging scenarios, such as detecting image manipulation in medical images associated with rare diseases, identifying forgeries in precious art, and addressing new social media trends, the availability of recaptured image datasets is often limited. To address these challenges, a few-shot learning approach for recaptured image detection is proposed, which effectively captures local and global information by introducing attention mechanisms at different scales. This enhances the feature representation capability and enables the identification of subtle differences between recaptured and original images. Three complementary modules are designed: parallel multi-scale feature fusion, cascade multi-scale feature fusion, and an attention mechanism, to improve feature representation, capture key information, and highlight significant features, thereby enhancing detection accuracy. Experimental results confirm the superior performance of our method in detecting recaptured images, particularly with limited training samples, effectively addressing the challenges in detecting recaptured images with few labeled samples.}
}
@article{LEE2025111196,
title = {Recursive reservoir concatenation for salt-and-pepper denoising},
journal = {Pattern Recognition},
volume = {160},
pages = {111196},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111196},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009476},
author = {In-mo Lee and Yoojeung Kim and Taehoon Kim and Hayoung Choi and Seung Yeop Yang and Yunho Kim},
keywords = {Salt-and-pepper noise, Reservoir computing, Image restoration, Small data},
abstract = {We propose a recursive reservoir concatenation architecture in reservoir computing for salt-and-pepper noise removal. The recursive algorithm consists of two components. One is the initial network training for the recursion. Since the standard reservoir computing does not appreciate images as input data, we designed a nonlinear image-specific forward operator that can extract image features from noisy input images, which are to be mapped into a reservoir for training. The other is the recursive reservoir concatenation to further improve the reconstruction quality. Training errors decrease as more reservoirs are concatenated due to the hierarchical structure of the recursive reservoir concatenation. The proposed method outperformed most analytic or machine-learning based denoising models for salt-and-pepper noise with a training cost much lower than other neural network-based models. Reconstruction is completely parallel, in that noise in different pixels can be removed in parallel.}
}