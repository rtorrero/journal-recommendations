@article{XU2024110042,
title = {Two-stage fine-grained image classification model based on multi-granularity feature fusion},
journal = {Pattern Recognition},
volume = {146},
pages = {110042},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110042},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007392},
author = {Yang Xu and Shanshan Wu and Biqi Wang and Ming Yang and Zebin Wu and Yazhou Yao and Zhihui Wei},
keywords = {Fine-grained, Transformer, Feature-fusion, Attention mechanism},
abstract = {Fine-grained visual classification (FGVC) is a difficult task due to the challenges of discriminative feature learning. Most existing methods directly use the final output of the network which always contains the global feature with high-level semantic information. However, the differences between fine-grained images are reflected in subtle local regions which often appear in the front of the network. When the texture of the background and object are similar or the proportion of the background is too large, the prediction will be greatly affected. In order to solve the above problems, this paper proposes multi-granularity feature fusion module (MGFF) and two-stage classification based on Vision-Transformer (ViT). The former comprehensively represents images by fusing features of different granularities, thus avoiding the limitations of single-scale features. The latter leverages the ViT model to separate the object from the background at a very small cost, thereby improving the accuracy of the prediction. We conduct comprehensive experiments and achieves the best performance in two fine-grained tasks on CUB-200-2011 and NA-Birds.}
}
@article{QIAO2024110058,
title = {Hierarchical image-to-image translation with nested distributions modeling},
journal = {Pattern Recognition},
volume = {146},
pages = {110058},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110058},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007550},
author = {Shishi Qiao and Ruiping Wang and Shiguang Shan and Xilin Chen},
keywords = {Image-to-image translation, Distribution modeling, Information entropy, Generative adversarial network},
abstract = {Unpaired image-to-image translation among category domains has achieved remarkable success in past decades. Recent studies mainly focus on two challenges. For one thing, such translation is inherently multi-modal (i.e. many-to-many mapping) due to variations of domain-specific information (e.g., the domain of house cat contains multiple sub-modes), which is usually addressed by predefined distribution sampling. For another, most existing multi-modal approaches have limits in handling more than two domains with one model, i.e. they have to independently build two distributions to capture variations for every pair of domains. To address these problems, we propose a Hierarchical Image-to-image Translation (HIT) method which jointly formulates the multi-domain and multi-modal problem in a semantic hierarchy structure by modeling a common and nested distribution space. Specifically, domains have inclusion relationships under a particular hierarchy structure. With the assumption of Gaussian prior for domains, distributions of domains at lower levels capture the local variations of their ancestors at higher levels, leading to the so-called nested distributions. To this end, we propose a nested distribution loss in light of the distribution divergence measurement and information entropy theory to characterize the aforementioned inclusion relations among domain distributions. Experiments on ImageNet, ShapeNet, and CelebA datasets validate the promising results of our HIT against state-of-the-arts, and as additional benefits of nested modeling, one can even control the uncertainty of multi-modal translations at different hierarchy levels.}
}
@article{YUAN2024110006,
title = {Weighted side-window based gradient guided image filtering},
journal = {Pattern Recognition},
volume = {146},
pages = {110006},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110006},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007033},
author = {Weimin Yuan and Cai Meng and Xiangzhi Bai},
keywords = {Guided filtering, Side window framework, Edge-preserving, Weighted aggregation},
abstract = {Image filtering under guidance image, known as guided filtering (GF), has been successfully applied to a variety of applications. Existing GF methods utilize either conventional full window-based framework (FWF) or simple uniformly weighted aggregation strategy (UWA); thereby they suffer from edge-blurring. In this paper, based upon gradient guided filtering (GGF), a weighted side-window based gradient guided filtering (WSGGF) is proposed to address the aforementioned problem. First, both regression and adaptive regularization terms in GGF are improved upon eight side windows by introducing side window-based framework (SWF). L1 norm is adopted to choose the results calculated in side windows. Second, UWA strategy in GGF is replaced by a refined variance-based weighted average (VWA) aggregation. In VWA, the value of each weight is chosen inversely proportional to the corresponding estimator. We show that with these improvements our method can well retain the edge sharpness and is robust to visual artifacts. To cut down the time consumption, a fast version of WSGGF (FWSGGF) is further proposed by incorporating a simple but effective down-sampling strategy, which is about four times faster while maintaining the superior performance. By comparing with the state-of-the-art (SOTA) methods on edge-aware smoothing, detail enhancement, high dynamic range image (HDR) compression, image luminance adjustment, depth map upsampling and single image haze removal, the effectiveness and flexibility of our proposed methods are verified. The source code is available at: https://github.com/weimin581/WSGGF}
}
@article{MIGENDA2024110030,
title = {Adaptive local Principal Component Analysis improves the clustering of high-dimensional data},
journal = {Pattern Recognition},
volume = {146},
pages = {110030},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110030},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007276},
author = {Nico Migenda and Ralf Möller and Wolfram Schenck},
keywords = {High-dimensional clustering, Potential function, Adaptive learning rate, Ranking criteria, Neural network-based PCA, Mixture PCA, Local PCA},
abstract = {In local Principal Component Analysis (PCA), a distribution is approximated by multiple units, each representing a local region by a hyper-ellipsoid obtained through PCA. We present an extension for local PCA which adaptively adjusts both the learning rate of each unit and the potential function which guides the competition between the local units. Our local PCA method is an online neural network method where unit centers and shapes are modified after the presentation of each data point. For several benchmark distributions, we demonstrate that our method improves the overall quality of clustering, especially for high-dimensional distributions where many conventional methods do not perform satisfactorily. Our online method is also well suited for the processing of streaming data: The two adaptive mechanisms lead to a quick reorganization of the clustering when the underlying distribution changes.}
}
@article{WAN2024110074,
title = {TMNet: Triple-modal interaction encoder and multi-scale fusion decoder network for V-D-T salient object detection},
journal = {Pattern Recognition},
volume = {147},
pages = {110074},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110074},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007719},
author = {Bin Wan and Chengtao lv and Xiaofei Zhou and Yaoqi Sun and Zunjie Zhu and Hongkui Wang and Chenggang Yan},
keywords = {V-D-T salient object detection, Triple-modal interaction encoder, Multi-scale fusion decoder, Triple-modal interaction unit},
abstract = {Salient object detection methods based on two-modal images have achieved remarkable success with the aid of image acquisition equipment. However, environmental factors often interfere with the Depth and Thermal maps, rendering them ineffective in providing object information. To address this weakness, we utilize the VDT dataset, which includes Visible, Depth, and Thermal images, and propose a triple-modal interaction encoder and multi-scale fusion decoder network (TMNet) to highlight the salient regions. The triple-modal interaction encoder comprises the separation context-aware feature module, channel-wise fusion module, and triple-modal refinement and fusion module, enabling us to fully explore and utilize the complementarity between Visible, Depth, and Thermal information. The multi-scale fusion decoder involves the semantic-aware localizing module and contour-aware refinement module to extract and fuse the location and boundary information, yielding a high-quality saliency map. Extensive experiments on the public VDT-2048 dataset demonstrate that our TMNet outperforms existing state-of-the-art methods in terms of all evaluation metrics.}
}
@article{ZHANG2024109992,
title = {Matrix randomized autoencoder},
journal = {Pattern Recognition},
volume = {146},
pages = {109992},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109992},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006908},
author = {Shichen Zhang and Tianlei Wang and Jiuwen Cao and Wandong Zhang and Badong Chen},
keywords = {Randomized autoencoder, Matrix representation, Within-class scatter matrix, Within-class interaction},
abstract = {Randomized autoencoder (RAE) has attracted much attention due to its strong capability of representation with fast learning speed. However, the mainstream RAEs are still designed for scalar/vector data, which inevitably destroys the structure information of tensor data. To alleviate this deficiency, a novel convolutions based matrix randomized autoencoder (MRAE) is developed for two-dimensional (2D) data in this paper, including a one-side MRAE (OMRAE) exploiting the row or column information and a double-side MRAE (DMRAE) that simultaneously extracts the row and column information by 2 parallel OMRAEs. To reduce meaningless encoded features, the within-class scatter matrix (WSI) and within-class interaction distance (WID) constraints are added into OMRAE resulting WSI-OMRAE and WID-OMRAE, respectively. To demonstrate the superiority, stacked MRAEs are embedded into hierarchical regularized least squares for one-class classification and comparisons with several state-of-the-art methods are provided. The source code would be available at https://github.com/ML-HDU/MRAE.}
}
@article{PAN2024110000,
title = {Dynamic gradient reactivation for backward compatible person re-identification},
journal = {Pattern Recognition},
volume = {146},
pages = {110000},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110000},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006982},
author = {Xiao Pan and Hao Luo and Weihua Chen and Fan Wang and Hao Li and Wei Jiang and Jianming Zhang and Jianyang Gu and Peike Li},
keywords = {Person re-identification, Backward compatible training, Deep learning},
abstract = {We study the backward compatible problem for person re-identification (Re-ID), which aims to constrain the features of an updated new model to be comparable with the existing features from the old model in galleries. Most of the existing works adopt distillation-based methods, which focus on pushing new features to imitate the distribution of the old ones. However, the distillation-based methods are intrinsically sub-optimal since it forces the new feature space to imitate the inferior old feature space. To address this issue, we propose the Ranking-based Backward Compatible Learning (RBCL), which directly optimizes the ranking metric between new features and old features. Different from previous methods, RBCL only pushes the new features to find best-ranking positions in the old feature space instead of strictly alignment, and is in line with the ultimate goal of backward retrieval. However, the sharp sigmoid function used to make the ranking metric differentiable also incurs the gradient vanish issue, therefore stems the ranking refinement during the later period of training. To address this issue, we propose the Dynamic Gradient Reactivation (DGR), which can reactivate the suppressed gradients by adding dynamically computed constants during the forward step. To further help target the best-ranking positions, we include the Neighbor Context Agents (NCAs) to approximate the entire old feature space during training. Unlike previous works that mainly test on the in-domain settings, we make the early attempt to introduce the cross-domain settings (including both supervised and unsupervised) for the backward compatible person Re-ID task, which are more challenging yet meaningful. The experimental results on all five settings show that the proposed RBCL outperforms previous state-of-the-art methods by large margins.}
}
@article{SUN2024110046,
title = {Conditional feature generation for transductive open-set recognition via dual-space consistent sampling},
journal = {Pattern Recognition},
volume = {146},
pages = {110046},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110046},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007434},
author = {Jiayin Sun and Qiulei Dong},
keywords = {Open-set recognition, Transductive learning, Generative learning},
abstract = {Open-set recognition (OSR) aims to simultaneously detect unknown-class samples and classify known-class samples. Most of the existing OSR methods are inductive methods, which generally suffer from the domain shift problem that the learned model from the known-class domain might be unsuitable for the unknown-class domain. Addressing this problem, inspired by the success of transductive learning for alleviating the domain shift problem in many other visual tasks, we propose an Iterative Transductive OSR framework, called IT-OSR, which implements three explored modules iteratively, including a reliability sampling module, a feature generation module, and a baseline update module. Specifically at the initialization stage, a baseline method, which could be an arbitrary inductive OSR method, is used for assigning pseudo labels to the test samples. At the iteration stage, based on the consistency of the assigned pseudo labels between the output/logit space and the latent feature space of the baseline method, a dual-space consistent sampling approach is presented in the reliability sampling module for sampling some reliable ones from the test samples. Then in the feature generation module, a conditional dual-adversarial generative network is designed to generate discriminative features of both known and unknown classes. This generative network employs two discriminators for implementing fake/real and known/unknown-class discriminations respectively. And it is trained by jointly utilizing the test samples with their pseudo labels selected in the reliability sampling module and the labeled training samples. Finally in the baseline update module, the above baseline method is updated/re-trained for sample re-prediction by jointly utilizing the generated features, the selected test samples with pseudo labels, and the training samples. Extensive experimental results on both the standard-dataset and the cross-dataset settings demonstrate that the derived transductive methods, by introducing two typical inductive OSR methods into the proposed IT-OSR framework, achieve better performances than 19 state-of-the-art methods in most cases.}
}
@article{GAO2024109964,
title = {Transformer-based visual object tracking via fine–coarse concatenated attention and cross concatenated MLP},
journal = {Pattern Recognition},
volume = {146},
pages = {109964},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109964},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006623},
author = {Long Gao and Langkun Chen and Pan Liu and Yan Jiang and Yunsong Li and Jifeng Ning},
keywords = {Visual object tracking, Transformer, Fine–coarse concatenated attention, Multi-layer perceptron, Siamese network},
abstract = {Transformer-based trackers have demonstrated promising performance in visual object tracking tasks. Nevertheless, two drawbacks limited the potential performance improvement of transformer-based trackers. Firstly, the static receptive field of the tokens within one attention layer of the original self-attention learning neglects the multi-scale nature in the object tracking task. Secondly, the learning procedure of the multi-layer perception (MLP) in the feed forward network (FFN) is lack of local interaction information among samples. To address the above issues, a new self-attention learning method, fine–coarse concatenated attention (FCA), is proposed to learn self-attention with fine and coarse granularity information. Moreover, the cross-concatenation MLP (CC-MLP) is developed to capture local interaction information across samples. Based on the two proposed modules, a novel encoder and decoder are constructed, and augmented in an all-attention tracking algorithm, FCAT. Comprehensive experiments on popular tracking datasets, OTB2015, LaSOT, GOT-10K and TrackingNet, reveal the effectiveness of FCA and CC-MLP, and FCAT achieves the state-of-art on the datasets.}
}
@article{NAKAMURA2024110034,
title = {Generative adversarial networks via a composite annealing of noise and diffusion},
journal = {Pattern Recognition},
volume = {146},
pages = {110034},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110034},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007318},
author = {Kensuke Nakamura and Simon Korman and Byung-Woo Hong},
keywords = {Generative adversarial networks, Optimization, Scale-space, Noise injection, Coarse-to-fine training},
abstract = {Generative adversarial network (GAN) is a framework for generating fake data using a set of real examples. However, GAN is unstable in the training stage. In order to stabilize GANs, the noise injection has been used to enlarge the overlap of the real and fake distributions at the cost of increasing variance. The diffusion process (or data smoothing in its spatial domain) removes fine details in order to capture the structure and important patterns in data but it suppresses the capability of GANs to learn high-frequency information in the training procedure. Based on these observations, we propose a data representation for the GAN training, called noisy scale-space (NSS), that recursively applies the smoothing with a balanced noise to data in order to replace the high-frequency information by random data, leading to a coarse-to-fine training of GANs. We experiment with NSS using DCGAN and StyleGAN2 based on benchmark datasets in which the NSS-based GANs outperforms the state-of-the-arts in most cases.}
}
@article{KIM2024109988,
title = {Controllable Style Transfer via Test-time Training of Implicit Neural Representation},
journal = {Pattern Recognition},
volume = {146},
pages = {109988},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109988},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006866},
author = {Sunwoo Kim and Youngjo Min and Younghun Jung and Seungryong Kim},
keywords = {Style transfer, Implicit neural representation},
abstract = {Existing CNN-based style transfer frameworks have suffered from inaccurate control of pixel-wise stylization. As the CNN operation is designed based on kernel-wise operation, such a design unavoidably makes pixels affect each other. To mitigate this problem, we propose a controllable style transfer framework that leverages Implicit Neural Representation to encode each pixel respectively and optimize each style and content pair via test-time training. Unlike previous CNN-based style transfer frameworks, this formulation naturally enables accurate pixel-wise stylization control. In addition, to give explicit controllability on the degree of stylization, we define two vectors that represent the content and style respectively, enabling control by interpolating these vectors. We further demonstrate that, after being test-time trained once, our framework can show a various range of applications by precisely controlling the stylized images pixel-wise and freely adjusting image resolution and degree of stylization without further optimization or training.}
}
@article{DONG2024110019,
title = {Coarse-to-fine online latent representations matching for one-stage domain adaptive semantic segmentation},
journal = {Pattern Recognition},
volume = {146},
pages = {110019},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110019},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007161},
author = {Zihao Dong and Sijie Niu and Xizhan Gao and Xiuli Shao},
keywords = {Unsupervised domain adaptation, Semantic segmentation, Coarse-to-fine matching, Latent representations, Adversarial structure},
abstract = {Domain adaptive semantic segmentation is meaningful since collecting numerous labeled samples in different domains is expensive and time-consuming. Recent domain adaptation methods yield not so efficient performance compared with supervised learning. With the hypothesis that semantic feature can be shared across domains, this paper proposes a coarse-to-fine online matching architecture (COM) for one-stage domain adaptation. We consider subsequent learning stages progressively refining the task in the latent feature space, i.e. the finer set at each component is hierarchically derived from the coarser set of the previous components, including cross-domain global prototypes, categories and instances matching and anchor-points contrastive learning, which further achieve self-supervised learning with region-level pseudo label generated only in a single training step. Beforehand, feature refinement are performed to realize edge perception and inter-feature augmentation. Then, coarse-to-fine network fuses global and local consistency matching via specific distribution alignment between the source and target domain. Finally, the adversarial structure controls the uncertainty of generator prediction through the maximization of classification results and minimization of two classifiers discrepancy. This proposed method is evaluated in two unsupervised domain adaptation tasks, i.e. GTA5 → Cityscapes and SYNTHIA → Cityscapes. Extensive experiments verify the effectiveness of our proposed COM model and demonstrate its superiority over several state-of-the-art approaches.}
}
@article{CHEN2024110059,
title = {High-order relational generative adversarial network for video super-resolution},
journal = {Pattern Recognition},
volume = {146},
pages = {110059},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110059},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007562},
author = {Rui Chen and Yang Mu and Yan Zhang},
keywords = {Video super-resolution, Motion compensation, Generative adversarial network, High-order relations},
abstract = {Video super-resolution can reconstruct a sequence of high-resolution frames with temporally consistent contents from their corresponding low-resolution sequences. The key challenge for this task is how to effectively utilize both inter-frame temporal relations and intra-frame spatial relations. The existing methods for super-resolving the videos commonly estimate optical flows to align the features of multiple frames based on temporal correlations. However, motion estimation is often error-prone and hence largely hinders the recovery of plausible details. Moreover, high-order contextual dependencies in the feature space are rarely exploited for further enhancing the spatio-temporal information fusion. To this end, we propose a novel generative adversarial network to super-resolve low-resolution videos, which makes full use of patch embeddings and is effective in exploring high-order spatio-temporal relations of the feature patches. Specifically, a motion-aware relation module is designed to handle the alignment between neighboring frames and reference ones. Depending on a patch-matching strategy for adaptive selection of multiple most similar patches, the cross-scale graph is constructed to reliably aggregate these patches using a feature pyramid. Based on the structure of multi-scale graph, a context-aware relation module is developed to capture high-order dependencies among resulting warped patches for better leveraging long-range complementary contexts. To further enhance reconstruction ability, the temporal position information of video sequences is also encoded into this module. Dual discriminators with cycle consistent constraints are adopted to provide more informative feedback to the generator while maintaining the global coherence. Extensive experiments have demonstrated the effectiveness of the proposed method in terms of quantitative and qualitative evaluation metrics.}
}
@article{SHAN2024110007,
title = {CS-GAC: Compressively sensed geodesic active contours},
journal = {Pattern Recognition},
volume = {146},
pages = {110007},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110007},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007045},
author = {Hao Shan},
keywords = {Compressed sensing/compressive sampling, Geodesic active contours, Edge detection, Sparse reconstruction, Image segmentation, Level set, Shearlet, Curvelet, Wavelet},
abstract = {This paper proposes an edge based compressively sensed (CS) geodesic active contour (GAC) model, termed CS-GAC, to ensure faithful edge detection and accurate object segmentation. The motivation behind this paper is that edge information driving the contour evolution can be iteratively obtained by incomplete CS measurements. In each iteration, the CS-GAC is a three-step process including edge detection, active contouring and sparse reconstruction. Instead of working on the final reconstructed images themselves, the evolution of the CS-GAC is driven by a few CS measurements and guided by updatable edge information. The edge information is generated by a complex shearlet transform (CST) based edge map. In the framework, reconstruction and edge detection work alternately. The iterative update property that takes advantages of both edge sparsity and edge detection can largely improve the evolution precision. Numerical experiments show that the CS-GAC can obtain challenging segmentation results in comparisons with the state of the art methods, and has competitive prospects.}
}
@article{CHEN2024109976,
title = {Learning node representations against perturbations},
journal = {Pattern Recognition},
volume = {145},
pages = {109976},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109976},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300674X},
author = {Xu Chen and Yuangang Pan and Ivor Tsang and Ya Zhang},
keywords = {Graph neural networks, Node representation learning, , , },
abstract = {Recent graph neural networks (GNN) has achieved remarkable performance in node representation learning. One key factor of GNN’s success is the smoothness property on node representations. Despite this, most GNN models are fragile to the perturbations on graph inputs and could learn unreliable node representations. In this paper, we study how to learn node representations against perturbations in GNN. Specifically, we consider that a node representation should remain stable under slight perturbations on the input, and node representations from different structures should be identifiable, which two are termed as the stability and identifiability on node representations, respectively. To this end, we propose a novel model called Stability-Identifiability GNN Against Perturbations (SIGNNAP) that learns reliable node representations in an unsupervised manner. SIGNNAP formalizes the stability and identifiability by a contrastive objective and preserves the smoothness with existing GNN backbones. The proposed method is a generic framework that can be equipped with many other backbone models (e.g. GCN, GraphSage and GAT). Extensive experiments on six benchmarks under both transductive and inductive learning setups of node classification demonstrate the effectiveness of our method. Codes and data are available online: https://github.com/xuChenSJTU/SIGNNAP-master-online}
}
@article{ZHANG2024110018,
title = {Convolutional neural networks rarely learn shape for semantic segmentation},
journal = {Pattern Recognition},
volume = {146},
pages = {110018},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110018},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300715X},
author = {Yixin Zhang and Maciej A. Mazurowski},
keywords = {Segmentation, Feature measurement, Machine learning, Computer vision},
abstract = {Shape learning, or the ability to leverage shape information, could be a desirable property of convolutional neural networks (CNNs) when target objects have specific shapes. While some research on the topic is emerging, there is no systematic study to conclusively determine whether and under what circumstances CNNs learn shape. Here, we present such a study in the context of segmentation networks where shapes are particularly important. We define shape and propose a new behavioral metric to measure the extent to which a CNN utilizes shape information. We then execute a set of experiments with synthetic and real-world data to progressively uncover under which circumstances CNNs learn shape and what can be done to encourage such behavior. We conclude that (i) CNNs do not learn shape in typical settings but rather rely on other features available to identify the objects of interest, (ii) CNNs can learn shape, but only if the shape is the only feature available to identify the object, (iii) sufficiently large receptive field size relative to the size of target objects is necessary for shape learning; (iv) a limited set of augmentations can encourage shape learning; (v) learning shape is indeed useful in the presence of out-of-distribution data.}
}
@article{CAI2024109977,
title = {Maximum Gaussianality training for deep speaker vector normalization},
journal = {Pattern Recognition},
volume = {145},
pages = {109977},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109977},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006751},
author = {Yunqi Cai and Lantian Li and Andrew Abel and Xiaoyan Zhu and Dong Wang},
keywords = {Speaker embedding, Normalization flow, Gaussianality training},
abstract = {Automatic Speaker Verification (ASV) is a critical task in pattern recognition and has been applied to various security-sensitive scenarios. The current state-of-the-art technique for ASV is based on deep embedding. However, a significant challenge with this approach is that the resulting deep speaker vectors tend to be irregularly distributed. To address this issue, this paper proposes a novel training method called Maximum Gaussianality (MG), which regulates the distribution of the speaker vectors. Compared to the conventional normalization approach based on maximum likelihood (ML), the new approach directly maximizes the Gaussianality of the latent codes, and therefore can both normalize the between-class and within-class distributions in a controlled and reliable way and eliminate the unbound likelihood problem associated with the conventional ML approach. Our experiments on several datasets demonstrate that our MG-based normalization can deliver much better performance than the baseline systems without normalization and outperform discriminative normalization flow (DNF), an ML-based normalization method, particularly when the training data is limited. In theory, the MG criterion can be applied to any task in any research domain where Gaussian distributions are needed, making the MG training a versatile tool.}
}
@article{WANG2024110047,
title = {Deep intra-image contrastive learning for weakly supervised one-step person search},
journal = {Pattern Recognition},
volume = {147},
pages = {110047},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110047},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007446},
author = {Jiabei Wang and Yanwei Pang and Jiale Cao and Hanqing Sun and Zhuang Shao and Xuelong Li},
keywords = {Person search, Weakly supervised, Contrastive learning, Spatial-invariant, Occlusion-invariant},
abstract = {Weakly supervised person search aims to perform joint pedestrian detection and re-identification (re-id) with only bounding-box annotations. Recently, the idea of contrastive learning is initially applied to weakly supervised person search, where two common contrast strategies are memory-based contrast and intra-image contrast. We argue that current intra-image contrast is shallow, which suffers from spatial-level and occlusion-level variance. In this paper, we present a novel deep intra-image contrastive learning using a Siamese network. Two key modules are spatial-invariant contrast (SIC) and occlusion-invariant contrast (OIC). SIC performs many-to-one contrasts between two branches of Siamese network and dense prediction contrasts in one branch of Siamese network. With these many-to-one and dense contrasts, SIC tends to learn discriminative scale-invariant and location-invariant features to solve spatial-level variance. OIC enhances feature consistency with the masking strategy to learn occlusion-invariant features. Extensive experiments are performed on two person search datasets. Our method achieves a state-of-the-art performance among weakly supervised one-step person search approaches.}
}
@article{SUN2024110075,
title = {Time pattern reconstruction for classification of irregularly sampled time series},
journal = {Pattern Recognition},
volume = {147},
pages = {110075},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110075},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007720},
author = {Chenxi Sun and Hongyan Li and Moxian Song and Derun Cai and Baofeng Zhang and Shenda Hong},
keywords = {Classification of irregularly sampled time series, Time pattern, Deep learning, Healthcare and medical application},
abstract = {Irregularly Sampled Time Series (ISTS) include partially observed feature vectors caused by the lack of temporal alignment across dimensions and the presence of variable time intervals. Especially in medical applications, because patients’ examinations depend on their health status, observations in this event-based medical time series are nonuniformly distributed. When using deep learning models to classify ISTS, most work defines the problem that needs to be solved as alignment-caused data missing or nonuniformity-caused dependency change. However, they only modeled relationships between observed values, ignoring the fact that time is the independent variable for a time series. In this paper, we emphasize that irregularity is active, time-depended, and class-associated and is reflected in the Time Pattern (TP). To this end, this paper focused on the TP of ISTS for the first time, proposing a Time Pattern Reconstruction (TPR) method. It first encodes time information by the time encoding mechanism, then imputes values from time codes by the continuous-discrete Kalman network, selects key time points by the conditional masking mechanism, and finally classifies ISTS based on the reconstructed TP. Experiments on four real-world medical datasets and three other datasets show that TPR outperforms all baselines. We also show that TP can reveal biomarkers and key time points for diseases.}
}
@article{LI2024110023,
title = {Inter-domain mixup for semi-supervised domain adaptation},
journal = {Pattern Recognition},
volume = {146},
pages = {110023},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110023},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007203},
author = {Jichang Li and Guanbin Li and Yizhou Yu},
keywords = {Semi-supervised domain adaptation, Inter-domain mixup, Neighborhood expansion},
abstract = {Semi-supervised domain adaptation (SSDA) aims to bridge source and target domain distributions, with a small number of target labels available, achieving better classification performance than unsupervised domain adaptation (UDA). However, existing SSDA work fails to make full use of label information from both source and target domains for feature alignment across domains, resulting in label mismatch in the label space during model testing. This paper presents a novel SSDA approach, Inter-domain Mixup with Neighborhood Expansion (IDMNE), to tackle this issue. Firstly, we introduce a cross-domain feature alignment strategy, Inter-domain Mixup, that incorporates label information into model adaptation. Specifically, we employ sample-level and manifold-level data mixing to generate compatible training samples. These newly established samples, combined with reliable and actual label information, display diversity and compatibility across domains, while such extra supervision thus facilitates cross-domain feature alignment and mitigates label mismatch. Additionally, we utilize Neighborhood Expansion to leverage high-confidence pseudo-labeled samples in the target domain, diversifying the label information of the target domain and thereby further increasing the performance of the adaptation model. Accordingly, the proposed approach outperforms existing state-of-the-art methods, achieving significant accuracy improvements on popular SSDA benchmarks, including DomainNet, Office-Home, and Office-31.}
}
@article{JEON2024110001,
title = {Low-light image enhancement using gamma correction prior in mixed color spaces},
journal = {Pattern Recognition},
volume = {146},
pages = {110001},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110001},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006994},
author = {Jong Ju Jeon and Jun Young Park and Il Kyu Eom},
keywords = {Low-light image enhancement, Gamma correction prior, Mixed color spaces, Transmission map, Inverted image, Atmospheric scattering model},
abstract = {In this paper, we propose an efficient and fast low-light image enhancement method using an atmospheric scattering model based on an inverted low-light image. The transmission map is derived as a function of two saturations of the original image in the two color spaces. Due to the difficulty in estimating the saturation of the original image, the transmission map is converted into a function of the average and maximum values of the original image. These two values are estimated from a given low-light image using the gamma correction prior. In addition, a pixel-adaptive gamma value determination algorithm is proposed to prevent under- or over-enhancement. The proposed algorithm is fast because it does not require the training or refinement process. The simulation results show that the proposed low-light enhancement scheme outperforms state-of-the-art approaches regarding both computational simplicity and enhancement efficiency. The code is available on https://github.com/TripleJ2543.}
}
@article{YANG2024110063,
title = {GNaN: A natural neighbor search algorithm based on universal gravitation},
journal = {Pattern Recognition},
volume = {146},
pages = {110063},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110063},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007604},
author = {Juntao Yang and Lijun Yang and Jinghui Zhang and Qiwen Liang and Wentong Wang and Dongming Tang and Tao Liu},
keywords = {Universal gravitation, Natural neighbor, Parameter selection, Dynamic neighborhood},
abstract = {The natural neighbor (NaN) method and its search algorithm (NaN-Searching) are widely used in many fields, including pattern recognition and image processing. NaN-Searching fundamentally overcomes the problem of the conventional nearest neighbor algorithm in selecting parameters for datasets with arbitrary shapes and achieves good results. However, this algorithm uses the conventional distance metric as the neighbor judgment criterion, which cannot accurately reflect the overall structure of the dataset in the process of neighbor search. Inspired by Newton’s law of universal gravitation, we propose a NaN search algorithm based on universal gravitation (GNaN-Searching). Our algorithm calculates gravitation using the structural features of data points in the dataset, it utilizes the gravitation between data as the neighbor judgment criterion, and inherits the no-parameter and dynamic neighborhood characteristics of the NaN search algorithm. Experimental results show that the natural neighborhood graph obtained by our method has a high performance in the representation of manifold data. We also applied the new method to clustering and outlier detection and achieved satisfactory results.}
}
@article{YANG2024109989,
title = {GDB: Gated Convolutions-based Document Binarization},
journal = {Pattern Recognition},
volume = {146},
pages = {109989},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109989},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006878},
author = {Zongyuan Yang and Baolin Liu and Yongping Xiong and Guibin Wu},
keywords = {Document binarization, Gated convolutions, Generative adversarial networks, Multi-scale fusion, Multi-branch learning},
abstract = {Document binarization is a crucial pre-processing step for various document analysis tasks. However, existing methods fail to accurately capture stroke edges, primarily due to the inherent limitations of vanilla convolutions and the absence of adequate boundary-related supervision during stroke edge extraction. In this paper, we formulate text extraction as the learning of gating values and propose an end-to-end network architecture based on gated convolutions, named GDB, to address the problem of imprecise stroke edge extraction. The gated convolutions enable the selective extraction of stroke feature with different attention. Our proposed framework comprises two stages. Firstly, a coarse sub-network with an extra edge branch is trained to enhance the precision of feature maps by incorporating a priori mask and edge information. Secondly, a refinement sub-network is cascaded to enhance the output of the first stage using gated convolutions based on the sharp edges. To effectively incorporate global information, GDB also integrates a parallelized multi-scale operation that combines local and global features. We conduct comprehensive experiments on ten Document Image Binarization Contest (DIBCO) datasets from 2009 to 2019 and Document Deblurring Datasets. Experimental results show that our proposed methods outperform the state-of-the-art methods across all metrics on average. Extensive ablation studys demonstrate the efficacy of key components. Available codes: https://github.com/Royalvice/GDB.}
}
@article{SUN2024109965,
title = {Updatable Siamese tracker with two-stage one-shot learning},
journal = {Pattern Recognition},
volume = {146},
pages = {109965},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109965},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006635},
author = {Xinglong Sun and Haijiang Sun and Jianan Li},
keywords = {Visual tracking, Siamese network, Online update, One-shot learning, Transformer},
abstract = {Offline-trained Siamese networks have realized very promising tracking precision and efficiency. However, the performance is still limited by the drawbacks in online update. Traditional strategies cannot tackle the irregular variations of object and the sampling noise, so it is quite risky to adopt them to update Siamese trackers. In this paper, we present a two-stage one-shot learner by exploring the learning scheme of Siamese network, which reveals there are two key issues during online update, i.e., feature fusion and feature comparison. Based on this finding, we propose an updatable Siamese tracker by introducing two independent transformers (SiamTOL). Concretely, a Cross-aware transformer is designed to combine the features of the initial and the dynamic templates, while a Decoder-favored transformer is exploited to compare the fusing template and the search region. By combining these transformers, our tracker is able to adequately model the feature dependencies between multi-frame object samples. Extensive experimental results on several popular benchmarks well manifest that the proposed approach achieves the leading performance, and outperforms other state-of-the-art trackers.}
}
@article{POYSER2024110052,
title = {Neural architecture search: A contemporary literature review for computer vision applications},
journal = {Pattern Recognition},
volume = {147},
pages = {110052},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110052},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007495},
author = {Matt Poyser and Toby P. Breckon},
keywords = {Neural architecture search, Classification, Detection, Segmentation},
abstract = {Deep Neural Networks have received considerable attention in recent years. As the complexity of network architecture increases in relation to the task complexity, it becomes harder to manually craft an optimal neural network architecture and train it to convergence. As such, Neural Architecture Search (NAS) is becoming far more prevalent within computer vision research, especially when the construction of efficient, smaller network architectures is becoming an increasingly important area of research, for which NAS is well suited. However, despite their promise, contemporary and end-to-end NAS pipeline require vast computational training resources. In this paper, we present a comprehensive overview of contemporary NAS approaches with respect to image classification, object detection, and image segmentation. We adopt consistent terminology to overcome contradictions common within existing NAS literature. Furthermore, we identify and compare current performance limitations in addition to highlighting directions for future NAS research.}
}
@article{JIANG2024110028,
title = {Deep orientated distance-transform network for geometric-aware centerline detection},
journal = {Pattern Recognition},
volume = {146},
pages = {110028},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110028},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007252},
author = {Zheheng Jiang and Hossein Rahmani and Plamen Angelov and Ritesh Vyas and Huiyu Zhou and Sue Black and Bryan Williams},
keywords = {Centerline detection, Geometric properties, Graph representation, Graph refinement},
abstract = {The detection of structure centerlines from imaging data plays a crucial role in the understanding, application and further analysis of many diverse problems, such as road mapping, crack detection, medical imaging and biometric identification. In each of these cases, pixel-wise segmentation is not sufficient to understand and quantify overall graph structure and connectivity without further processing that can lead to compound error. We thus require a method for automatic extraction of graph representations of patterning. In this paper, we propose a novel Deep Orientated Distance-transform Network (DODN), which predicts the centerline map and an orientated distance map, comprising orientation and distance in relation to the centerline and allowing exploitation of its geometric properties. This is refined by jointly modeling the relationship between neighboring pixels and connectivity to further enhance the estimated centerline and produce a graph of the structure. The proposed approach is evaluated on a diverse range of problems, including crack detection, road mapping and superficial vein centerline detection from infrared/ color images, improving over the state-of-the-art by 2.1%, 10.9% and 17.3%/ 4.6% respectively in terms of quality, demonstrating its generalizability and performance in a wide range of mapping problems.}
}
@article{ZHOU2024109974,
title = {Source-free domain adaptation with Class Prototype Discovery},
journal = {Pattern Recognition},
volume = {145},
pages = {109974},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109974},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006726},
author = {Lihua Zhou and Nianxin Li and Mao Ye and Xiatian Zhu and Song Tang},
keywords = {Source-free domain adaptation, Class prototype discovery, Pseudo-labels, Prototype regularization},
abstract = {Source-free domain adaptation requires no access to the source domain training data during unsupervised domain adaption. This is critical for meeting particular data sharing, privacy, and license constraints, whilst raising novel algorithmic challenges. Existing source-free domain adaptation methods rely on either generating pseudo samples/prototypes of source or target domain style, or simply leveraging pseudo-labels (self-training). They suffer from low-quality generated samples/prototypes or noisy pseudo-label target samples. In this work, we address both limitations by introducing a novel Class Prototype Discovery (CPD) method. In contrast to all alternatives, our CPD is established on a set of semantic class prototypes each constructed for representing a specific class. By designing a classification score based prototype learning mechanism, we reformulate the source-free domain adaptation problem to class prototype optimization using all the target domain training data, and without the need for data generation. Then, class prototypes are used to cluster target features to assign them pseudo-labels, which highly complements the conventional self-training strategy. Besides, a prototype regularization is introduced for exploiting well-established distribution alignment based on pseudo-labeled target samples and class prototypes. Along with theoretical analysis, we conduct extensive experiments on three standard benchmarks to validate the performance advantages of our CPD over the state-of-the-art models.}
}
@article{GU2024109998,
title = {Learning disentangled representations for controllable human motion prediction},
journal = {Pattern Recognition},
volume = {146},
pages = {109998},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109998},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006969},
author = {Chunzhi Gu and Jun Yu and Chao Zhang},
keywords = {Stochastic motion prediction, Deep generative model, Disentanglement learning},
abstract = {Generative model-based motion prediction techniques have recently realized predicting controlled human motions, such as predicting multiple upper human body motions with similar lower-body motions. However, to achieve this, the state-of-the-art methods require either subsequently learning mapping functions to seek similar motions or training the model repetitively to enable control over the desired portion of body. In this paper, we propose a novel framework to learn disentangled representations for controllable human motion prediction. Our task is to predict multiple future human motions based on the past observed sequence, with the control of partial-body movements. Our network involves a conditional variational auto-encoder (CVAE) architecture to model full-body human motion, and an extra CVAE path to learn only the corresponding partial-body (e.g., lower-body) motion. Specifically, the inductive bias imposed by the extra CVAE path encourages two latent variables in two paths to respectively govern separate representations for each partial-body motion. With a single training, our model is able to provide two types of controls for the generated human motions: (i) strictly controlling one portion of human body and (ii) adaptively controlling the other portion, by sampling from a pair of latent spaces. Additionally, we extend and adapt a sampling strategy to our trained model to diversify the controllable predictions. Our framework also potentially allows new forms of control by flexibly customizing the input for the extra CVAE path. Extensive experimental results and ablation studies demonstrate that our approach is capable of predicting state-of-the-art controllable human motions both qualitatively and quantitatively.}
}
@article{CHEN2024110083,
title = {Separated Fan-Beam Projection with Gaussian Convolution for Invariant and Robust Butterfly Image Retrieval.},
journal = {Pattern Recognition},
volume = {147},
pages = {110083},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110083},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300780X},
author = {Xin Chen and Bin Wang and Yongsheng Gao},
keywords = {Butterfly image retrieval, fan-beam projection, patch pattern, texture feature, deep-learning feature, feature fusion},
abstract = {Butterfly image retrieval is a challenging issue requiring the feature representation not only to be sensitive to the subtle inter-class difference but also to remain robust to large intra-class variations. Fan-beam projection is a mathematical tool originally applied to computed tomographic (CT) reconstruction of objects. In this paper, we introduce it for the first time into object recognition field. Separated fan-beam projection followed by Gaussian convolutions of different widths are designed to extract multiscale invariant features, patch-projection angles (PPA) and texture-projection angles (TPA), for separately depicting the patch patterns and texture properties of butterfly images. The PPA and TPA are then treated as heterogeneous co-occurrence patterns to be fused by a 2D histograms as final feature representation. We present a comprehensive experimental evaluation including image retrieval at species and subspecies levels, complementarity to deep-learning features, invariance and robustness. All the results consistently show the superior performance of the proposed method over the state-of-the arts.}
}
@article{YU2024110016,
title = {Self-distillation and self-supervision for partial label learning},
journal = {Pattern Recognition},
volume = {146},
pages = {110016},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110016},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007136},
author = {Xiaotong Yu and Shiding Sun and Yingjie Tian},
keywords = {Knowledge distillation, Self-supervised learning, Partial label learning, Machine learning},
abstract = {As a main branch of weakly supervised learning paradigm, partial label learning (PLL) copes with the situation where each sample corresponds to ambiguous candidate labels containing the unknown true label. The primary difficulty of PLL lies in label ambiguities, most existing researches focus on individual instance knowledge while ignore the importance of cross-sample knowledge. To circumvent this difficulty, an innovative multi-task framework is proposed in this work to integrate self-supervision and self-distillation to tackle PLL problem. Specifically, in the self-distillation task, cross-sample knowledge in the same batch is utilized to refine ensembled soft targets to supervise the distillation operation without using multiple networks. The auxiliary self-supervised task of recognizing rotation transformations of images provides more supervisory signal for feature learning. Overall, training supervision is constructed not only from the input data itself but also from other instances within the same batch. Empirical results on benchmark datasets reveal that this method is effective in learning from partially labeled data.}
}
@article{DU2024110044,
title = {DataMap: Dataset transferability map for medical image classification},
journal = {Pattern Recognition},
volume = {146},
pages = {110044},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110044},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007410},
author = {Xiangtong Du and Zhidong Liu and Zunlei Feng and Hai Deng},
keywords = {Transfer learning, Transferability, Gradient attribution},
abstract = {Deep learning (DL)-based models especially Convolutional Neural Network (CNN) models have recently achieved great success in medical image classifications. It is usually time-consuming and labor-intensive to train a practical classification model due to the requirement of large data volume. Since medical images are more difficult to acquire and label for model training, many scholars have applied transfer learning by pre-training a model on a larger dataset and then fine-tuning it on the target dataset to obtain better classification results. However, such approach, relying on individual expertise to select related datasets, is subjective and inconsistent in performance. In this paper, we propose a simple yet effective method for measuring the transferability between different datasets, and build a Dataset Map (DataMap) that can be used as a tool to find the most relevant datasets for transfer learning on target dataset. Recent studies show that the convolutional kernels in CNN models have different function roles. Therefore, we adopt the similarity between the convolution kernels to measure the transferability between datasets. Firstly, the gradient attribution is adopted to attribute the task related convolution kernels from last few convolution layers of the same pre-trained model architecture trained with different datasets. Then, the similarity between attributed convolutional kernels is calculated to denote the transferability between different datasets. Finally, we build a DataMap with 20 medical image datasets. Extensive experimental tests on 3 mainstream CNN architectures show that the proposed method can effectively measure the transferability between different datasets. With the guidance of the DataMap, the transfer learning can achieve the best performance on various training tasks, and the accuracy of the CNN classifier can be improved by 1% to 5% through pre-training.}
}
@article{GAO2024110040,
title = {HIE-EDT: Hierarchical interval estimation-based evidential decision tree},
journal = {Pattern Recognition},
volume = {146},
pages = {110040},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110040},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007379},
author = {Bingjie Gao and Qianli Zhou and Yong Deng},
keywords = {Evidential decision tree, Dempster–Shafer theory, Hierarchical interval estimation, Classification, Fractal-based belief entropy},
abstract = {Decision tree algorithm, because of its strong interpretability and high algorithm efficiency, is widely used in the field of pattern recognition and classification. When the number of data samples is small and there is uncertainty in the data, it is difficult for the traditional decision tree algorithm to fully mine the effective information in the data. In this paper, we use the Dempster–Shafer framework to model data uncertainty and propose a hierarchical interval estimation method to improve decision tree algorithms. The proposed method constructs intervals through two methods of attribute boundary and mean square error estimation, which not only utilizes the characteristics of intervals to model the inaccuracy of data, but also constrains intervals from two aspects, narrowing the representation range of available information. By comparing with the classic decision tree algorithm and the decision tree algorithm based on single interval estimation, the proposed method can perform classification tasks robustly and accurately in different types of data under seven data sets.}
}
@article{ZHAO2024109986,
title = {Information geometry based extreme low-bit neural network for point cloud},
journal = {Pattern Recognition},
volume = {146},
pages = {109986},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109986},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006842},
author = {Zhi Zhao and Yanxin Ma and Ke Xu and Jianwei Wan},
keywords = {Information geometry, Binary, Ternary, Neural network, Point cloud},
abstract = {Deep learning has significantly advanced three-dimensional computer vision applied to point clouds. Nevertheless, the substantial consumption of time, storage, and energy substantially limits its deployment on edge devices with constrained resources. Extremely low bit quantization has received wide attention due to its extremely high compression ratio, but the problem of a significant drop in accuracy cannot be ignored. To alleviate the obvious accuracy degradation for extreme low-bit quantization, this paper proposes a novel compression framework for binary and ternary neural networks applied to point clouds, which introduces information geometry to model quantization to compensate the severe feature manifold distortion. It applies differential geometry on manifolds to study the implicit information of the point cloud feature data. Based on the theoretical analysis from the novel perspective of information geometry, two optimization modules are proposed to alleviate severe geometry distortions on differential manifolds. The first module, scaling recovery, provides layer-wise scaling parameters to reduce geometric distortion caused by quantization. The second module, Pooling Recovery, is specially designed to alleviate more severe pooling geometry distortions in point clouds. These two modules benefit both binary and ternary neural networks with ignored overheads. For ternary quantization, optimizations on convolution weights and gradients are additionally introduced. The proposed self-adaptive gradient estimation provides a more accurate approximation to the non-differential ternary staircase function. Convolution weight optimization is implemented on an information-geometry optimized model to achieve even higher accuracy and less memory consumption. Experimental results validate that the proposed models significantly outperform state-of-the-art methods and demonstrate better scalability. Overall, this compression framework has the potential to facilitate the deployment of deep learning models on edge devices with limited resources, opening up new opportunities for applications of three-dimensional computer vision.}
}
@article{TU2024110056,
title = {Weighted subspace anomaly detection in high-dimensional space},
journal = {Pattern Recognition},
volume = {146},
pages = {110056},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110056},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007537},
author = {Jiankai Tu and Huan Liu and Chunguang Li},
keywords = {Anomaly detection, High-dimensional space, Subspace method, Correntropy, Block sparsity},
abstract = {Anomaly detection aims at finding anomalies deviating from the normal data patterns. Virtually all anomaly detection methods create a model of the normal patterns before finding anomalies. In high-dimensional scenarios, due to the curse of dimensionality, it is difficult to construct the model of normal patterns in the full dimensional space. Subspace methods assuming that data can be characterized by low-dimensional manifolds have attracted a great deal of research. However, in unsupervised setting, unlabeled data is composed of both the normal and the abnormal data. The existence of anomalies might affect the establishment of the underlying normal subspaces. The undetermined number of the underlying subspaces also brings difficulties in subspace selection. To tackle the aforementioned problems, we come up with a weighted subspace anomaly detection (WSAD) method. We utilize correntropy to construct an objective function to mitigate the influence of the anomalies, which can be regarded as a weighting method for different data. Besides, we introduce an auxiliary variable with block sparsity regularization to achieve adaptive subspace selection, which can be regarded as a weighting method for different subspaces. After the normal underlying subspaces being established, we define the outlier scores by considering the deviation from the underlying subspaces, the local outlier score within subspaces, and the subspace scale. We use the half-quadratic theory to transform the optimization problem defined in WSAD, and apply alternating optimization to solve the transformed problem. Theoretically, we prove the convergence of the optimization algorithm. Experimentally, we demonstrate the effectiveness of the proposed method on both synthetic data and real datasets.}
}
@article{HUANG2024110041,
title = {Discriminative features enhancement for low-altitude UAV object detection},
journal = {Pattern Recognition},
volume = {147},
pages = {110041},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110041},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007380},
author = {Shuqin Huang and Shasha Ren and Wei Wu and Qiong Liu},
keywords = {Object detection, Feature enhancement, Small scale object, Loss function, Low-altitude-UAV vision},
abstract = {Object detection is a pivotal task in low-altitude UAV application. Here the small scale objects are dominant due to shooting distance and angle and insufficient feature information due to the data from real world scenes. Although general detector has made great progress, it is not suitable for small scale object detection directly. Dense detector has potential because of the pixel-by-pixel detection but the resolving power of complex background and objects especially small scale objects is still insufficient. We propose a Feature Guided Enhancement module by designing two non-linear learning operators to guide more discriminative features when training. Further, a Scale-Aware Weighted loss function is proposed to dynamically weight the loss of various scale objects by statistical computing and highlight the contribution of small scale objects. Experimental results show that our method can effectively improve FCOS and ATSS, and our models obtain better performance by 1.5% and 0.6% AP respectively on VisDrone 2018 dataset.}
}
@article{HU2024110029,
title = {Domain generalization via Inter-domain Alignment and Intra-domain Expansion},
journal = {Pattern Recognition},
volume = {146},
pages = {110029},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110029},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007264},
author = {Jiajun Hu and Lei Qi and Jian Zhang and Yinghuan Shi},
keywords = {Domain generalization, Contrastive learning, Image recognition},
abstract = {The performance of traditional deep learning models tends to drop dramatically during being deployed in real-world scenarios when the distribution shift between the seen training and unseen test data occurs. Domain Generalization methods are designed to achieve generalizability to deal with the above issue. Since the features extracted by softmax cross-entropy loss are not adequately domain-invariant, previous works in Domain Generalization have attempted to overcome this problem by employing contrastive-based losses which pull positive pairs (i.e., samples with the same class label) from different domains closer. Unfortunately, these approaches tend to produce an extremely small feature space, which is not robust facing unseen domain and easily overfits to source domains. To address the aforementioned issue, we propose a novel loss named IAIE Loss to simultaneously perform Inter-domain Alignment and Intra-domain Expansion for positive pairs, which facilitates the model to extract domain-invariant features and mitigates overfitting. Specifically, we design two sets of positive samples named “easy positive samples” and “hard positive samples”. IAIE Loss pulls the hard positive pairs closer (alignment) while pushing the easy positive pairs apart (expansion). The state-of-the-art results on multiple DG benchmark datasets verify the effectiveness of our method.}
}
@article{FARDO2024110004,
title = {A Sparse Local Binary Pattern extraction algorithm applied to event sensor data for object classification},
journal = {Pattern Recognition},
volume = {146},
pages = {110004},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110004},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007021},
author = {Fernando Azevedo Fardo and Paulo Sérgio Silva Rodrigues},
keywords = {Sparse, LBP, Events},
abstract = {Recently, new sensors with active pixels were brought to market. These sensors export local variations of light intensity in the form of asynchronous events with low latency. Since the data output format is a stream of addressable events and not a complete image of light intensities, new algorithms are required for known problems in the field of Computer Vision, such as segmentation, VO, SLAM, object, and scene recognition. There are some proposed methodologies for object recognition using conventional methods, convolutional neural networks, and third-generation neural networks based on spikes. However, convolutional neural networks and spike neural networks require specific hardware for processing, hard to miniaturize. Also, several traditional Computer Vision operators and feature descriptors have been neglected in the context of event sensors and could contribute to lighter methodologies in object recognition. This paper proposes an algorithm for local binary pattern extraction in sparse structures, typically found in this context. This paper also proposes two methodologies using local binary patterns to captures with event-based sensors for object recognition. The first methodology exploits the known motion performed by the sensor, while the second is motion agnostic. It is demonstrated experimentally that the LBP operator is a fast and light alternative that enables variable reduction using PCA in some cases. The experiments also show that it is possible to reduce the final feature vector for classification by up to 99,73% when compared to conventional methods considered state-of-the-art while maintaining comparable accuracy.}
}
@article{SONG2024109975,
title = {TransBoNet: Learning camera localization with Transformer Bottleneck and Attention},
journal = {Pattern Recognition},
volume = {146},
pages = {109975},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109975},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006738},
author = {Xiaogang Song and Hongjuan Li and Li Liang and Weiwei Shi and Guo Xie and Xiaofeng Lu and Xinhong Hei},
keywords = {Camera localization, 6DoF pose, Hybrid attention, Pose regression},
abstract = {6DoF camera localization is an important component of autonomous driving and navigation. Deep learning has achieved impressive results in localization, but its robustness in dynamic environments has not been adequately addressed. In this paper, we propose a framework based on hybrid attention mechanism which can be generally applied to existing CNN-based pose regressors to improve their robustness in dynamic environments. Specifically, we propose a novel Transformer Bottleneck (TBo) block including convolution, channel attention, and a position-aware self-attention mechanism, which extracts more geometrically robust features by capturing the corresponding long-term dependencies between pixels. Furthermore, we introduce shuffle attention (SA) before the pose regressor, which integrates feature information in both spatial and channel dimensions, forcing the network to learn geometrically robust features, reducing the effects of dynamic objects and illumination conditions to improve camera localization accuracy. We evaluate our method on commonly benchmarked indoor and outdoor datasets and the experimental results show that our proposed method can significantly improve localization performance compared compare favorably to contemporary pose regressors schemes. In addition, extensive ablation evaluations are conducted to prove the effectiveness of our proposed hybrid attention bottleneck block for pose regression networks.}
}
@article{WANG2024110045,
title = {A Novel Attention-Driven Framework for Unsupervised Pedestrian Re-identification with Clustering Optimization},
journal = {Pattern Recognition},
volume = {146},
pages = {110045},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110045},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007422},
author = {Xuan Wang and Zhaojie Sun and Abdellah Chehri and Gwanggil Jeon and Yongchao Song},
keywords = {Pattern recognition, Unsupervised pedestrian re-identification, Attention mechanism, Instance learning, Group sampling method, Improve pseudo-labels},
abstract = {Unsupervised pedestrian re-identification (re-ID) is not merely a visual recognition task; it represents a significant sub-field within the domain of pattern recognition. Despite the remarkable success of Convolutional Neural Networks (CNN) in re-ID, they still face challenges in handling variations in pose, occlusion, and lighting conditions. To effectively tackle these challenges, it is imperative to prioritize implementing efficient sampling strategies. We propose a Novel Attention-Driven Framework for Unsupervised Pedestrian re-ID with Clustering Optimization (AFC) to address the above issues. First, we introduce a new attention mechanism that enhances multi-scale spatial attention and reduces the number of trainable parameters. Then, we employed a straightforward and effective method of group sampling. In addition, we apply a clustering consensus approach to estimate pseudo-label similarity in continuous training and use temporal propagation and ensembles to improve pseudo-labels. Extensive experiments on Market-1501, duketmc-reID and MSMT17 datasets show that our method achieves significant performance improvement in unsupervised pedestrian re-ID, which provides important theoretical and practical value for the research on deep fusion of pattern recognition field with pedestrian re-ID and promotes the further development of the related fields.}
}
@article{2025110990,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {157},
pages = {110990},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(24)00741-6},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007416}
}
@article{LIU2024109971,
title = {LCReg: Long-tailed image classification with Latent Categories based Recognition},
journal = {Pattern Recognition},
volume = {145},
pages = {109971},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109971},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006696},
author = {Weide Liu and Zhonghua Wu and Yiming Wang and Henghui Ding and Fayao Liu and Jie Lin and Guosheng Lin},
keywords = {Long-tailed, Image classification, Latent Categories},
abstract = {In this work, we tackle the challenging problem of long-tailed image recognition. Previous long-tailed recognition approaches mainly focus on data augmentation or re-balancing strategies for the tail classes to give them more attention during model training. However, these methods are limited by the small number of training images for the tail classes, which results in poor feature representations. To address this issue, we propose the Latent Categories based long-tail Recognition (LCReg) method. Our hypothesis is that common latent features shared by head and tail classes can be used to improve feature representation. Specifically, we learn a set of class-agnostic latent features shared by both head and tail classes, and then use semantic data augmentation on the latent features to implicitly increase the diversity of the training sample. We conduct extensive experiments on five long-tailed image recognition datasets, and the results show that our proposed method significantly improves the baselines.}
}
@article{DING2024109999,
title = {MF-Net: Multi-frequency intrusion detection network for Internet traffic data},
journal = {Pattern Recognition},
volume = {146},
pages = {109999},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109999},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006970},
author = {Zhaoxu Ding and Guoqiang Zhong and Xianping Qin and Qingyang Li and Zhenlin Fan and Zhaoyang Deng and Xiao Ling and Wei Xiang},
keywords = {Cyber security, Intrusion detection, Deep learning, Multi-frequency transformer, Multi-frequency LSTM},
abstract = {The rapid growth of Internet technology renders intrusion detection an important research topic in the field of pattern recognition. Considering that traffic data relate to not only temporal information, but also attack frequency, this paper presents a novel deep learning framework termed the multi-frequency intrusion detection network (MF-Net). MF-Net regards the pattern of Internet traffic as a superposition of sequential data with various frequencies, and is able to recognize the multi-frequency nature of network traffic data. The core of MF-Net is the multi-frequency LSTM (MF-LSTM) and multi-frequency transformer (MF-Transformer) module, both of which consist of high-frequency and low-frequency layers. In comparison with other state-of-the-art approaches on 4 public datasets, namely UNSW-NB15, KDD Cup 99, NSL-KDD and CICIDS 2017, as well as an IPv6 traffic dataset we created, MF-Net has shown better result in both binary and multi-class classification, which demonstrates the superiority of MF-Net over other compared approaches on network traffic intrusion detection.}
}
@article{ZHANG2024110069,
title = {Regional context-based recalibration network for cataract recognition in AS-OCT},
journal = {Pattern Recognition},
volume = {147},
pages = {110069},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110069},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007665},
author = {Xiaoqing Zhang and Zunjie Xiao and Bing Yang and Xiao Wu and Risa Higashita and Jiang Liu},
keywords = {Cataract classification, Explanation, AS-OCT, RCRNet, Performance evaluation method},
abstract = {Deep convolutional neural networks (CNNs) have been widely applied to cataract recognition tasks and have achieved promising results. However, most existing methods focused on designing data-driven CNN architectures, and failed to exploit asymmetric opacity distribution prior of cataract, which is significant for cataract diagnosis. To this end, this paper proposes a regional context-based recalibration (RCR) module, which fully leverages the clinical prior to recalibrate the feature maps with regional pooling, region-based context integration, and integrated context fusion. We stack these RCR modules to form an RCRNet based on anterior segment optical coherence tomography (AS-OCT) images for cataract recognition. Experiments on the AS-OCT-NC2 dataset and two publicly available medical datasets demonstrate that RCRNet achieves a better trade-off between performance and efficiency than state-of-the-art channel attention-based networks. We also explain the inherent behavior of RCRNet with the aid of the visual analysis. In addition, this paper is the first to study the effects of two performance evaluation methods on AS-OCT image-based cataract classification results: the single-image level and the single-eye level, suggesting that adopting the single-eye level to evaluate cataract classification performance according to clinical diagnosis requirement.}
}
@article{NGUYEN2024110017,
title = {FoodMask: Real-time food instance counting, segmentation and recognition},
journal = {Pattern Recognition},
volume = {146},
pages = {110017},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110017},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007148},
author = {Huu-Thanh Nguyen and Yu Cao and Chong-Wah Ngo and Wing-Kwong Chan},
keywords = {Food counting, Food instance segmentation, Food recognition},
abstract = {Food computing has long been studied and deployed to several applications. Understanding a food image at the instance level, including recognition, counting and segmentation, is essential to quantifying nutrition and calorie consumption. Nevertheless, existing techniques are limited to either category-specific instance detection, which does not reflect precisely the instance size at the pixel level, or category-agnostic instance segmentation, which is insufficient for dish recognition. This paper presents a compact and fast multi-task network, namely FoodMask, for clustering-based food instance counting, segmentation and recognition. The network learns a semantic space simultaneously encoding food category distribution and instance height at pixel basis. While the former value addresses instance recognition, the latter value provides prior knowledge for instance extraction. Besides, we integrate into the semantic space a pathway for class-specific counting. With these three outputs, we propose a clustering algorithm to segment and recognize food instances at a real-time speed. Empirical studies are made on three large-scale food datasets, including Mixed Dishes, UECFoodPixComp and FoodSeg103, which cover Western, Chinese, Japanese and Indian cuisines. The proposed networks outperform benchmarks in both terms of instance map quality and speed efficiency.}
}
@article{WANG2024109987,
title = {Fast generalized ramp loss support vector machine for pattern classification},
journal = {Pattern Recognition},
volume = {146},
pages = {109987},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109987},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006854},
author = {Huajun Wang and Yuanhai Shao},
keywords = {Generalized ramp loss, -SVM, P-stationary point,  proximal operator, Working set,  support vectors, -},
abstract = {Support vector machine (SVM) is widely recognized as an effective classification tool and has demonstrated superior performance in diverse applications. However, for large-scale pattern classification problems, it may require much memory and incur prohibitive computational costs. Motivated by this, we propose a new SVM model with novel generalized ramp loss (LR-SVM). The first-order optimality conditions for the non-convex and non-smooth LR-SVM are developed by the newly developed P-stationary point, based on which, the LR support vectors and working set of LR-SVM are defined, interestingly, which shows that all of the LR support vectors are on the two support hyperplanes under mild conditions. A fast proximal alternating direction method of multipliers with working set (LR-ADMM) is developed to handle LR-SVM and LR-ADMM has been demonstrated to achieve global convergence while maintaining a significantly low computational complexity. Numerical comparisons with nine leading solvers show that LR-ADMM demonstrates outstanding performance, particularly when applied to large-scale pattern classification problems with fewer support vectors, higher prediction accuracy and shorter computational time.}
}
@article{YELLENI2024110003,
title = {Monte Carlo DropBlock for modeling uncertainty in object detection},
journal = {Pattern Recognition},
volume = {146},
pages = {110003},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110003},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300701X},
author = {Sai Harsha Yelleni and Deepshikha Kumari and Srijith P.K. and Krishna Mohan C.},
keywords = {Monte Carlo method, DropBlock, Object detection, Bayesian deep learning, Uncertainty estimation, Instance segmentation},
abstract = {With the advancements made in deep learning, computer vision problems have seen a great improvement in performance. However, in many real-world applications such as autonomous driving vehicles, the risk associated with incorrect predictions of objects or segmentation of images is very high. Standard deep learning models for object detection and segmentation such as YOLO models are often overconfident in their predictions and do not take into account the uncertainty in predictions on out-of-distribution data. In this work, we propose an efficient and effective approach, Monte-Carlo DropBlock (MC-DropBlock), to model uncertainty in YOLO and convolutional vision Transformers for object detection. The proposed approach applies drop-block during training time and testing time on the convolutional layer of the deep learning models such as YOLO and convolutional transformer. We theoretically show that this leads to a Bayesian convolutional neural network capable of capturing the epistemic uncertainty in the model. Additionally, we capture the aleatoric uncertainty in the data using a Gaussian likelihood. We demonstrate the effectiveness of the proposed approach on modeling uncertainty in object detection and segmentation tasks using out-of-distribution experiments. Experimental results show that MC-DropBlock improves the generalization, calibration, and uncertainty modeling capabilities of YOLO models and convolutional Transformer models for object detection and segmentation.}
}
@article{MORALESALVAREZ2024110057,
title = {Introducing instance label correlation in multiple instance learning. Application to cancer detection on histopathological images},
journal = {Pattern Recognition},
volume = {146},
pages = {110057},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110057},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007549},
author = {Pablo Morales-Álvarez and Arne Schmidt and José Miguel Hernández-Lobato and Rafael Molina},
keywords = {Multiple instance learning, Gaussian processes, Ising model, Variational inference, Whole slide images, Histopathology},
abstract = {In the last years, the weakly supervised paradigm of multiple instance learning (MIL) has become very popular in many different areas. A paradigmatic example is computational pathology, where the lack of patch-level labels for whole-slide images prevents the application of supervised models. Probabilistic MIL methods based on Gaussian Processes (GPs) have obtained promising results due to their excellent uncertainty estimation capabilities. However, these are general-purpose MIL methods that do not take into account one important fact: in (histopathological) images, the labels of neighboring patches are expected to be correlated. In this work, we extend a state-of-the-art GP-based MIL method, which is called VGPMIL-PR, to exploit such correlation. To do so, we develop a novel coupling term inspired by the statistical physics Ising model. We use variational inference to estimate all the model parameters. Interestingly, the VGPMIL-PR formulation is recovered when the weight that regulates the strength of the Ising term vanishes. The performance of the proposed method is assessed in two real-world problems of prostate cancer detection. We show that our model achieves better results than other state-of-the-art probabilistic MIL methods. We also provide different visualizations and analysis to gain insights into the influence of the novel Ising term. These insights are expected to facilitate the application of the proposed model to other research areas.}
}
@article{NI2024110033,
title = {Feature incremental learning with causality},
journal = {Pattern Recognition},
volume = {146},
pages = {110033},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110033},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007306},
author = {Haotian Ni and Shilin Gu and Ruidong Fan and Chenping Hou},
keywords = {Feature incremental, Causal inference, Balancing regularizer},
abstract = {With the emerging of new data collection ways, the features are incremental and accumulated gradually. Due to the expansion of feature spaces, it is more common that there are unknown biases between the distribution of training and testing datasets. It is known as the unknown data selection bias, which belongs to the learning scenario with non-i.i.d samples. The performance of traditional approaches, which need the i.i.d. assumption, will be aggravated seriously. How to design an algorithm to address the problem of data selection bias in this feature incremental scenario is crucial but rarely studied. In this paper, we propose a feature incremental classification algorithm with causality. Firstly, we embed the confounding variable balance algorithm in causal learning into the prediction modeling and utilize the logical regression algorithm with balancing regular terms as a baseline. Then, to satisfy the special requirement of feature increment, we design a new regularizer, which maintains the consistency of the regression coefficients between the data in the current and previous stages. It retains the correlation between the old features and labels. Finally, we propose the Multiple Balancing Logistic Regression model (MBRLR) to jointly optimize the balancing regularizer and weighted logistic regression model with multiple feature sets. We also present theoretical results to show that our proposed algorithm can make precise and stable predictions. Besides, the numerical results also demonstrate that our MBRLR algorithm is superior to other methods.}
}
@article{ZHAO2024110050,
title = {Sharing-Net: Lightweight feedforward network for skeleton-based action recognition based on information sharing mechanism},
journal = {Pattern Recognition},
volume = {146},
pages = {110050},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110050},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007471},
author = {Yinan Zhao and Qing Gao and Zhaojie Ju and Jian Zhou and Yulan Guo},
keywords = {Skeleton-based action recognition, Lightweight structure, Multi-feature input, Information sharing mechanism},
abstract = {With the development of metaverse, augmented reality and human–robot teleoperation, action recognition plays an increasingly important role. In this work, we propose a Lightweight Feedforward Cross-channel Information Sharing Network (Sharing-Net) for action recognition. A multi-feature input module is constructed, which includes Cartesian Motion features, Global Joint Distances (GJD), and Global Joint Angles (GJA). The three types of features can tackle the problems of velocity differentiation, viewpoint diversification and object-distance variation, respectively. In order to take full use of the restricted parameters caused by the lightweight structure to enhance the accuracy under the premise of guaranteeing high speed, a multi-feature cross-channel information sharing mechanism is proposed. Dynamic nonlinear composite mapping between feature channels uses cross-channel residual blocks to share data information and establish coupling relationships. Extensive experiments on 3 public datasets and a self-built dataset verify the effectiveness of proposed methods. Compared with the state-of-the-art (SOAT) methods, Sharing-Net achieves the best accuracy with high speed on JHMDB and SHREC and performs superior balance of accuracy and computational cost on NTU RGB+D.}
}
@article{HUO2024110022,
title = {Semi-supervised class-conditional image synthesis with Semantics-guided Adaptive Feature Transforms},
journal = {Pattern Recognition},
volume = {146},
pages = {110022},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110022},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007197},
author = {Xiaoyang Huo and Yunfei Zhang and Si Wu},
keywords = {Generative adversarial networks, Semi-supervised learning, Image synthesis, Feature transformation},
abstract = {Generative Adversarial Networks (GANs) have become the mainstream models for class-conditional synthesis of high-fidelity images. To reduce the demand for labeled data, we propose a class-conditional GAN with Semantic-guided Adaptive Feature Transforms, which is referred to as SAFT-GAN for semi-supervised image synthesis. Instead of simply incorporating a classifier to infer the class labels of unlabeled data, the key idea behind SAFT-GAN is to incorporate class-semantic guidance in real-fake discrimination. More specifically, we adopt a two-head architecture for a discriminator: A label-embedded head identifies real and fake instances, conditioned on class label. To focus more on class-related regions, we exploit class-aware attention information to regularize this head via regional feature transforms. On the other hand, to make better use of unlabeled data, we design a label-free head, on which channel-adaptive feature transforms are imposed to fuse the discriminator and classifier features, such that the class semantics of synthesized images can be improved. Extensive experiments are performed to demonstrate how class-conditional image synthesis can benefit from the proposed feature transforms, and also demonstrate the superiority of SAFT-GAN.}
}
@article{PANG2024109968,
title = {Structure-preserving feature alignment for old photo colorization},
journal = {Pattern Recognition},
volume = {145},
pages = {109968},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109968},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006660},
author = {Yingxue Pang and Xin Jin and Jun Fu and Zhibo Chen},
keywords = {Old photo colorization, Structure preservation, Feature alignment, Hierarchical training},
abstract = {Deep learning techniques have made significant advancements in reference-based colorization by training on large-scale datasets. However, directly applying these methods to the task of colorizing old photos is challenging due to the lack of ground truth and the notorious domain gap between natural gray images and old photos. To address this issue, we propose a novel CNN-based algorithm called SFAC, i.e., Structure-preserving Feature Alignment Colorizer. SFAC is trained on only two images for old photo colorization, eliminating the reliance on big data and allowing direct processing of the old photo itself to overcome the domain gap problem. Our primary objective is to establish semantic correspondence between the two images, ensuring that semantically related objects have similar colors. We achieve this through a feature distribution alignment loss that remains robust to different metric choices. However, utilizing robust semantic correspondence to transfer color from the reference to the old photo can result in inevitable structure distortions. To mitigate this, we introduce a structure-preserving mechanism that incorporates a perceptual constraint at the feature level and a frozen-updated pyramid at the pixel level. Extensive experiments demonstrate the effectiveness of our method for old photo colorization, as confirmed by qualitative and quantitative metrics.}
}
@article{WANG2024109996,
title = {Fast anchor graph preserving projections},
journal = {Pattern Recognition},
volume = {146},
pages = {109996},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109996},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006945},
author = {Jikui Wang and Yiwen Wu and Bing Li and Zhenguo Yang and Feiping Nie},
keywords = {Dimensionality reduction, Principal component analysis, Anchor graph, Unsupervised learning},
abstract = {The existing graph-based dimensionality reduction algorithms need to learn an adjacency matrix or construct it in advance, therefore the time complexity of the graph-based dimensionality reduction algorithms is not less than O(n2d), where n denotes the number of samples, d denotes the number of dimensions. Moreover, the existing dimensionality reduction algorithms do not consider the cluster information in the original space, resulting in the weakening or even loss of valuable information after dimensionality reduction. To address the above problems, we propose Fast Anchor Graph Preserving Projections (FAGPP), which learns the projection matrix, the anchors and the membership matrix at the same time. Especially, FAGPP has a built-in Principal Component Analysis (PCA) item, which makes our model not only deal with the cluster information of data, but also deal with the global information of data. The time complexity of FAGPP is O(nmd), where m denotes the number of the anchors and m is much less than n. We propose a novel iterative algorithm to solve the proposed model and the convergence of the algorithm is proved theoretically. The experimental results on a large number of high-dimensional benchmark image data sets demonstrate the efficiency of FAGPP. The data sets and the source code are available from https://github.com/511lab/FAGPP.}
}
@article{ZHANG2024109984,
title = {Multiple instance learning from similarity-confidence bags},
journal = {Pattern Recognition},
volume = {146},
pages = {109984},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109984},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006829},
author = {Xuan Zhang and Yitian Xu and Xuhua Liu},
keywords = {Multiple instance learning, Similarity-confidence, Empirical risk minimization},
abstract = {Multiple instance learning (MIL) is a classic weakly supervised learning approach, in which samples are grouped into bags that may contain varying numbers of instances. A bag is designated as positive if it contains at least one positive instance; otherwise, it is considered negative. Previous studies have consistently assumed that the bag labels are completely known. In fact, labeling every bag can be extremely challenging or even unfeasible due to the exorbitant expenses in terms of time and labor. Fortunately, it is much easier to obtain the similarity confidence, which represents the probability of two bags sharing the same label. How to employ it in MIL is worthy of study. Inspired by the above study, we present the first attempt to investigate MIL from similarity-confidence bags. Therefore, this paper proposes a new framework for training bag-level classifiers that adheres to the principle of empirical risk minimization. Moreover, we theoretically derive a generalization error bound to guarantee model convergence. Finally, we implement risk correction to mitigate potential over-fitting problem and provide theoretical consistency. Numerical experiments on eight datasets further validate the effectiveness of the proposed bag-level classifier.}
}
@article{WANG2024110062,
title = {Coordinate Descent Optimized Trace Difference Model for Joint Clustering and Feature Extraction},
journal = {Pattern Recognition},
volume = {146},
pages = {110062},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110062},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007598},
author = {Quan Wang and Fei Wang and Zhongheng Li and Zheng Wang and Feiping Nie},
keywords = {Clustering, Coordinate descent method, Feature extraction, Trace difference criterion, Unsupervised learning},
abstract = {Joint clustering and dimensionality reduction methods are a promising solution to clustering due to its scalability to high-dimensional data. Some methods leverage trace ratio criterion and attain clusters by borrowing the K-means algorithm. However, trace ratio criterion has no close-formed solution for the discriminative projection matrix and the K-means algorithm has a limited capacity to handle the many-cluster problem. In this paper, Coordinate Descent Optimized Trace Difference model (CDOTD) is proposed for joint clustering and feature extraction. Formulating the objective function as a direct trace difference criterion containing a balance parameter, CDOTD harmonizes between-cluster scatter maximization and within-cluster scatter minimization by the balance parameter. Using the direct trace difference criterion, CDOTD can straightforward solve for the discriminative projection matrix and avoid obtaining a poor discriminative projection matrix in the iterative manner when a bad cluster start is given. CDOTD uses the coordinate descent method for clustering optimization, improving the ability to address the many-cluster problem. Extensive experiments show that CDOTD has achieved significant performance improvements compared to previous trace ratio criterion related joint clustering and feature extraction methods, and also outperformed other clustering methods in most cases.}
}
@article{SUN2024110038,
title = {Efficient search of comprehensively robust neural architectures via multi-fidelity evaluation},
journal = {Pattern Recognition},
volume = {146},
pages = {110038},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110038},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007355},
author = {Jialiang Sun and Wen Yao and Tingsong Jiang and Xiaoqian Chen},
keywords = {Model robustness, Adversarial attacks, Neural architecture search, Surrogate model},
abstract = {Neural architecture search (NAS) has emerged as one successful technique to find robust deep neural network (DNN) architectures. However, most existing robustness evaluations in NAS only consider l∞ norm-based adversarial noises. In order to improve the robustness of DNN models against multiple types of noises, it is necessary to consider a comprehensive evaluation in NAS for robust architectures. But with the increasing number of types of robustness evaluations, it also becomes more time-consuming to find comprehensively robust architectures. To alleviate this problem, we propose a novel efficient search of comprehensively robust neural architectures via multi-fidelity evaluation (ES-CRNA-ME). Specifically, we first search for comprehensively robust architectures under multiple types of evaluations using the weight-sharing-based NAS method, including different lp norm attacks, semantic adversarial attacks, and composite adversarial attacks. In addition, we reduce the number of robustness evaluations by the correlation analysis, which can incorporate similar evaluations and decrease the evaluation cost. Finally, we propose a multi-fidelity online surrogate during optimization to further decrease the search cost. On the basis of the surrogate constructed by low-fidelity data, the online high-fidelity data is utilized to finetune the surrogate. Experiments on CIFAR10 and CIFAR100 datasets show the effectiveness of our proposed method.}
}
@article{ZHANG2024110049,
title = {Coupled discriminative manifold alignment for low-resolution face recognition},
journal = {Pattern Recognition},
volume = {147},
pages = {110049},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110049},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300746X},
author = {Kaibing Zhang and Dongdong Zheng and Jie Li and Xinbo Gao and Jian Lu},
keywords = {Low-resolution face recognition, Discriminative manifold embedding, Discriminative locality alignment, Coupled mapping, Feature subspace},
abstract = {In practical applications, due to a long distance between the monitored population and monitoring equipment, the face images or human pose captured by the cameras often incur low-resolution (LR), small size, and poor quality, which leads to extreme difficulty in directly matching an LR face with the high-resolution (HR) ones in the gallery. In this paper, we propose a novel coupled discriminative manifold alignment (CDMA) method for LR face recognition. Specifically, in the training stage the principal component analysis (PCA) is first used to reduce the dimensional gap between LR and HR facial features. Next the LR face images and the corresponding HR face images are converted into a common shared feature subspace by learning two linear mappings in a supervised manner, where the neighborhood samples within the same class and from different classes are jointly exploited to align the manifold structures of LR and HR faces. In the test stage, for a given LR face in the probe set, two learned coupled mappings (CMs) are applied to match the HR images in the gallery set through the correlative metric. Thorough experimental results on three representative face databases verify the effectiveness of the proposed method in comparing with other state-of-the-art competitors. In particular, the proposed method is capable of yielding more competitive recognition performance than other predecessors when lower dimensional feature subspaces are applied to match the expected HR faces.}
}
@article{WANG2024109956,
title = {Restoring vision in hazy weather with hierarchical contrastive learning},
journal = {Pattern Recognition},
volume = {145},
pages = {109956},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109956},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006544},
author = {Tao Wang and Guangpin Tao and Wanglong Lu and Kaihao Zhang and Wenhan Luo and Xiaoqin Zhang and Tong Lu},
keywords = {Image dehazing, Hierarchical contrastive loss, Feature fusion, Contrastive learning},
abstract = {Image restoration under hazy weather condition, which is called single image dehazing, has been of significant interest for various computer vision applications. In recent years, deep learning-based methods have achieved success. However, existing image dehazing methods typically neglect the hierarchy of features in the neural network and fail to exploit their relationships fully. To this end, we propose an effective image dehazing method named Hierarchical Contrastive Dehazing (HCD), which is based on feature fusion and contrastive learning strategies. HCD consists of a hierarchical dehazing network (HDN) and a novel hierarchical contrastive loss (HCL). Specifically, the core design in the HDN is a hierarchical interaction module, which utilizes multi-scale activation to revise the feature responses hierarchically. To cooperate with the training of HDN, we propose HCL which performs contrastive learning on hierarchically paired exemplars, facilitating haze removal. Extensive experiments on public datasets, RESIDE, HazeRD, and DENSE-HAZE, demonstrate that HCD quantitatively outperforms the state-of-the-art methods in terms of PSNR, SSIM and achieves better visual quality.}
}
@article{LI2024110078,
title = {Efficient Long-Short Temporal Attention network for unsupervised Video Object Segmentation},
journal = {Pattern Recognition},
volume = {146},
pages = {110078},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110078},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007756},
author = {Ping Li and Yu Zhang and Li Yuan and Huaxin Xiao and Binbin Lin and Xianghua Xu},
keywords = {Unsupervised video object segmentation, Long temporal memory, Short temporal attention, Efficient projection},
abstract = {Unsupervised Video Object Segmentation (VOS) aims at identifying the contours of primary foreground objects in videos without any prior knowledge. However, previous methods do not fully use spatial–temporal context and fail to tackle this challenging task in real-time. This motivates us to develop an efficient Long-Short Temporal Attention network (termed LSTA) for unsupervised VOS task from a holistic view. Specifically, LSTA consists of two dominant modules, i.e., Long Temporal Memory and Short Temporal Attention. The former captures the long-term global pixel relations of the past frames and the current frame, which models constantly present objects by encoding appearance pattern. Meanwhile, the latter reveals the short-term local pixel relations of one nearby frame and the current frame, which models moving objects by encoding motion pattern. To speedup the inference, the efficient projection and the locality-based sliding window are adopted to achieve nearly linear time complexity for the two light modules, respectively. Extensive empirical studies on several benchmarks have demonstrated promising performances of the proposed method with high efficiency.}
}
@article{HUANG2024110026,
title = {Customized meta-dataset for automatic classifier accuracy evaluation},
journal = {Pattern Recognition},
volume = {146},
pages = {110026},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110026},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007239},
author = {Yan Huang and Zhang Zhang and Yan Huang and Qiang Wu and Han Huang and Yi Zhong and Liang Wang},
keywords = {Auto-evaluation, Automatic classifier accuracy evaluation},
abstract = {Automatic classifier accuracy evaluation (ACAEval) on unlabeled test sets is critical for unseen real-world environments. The use of dataset-level regression on synthesized meta-datasets (comprised of many sample sets) has shown promising results for ACAEval. However, the existing meta-dataset for ACAEval is created using simple image transformations such as rotation and background substitution, which can make it difficult to ensure a reasonable distribution shift between the sample set and the test set. When the distribution shift is large, it becomes challenging to estimate the classifier accuracy on the test set using those sample sets. To ensure more robust ACAEval, this paper attempts to customize a meta-dataset in which each sample set has a reasonable distribution shift to the test set. An intra-class cycle-consistent adversarial learning (ICAL) method is introduced to transfer the style of a labeled training set to the style of the test set, by jointly considering the domain shift issue, the label flipping issue (the semantic information may be changed after style transformation), and the diversity of multiple sample sets in the meta-dataset. Experiments validate that under the same experimental setup, our method outperforms the existing ACAEval methods by a good margin, and achieves state-of-the-art performance on several standard benchmark datasets, including digit classification and natural image classification.}
}
@article{SEZAVAR2024110054,
title = {DCapsNet: Deep capsule network for human activity and gait recognition with smartphone sensors},
journal = {Pattern Recognition},
volume = {147},
pages = {110054},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110054},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007513},
author = {Ahmadreza Sezavar and Randa Atta and Mohammed Ghanbari},
keywords = {Gait recognition, Human activity recognition, Capsule network, Smartphone sensors},
abstract = {Recently, deep neural networks are used to recognize human activity/gait through mobile sensors which have attracted a great attention. Although the existing deep neural networks that perform automatic feature extraction have achieved desirable performance, their classification accuracy needs to be improved. In this paper, a deep neural network that combines a set of convolutional layers and capsule network is proposed. The proposed architecture named DCapsNet is suited to automatically extract the activity or gait features through built in sensors and classify them. The convolutional layers of the DCapsNet are more suitable for processing temporal sequences and provide scalar outputs but not the equivariance. The capsule network (CapsNet) is then trained by a dynamic routing algorithm to capture the equivariance having a magnitude and orientation, which increases the efficiency of the model classification. The performance of the proposed model is evaluated on four public datasets: two HAR datasets (UCI-HAR and WISDM) and two gait datasets (WhuGAIT). The recognition accuracy of the proposed model for the UCI-HAR and WISDM datasets are 97.92 % and 99.30 %, respectively, and for the WhuGAIT Dataset #1 and Dataset #2 are 94.75 % and 97.16 %, respectively. Experimental results show that the proposed model achieves the highest recognition accuracy over the reported results of the state-of-the-art models.}
}
@article{SUN2024109980,
title = {Gaze estimation with semi-supervised eye landmark detection as an auxiliary task},
journal = {Pattern Recognition},
volume = {146},
pages = {109980},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109980},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006787},
author = {Yunjia Sun and Jiabei Zeng and Shiguang Shan},
keywords = {Computer vision, Gaze estimation, Semi-supervised learning},
abstract = {The changes in gaze are often reflected in the movements of eye landmarks, highlighting the relevance of eye landmark learning for accurate gaze estimation. To leverage eye landmarks, we propose a gaze estimation framework that incorporates eye landmark detection as an auxiliary task. However, obtaining eye landmark annotations for real-world gaze datasets is challenging. To address this, we exploit synthetic data, which provides precise eye landmark labels, by jointly training an eye landmark detector using labeled synthetic data and unlabeled real-world data in a semi-supervised manner. To reduce the influence of discrepancy between synthetic and real-world data, we improve the generalization ability of the landmark detector by performing a self-supervised learning strategy on a large scale of unlabeled real-world images. The proposed method outperforms other state-of-the-art gaze estimation methods on three gaze datasets, indicating the effectiveness of leveraging eye landmark detection as an auxiliary task to enhance gaze estimation performance.}
}
@article{LI2024109972,
title = {Robust online hashing with label semantic enhancement for cross-modal retrieval},
journal = {Pattern Recognition},
volume = {145},
pages = {109972},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109972},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006702},
author = {Li Li and Zhenqiu Shu and Zhengtao Yu and Xiao-Jun Wu},
keywords = {Robust, Noise, Low-rank, Sparse, Multi-label semantic correlations, Similarity, Online hashing, Cross-modal retrieval},
abstract = {Online hashing technology has attracted extensive attention owing to its effectiveness and efficiency in processing large-scale streaming data. However, there are still some limitations: (1) In practical applications, the observed labels of multimedia data are obtained through manual annotation, which may inevitably introduce some noises into labels. This may lead to retrieval performance degradation when the noisy labels are directly applied to retrieval tasks. (2) The potential semantic correlation of multi-labels cannot be fully explored. To overcome these limitations, in this paper, we propose robust online hashing with label semantic enhancement (ROHLSE). Specifically, ROHLSE seeks to recover the clean labels from the provided noisy labels by imposing low-rank and sparse constraints. Meanwhile, it employs the representation of samples in the feature space to predict the labels via the dependency between sample instances and labels. To efficiently handle streaming data, ROHLSE preserves the similarity between new data, and establishes the semantic relationships between new and old data through chunk similarity, simultaneously. Furthermore, ROHLSE can fully utilize the semantic correlations between multiple labels of each instance. Extensive experiments are conducted on three benchmark datasets to demonstrate the superiority of the proposed ROHLSE approach.}
}
@article{YANG2024110027,
title = {A segmentation method based on the deep fuzzy segmentation model in combined with SCANDLE clustering},
journal = {Pattern Recognition},
volume = {146},
pages = {110027},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110027},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007240},
author = {Zenan Yang and Haipeng Niu and Xiaoxuan Wang and Liangxin Fan},
keywords = {Fuzzy clustering segmentation algorithm, Deep fuzzy segmentation model, SCANDLE, The automatic coding structure, Matrix construction algorithm},
abstract = {To enhance the low clustering accuracy of the fuzzy clustering segmentation algorithm for analyzing high spatial resolution remote sensing images (HSRRSIs), a deep fuzzy segmentation model (DFSM)combined with Spectral Clustering with Adaptive Neighbors for Deep Learning (SCANDLE) clustering is proposed. The DFSM is used to over-segment the image, and the automatic coding structure is used to adaptively fuse the image features, minimizing the internal compactness and maximizing the external separability of the clustering, yielding better results. Meanwhile, the SCANDLE clustering model is used to cluster the over-segmentation results, and the matrix construction algorithm for adaptive neighborhood allocation is used to map the frame of the connected layer and optimally combine the over-segmentation images to realize the final segmentation results. The new method can accurately segment HSRRSIs with good segmentation performance.}
}
@article{MITSUZUMI2024110051,
title = {Phase Randomization: A data augmentation for domain adaptation in human action recognition},
journal = {Pattern Recognition},
volume = {146},
pages = {110051},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110051},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007483},
author = {Yu Mitsuzumi and Go Irie and Akisato Kimura and Atsushi Nakazawa},
keywords = {Domain adaptation, Human action recognition, Time-series data},
abstract = {Human action recognition models often suffer from achieving both accurate recognition and subject independence when the amount of training data is limited. In this paper, we propose a data-efficient domain adaptation approach to learning a subject-agnostic action recognition classifier. The core component of our approach is a novel data augmentation called Phase Randomization. On the basis of the observation that individual body size is highly correlated with the amplitude component of the motion sequence, we disentangle the individuality and action features by using contrastive self-supervised learning with data augmentation that randomizes only the phase component of the motion sequence. This enables us to estimate the subject label of each motion sequence and to train a subject-agnostic action recognition classifier by performing adversarial learning with the estimated subject labels. We empirically demonstrate the superiority of our method on two different action recognition tasks (skeleton-based action recognition and sensor-based activity recognition).}
}
@article{LI2024109997,
title = {MFAN: Mixing Feature Attention Network for trajectory prediction},
journal = {Pattern Recognition},
volume = {146},
pages = {109997},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109997},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006957},
author = {Jingzhong Li and Lin Yang and Yuxuan Chen and Yue Jin},
keywords = {Mixing feature attention, Trajectory prediction, Domain adaption},
abstract = {Accurate trajectory prediction of surrounding agents is essential for autonomous vehicles, where the key challenge is to understand the complex interactions among agents. Previous works treat all interacted features between agents equally in modeling interaction, while neglecting their different importance to the interaction, thus inevitably limiting the interaction modeling ability. Besides, existing methods suffer from significant performance degradation when domain shifts, resulting in severely deviant prediction from reality. To address these issues, we propose a novel prediction framework, dubbed Mixing Feature Attention Network (MFAN). Specifically, the proposed mixing feature attention is a parallel design to adaptively determine the importance of different interacted features and simultaneously capture the global interaction feature to improve interaction modeling. Meanwhile, the spatial global interaction is modeled from a spatial edge-featured graph input to capture the enhanced spatial interaction. The temporal motion pattern is modeled from a temporal edge-featured graph input to enhance the domain adaption. Finally, we estimate the parameters of bivariant Gaussian distribution for trajectory prediction. Experimental results show that our method achieves superior performance in trajectory prediction while maintaining low computational complexity and performs accurate prediction even when domain shifts.}
}
@article{MUMCU2024110066,
title = {Sequential architecture-agnostic black-box attack design and analysis},
journal = {Pattern Recognition},
volume = {147},
pages = {110066},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110066},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300763X},
author = {Furkan Mumcu and Yasin Yilmaz},
keywords = {Adversarial machine learning, Black-box attacks, Transferability of attacks, Vision transformers, Sequential hypothesis testing},
abstract = {Although adversarial machine learning attacks on image recognition models have been heavily investigated, the rising popularity of vision transformers revitalized the research on this topic. Due to the fundamental architectural differences between CNNs, which still dominate the image recognition applications, and transformers, the state-of-the-art attacks designed for CNNs are not effective against transformers, and vice versa. Such lack of transferability in attacks and the growing architectural heterogeneity in practice make the black-box attack design increasingly challenging. However, skillful attackers can handle the increasing uncertainty in target model architecture following two main approaches: designing transferable attacks that are robust to the architectural uncertainty in target model, and identifying the target architecture for attack selection. In this work, following the latter approach we propose a novel architecture-agnostic black-box attack design and analyze its performance. Experiments show that the proposed method, with a reasonable query overhead, outperforms the recent robust attack designs that follow the former approach. Different from the existing methods, the proposed method optimizes a trade-off between prior information about the target model and number of queries.}
}
@article{JI2024109973,
title = {Transfer easy to hard: Adversarial contrastive feature learning for unsupervised person re-identification},
journal = {Pattern Recognition},
volume = {145},
pages = {109973},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109973},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006714},
author = {Haoxuanye Ji and Le Wang and Sanping Zhou and Wei Tang and Nanning Zheng and Gang Hua},
keywords = {Person re-identification, Unsupervised learning, Hard sample generation},
abstract = {Unsupervised Person Re-Identification (Re-ID) is challenging due to the lack of ground-truth labels. Most existing methods address this problem by progressively mining high-confidence pseudo labels to guide the feature learning process. However, how to construct hard-enough samples while maintaining the fidelity of pseudo labels in these samples remains an open issue in the machine learning community. To tackle this challenge, we design a simple yet effective adversarial contrastive feature learning (ACFL) framework, which enhances the discriminative capability of features by introducing more transformed hard samples in the feature learning process. Specifically, it mainly consists of a discriminative feature learning module and a hard sample generation module. The discriminative feature learning module extracts recognizable features of unlabeled training samples to estimate the high-confidence relationship between samples. Then, the hard sample generation module utilizes these high-confidence relationships between samples to transfer all samples into the hard ones via an adversarial learning strategy. Finally, the generated hard samples are further fed into DFL to learn discriminative features for person Re-ID. Extensive experiments on Market-1501, DukeMTMC-reID, and MSMT17 datasets show that our method compares favorably with state-of-the-art methods.}
}
@article{ZHOU2024110043,
title = {Frequency-aware feature aggregation network with dual-task consistency for RGB-T salient object detection},
journal = {Pattern Recognition},
volume = {146},
pages = {110043},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110043},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007409},
author = {Heng Zhou and Chunna Tian and Zhenxi Zhang and Chengyang Li and Yongqiang Xie and Zhongbo Li},
keywords = {RGB-thermal, Salient object detection, Frequency feature aggregation, Dual-task consistency},
abstract = {RGB-Thermal salient object detection (SOD) aims to merge two spectral images to segment visually appealing objects. Current methods primarily extract salient object information in the pixel perspective. However, biological and psychological research indicates notable frequency sensitivity of the human visual system (HVS). The high-frequency (HF) and low-frequency (LF) information in images are processed by different neural channels, which has been overlooked in SOD. In this study, we argue that the objective of RGB-T SOD is not only to enhance feature representation in the pixel-aware but also to emulate human visual mechanisms. To our best knowledge, we explore RGB-T SOD from the frequency perspective for the first time. Specifically, we first present a frequency-aware multi-spectral feature aggregation module (FMFA) to exploit the separability and complementarity of frequency-aware features, generating and making full use of LF and HF cues. FMFA improves the feature representation of RGB-T from the frequency perspective and provides stronger frequency cues for boundary auxiliary tasks. Then, we develop an HF-guided signed distance map prediction module (HF-SDM) with dual-task consistency to effectively alleviate the coarse mask caused by blur boundary. HF-SDM employs the geometric relationship of objects which boosts the interaction between salient regions and boundaries. As a result, the model can gain sharper boundaries for salient objects. Finally, we propose a frequency-aware feature aggregation network (FFANet) incorporated with dual-task learning. Extensive experiments on RGB-T SOD datasets demonstrate that our proposed method outperforms other state-of-the-art methods. Ablation studies and visualizations further verify the effectiveness and interpretability of our method.}
}
@article{CHEN2024110039,
title = {SSL-Net: Sparse semantic learning for identifying reliable correspondences},
journal = {Pattern Recognition},
volume = {146},
pages = {110039},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110039},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007367},
author = {Shunxing Chen and Guobao Xiao and Ziwei Shi and Junwen Guo and Jiayi Ma},
keywords = {Feature matching, Deep learning, Outlier rejection, Attention mechanism},
abstract = {Feature matching aims to identify reliable correspondences between two sets of given initial feature points, which is of considerable importance to photogrammetry and computer vision. In this study, we propose an innovative sparse semantic learning-based network, named SSL-Net, for feature matching. Specifically, SSL-Net includes a novel sparsity constraint (SC) block, which builds a sparse graph for sparse semantic learning. The SC block adopts a region-to-whole learning strategy to measure the confidence of nodes in the sparse graph. It helps the sparse graph preserve the semantic information of positive influence while rejecting unnecessary ones, thereby suppressing the negative influence of incorrect correspondences. In addition, SSL-Net also includes a channel-spatial attention feature gathering block, which gathers features along the spatial direction and channel dimension of correspondences. To mitigate the existence of label ambiguity, we incorporate the accommodation factor into the loss function of SSL-Net for feature matching. As a result, our network outperforms the state-of-the-art method by a considerable margin. Notably, SSL-Net achieves a 9.05% improvement under an error threshold of 5° over the state-of-the-art method for the relative pose estimation task on the YFCC100M dataset. Our code will be available at https://github.com/guobaoxiao/SSL-Net.}
}
@article{YIN2024110015,
title = {GITGAN: Generative inter-subject transfer for EEG motor imagery analysis},
journal = {Pattern Recognition},
volume = {146},
pages = {110015},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110015},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007124},
author = {Kang Yin and Elissa Yanting Lim and Seong-Whan Lee},
keywords = {Brain-computer interface (BCI), Electroencephalogram (EEG), Unsupervised domain adaptation (UDA), Generative adversarial learning},
abstract = {Domain adaptation (DA) plays a crucial role in achieving subject-independent performance in Brain-Computer Interface (BCI). However, previous studies have primarily focused on developing intricate network architecture designs, neglecting the impact of source data quality and the challenges posed by the out-of-distribution target data problem. To address these limitations, we argue that a target data-centered space, augmented by a carefully selected set of high-quality source data, can significantly enhance DA. In this study, we present an unsupervised end-to-end subject adaptation approach called GITGAN, a generative inter-subject transfer for electroencephalography motor imagery analysis. We also propose a practical and effective method for selecting source data, which further enhances performance. Our approach is non-intrusive, as it does not modify the target data distribution, thus preserving its integrity for further numerical and visual analysis. Our extensive experiments with two different datasets demonstrate not only the superiority of our approach compared to existing methods, but also its phenomenal potential for practical BCI applications. The findings of this study provide valuable insights into the potential of BCI and illustrate the importance of considering source data quality in DA. The implementation is available at https://github.com/Kang1121/GITGAN.}
}
@article{ZHANG2024109985,
title = {Temporal segment dropout for human action video recognition},
journal = {Pattern Recognition},
volume = {146},
pages = {109985},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109985},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006830},
author = {Yu Zhang and Zhengjie Chen and Tianyu Xu and Junjie Zhao and Siya Mi and Xin Geng and Min-Ling Zhang},
keywords = {Action recognition, Temporal regularization, Temporal segment dropout},
abstract = {Temporal information is important for human action video recognition. With the widely used spatio-temporal neural networks, researchers have found that the learned high-level features preserve overfitted spatial information and limited temporal information, leading to inferior performance. This is because existing networks lack efficient regularization for the temporal structure. To learn more robust temporal features, we propose a temporal regularization method named Temporal Segment Dropout (TSD). TSD drops the most salient spatial features in order to enhance the temporal features in a clip of temporal segments. Without learning from complex examples, TSD can be easily deployed in existing networks. In the experiment, TSD is extensively evaluated on benchmark action recognition datasets, which brings consistent improvements over the baselines, especially for the action-centric classes.}
}
@article{WANG2024110055,
title = {Open set transfer learning through distribution driven active learning},
journal = {Pattern Recognition},
volume = {146},
pages = {110055},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110055},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007525},
author = {Min Wang and Ting Wen and Xiao-Yu Jiang and An-An Zhang},
keywords = {Active learning, Transfer learning, Evidence learning, Uncertainty analysis},
abstract = {Domain adaptation enables effective transfer between source and target domains with different distributions. The latest research focuses on open set domain adaptation; that is, the target domain contains unknown categories that do not exist in the source domain. The existing open set domain adaptation cannot realize the fine-grained recognition of unknown categories. In this paper, we propose an uncertainty analysis evidence model and design a distribution driven active transfer learning (DATL) algorithm. DATL realizes fine-grained recognition of unknown categories with no requirements on the source domain to contain the unknown categories. To explore unknown distributions, the uncertainty analysis evidence model was adopted to divide the high uncertainty space. To select critical instances, a cluster-diversity query strategy was proposed to identify new categories. To enrich the label categories of the source domain, a global dynamic alignment strategy was designed to avoid negative transfers. Comparative experiments with state-of-the-art methods on the standard Office-31/Office-Home/Office-Caltech10 benchmarks showed that the DATL algorithm: (1) outperformed its competitors; (2) realized accurate identification of unknown subcategories from a fine-grained perspective; and (3) achieved outstanding performance even with a very high degree of openness.}
}
@article{FANG2024110002,
title = {Fairness in face presentation attack detection},
journal = {Pattern Recognition},
volume = {147},
pages = {110002},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110002},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007008},
author = {Meiling Fang and Wufei Yang and Arjan Kuijper and Vitomir S̆truc and Naser Damer},
keywords = {Face presentation attack detection, Fairness assessment, Bias mitigation},
abstract = {Face recognition (FR) algorithms have been proven to exhibit discriminatory behaviors against certain demographic and non-demographic groups, raising ethical and legal concerns regarding their deployment in real-world scenarios. Despite the growing number of fairness studies in FR, the fairness of face presentation attack detection (PAD) has been overlooked, mainly due to the lack of appropriately annotated data. To avoid and mitigate the potential negative impact of such behavior, it is essential to assess the fairness in face PAD and develop fair PAD models. To enable fairness analysis in face PAD, we present a Combined Attribute Annotated PAD Dataset (CAAD-PAD), offering seven human-annotated attribute labels. Then, we comprehensively analyze the fairness of PAD and its relation to the nature of the training data and the Operational Decision Threshold Assignment (ODTA) through a set of face PAD solutions. Additionally, we propose a novel metric, the Accuracy Balanced Fairness (ABF), that jointly represents both the PAD fairness and the absolute PAD performance. The experimental results pointed out that female and faces with occluding features (e.g. eyeglasses, beard, etc.) are relatively less protected than male and non-occlusion groups by all PAD solutions. To alleviate this observed unfairness, we propose a plug-and-play data augmentation method, FairSWAP, to disrupt the identity/semantic information and encourage models to mine the attack clues. The extensive experimental results indicate that FairSWAP leads to better-performing and fairer face PADs in 10 out of 12 investigated cases.}
}
@article{ZHANG2024109981,
title = {A uniform representation model for OCT-based fingerprint presentation attack detection and reconstruction},
journal = {Pattern Recognition},
volume = {145},
pages = {109981},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109981},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006799},
author = {Wentian Zhang and Haozhe Liu and Feng Liu and Raghavendra Ramachandra},
keywords = {Presentation Attack Detection, Fingerprint representation, Semantic segmentation, Optical Coherence Tomography},
abstract = {In current Optical Coherence Tomography (OCT)-based fingerprint recognition systems, Presentation Attack Detection (PAD) and subsurface fingerprint reconstruction are treated as two independent branches, resulting in high computation and system building complexity. Therefore, this paper proposes a uniform representation model for simultaneous PAD and subsurface fingerprint reconstruction. A novel semantic segmentation network using attention mechanisms was designed to extract and segment multiple subsurface structures from real finger slices (i.e., B-scans). The latent codes derived from the network are directly used to effectively detect PA because they contain abundant subsurface biological information, which is independent of PA materials and has strong robustness for unknown PAs. Segmented subsurface structures were adopted to reconstruct multiple subsurface 2D fingerprints. Extensive experiments were carried out on an in-house database, which is the largest public OCT-based fingerprint database with 2449 volumes. PAD performance was evaluated by comparing the results of the existing methods with those of other segmentation networks. The proposed uniform representation model can obtain an accuracy (Acc) of 96.63%, which achieves a state-of-the-art performance. The effectiveness of subsurface reconstruction was evaluated from the segmentation and recognition results. In the segmentation experiments, the proposed method achieved the best results with an Intersection of Union (mIOU) of 0.834 and a Pixel Accuracy of 0.937. By comparing the recognition performance on surface 2D fingerprints (e.g., commercial and high-resolution), the lowest results with an Equal Error Rate (EER) of 2.25% by minutiae matching and an EER of 5.42% by pore matching are achieved, which indicates the excellent reconstruction capability of the proposed uniform representation model.}
}
@article{SONG2024110079,
title = {Deep self-enhancement hashing for robust multi-label cross-modal retrieval},
journal = {Pattern Recognition},
volume = {147},
pages = {110079},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110079},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007768},
author = {Ge Song and Hanwen Su and Kai Huang and Fengyi Song and Ming Yang},
keywords = {Cross-modal retrieval, Deep hashing, Out-of-distribution, Multi-label},
abstract = {The goal of cross-modal hashing is to map data from several modalities into a compact Hamming space for efficient and accurate retrieval. Despite the satisfactory performance, existing approaches are reliant on the closed-world assumption. When confronted with real-world retrieval tasks involving out-of-distribution (OOD) semantic data, the similarity relationships of known data retained in hash codes tend to be disrupted by these unknown ones, resulting in retrieval performance degradation. To this end, we present a deep self-enhancing hashing (DSEH) method, simultaneously learning multi-level similarity-preserved hash codes of the known multi-label cross-modal data and robustness to OOD instances. Specifically, we propose to construct pseudo-OOD samples in the feature space using random linear combinations to explore OOD semantics, during the training process. Meanwhile, a prototype-based generative model is incorporated to aggregate batch data to enhance the data representation’s differences in known and unknown semantics. Furthermore, we describe a bounded cosine quadrupled loss with distance bound to preserve the multi-level similarity of multi-label data and control the maximum distance between known data and the minimum distance between known and pseudo-OOD data for learning OOD robustness. Extensive experiments show that the DSEH achieves state-of-the-art performance on closed-world tasks and good performance on simulated real-world tasks.}
}
@article{TANG2024110020,
title = {Semi-supervised medical image segmentation via hard positives oriented contrastive learning},
journal = {Pattern Recognition},
volume = {146},
pages = {110020},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110020},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007173},
author = {Cheng Tang and Xinyi Zeng and Luping Zhou and Qizheng Zhou and Peng Wang and Xi Wu and Hongping Ren and Jiliu Zhou and Yan Wang},
keywords = {Hard positives, Contrastive learning, Semi-supervised learning, Medical image segmentation},
abstract = {Semi-supervised learning (SSL) has been a popular technique to resolve the annotation scarcity problem in pattern recognition and medical image segmentation, which usually focuses on two critical issues: 1) learning a well-structured categorizable embedding space, and 2) establishing a robust mapping from the embedding space to the pixel space. In this paper, to resolve the first issue, we propose a hard positives oriented contrastive (HPC) learning strategy to pre-train an encoder-decoder-based segmentation model. Different from vanilla contrastive learning tending to focus only on hard negatives, our HPC learning strategy additionally concentrates on hard positives (i.e., samples with the same category but dissimilar feature representations to the anchor), which are considered to play an even more crucial role in delivering discriminative knowledge for semi-supervised medical image segmentation. Specifically, the HPC is constructed from two levels, including an unsupervised image-level HPC (IHPC) and a supervised pixel-level HPC (PHPC), empowering the embedding space learned by the encoder with both local and global senses. Particularly, the PHPC learning strategy is implemented in a region-based manner, saving memory usage while delivering more multi-granularity information. In response to the second issue, we insert several feature swap (FS) modules into the pre-trained decoder. These FS modules aim to perturb the mapping from the intermediate embedding space towards the pixel space, trying to encourage more robust segmentation predictions. Experiments on two public clinical datasets demonstrate that our proposed framework surpasses the state-of-the-art methods by a large margin. Source codes are available at https://github.com/PerPerZXY/BHPC.}
}
@article{ZHANG2024110060,
title = {Pseudo-label estimation via unsupervised Identity Link Prediction for one-shot person Re-Identification},
journal = {Pattern Recognition},
volume = {146},
pages = {110060},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110060},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007574},
author = {Yulin Zhang and Bo Ma and Meng Li and Ying Liu and Feng Chen and Junyu Hou},
keywords = {One-shot person Re-Identification, Pseudo-label estimation, Link prediction, Dual-Branch Fusion},
abstract = {In this paper, we propose an unsupervised identity link prediction (ILP) method for label estimation in one-shot person Re-ID. ILP aims to relax the constraints of labeled samples and group a set of unlabeled pedestrians by their potential identities. The main idea is that the category relationships between pedestrians (nodes) can be inferred from their local context in the feature space. Specifically, an Identity Link Subgraph (ILS) describes the link relationship between nodes and their nearest neighbors, which is constructed by a two-step procedure. Meanwhile, a Dynamic Penalty Module (DPM) is introduced at each ILS construction step to infer which linkage between pairs in the ILS should be pruned to assign higher-quality classification labels. To fully use the accurate identity information in initial labeled samples, we jointly use identity pseudo-labels (which are estimated by adopting the Nearest Neighbors classifier) with classification pseudo-labels for model training. Moreover, we design a Dual-Branch Fusion network (DBF-Net) to optimize the CNN model simultaneously through all (pseudo-)labeled samples. Results on multiple datasets prove that DBF-Net outperforms traditional one-shot Re-ID methods by a large margin.}
}
@article{PING2024110036,
title = {Beyond k-Means++: Towards better cluster exploration with geometrical information},
journal = {Pattern Recognition},
volume = {146},
pages = {110036},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110036},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007331},
author = {Yuan Ping and Huina Li and Bin Hao and Chun Guo and Baocang Wang},
keywords = {Cluster analysis, -means++, Support vector data description, Edge pattern, Division and aggregation},
abstract = {Although k-means and its variants are known for their remarkable efficiency, they suffer from a strong dependence on the prior knowledge of K and the assumption of a circle-like pattern, which can result in the algorithms dividing the input space instead of discovering non-predetermined data patterns. Thus, we propose beyond k-means++ that infers and utilizes explicit clusters by emphasizing local geometrical information for better cluster exploration. To avoid the K dependence, a novel framework of iterative division and aggregation (IDA) over k-means++ is presented. It begins with any K≥1, then increases and reduces K along with the procedure of clusters’ division and aggregation, respectively. To break through the circle-like pattern limitation, we introduce a reasonability checking strategy (RCS) for cluster division. Given local geometrical information, RCS achieves arbitrary cluster shape support by rejecting edge patterns with distinguished convergence direction and merging adjacent clusters with pseudo-edge patterns. Furthermore, we design an edge shrinkage strategy (ESS). Taking edge patterns as the cluster prototype, it benefits accuracy by effectively avoiding representability reduction due to irregular distribution. To compensate for the loss of efficiency, a near maximin and random sampling algorithm is suggested for large-scale data with high dimensionality. Experimental results confirm that beyond k-means++ is featured by handling arbitrary cluster shapes with remarkable accuracy.}
}
@article{TAN2024110064,
title = {NCL++: Nested Collaborative Learning for long-tailed visual recognition},
journal = {Pattern Recognition},
volume = {147},
pages = {110064},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110064},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007616},
author = {Zichang Tan and Jun Li and Jinhao Du and Jun Wan and Zhen Lei and Guodong Guo},
keywords = {Long-tailed visual recognition, Collaborative learning, Online distillation, Deep learning},
abstract = {Long-tailed visual recognition has received increasing attention in recent years. Due to the extremely imbalanced data distribution in long-tailed learning, the learning process shows great uncertainties. For example, the predictions of different experts on the same image vary remarkably despite the same training settings. To alleviate the uncertainty, we propose a Nested Collaborative Learning (NCL++) which tackles the long-tailed learning problem by a collaborative learning. To be specific, the collaborative learning consists of two folds, namely inter-expert collaborative learning (InterCL) and intra-expert collaborative learning (IntraCL). InterCL learns multiple experts collaboratively and concurrently, aiming to transfer the knowledge among different experts. IntraCL is similar to InterCL, but it aims to conduct the collaborative learning on multiple augmented copies of the same image within the single expert. To achieve the collaborative learning in long-tailed learning, the balanced online distillation is proposed to force the consistent predictions among different experts and augmented copies, which reduces the learning uncertainties. Moreover, in order to improve the meticulous distinguishing ability on the confusing categories, we further propose a Hard Category Mining (HCM), which selects the negative categories with high predicted scores as the hard categories. Then, the collaborative learning is formulated in a nested way, in which the learning is conducted on not just all categories from a full perspective but some hard categories from a partial perspective. Extensive experiments manifest the superiority of our method with outperforming the state-of-the-art whether with using a single model or an ensemble. The code will be publicly released.}
}
@article{TANG2024109982,
title = {CATNet: Convolutional attention and transformer for monocular depth estimation},
journal = {Pattern Recognition},
volume = {145},
pages = {109982},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109982},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006805},
author = {Shuai Tang and Tongwei Lu and Xuanxuan Liu and Huabing Zhou and Yanduo Zhang},
keywords = {Monocular depth estimation, Convolutional attention, Transformer, Adaptive bins},
abstract = {Monocular depth estimation has received more and more attention due to its wide range of application scenarios. In this paper, we propose a novel simple framework, called CATNet, which treats monocular depth estimation as an ordinal regression problem. At present, in order to obtain higher performance, the research on monocular depth estimation is achieved by increasing the amount of calculation and parameters of the model. Based on this, we propose a novel simple encoder–decoder architecture that aims to reduce the SOTA model parameters and complexity while keeping the depth estimation accuracy as high as possible rather than aiming for extremely lightweight. Meanwhile, in order to further refine the multi-scale information extracted by the encoder, we propose a Multi-dimensional Convolutional Attention (MCA) module. To enhance the extraction of global information for accurate pixel classification, we propose a Dual Attention Transformer (DAT) module to extract global features of images. Furthermore, experimental results on the KITTI and NYU datasets demonstrate that the advantage of our proposed framework is that it achieves almost equivalent depth estimation performance to the current SOTA with fewer parameters and lower complexity. To the best of our knowledge, CATNet is the first work that achieves nearly the same depth estimation accuracy as Transformer-based large model encoders with so few parameters.}
}
@article{HU2024109993,
title = {Learning Foreground Information Bottleneck for few-shot semantic segmentation},
journal = {Pattern Recognition},
volume = {146},
pages = {109993},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109993},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300691X},
author = {Yutao Hu and Xin Huang and Xiaoyan Luo and Jungong Han and Xianbin Cao and Jun Zhang},
keywords = {Information bottleneck, Semantic segmentation, Few-shot learning, Feature undermining},
abstract = {Few-shot semantic segmentation aims to segment unseen classes with only a few annotated samples, which has great values for the real-world application in the wild. However, since the target class is treated as the background in the training, the network tends to extract much irrelevant nuisance factors, which results in the feature undermining problem for the target class. Consequently, it is difficult to produce an accurate segmentation map. To address this problem, in this paper, we apply the information bottleneck theory to few-shot semantic segmentation and propose the Foreground Information Bottleneck (FIB) module. Based on the support information, FIB module filters out the irrelevant information and promotes the foreground-related feature paradigms. Meanwhile, to solve the intractable mutual information and enable the end-to-end optimization of FIB module, we derive the Foreground Information Bottleneck Loss (FIBLoss) according to the inherent attribute of few-shot segmentation. Moreover, since there exists severe noise interference in the wild, we design a Target Information Refinement (TIR) block to further exploit discriminative cues of foreground. TIR block calculates the pairwise interaction and exploits the detailed information of the foreground object, which is beneficial to the feature refinement. Extensive experiments on two challenging datasets reflect the proposed FIB module significantly improves the performance of few-shot segmentation and delivers the state-of-the-art results.}
}
@article{MING2024110024,
title = {L0 regularized logistic regression for large-scale data},
journal = {Pattern Recognition},
volume = {146},
pages = {110024},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110024},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007215},
author = {Hao Ming and Hu Yang},
keywords = {Distributed learning,  penalty, KKT conditions, Oracle property, Correlated effects},
abstract = {In this paper, we investigate L0-regularized logistic regression models, and design two fast and efficient algorithms for high-dimensional correlated data and massive data, respectively. Our first algorithm, the Variable Sorted Active Set (VSAS) algorithm, is based on the local quadratic approximation of the KKT conditions for L0-penalized maximum log-likelihood function in high-dimensional correlated data. We establish an L∞ error upper bound for the estimator obtained by the VSAS algorithm and prove its optimal convergence rate. Moreover, when the target signal exceeds the detectable level, the estimator obtained by the VSAS algorithm can achieve the oracle estimator with high probability. Our second algorithm, Communication Effective Variable Sorted Active Set (CEVSAS), aims to solve high-dimensional and large-sample L0-regularized logistic regression models by reduce computational and communication costs, while maintaining estimation efficiency. Finally, simulations and real data demonstrate the effectiveness of our proposed VSAS and CEVSAS algorithms.}
}
@article{BAYRAKTAR2024109978,
title = {Conditional-pooling for improved data transmission},
journal = {Pattern Recognition},
volume = {145},
pages = {109978},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109978},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006763},
author = {Ertugrul Bayraktar and Cihat Bora Yigit},
keywords = {Pooling, Data sampling, Noise reduction, Feature selection},
abstract = {The progress in technology has led to an increase in the amount of data that can be generated and processed. Downsampling methods are employed to eliminate unnecessary data but can also lead to the loss of valuable information. Herein, we designed a pooling algorithm, Conditional-Pooling, that provides a transitive structure composed of average (avg.) and max-pooling methods, and it hosts the advantageous behaviors from both. We examined our approach on several image recognition and detection tasks using deep neural networks. The results reveal that our method excels over major rivals like max-pooling and avg. pooling when tested on MNIST, Fashion-MNIST, and CIFAR for classification, Pascal VOC for detection, COCO minitrains, and ADORESet for both classification and detection. The output of Conditional-Pooling preserves important image features such as edges and corners and contains more salient features that lead to more accurate results. The code for Conditional-Pooling will be available at https://github.com/bayraktare/conditional_pooling.}
}
@article{SHAO2024110048,
title = {A temporal densely connected recurrent network for event-based human pose estimation},
journal = {Pattern Recognition},
volume = {147},
pages = {110048},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110048},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007458},
author = {Zhanpeng Shao and Xueping Wang and Wen Zhou and Wuzhen Wang and Jianyu Yang and Youfu Li},
keywords = {Event signal, Human pose estimation, Dense connections, Recurrent network, Dataset},
abstract = {Event camera is an emerging bio-inspired vision sensors that report per-pixel brightness changes asynchronously. It holds noticeable advantage of high dynamic range, high speed response, and low power budget that enable it to best capture local motions in uncontrolled environments. This motivates us to unlock the potential of event cameras for human pose estimation, as the human pose estimation with event cameras is rarely explored. Due to the novel paradigm shift from conventional frame-based cameras, however, event signals in a time interval contain very limited information, as event cameras can only capture the moving body parts and ignores those static body parts, resulting in some parts to be incomplete or even disappeared in the time interval. This paper proposes a novel densely connected recurrent architecture to address the problem of incomplete information. By this recurrent architecture, we can explicitly model not only the sequential but also non-sequential geometric consistency across time steps to accumulate information from previous frames to recover the entire human bodies, achieving a stable and accurate human pose estimation from event data. Moreover, to better evaluate our model, we collect a large-scale multimodal event-based dataset that comes with human pose annotations, which is by far the most challenging one to the best of our knowledge. The experimental results on two public datasets and our own dataset demonstrate the effectiveness and strength of our approach. Code is available online for facilitating the future research.}
}
@article{WANG2024110035,
title = {Traffic sign attack via pinpoint region probability estimation network},
journal = {Pattern Recognition},
volume = {146},
pages = {110035},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110035},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300732X},
author = {Yue Wang and Minjie Liu and Yanli Ren and Xinpeng Zhang and Guorui Feng},
keywords = {Adversarial examples, Traffic sign attack, AI security, Neural networks, Probability estimation},
abstract = {Recent work show that Deep Neural Networks (DNNs) have created great performance in many tasks, but they are vulnerable to adversarial examples which trigger Artificial Intelligence (AI) security risks. Especially in the autonomous driving field, attacking a traffic sign classification network results in a serious consequence. Most existing researches prefer to digital level attacks focusing on smaller or more imperceptible adversarial noise. Given that attacks available to real-world implementation usually emerge in more security-critical scenarios, we propose an adaptively adversarial example generation algorithm for physical attacks in the real-world setting. Taking account of the traffic sign classification, our approach is divided into two steps. The first step is to generate a probability map which precisely predicts the probability of being attacked for each pixel in the input image through the proposed Pinpoint Region Probability Estimation Network (PRPEN) and meanwhile, try to reduce the size of the highlighted area in the map. It can also be regarded as a classification problem in which every pixel has two classes, suitable for attacking or not, including the restrictions on the number of items in the suitable sort. The second one is to remake a mask depending on the probability map and optimize adversarial patches only on what mask decides. Experimental results show that our method achieves almost 100% misclassification rate in several widely used networks with even smaller patches. We also find how to effectively disguise as a target class to mislead the DNN classifiers and improve AI security.}
}
@article{MO2024109994,
title = {RIC-CNN: Rotation-Invariant Coordinate Convolutional Neural Network},
journal = {Pattern Recognition},
volume = {146},
pages = {109994},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109994},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006921},
author = {Hanlin Mo and Guoying Zhao},
keywords = {Convolutional neural network, Image rotation, Rotational invariance, Data augmentation, Image classification, Patch verification},
abstract = {Due to the lack of rotation invariance in traditional convolution operations, even acting a slight rotation on the input can severely degrade the performance of Convolutional Neural Networks (CNNs). To address this, we propose a Rotation-Invariant Coordinate Convolution (RIC-C), which achieves natural invariance to arbitrary rotations around the input center without additional trainable parameters or data augmentation. We first evaluate the rotational invariance of RIC-C using the MNIST dataset and compare its performance with most previous rotation-invariant CNN models. RIC-C achieves state-of-the-art classification on the MNIST-rot test set without data augmentation and with lower computational costs. Then, the interchangeability of RIC-C with traditional convolution operations is demonstrated by seamlessly integrating it into common CNN models like VGG, ResNet, and DenseNet. We conduct remote sensing image classification on the NWPU VHR-10, MTARSI and AID datasets and patch matching experiments on the UBC benchmark dataset, showing that RIC-C significantly enhances the performance of CNN models across different applications, especially when training data is limited. Our codes can be downloaded from https://github.com/HanlinMo/Rotation-Invariant-Coordinate-Convolutional-Neural-Network.git.}
}
@article{CHENG2024110021,
title = {Parallel disentangling network for human–object interaction detection},
journal = {Pattern Recognition},
volume = {146},
pages = {110021},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110021},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007185},
author = {Yamin Cheng and Hancong Duan and Chen Wang and Zhijun Chen},
keywords = {Human–object interaction detection, Transformer},
abstract = {Human–object interaction (HOI) detection aims to localize and classify triplets of human, object and interaction from a given image. Earlier two-stage methods suffer both from mutually independent training processes and the interference of redundant negative human–object pairs. Prevailing one-stage transformer-based methods are free from the above problems by tackling HOI in an end-to-end manner. However, one-stage transformer-based methods carry the unnecessary entanglements of the query for different tasks, i.e., human–object detection and interaction classification, and thus bring in poor performance. In this paper, we propose a new transformer-based approach that parallelly disentangles human–object detection and interaction classification in a triplet-wise manner. To make each query focus on one specific task clearly, we exhaustively disentangle HOI by parallelly expanding the naive query in vanilla transformer as triple explicit queries. Then, we introduce a semantic communication layer to preserve the consistent semantic association of each HOI through mixing the feature representations of each query triplet of the correspondence constraint. Extensive experiments demonstrate that our proposed framework outperforms the existing methods and achieves the state-of-the-art performance, with significant reduction in parameters and FLOPs.}
}
@article{OJO2024110037,
title = {A topic modeling and image classification framework: The Generalized Dirichlet variational autoencoder},
journal = {Pattern Recognition},
volume = {146},
pages = {110037},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110037},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007343},
author = {Akinlolu Oluwabusayo Ojo and Nizar Bouguila},
keywords = {Generalized Dirichlet distribution, Correlation, Variational autoencoder, Topic models, Reparameterization, Image classification},
abstract = {Latent Dirichlet allocation model (LDA) has been widely used in topic modeling. Recent works have shown the effectiveness of integrating neural network mechanisms with this generative model for learning text representation. However, one of the significant setbacks of LDA is that it is based on a Dirichlet prior that has a restrictive covariance structure. All its variables are considered to be negatively correlated, which makes the model restrictive. In a practical sense, topics can be positively or negatively correlated. To address this problem, we proposed a generalized Dirichlet variational autoencoder (GD-VAE) for topic modeling. The Generalized Dirichlet (GD) distribution has a more general covariance structure than the Dirichlet distribution because it takes into account both positively and negatively correlated topics in the corpus. Our proposed model leverages rejection sampling variational inference using a reparameterization trick for effective training. GD-VAE compares favorably to recent works on topic models on several benchmark corpora. Experiments show that accounting for topics’ positive and negative correlations results in better performance. We further validate the superiority of our proposed framework on two image data sets. GD-VAE demonstrates its significance as an integral part of a classification architecture. For reproducibility and further research purposes, code for this work can be found at https://github.com/hormone03/GD-VAE.}
}
@article{ZHAO2024110032,
title = {Contrastive clustering with a graph consistency constraint},
journal = {Pattern Recognition},
volume = {146},
pages = {110032},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110032},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300729X},
author = {Yunxiao Zhao and Liang Bai},
keywords = {Contrastive learning, Contrastive clustering, Graph consistency constraint, Clustering uncertainty},
abstract = {Compared with classical contrastive learning methods, the performance of contrastive clustering is more easily affected by the quality of positive and negative samples, due to the fact that the clustering assumption requires neighbors of points as their positives. In order to reduce the effect of the uncertainty of positives and negatives on contrastive clustering, we propose a new contrastive clustering algorithm with graph consistency constraint. In this algorithm, a loss of graph consistency is proposed to reduce the impact of false negative samples by comparing the neighbor distribution of positive samples. Furthermore, an incremental training method is designed to control the quality of the selected positives. Extensive experiments show that our algorithm outperforms other deep clustering methods on wide-used benchmark data sets.}
}
@article{WEI2024109991,
title = {Towards self-explainable graph convolutional neural network with frequency adaptive inception},
journal = {Pattern Recognition},
volume = {146},
pages = {109991},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109991},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006891},
author = {Feifei Wei and Kuizhi Mei},
keywords = {Self-explainable neural network, Frequency adaptive filter, Graph convolutional neural networks (GCN)},
abstract = {Graph convolutional neural networks (GCNs) have demonstrated powerful representing ability of irregular data, e.g., skeletal data and graph-structured data, providing the effective mechanism to fuse the neighbor nodes. However, inheriting from the deep learning, GCN also lacks interpretability, which hinders its application to scenarios that have high demand for transparency. Although, there have been many efforts on the interpretability of deep learning, they mainly concentrate on i.i.d data that is hard to be deployed to GCNs, which involve not only the node feature, but also the graph structure. There are few works that attempt to explain it with post-hoc manner, which can be biased, resulting in mis-representation of the true explanation. Therefore, in this paper, we propose a framework, namely ExpFiGCN, that reveals explainability of the GCNs from the perspective of graph structure and mathematical analysis. Specifically, ExpFiGCN can find the most intrinsically relevant node to the central node and obtain the informative and discriminative signals while performing denoising. For the graph structure, we find K-nearest nodes; for the mathematical analysis, every channel of a node and its neighborhoods contribute dynamically to the final channel signal, which can capture the inherent difference of different channels and neighbor nodes. Meanwhile, it can enhance the representation ability of nodes and ameliorate the over-smoothing problem. On the other hand, our model can dynamically adjust the importance of neighborhoods to the central vertex. We empirically validate the effectiveness of the proposed framework ExpFiGCN on various benchmark datasets. Experimental results show that our method achieves substantial improvements and outperforms the state-of-the-art performance strikingly.}
}
@article{XU2024110065,
title = {Deep image clustering with contrastive learning and multi-scale graph convolutional networks},
journal = {Pattern Recognition},
volume = {146},
pages = {110065},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110065},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007628},
author = {Yuankun Xu and Dong Huang and Chang-Dong Wang and Jian-Huang Lai},
keywords = {Data clustering, Deep clustering, Image clustering, Graph convolutional network, Multi-scale structure learning},
abstract = {Deep clustering has shown its promising capability in joint representation learning and clustering via deep neural networks. Despite the significant progress, the existing deep clustering works mostly utilize some distribution-based clustering loss, lacking the ability to unify representation learning and multi-scale structure learning. To address this, this paper presents a new deep clustering approach termed Image clustering with contrastive learning and multi-scale Graph Convolutional Networks (IcicleGCN), which bridges the gap between convolutional neural network (CNN) and graph convolutional network (GCN) as well as the gap between contrastive learning and multi-scale structure learning for the deep clustering task. Our framework consists of four main modules, namely, the CNN-based backbone, the Instance Similarity Module (ISM), the Joint Cluster Structure Learning and Instance reconstruction Module (JC-SLIM), and the Multi-scale GCN module (M-GCN). Specifically, the backbone network with two weight-sharing views is utilized to learn the representations for the two augmented samples (from each image). The learned representations are then fed to ISM and JC-SLIM for joint instance-level and cluster-level contrastive learning, respectively, during which an auto-encoder in JC-SLIM is also pretrained to serve as a bridge to the M-GCN module. Further, to enforce multi-scale neighborhood structure learning, two streams of GCNs and the auto-encoder are simultaneously trained via (i) the layer-wise interaction with representation fusion and (ii) the joint self-adaptive learning. Experiments on multiple image datasets demonstrate the superior clustering performance of IcicleGCN over the state-of-the-art. The code is available at https://github.com/xuyuankun631/IcicleGCN.}
}
@article{WANG2024109967,
title = {Transformer-based network with temporal depthwise convolutions for sEMG recognition},
journal = {Pattern Recognition},
volume = {145},
pages = {109967},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109967},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006659},
author = {Zefeng Wang and Junfeng Yao and Meiyan Xu and Min Jiang and Jinsong Su},
keywords = {Surface electromyography, Feature learning, Gesture recognition, Transformer, Self-attention, Temporal depthwise convolution},
abstract = {Considerable progress has been made in pattern recognition of surface electromyography (sEMG) with deep learning, bringing improvements to sEMG-based gesture classification. Current deep learning techniques are mainly based on convolutional neural networks (CNNs), recurrent neural networks (RNNs), and their hybrids. However, CNNs focus on spatial and local information, while RNNs are unparallelizable, and they suffer from gradient vanishing/exploding. Their hybrids often face problems of model complexity and high computational cost. Because sEMG signals have a sequential nature, motivated by the sequence modeling network Transformer and its self-attention mechanism, we propose a Transformer-based network, temporal depthwise convolutional Transformer (TDCT), for sparse sEMG recognition. With this network, higher recognition accuracy is achieved with fewer convolution parameters and a lower computational cost. Specifically, this network has parallel capability and can capture long-range features inside sEMG signals. We improve the locality and channel correlation capture of multi-head self-attention (MSA) for sEMG modeling by replacing the linear transformation with the proposed temporal depthwise convolution (TDC), which can reduce the convolution parameters and computations for feature learning performance. Four sEMG datasets, Ninapro DB1, DB2, DB5, and OYDB, are used for evaluations and comparisons. In the results, our model outperforms other methods, including Transformer-based networks, in most windows at recognizing the raw signals of sparse sEMG, thus achieving state-of-the-art classification accuracy.}
}
@article{ZHAO2024109983,
title = {Coherent chord computation and cross ratio for accurate ellipse detection},
journal = {Pattern Recognition},
volume = {146},
pages = {109983},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109983},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006817},
author = {Mingyang Zhao and Xiaohong Jia and Lei Ma and Li-Ming Hu and Dong-Ming Yan},
keywords = {Ellipse detection, Chord computation, Cross ratio, Hough transform},
abstract = {This paper presents a new method for detecting ellipses in images, which has many applications in pattern recognition and robotic tasks. Previous approaches typically use sophisticated arc grouping strategies or calculate differential such as tangents, and thereby they are less efficient or more sensitive to noise. In this work, we present a novel ellipse detector, based on the simple yet effective chord computation, and on the projective invariant cross ratio, which achieves promising performance in both accuracy and efficiency. First, elliptical arcs are extracted by fast vector computations along with the removal of straight segments to speed up detection. Then, arcs from the same ellipse are grouped together according to the relative location and the intersecting chord constraints, both are on coherent chord computation without differential. Additionally, an efficient additive principle is applied to further accelerate the grouping process. Finally, a novel and robust verification by area-deduced cross ratio is introduced to pick out salient ellipses. Compared with predecessor methods, cross ratio is not only simple for computation, but also has invariant properties (used to discriminate ellipses). Extensive experiments on seven public datasets (including synthetic and real-world images) are implemented. The results highlight the salient advantages of the proposed method compared to state-of-the-art detectors: Easier to implementation, more robust against occlusion and noise, as well as attaining higher F-measure.}
}
@article{TAO2024110061,
title = {Objformer: Boosting 3D object detection via instance-wise interaction},
journal = {Pattern Recognition},
volume = {146},
pages = {110061},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110061},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007586},
author = {Manli Tao and Chaoyang Zhao and Ming Tang and Jinqiao Wang},
keywords = {3D object detection, Point clouds, Incompletion and occlusion, Instance-wise interaction},
abstract = {Deep learning on point clouds drives 3D object detection. Despite rapid progress, point-based methods still suffer from the problems such as incompletion and occlusion, which are caused by the material properties of objects and cluttered scenes. These difficult targets increase the difficulty of identification or even lead to misidentification, severely weakening the performance of point-based methods on 3D object detection. To alleviate the above problems, we propose the Objformer to boost point-based 3D object detection via instance-wise interaction. We design an instance feature encoder to encode clean instance features, which contain key geometric priors and holistic semantic information. Further, an instance interaction module is devised to aggregate the complementary features across instances with label-guided interaction, boosting the performance of the 3D object detection. Experiments show that Objformer outperforms previous point-based state-of-the-arts on two popular benchmarks, ScanNet V2 and SUN RGB-D. Especially, our single-modal Objformer even outperforms the competing advanced multi-modal fusion method on both SUN RGB-D and ScanNet V2.}
}
@article{ZHANG2024110053,
title = {Joint discriminative representation learning for end-to-end person search},
journal = {Pattern Recognition},
volume = {147},
pages = {110053},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110053},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007501},
author = {Pengcheng Zhang and Xiaohan Yu and Xiao Bai and Chen Wang and Jin Zheng and Xin Ning},
keywords = {Person search, Person re-identification, Part segmentation, Batch sampling},
abstract = {Person search simultaneously detects and retrieves a query person from uncropped scene images. Existing methods are either two-step or end-to-end. The former employs two standalone models for the two sub-tasks, while the latter conducts person search with a unified model. Despite encouraging progress, most existing end-to-end methods focus on balancing the model between detection and retrieval sub-tasks, while ignoring to enhance the learned representation for retrieval, which leads to inferior accuracy to two-step approaches. To that end, we propose a novel hierarchical framework that jointly optimizes instance-aware and part-aware embedding to enable discriminative representation learning. Specifically, we develop a region-of-interest cosegment (ROICoseg) module that captures part-aware information without requiring extra annotations to enable fine-grained discriminative representation. On top of that, a Contextual Instance Batch Sampling (CIBS) method is introduced to effectively employ contextual information for constructing training batches, thus facilitating effective instance-aware representation learning. We further introduce the first cross-door person search dataset (CDPS) that retrieves a target person in outdoor cameras with an indoor captured image or vice versa. Extensive experiments show that our proposed model achieves competitive performance on CUHK-SYSU and outperforms state-of-the-art end-to-end methods on the more challenging PRW and CDPS.11Code and dataset are available at https://github.com/PatrickZad/CDPS.}
}
@article{ZHANG2024109979,
title = {A vision transformer for fine-grained classification by reducing noise and enhancing discriminative information},
journal = {Pattern Recognition},
volume = {145},
pages = {109979},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109979},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006775},
author = {Zi-Chao Zhang and Zhen-Duo Chen and Yongxin Wang and Xin Luo and Xin-Shun Xu},
keywords = {Vision transformer, Complementary information integration, Region attention, Fine-grained image recognition},
abstract = {Recently, several Vision Transformer (ViT) based methods have been proposed for Fine-Grained Visual Classification (FGVC). These methods significantly surpass existing CNN-based ones, demonstrating the effectiveness of ViT in FGVC tasks. However, there are some limitations when applying ViT directly to FGVC. First, ViT needs to split images into patches and calculate the attention of every pair, which may result in heavy noise calculation during the training phase and unsatisfying performance when handling fine-grained images with complex backgrounds and small objects. Second, complementary information is important for FGVC, but a standard ViT works by using the class token in the final layer for classification which is not enough to extract comprehensive fine-grained information at different levels. Third, the class token fuses the information of all patches in the same manner, in other words, the class token treats each patch equally. However, the discriminative parts should be more critical. To address these issues, we propose ACC-ViT including three novel components, i.e., Attention Patch Combination (APC), Critical Regions Filter (CRF), and Complementary Tokens Integration (CTI). Thereinto, APC pieces informative patches from two images to generate a new image to mitigate the noisy calculation and reinforce the differences between images. CRF emphasizes tokens corresponding to discriminative regions to generate a new class token for subtle feature learning. To extract comprehensive information, CTI integrates complementary information captured by class tokens in different ViT layers. We conduct comprehensive experiments on four widely used datasets and the results demonstrate that ACC-ViT can achieve competitive performance. The source code is available at https://github.com/Hector0426/fine-grained-image-classification-with-vit.}
}
@article{SHI2024109955,
title = {Attack-invariant attention feature for adversarial defense in hyperspectral image classification},
journal = {Pattern Recognition},
volume = {145},
pages = {109955},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109955},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006532},
author = {Cheng Shi and Ying Liu and Minghua Zhao and Chi-Man Pun and Qiguang Miao},
keywords = {Hyperspectral image classification, Adversarial defense, Attack-invariant attention feature, Adversarial attack},
abstract = {Although deep neural networks (DNNs) have achieved excellent performance on hyperspectral image (HSI) classification tasks, their robustness is threatened by carefully created adversarial examples. Therefore, adversarial defense methods have provided an effective defense strategy to protect HSI classification networks. However, most defense models are highly dependent on known types of adversarial examples, which leads to poor generalization to defend against unknown attacks. In this study, we propose an attack-invariant attention feature-based defense (AIAF-Defense) model to improve the generalization ability of the defense model. Specifically, the AIAF-Defense model has an encoder–decoder structure to remove the perturbations from the HSI adversarial examples. We design a feature-disentanglement network as the encoder structure to decouple the attack-invariant spectral–spatial feature and attack-variant feature in the adversarial example and apply a decoder structure to reconstruct the legitimate HSI example. In addition, an attention-guided reconstruction loss is proposed to address the attention-shift problem caused by perturbation and provide an attention constraint for the extraction of attack-invariant attention features. Extensive experiments are conducted on three benchmark hyperspectral image datasets, the PaviaU, HoustonU 2018, and Salinas datasets, and the obtained results show that the proposed AIAF-Defense model improves the defense ability on both known and unknown adversarial attacks. The code is available at https://github.com/AAAA-CS/AIAF_HyperspectralAdversarialDefense.}
}
@article{ZHAO2024110025,
title = {A non-regularization self-supervised Retinex approach to low-light image enhancement with parameterized illumination estimation},
journal = {Pattern Recognition},
volume = {146},
pages = {110025},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110025},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007227},
author = {Zunjin Zhao and Hexiu Lin and Daming Shi and Guoqing Zhou},
keywords = {Low-light image enhancement, Illumination estimation, Parameterization, Bilateral grid, Non-regularization},
abstract = {In current Retinex-based low-light image enhancement (LLIE) methods, fine-tuning regularization parameters for Retinex decomposition and illumination estimation can be cumbersome. To address this, we present a novel non-regularization self-supervised Retinex approach for illumination estimation. Our contributions are twofold: First, we introduce a self-supervised method that incorporates edge-aware smoothness properties in bilateral learning, eliminating the need for regularization terms and simplifying parameter adjustments. Second, to enforce smoothness constraints on the estimated bilateral grid, we propose a bilateral grid parameterization network. This network employs a generative encoder to parameterize the bilateral grid of illumination and a trainable slicing layer guided by a map, reconstructing the grid into an illumination map. Despite the absence of regularization terms, our model excels in generating piece-wise smooth illumination, resulting in enhanced naturalness and improved contrast in images. Our model offers exceptional flexibility by eliminating the need for additional regularization terms and parameter fine-tuning. Moreover, it does not depend on external datasets for training, overcoming dataset collection challenges. Extensive experiments, comparing our model with eight state-of-the-art methods across five public available datasets, unequivocally demonstrate our model’s state-of-the-art performance based on key metrics such as NIQE, NIQMC, and CPCQI. These results reaffirm the effectiveness of our approach in low-light image enhancement. Code will be available at: https://github.com/zhaozunjin/NeurBR.}
}
@article{WU2024109995,
title = {Collaborative contrastive learning for hypergraph node classification},
journal = {Pattern Recognition},
volume = {146},
pages = {109995},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109995},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006933},
author = {Hanrui Wu and Nuosi Li and Jia Zhang and Sentao Chen and Michael K. Ng and Jinyi Long},
keywords = {Hypergraph, Hypergraph convolution, Contrastive learning, Graph convolution, Node classification},
abstract = {Plenty of models have been presented to handle the hypergraph node classification. However, very few of these methods consider contrastive learning, which is popular due to its great power to represent instances. This paper makes an attempt to leverage contrastive learning to hypergraph representation learning. Specifically, we propose a novel method called Collaborative Contrastive Learning (CCL), which incorporates a generated standard graph with the hypergraph. The main technical contribution here is that we develop a collaborative contrastive schema, which performs contrast between the node views obtained from the standard graph and hypergraph in each network layer, thus making the contrast collaborative. To be precise, in the first layer, the view from the standard graph is used to augment that from the hypergraph. Then, in the next layer, the augmented features are adopted to train a new representation to augment the view from the standard graph conversely. With this setting, the learning procedure is alternated between the standard graph and hypergraph. As a result, the learning on the standard graph and hypergraph is collaborative and leads to the final informative node representation. Experimental results on several widely used datasets validate the effectiveness of the proposed model.}
}
@article{DU2024109990,
title = {Aggregated-attention deformable convolutional network for few-shot SAR jamming recognition},
journal = {Pattern Recognition},
volume = {146},
pages = {109990},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109990},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300688X},
author = {Jinbiao Du and Weiwei Fan and Chen Gong and Jun Liu and Feng Zhou},
keywords = {Few-shot learning, Modulated deformable convolution, Aggregated attention, SAR jamming recognition},
abstract = {This work simultaneously addresses the challenges of unseen classes and low-data problems on synthetic aperture radar jamming recognition (SAR-JR). Currently, very few studies have tackled both challenges. Inspired by the success of few-shot learning, which learns a robust model from a few instances, we formulate SAR-JR as a few-shot task in a metric-learning framework to alleviate the above challenges. Against the jamming features with significant dispersion and complex geometric transformations, as well as feature obscuration in time–frequency images (TF), we propose an aggregated-attention deformable convolutional network (A2-DCNet) framework consisting of an aggregated-attention deformable convolutional module (A2-DC-Module) and a prototype classification module based on polynomial loss (PolyLoss-PC-Module). The former learns informative and refined embeddings from the TF images, while the latter performs the SAR-JR in an embedding space by calculating distances to prototypes of each class. Specifically, the modulated deformable convolution of the A2-DC-Module can capture long-range spatial contextual information from a global perspective, while the aggregated attention is designed to refine the representations of obscured features in the TF images. To further optimize the framework, we introduce a novel PolyLoss and customize the optimal form for our model to learn an embedding space with robust inter-class separability. Finally, to realize few-shot SAR-JR tasks, we simulate a novel dataset called JamSet. Extensive experimental results on our dataset have demonstrated substantial improvement of our proposed A2-DCNet method over the benchmarks.}
}