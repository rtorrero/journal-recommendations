@article{JIANG2023109227,
title = {Deep hybrid model for single image dehazing and detail refinement},
journal = {Pattern Recognition},
volume = {136},
pages = {109227},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109227},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007063},
author = {Nanfeng Jiang and Kejian Hu and Ting Zhang and Weiling Chen and Yiwen Xu and Tiesong Zhao},
keywords = {Haze removal, Details refinement, Image processing},
abstract = {Deep learning technologies have been applied in Single Image Dehazing (SID) tasks successfully. However, most SID algorithms seldom consider to refine image details during dehazing. Therefore, there exist some detail-loss regions in dehazed results. To solve this issue, we design a deep hybrid network to improve dehazing performance and remedy the loss of details. Different from existing algorithms that usually ignore detail refinement and adopt a unified framework to remove haze, we propose to treat dehazing and detail refinement as two separate tasks, so that each task could be solved via different ways. Particularly, we design two sub-networks with a multi-term loss function. First, for removing haze effectively, we introduce the Squeeze-and-Excitation (SE) to design a haze residual attention sub-network, which is used to reconstruct the dehazed image. Second, as for remedying details, we take the previous dehazed image as the input to a detail refinement sub-network, where the image details can be enhanced via multi-scale contextual information aggregation. Through the joint training of two sub-network, the haze can be removed clearly and the image details can be preserved well. Moreover, the detail refinement sub-network can be detached into other existing dehazing methods to improve their model performances. Extensive experiments also verify the superiority of our proposed network against recently proposed state-of-the-arts.}
}
@article{YAN2023109267,
title = {Hybrid optimization with unconstrained variables on partial point cloud registration},
journal = {Pattern Recognition},
volume = {136},
pages = {109267},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109267},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007464},
author = {Yuanjie Yan and Junyi An and Jian Zhao and Furao Shen},
keywords = {Point cloud registration, Optimization},
abstract = {3D point cloud registration is a fundamental problem in computer vision (CV) and computer graphics (CG). Recently, a series of learning-based algorithms have been proposed to show the advantages in registration accuracy and inference speed. However, those learning-based methods usually ignore transformations with constrained rotations and translations in registration. In this paper, we propose a novel hybrid optimization method to solve the constrained rotational and translational transformations. A mapping function is introduced to deal with the restrained variables in optimization. Our method achieves superior performance on the Multi-View Partial Point dataset, which won the first place on the registration challenge in ICCV 2021. The method is also validated on the synthetic datasets ModelNet, ICL-NUIM, and the realistic 3DMatch dataset. We demonstrate that the global optimization methods still have great potential research for point cloud registration. The code is available at https://github.com/Dizzy-cell/HOUV.}
}
@article{HUANG2023109170,
title = {SAPENet: Self-Attention based Prototype Enhancement Network for Few-shot Learning},
journal = {Pattern Recognition},
volume = {135},
pages = {109170},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109170},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006495},
author = {Xilang Huang and Seon Han Choi},
keywords = {Few-shot learning, Multi-head self-attention mechanism, Image classification, -Nearest neighbor},
abstract = {Few-shot learning considers the problem of learning unseen categories given only a few labeled samples. As one of the most popular few-shot learning approaches, Prototypical Networks have received considerable attention owing to their simplicity and efficiency. However, a class prototype is typically obtained by averaging a few labeled samples belonging to the same class, which treats the samples as equally important and is thus prone to learning redundant features. Herein, we propose a self-attention based prototype enhancement network (SAPENet) to obtain a more representative prototype for each class. SAPENet utilizes multi-head self-attention mechanisms to selectively augment discriminative features in each sample feature map, and generates channel attention maps between intra-class sample features to attentively retain informative channel features for that class. The augmented feature maps and attention maps are finally fused to obtain representative class prototypes. Thereafter, a local descriptor-based metric module is employed to fully exploit the channel information of the prototypes by searching k similar local descriptors of the prototype for each local descriptor in the unlabeled samples for classification. We performed experiments on multiple benchmark datasets: miniImageNet, tieredImageNet, and CUB-200-2011. The experimental results on these datasets show that SAPENet achieves a considerable improvement compared to Prototypical Networks and also outperforms related state-of-the-art methods.}
}
@article{HUANG2023109255,
title = {An ensemble hierarchical clustering algorithm based on merits at cluster and partition levels},
journal = {Pattern Recognition},
volume = {136},
pages = {109255},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109255},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007348},
author = {Qirui Huang and Rui Gao and Hoda Akhavan},
keywords = {Ensemble clustering, Cluster consensus, Hyper-cluster, Merit level, Robustness measure},
abstract = {Ensemble clustering has emerged as a combination of several basic clustering algorithms to achieve high quality final clustering. However, this technique is challenging due to the complexities in primary clusters such as overlapping, vagueness, instability and uncertainty. Typically, ensemble clustering uses all the primary clusters into partitions for consensus, where the merits of a cluster or a partition can be considered to improve the quality of the consensus. In general, the robustness of a partition may be poorly measured, while having some high-quality clusters. Inspired by the evaluation of cluster and partition, this paper proposes an ensemble hierarchical clustering algorithm based on the cluster consensus selection approach. Here, the selection of a subset of primary clusters from partitions based on their merit level is emphasized. Merit level is defined using the development of Normalized Mutual Information measure. Clusters of basic clustering algorithms that satisfy the predefined threshold of this measure are selected to participate in the final consensus. In addition, the consensus of the selected primary clusters to create the final clusters is performed based on the clusters clustering technique. In this technique, the selected primary clusters are re-clustered to create hyper-clusters. Finally, the final clusters are formed by assigning instances to hyper-clusters with the highest similarity. Here, an innovative criterion based on merit and cluster size for defining similarity is presented. The performance of the proposed algorithm has been proven by extensive experiments on real-world datasets from the UCI repository compared to state-of-the-art algorithms such as CPDM, ENMI, IDEA, CFTLC and SSCEN.}
}
@article{MIAO2023109210,
title = {On better detecting and leveraging noisy samples for learning with severe label noise},
journal = {Pattern Recognition},
volume = {136},
pages = {109210},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109210},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006896},
author = {Qing Miao and Xiaohe Wu and Chao Xu and Wangmeng Zuo and Zhaopeng Meng},
keywords = {Severe label noise, Lipschitz regularization, Adaptive modeling and detection of label noise, Semi-supervised learning},
abstract = {Despite the success of learning with noisy labels, existing approaches show limited performance when the noise level is extremely high, since deep neural networks (DNNs) are easily overfit to the training set with corrupted labels. In this paper, we introduce Lipschitz regularization to prevent the DNNs from over-fitting to noisy labels quickly. Meanwhile, to better detect and leverage the noisy samples, we propose a Lipschitz regularization based framework with a combination of adaptive modeling and detection module and improved semi-supervised learning. We propose to adaptively model the real distribution of the training set, and the implicit individual clean/noisy distribution, instead of parametric models. With Bayes’ rule, we then compute the posterior probability of a sample being clean, which provides a dynamic threshold for the detection of noisy labels. To reduce training instability caused by less labeled data with severe label noise, we improve the semi-supervised learning by combining the advantages of Mixup and FixMatch. It can not only increase the diversity of unlabeled samples, but also improve the generalization capability of the DNNs to avoid over-fitting. Experiments on several benchmarks demonstrate that our approach achieves comparable results with the state-of-the-art methods in the less-noisy environment, and obtains a substantial improvement (∼ 8% and ∼ 6% in accuracy on CIFAR-10 and CIFAR-100 respectively) with severe noise.}
}
@article{GUTIERREZLOPEZ2023109158,
title = {Optimum Bayesian thresholds for rebalanced classification problems using class-switching ensembles},
journal = {Pattern Recognition},
volume = {135},
pages = {109158},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109158},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006379},
author = {Aitor Gutiérrez-López and Francisco-Javier González-Serrano and Aníbal R. Figueiras-Vidal},
keywords = {Bayesian framework, Ensembles, Rebalancing techniques, Imbalanced classification, Label switching},
abstract = {Asymmetric label switching is an effective and principled method for creating a diverse ensemble of learners for imbalanced classification problems. This technique can be combined with other rebalancing mechanisms, such as those based on cost policies or class proportion modifications. In this study, and under the Bayesian theory framework, we specify the optimal decision thresholds for the combination of these mechanisms. In addition, we propose using a gating network to aggregate the learners contributions as an additional mechanism to improve the overall performance of the system.}
}
@article{WU2023109231,
title = {SpatioTemporal focus for skeleton-based action recognition},
journal = {Pattern Recognition},
volume = {136},
pages = {109231},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109231},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007105},
author = {Liyu Wu and Can Zhang and Yuexian Zou},
keywords = {Action recognition, Skeleton topology, Graph convolutional network},
abstract = {Graph convolutional networks (GCNs) are widely adopted in skeleton-based action recognition due to their powerful ability to model data topology. We argue that the performance of recent proposed skeleton-based action recognition methods is limited by the following factors. First, the predefined graph structures are shared throughout the network, lacking the flexibility and capacity to model the multi-grain semantic information. Second, the relations among the global joints are not fully exploited by the graph local convolution, which may lose the implicit joint relevance. For instance, actions such as running and waving are performed by the co-movement of body parts and joints, e.g., legs and arms, however, they are located far away in physical connection. Inspired by the recent attention mechanism, we propose a multi-grain contextual focus module, termed MCF, to capture the action associated relation information from the body joints and parts. As a result, more explainable representations for different skeleton action sequences can be obtained by MCF. In this study, we follow the common practice that the dense sample strategy of the input skeleton sequences is adopted and this brings much redundancy since number of instances has nothing to do with actions. To reduce the redundancy, a temporal discrimination focus module, termed TDF, is developed to capture the local sensitive points of the temporal dynamics. MCF and TDF are integrated into the standard GCN network to form a unified architecture, named STF-Net. It is noted that STF-Net provides the capability to capture robust movement patterns from these skeleton topology structures, based on multi-grain context aggregation and temporal dependency. Extensive experimental results show that our STF-Net significantly achieves state-of-the-art results on three challenging benchmarks NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics-Skeleton.}
}
@article{HEZAM2023109186,
title = {COVID-19 and Rumors: A Dynamic Nested Optimal Control Model},
journal = {Pattern Recognition},
volume = {135},
pages = {109186},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109186},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006653},
author = {Ibrahim M. Hezam and Abdulkarem Almshnanah and Ahmed A. Mubarak and Amrit Das and Abdelaziz Foul and Adel Fahad Alrasheedi},
keywords = {COVID-19, genetic algorithm, KKT, nested optimal control, rumors},
abstract = {Unfortunately, the COVID-19 outbreak has been accompanied by the spread of rumors and depressing news. Herein, we develop a dynamic nested optimal control model of COVID-19 and its rumor outbreaks. The model aims to curb the epidemics by reducing the number of individuals infected with COVID-19 and reducing the number of rumor-spreaders while minimizing the cost associated with the control interventions. We use the modified approximation Karush–Kuhn–Tucker conditions with the Hamiltonian function to simplify the model before solving it using a genetic algorithm. The present model highlights three prevention measures that affect COVID-19 and its rumor outbreaks. One represents the interventions to curb the COVID-19 pandemic. The other two represent interventions to increase awareness, disseminate the correct information, and impose penalties on the spreaders of false rumors. The results emphasize the importance of interventions in curbing the spread of the COVID-19 pandemic and its associated rumor problems alike.}
}
@article{CHEN2023109242,
title = {Position-aware and structure embedding networks for deep graph matching},
journal = {Pattern Recognition},
volume = {136},
pages = {109242},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109242},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200721X},
author = {Dongdong Chen and Yuxing Dai and Lichi Zhang and Zhihong Zhang and Edwin R. Hancock},
keywords = {Graph Matching, Graph Embedding, Deep Neural Network},
abstract = {Graph matching refers to the process of establishing node correspondences based on edge-to-edge constraints between graph nodes. This can be formulated as a combinatorial optimization problem under node permutation and pairwise consistency constraints. The main challenge of graph matching is to effectively find the correct match while reducing the ambiguities produced by similar nodes and edges. In this paper, we present a novel end-to-end neural framework that converts graph matching to a linear assignment problem in a high-dimensional space. This is combined with relative position information at the node level, and high-order structural arrangement information at the subgraph level. By capturing the relative position attributes of nodes between different graphs and the subgraph structural arrangement attributes, we can improve the performance of graph matching tasks, and establish reliable node-to-node correspondences. Our method can be generalized to any graph embedding setting, which can be used as components to deal with various graph matching problems answered with deep learning methods. We validate our method on several real-world tasks, by providing ablation studies to evaluate the generalization capability across different categories. We also compare state-of-the-art alternatives to demonstrate performance.}
}
@article{NIU2023109202,
title = {A multi-layer memory sharing network for video captioning},
journal = {Pattern Recognition},
volume = {136},
pages = {109202},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109202},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006811},
author = {Tian-Zi Niu and Shan-Shan Dong and Zhen-Duo Chen and Xin Luo and Zi Huang and Shanqing Guo and Xin-Shun Xu},
keywords = {Video captioning, Multi-layer network, Memory sharing, Enhanced gated recurrent unit},
abstract = {Over the past several years, video captioning has received much attention in computer vision and machine learning communities. Many models utilize an RNN-based decoder to generate sentences describing the content of a video. They have achieved much progress; however, few methods adopt a decoder with more than three layers because an RNN-based model with more layers may become hard to train, time-consuming or even deteriorate at a certain depth. To address the limitation, we propose a Multi-layer memory sharing Network, MesNet for short, which allows more layers to be stacked without compromising performance. In MesNet, we construct a novel memory sharing structure to strengthen the connections between layers and make the model easier to train. More specifically, we design an Enhanced Gated Recurrent Unit (En-GRU) and stack it to construct a deeper network. Unlike traditional RNN-based multi-layer networks, the memory states of all layers in MesNet are cross-used at each iteration to mimic the brain’s complex connections. Extensive experiments on MSVD and MSR-VTT demonstrate that our method performs well and outperforms some state-of-the-art methods significantly. Our code is available at https://github.com/nbbb/MesNet.}
}
@article{LAN2023109214,
title = {Coherence-aware context aggregator for fast video object segmentation},
journal = {Pattern Recognition},
volume = {136},
pages = {109214},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109214},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006938},
author = {Meng Lan and Jing Zhang and Zengmao Wang},
keywords = {Video object segmentation, Semi-supervised learning, Spatio-temporal representation, Context},
abstract = {Semi-supervised video object segmentation (VOS) is a highly challenging problem that has attracted much research attention in recent years. Temporal context plays an important role in VOS by providing object clues from the past frames. However, most of the prevailing methods directly use the predicted temporal results to guide the segmentation of the current frame, while ignoring the coherence of temporal context, which may be misleading and degrade the performance. In this paper, we propose a novel model named Coherence-aware Context Aggregator (CCA) for VOS, which consists of three modules. First, a coherence-aware module (CAM) is proposed to evaluate the coherence of the predicted result of the current frame and then fuses the coherent features to update the temporal context. CAM can determine whether the prediction is accurate, thus guiding the update of the temporal context and avoiding the introduction of erroneous information. Second, we devise a spatio-temporal context aggregation (STCA) module to aggregate the temporal context with the spatial feature of the current frame to learn a robust and discriminative target representation in the decoder part. Third, we design a refinement module to refine the coarse feature generated from the STCA module for more precise segmentation. Additionally, CCA uses a cropping strategy and takes small-size images as input, thus making it computationally efficient and achieving a real-time running speed. Extensive experiments on four challenging benchmarks show that CCA achieves a better trade-off between efficiency and accuracy compared to state-of-the-art methods. The code will be public.}
}
@article{MATSUO2023109201,
title = {Deep attentive time warping},
journal = {Pattern Recognition},
volume = {136},
pages = {109201},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109201},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200680X},
author = {Shinnosuke Matsuo and Xiaomeng Wu and Gantugs Atarsaikhan and Akisato Kimura and Kunio Kashino and Brian Kenji Iwana and Seiichi Uchida},
keywords = {Dynamic time warping, Attention model, Metric learning, Time series classification, Online signature verification},
abstract = {Similarity measures for time series are important problems for time series classification. To handle the nonlinear time distortions, Dynamic Time Warping (DTW) has been widely used. However, DTW is not learnable and suffers from a trade-off between robustness against time distortion and discriminative power. In this paper, we propose a neural network model for task-adaptive time warping. Specifically, we use the attention model, called the bipartite attention model, to develop an explicit time warping mechanism with greater distortion invariance. Unlike other learnable models using DTW for warping, our model predicts all local correspondences between two time series and is trained based on metric learning, which enables it to learn the optimal data-dependent warping for the target task. We also propose to induce pre-training of our model by DTW to improve the discriminative power. Extensive experiments demonstrate the superior effectiveness of our model over DTW and its state-of-the-art performance in online signature verification.}
}
@article{ZHANG2023109250,
title = {Learning to restore multiple image degradations simultaneously},
journal = {Pattern Recognition},
volume = {136},
pages = {109250},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109250},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007294},
author = {Le Zhang and Kevin Bronik and Bartłomiej W. Papież},
keywords = {Image restoration, Image quality, Multiple degradations, MRI},
abstract = {Image corruptions are common in the real world, for example images in the wild may come with unknown blur, bias field, noise, or other kinds of non-linear distributional shifts, thus hampering encoding methods and rendering downstream task unreliable. Image upgradation requires a complicated balance between high-level contextualised information and spatial specific details. Existing approaches to solving the problems are designed to focus on single corruption, which unavoidably results in poor performance when the acquisitions suffer from multiple degradations. In this study, we investigate the possibility of handling multiple degradations and enhancing the quality of images via deblurring, bias field correction, and denoising. To tackle the problems with propagating errors caused by independent learning, we propose a unified and scalable framework, which consists of three special decoders. Two decoders learn artifact attention from provided images thereby generating realistic individual artifact and multiple artifacts on single image; the third decoder is trained towards removing artifact on the synthetic image with multiple corruptions thereby generating high quality image. We additionally provide improvements over previous image degradation synthesis approaches by modelling multiple image degradations directly from data observations. We first create a toy MNIST dataset and investigate the properties of the proposed algorithm. We then use brain MRI datasets to demonstrate our method’s robustness, including both simulated (where necessary) and real-world artifacts. In addition, our method can be used for single/or multiple degradation(s) synthesis by implementing the learned degradation operators in a new domain from a given dataset. The code will be released upon acceptance of the paper.}
}
@article{DING2023109238,
title = {A Sampling-Based Density Peaks Clustering Algorithm for Large-Scale Data},
journal = {Pattern Recognition},
volume = {136},
pages = {109238},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109238},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007178},
author = {Shifei Ding and Chao Li and Xiao Xu and Ling Ding and Jian Zhang and Lili Guo and Tianhao Shi},
keywords = {Density peaks clustering, Sampling method, TI search strategy, Large-scale data},
abstract = {With the rapid development of information technology, massive amount of data is generated. How to discover useful information to support decision-making has become one of the focuses of scholar's research. Clustering is thought to be one of the main means to deal with large-scale data. Density peaks clustering (DPC) is an effective density-based clustering algorithm which is widely applied in numerous fields because of its satisfactory performance. However, the computational complexity of DPC is O(N2) which is not friendly to large-scale data. To solve this issue, a sampling-based density peaks clustering algorithm for large-scale data (SDPC) is proposed. Firstly, a sampling method is used to reduce the distance calculations. Secondly, approximate representatives are identified by an improved TI search strategy which further accelerates the clustering process. Afterwards, the approximate representatives are clustered by DPC. Finally, the remaining points are allocated to the same cluster as its nearest representatives. Experimental results on both synthetic datasets and real-world datasets illustrate that SDPC is more efficient than DPC, while its clustering performance maintains the same level as DPC.}
}
@article{WANG2023109266,
title = {AdaNS: Adaptive negative sampling for unsupervised graph representation learning},
journal = {Pattern Recognition},
volume = {136},
pages = {109266},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109266},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007452},
author = {Yu Wang and Liang Hu and Wanfu Gao and Xiaofeng Cao and Yi Chang},
keywords = {Graph representation learning, Negative sampling, Noise contrastive estimation},
abstract = {Recently, unsupervised graph representation learning has attracted considerable attention through effectively encoding graph-structured data without semantic annotations. To accelerate its training, noise contrastive estimation (NCE) samples uniformly negative examples to fit an unnormalized graph model. However, this uniform sampling strategy may easily lead to slow convergence, even the vanishing gradient problem. In this paper, we theoretically show that sampling those hard negatives close to the current anchor can relieve the above difficulties. With this finding, we then propose an Adaptive Negative Sampling strategy, namely AdaNS, which efficiently samples the hard negatives from the mixing distribution regarding the dimensional elements of the current node representation. Experiments show that our AdaNS sampling strategy applied on top of representative unsupervised models, e.g., DeepWalk, GraphSAGE, can outperform the existing negative sampling strategies in the tasks of node classification and visualization. This also further demonstrates that sampling those hard negatives can bring performance improvements for learning the node representations.}
}
@article{JIANG2023109169,
title = {Robust low tubal rank tensor completion via factor tensor norm minimization},
journal = {Pattern Recognition},
volume = {135},
pages = {109169},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109169},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006483},
author = {Wei Jiang and Jun Zhang and Changsheng Zhang and Lijun Wang and Heng Qi},
keywords = {Low tubal rank tensor completion, Schatten- norm, Tensor double nuclear norm, Tensor frobenius/nuclear norm},
abstract = {Recent research has demonstrated that low tubal rank recovery based on tensor has received extensive attention. In this correspondence, we define tensor double nuclear norm and tensor Frobenius/nuclear hybrid norm to induce a surrogate for tensor tubal rank, and prove that they are equivalent to tensor Schatten-p norm for p=1/2 and p=2/3. Based on the definition, we propose two novel tractable tensor completion models called Double Nuclear norm regularized Tensor Completion (DNTC) and Frobenius/Nuclear hybrid norm regularized Tensor Completion (FNTC) by integrating these two norm minimization and factorization methods into a joint learning framework. Furthermore, we adopt invertible linear transforms to obtain low tubal rank tensors, which makes the model more flexible and effective. Two efficient algorithms are designed to solve the proposed tensor completion models by incorporating the convexity of the factor norms. Comprehensive experiments are conducted on synthetic and real datasets to achieve better results in comparison with some state-of-the-art approaches.}
}
@article{CHEN2023109185,
title = {LPCL: Localized prominence contrastive learning for self-supervised dense visual pre-training},
journal = {Pattern Recognition},
volume = {135},
pages = {109185},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109185},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006641},
author = {Zihan Chen and Hongyuan Zhu and Hao Cheng and Siya Mi and Yu Zhang and Xin Geng},
keywords = {Self-supervised learning, Contrastive learning, Dense representation},
abstract = {Self-supervised pre-training has attracted increasing attention given its promising performance in training backbone networks without using labels. By far, most methods focus on image classification with datasets containing iconic objects and simple background, e.g. ImageNet. However, these methods show sub-optimal performance for dense prediction tasks (e.g. object detection and scene parsing) when directly pre-training on datasets (e.g. PASCAL VOC and COCO) with multiple objects and cluttered backgrounds. Researchers explored self-supervised dense pre-training methods by adapting recent image pre-training methods. Nevertheless, they require a large number of negative samples and a long training time to reach reasonable performance. In this paper, we propose LPCL, a novel self-supervised representation learning method for dense predictions to settle these issues. To guide the instance information in multi-instance datasets, we define an online object patch selection module to select the local patches with the high possibility of containing instance area in the augmented views efficiently during learning. After obtaining the patches, we present a novel multi-level contrastive learning method considering the instance representation of global-level, local-level and position-level without using negative samples. We conduct extensive experiments with LPCL directly pre-trained on PASCAL VOC and COCO. For PASCAL VOC image classification task, our model achieves state-of-the-art 86.2% accuracy pre-trained on COCO(+9.7% top-1 accuracy compared with baseline BYOL). On object detection, instance segmentation and semantic segmentation task, our proposed model also achieved competitive results compared with other state-of-the-art methods.}
}
@article{DELMORAL2023109225,
title = {Pitfalls of assessing extracted hierarchies for multi-class classification},
journal = {Pattern Recognition},
volume = {136},
pages = {109225},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109225},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200704X},
author = {Pablo {del Moral} and Sławomir Nowaczyk and Anita Sant’Anna and Sepideh Pashami},
keywords = {Hierarchical multi-class classification, Multi-class classification, Class hierarchies},
abstract = {Using hierarchies of classes is one of the standard methods to solve multi-class classification problems. In the literature, selecting the right hierarchy is considered to play a key role in improving classification performance. Although different methods have been proposed, there is still a lack of understanding of what makes a hierarchy good and what makes a method to extract hierarchies perform better or worse. To this effect, we analyze and compare some of the most popular approaches to extracting hierarchies. We identify some common pitfalls that may lead practitioners to make misleading conclusions about their methods. To address some of these problems, we demonstrate that using random hierarchies is an appropriate benchmark to assess how the hierarchy’s quality affects the classification performance. In particular, we show how the hierarchy’s quality can become irrelevant depending on the experimental setup: when using powerful enough classifiers, the final performance is not affected by the quality of the hierarchy. We also show how comparing the effect of the hierarchies against non-hierarchical approaches might incorrectly indicate their superiority. Our results confirm that datasets with a high number of classes generally present complex structures in how these classes relate to each other. In these datasets, the right hierarchy can dramatically improve classification performance.}
}
@article{BECKHAM2023109209,
title = {Visual question answering from another perspective: CLEVR mental rotation tests},
journal = {Pattern Recognition},
volume = {136},
pages = {109209},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109209},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006884},
author = {Christopher Beckham and Martin Weiss and Florian Golemo and Sina Honari and Derek Nowrouzezahrai and Christopher Pal},
keywords = {Deep learning, Computer vision, Visual question answering, Contrastive learning, Clevr},
abstract = {Different types of mental rotation tests have been used extensively in psychology to understand human visual reasoning and perception. Understanding what an object or visual scene would look like from another viewpoint is a challenging problem that is made even harder if it must be performed from a single image. We explore a controlled setting whereby questions are posed about the properties of a scene if that scene was observed from another viewpoint. To do this we have created a new version of the CLEVR dataset that we call CLEVR Mental Rotation Tests (CLEVR-MRT). Using CLEVR-MRT we examine standard methods, show how they fall short, then explore novel neural architectures that involve inferring volumetric representations of a scene. These volumes can be manipulated via camera-conditioned transformations to answer the question. We examine the efficacy of different model variants through rigorous ablations and demonstrate the efficacy of volumetric representations.}
}
@article{SUN2023109157,
title = {A discriminatively deep fusion approach with improved conditional GAN (im-cGAN) for facial expression recognition},
journal = {Pattern Recognition},
volume = {135},
pages = {109157},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109157},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006367},
author = {Zhe Sun and Hehao Zhang and Jiatong Bai and Mingyang Liu and Zhengping Hu},
keywords = {Facial expression recognition, Discriminatively deep fusion approach, Improved conditional generative adversarial network, Discriminative loss function},
abstract = {Considering most deep learning-based methods heavily depend on huge labels, it is still a challenging issue for facial expression recognition to extract discriminative features of training samples with limited labels. Given above, we propose a discriminatively deep fusion (DDF) approach based on an improved conditional generative adversarial network (im-cGAN) to learn abstract representation of facial expressions. First, we employ facial images with action units (AUs) to train the im-cGAN to generate more labeled expression samples. Subsequently, we utilize global features learned by the global-based module and the local features learned by the region-based module to obtain the fused feature representation. Finally, we design the discriminative loss function (D-loss) that expands the inter-class variations while minimizing the intra-class distances to enhance the discrimination of fused features. Experimental results on JAFFE, CK+, Oulu-CASIA, and KDEF datasets demonstrate the proposed approach is superior to some state-of-the-art methods.}
}
@article{SHEHATA2023109197,
title = {Annotator-dependent uncertainty-aware estimation of gait relative attributes},
journal = {Pattern Recognition},
volume = {136},
pages = {109197},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109197},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006768},
author = {Allam Shehata and Yasushi Makihara and Daigo Muramatsu and Md Atiqur Rahman Ahad and Yasushi Yagi},
keywords = {Gait relative attribute, Relative label distribution, Relative score distribution, Annotator’s uncertainty, Transition matrix},
abstract = {In this paper, we describe an uncertainty-aware estimation framework for gait relative attributes. We specifically design a two-stream network model that takes a pair of gait videos as input. It then outputs a corresponding pair of Gaussian distributions of gait absolute attribute scores and annotator-dependent gait relative attribute label distributions. Moreover, we propose a differentiable annotator-independent uncertainty layer to estimate the gait relative attribute score distribution from the absolute distributions then map it to a relative attribute label distribution using the computation of cumulative distribution functions. Furthermore, we propose another annotator-dependent uncertainty layer to estimate the uncertainty on the gait relative attribute labels in terms of a set of trainable transition matrices. Finally, we design a joint loss function on the relative attribute label distribution to learn the model parameters. Experiments on two gait relative attribute datasets demonstrated the effectiveness of the proposed method against baselines in quantitative and qualitative evaluations.}
}
@article{YOU2023109173,
title = {Unsupervised Feature Selection via Neural Networks and Self-Expression with Adaptive Graph Constraint},
journal = {Pattern Recognition},
volume = {135},
pages = {109173},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109173},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006525},
author = {Mengbo You and Aihong Yuan and Dongjian He and Xuelong Li},
keywords = {Unsupervised feature selection, Manifold structure, Adaptive graph constraint, Neural networks},
abstract = {Unsupervised feature selection (UFS), which selects the most important feature subset and eliminates the unnecessary information for the upcoming data analysis, is a significant problem in machine learning and has been explored for years. Most UFS methods map features into a pseudo label space by multiplying a projection matrix constrained with sparsity to learn the mapping from the features to the labels. However, the mapping relationship is usually not linear, and linear regression may result in a suboptimal selection. To address this issue, we propose a novel UFS method, called neural networks embedded self-expression (NNSE). NNSE replaces the linear regression of traditional spectral analysis methods with neural networks to learn the pseudo label space. Besides, we embed neural networks into the self-expression model to improve the representative ability by preserving the local structure with an adaptive graph regularization module. Then we propose an efficient alternative iterative algorithm to solve the proposed model. Experimental results on 8 public datasets show NNSE outperforms the other state-of-the-art methods. Moreover, experimental results are also presented to show the convergence of the proposed method. The source code is available at: https://github.com/misteru/NNSE.}
}
@article{CAO2023109213,
title = {Joint classification and prediction of random curves using heavy‐tailed process functional regression},
journal = {Pattern Recognition},
volume = {136},
pages = {109213},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109213},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006926},
author = {Chunzheng Cao and Xin Liu and Shuren Cao and Jian Qing Shi},
keywords = {Functional data analysis, Outliers, Heavy-tailed process, Bayesian estimation, MCMC},
abstract = {We propose a heavy-tailed process functional regression to jointly perform classification and prediction of time-varying functional data. We use two independent scale mixtures of Gaussian Processes to respectively model random effects and random errors, yielding robust inferences against both magnitude and shape outliers. We classify random curves by posterior predictive probabilities of class labels and offer a weighted prediction of future curve trends. A Bayesian estimation procedure is implemented through an MCMC sampling algorithm. The performance of classification and prediction of the proposed model is evaluated using simulated studies and some real data sets.}
}
@article{LIU2023109184,
title = {Combining Deep Denoiser and Low-rank Priors for Infrared Small Target Detection},
journal = {Pattern Recognition},
volume = {135},
pages = {109184},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109184},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200663X},
author = {Ting Liu and Qian Yin and Jungang Yang and Yingqian Wang and Wei An},
keywords = {Low-rank prior, Deep denoiser prior, Infrared small target detection, Plug-and-play},
abstract = {Many existing low-rank methods have achieved good detection performance in uniform scenes, but they suffer from a high false alarm rate in complex noisy scenes. Therefore, it is important to improve the detection performance of low-rank models in noisy scenes. In this paper, we first formulate an implicit regularizer by plugging a denoising neural network (termed as deep denoiser), which can learn deep image priors from a large number of natural images. Then, we use the weighted sum of weighted tensor nuclear norm for more accurate background estimation. Finally, alternating direction multiplier method is used to solve the model under the plug-and-play framework. By integrating low-rank prior with deep denoiser prior, our model achieves higher accuracy. Experiments on different scenes demonstrate that our method achieves an improved performance in terms of visual effects and quantitative metrics. Specially, the overall accuracy of AUC value (AUCOA) achieved by the proposed method on Sequences 1-6 are 1.24%, 1.16%, 0.63%, 1.9%, 0.82%, 2.06% higher than those achieved by the second top performing methods, respectively.}
}
@article{CHEN2023109168,
title = {Rethinking Local and Global Feature Representation for Dense Prediction},
journal = {Pattern Recognition},
volume = {135},
pages = {109168},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109168},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006471},
author = {Mohan Chen and Li Zhang and Rui Feng and Xiangyang Xue and Jianfeng Feng},
keywords = {Dense prediction, Vision transformer, Semantic segmentation, Depth estimation, Object detection},
abstract = {Although fully convolution networks (FCNs) have dominated dense prediction tasks (e.g., semantic segmentation, depth estimation and object detection) for decades, they are inherently limited in capturing long-range structured relationship with the layers of local kernels. While recent Transformer-based models have proven extremely successful in computer vision tasks by capturing global representation, they would deteriorate dense prediction results by over-smoothing the regions containing fine details (e.g., boundaries and small objects). To this end, we aim to provide an alternative perspective by rethinking local and global feature representation for the dense prediction task. Specifically, we deploy a Dual-Stream Convolution-Transformer architecture, called DSCT, by taking advantage of both the convolution and Transformer to learn a rich feature representation, combining with a task decoder to provide a powerful dense prediction model. DSCT extracts high resolution local feature representation from convolution layers and global feature representation from Transformer layers. With the local and global context modeled explicitly in every layer, the two streams can be combined with a decoder to perform task of semantic segmentation, monocular depth estimation or object detection. Extensive experiments show that DSCT can achieve superior performance on the three tasks above. For semantic segmentation, DSCT builds a new state of the art on Cityscapes validation set (83.31% mIoU) with only 80,000 training iterations and appealing performance (49.27% mIoU) on ADE20K validation set, outperforming most of the alternatives. For monocular depth estimation, our model achieves 2.423 RMSE on KITTI Eigen split, superior to most of the convolution or Transformer counterparts. For object detection, without using FPN, we can achieve 44.5% APb on COCO dataset when using Faster R-CNN, which is higher than Conformer.}
}
@article{WANG2023109237,
title = {Learning a bi-directional discriminative representation for deep clustering},
journal = {Pattern Recognition},
volume = {137},
pages = {109237},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109237},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007166},
author = {Yiming Wang and Dongxia Chang and Zhiqiang Fu and Yao Zhao},
keywords = {Deep clustering, Representation learning, Manifold learning, Mutual information},
abstract = {Nowadays, deep clustering achieves superior performance by jointly performing representation learning and cluster assignment. Although numerous deep clustering algorithms have emerged, most of them have difficulty learning representations that fit the clustering distribution. To address this issue, we propose a bi-directional discriminative representation learning clustering (BDRC) framework in this paper. In our framework, a dual autoencoder network, a bi-directional mutual information maximization module and a self-supervised cluster prediction module are combined into a joint optimization framework. To learn more cluster-friendly representations, the bi-directional mutual information maximization module is executed on both samples and their nearest neighbors to explore the cluster relationships between samples. In order to improve the stability of the model, a self-supervised cluster prediction module is devised to predict clustering assignments to supervise the autoencoder using the KL-divergence. Moreover, the UMAP is used to find the manifold of the latent representations which can better preserve the global structure. Experiments on some benchmark datasets demonstrate the superiority of the proposed BDRC algorithm.}
}
@article{ZHAI2023109167,
title = {Joint optimization of scoring and thresholding models for online multi-label classification},
journal = {Pattern Recognition},
volume = {136},
pages = {109167},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109167},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200646X},
author = {Tingting Zhai and Hao Wang and Hongcheng Tang},
keywords = {online multi-label classification, online thresholding, adaptive thresholding, online learning},
abstract = {Existing online multi-label classification works cannot well handle the online label thresholding problem and lack regret analysis for their online algorithms. This paper proposes a novel framework of joint optimization of scoring and thresholding models for online multi-label classification, with the aim to overcome the above drawbacks. The key feature of our framework is that both scoring and thresholding models are included as important components of the online multi-label classifier and are incorporated into one online optimization problem. Based on this framework, we present two adaptive label thresholding algorithms and two fixed thresholding algorithms. For each type of algorithms, a first-order method and a second-order one are provided for updating the online multi-label classifier. Both methods enjoy a closed-form update. Our proposed algorithms are proved to achieve a sub-linear regret. Using Mercer kernels, two first-order algorithms can be extended to handle nonlinear multi-label prediction tasks. Experiments show the advantage of the adaptive and the fixed thresholding algorithms, in terms of various multi-label performance metrics.}
}
@article{VALVERDE2023109208,
title = {Region-wise loss for biomedical image segmentation},
journal = {Pattern Recognition},
volume = {136},
pages = {109208},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109208},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006872},
author = {Juan Miguel Valverde and Jussi Tohka},
keywords = {Deep learning, Segmentation, Medical imaging, Loss function},
abstract = {We propose Region-wise (RW) loss for biomedical image segmentation. Region-wise loss is versatile, can simultaneously account for class imbalance and pixel importance, and it can be easily implemented as the pixel-wise multiplication between the softmax output and a RW map. We show that, under the proposed RW loss framework, certain loss functions, such as Active Contour and Boundary loss, can be reformulated similarly with appropriate RW maps, thus revealing their underlying similarities and a new perspective to understand these loss functions. We investigate the observed optimization instability caused by certain RW maps, such as Boundary loss distance maps, and we introduce a mathematically-grounded principle to avoid such instability. This principle provides excellent adaptability to any dataset and practically ensures convergence without extra regularization terms or optimization tricks. Following this principle, we propose a simple version of boundary distance maps called rectified Region-wise (RRW) maps that, as we demonstrate in our experiments, achieve state-of-the-art performance with similar or better Dice coefficients and Hausdorff distances than Dice, Focal, weighted Cross entropy, and Boundary losses in three distinct segmentation tasks. We quantify the optimization instability provided by Boundary loss distance maps, and we empirically show that our RRW maps are stable to optimize. The code to run all our experiments is publicly available at: https://github.com/jmlipman/RegionWiseLoss.}
}
@article{QIAN2023109156,
title = {Weight matrix sharing for multi-label learning},
journal = {Pattern Recognition},
volume = {136},
pages = {109156},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109156},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006355},
author = {Kun Qian and Xue-Yang Min and Yusheng Cheng and Fan Min},
keywords = {Low-rank, Missing labels, Multi-label learning, Shared weight, Sparse},
abstract = {Multi-label learning on real-world data is a challenging task due to sparse labels, missing labels, and sparse structures. Some existing approaches are effective in addressing the former two issues. In this paper, we propose a shared weight matrix with low-rank and sparse regularization for multi-label learning (2SML) algorithm to address the issues simultaneously. First, two explicit correlation matrices are constructed from the feature matrix and label matrix. Second, we select informative labels by instance representativeness to learn implicit correlations. Third, a feature manifold and label manifold are employed to guide the shared weight learning process. Extensive experiments are undertaken on multiple benchmark datasets with and without missing labels. The results show that the proposed method outperforms the state-of-the-art methods.}
}
@article{SHI2023109180,
title = {JRA-Net: Joint representation attention network for correspondence learning},
journal = {Pattern Recognition},
volume = {135},
pages = {109180},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109180},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006598},
author = {Ziwei Shi and Guobao Xiao and Linxin Zheng and Jiayi Ma and Riqing Chen},
keywords = {Correspondences, Joint representation, Attention mechanism, Outlier rejection, Pose estimation},
abstract = {In this paper, we propose a Joint Representation Attention Network (JRA-Net), an end-to-end network, to establish reliable correspondences for image pairs. The initial correspondences generated by the local feature descriptor usually suffer from heavy outliers, which makes the network unable to learn a powerful enough representation for distinguishing inliers and outliers. To this end, we design a novel attention mechanism. The proposed attention mechanism not only takes into account the correlations between global context and geometric information, but also introduces the joint representation of different scales to suppress trivial correspondences and highlight crucial correspondences. In addition, to improve the generalization ability of attention mechanism, we present an innovative weight function, to effectively adjust the importance of the attention mechanism in a learning manner. Finally, by combining the above components, the proposed JRA-Net is able to effectively infer the probabilities of correspondences being inliers. Empirical experiments on challenging datasets demonstrate the effectiveness and generalization of JRA-Net. We achieve remarkable improvements compared with the current state-of-the-art approaches on outlier rejection and relative pose estimation.}
}
@article{DU2023109241,
title = {A new image decomposition approach using pixel-wise analysis sparsity model},
journal = {Pattern Recognition},
volume = {136},
pages = {109241},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109241},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007208},
author = {Shuangli Du and Yiguang Liu and Minghua Zhao and Zhenyu Xu and Jie Li and Zhenzhen You},
keywords = {Image decomposition, Rain streaks removal, Retinex theory, Pixel-wise analysis sparsity model, Synthesis sparsity model},
abstract = {Decomposing an image into two ‘simpler’ layers has been widely used in low-level vision tasks, such as image recovery and enhancement. It is an ill-posed problem since the number of unknowns are larger than the input. In this paper, a two-step strategy is introduced, including task-aware priors estimate and a decomposition model. A pixel-wise analysis sparsity model is proposed to regularize the separation layers, which supposes the transformed image generated with analysis operator is sparse. Unlike regularizing all pixels with one penalty weight, we try to estimate each pixel’s sparsity level with task-aware priors and to achieve pixel-wise sparse penalty. Additionally, one separation layer is regularized with both synthesis sparsity model and pixel-wise analysis sparsity model to exploit their complementary mechanisms. Unlike the analysis one utilizing image local features, the synthesis one exploits an over-complete dictionary and non-local similarity cues to provide flexible prior for regularizing the decomposition results. The proposed model is solved by an alternating optimization algorithm. We evaluate it with two applications, Retinex model and rain streaks removal. Extensive experiments on multiple enhancement datasets, many synthetic and real rainy images demonstrate that our method can remove imaging noise during Retinex decomposition, and can produce high fidelity deraining results. It achieves competing performance in terms of quantitative metrics and visual quality compared with the state-of-the-art methods.}
}
@article{GAUTAM2023109172,
title = {This looks More Like that: Enhancing Self-Explaining Models by Prototypical Relevance Propagation},
journal = {Pattern Recognition},
volume = {136},
pages = {109172},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109172},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006513},
author = {Srishti Gautam and Marina M.-C. Höhne and Stine Hansen and Robert Jenssen and Michael Kampffmeyer},
keywords = {Self-explaining models, Explainable AI, Deep learning, Spurious Correlation Detection},
abstract = {Current machine learning models have shown high efficiency in solving a wide variety of real-world problems. However, their black box character poses a major challenge for the comprehensibility and traceability of the underlying decision-making strategies. As a remedy, numerous post-hoc and self-explanation methods have been developed to interpret the models’ behavior. Those methods, in addition, enable the identification of artifacts that, inherent in the training data, can be erroneously learned by the model as class-relevant features. In this work, we provide a detailed case study of a representative for the state-of-the-art self-explaining network, ProtoPNet, in the presence of a spectrum of artifacts. Accordingly, we identify the main drawbacks of ProtoPNet, especially its coarse and spatially imprecise explanations. We address these limitations by introducing Prototypical Relevance Propagation (PRP), a novel method for generating more precise model-aware explanations. Furthermore, in order to obtain a clean, artifact-free dataset, we propose to use multi-view clustering strategies for segregating the artifact images using the PRP explanations, thereby suppressing the potential artifact learning in the models.}
}
@article{LI2023109196,
title = {Linear discriminant analysis with generalized kernel constraint for robust image classification},
journal = {Pattern Recognition},
volume = {136},
pages = {109196},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109196},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006756},
author = {Shuyi Li and Hengmin Zhang and Ruijun Ma and Jianhang Zhou and Jie Wen and Bob Zhang},
keywords = {Linear discriminant analysis, Kernel constraint, Intra-class and inter-class distance, Separability, Image classification},
abstract = {Linear discriminant analysis (LDA) as a classical supervised dimensionality reduction method has shown powerful capability in various image classification tasks. The purpose of LDA seeks an optimal linear transformation that maps the original data to a low-dimensional space. Inspired by the fact that the kernel trick can capture the nonlinear similarity of features, we propose a novel generalized distance constraint dubbed intra-class and inter-class kernel constraint (IIKC). The proposed IIKC explicitly models the category kernel distance and focuses on helping the original LDA capture more discriminant features in order to further improve the separability and magnitude difference between nearby data points. Our proposed method with IIKC aims to achieve maximum category separability by minimizing the intra-class kernel distances as well as maximizing the inter-class kernel distance, simultaneously. Extensive experimental results on six publicly available benchmark databases illustrate that the LDA-based methods embedded with the proposed IIKC significantly improve the discrimination ability and achieve a better classification performance than the original and state-of-the-art LDA algorithms.}
}
@article{BAGIROV2023109144,
title = {Finding compact and well-separated clusters: Clustering using silhouette coefficients},
journal = {Pattern Recognition},
volume = {135},
pages = {109144},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109144},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006239},
author = {Adil M. Bagirov and Ramiz M. Aliguliyev and Nargiz Sultanova},
keywords = {Cluster analysis, Cluster validity index, Silhouette coefficients, Nonsmooth optimization, Incremental algorithm},
abstract = {Finding compact and well-separated clusters in data sets is a challenging task. Most clustering algorithms try to minimize certain clustering objective functions. These functions usually reflect the intra-cluster similarity and inter-cluster dissimilarity. However, the use of such functions alone may not lead to the finding of well-separated and, in some cases, compact clusters. Therefore additional measures, called cluster validity indices, are used to estimate the true number of well-separated and compact clusters. Some of these indices are well-suited to be included into the optimization model of the clustering problem. Silhouette coefficients are among such indices. In this paper, a new optimization model of the clustering problem is developed where the clustering function is used as an objective and silhouette coefficients are used to formulate constraints. Then an algorithm, called CLUSCO (CLustering Using Silhouette COefficients), is designed to construct clusters incrementally. Three schemes are discussed to reduce the computational complexity of the algorithm. Its performance is evaluated using fourteen real-world data sets and compared with that of three state-of-the-art clustering algorithms. Results show that the CLUSCO is able to compute compact clusters which are significantly better separable in comparison with those obtained by other algorithms.}
}
@article{ARUMUGAM2023109212,
title = {Interpreting denoising autoencoders with complex perturbation approach},
journal = {Pattern Recognition},
volume = {136},
pages = {109212},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109212},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006914},
author = {Dharanidharan Arumugam and Ravi Kiran},
keywords = {Complex step derivative approximation, Saliency maps, Trustworthiness, Pixel attributions, Sanity checks and deep neural networks (DNNs)},
abstract = {The goal of this study is to interpret denoising autoencoders by quantifying the importance of input pixel features for image reconstruction. The importance of pixel features is evaluated using the attributions of the pixel features to the latent variables of a denoising autoencoder used for image reconstruction. Pixel attributions are computed using a highly accurate and automatable perturbation approach and are plotted as saliency maps. Saliency maps highlight the contribution of the pixels for image reconstruction. The proposed approach produces more meaningful and understandable explanations than guided backpropagation and layer wise propagation methods. Three sanity checks are introduced to verify the fidelity of the generated saliency maps and also to elucidate the influence of inputs on the latent variables. The classification accuracy of images is significantly lowered when the most important pixel regions highlighted by the saliency maps are corrupted validating the proposed approach.}
}
@article{FANG2023109248,
title = {A novel DAGAN for synthesizing garment images based on design attribute disentangled representation},
journal = {Pattern Recognition},
volume = {136},
pages = {109248},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109248},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007270},
author = {Naiyu Fang and Lemiao Qiu and Shuyou Zhang and Zili Wang and Kerui Hu and Kang Wang},
keywords = {DAGAN, Garment design attributes, Disentangled representation, Garment image synthesis, Online costume design},
abstract = {In online costume design, it is vital to preview the design effect rapidly by entangling design attributes from reference images. This paper proposes a novel method, named design attributes generative adversarial network (DAGAN) for synthesizing garment images based on design attribute disentangled representation. The garment style is disentangled into the shape, texture, shadow, and decoration design attributes. The shape mask, repeating texture region, Laplace image gradient, and local logo are leveraged as visual representations for clothing design attributes from reference images. Following the design sequence from global to local, GDA-Net and LDA-Net in DAGAN entangle global and local design attributes, respectively, in the latent space. Then, the desired garment image is synthesized to represent design intentions explicitly. The DA-dataset for clothing design attributes is released. Extensive experiments demonstrate that the DAGAN is robust to various instances of design attributes on Design Attributes dataset (DA-dataset), and that is superior to the cross-domain transfer models in entangling design attributes from reference images.}
}
@article{2024110346,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {149},
pages = {110346},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(24)00097-9},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000979}
}
@article{WAN2023109150,
title = {G2DA: Geometry-guided dual-alignment learning for RGB-infrared person re-identification},
journal = {Pattern Recognition},
volume = {135},
pages = {109150},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109150},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200629X},
author = {Lin Wan and Zongyuan Sun and Qianyan Jing and Yehansen Chen and Lijing Lu and Zhihang Li},
keywords = {Person re-identification, Cross-modality matching, optimal transport, Feature alignment, Channel exchange},
abstract = {RGB-Infrared (IR) person re-identification aims to retrieve person-of-interest from heterogeneous cameras, easily suffering from large image modality discrepancy caused by different sensing wavelength ranges. Existing works usually minimize such discrepancy by aligning modality distribution of global features, while neglecting deep semantics and high-order structural relations within each class. This might render the misalignment between heterogeneous samples. In this paper, we propose Geometry-Guided Dual-Alignment (G2DA) learning, which yields better sample-level modality alignment for RGB-IR ReID by solving a graph-enabled distribution matching task that maximizes agreement between multi-modality node representations considering edge topology. Specifically, we covert RGB/IR images into semantic-aligned graphs, in which whole-part features and their similarities are represented by nodes and associated edges, respectively. To simultaneously implement node- and edge-wise alignment (Dual Alignment), we introduce Optimal Transport (OT) as a metric to calculate cross-modality human body matching scores. By minimizing the displacement cost across RGB-IR graphs, G2DA could learn not just modality-invariant but structurally consistent cross-modality representations. Furthermore, we advance a Message Fusion Attention (MFA) mechanism to adaptively smooth the node representations within each RGB/IR graph, effectively alleviating occlusions caused by other individuals and/or objects. Extensive experiments on two standard benchmark datasets validate the superiority of G2DA, yielding competitive performance against previous state-of-the-arts.}
}
@article{ONVUNGC2023109207,
title = {The Dahu graph-cut for interactive segmentation on 2D/3D images},
journal = {Pattern Recognition},
volume = {136},
pages = {109207},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109207},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006860},
author = {Minh {Ôn Vũ Ngọc} and Edwin Carlinet and Jonathan Fabrizio and Thierry Géraud},
keywords = {Vectorial Dahu pseudo-distance, Minimum barrier distance, Visual saliency, Object segmentation, Mathematical morphology},
abstract = {Interactive image segmentation is an important application in computer vision for selecting objects of interest in images. Several interactive segmentation methods are based on distance transform algorithms. However, the most known distance transform, geodesic distance, is sensitive to noise in the image and to seed placement. Recently, the Dahu pseudo-distance, a continuous version of the minimum barrier distance (MBD), is proved to be more powerful than the geodesic distance in noisy and blurred images. This paper presents a method for combining the Dahu pseudo-distance with edge information in a graph-cut optimization framework and leveraging each’s complementary strengths. Our method works efficiently on both 2D/3D images and videos. Results show that our method achieves better performance than other distance-based and graph-cut methods, thereby reducing the user’s efforts.}
}
@article{NGUYEN2023109155,
title = {A novel multi-branch wavelet neural network for sparse representation based object classification},
journal = {Pattern Recognition},
volume = {135},
pages = {109155},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109155},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006343},
author = {Tan-Sy Nguyen and Marie Luong and Mounir Kaaniche and Long H. Ngo and Azeddine Beghdadi},
keywords = {Object classification, Sparse coding, Wavelet transform, Neural networks, Multi-branch architecture},
abstract = {Recent advances in acquisition and display technologies have led to an enormous amount of visual data, which requires appropriate storage and management tools. One of the fundamental needs is the design of efficient image classification and recognition solutions. In this paper, we propose a wavelet neural network approach for sparse representation-based object classification. The proposed approach aims to exploit the advantages of sparse coding, multi-scale wavelet representation as well as neural networks. More precisely, a wavelet transform is firstly applied to the image datasets. The generated approximation and detail wavelet subbands are then fed into a multi-branch neural network architecture. This architecture produces multiple sparse codes that are efficiently combined during the classification stage. Extensive experiments, carried out on various types of standard object datasets, have shown the efficiency of the proposed method compared to the existing sparse coding and deep learning-based methods.}
}
@article{WANG2023109260,
title = {Joint depth map super-resolution method via deep hybrid-cross guidance filter},
journal = {Pattern Recognition},
volume = {136},
pages = {109260},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109260},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007397},
author = {Ke Wang and Lijun Zhao and Jinjing Zhang and Jialong Zhang and Anhong Wang and Huihui Bai},
keywords = {Joint image filter, Depth image, Super-resolution, Hybrid-cross guidance, Space-aware group-compensation},
abstract = {Nowadays color-guided Depth map Super-Resolution (DSR) methods mainly have three thorny problems: (1) joint DSR methods have serious detail and structure loss at very high sampling rate; (2) existing DSR networks have high computational complexity; (3) color-depth inconsistency makes it hard to fuse dual-modality features. To resolve these problems, we propose a joint hybrid-cross guidance filter method to progressively recover the quality of degraded Low-Resolution (LR) depth maps by exploiting color-depth consistency from multiple perspectives. Specifically, the proposed method leverages pyramid structure to extract multi-scale features from High-Resolution (HR) color image. At each scale, hybrid side window filter block is proposed to achieve high-efficiency color feature extraction after each down-sampling for HR color image. This block is also used to extract depth features from the LR depth map. Meanwhile, we propose a multi-perspective cross-guided fusion filter block to progressively fuse high-quality multi-scale structure information of color image with corresponding enhanced depth features. In this filter block, two kinds of space-aware group-compensation modules are introduced to capture various spatial features from different perspectives. Meanwhile, color-depth cross-attention module is proposed to extract color-depth consistency features for impactful boundary preservation. Comprehensively qualitative and quantitative experimental results have demonstrated that our method can achieve superior performances against a lot of state-of-the-art depth SR approaches in terms of mean absolute deviation and root mean square error on Middlebury, NYU-v2 and RGB-D-D datasets.}
}
@article{SHEN2023109236,
title = {Compact network embedding for fast node classification},
journal = {Pattern Recognition},
volume = {136},
pages = {109236},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109236},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007154},
author = {Xiaobo Shen and Yew-Soon Ong and Zheng Mao and Shirui Pan and Weiwei Liu and Yuhui Zheng},
keywords = {Network embedding, Hashing, Compact representation, Graph},
abstract = {Network embedding has shown promising performance in real-world applications. The network embedding typically lies in a continuous vector space, where storage and computation costs are high, especially in large-scale applications. This paper proposes more compact representation to fulfill the gap. The proposed discrete network embedding (DNE) leverages hash code to represent node in Hamming space. The Hamming similarity between hash codes approximates the ground-truth similarity. The embedding and classifier are jointly learned to improve compactness and discrimination. The proposed multi-class classifier is further constrained to be discrete to expedite classification. In addition, this paper further extends DNE and proposes deep discrete attributed network embedding (DDANE) to learn compact deep embedding from more informative attributed network. From the perspective of generalized signal smoothing, the proposed DDANE trains an improved graph convolutional network autoencoder to effectively leverage node attribute and network structure. Extensive experiments on node classification demonstrate the proposed methods exhibit lower storage and computational complexity than state-of-the-art network embedding methods, and achieve satisfactory accuracy.}
}
@article{ZHANG2023109171,
title = {Lower bound estimation of recommendation error through user uncertainty modeling},
journal = {Pattern Recognition},
volume = {136},
pages = {109171},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109171},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006501},
author = {Heng-Ru Zhang and Ying Qiu and Ke-Lin Zhu and Fan Min},
keywords = {Magic barrier estimation, Mixture of exponential power, Recommender system, Uncertainty modeling},
abstract = {In machine learning, the Bayesian error is the lower bound of the prediction error induced by data distribution. In recommender systems, this is also known as the magic barrier (MGBR). MGBR estimation is an important issue because the recommended data frequently contain considerable uncertainties that are difficult to quantify. It is possible to determine the extent to which the recommendation algorithm can be optimized by obtaining the MGBR for a given dataset. MGBR estimation generally requires real user ratings that are not affected by external factors such as human emotions and living environment, which can be extremely difficult or even impossible to gather. Existing theoretical approaches based on simple models, such as Gaussian distributions, have limited estimation capabilities. In this paper, we propose a more sophisticated mixture of exponential power (MoEP) model, which enables adaptive parameter selection for intricate uncertainty. To fit the distribution of the real data, we constructed a flexible learning model that automatically adjusts super- or sub-Gaussian uncertainties using the MoEP components. To select parameters adaptively, we employed an expectation-maximization algorithm to infer the parameters of the components. To estimate the MGBR, we explored an approach for calculating the lower bound of the prediction error under the guidance of a probability model. Experiments on the four datasets validated the rationality of the proposed method. The results show that the MGBR estimated using the new model is marginally lower than the prediction error of state-of-the-art algorithms.}
}
@article{CHEN2023109179,
title = {Watching the BiG artifacts: Exposing DeepFake videos via Bi-granularity artifacts},
journal = {Pattern Recognition},
volume = {135},
pages = {109179},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109179},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006586},
author = {Han Chen and Yuezun Li and Dongdong Lin and Bin Li and Junqiang Wu},
keywords = {Multimedia forensics, Deepfake detection, Granularity artifacts, Multi-task learning},
abstract = {Recent years have witnessed significant advances in AI-based face manipulation techniques, known as DeepFakes, which has brought severe threats to society. Hence, an emerging and increasingly important research topic is how to detect DeepFake videos. In this paper, we propose a new DeepFake detection method based on Bi-granularity artifacts (BiG-Arts). We observe that the most of DeepFake video generation can commonly introduce bi-granularity artifacts: the intrinsic-granularity artifacts and extrinsic-granularity artifacts. Specifically, the intrinsic-granularity artifacts are caused by a common series of operations in model generation such as up-convolution or up-sampling, while the extrinsic-granularity artifacts are introduced by a common step in post-processing that blends the synthesized face to original video. To this end, we formulate DeepFake detection as multi-task learning problem, to simultaneously predict the intrinsic and extrinsic artifacts. Benefiting from the guidance of detecting Bi-granularity artifacts, our method is notably boosted in both within-datasets and cross-datasets scenarios. Extensive experiments are conducted on several DeepFake datasets, which corroborates the superiority of our method. Our method has been contributed as a part of the solution to achieve the Top-1 rank in DFGC competition (https://competitions.codalab.org/competitions/29583).}
}
@article{RUIZSANTAQUITERIA2023109252,
title = {Improving handgun detection through a combination of visual features and body pose-based data},
journal = {Pattern Recognition},
volume = {136},
pages = {109252},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109252},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007312},
author = {Jesus Ruiz-Santaquiteria and Alberto Velasco-Mata and Noelia Vallez and Oscar Deniz and Gloria Bueno},
keywords = {Handgun detection, Human pose estimation, CCTV Surveillance, Transformers, False positive filtering},
abstract = {Early detection of the presence of dangerous objects such as handguns in Closed-Circuit Television (CCTV) images is vital to reduce the potential damage. In this work, a novel method for automatic detection of handguns in CCTV-like images based on a combination architecture which leverages body pose estimation is proposed. Weapon appearance features along with body pose features are combined to perform robust detection in typical surveillance environments where appearance features alone are not sufficient (e.g., because the handgun may appear too small or dark). Both CNN and recent transformer-based architectures are applied for visual feature extraction. Experiments on multiple datasets show that this approach improves state-of-the-art pose-based handgun detectors. An ablation study is also performed to verify the contribution of the pose processing branch and the false positive filter.}
}
@article{MEI2023109264,
title = {Multi-order similarity learning for multi-view spectral clustering},
journal = {Pattern Recognition},
volume = {137},
pages = {109264},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109264},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007439},
author = {Yanying Mei and Zhenwen Ren and Bin Wu and Tao Yang and Yanhua Shao},
keywords = {Spectral clustering, Multi-view clustering, Multi-order similarity, Graph learning, Tensor},
abstract = {This paper explores the problem of multi-view spectral clustering (MVSC) based on multi-order similarity learning. Unlike the existing methods that focus on direct similarity of pairwise data points without considering the hidden multi-order similarity among different data points, a novel multi-order similarity learning model for MVSC (MOSL) is proposed. Specifically, the first-order similarity (FOS) and second-order similarity (SOS) are learned to excavate the local structure relation and adjacent structure relation of pairwise data points. Afterwards, the third-order similarity (TOS) based on low-rank tensor is learned to excavate the view-specific information and consensus information from multiple views. Moreover, a trace constraint on each affinity graph from multiple views is learned to ensure the strict block diagonal structure of each affinity graph. Extensive experiments on six commonly benchmark datasets show that the proposed method outperforms state-of-the-art methods in most scenarios and is capable of revealing a reliable affinity graph structure concealed in different data points.}
}
@article{CAI2023109195,
title = {Brain-like retinex: A biologically plausible retinex algorithm for low light image enhancement},
journal = {Pattern Recognition},
volume = {136},
pages = {109195},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109195},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006744},
author = {Rongtai Cai and Zekun Chen},
keywords = {Retinex, Low light image enhancement, Contour detection, Edge detection, Brain-inspired computation, Color constancy, Visual cortex, Retinal circuit},
abstract = {Retinex theory was first proposed by Land and McCann [1], where retinex is a portmanteau derived from the words of retina and cortex, implying that both the retina and cerebral cortex may participate in the perception of lightness and color. However, there are no recent reports on how the retina and visual cortex perform retinex decomposition. In this paper, we propose a biologically plausible solution to retinex decomposition. We develop an algorithm motivated by the primate’s retinal circuit to detect textural gradients, design an algorithm originating from the visual cortex to extract image contours, and thus split image edges into image contours and textural gradients. Then, we establish a variational model for retinex decomposition by using image contours and textural gradients to encode discontinuities in illumination and variations in reflectance, respectively. We also apply the proposed retinex model to low light image enhancement, high dynamic resolution image toning, and color constancy. Experiments show consistent superiority of the proposed algorithm. The code is available at Github.}
}
@article{HUANG2023109142,
title = {PLFace: Progressive Learning for Face Recognition with Mask Bias},
journal = {Pattern Recognition},
volume = {135},
pages = {109142},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109142},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006227},
author = {Baojin Huang and Zhongyuan Wang and Guangcheng Wang and Kui Jiang and Zhen Han and Tao Lu and Chao Liang},
keywords = {Face recognition, Progressive learning, Mask bias},
abstract = {The outbreak of the COVID-19 coronavirus epidemic has promoted the development of masked face recognition (MFR). Nevertheless, the performance of regular face recognition is severely compromised when the MFR accuracy is blindly pursued. More facts indicate that MFR should be regarded as a mask bias of face recognition rather than an independent task. To mitigate mask bias, we propose a novel Progressive Learning Loss (PLFace) that achieves a progressive training strategy for deep face recognition to learn balanced performance for masked/mask-free faces recognition based on margin losses. Particularly, our PLFace adaptively adjusts the relative importance of masked and mask-free samples during different training stages. In the early stage of training, PLFace mainly learns the feature representations of mask-free samples. At this time, the regular sample embeddings shrink to the corresponding prototype, which represents the center of each class while being stored in the last linear layer. In the later stage of training, PLFace converges on mask-free samples and further focuses on masked samples until the masked sample embeddings are also gathered in the center of the class. The entire training process emphasizes the paradigm that normal samples shrink first and masked samples gather afterward. Extensive experimental results on popular regular and masked face benchmarks demonstrate that our proposed PLFace can effectively eliminate mask bias in face recognition. Compared to state-of-the-art competitors, PLFace significantly improves the accuracy of MFR while maintaining the performance of normal face recognition.}
}
@article{WU2023109211,
title = {Semi-supervised cross-modal hashing via modality-specific and cross-modal graph convolutional networks},
journal = {Pattern Recognition},
volume = {136},
pages = {109211},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109211},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006902},
author = {Fei Wu and Shuaishuai Li and Guangwei Gao and Yimu Ji and Xiao-Yuan Jing and Zhiguo Wan},
keywords = {Cross-modal hashing, semi-supervised learning, graph convolutional networks, modality-specific features, modality-shared features},
abstract = {Cross-modal hashing maps heterogeneous multimedia data into Hamming space for retrieving relevant samples across modalities, which has received great research interests due to its rapid retrieval and low storage cost. In real-world applications, due to high manual annotation cost of multi-media data, we can only make use of limited number of labeled data with rich unlabeled data. In recent years, several semi-supervised cross-modal hashing (SCH) methods have been presented. However, how to fully explore and jointly utilize the modality-specific (complementarity) and modality-shared (correlation) information for retrieval has not been well studied for existing SCH works. In this paper, we propose a novel SCH approach named Modality-specific and Cross-modal Graph Convolutional Networks (MCGCN). The network architecture contains two modality-specific channels and a cross-modal channel to learn modality-specific and shared representations for each modality, respectively. Graph convolutional network (GCN) is leveraged in these three channels to explore intra-modal and inter-modal similarity, and perform semantic information propagation from labeled data to unlabeled data. Modality-specific and shared representations for each modality are fused with attention scheme. To further reduce the modality gap, a discriminative model is designed, learning to classify the modality of representations, and network training is guided by adversarial scheme. Experiments on two widely used multi-modal datasets demonstrate MCGCN outperforms state-of-the-art semi-supervised/supervised cross-modal hashing methods.}
}
@article{AZIZ2023109183,
title = {Fast geometrical extraction of nearest neighbors from multi-dimensional data},
journal = {Pattern Recognition},
volume = {136},
pages = {109183},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109183},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006628},
author = {Yasir Aziz and Kashif Hussain Memon},
keywords = {Nearest neighbors, Classification, Hashing, Windowing operation},
abstract = {K-Nearest Neighbor (KNN) algorithm plays a significant role in various fields of data science and machine learning. Most variants of the KNN algorithm involve distance computations and a parameter (K) that represents the required number of neighbors. The recent research regarding distance computations and finding the optimal value of K have made neighborhood extraction a slow process. This research presents a fast geometrical approach for neighborhood extraction from multi-dimensional data. Instead of distance computations, the proposed algorithm creates a geometrical shape based on the number of features of data. This geometrical shape encompasses the reference data point and the neighboring points. The proposed algorithm's efficiency of time, classification, and hashing are evaluated and compared with existing state-of-the-art algorithms.}
}
@article{ZHANG2023109235,
title = {Meta-hallucinating prototype for few-shot learning promotion},
journal = {Pattern Recognition},
volume = {136},
pages = {109235},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109235},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007142},
author = {Lei Zhang and Fei Zhou and Wei Wei and Yanning Zhang},
keywords = {Few-shot learning, Prototype hallucination, Meta-learning},
abstract = {An effective way for few-shot learning (FSL) is to establish a metric space where the distance between a query and the prototype of each class is computed for classification, and the key lies on hallucinating the appropriate prototypes for each class of the given FSL task. Most existing prototypical approaches hallucinate the class-wise prototype based on the given support samples with an equal contribution assumption, i.e., each support sample contributes equally to the corresponding prototype. However, due to the limited-data regime as well as the strict assumption, the hallucinated prototypes often deviate from the ideal ones that are determined by the sample distribution of each unseen class, and thus causing poor generalization performance. To mitigate this problem, we present a prototype meta-hallucination approach which shows two aspects of advantages. On one hand, instead of directly inferring the complicated sample distribution, it meta-learns to establish a difference distribution based generative model that infers the distribution of inter-sample difference and synthesizes new labeled samples through fusing the sampled inter-sample difference and each given support sample. This empowers us to augment the support set with more content-diverse samples and is beneficial to reduce the bias in prototype hallucination. On the other hand, we argue that each support sample may contribute no-equally to the ideal prototype that it belongs to and their relations vary with class characteristics. Following this, our approach meta-learns to dynamically re-weight all support samples in prototype hallucination, which makes it flexible to locate the ideal prototype for each unseen class based on its characteristics. Experiments on four FSL benchmark datasets show that our approach can effectively improve the performance of the prototypical baseline and outperform several state-of-the-art competitors with a clear margin.}
}
@article{VALEROMAS2023109190,
title = {Multilabel Prototype Generation for data reduction in K-Nearest Neighbour classification},
journal = {Pattern Recognition},
volume = {135},
pages = {109190},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109190},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006690},
author = {Jose J. Valero-Mas and Antonio Javier Gallego and Pablo Alonso-Jiménez and Xavier Serra},
keywords = {Multilabel classification, Prototype generation, Efficient NN},
abstract = {Prototype Generation (PG) methods are typically considered for improving the efficiency of the k-Nearest Neighbour (kNN) classifier when tackling high-size corpora. Such approaches aim at generating a reduced version of the corpus without decreasing the classification performance when compared to the initial set. Despite their large application in multiclass scenarios, very few works have addressed the proposal of PG methods for the multilabel space. In this regard, this work presents the novel adaptation of four multiclass PG strategies to the multilabel case. These proposals are evaluated with three multilabel kNN-based classifiers, 12 corpora comprising a varied range of domains and corpus sizes, and different noise scenarios artificially induced in the data. The results obtained show that the proposed adaptations are capable of significantly improving—both in terms of efficiency and classification performance—the only reference multilabel PG work in the literature as well as the case in which no PG method is applied, also presenting statistically superior robustness in noisy scenarios. Moreover, these novel PG strategies allow prioritising either the efficiency or efficacy criteria through its configuration depending on the target scenario, hence covering a wide area in the solution space not previously filled by other works.}
}
@article{XU2023109137,
title = {Classification of single-view object point clouds},
journal = {Pattern Recognition},
volume = {135},
pages = {109137},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109137},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006173},
author = {Zelin Xu and Kangjun Liu and Ke Chen and Changxing Ding and Yaowei Wang and Kui Jia},
keywords = {Point cloud classification, Rotation equivariance, Pose estimation},
abstract = {Object point cloud classification has drawn great research attention since the release of benchmarking datasets, such as the ModelNet and the ShapeNet. These benchmarks assume point clouds covering complete surfaces of object instances, for which plenty of high-performing methods have been developed. However, their settings deviate from those often met in practice, where, due to (self-)occlusion, a point cloud covering partial surface of an object is captured from an arbitrary view. We show in this paper that performance of existing point cloud classifiers drops drastically under the considered single-view, partial setting; the phenomenon is consistent with the observation that semantic category of a partial object surface is less ambiguous only when its distribution on the whole surface is clearly specified. To this end, we argue for a single-view, partial setting where supervised learning of object pose estimation should be accompanied with classification. Technically, we propose a baseline method of Pose-Accompanied Point cloud classification Network (PAPNet); built upon SE(3)-equivariant convolutions, the PAPNet learns intermediate pose transformations for equivariant features defined on vector fields, which makes the subsequent classification easier (ideally) in the category-level, canonical pose. By adapting existing ModelNet40 and ScanNet datasets to the single-view, partial setting, experiment results can verify the necessity of object pose estimation and superiority of our PAPNet to existing classifiers.}
}
@article{DENG2023109251,
title = {RGB-D salient object ranking based on depth stack and truth stack for complex indoor scenes},
journal = {Pattern Recognition},
volume = {137},
pages = {109251},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109251},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007300},
author = {Jingzheng Deng and Jinxia Zhang and Zewen Hu and Liantao Wang and Jiacheng Jiang and Xinchao Zhu and Xinyi Chen and Yin Yuan and Chao Wang},
keywords = {Complex scenes, RGB-D, Salient object ranking, Indoor, Depth},
abstract = {RGB-D salient object detection has achieved a great development in recent years due to its extensive applications. Previous studies mainly focus on simple scene images with one single object. These models usually become overwhelmed by complex scenes with multiple objects. Moreover, these methods model salient object detection as a binary segmentation problem. However, psychology studies show that humans shift their visual attention from one object to another and rank salient objects, especially in complex indoor scenes. Following the psychological studies, we propose to rank salient objects in RGB-D images of complex indoor scenes. Due to the lack of such data, we first construct a RGB-D salient object ranking dataset containing complex indoor scenes with multiple objects. The saliency ranking of different objects is defined based on the order that an observer notices these objects. The final salient object ranking result is an average across the saliency rankings of 13 observers. This RGB-D salient object ranking dataset is also analyzed with current mainstream RGB-D salient object detection dataset for comparison. Since location information provided by depth images can help to determine the saliency ranking of objects, we further propose an end-to-end network exploiting depth stack and ground truth stack to predict the order of salient objects in complex scenes. The quantitative and qualitative comparisons demonstrate the effectiveness of the proposed method.}
}
@article{DU2023109154,
title = {Prototype-Guided Feature Learning for Unsupervised Domain Adaptation},
journal = {Pattern Recognition},
volume = {135},
pages = {109154},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109154},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006331},
author = {Yongjie Du and Deyun Zhou and Yu Xie and Yu Lei and Jiao Shi},
keywords = {Unsupervised domain adaptation, Class prototype, Pseudo labeling, Label filtering},
abstract = {Unsupervised Domain Adaptation transfers knowledge from the source domain to the target domain. It makes remarkable progress in alleviating the label-shortage problem in machine learning. Existing methods focus on aligning the two domain distributions directly. However, due to domain discrepancy, there may be some samples in the source domain being unnecessary or even harmful to the target tasks. Avoiding transferring knowledge from these samples is crucial. Existing researches are limited in this area. To this end, we propose a new unsupervised domain adaptation approach named the prototype-guided feature learning. The proposed method contains three main innovations. Firstly, we propose to utilize the more representative source-domain samples, class prototypes, to learn a domain-invariant subspace with the target samples. Secondly, the modified nearest class prototype method is proposed to predict the target samples by exploiting the structural information of the target domain efficiently. Thirdly, a multi-stage label filtering method is proposed to alleviate the mislabeling problem during training. Extensive experiments manifest that our method is competitive compared to the current mainstream unsupervised domain adaptive methods.}
}
@article{WANG2023109259,
title = {TETFN: A text enhanced transformer fusion network for multimodal sentiment analysis},
journal = {Pattern Recognition},
volume = {136},
pages = {109259},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109259},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007385},
author = {Di Wang and Xutong Guo and Yumin Tian and Jinhui Liu and LiHuo He and Xuemei Luo},
keywords = {Multimodal sentiment analysis, Transformer, Text-oriented pairwise cross-modal mappings},
abstract = {Multimodal sentiment analysis (MSA), which aims to recognize sentiment expressed by speakers in videos utilizing textual, visual and acoustic cues, has attracted extensive research attention in recent years. However, textual, visual and acoustic modalities often contribute differently to sentiment analysis. In general, text contains more intuitive sentiment-related information and outperforms nonlinguistic modalities in MSA. Seeking a strategy to take advantage of this property to obtain a fusion representation containing more sentiment-related information and simultaneously preserving inter- and intra-modality relationships becomes a significant challenge. To this end, we propose a novel method named Text Enhanced Transformer Fusion Network (TETFN), which learns text-oriented pairwise cross-modal mappings for obtaining effective unified multimodal representations. In particular, it incorporates textual information in learning sentiment-related nonlinguistic representations through text-based multi-head attention. In addition to preserving consistency information by cross-modal mappings, it also retains the differentiated information among modalities through unimodal label prediction. Furthermore, the vision pre-trained model Vision-Transformer is utilized to extract visual features from the original videos to preserve both global and local information of a human face. Extensive experiments on benchmark datasets CMU-MOSI and CMU-MOSEI demonstrate the superior performance of the proposed TETFN over state-of-the-art methods.}
}
@article{BI2023109194,
title = {Cross-modal hierarchical interaction network for RGB-D salient object detection},
journal = {Pattern Recognition},
volume = {136},
pages = {109194},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109194},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006732},
author = {Hongbo Bi and Ranwan Wu and Ziqi Liu and Huihui Zhu and Cong Zhang and Tian-Zhu Xiang},
keywords = {Saliency detection, Salient object detection, RGB-D, Feature fusion, Cross-modal interaction},
abstract = {How to effectively exchange and aggregate the information of multiple modalities (e.g. RGB image and depth map) is a big challenge in the RGB-D salient object detection community. To address this problem, in this paper, we propose a cross-modal Hierarchical Interaction Network (HINet), which boosts the salient object detection by excavating the cross-modal feature interaction and progressively multi-level feature fusion. To achieve it, we design two modules: cross-modal information exchange (CIE) module and multi-level information progressively guided fusion (PGF) module. Specifically, the CIE module is proposed to exchange the cross-modal features for learning the shared representations, as well as the beneficial feedback to facilitate the discriminative feature learning of different modalities. Besides, the PGF module is designed to aggregate the hierarchical features progressively with the reverse guidance mechanism, which employs the high-level feature fusion to guide the low-level feature fusion and thus improve the saliency detection performance. Extensive experiments show that our proposed model significantly outperforms the existing nine state-of-the-art models on five challenging benchmark datasets. Codes and results are available at: https://github.com/RanwanWu/HINet.}
}
@article{ZHANG2023109247,
title = {Coarse-to-fine feature representation based on deformable partition attention for melanoma identification},
journal = {Pattern Recognition},
volume = {136},
pages = {109247},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109247},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007269},
author = {Dong Zhang and Jing Yang and Shaoyi Du and Hongcheng Han and Yuyan Ge and Longfei Zhu and Ce Li and Meifeng Xu and Nanning Zheng},
keywords = {Histopathological image, Melanoma identification, Deformable convolution, Attention mechanism, Feature representation, Deep learning},
abstract = {In the histopathological melanoma image diagnosis system, manual identification of super-scale slides with dense cells is tedious, time-consuming, and subjective. To deal with this problem, we propose an automatic identification network based on the deformable partition attention to identify lots of dense slides as an assistant. A coarse-to-fine strategy is adopted in feature representation and qualitative identification to improve the identification accuracy of melanomas and nevi. First of all, because it is difficult to extract features in the lesion area with blurred boundaries and uneven distribution, we develop a deformable partition attention module, which integrates the advantage of the attention mechanism and deformable convolution. The module overcomes the limitation of rectangular convolution and gradually refines the channel and spatial features, which enriches feature representation by combining global and local features. Secondly, to address the problem of difficult convergence and poor recognition rate caused by the excessive non-aligned distance between benign-malignant and benign subcategories, we propose a progressive architecture via a coarse sub-network closely followed by a fine sub-network. Moreover, to further increase the inter-class differences and reduce the intra-class disparities, we propose a joint loss function to mine hard samples, which effectively improves the identification performance. Experimental results on the clinical dataset show that the proposed algorithm has higher sensitivity and specificity and outperforms state-of-the-art deep neural networks.}
}
@article{MADHU2023109153,
title = {ICC++: Explainable feature learning for art history using image compositions},
journal = {Pattern Recognition},
volume = {136},
pages = {109153},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109153},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200632X},
author = {Prathmesh Madhu and Tilman Marquart and Ronak Kosti and Dirk Suckow and Peter Bell and Andreas Maier and Vincent Christlein},
keywords = {Image/Scene compositions, Computer vision, Art history},
abstract = {Image compositions are helpful in the study of image structures and assist in discovering the semantics of the underlying scene portrayed across art forms and styles. With the digitization of artworks in recent years, thousands of images of a particular scene or narrative could potentially be linked together. However, manually linking this data with consistent objectiveness can be a highly challenging and time-consuming task. In this work, we present a novel approach called Image Composition Canvas (ICC++) to compare and retrieve images having similar compositional elements. ICC++ is an improvement over ICC, specializing in generating low and high-level features (compositional elements) motivated by Max Imdahl’s work. To this end, we present a rigorous quantitative and qualitative comparison of our approach with traditional and state-of-the-art (SOTA) methods showing that our proposed method outperforms all of them. In combination with deep features, our method outperforms the best deep learning-based method, opening the research direction for explainable machine learning for digital humanities. We will release the code and the data post-publication.}
}
@article{FU2023109263,
title = {AuxBranch: Binarization residual-aware network design via auxiliary branch search},
journal = {Pattern Recognition},
volume = {136},
pages = {109263},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109263},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007427},
author = {Siming Fu and Huanpeng Chu and Lu Yu and Bo Peng and Zheyang Li and Wenming Tan and Haoji Hu},
keywords = {Binary neural network, Binarization residual, Performance estimation indicator, Neural architecture search},
abstract = {While network binarization is a promising method in memory saving and speedup on hardware, it inevitably leads to binarization residual of intermediate features, resulting in performance capability degradation. To alleviate the above issue, we focus on the network topology design scheme to the more suitable network structure for the extreme-low-bit scenario. In this paper, we propose the baseline-auxiliary expanding network design method to compensate for the binarization residual of features via searching for auxiliary branches, denoted as AuxBranch. The intermediate feature maps are reasonably enhanced by combining baseline and auxiliary features, mimicking the corresponding feature output of the full-precision network. In addition, we devise a hybrid performance estimator (PE) with three elements of preliminary accuracy, feature similarity, and computational complexity. The PE jointly performs an efficient architecture search for binarization baseline and enables automatic computation complexity adjustment under diverse constraints. Extensive experiments show that our approach is superior in terms of accuracy and computational performance, and is plug-and-play for different network backbones and binarization policies. Our code is available at https://github.com/VipaiLab/AuxBranch.}
}
@article{WANG2023109166,
title = {M-CBN: Manifold constrained joint image dehazing and super-resolution based on chord boosting network},
journal = {Pattern Recognition},
volume = {135},
pages = {109166},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109166},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006458},
author = {Pengyu Wang and Hongqing Zhu and Han Zhang and Nan Wang},
keywords = {Image dehazing, Chord boosting network, Super-resolution, Frequency feature cross-collaboration, Manifold constraint},
abstract = {This paper proposes a Manifold Constrained Chord Boosting Network (M-CBN), which incorporates the super-resolution principle to achieve image dehazing. M-CBN is a task-specific image restoration network that explicitly learns the mapping from Low-resolution (LR) hazy images to High-resolution (HR) haze-free images. Hence, we design a preliminary image degradation to imitate super-resolution training on hazy images. In M-CBN, a plug-and-play Cross-linked Dual Projection Module (CDPM) for skip connections is developed. In CDPM, back-projections for HR encoder features and LR decoder features are cross-linked for better recovery of spatial information, and a cross-resolution spatial attention is designed to enhance fusion features. Then, to boost the generation of image details and textures, we propose a Chord Residual Module (CRM), which can separately process High-frequency (HF) and Low-frequency (LF) features by progressive inner-frequency updating and dense inter-frequency cross-collaboration to enhance decoding features. Finally, a manifold constraint dual discriminator is established. The static discriminator explicitly constrains dehazed images in the expected manifold to unify the joint learning of image dehazing and super-resolution. And the dynamic discriminator implicitly optimizes the network by adversarial training. Extensive experiments on general, dense and non-homogeneous haze datasets and cross-domain dehazing tasks show the proposed M-CBN presents high-quality dehazed results with natural colors and clear details.}
}
@article{CHENG2023109182,
title = {Adversarial training with distribution normalization and margin balance},
journal = {Pattern Recognition},
volume = {136},
pages = {109182},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109182},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006616},
author = {Zhen Cheng and Fei Zhu and Xu-Yao Zhang and Cheng-Lin Liu},
keywords = {Adversarial robustness, Adversarial training, Distribution normalization, Margin balance},
abstract = {Adversarial training is the most effective method to improve adversarial robustness. However, it does not explicitly regularize the feature space during training. Adversarial attacks usually move a sample iteratively along the direction which causes the steepest ascent of classification loss by crossing decision boundary. To alleviate this problem, we propose to regularize the distributions of different classes to increase the difficulty of finding an attacking direction. Specifically, we propose two strategies named Distribution Normalization (DN) and Margin Balance (MB) for adversarial training. The purpose of DN is to normalize the features of each class to have identical variance in every direction, in order to eliminate easy-to-attack intra-class directions. The purpose of MB is to balance the margins between different classes, making it harder to find confusing class directions (i.e., those with smaller margins) to attack. When integrated with adversarial training, our method can significantly improve adversarial robustness. Extensive experiments under white-box, black-box, and adaptive attacks demonstrate the effectiveness of our method over other state-of-the-art methods.}
}
@article{TANG2023109206,
title = {Training Compact DNNs with ℓ1/2 Regularization},
journal = {Pattern Recognition},
volume = {136},
pages = {109206},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109206},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006859},
author = {Anda Tang and Lingfeng Niu and Jianyu Miao and Peng Zhang},
keywords = {Deep neural networks, Model compression,  Quasi-norm, Non-Lipschitz regularization, Sparse optimization},
abstract = {Deep neural network(DNN) has achieved unprecedented success in many fields. However, its large model parameters which bring a great burden on storage and calculation hinder the development and application of DNNs. It is worthy of compressing the model to reduce the complexity of the DNN. Sparsity-inducing regularizer is one of the most common tools for compression. In this paper, we propose utilizing the ℓ1/2 quasi-norm to zero out weights of neural networks and compressing the networks automatically during the learning process. To our knowledge, it is the first work applying the non-Lipschitz continuous regularizer for the compression of DNNs. The resulting sparse optimization problem is solved by stochastic proximal gradient algorithm. For further convenience of calculation, an approximation of the threshold-form solution to the proximal operator with ℓ1/2 is given at the same time. Extensive experiments with various datasets and baselines demonstrate the advantages of our new method.}
}
@article{ZHU2023109258,
title = {A Dual Self-Attention mechanism for vehicle re-Identification},
journal = {Pattern Recognition},
volume = {137},
pages = {109258},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109258},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007373},
author = {Wenqian Zhu and Zhongyuan Wang and Xiaochen Wang and Ruimin Hu and Huikai Liu and Cheng Liu and Chao Wang and Dengshi Li},
keywords = {Cross-region attention, Dual self-attention, Multi-attention network, Vehicle re-identification, Feature embedding},
abstract = {Vehicle re-identification has attracted tremendous attention from computer vision communities for its extensive applications in intelligent transportation and public security, while the high inter-class similarity and the large intra-class difference between vehicles bring out great challenges for re-identification (re-ID). To tackle these challenges, we learn from the self-attention mechanism in Natural Language Processing and propose a dual self-attention module to learn different regional dependencies: static self-attention for selectively refining semantic features and dynamic self-attention (called cross-region attention) for enhancing the spatial awareness of local feature. The static self-attention refines attended pixels within the entire image and salient regions, while the cross-region attention creatively captures the position-related regional dependencies for pixels within the windscreen area. These attention modules capture long-range dependencies and relative position information between different pixels or regions for vehicle feature learning globally and locally, realizing an efficient vehicle feature embedding by concatenating these augmented features for vehicle re-ID. Extensive experiments demonstrate the effectiveness and promising performance of our approach against the state-of-the-arts.}
}
@article{LI2023109234,
title = {Learning spatiotemporal embedding with gated convolutional recurrent networks for translation initiation site prediction},
journal = {Pattern Recognition},
volume = {136},
pages = {109234},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109234},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007130},
author = {Weihua Li and Yanbu Guo and Bingyi Wang and Bei Yang},
keywords = {Translation initiation sites, Gated convolutional networks, Residual networks, Gated recurrent units},
abstract = {Accurately predicting translation initiation sites (TIS) from genomic sequences is crucial for understanding gene regulation and function. TIS prediction methods’ feature vectors are not discriminative enough to lead to unsatisfactory predictive results. In this work, we devise an efficient gated convolutional recurrent network (GCR-Net) with residual learning to dynamically extract dependency patterns of raw genomic sequences in an efficient fusion strategy and successfully improve the performance of the TIS prediction. GCR-Net mainly includes exponential gated convolutional residual networks (EGCRN) and bidirectional gated recurrent unit (Bi-GRU) networks. Particularly, we devise the novel EGCRN to extract multiple complex patterns of the spatial dimension from genomic sequences, where we design an exponential gated linear unit (EGLU) to reduce the vanishing gradient problem. Moreover, we combine EGLU with shortcut connections to develop the stacked gated mechanism based on convolutions that benefit information propagation across layers. Then, we use Bi-GRU with identity connections to learn long-term dependency patterns of the temporal dimension from genomic sequences. Besides, we evaluate our GCR-Net model on four TIS datasets, and experiments demonstrate that GCR-Net is an efficient deep learning-based TIS prediction tool and obtains superior performance compared to the baseline methods.}
}
@article{ZHU2023109143,
title = {Shape robustness in style enhanced cross domain semantic segmentation},
journal = {Pattern Recognition},
volume = {135},
pages = {109143},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109143},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006161},
author = {Siyu Zhu and Yingjie Tian},
keywords = {Domain adaptation, Semantic segmentation, Transfer learning},
abstract = {This paper focuses on domain adaptation method based on style transfer. Previous methods based on style transfer pay attention to the transformation of texture features between domains and maintain semantic consistency to the greatest extent. However, these methods have different effects on domain gaps in different types of categories. The categories with large texture difference and small structure difference can be improved better. For the categories with small texture difference and large structure difference, it causes negative transfer. In this paper, a shape robustness enhanced domain adaptive segmentation algorithm is proposed. Firstly, we adopt adjustable style transfer methods to enhance the style diversity of source domain images. Next, we differentiated different types of image features to weaken the negative transfer in the process of adversarial training. The results of this paper on general data sets GTA5 and SYNTHIA are better than other style transfer methods. Further experiments show that we improve the shape robustness of style enhancement method in domain adaptive segmentation task.}
}
@article{XU2023109149,
title = {Event-driven daily activity recognition with enhanced emergent modeling},
journal = {Pattern Recognition},
volume = {135},
pages = {109149},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109149},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006288},
author = {Zimin Xu and Guoli Wang and Xuemei Guo},
keywords = {Emergent paradigm, Marker-based stigmergy, Directed-weighted network, Activity modeling, Daily activity recognition},
abstract = {With the population aging, elderly health monitoring is triggering more studies on daily activity recognition as the fundamental of ambient assisted living. It is remarkable that activity recognition remains difficulties including how to adequately extract feature structure and settle the issue of activity confusion. To address these challenges, we propose a novel activity modeling method under the emergent paradigm with marker-based stigmergy and the directed-weighted network with additional context-aware information. In the modeling process, stigmergy is first introduced to aggregate the context information at the low level for generating activity pheromone trails, and then the constructed stigmergic trails are represented in form of directed-weighted network with distinguishability of individual pheromone source corresponding to location. The potential advantage is that the robust trails with distinguishable individual initial positions are feasible to supplement user’s daily habits and thus both inter-class and intra-class distances can be kept at acceptable levels. Experiments on Aruba demonstrates that the proposed emergent modeling method can effectively deal with the problems of feature extraction and activity ambiguity and achieve good classification performance.}
}
@article{XIE2023109230,
title = {Scalable clustering by aggregating representatives in hierarchical groups},
journal = {Pattern Recognition},
volume = {136},
pages = {109230},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109230},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007099},
author = {Wen-Bo Xie and Zhen Liu and Debarati Das and Bin Chen and Jaideep Srivastava},
keywords = {Hierarchical clustering, Election tree, Representative node, Root},
abstract = {Appropriately handling the scalability of clustering is a long-standing challenge for the study of clustering techniques and is of fundamental interest to researchers in the community of data mining and knowledge discovery. In comparison to other clustering methods, hierarchical clustering demonstrates better interpretability of clustering results but poor scalability while handling large-scale data. Thus, more comprehensive studies on this problem need to be conducted. This paper develops a new scalable hierarchical clustering model called Election Tree, which can detect the most representative point for each sub-cluster via the process of node election in split data and adjust the members in sub-clusters by the operations of node merging and swap. Extensive experiments on real-world datasets reveal that the proposed computational framework has better clustering accuracy as opposed to the competing baseline methods. Meanwhile, with respect to the scalability tests on incremental synthetic datasets, the results show that the new model has a significantly lower time consumption than the state-of-the-art hierarchical clustering models such as PERCH, GRINCH, SCC and other classic baselines.}
}
@article{WANG2023109193,
title = {FP-DARTS: Fast parallel differentiable neural architecture search for image classification},
journal = {Pattern Recognition},
volume = {136},
pages = {109193},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109193},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006720},
author = {Wenna Wang and Xiuwei Zhang and Hengfei Cui and Hanlin Yin and Yannnig Zhang},
keywords = {Neural architecture search, Computing overheads, Operator sub-sets, Two-parallel-path, Binary gate, Sigmoid function},
abstract = {Neural Architecture Search (NAS) has made remarkable progress in automatic machine learning. However, it still suffers massive computing overheads limiting its wide applications. In this paper, we present an efficient search method, Fast Parallel Differential Neural Architecture Search (FP-DARTS). The proposed method is carefully designed from three levels to construct and train the super-network. Firstly, at the operation-level, to reduce the computational burden, different from the standard DARTS search space (8 operations), we decompose the operation set into two non-overlapping operator sub-sets (4 operations for each). Adopting these two reduced search spaces, two over-parameterized sub-networks are constructed. Secondly, at the channel-level, the partially-connected strategy is adopted, where each sub-network only adopts partial channels. Then these two sub-networks construct a two-parallel-path super-network by addition. Thirdly, at the training-level, the binary gate is introduced to control whether a path participates in the super-network training. It may suffer an unfair issue when using softmax to select the best input for intermediate nodes across two operator sub-sets. To tackle this problem, the sigmoid function is introduced, which measures the performance of operations without compression. Extensive experiments demonstrate the effectiveness of the proposed algorithm. Specifically, FP-DARTS achieves 2.50% test error with only 0.08 GPU-days on CIFAR10, and a state-of-the-art top-1 error rate of 23.7% on ImageNet using only 2.44 GPU-days for search.}
}
@article{MAHDIKHANLOU2023109217,
title = {3D hand pose estimation from a single RGB image by weighting the occlusion and classification},
journal = {Pattern Recognition},
volume = {136},
pages = {109217},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109217},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006963},
author = {Khadijeh Mahdikhanlou and Hossein Ebrahimnezhad},
keywords = {3D hand pose estimation, Semantic segmentation, Occlusion weight, Hand pose classification},
abstract = {In this paper, a new framework for 3D hand pose estimation using a single RGB image is proposed. The framework is composed of two blocks. The first block formulates the hand pose estimation as a classification problem. Since the human hand can perform numerous poses, the classification network needs a huge number of parameters. So, we propose to classify hand poses based on three different aspects, including hand gesture, hand direction, and palm direction. In this way, the number of parameters will be significantly reduced. The motivation behind the classification block is that the model deals with the image as a whole and extracts global features. Furthermore, the output of the classification model is a valid pose that does not include any unexpected angle at joints. The second block estimates the 3D coordinates of the hand joints and focuses more on the details of the image pattern. RGB-based 3D hand pose estimation is an inherently ill-posed problem due to the lack of depth information in the 2D image. We propose to use the occlusion status of the hand joints to solve this problem. The occlusion status of the joints has been labeled manually. Some joints are partially occluded, and we propose to compute the extent of the occlusion by semantic segmentation. The existing methods in this field mostly used synthetic datasets. But all the models proposed in this paper are trained on more than 50 K real images. Extensive experiments on our new dataset and two other benchmark datasets show that the proposed method can achieve good performance. We also analyze the validity of the predicted poses, and the results show that the classification block increases the validity of the poses.}
}
@article{ZHANG2023109165,
title = {Weakly Supervised Instance Segmentation via Category-aware Centerness Learning with Localization Supervision},
journal = {Pattern Recognition},
volume = {136},
pages = {109165},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109165},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006446},
author = {Jiabin Zhang and Hu Su and Yonghao He and Wei Zou},
keywords = {Weakly supervised learning, Instance segmentation, Centerness, Coarse localization annotation},
abstract = {Deep convolutional neural networks (DCNN) trained with pixel-level segmentation masks achieve high performance in the task of instance segmentation. The difficulty of acquiring such annotation limits the application and popularization of the DCNN-based approaches. To address the issue, a weakly supervised approach is proposed in the paper which performs instance segmentation only with the supervision of bounding box or coarse localization annotation. A novel DCNN model is constructed which consists of two branches: the centerness branch and the segmentation branch. The former branch is to learn the semantically spatial importance over the areas of object instances under the localization supervision. Object proposals with exact boundaries are automatically generated and are then ranked under the guidance of the output of the centerness branch. The most matched instance proposal is assigned to each object, which is then used to supervise the segmentation branch. The losses are calculated by both the outputs of the two branches and the entire DCNN model is trained end-to-end. Experiments are extensively conducted to verify the effectiveness. With the supervision of precise bounding box annotation, our approach achieves state-of-the-art (SOTA) accuracy in the comparison with recent related works. And in the case of coarse localization annotation, our approach only deduces a slight reduction in accuracy, which significantly outperforms other approaches. The excellent performance demonstrates that our approach would be helpful to further alleviate the workload of image annotation while maintaining competitive accuracy.}
}
@article{KHATUN2023109246,
title = {Pose-driven attention-guided image generation for person re-Identification},
journal = {Pattern Recognition},
volume = {137},
pages = {109246},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109246},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007257},
author = {Amena Khatun and Simon Denman and Sridha Sridharan and Clinton Fookes},
keywords = {Person re-identification, Pose-transfer, Attention mechanism, Semantic-consistency loss},
abstract = {Person re-identification (re-ID) concerns the matching of subject images across different camera views in a multi camera surveillance system. One of the major challenges in person re-ID is pose variations across the camera network, which significantly affects the appearance of a person. Existing development data lack adequate pose variations to carry out effective training of person re-ID systems. To solve this issue, in this paper we propose an end-to-end pose-driven attention-guided generative adversarial network, to generate multiple poses of a person. We propose to attentively learn and transfer the subject pose through an attention mechanism. A semantic-consistency loss is proposed to preserve the semantic information of the person during pose transfer. To ensure fine image details are realistic after pose translation, an appearance discriminator is used while a pose discriminator is used to ensure the pose of the transferred images will exactly be the same as the target pose. We show that by incorporating the proposed approach in a person re-identification framework, realistic pose transferred images and state-of-the-art re-identification results can be achieved.}
}
@article{ZHANG2023109181,
title = {Large motion anime head animation using a cascade pose transform network},
journal = {Pattern Recognition},
volume = {135},
pages = {109181},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109181},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006604},
author = {Jiale Zhang and Chengxin Liu and Ke Xian and Zhiguo Cao},
keywords = {Talking head animation, Generative adversarial networks, Pose transformation, Anime image generation, Anime dataset},
abstract = {We study the problem of talking head animation from a single image where a target anime talking head is generated to mimic the change of facial expression and head movement of source anime characters. Most existing methods focus on generating talking heads from real humans. However, few efforts have been made to create anime talking head. Compared with human head generation, the key challenges of anime head generation are: how to align the pose and facial expression of the target head with that of the source head without explicit facial landmarks. To address this, we propose CPTNetV2, a cascaded pose transform network that unifies face pose transformation and head pose transformation. At the core of CPTNetV2 is the implicit encoding of facial changes and head movement by a pose vector. Given the pose vector, we introduce a mask generator to animate facial expression (e.g., close eyes and open mouth) and a grid generator to simulate head movement, followed by a fusion module to generate talking heads. To tackle large displacement and improve the quality of generation, we further design a details inpainting module with pose vector decomposition to reduce the receptive field of network required for pose transformation. In particular, we collect an anime talking head dataset AniHead-2K that includes around 2000 anime characters with different face/head poses. Extensive experiments on AniHead-2K demonstrate that CPTNetV2 can achieve arbitrary pose transformation conditioned on the target pose vector and outperforms other state-of-the-art methods. We also verify the effectiveness of each module through ablative studies. Additional results show that CPTNetV2 has good generalization and is applicable to generate anime talking head even based on human videos. The dataset will be made available at: https://github.com/zhangjiale487/AniHead-2K.}
}
@article{TAN2023109189,
title = {A Novel Label Enhancement Algorithm Based on Manifold Learning},
journal = {Pattern Recognition},
volume = {135},
pages = {109189},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109189},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006689},
author = {Chao Tan and Sheng Chen and Xin Geng and Genlin Ji},
keywords = {Multi-label learning, Label enhancement, Incremental subspace learning, Label propagation, Manifold learning, Conditional random field},
abstract = {We propose a label enhancement model to solve the multi-label learning (MLL) problem by using the incremental subspace learning to enrich the label space and to improve the ability of label recognition. In particular, we use the incremental estimation of the feature function representing the manifold structure to guide the construction of the label space and to transform the local topology from the feature space to the label space. First, we build a recursive form for incremental estimation of the feature function representing the feature space information. Second, the label propagation is used to obtain the hidden supervisory information of labels in the data. Finally, an enhanced maximum entropy model based on conditional random field is established as the objective, to obtain the predicted label distribution. The enriched label information in the manifold space obtained in first step and the estimated label distributions provided in second step are employed to train this enhanced maximum entropy model by a gradient-descent iterative optimization to obtain the label distribution predictor’s parameters with enhanced accuracy. We evaluate our method on 24 real-world datasets. Experimental results demonstrate that our label enhancement manifold learning model has advantages in predictive performance over the latest MLL methods.}
}
@article{CAO2023109262,
title = {Learning generalized visual odometry using position-aware optical flow and geometric bundle adjustment},
journal = {Pattern Recognition},
volume = {136},
pages = {109262},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109262},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007415},
author = {Yi-Jun Cao and Xian-Shi Zhang and Fu-Ya Luo and Peng Peng and Chuan Lin and Kai-Fu Yang and Yong-Jie Li},
keywords = {Visual odometry, Self-supervise learning, Optical flow, Monocular depth estimation, Joint learning, Generalization capability},
abstract = {Recent visual odometry (VO) methods incorporating geometric algorithm into deep-learning architecture have shown outstanding performance on the challenging monocular VO task. Despite encouraging results are shown, previous methods ignore the requirement of generalization capability under noisy environment and various scenes. To address this challenging issue, this work first proposes a novel optical flow network (PANet). Compared with previous methods that predict optical flow as a direct regression task, our PANet computes optical flow by predicting it into the discrete position space with optical flow probability volume, and then converting it to optical flow. Next, we improve the bundle adjustment module to fit the self-supervised training pipeline by introducing multiple sampling, ego-motion initialization, dynamic damping factor adjustment, and Jacobi matrix weighting. In addition, a novel normalized photometric loss function is advanced to improve the depth estimation accuracy. The experiments show that the proposed system not only achieves comparable performance with other state-of-the-art self-supervised learning-based methods on the KITTI dataset, but also significantly improves the generalization capability compared with geometry-based, learning-based and hybrid VO systems on the noisy KITTI and the challenging outdoor (KAIST) scenes.}
}
@article{XUE2023109205,
title = {Local Linear Embedding with Adaptive Neighbors},
journal = {Pattern Recognition},
volume = {136},
pages = {109205},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109205},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006847},
author = {Jiaqi Xue and Bin Zhang and Qianyao Qiang},
keywords = {dimensionality reduction, Locally Linear Embedding, manifold learning, adaptive neighbor strategy},
abstract = {Dimensionality reduction is one of the most important techniques in the field of data mining. It embeds high-dimensional data into a low-dimensional vector space while keeping the main information as much as possible. Locally Linear Embedding (LLE) as a typical manifold learning algorithm computes neighborhood preserving embeddings of high-dimensional inputs. Based on the thought of LLE, we propose a novel unsupervised dimensionality reduction model called Local Linear Embedding with Adaptive Neighbors (LLEAN). To achieve a desirable dimensionality reduction result, we impose adaptive neighbor strategy and adopt a projection matrix to project data into an optimal subspace. The relationship between every pair-wise data is investigated to help reveal the data structure. Augmented Lagrangian Multiplier (ALM) is devised in optimization procedure to effectively solve the proposed objective function. Comprehensive experiments on toy data and benchmark datasets have been done and the results show that LLEAN outperforms other state-of-the-art dimensionality reduction methods.}
}
@article{LI2023109229,
title = {Understanding and combating robust overfitting via input loss landscape analysis and regularization},
journal = {Pattern Recognition},
volume = {136},
pages = {109229},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109229},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007087},
author = {Lin Li and Michael Spratling},
keywords = {Adversarial robustness, Adversarial training, Robust overfitting, Loss landscape analysis, Logit regularization},
abstract = {Adversarial training is widely used to improve the robustness of deep neural networks to adversarial attack. However, adversarial training is prone to overfitting, and the cause is far from clear. This work sheds light on the mechanisms underlying overfitting through analyzing the loss landscape w.r.t. the input. We find that robust overfitting results from standard training, specifically the minimization of the clean loss, and can be mitigated by regularization of the loss gradients. Moreover, we find that robust overfitting turns severer during adversarial training partially because the gradient regularization effect of adversarial training becomes weaker due to the increase in the loss landscape’s curvature. To improve robust generalization, we propose a new regularizer to smooth the loss landscape by penalizing the weighted logits variation along the adversarial direction. Our method significantly mitigates robust overfitting and achieves the highest robustness and efficiency compared to similar previous methods. Code is available at https://github.com/TreeLLi/Combating-RO-AdvLC.}
}
@article{SHU2023109257,
title = {ALVLS: Adaptive local variances-Based levelset framework for medical images segmentation},
journal = {Pattern Recognition},
volume = {136},
pages = {109257},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109257},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007361},
author = {Xiu Shu and Yunyun Yang and Jun Liu and Xiaojun Chang and Boying Wu},
keywords = {Image segmentation, Local fitting variance, Edge-based information, Level set framework},
abstract = {Medical image segmentation is a very challenging task, not only because the intensity of the medical image itself is not uniform, but also it may be accompanied by the impact of noise. Although mathematics, computer science, medicine, and other interdisciplinary fields have begun to study the problem of medical image segmentation, and have put forward a variety of segmentation algorithms, there is still much room for further improvement and enhancement. In the process of medical image collection and reconstruction, it is easy to produce intensity inhomogeneity and noises, as well as interference from other tissues, resulting in the difficulty of accurate segmentation. In this paper, we propose the adaptive local variances-based level set (ALVLS) model to segment medical images with intensity inhomogeneity and noises, including cardiac MR images, brain MR images, and breast ultrasound images. According to the variance difference information, the ALVLS model can adjust the effect of the area term adaptively. The local intensity variances are designed to optimize the ability to resist noise, which improves the segmentation accuracy of medical images. We also propose the two-layer level set model for segmenting left ventricles and left epicardium simultaneously. Experimental results for medical images and synthetic images show the desirable performance of the ALVLS model in accuracy, efficiency, and robustness to noise. In medical image competition, the Dice coefficient is used to calculate the similarity between the segmentation result and the ground truth. Thus we do comparisons with other methods and show that the Dice coefficient of the proposed method is higher than other testing methods.}
}
@article{YANG2023109245,
title = {HAMIL: Hierarchical aggregation-based multi-instance learning for microscopy image classification},
journal = {Pattern Recognition},
volume = {136},
pages = {109245},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109245},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007245},
author = {Yang Yang and Yanlun Tu and Houchao Lei and Wei Long},
keywords = {Multi-instance learning, Biomedical image, Hierarchical aggregation},
abstract = {Multi-instance learning is common for computer vision tasks, especially in biomedical image processing. Traditional methods for multi-instance learning focus on designing feature aggregation methods and multi-instance classifiers, where the aggregation operation is performed either in the feature extraction or learning phase. As deep neural networks (DNNs) achieve great success in image processing via automatic feature learning, certain feature aggregation mechanisms need to be incorporated into common DNN architecture for multi-instance learning. Moreover, flexibility and reliability are crucial considerations to deal with varying quality and number of instances. In this study, we propose a hierarchical aggregation network for multi-instance learning, called HAMIL. The hierarchical aggregation protocol enables feature fusion in a defined order, and the simple convolutional aggregation units lead to an efficient and flexible architecture. We assess the model performance on two microscopy image classification tasks, namely protein subcellular localization using immunofluorescence images and gene annotation using spatial gene expression images. The experimental results show that HAMIL outperforms the state-of-the-art feature aggregation methods and the existing models for addressing these two tasks. The visualization analyses also demonstrate the ability of HAMIL to focus on high-quality instances.}
}
@article{LI2023109200,
title = {Progressive generation of 3D point clouds with hierarchical consistency},
journal = {Pattern Recognition},
volume = {136},
pages = {109200},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109200},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006793},
author = {Peipei Li and Xiyan Liu and Jizhou Huang and Deguo Xia and Jianzhong Yang and Zhen Lu},
keywords = {3D Point cloud generation, Point cloud analysis, Generative adversarial networks, Variational autoencoder, Hierarchical consistency},
abstract = {Generating 3D point cloud directly from latent prior (e.g., Gaussian distribution) plays a vital role in the representation learning and data augmentation in 3D vision tasks. Since point cloud is formed by irregular points, the generation process of point cloud requires rich semantic information, yet few studies are devoted to it. In this paper, we recast this generation task as a progressive learning problem to model the two-level hierarchy of distributions and address it by proposing a novel model called hierarchical consistency variational autoencoder (HC-VAE). This framework introduces a hierarchical consistent mechanism (HCM) to model the shape consistency and the pointwise representation consistency in a complementary manner. Specifically, we propose a stackable encoder-decoder framework and constrain the generation quality progressively to ensure that the underlying shape and fine-grained parts can be reconstructed with high fidelity. Additionally, given the progressively generated intermediate point cloud instances, a hierarchical-positive contrastive loss is introduced to learn the point-distribution-free instance representations to avoid explicitly parametrizing the distribution of points in a shape. In this way, our model suffices to generate diverse, high-resolution, and uniform point cloud instances. Extensive experimental results demonstrate that the proposed method achieves state-of-the-art performance in point cloud generation.}
}
@article{LIU2023109261,
title = {A pyramid input augmented multi-scale CNN for GGO detection in 3D lung CT images},
journal = {Pattern Recognition},
volume = {136},
pages = {109261},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109261},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007403},
author = {Weihua Liu and Xiabi Liu and Xiongbiao Luo and Murong Wang and Guanghui Han and Xinming Zhao and Zheng Zhu},
keywords = {GGO detection, Multi-scale processing, 3D CT scans, Pyramid inputs},
abstract = {This paper proposes a new convolutional neural network (CNN) with multi-scale processing for detecting ground-glass opacity nodules (GGO) in 3D computed tomography (CT) images, which is referred to as PiaNet for short. PiaNet consists of a feature-extraction module and a prediction module. The former module is constructed by introducing pyramid multi-scale source connections into a contracting-expanding structure. Besides, a new multi-receptive-field convolution block (MRCB) is presented to fuse the convolutions with multiple kernels of varying sizes for capturing features in each scale of information better. The latter module includes a bounding-box regressor and a classifier that are employed to simultaneously recognize GGO nodules and estimate bounding boxes at multiple scales. To train the proposed PiaNet, a two-stage transfer learning strategy is developed. In the first stage, the feature-extraction module is embedded into a classifier network that is trained on a large data set of GGO and non-GGO patches, which are generated by performing data augmentation from a small number of annotated CT scans. In the second stage, the pretrained feature-extraction module is loaded into PiaNet, and then PiaNet is fine-tuned using the annotated CT scans. We evaluate the proposed PiaNet with the LIDC-IDRI dataset. The experimental results demonstrate that our method outperforms state-of-the-art counterparts, including the Subsolid CAD and Aidence systems and CPM-Net and S4ND and GA-SSD methods. PiaNet achieves a sensitivity of 93.6% with only one false positive per scan.}
}
@article{NING2023109216,
title = {Hyper-sausage coverage function neuron model and learning algorithm for image classification},
journal = {Pattern Recognition},
volume = {136},
pages = {109216},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109216},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006951},
author = {Xin Ning and Weijuan Tian and Feng He and Xiao Bai and Le Sun and Weijun Li},
keywords = {Pattern recognition, Deep neural networks, Neuron model, Brain-inspired, Computer vision},
abstract = {Recently, deep neural networks (DNNs) promote mainly by network architectures and loss functions; however, the development of neuron models has been quite limited. In this study, inspired by the mechanism of human cognition, a hyper-sausage coverage function (HSCF) neuron model possessing a high flexible plasticity. Then, a novel cross-entropy and volume-coverage (CE_VC) loss is defined, which compresses the volume of the hyper-sausage to the hilt, and helps alleviate confusion among different classes, thus ensuring the intra-class compactness of the samples. Finally, a divisive iteration method is introduced, which considers each neuron model as a weak classifier, and iteratively increases the number of weak classifiers. Thus, the optimal number of the HSCF neuron is adaptively determined and an end-to-end learning framework is constructed. In particular, to improve the classification performance, the HSCF neuron can be applied to classical DNNs. Comprehensive experiments on eight datasets in several domains demonstrate the effectiveness of the proposed method. The proposed method exhibits the feasibility of boosting DNNs with neuron plasticity and provides a novel perspective for further developments in DNNs. The source code is available at https://github.com/Tough2011/HSCFNet.git .}
}
@article{HE2023109188,
title = {Generalized minimum error entropy for robust learning},
journal = {Pattern Recognition},
volume = {135},
pages = {109188},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109188},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006677},
author = {Jiacheng He and Gang Wang and Kui Cao and He Diao and Guotai Wang and Bei Peng},
keywords = {Generalized Gaussian density, Generalized error entropy, Quantized generalized error entropy, Adaptive filtering, Kernel recursive least squares, Multilayer perceptron},
abstract = {The applications of error entropy (EE) are sometimes limited because its shape cannot be flexibly adjusted by the default Gaussian kernel function to adapt to noise variation and thus lowers the performance of algorithms based on minimum error entropy (MEE) criterion. In this paper, a generalized EE (GEE) is proposed by introducing the generalized Gaussian density (GGD) as its kernel function to improve the robustness of EE. In addition, GEE can be further improved to reduce its computational load by the quantized GEE (QGEE). Furthermore, two learning criteria, called generalized minimum error entropy (GMEE) and quantized generalized minimum error entropy (QGMEE), are developed based on GEE and QGEE, and new adaptive filtering (AF), kernel recursive least squares (KRLS), and multilayer perceptron (MLP) based on the proposed criteria are presented. Several numerical simulations indicate that the performance of proposed algorithms performs better than that of algorithms based on MEE.}
}
@article{MUSSABAYEV2023109269,
title = {How to Use K-means for Big Data Clustering?},
journal = {Pattern Recognition},
volume = {137},
pages = {109269},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109269},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007488},
author = {Rustam Mussabayev and Nenad Mladenovic and Bassem Jarboui and Ravil Mussabayev},
keywords = {Big data, Clustering, Minimum sum-of-squares, Divide and conquer algorithm, Decomposition, K-means, K-means++, Global optimization, Unsupervised learning},
abstract = {K-means plays a vital role in data mining and is the simplest and most widely used algorithm under the Euclidean Minimum Sum-of-Squares Clustering (MSSC) model. However, its performance drastically drops when applied to vast amounts of data. Therefore, it is crucial to improve K-means by scaling it to big data using as few of the following computational resources as possible: data, time, and algorithmic ingredients. We propose a new parallel scheme of using K-means and K-means++ algorithms for big data clustering that satisfies the properties of a “true big data” algorithm and outperforms the classical and recent state-of-the-art MSSC approaches in terms of solution quality and runtime. The new approach naturally implements global search by decomposing the MSSC problem without using additional metaheuristics. This work shows that data decomposition is the basic approach to solve the big data clustering problem. The empirical success of the new algorithm allowed us to challenge the common belief that more data is required to obtain a good clustering solution. Moreover, the present work questions the established trend that more sophisticated hybrid approaches and algorithms are required to obtain a better clustering solution.}
}
@article{TU2023109204,
title = {Relation-aware attention for video captioning via graph learning},
journal = {Pattern Recognition},
volume = {136},
pages = {109204},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109204},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006835},
author = {Yunbin Tu and Chang Zhou and Junjun Guo and Huafeng Li and Shengxiang Gao and Zhengtao Yu},
keywords = {Video captioning, Relation-aware attention, Graph learning},
abstract = {Video captioning often uses an attentive encoder-decoder as the baseline model. However, the conventional attention mechanism still remains two problems. First, the attended visual feature is often irrelevant to the target word state, because the attention process only uses the unidirectional flow from vision to linguistics, while lacking the reverse flow. Second, each attention result is independent, because it is computed only based on the previous word states while not considering the attention information from the past and future. This does not suit the attention habits of human beings. In this paper, we improve the conventional attention mechanism to a relation-aware attention mechanism. To this end, we propose two kinds of graph learning strategies, namely the linguistics-to-vision heterogeneous graph (HTG) and the vision-to-vision homogeneous graph (HMG). The HTG aims to enhance the inter-relation of attention by reversely modeling the relation of each word with respect to every attended visual feature, supporting proper semantic alignment in between. The HMG aims to enhance the intra-relation of attention by capturing the relations among all of the attended visual features, which can leverage the attention information from the past and future to guide the current attention process. Extensive experiments on two public datasets show that our proposed method not only significantly improves the baseline model, but also outperforms state-of-the-art methods.}
}
@article{XU2023109152,
title = {Fast subspace clustering by learning projective block diagonal representation},
journal = {Pattern Recognition},
volume = {135},
pages = {109152},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109152},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006318},
author = {Yesong Xu and Shuo Chen and Jun Li and Chunyan Xu and Jian Yang},
keywords = {Subspace clustering, Block diagonal representation, Large-scale data},
abstract = {Block Diagonal Representation (BDR) has attracted massive attention in subspace clustering, yet the high computational cost limits its widespread application. To address this issue, we propose a novel approach called Projective Block Diagonal Representation (PBDR), which rapidly pursues a representation matrix with the block diagonal structure. Firstly, an effective sampling strategy is utilized to select a small subset of the original large-scale data. Then, we learn a projection mapping to match the block diagonal representation matrix on the selected subset. After training, we employ the learned projection mapping to quickly generate the representation matrix with an ideal block diagonal structure for the original large-scale data. Additionally, we further extend the proposed PBDR model (i.e., PBDRℓ1 and PBDR*) by capturing the global or local structure of the data to enhance block diagonal coding capability. This paper also proves the effectiveness of the proposed model theoretically. Especially, this is the first work to directly learn a representation matrix with a block diagonal structure to handle the large-scale subspace clustering problem. Finally, experimental results on publicly available datasets show that our approaches achieve faster and more accurate clustering results compared to the state-of-the-art block diagonal-based subspace clustering approaches, which demonstrates its practical usefulness.}
}
@article{XIE2023109233,
title = {Multi-scale local-temporal similarity fusion for continuous sign language recognition},
journal = {Pattern Recognition},
volume = {136},
pages = {109233},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109233},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007129},
author = {Pan Xie and Zhi Cui and Yao Du and Mengyi Zhao and Jianwei Cui and Bin Wang and Xiaohui Hu},
keywords = {Sign language recognition, Temporal similarity, Content-aware feature selector, Position-aware convolution, Content-dependent aggregator},
abstract = {Continuous sign language recognition (cSLR) is a public significant task that transcribes a sign language video into an ordered gloss sequence. It is important to capture the fine-grained gloss-level details, since there is no explicit alignment between sign video frames and the corresponding glosses. Among the past works, one promising way is to adopt a one-dimensional convolutional network (1D-CNN) to temporally fuse the sequential frames. However, CNNs are agnostic to similarity or dissimilarity, and thus are unable to capture local consistent semantics within temporally neighboring frames. To address the issue, we propose to adaptively fuse local features via temporal similarity for this task. Specifically, we devise a Multi-scale Local-Temporal Similarity Fusion Network (mLTSF-Net) as follows: 1) In terms of a specific video frame, we firstly select its similar neighbours with multi-scale receptive regions to accommodate different lengths of glosses. 2) To ensure temporal consistency, we then use position-aware convolution to temporally convolve each scale of selected frames. 3) To obtain a local-temporally enhanced frame-wise representation, we finally fuse the results of different scales using a content-dependent aggregator. We train our model in an end-to-end fashion, and the experimental results on RWTH-PHOENIX-Weather 2014 datasets (RWTH) demonstrate that our model achieves competitive performance compared with several state-of-the-art models.}
}
@article{LI2023109215,
title = {Neural operator search},
journal = {Pattern Recognition},
volume = {136},
pages = {109215},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109215},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200694X},
author = {Wei Li and Shaogang Gong and Xiatian Zhu},
keywords = {Neural architecture search, Search space, Self-calibration operations, Dynamic convolution, Attention learning, Block design, Neural operation, Knowledge distillation},
abstract = {Existing neural architecture search (NAS) methods usually explore a limited feature-transformation-only search space, ignoring other advanced feature operations such as feature self-calibration by attention and dynamic convolutions. This disables the NAS algorithms to discover more advanced network architectures. We address this limitation by additionally exploiting feature self-calibration operations, resulting in a heterogeneous search space. To solve the challenges of operation heterogeneity and significantly larger search space, we formulate a neural operator search (NOS) method. NOS presents a novel heterogeneous residual block for integrating the heterogeneous operations in a unified structure, and an attention guided search strategy for facilitating the search process over a vast space. Extensive experiments show that NOS can search novel cell architectures with highly competitive performance on the CIFAR and ImageNet benchmarks.}
}
@article{XIE2023109192,
title = {Laplacian Lp norm least squares twin support vector machine},
journal = {Pattern Recognition},
volume = {136},
pages = {109192},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109192},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006719},
author = {Xijiong Xie and Feixiang Sun and Jiangbo Qian and Lijun Guo and Rong Zhang and Xulun Ye and Zhijin Wang},
keywords = {Semi-supervised learning, Laplacian Lp norm least squares twin support vector machine, Lp norm graph regularization, Geometric information},
abstract = {Semi-supervised learning has become a hot learning framework, where large amounts of unlabeled data and small amounts of labeled data are available during the training process. The recently proposed Laplacian least squares twin support vector machine (Lap-LSTSVM) is an excellent tool to solve the semi-supervised classification problem. Motivated by the success of Lap-LSTSVM, in this paper, we propose a novel Laplacian Lp norm least squares twin support vector machine (Lap-LpLSTSVM). There are several advantages of our proposed method: (1) The performance of our proposed Lap-LpLSTSVM can be improved by the adjustability of the value of p. (2) The introduced Lp norm graph regularization term can efficiently exploit the geometric information embedded in the data. (3) An efficient iterative strategy is employed to solve the optimization problem. Besides, to demonstrate that our proposed method can make use of unlabeled data effectively, least squares twin support vector machine (LSTSVM) which only uses the same labeled data is used to compare with our proposed method. The experimental results on both synthetic and real-world datasets show that our proposed method outperforms other state-of-the-art methods and can also deal with noisy datasets.}
}
@article{SALAZAR2023109240,
title = {A proxy learning curve for the Bayes classifier},
journal = {Pattern Recognition},
volume = {136},
pages = {109240},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109240},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007191},
author = {Addisson Salazar and Luis Vergara and Enrique Vidal},
keywords = {Classification, Parameter learning, Sample size, Training set size, Probability of error},
abstract = {In this paper, a theoretical learning curve is derived for the multi-class Bayes classifier. This curve fits general multivariate parametric models of the class-conditional probability density. The derivation uses a proxy approach based on analyzing the convergence of a statistic which is proportional to the posterior probability of the true class. By doing so, the curve depends only on the training set size and on the dimension of the feature vector; it does not depend on the model parameters. Essentially, the learning curve provides an estimate of the reduction in the excess of the probability of error that can be obtained by increasing the training set size. This makes it attractive in order to deal with the practical problems of defining appropriate training set sizes.}
}
@article{YUAN2023109228,
title = {An effective CNN and Transformer complementary network for medical image segmentation},
journal = {Pattern Recognition},
volume = {136},
pages = {109228},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109228},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007075},
author = {Feiniu Yuan and Zhengxiao Zhang and Zhijun Fang},
keywords = {Transformer, Medical image segmentation, Feature complementary module, Cross-domain fusion, Convolutional Neural Network},
abstract = {The Transformer network was originally proposed for natural language processing. Due to its powerful representation ability for long-range dependency, it has been extended for vision tasks in recent years. To fully utilize the advantages of Transformers and Convolutional Neural Networks (CNNs), we propose a CNN and Transformer Complementary Network (CTCNet) for medical image segmentation. We first design two encoders by Swin Transformers and Residual CNNs to produce complementary features in Transformer and CNN domains, respectively. Then we cross-wisely concatenate these complementary features to propose a Cross-domain Fusion Block (CFB) for effectively blending them. In addition, we compute the correlation between features from the CNN and Transformer domains, and apply channel attention to the self-attention features by Transformers for capturing dual attention information. We incorporate cross-domain fusion, feature correlation and dual attention together to propose a Feature Complementary Module (FCM) for improving the representation ability of features. Finally, we design a Swin Transformer decoder to further improve the representation ability of long-range dependencies, and propose to use skip connections between the Transformer decoded features and the complementary features for extracting spatial details, contextual semantics and long-range information. Skip connections are performed in different levels for enhancing multi-scale invariance. Experimental results show that our CTCNet significantly surpasses the state-of-the-art image segmentation models based on CNNs, Transformers, and even Transformer and CNN combined models designed for medical image segmentation. It achieves superior performance on different medical applications, including multi-organ segmentation and cardiac segmentation.}
}
@article{PENG2023109244,
title = {DIODE: Dilatable Incremental Object Detection},
journal = {Pattern Recognition},
volume = {136},
pages = {109244},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109244},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007233},
author = {Can Peng and Kun Zhao and Sam Maksoud and Tianren Wang and Brian C. Lovell},
keywords = {Incremental learning, Object detection},
abstract = {To accommodate rapid changes in the real world, the cognition system of humans is capable of continually learning concepts. On the contrary, conventional deep learning models lack this capability of preserving previously learned knowledge. When a neural network is fine-tuned to learn new tasks, its performance on previously trained tasks will significantly deteriorate. Many recent works on incremental object detection tackle this problem by introducing advanced regularization. Although these methods have shown promising results, the benefits are often short-lived after the first incremental step. Under multi-step incremental learning, the trade-off between old knowledge preserving and new task learning becomes progressively more severe. Thus, the performance of regularization-based incremental object detectors gradually decays for subsequent learning steps. In this paper, we aim to alleviate this performance decay on multi-step incremental detection tasks by proposing a dilatable incremental object detector (DIODE). For the task-shared parameters, our method adaptively penalizes the changes of important weights for previous tasks. At the same time, the structure of the model is dilated or expanded by a limited number of task-specific parameters to promote new task learning. Extensive experiments on PASCAL VOC and COCO datasets demonstrate substantial improvements over the state-of-the-art methods. Notably, compared with the state-of-the-art methods, our method achieves up to 6.0% performance improvement by increasing the number of parameters by just 1.2% for each newly learned task.}
}
@article{LABER2023109239,
title = {Shallow decision trees for explainable k-means clustering},
journal = {Pattern Recognition},
volume = {137},
pages = {109239},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109239},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200718X},
author = {Eduardo Laber and Lucas Murtinho and Felipe Oliveira},
keywords = {Clustering, Explainability, K-means, Decision trees},
abstract = {A number of recent works have employed decision trees for the construction of explainable partitions that aim to minimize the k-means cost function. These works, however, largely ignore metrics related to the depths of the leaves in the resulting tree, which is perhaps surprising considering how the explainability of a decision tree depends on these depths. To fill this gap in the literature, we propose an efficient algorithm with a penalty term in its loss function to favor the construction of shallow decision trees – i.e., trees whose leaves are not very deep, which translate to clusters that are defined by a small number of attributes and are therefore easier to explain. In experiments on 16 datasets, our algorithm yields better results than decision-tree clustering algorithms recently presented in the literature, typically achieving lower or equivalent costs with considerably shallower trees.}
}
@article{ZHAO2023109199,
title = {The neglected background cues can facilitate finger vein recognition},
journal = {Pattern Recognition},
volume = {136},
pages = {109199},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109199},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006781},
author = {Pengyang Zhao and Shuping Zhao and Jing-Hao Xue and Wenming Yang and Qingmin Liao},
keywords = {Finger vein recognition, Vein trait, Background cue, Intensity orientation vector, Binary feature learning},
abstract = {Recently, finger vein based biometric authentication has attracted considerable attention due to its high efficiency and high security. However, most existing finger vein representation methods focus on vein traits while ignoring background cues, although background cues also convey identity information specific to each individual. In this paper, we leverage background intensity variations in finger vein images as new features to enrich discriminative representation, and accordingly propose a new descriptor named Intensity Orientation Vector (IOV). IOV, scaleable to reflect characteristics of finger tissues, offers additional informative cues for finger vein representation. Furthermore, we propose a new learning scheme named Semantic Similarity Preserved Discrete Binary Feature Learning (SSP-DBFL) for finger vein recognition. Unlike the most bimodal binary feature representation methods, SSP-DBFL preserves high-level semantic similarity in a common Hamming space to exploit the consensus between vein traits and background cues. Specifically, given a finger vein image, we first extract the direction difference vectors (DDV) as the main vein traits and the IOV as the auxiliary background cues. Subsequently, we jointly learn projection functions from these two types of features in a supervised manner, converting the two features into discriminative binary codes with their semantic similarity preserved. Finally, the binary codes are pooled into histogram-based vectors for finger vein representation. Extensive experiments are conducted on five widely used finger vein databases and demonstrate the effectiveness of our proposed IOV and SSP-DBFL.}
}
@article{HUANG2023109147,
title = {Face anti-spoofing using feature distilling and global attention learning},
journal = {Pattern Recognition},
volume = {135},
pages = {109147},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109147},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006264},
author = {Rui Huang and Xin Wang},
keywords = {Face anti-spoofing, Anti-interference, Multi-level distillation, Global spatial attention learning, Pixel-wise supervision},
abstract = {Face anti-spoofing (FAS) is essential to assure the security of face recognition systems. Recently, some deep learning based FAS methods have achieved promising results under intra-dataset testing. However, they often fail in generalizing to unseen attacks due to the failure of extracting intrinsic features from face images. In this paper, we propose an end-to-end FAS method which consists of an anti-interference feature distillation module, a global spatial attention learning module and a pyramid binary mask supervision module. The deep features from the pretrained ResNet34 network are first distilled at multiple levels to capture intrinsic information via removing interference of features. Then, the multi-level distilled features are further refined by using a global spatial learning mechanism. Finally, the pyramid pixel-wise supervision is assembled to boost performance. Extensive experimental results on five benchmark datasets show the superior performance of our proposed method on intra-dataset testing and on cross-dataset testing.}
}
@article{YANG2023109232,
title = {Detecting and grouping keypoints for multi-person pose estimation using instance-aware attention},
journal = {Pattern Recognition},
volume = {136},
pages = {109232},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109232},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007117},
author = {Sen Yang and Ze Feng and Zhicheng Wang and Yanjie Li and Shoukui Zhang and Zhibin Quan and Shu-tao Xia and Wankou Yang},
keywords = {Multi-person human pose estimation, Self-attention, Bottom-up, Transformer, Grouping, Keypoints association},
abstract = {Bottom-up human pose estimation models detect keypoints and learn associative information between keypoints, usually requiring human predefined offset fields or embeddings for keypoints grouping (clustering). In this paper, we present a brand new method that can entirely solve these problems based on Transformer, making the grouping process free of the human-defined associative signals. Specifically, the self-attention in vision Transformer measures feature similarity between any pair of locations, which provides a metric space to associate keypoints together into corresponding human instances. However, the naive attention patterns formed in Transformer are still not subjectively controlled, so there is no guarantee that the keypoints only attend to the instances to which they belong. To address it we propose a novel approach of supervising self-attention to be instance-aware, simultaneously accomplishing multi-person keypoint detection and clustering. By doing so, we can group the detected keypoints to their corresponding instances, according to the pairwise attention scores. An additional benefit of our method is that the instance segmentation results of any number of people can be directly obtained from the supervised attention matrix, thereby simplifying the pixel assignment pipeline. The qualitative and quantitative results on the COCO shows that, with a very simple architecture design, our method can achieve comparable performance against the CNN-based bottom-up counterparts with fewer parameters, which also demonstrate a promising way to control self-attention mechanism behavior for specific purposes.}
}
@article{WU2023109187,
title = {Pure graph-guided multi-view subspace clustering},
journal = {Pattern Recognition},
volume = {136},
pages = {109187},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109187},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006665},
author = {Hongjie Wu and Shudong Huang and Chenwei Tang and Yancheng Zhang and Jiancheng Lv},
keywords = {Multi-view learning, Subspace clustering, Graph learning, Pure graph},
abstract = {Multi-view subspace clustering approaches have shown outstanding performance in revealing similarity relationships and complex structures hidden in data. Despite the progress, previous multi-view clustering methods still face two challenges: (1) it is difficult to simultaneously achieve sparsity and connectivity of the affinity graph; (2) existing methods usually separate the graph learning step from the clustering process, which leads to unsatisfactory clustering performance as the final results critically rely on the learned graph. In this paper, we propose to achieve a structured consensus graph for multi-view subspace clustering by leveraging the sparsity and connectivity of each affinity graph. In the proposed method, the pure graph for each view is searched by finding the good neighbors. The multiple pure graphs are further fused into a consensus graph with a block-diagonal structure. That is, the consensus graph is enforced to contain exactly c connected components where c is the number of the clusters. Hence the label to each sample can be directly assigned since each connected component precisely corresponds to an individual cluster. As a result, the proposed model seamlessly accomplishes the subtasks including graph construction, pure graph learning (i.e., good neighbors searching), and cluster label allocation in a mutual reinforcement manner. Extensive experimental results demonstrate the superiority and reliability of our proposed method.}
}
@article{ZHOU2023109203,
title = {Feature learning network with transformer for multi-label image classification},
journal = {Pattern Recognition},
volume = {136},
pages = {109203},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109203},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006823},
author = {Wei Zhou and Peng Dou and Tao Su and Haifeng Hu and Zhijie Zheng},
keywords = {Multi-label classification, Transformer, Multi-scale features, Spatial attention, Salient features, Feature suppression},
abstract = {The purpose of multi-label image classification task is to accurately assign a set of labels to the objects in images. Although promising results have been achieved, most of the existing methods cannot effectively learn multi-scale features, so it is difficult to identify small-scale objects from images. Besides, current attention-based methods tend to learn the most salient feature regions in images, but fail to excavate various potential useful features concealed by the most salient feature, thus limiting the further improvement of model performance. To address above issues, we propose a novel Feature Learning network based on Transformer to learn salient features and excavate potential useful features (FL-Tran). Specifically, in order to solve the problem that current methods are difficult to identify small-scale objects, we first present a novel multi-scale fusion module (MSFM) to align high-level features and low-level features to learn multi-scale features. Additionally, a spatial attention module (SAM) utilizing transformer encoder is introduced to capture salient object features in images to enhance the model performance. Furthermore, we devise a feature enhancement and suppression module (FESM) with the aim of excavating potential useful features concealed by the most salient features. By suppressing the most salient features obtained in current SAM layer, and then forcing subsequent SAM layer to excavate potential salient features in feature maps, FL-Tran model can learn various useful features more comprehensively. Extensive experiments on MS-COCO 2014, PASCAL VOC 2007, and NUS-WIDE datasets demonstrate that our proposed FL-Tran model outperforms current state-of-the-art methods.}
}
@article{SONG2023109198,
title = {Prior depth-based multi-view stereo network for online 3D model reconstruction},
journal = {Pattern Recognition},
volume = {136},
pages = {109198},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109198},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200677X},
author = {Soohwan Song and Khang Giang Truong and Daekyum Kim and Sungho Jo},
keywords = {Multi-view stereo, Deep learning, Online 3D reconstruction},
abstract = {This study addresses the online multi-view stereo (MVS) problem when reconstructing precise 3D models in real time. To solve this problem, most previous studies adopted a motion stereo approach that sequentially estimates depth maps from multiple localized images captured in a local time window. To compute the depth maps quickly, the motion stereo methods process down-sampled images or use a simplified algorithm for cost volume regularization; therefore, they generally produce reconstructed 3D models that are inaccurate. In this paper, we propose a novel online MVS method that accurately reconstructs high-resolution 3D models. This method infers prior depth information based on sequentially estimated depths and leverages it to estimate depth maps more precisely. The method constructs a cost volume by using the prior-depth-based visibility information and then fuses the prior depths into the cost volume. This approach significantly improves the stereo matching performance and completeness of the estimated depths. Extensive experiments showed that the proposed method outperforms other state-of-the-art MVS and motion stereo methods. In particular, it significantly improves the completeness of 3D models.}
}
@article{LIU2023109191,
title = {Automated lesion segmentation in fundus images with many-to-many reassembly of features},
journal = {Pattern Recognition},
volume = {136},
pages = {109191},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109191},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006707},
author = {Qing Liu and Haotian Liu and Wei Ke and Yixiong Liang},
keywords = {Feature reassembly, Upsampling operator, Downsampling operator, Lesion segmentation, Fundus image analysis},
abstract = {Existing CNN-based segmentation approaches have achieved remarkable progresses on segmenting objects in regular sizes. However, when migrating them to segment tiny retinal lesions, they encounter challenges. The feature reassembly operators that they adopt are prone to discard the subtle activations about tiny lesions and fail to capture long-term dependencies. This paper aims to solve these issues and proposes a novel Many-to-Many Reassembly of Features (M2MRF) for tiny lesion segmentation. Our proposed M2MRF reassembles features in a dimension-reduced feature space and simultaneously aggregates multiple features inside a large predefined region into multiple output features. In this way, subtle activations about small lesions can be maintained as much as possible and long-term spatial dependencies can be captured to further enhance the lesion features. Experimental results on two lesion segmentation benchmarks, i.e., DDR and IDRiD, show that 1) our M2MRF outperforms existing feature reassembly operators, and 2) equipped with our M2MRF, the HRNetV2 is able to achieve substantially better performances and generalisation ability than existing methods. Our code is made publicly available at https://github.com/CVIU-CSU/M2MRF-Lesion-Segmentation.}
}