@article{WANG2023109460,
title = {Skeleton estimation of directed acyclic graphs using partial least squares from correlated data},
journal = {Pattern Recognition},
volume = {139},
pages = {109460},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109460},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001607},
author = {Xiaokang Wang and Shan Lu and Rui Zhou and Huiwen Wang},
keywords = {Directed acyclic graph, partial least squares, hierarchical clustering, sparse learning},
abstract = {Directed acyclic graphs (DAGs) are directed graphical models that are well known for discovering causal relationships between variables in a high-dimensional setting. When the DAG is not identifiable due to the lack of interventional data, the skeleton can be estimated using observational data, which is formed by removing the direction of the edges in a DAG. In real data analyses, variables are often highly correlated due to some form of clustered sampling, and ignoring this correlation will inflate the standard errors of the parameter estimates in the regression-based DAG structure learning framework. In this work, we propose a two-stage DAG skeleton estimation approach for highly correlated data. First, we propose a novel neighborhood selection method based on sparse partial least squares (PLS) regression, and a cluster-weighted adaptive penalty is imposed on the PLS weight vectors to exploit the local information. In the second stage, the DAG skeleton is estimated by evaluating a set of conditional independence hypotheses. Simulation studies are presented to demonstrate the effectiveness of the proposed method. The algorithm is also tested on publicly available datasets, and we show that our algorithm obtains higher sensitivity with comparable false discovery rates for high-dimensional data under different network structures.}
}
@article{NIKPOUR2023109428,
title = {Spatio-temporal hard attention learning for skeleton-based activity recognition},
journal = {Pattern Recognition},
volume = {139},
pages = {109428},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109428},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001292},
author = {Bahareh Nikpour and Narges Armanfard},
keywords = {Temporal attention, Spatial attention, Spatio-temporal attention, Activity recognition, Skeleton data, Deep reinforcement learning},
abstract = {The use of skeleton data for activity recognition has become prevalent due to its advantages over RGB data. A skeleton video includes frames showing two- or three-dimensional coordinates of human body joints. For recognizing an activity, not all the video frames are informative, and only a few key frames can well represent an activity. Moreover, not all joints participate in every activity; i.e., the key joints may vary across frames and activities. In this paper, we propose a novel framework for finding temporal and spatial attentions in a cooperative manner for activity recognition. The proposed method, which is called STH-DRL, consists of a temporal agent and a spatial agent. The temporal agent is responsible for finding the key frames, i.e., temporal hard attention finding, and the spatial agent attempts to find the key joints, i.e., spatial hard attention finding. We formulate the search problems as Markov decision processes and train both agents through interacting with each other using deep reinforcement learning. Experimental results on three widely used activity recognition benchmark datasets demonstrate the effectiveness of our proposed method.}
}
@article{PARK2023109387,
title = {Elucidating robust learning with uncertainty-aware corruption pattern estimation},
journal = {Pattern Recognition},
volume = {138},
pages = {109387},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109387},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000882},
author = {Jeongeun Park and Seungyoun Shin and Sangheum Hwang and Sungjoon Choi},
keywords = {Robust learning, Training with noisy labels, Uncertainty estimation, Corruption pattern estimation},
abstract = {Robust learning methods aim to learn a clean target distribution from noisy and corrupted training data where a specific corruption pattern is often assumed a priori. Our proposed method can not only successfully learn the clean target distribution from a dirty dataset but also can estimate the underlying noise pattern. To this end, we leverage a mixture-of-experts model that can distinguish two different types of predictive uncertainty, aleatoric and epistemic uncertainty. We show that the ability to estimate the uncertainty plays a significant role in elucidating the corruption patterns as these two objectives are tightly intertwined. We also present a novel validation scheme for evaluating the performance of the corruption pattern estimation. Our proposed method is extensively assessed in terms of both robustness and corruption pattern estimation in the computer vision domain. Code has been made publicly available at https://github.com/jeongeun980906/Uncertainty-Aware-Robust-Learning.}
}
@article{YAN2023109432,
title = {3D Medical image segmentation using parallel transformers},
journal = {Pattern Recognition},
volume = {138},
pages = {109432},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109432},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001334},
author = {Qingsen Yan and Shengqiang Liu and Songhua Xu and Caixia Dong and Zongfang Li and Javen Qinfeng Shi and Yanning Zhang and Duwei Dai},
keywords = {3D Medical image segmentation, Deep learning, Transformers, Attention, Fusion, High-resolution representations, Low-resolution representations},
abstract = {Most recent 3D medical image segmentation methods adopt convolutional neural networks (CNNs) that rely on deep feature representation and achieve adequate performance. However, due to the convolutional architectures having limited receptive fields, they cannot explicitly model the long-range dependencies in the medical image. Recently, Transformer can benefit from global dependencies using self-attention mechanisms and learn highly expressive representations. Some works were designed based on the Transformers, but the existing Transformers suffer from extreme computational and memories, and they cannot take full advantage of the powerful feature representations in 3D medical image segmentation. In this paper, we aim to connect the different resolution streams in parallel and propose a novel network, named Transformer based High Resolution Network (TransHRNet), with an Effective Transformer (EffTrans) block, which has sufficient feature representation even at high feature resolutions. Given a 3D image, the encoder first utilizes CNN to extract the feature representations to capture the local information, and then the different feature maps are reshaped elaborately for tokens that are fed into each Transformer stream in parallel to learn the global information and repeatedly exchange the information across streams. Unfortunately, the proposed framework based on the standard Transformer needs a huge amount of computation, thus we introduce a deep and effective Transformer to deliver better performance with fewer parameters. The proposed TransHRNet is evaluated on the Multi-Atlas Labeling Beyond the Cranial Vault (BCV) dataset that consists of 11 major human organs and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Experimental results show that it performs better than the convolutional and other related Transformer-based methods on the 3D multi-organ segmentation tasks. Code is available at https://github.com/duweidai/TransHRNet.}
}
@article{ZHAO2023109445,
title = {A gradient optimization and manifold preserving based binary neural network for point cloud},
journal = {Pattern Recognition},
volume = {139},
pages = {109445},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109445},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001450},
author = {Zhi Zhao and Ke Xu and Yanxin Ma and Jianwei Wan},
keywords = {Binarization, Gradient optimization, Manifold, Point cloud},
abstract = {With significant progress of deep learning on 3D point cloud, the demand for deployment of point cloud neural network on the edge devices is growing. Binary neural network, a type of quantization compression method, with extreme low bit and fast inference speed, attracts more attention. It is more challenging, but has greater potentiality. Most of the researches on binary networks focus on images rather than point cloud. Considering the particularity of point cloud neural network, this paper presents a novel binarization framework, which includes two main contributions. Firstly, a gradient optimization method is proposed to overcome the shortcomings of Straight Through Estimator (STE) commonly used in the back propagation of binary network training. Secondly, based on the analysis of manifold distortion caused by the binary convolution and pooling operations, we propose an optimized scaling recovery method to restore manifold for the convoluted feature, and also, a pooling correction method to improve the pooled feature's fidelity. Manifold distortion leads to the severe feature homogeneity problem, which brings trouble in generating features with sufficient discrimination for classification and segmentation. The manifold preserving optimizations are designed to introduce minimum extra parameters to balance the accuracy with the computation and storage consumption. Experiments show that the proposed method outperforms state-of-the-art in accuracy with ignored overhead, and also has good scalability.}
}
@article{ZHENG2023109469,
title = {Deep embedded clustering with distribution consistency preservation for attributed networks},
journal = {Pattern Recognition},
volume = {139},
pages = {109469},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109469},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001693},
author = {Yimei Zheng and Caiyan Jia and Jian Yu and Xuanya Li},
keywords = {Deep embedded clustering, Autoencoder, Graph autoencoder, Node representation learning, Cluster distribution consistency},
abstract = {Many complex systems in the real world can be characterized as attributed networks. To mine the potential information in these networks, deep embedded clustering, which obtains node representations and clusters simultaneously, has been given much attention in recent years. Under the assumption of consistency for data in different views, the cluster structure of network topology and that of node attributes should be consistent for an attributed network. However, many existing methods ignore this property, even though they separately encode node representations from network topology and node attributes and cluster nodes on representation vectors learned from one of the views. Therefore, in this study, we propose an end-to-end deep embedded clustering model for attributed networks. It utilizes graph autoencoder and node attribute autoencoder to learn node representations and cluster assignments. In addition, a distribution consistency constraint is introduced to maintain the latent consistency of cluster distributions in two views. Extensive experiments on several datasets demonstrate that the proposed model achieves significantly better or competitive performance compared with the state-of-the-art methods. The source code can be found at https://github.com/Zhengymm/DCP.}
}
@article{LIN2023109416,
title = {Cycle-object consistency for image-to-image domain adaptation},
journal = {Pattern Recognition},
volume = {138},
pages = {109416},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109416},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001176},
author = {Che-Tsung Lin and Jie-Long Kew and Chee Seng Chan and Shang-Hong Lai and Christopher Zach},
keywords = {Generative adversarial networks, Instance-aware image-translation, Domain adaptation, Cross-domain object detection},
abstract = {Recent advances in generative adversarial networks (GANs) have been proven effective in performing domain adaptation for object detectors through data augmentation. While GANs are exceptionally successful, those methods that can preserve objects well in the image-to-image translation task usually require an auxiliary task, such as semantic segmentation to prevent the image content from being too distorted. However, pixel-level annotations are difficult to obtain in practice. Alternatively, instance-aware image-translation model treats object instances and background separately. Yet, it requires object detectors at test time, assuming that off-the-shelf detectors work well in both domains. In this work, we present AugGAN-Det, which introduces Cycle-object Consistency (CoCo) loss to generate instance-aware translated images across complex domains. The object detector of the target domain is directly leveraged in generator training and guides the preserved objects in the translated images to carry target-domain appearances. Compared to previous models, which e.g., require pixel-level semantic segmentation to force the latent distribution to be object-preserving, this work only needs bounding box annotations which are significantly easier to acquire. Next, as to the instance-aware GAN models, our model, AugGAN-Det, internalizes global and object style-transfer without explicitly aligning the instance features. Most importantly, a detector is not required at test time. Experimental results demonstrate that our model outperforms recent object-preserving and instance-level models and achieves state-of-the-art detection accuracy and visual perceptual quality.}
}
@article{ASIF2023109484,
title = {DeepActsNet: A deep ensemble framework combining features from face, hands, and body for action recognition},
journal = {Pattern Recognition},
volume = {139},
pages = {109484},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109484},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300184X},
author = {Umar Asif and Deval Mehta and Stefan {Von Cavallar} and Jianbin Tang and Stefan Harrer},
keywords = {Activity recognition, Convolutional neural networks, Deep learning},
abstract = {Human action recognition from videos has gained substantial focus due to its wide applications in the field of video understanding. Most of the existing approaches extract human skeleton data from videos to encode actions because of the invariance nature of the skeleton information with respect to lightning conditions and background changes. Despite their success in achieving high recognition accuracy, methods based on limited body joints fail to capture the nuances of subtle body parts which are highly relevant for discriminating similar actions. In this paper, we overcome this limitation by presenting a holistic framework for combining spatial and motion features from the body, face, and hands to develop a novel data representation termed “Deep Actions Stamps (DeepActs)” for video-based action recognition. Compared to the skeleton sequences based on limited body joints, DeepActs encode more effective spatio-temporal features that provide robustness against pose estimation noises and improve action recognition accuracy. We also present “DeepActsNet”, a deep learning based ensemble model which learns convolutional and structural features from Deep Action Stamps for highly accurate action recognition. Experiments on three challenging action recognition datasets (NTU60, NTU120, and SYSU) show that the proposed model produces significant improvements in the action recognition accuracy with less computational cost compared to the state-of-the-art methods.}
}
@article{KONG2023109473,
title = {Flexible model weighting for one-dependence estimators based on point-wise independence analysis},
journal = {Pattern Recognition},
volume = {139},
pages = {109473},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109473},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001735},
author = {He Kong and Limin Wang},
keywords = {Point-wise independence analysis, Independence assumption, Point-wise log likelihood, Weighted one-dependence estimators},
abstract = {Recent studies have shown that Bayesian network classifiers (BNCs) are powerful tools for knowledge representation and classification, and averaged one-dependence estimators (AODE) is one of the most popular and effective BNCs since it can achieve the tradeoff between bias and variance due to its independence assumptions and ensemble learning strategy. However, unverified independence assumptions may result in biased estimates of probability distribution and then degradation in classification performance. In this paper, we prove theoretically the uncertainty of probability-theoretic independence and propose to measure the independence between attribute values implicated in specific instance. The estimates of conditional probability can be finely tuned based on point-wise independence analysis. Point-wise log likelihood function is then applied as weighting metric for committee members of AODE to improve the estimate of joint probability. Extensive experiments on 36 benchmark datasets show that, compared to other state-of-the-art classifiers, weighted one-dependence estimators using point-wise independence analysis can achieve competitive classification performance in terms of zero-one loss, RMSE, bias-variance decomposition and conditional log likelihood.}
}
@article{MA2023109420,
title = {Towards local visual modeling for image captioning},
journal = {Pattern Recognition},
volume = {138},
pages = {109420},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109420},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001218},
author = {Yiwei Ma and Jiayi Ji and Xiaoshuai Sun and Yiyi Zhou and Rongrong Ji},
keywords = {Image captioning, Attention mechanism, Local visual modeling},
abstract = {In this paper, we study the local visual modeling with grid features for image captioning, which is critical for generating accurate and detailed captions. To achieve this target, we propose a Locality-Sensitive Transformer Network (LSTNet) with two novel designs, namely Locality-Sensitive Attention (LSA) and Locality-Sensitive Fusion (LSF). LSA is deployed for the intra-layer interaction in Transformer via modeling the relationship between each grid and its neighbors. It reduces the difficulty of local object recognition during captioning. LSF is used for inter-layer information fusion, which aggregates the information of different encoder layers for cross-layer semantical complementarity. With these two novel designs, the proposed LSTNet can model the local visual information of grid features to improve the captioning quality. To validate LSTNet, we conduct extensive experiments on the competitive MS-COCO benchmark. The experimental results show that LSTNet is not only capable of local visual modeling, but also outperforms a bunch of state-of-the-art captioning models on offline and online testings, i.e., 134.8 CIDEr and 136.3 CIDEr, respectively. Besides, the generalization of LSTNet is also verified on the Flickr8k and Flickr30k datasets. The source code is available on GitHub: https://www.github.com/xmu-xiaoma666/LSTNet.}
}
@article{QIU2023109497,
title = {Weakly-supervised pre-training for 3D human pose estimation via perspective knowledge},
journal = {Pattern Recognition},
volume = {139},
pages = {109497},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109497},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001978},
author = {Zhongwei Qiu and Kai Qiu and Jianlong Fu and Dongmei Fu},
keywords = {Human pose estimation, Pre-training, Relative depth, Weakly-supervised},
abstract = {Modern deep learning-based 3D pose estimation approaches require plenty of 3D pose annotations. However, existing 3D datasets lack diversity, which limits the performance of current methods and their generalization ability. Although existing methods utilize 2D pose annotations to help 3D pose estimation, they mainly focus on extracting 2D structural constraints from 2D poses, ignoring the 3D information hidden in the images. In this paper, we propose a novel method to extract weak 3D information directly from 2D images without 3D pose supervision. Firstly, we utilize 2D pose annotations and perspective prior knowledge to generate the relative depth of human joints. Then, we collect a 2D pose dataset (MCPC) and generate relative depth labels. Based on MCPC, we propose a weakly-supervised pre-training (WSP) strategy to distinguish the depth relationship between two points in an image. WSP enables the learning of the relative depth of two keypoints on lots of in-the-wild images, which is more capable of predicting depth and generalization ability for 3D human pose estimation. After fine-tuning the pose model on 3D pose datasets, WSP achieves state-of-the-art results on two widely-used benchmarks.}
}
@article{UMATANI2023109375,
title = {Time series clustering with an EM algorithm for mixtures of linear Gaussian state space models},
journal = {Pattern Recognition},
volume = {138},
pages = {109375},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109375},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000766},
author = {Ryohei Umatani and Takashi Imai and Kaoru Kawamoto and Shutaro Kunimasa},
keywords = {Time series clustering, Model-based clustering, State space model, EM algorithm, Mixture model},
abstract = {In this paper, we consider the task of clustering a set of individual time series while modeling each cluster, that is, model-based time series clustering. The task requires a parametric model with sufficient flexibility to describe the dynamics in various time series. To address this problem, we propose a novel model-based time series clustering method with mixtures of linear Gaussian state space models, which have high flexibility. The proposed method uses a new expectation-maximization algorithm for the mixture model to estimate the model parameters, and determines the number of clusters using the Bayesian information criterion. Experiments on a simulated dataset demonstrate the effectiveness of the method in clustering, parameter estimation, and model selection. The method is applied to real datasets commonly used to evaluate time series clustering methods. Results showed that the proposed method produces clustering results that are as accurate or more accurate than those obtained using previous methods.}
}
@article{MOORTHY2023109457,
title = {Adaptive spatial-temporal surrounding-aware correlation filter tracking via ensemble learning},
journal = {Pattern Recognition},
volume = {139},
pages = {109457},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109457},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001577},
author = {Sathishkumar Moorthy and Young Hoon Joo},
keywords = {Surrounding information, Object tracking, Feature fusion, Correlation filters framework, Selective spatial regularization},
abstract = {With the advancement of computer vision, object trackers based on discriminative correlation filters (DCF) have demonstrated superior performance and accuracy compared to traditional trackers. However, most existing DCF-based trackers are easily affected by various factors, such as cluttered background, illumination variations, occlusions, rotations etc. Therefore, in order to accurately track the target, further investigation into the characteristics of the correlation filter is required. In this study, we propose an adaptive spatial-temporal surrounding-aware correlation filter tracker via the ensemble learning (ASTSAELT) technique. Specifically, the adaptive spatial-temporal regularized correlation filter to remove the boundary effects and temporal degradation is presented. And then, a method of extracting surrounding samples according to the size and shape of the tracking object, designed to preserve the integrity of the object, is proposed. Moreover, our tracker utilizes a multi-expert tracking framework to improve its performance by integrating both handcrafted features and deep convolutional layer features. And then, the update strategy is proposed to measure the reliability of the current tracking result and mitigate model corruption. Finally, numerous experiments on visual tracking benchmarks including OTB2013, OTB2015, TempleColor128, UAV123, UAVDT and DTB70 are implemented to verify the developed method achieves superior performance compared with several state-of-the-art methods.}
}
@article{SORIA2023109461,
title = {Dense extreme inception network for edge detection},
journal = {Pattern Recognition},
volume = {139},
pages = {109461},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109461},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001619},
author = {Xavier Soria and Angel Sappa and Patricio Humanante and Arash Akbarinia},
keywords = {Edge detection, Deep learning, CNN, Contour detection, Boundary detection, Segmentation},
abstract = {Edge detection is the basis of many computer vision applications. State of the art predominantly relies on deep learning with two decisive factors: dataset content and network architecture. Most of the publicly available datasets are not curated for edge detection tasks. Here, we address this limitation. First, we argue that edges, contours and boundaries, despite their overlaps, are three distinct visual features requiring separate benchmark datasets. To this end, we present a new dataset of edges. Second, we propose a novel architecture, termed Dense Extreme Inception Network for Edge Detection (DexiNed), that can be trained from scratch without any pre-trained weights. DexiNed outperforms other algorithms in the presented dataset. It also generalizes well to other datasets without any fine-tuning. The higher quality of DexiNed is also perceptually evident thanks to the sharper and finer edges it outputs.}
}
@article{LU2023109405,
title = {Robust weighted co-clustering with global and local discrimination},
journal = {Pattern Recognition},
volume = {138},
pages = {109405},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109405},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001061},
author = {Zhoumin Lu and Shiping Wang and Genggeng Liu and Feiping Nie},
keywords = {Machine learning, Co-clustering, Nonnegative matrix factorization, Global discrimination, Local discrimination},
abstract = {In the past few decades, the clustering problem has made considerable progress, and co-clustering algorithms have attracted more attention. Compared with one-side clustering, co-clustering not only groups samples according to the distribution of features but also groups features according to the distribution of samples at the same time. This duality helps to explore the structural information of data, such as genes and texts. In this paper, a new co-clustering algorithm is proposed to simultaneously consider feature weights, data noise, local manifolds, and global scatter, named robust weighted co-clustering with global and local discrimination. Furthermore, an alternate update rule is put forward to optimize objective, theoretically proven to converge. Then, the algorithm’s duality, robustness, and effectiveness have been verified on synthetic, corrupted, and real datasets, respectively. The runtime and parameter sensitivity of the algorithm are also analyzed. Finally, sufficient experiments clarify the competitiveness of our algorithm compared to other ones.}
}
@article{2024110523,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {152},
pages = {110523},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(24)00274-7},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002747}
}
@article{HU2023109404,
title = {An Effective and Adaptable K-means Algorithm for Big Data Cluster Analysis},
journal = {Pattern Recognition},
volume = {139},
pages = {109404},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109404},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300105X},
author = {Haize Hu and Jianxun Liu and Xiangping Zhang and Mengge Fang},
keywords = {-means algorithm, Local optimization, Lévy flight, Global search, Clustering centroids},
abstract = {Tradition K-means clustering algorithm is easy to fall into local optimum, poor clustering effect on large capacity data and uneven distribution of clustering centroids. To solve these problems, a novel k-means clustering algorithm based on Lévy flight trajectory (Lk-means) is proposed in the paper. In the iterative process of LK-means algorithm, Lévy flight is used to search new positions to avoid premature convergence in clustering. It is also applied to increase the diversity of the cluster, strengthen the global search ability of K-means algorithm, and avoid falling into the local optimal value too early. Nevertheless, the complexity of hybrid algorithm is not increased in the process of Lévy flight optimization. To verify the data clustering effect of LK-means algorithm, experiments are conducted to compare it with the k-means algorithm, XK-means algorithm, DDKmeans algorithm and Canopyk-means algorithm on 10 open source data sets. The results show that LK-means algorithm has better search results and more evenly distributed cluster centroids, which greatly improves the global search ability, big data processing ability and uneven distribution centroids of cluster of K-means algorithm.}
}
@article{ZHANG2023109462,
title = {Deep collaborative graph hashing for discriminative image retrieval},
journal = {Pattern Recognition},
volume = {139},
pages = {109462},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109462},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001620},
author = {Zheng Zhang and Jianning Wang and Lei Zhu and Yadan Luo and Guangming Lu},
keywords = {Deep hashing, Image retrieval, Collaborative learning, Graph convolutional hashing, Semantic encoding},
abstract = {The most striking success of deep hashing for large-scale image retrieval benefits from its powerful discriminative representation of deep learning and the attractive computational efficiency of compact hash code learning. Most existing deep semantic-preserving hashing regard the available semantic labels as the ground truth for classification or transform them into prevalent pairwise similarities. However, such strategies fail to capture the interactive correlations between the visual semantics embedded in images and the given category-level labels. Moreover, they utilize the fixed piecewise or pairwise semantics as the optimization objectives, which suffers from the limited flexibility on semantic representation and adaptive knowledge communication in hash code learning. In this paper, we propose a novel Deep Collaborative Graph Hashing (DCGH), which collectively considers multi-level semantic embeddings, latent common space construction, and intrinsic structure mining in discriminative hash codes learning, for large-scale image retrieval. To the best of our knowledge, this is the first collaborative graph hashing for image retrieval. Specifically, instead of using the conventional single-flow visual network architecture, we design a dual-stream feature encoding network to jointly explore the multi-level semantic information across visual and semantic features. Moreover, a well-established shared latent space is constructed based on space reconstruction to explore the concurrent information and bridge the semantic gap between visual and semantic space. Furthermore, a graph convolutional network is introduced to preserve the latent structural relations in the optimal pairwise similarity-preserving hash codes. The whole learning framework is optimized in an end-to-end fashion. Extensive experiments on different datasets demonstrate that our DCGH can achieve superb image retrieval performance against state-of-the-art supervised hashing methods. The source codes of the proposed DCGH are available at https://github.com/JalinWang/DCGH .}
}
@article{JIA2023109388,
title = {Global and local structure preserving nonnegative subspace clustering},
journal = {Pattern Recognition},
volume = {138},
pages = {109388},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109388},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000894},
author = {Hongjie Jia and Dongxia Zhu and Longxia Huang and Qirong Mao and Liangjun Wang and Heping Song},
keywords = {Subspace clustering, Global structure, Local structure, Nonnegative Lagrangian relaxation, Kernel clustering},
abstract = {Most subspace clustering methods construct the similarity matrix based on self-expressive property and apply the spectral relaxation on the similarity matrix to get the final clusters. Despite the advantages of this framework, it has two limitations that are easily ignored. Firstly, the original self-expressive model only considers the global structure of data, and the ubiquitous local structure among data is not paid enough attention. Secondly, spectral relaxation is naturally suitable for 2-way clustering tasks, but when dealing with multi-way clustering tasks, the assignment of cluster members becomes indirect and requires additional steps. To overcome these problems, this paper proposes a global and local structure preserving nonnegative subspace clustering method, which learns data similarities and cluster indicators in a mutually enhanced way within a unified framework. Besides, the model is extended to kernel space to strengthen its capability of dealing with nonlinear data structures. For optimizing the objective function of the method, multiplicative updating rules based on nonnegative Lagrangian relaxation are developed, and the convergence is guaranteed in theory. Abundant experiments have shown that the proposed model is better than many advanced clustering methods in most cases.}
}
@article{BADEA2023109417,
title = {Timid semi–supervised learning for face expression analysis},
journal = {Pattern Recognition},
volume = {138},
pages = {109417},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109417},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001188},
author = {Mihai Badea and Corneliu Florea and Andrei Racoviţeanu and Laura Florea and Constantin Vertan},
keywords = {Face expression, Action units, Semi–supervised learning, Diversity},
abstract = {In the last years, semi–supervised learning has been proposed as a strategy with high potential for improving machine learning capabilities. Face expression recognition may highly benefit from such a technique, as accurate labeling is both difficult and costly, whereas millions of unlabeled images with human faces are available on the Internet, but without annotations. In this paper we evaluate the benefits of semi–supervised learning in the practical scenarios of face expression analysis. Our conclusion is that better performance is indeed achievable, but by methods that put a distinct emphasis on the diversity of exploring patterns in the unlabeled data domain. The evaluation is carried on multiple tasks such as detecting Action Units on EmotioNet, assessing Action Units intensity on the spontaneous DISFA database and, respectively, recognizing expressions on static images acquired in the wild, from the RAF-DB and FER+ databases. We show that, in these scenarios, a so–called timid semi–supervised learner is more robust and achieves higher performance than standard, confident semi–supervised learners.}
}
@article{YAN2023109446,
title = {A bi-level metric learning framework via self-paced learning weighting},
journal = {Pattern Recognition},
volume = {139},
pages = {109446},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109446},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001462},
author = {Jing Yan and Wei Wei and Xinyao Guo and Chuangyin Dang and Jiye Liang},
keywords = {Metric learning, Self-paced learning, Adaptive neighborhood, Weighting tuples},
abstract = {Distance metric learning (DML) has achieved great success in many real-world applications. However, most existing DML models characterize the quality of tuples on the tuple level while ignoring the anchor level. Therefore, the models are less accurate to portray the quality of tuples and tend to be over-fitting when anchors are noisy samples. In this paper, we devise a bi-level metric learning framework (BMLF), which characterizes the quality of tuples more finely on both levels, enhancing the generalization performance of the DML model. Furthermore, we present an implementation of BMLF based on a self-paced learning regular term and design the corresponding optimization algorithm. By weighing tuples on the anchor level and training the model using tuples with higher weights preferentially, the side effect of low-quality noisy samples will be alleviated. We empirically demonstrate that the effectiveness and robustness of the proposed method outperform the state-of-the-art methods on several benchmark datasets.}
}
@article{MA2023109363,
title = {Lambertian-based adversarial attacks on deep-learning-based underwater side-scan sonar image classification},
journal = {Pattern Recognition},
volume = {138},
pages = {109363},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109363},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300064X},
author = {Qixiang Ma and Longyu Jiang and Wenxue Yu},
keywords = {Adversarial attack, Classification, Side-scan sonar, Lambertian model},
abstract = {Deep convolutional neural networks (CNNs) are extensively applied to the classification tasks for Side-scan sonar (SSS) images. However, state-of-the-art neural networks are prone to be confused by adversarial attacks that generate a tiny modification of the images, threatening the security of SSS classification. The robustness of CNN to adversarial attacks can be improved by introducing adversarial examples through adversarial training. Practical adversarial examples are often generated from elaborate adversarial attackers. For the underwater scenario of sonar, a specially designed adversarial attack method to weaken SSS image classification can make the research community better understand the weakness of CNN in this scenario and improve the security measures in a well-directed way. Thus, exploring adversarial attack methods for SSS image classification is essential. Nevertheless, the existing adversarial attack methods are designed for optical images, reflecting no physical characteristics of sonar images. To fill this gap and investigate the adversarial attack related to real-world conditions, in this paper, we propose an adversarial attack method named Lambertian Adversarial Sonar Attack (LASA). It initially leverages the Lambertian model to simulate the formation of the SSS image, factoring the image to three parameters, then updates the parameters on the direction of gradients by the chain rule. Finally, the parameters regenerate the adversarial example to fool the classifier. To validate the performance of LASA, we constructed a diversified SSS image dataset containing three categories. On our dataset, LASA reduces the Top-1 accuracy of a well-trained ResNet-101 to 7.31%±0.21 (one-shot version) and 0.00% (iterative version), the success rate of targeted attack reaches 97.03±2.24, far beyond the performance of the existing state-of-the-art adversarial attack methods. Meanwhile, we show that the adversarial training using examples generated from LASA makes the classifier more robust. We expect that our methods can be applied as a benchmark of adversarial attacks on SSS images, motivating future research to design novel neural networks or defensive methods to resist real-world adversarial attacks on SSS images.}
}
@article{YIN2023109450,
title = {Discriminative subspace learning via optimization on Riemannian manifold},
journal = {Pattern Recognition},
volume = {139},
pages = {109450},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109450},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001504},
author = {Wanguang Yin and Zhengming Ma and Quanying Liu},
keywords = {Discriminative subspace learning, Riemannian manifold optimization, Dimensionality reduction, Classification},
abstract = {Discriminative subspace learning is an important problem in machine learning, which aims to find the maximum separable decision subspace. Traditional Euclidean-based methods usually use Fisher discriminant criterion for finding an optimal linear mapping from a high-dimensional data space to a lower-dimensional subspace, which hardly guarantee a quadratic rate of global convergence and suffers from the singularity problem. Here, we propose the manifold optimization-based discriminant analysis (MODA) which is constructed by using the latent subspace alignment and the geometry of objective function with orthogonality constraint. MODA is solved by using Riemannian version of trust-region algorithm. Experimental results on various image datasets and electroencephalogram (EEG) datasets show that MODA achieves the best separability and is significantly superior to the competing algorithms. Especially for the time series of EEG signals, the accuracy of MODA is 20–30% higher than existing algorithms. The code for MODA is available at https://github.com/ncclabsustech/MODA-algorithm.}
}
@article{LUO2023109376,
title = {Independent vector analysis: Model, applications, challenges},
journal = {Pattern Recognition},
volume = {138},
pages = {109376},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109376},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000778},
author = {Zhongqiang Luo},
keywords = {IVA, BSS, ICA, Source priori models, Unsupervised learning, Audio source separation},
abstract = {This paper overviews an appealing unsupervised learning method named independent vector analysis (IVA) for its promising applications, such as in audio/speech signal separation, medical signal processing, remote sensing, video/image processing, wireless communication processing, and so on. As a useful data-driven technique in blind source separation (BSS) field, IVA has played an increasingly vital role in dealing with the problems of convolutive mixture separation, multivariate latent variable analysis and multivariate data fusion. IVA extends the conventional independent component analysis (ICA) to multidimensional components, which can result in more available information utilization. Compared with ICA mechanism, IVA is not only to utilize the statistical independence of multivariate signals but also the statistical inner dependency of each multivariate signal. With this generalization, IVA can manipulate some prominent ill-pose issues faced in sensor receiving models and has the advantage to overcome the inherent random permutation ambiguity problem in joint BSS. Motivated by the flexible and versatile technology superiorities of IVA, this paper concentrates on reviewing the IVA model in details, associated methods briefly, and its potential applications as well as prospects. Moreover, some significant open problems about IVA challenges are also discussed in this paper.}
}
@article{XU2023109474,
title = {Fourier-based augmentation with applications to domain generalization},
journal = {Pattern Recognition},
volume = {139},
pages = {109474},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109474},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001747},
author = {Qinwei Xu and Ruipeng Zhang and Ziqing Fan and Yanfeng Wang and Yi-Yan Wu and Ya Zhang},
keywords = {Domain shift, Domain generalization, Fourier-based augmentation, Consistency training},
abstract = {When deployed on a new domain different from the training set, deep learning often suffers from severe performance degradation. To combat domain shift, domain adaptation and domain generalization are proposed, where the former aims at transferring knowledge from related source domains to a known target domain, while the latter is more challenging by requiring the model to generalize to unknown target domains. This paper focuses on domain generalization and introduce a novel Fourier-based perspective for it. The main idea comes from the fact that Fourier amplitude component contains low-level statistics while phase component preserves high-level semantics. We thus propose a novel Fourier-based data augmentation strategy called AmpMix by linearly interpolating the amplitudes of two images while keeping their phases unchanged, to highlight the generalizable semantics contained in phase. To make full use of Fourier-augmented samples, we further incorporate consistency training between different augmentation views and devise a Fourier-based framework for three different domain generalization settings. Extensive experiments demonstrate the effectiveness of our Fourier-based method.}
}
@article{ZHANG2023109486,
title = {Dynamic graph convolutional networks by semi-supervised contrastive learning},
journal = {Pattern Recognition},
volume = {139},
pages = {109486},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109486},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001863},
author = {Guolin Zhang and Zehui Hu and Guoqiu Wen and Junbo Ma and Xiaofeng Zhu},
keywords = {Topology, Dynamic feature graph, Semi-supervised contrastive learning},
abstract = {The traditional graph convolutional network(GCN) and its variants usually only propagate node information through the topology given by the dataset. However, the given topology can only represent a certain relationship and ignore some correlative feature information between nodes, which may make the graph convolutional networks unable to fully utilize the data information. To address the above issue, a novel model named Dynamic Graph Convolutional Networks by Semi-Supervised Contrastive Learning (DGSCL) is proposed in this paper. First, a feature graph is dynamically constructed from the input node features to exploit the potential correlative feature information between nodes. Then, to ensure a high-quality feature graph, a semi-supervised contrastive learning method is designed to learn discriminative node embeddings, which can iteratively refine the constructed feature graph with the learned node embeddings. Finally, we fuse the node embeddings obtained from the given topology and the dynamic feature graph by two co-attention modules to produce more informative embeddings for the classification task. Through a series of experiments, we demonstrate the competitive performance of our model on seven node classification benchmarks.}
}
@article{XIAO2023109458,
title = {Where you edit is what you get: Text-guided image editing with region-based attention},
journal = {Pattern Recognition},
volume = {139},
pages = {109458},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109458},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001589},
author = {Changming Xiao and Qi Yang and Xiaoqiang Xu and Jianwei Zhang and Feng Zhou and Changshui Zhang},
keywords = {Generative adversarial networks, Text-guided image editing, Spatial disentanglement},
abstract = {Leveraging the abundant knowledge learned from pre-trained multi-modal models like CLIP has recently proved to be effective for text-guided image editing. Though convincing results have been made when combining the image generator StyleGAN with CLIP, most methods need to train separate models for different prompts, and irrelevant regions are often changed after editing due to the lack of spatial disentanglement. We propose a novel framework that can edit different images according to different prompts in one model. Besides, an innovative region-based spatial attention mechanism is adopted to explicitly guarantee the locality of editing. Experiments mainly in the face domain verify the feasibility of our framework and show that when multi-text editing and local editing are accomplishable, our method can complete practical applications like sequential editing and regional style transfer.}
}
@article{GEDAMU2023109455,
title = {Relation-mining self-attention network for skeleton-based human action recognition},
journal = {Pattern Recognition},
volume = {139},
pages = {109455},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109455},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001553},
author = {Kumie Gedamu and Yanli Ji and LingLing Gao and Yang Yang and Heng Tao Shen},
keywords = {Action recognition, Relation-mining self-attention, Pairwise self-attention, Unary self-attention, Position attention},
abstract = {Modeling spatiotemporal global dependencies and dynamics of body joints are crucial to recognizing actions from 3D skeleton sequences. We propose a Relation-mining Self-Attention Network (RSA-Net) for skeleton-based human action recognition. The proposed RSA-Net is motivated by two important observations: (1) body joint relationships can be modeled independently as pairwise and unary to reduce the difficulty of action feature learning. (2) Computing action semantics and position information independently removes noisy correlations over heterogeneous embedding. The proposed RSA-Net contains pairwise self-attention, unary self-attention, and position embedding attention modules. The pairwise self-attention captures the relationship between every two body joints. The unary self-attention learns a general correlation features among one key joint over all other query joints. The position embedding attention module computes the correlation between action semantics and position information independently with separate projection matrices. Extensive evaluations are performed in the NTU-60, NTU-120, and UESTC datasets with CS, CV, CSet, and A-view evaluation benchmarks. The proposed RSA-Net outperforms existing transformer-based approaches and comparable results with state-of-the-art graph ConvNet methods. The source code is available in Github11https://github.com/GedamuA/RSA-Net..}
}
@article{ZHANG2023109402,
title = {Cross-task and cross-domain SAR target recognition: A meta-transfer learning approach},
journal = {Pattern Recognition},
volume = {138},
pages = {109402},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109402},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001036},
author = {Yukun Zhang and Xiansheng Guo and Henry Leung and Lin Li},
keywords = {Meta learning, Transfer learning, Domain confusion, Synthetic aperture radar (SAR)},
abstract = {Meta learning and transfer learning offer promising solutions to the problem of requiring large amounts of data in deep learning approaches for synthetic aperture radar (SAR) target recognition. To improve their performance further, we propose a novel Meta-transfer learning approach for cross-task and cross-domain SAR target recognition (MetraSAR). In the meta training phase, we train a robust meta learner with the human-like ability to master new knowledge quickly across tasks and domains. By designing the weighted classification loss with class weights, we conduct hard class mining that forces the meta learner to grow stronger. In addition to the external knowledge transfer across different tasks, we achieve the internal transfer across domains by using the domain confusion loss with a domain discriminator. To balance the two designed loss terms, we adopt the multi-gradient descent algorithm to optimize the meta learner adaptively. In the meta testing phase, the trained robust meta learner is transferred to solve the new task with few shot samples and a quick generalization. Extensive experiments on the moving and stationary target acquisition and recognition (MSTAR) dataset validate that MetraSAR has better performance than conventional SAR target recognition methods.}
}
@article{WANG2023109426,
title = {Complementary adversarial mechanisms for weakly-supervised temporal action localization},
journal = {Pattern Recognition},
volume = {139},
pages = {109426},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109426},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001279},
author = {Chuanxu Wang and Jing Wang and Peng Liu},
keywords = {Temporal action localization, Boundary regression, Complementary adversarial mechanism, Weakly supervision learning, Action recognition},
abstract = {Weakly supervised Temporal Action Localization (WTAL) aims to locate the start and end boundaries of action instances and recognize their corresponding categories. Classical methods mainly rely on random erasure mechanisms, attention mechanisms, or imposing loss constraints. Despite their great progress, there are still two challenges of incomplete positioning and context confusion. Therefore, we propose a framework with complementary adversarial mechanisms to address these issues. In the adversarial learning stage, for an input snippet, we roughly determine its proper duration, by matching it with the specified multi-scaled anchors based on CAS score loss; then, it undergoes a frame-level iterative regression to precisely figure out its boundary, which can reject none closely related frames merged in, and ensures no overlapping between different action proposals. Subsequently, the GCN module explicitly enhances the feature representation for this fine localized snippet, aiming to strengthen the exclusiveness between different action snippets. Afterward, our complementary learning module calculates the similarity between the original input video Vg and the video Vr reconstructed with the above localization refined snippets, aiming to ensure no closely relevant frames missing, this checking mechanism works as feedback to guide the regression module for more accurate localization regression. Finally, each refined snippet undergoes multi-instance learning to obtain its classification score, and the top-k strategy is used to aggregate temporally adjacent snippets based on their content similarity, which can avoid fragmentation of an action proposal. This method is tested on datasets of THUMOS14 and ActivityNet1.2, and their average accuracy is 64.68% and 32.94% respectively, its comparisons with other articles prove its effectiveness.}
}
@article{BIGOLINLANFREDI2023109430,
title = {Quantifying the preferential direction of the model gradient in adversarial training with projected gradient descent},
journal = {Pattern Recognition},
volume = {139},
pages = {109430},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109430},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001310},
author = {Ricardo {Bigolin Lanfredi} and Joyce D. Schroeder and Tolga Tasdizen},
keywords = {Robustness, Robust models, Gradient direction, Gradient alignment, Deep learning, PGD, Adversarial training, GAN},
abstract = {Adversarial training, especially projected gradient descent (PGD), has proven to be a successful approach for improving robustness against adversarial attacks. After adversarial training, gradients of models with respect to their inputs have a preferential direction. However, the direction of alignment is not mathematically well established, making it difficult to evaluate quantitatively. We propose a novel definition of this direction as the direction of the vector pointing toward the closest point of the support of the closest inaccurate class in decision space. To evaluate the alignment with this direction after adversarial training, we apply a metric that uses generative adversarial networks to produce the smallest residual needed to change the class present in the image. We show that PGD-trained models have a higher alignment than the baseline according to our definition, that our metric presents higher alignment values than a competing metric formulation, and that enforcing this alignment increases the robustness of models.}
}
@article{WANG2023109438,
title = {Attention reweighted sparse subspace clustering},
journal = {Pattern Recognition},
volume = {139},
pages = {109438},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109438},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001395},
author = {Libin Wang and Yulong Wang and Hao Deng and Hong Chen},
keywords = {Subspace clustering, Sparse, Robust representation},
abstract = {Subspace clustering has attracted much attention in many applications of computer vision and pattern recognition. Spectral clustering based methods, such as sparse subspace clustering (SSC) and low-rank representation (LRR), have become popular due to their theoretical guarantees and impressive performance. However, many state-of-the-art subspace clustering methods specify the mean square error (MSE) criterion as the loss function, which is sensitive to outliers and complex noises in reality. These methods have poor performance when the data are corrupted by complex noise. In this paper, we propose a robust sparse subspace clustering method, termed Attention Reweighted SSC (ARSSC), by paying less attention to the corrupted entries (adaptively assigning small weights to the corrupted entries in each data point). To reduce the extra bias in estimation introduced by ℓ1 regularization, we also utilize non-convex penalties to overcome the overpenalized problem. In addition, we provide theoretical guarantees for ARSSC and theoretically show that our method gives a subspace-preserving affinity matrix under appropriate conditions. To solve the ARSSC optimization problem, we devise an optimization algorithm using an Alternating Direction Method of Multipliers (ADMM) method. Experiments on real-world databases validate the effectiveness of the proposed method.}
}
@article{CEVIKALP2023109385,
title = {From anomaly detection to open set recognition: Bridging the gap},
journal = {Pattern Recognition},
volume = {138},
pages = {109385},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109385},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000869},
author = {Hakan Cevikalp and Bedirhan Uzun and Yusuf Salk and Hasan Saribas and Okan Köpüklü},
keywords = {Anomaly detection, Open set recognition, Hypersphere classifier, Deep learning},
abstract = {The classifiers that return compact acceptance regions are crucial for the success in anomaly detection and open set recognition settings since we have to determine and reject the anomalies and samples coming from the unknown classes. This paper introduces novel methods that approximate the class acceptance regions with compact hypersphere models for anomaly detection and open set recognition. As opposed to the other deep hypersphere classifiers, we treat the hypersphere centers as learnable parameters and update them based on the changing deep feature representations. In addition, we propose novel loss terms that are more robust to the noisy labels within the outlier exposure and background datasets. The proposed methods bear similarity to the deep distance metric learning classifiers using the triplet loss function with the exception that the anchors are set to the hypersphere centers which are updated dynamically. The experimental results show that the proposed methods achieve the state-of-the-art accuracies on the majority of the tested datasets in the context of anomaly detection and open set recognition.}
}
@article{KOPOREC2023109397,
title = {Human-centered deep compositional model for handling occlusions},
journal = {Pattern Recognition},
volume = {138},
pages = {109397},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109397},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000985},
author = {Gregor Koporec and Janez Perš},
keywords = {Convolutional neural networks, Hierarchical compositonal model, Instance segmentation, Occlusion handling, Discriminability, Generalizability, Interpretability, Domain knowledge},
abstract = {Despite their powerful discriminative abilities, Convolutional Neural Networks (CNNs) lack the properties of generative models. This leads to a decreased performance in environments where objects are poorly visible. Solving such a problem by adding more training samples can quickly lead to a combinatorial explosion, therefore the underlying architecture has to be changed instead. This work proposes a Human-Centered Deep Compositional model (HCDC) that combines low-level visual discrimination of a CNN and the high-level reasoning of a Hierarchical Compositional model (HCM). Defined as a transparent model, it can be optimized to real-world environments by adding compactly encoded domain knowledge from human studies and physical laws. The new FridgeNetv2 dataset and a mixture of publicly available datasets are used as a benchmark. The experimental results show the proposed model is explainable, has higher discriminative and generative power, and better handles the occlusion than the current state-of-the-art Mask-RCNN in instance segmentation tasks.}
}
@article{HUA2023109511,
title = {Underwater object detection algorithm based on feature enhancement and progressive dynamic aggregation strategy},
journal = {Pattern Recognition},
volume = {139},
pages = {109511},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109511},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300211X},
author = {Xia Hua and Xiaopeng Cui and Xinghua Xu and Shaohua Qiu and Yingjie Liang and Xianqiang Bao and Zhong Li},
keywords = {Underwater image, Dynamic feature fusion, Small object detection, Rapid spatial pyramid pooling, Feature enhancement},
abstract = {To solve the problems that the conventional object detector is hard to extract features and miss detection of small objects when detecting underwater objects due to the noise of underwater environment and the scale change of objects, this paper designs a novel feature enhancement & progressive dynamic aggregation strategy, and proposes a new underwater object detector based on YOLOv5s. Firstly, a feature enhancement gating module is designed to selectively suppress or enhance multi-level features and reduce the interference of underwater complex environment noise on feature fusion. Then, the adjacent feature fusion mechanism and dynamic fusion module are designed to dynamically learn fusion weights and perform multi-level feature fusion progressively, so as to suppress the conflict information in multi-scale feature fusion and prevent small objects from being submerged by the conflict information. At last, a spatial pyramid pool structure (FMSPP) based on the same size quickly mixed pool layer is proposed, which can make the network obtain stronger description ability of texture and contour features, reduce the parameters, and further improve the generalization ability and classification accuracy. The ablation experiments and multi-method comparison experiments on URPC and DUT-USEG data sets prove the effectiveness of the proposed strategy. Compared with the current mainstream detectors, our detector achieves obvious advantages in detection performance and efficiency.}
}
@article{SU2023109443,
title = {Hybrid token transformer for deep face recognition},
journal = {Pattern Recognition},
volume = {139},
pages = {109443},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109443},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001437},
author = {Weicong Su and Yali Wang and Kunchang Li and Peng Gao and Yu Qiao},
keywords = {Face recognition, Hybrid tokens, Relation learning},
abstract = {Although Convolutional Neural Networks have achieved remarkable successes in face recognition, they still suffer a critical limitation on capturing long range relations among facial regions. The recent vision transformers can naturally alleviate this problem, by learning global token dependencies. However, They are insufficient to discover high-level facial semantics since tokens in these transformers are based on small and fixed regions. To tackle such difficulty, we propose a novel Hybrid tOken Transformer (HOTformer) module to identify key facial semantics for effective recognition with cooperation of atomic and holistic tokens. Specifically, atomic tokens are generated from small fixed-size regions that can learn fine-grained core representation. Alternatively, holistic tokens are constructed from big adaptively-learned regions that can capture coarse-grained contextual representation. Furthermore, our HOTformer is a plug-and-play module. By hierarchically inserting it into convolutional networks, we can build a concise HOTformer-Net that achieves a preferable computation like CNN while boosting accuracy like transformer.}
}
@article{ZHANG2023109373,
title = {A graph model-based multiscale feature fitting method for unsupervised anomaly detection},
journal = {Pattern Recognition},
volume = {138},
pages = {109373},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109373},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000742},
author = {Fanghui Zhang and Shichao Kan and Damin Zhang and Yigang Cen and Linna Zhang and Vladimir Mladenovic},
keywords = {Anomaly detection, Unsupervised learning, Graph model, Feature fitting representation},
abstract = {Anomaly detection and localization without prior knowledge is a challenging problem in industrial manufacturing due to the complexity and variety of anomaly types. Most of the existing methods have achieved considerable anomaly detection performance based on the distance between normal features and abnormal features. However, when the defect area is hard to distinguish from the background or the defect area is small, the distance between normal and abnormal features will be too close to detect anomaly areas. In addition, existing methods do not consider the influences of features in different layers with different anomaly sizes. In this paper, a graph model-based multiscale feature fitting method is proposed for unsupervised anomaly detection. Specifically, we build a graph model based on the K nearest neighbors of an anchor image. The feature fitting and anomaly scores of the anchor images in the graph vertices are calculated next. Finally, a weighted multiscale anomaly map matching method is proposed to detect and locate the anomaly regions of test images. Compared with the state-of-the-art methods, our proposed method achieves competitive improvement in anomaly detection and localization on the MVTec AD dataset, the two KolektorSDD datasets, and the mSTC dataset.}
}
@article{DENG2023109470,
title = {Strongly augmented contrastive clustering},
journal = {Pattern Recognition},
volume = {139},
pages = {109470},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109470},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300170X},
author = {Xiaozhi Deng and Dong Huang and Ding-Hua Chen and Chang-Dong Wang and Jian-Huang Lai},
keywords = {Data clustering, Deep clustering, Image clustering, Contrastive learning, Deep neural network},
abstract = {Deep clustering has attracted increasing attention in recent years due to its capability of joint representation learning and clustering via deep neural networks. In its latest developments, the contrastive learning has emerged as an effective technique to substantially enhance the deep clustering performance. However, the existing contrastive learning based deep clustering algorithms mostly focus on some carefully-designed augmentations (often with limited transformations to preserve the structure), referred to as weak augmentations, but cannot go beyond the weak augmentations to explore the more opportunities in stronger augmentations (with more aggressive transformations or even severe distortions). In this paper, we present an end-to-end deep clustering approach termed Strongly Augmented Contrastive Clustering (SACC), which extends the conventional two-augmentation-view paradigm to multiple views and jointly leverages strong and weak augmentations for strengthened deep clustering. Particularly, we utilize a backbone network with triply-shared weights, where a strongly augmented view and two weakly augmented views are incorporated. Based on the representations produced by the backbone, the weak-weak view pair and the strong-weak view pairs are simultaneously exploited for the instance-level contrastive learning (via an instance projector) and the cluster-level contrastive learning (via a cluster projector), which, together with the backbone, can be jointly optimized in a purely unsupervised manner. Experimental results on five challenging image datasets have shown the superiority of our SACC approach over the state-of-the-art. The code is available at https://github.com/dengxiaozhi/SACC.}
}
@article{WEI2023109483,
title = {Deep debiased contrastive hashing},
journal = {Pattern Recognition},
volume = {139},
pages = {109483},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109483},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001838},
author = {Rukai Wei and Yu Liu and Jingkuan Song and Yanzhao Xie and Ke Zhou},
keywords = {Learning to hash, Contrastive learning, Instance discrimination, EM Algorithm, Neighborhood discovery},
abstract = {Hashing has achieved great success in multimedia retrieval due to its high computing efficiency and low storage cost. Recently, contrastive-learning-based hashing methods have achieved decent retrieval performance in label-free scenarios by learning distortion-invariant representations with Siamese networks. Their learning principle, i.e., instance discrimination, maximizes the correlation between self-augmented views and treats all others as negative samples. However, it may learn with false negative samples that are naturally similar, resulting in biased hash learning. To bridge this flaw, we reveal the between-instance similarity of naturally similar samples by exploring the latent structure of the training data. As a result, we propose the Deep Debiased Contrastive Hashing (DDCH) algorithm, using the neighborhood discovery module to explore the intrinsic similarity relationship that can help contrastive hashing reduce false negatives for superior discriminatory ability. Furthermore, we elucidate the rationale for incorporating the module into the contrastive hashing framework and explain our hashing process from an Expectation-Maximization (EM) perspective. Extensive experimental results on three benchmark image datasets demonstrate that DDCH significantly outperforms the state-of-the-art unsupervised hashing methods for image retrieval.}
}
@article{ZHONG2023109427,
title = {Geometric algebra-based multiview interaction networks for 3D human motion prediction},
journal = {Pattern Recognition},
volume = {138},
pages = {109427},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109427},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001280},
author = {Jianqi Zhong and Wenming Cao},
keywords = {Human motion prediction, Geometric algebra, Motion generation},
abstract = {3D skeleton-based human motion prediction is an essential and challenging task for human-machine interactions, which aims to forecasts future poses given a history of their previous motions. Recent works based on Graph Neural Networks (GCNs) show promising performance for motion prediction due to the powerful ability of feature aggregation of GCNs. However, with the deep and multi-stage GCN model deployment, its feature extraction mechanism tends to result in feature similarity over all joints, and degrade the prediction performance. In addition, such a graph structure in recent works was still insufficient to process the high dimensional structural data in Euclidean space when inference through multi-layer networks. To solve the problem, we propose a novel Geometric Algebra-based Multi-view Interaction network (GA-MIN), which captures and aggregates motion features from two interactions: 1) global-interaction, which refactors various spectrum dependencies using geometric algebra-based structure, and 2) self-interaction, which leverage self-attention mechanism to capture compact representations. Extensive experiments are conducted on three public datasets: Human3.6M, CMU Mocap, and 3DPW, which prove that the proposed GA-MIN outperforms state-of-the-art methods on 3D Mean Per Joint Position Error (MPJPE) and Mean Angle Error (MAE) on average.}
}
@article{LUO2023109431,
title = {Infrared and visible image fusion based on Multi-State contextual hidden Markov Model},
journal = {Pattern Recognition},
volume = {138},
pages = {109431},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109431},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001322},
author = {Xiaoqing Luo and Yuting Jiang and Anqi Wang and Juan Wang and Zhancheng Zhang and Xiao-Jun Wu},
keywords = {Image fusion, non-subsampled Shearlet transform, contextual hidden Markov model, multi-state, soft context variable},
abstract = {In this paper, we propose a novel multi-state contextual hidden Markov model (MCHMM) in the non-subsampled Shearlet transform (NSST) domain for image fusion. The traditional two-state hidden Markov model divides the multi-scale coefficients only into large and small states, which can lead to an inaccurate statistical model and reduce the quality of the fusion result. Our method improves upon this by developing a multi-state model and a soft context variable to provide a fine-grained representation of the high-frequency subbands, resulting in improved fusion results. Additionally, the fusion of low-frequency subbands is performed on the difference of regional energy to ensure visual quality. Our experimental results on several datasets demonstrate that the proposed method outperforms other fusion methods in both subjective and objective evaluations.}
}
@article{HE2023109456,
title = {Fuzzy granular recurrence plot and quantification analysis: A novel method for classification},
journal = {Pattern Recognition},
volume = {139},
pages = {109456},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109456},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001565},
author = {Qian He and Fusheng Yu and Jiaqi Chang and Chenxi Ouyang},
keywords = {Fuzzy granular recurrence plot, Recurrence plot, Fuzzy information granule, Noise, Classification},
abstract = {Recently, recurrence plot (RP) and its quantification techniques have become an important research tool in nonlinear analysis. In the existing researches, an RP is directly established on a time series ignoring the influence of noise on data, which will affect our judgement on the dynamic properties of a system. To tackle the problem there, this paper proposes a novel recurrence plot, namely fuzzy granular recurrence plot (FGRP). An FGRP of a time series is built not directly on the time series itself but on its corresponding granular time series which is composed of fuzzy information granules. With specific capability, fuzzy information granules are used as building blocks of an FGRP to achieve high-level, compact and understandable signal models. In order to apply the FGRP method to time series classification tasks, an FGRP based classification model is designed in this paper. Subsequent experiments show that the FGRP of a time series can reduce the effect of noise, and the FGRP based classification model can improve the classification performance.}
}
@article{CHENG2023109403,
title = {Bottom-up 2D pose estimation via dual anatomical centers for small-scale persons},
journal = {Pattern Recognition},
volume = {139},
pages = {109403},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109403},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001048},
author = {Yu Cheng and Yihao Ai and Bo Wang and Xinchao Wang and Robby T. Tan},
keywords = {Multi-person pose estimation, Human pose estimation, Anatomical centers},
abstract = {In multi-person 2D pose estimation, the bottom-up methods simultaneously predict poses for all persons, and unlike the top-down methods, do not rely on human detection. However, the SOTA bottom-up methods’ accuracy is still inferior compared to the existing top-down methods. This is due to the predicted human poses being regressed based on the inconsistent human bounding box center and the lack of human-scale normalization, leading to the predicted human poses being inaccurate and small-scale persons being missed. To push the envelope of the bottom-up pose estimation, we firstly propose multi-scale training to enhance the network to handle scale variation with single-scale testing, particularly for small-scale persons. Secondly, we introduce dual anatomical centers (i.e., head and body), where we can predict the human poses more accurately and reliably, especially for small-scale persons. Moreover, existing bottom-up methods use multi-scale testing to boost the accuracy of pose estimation at the price of multiple additional forward passes, which weakens the efficiency of bottom-up methods, the core strength compared to top-down methods. By contrast, our multi-scale training enables the model to predict high-quality poses in a single forward pass (i.e., single-scale testing). Our method achieves 38.4% improvement on bounding box precision and 39.1% improvement on bounding box recall over the state of the art (SOTA) on the challenging small-scale persons subset of COCO. For the human pose AP evaluation, we achieve a new SOTA (71.0 AP) on the COCO test-dev set with the single-scale testing. We also achieve the top performance (40.3 AP) on the OCHuman dataset in cross-dataset evaluation.}
}
@article{LIU2023109468,
title = {Cycle optimization metric learning for few-shot classification},
journal = {Pattern Recognition},
volume = {139},
pages = {109468},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109468},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001681},
author = {Qifan Liu and Wenming Cao and Zhihai He},
keywords = {Few-shot learning, Cycle optimization, Image classification},
abstract = {Metric learning methods are widely used in few-shot learning due to their simplicity and effectiveness. Most existing methods directly predict query labels by comparing the similarity between support and query samples. In this paper, we design a cycle optimization metric network for few-shot classification task that optimizes model performance based on loop-prediction of the labels of query samples and support samples. Specifically, we construct a forward network and reverse network based on a geometric algebra Graph Neural Network (GA-GNN). These two networks form the loop prediction from support samples to query samples and then back to support samples, guided by a cycle-consistency loss. We also introduce an optimization module that is able to correct the predicted results of query samples to further improve the network performance. Our extensive experimental results demonstrate that the proposed cycle optimization metric network outperforms existing state-of-the-art few-shot learning methods on classification tasks.}
}
@article{LIU2023109415,
title = {Accurate light field depth estimation under occlusion},
journal = {Pattern Recognition},
volume = {138},
pages = {109415},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109415},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001164},
author = {Yuxuan Liu and Mitko Aleksandrov and Zhihua Hu and Yan Meng and Li Zhang and Sisi Zlatanova and Haibin Ai and Pengjie Tao},
keywords = {Light field, Depth estimation, EPI, Multi-view depth maps integration, Occlusion handling},
abstract = {Epipolar plane images (EPIs) have advantages in light field depth estimation, but the current EPI-based methods only use one aspect of the information related to the line or its surroundings in EPIs. Moreover, most current methods merely extract the depth map of the target view, ignoring the depth information from other views. To fully utilize the available information, we first introduce a novel data cost by comprehensively utilizing the characteristics of pixel consistency on the line and region difference around the line in EPIs to improve the robustness against occlusion and noise. Then, we put forward a multi-view depth integration strategy that copes with weak texture and occlusion areas. Finally, an edging preserving filter is applied to further refine the depth map. Experiments on synthetic and real light field datasets show that the proposed method outperforms the state-of-the-art light field depth estimation algorithms, especially in the presence of occluded pixels.}
}
@article{KRAWCZYK2023109444,
title = {Segmentation of 3D Point Cloud Data Representing Full Human Body Geometry: A Review},
journal = {Pattern Recognition},
volume = {139},
pages = {109444},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109444},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001449},
author = {Damian Krawczyk and Robert Sitnik},
keywords = {human body silhouette segmentation, automated point cloud segmentation, surface measurement, 3D data acquisition},
abstract = {This article aims to present a review of the segmentation techniques of the 3D data representing human body in the form of point clouds. The techniques discussed are divided into three subgroups: 2D contour approaches, topological techniques, and machine learning heuristics. These subgroups are then reviewed regarding the following aspects: computation time, accuracy, reliability, dependency on human pose, and segment count. The authors emphasize an analysis of these algorithms with respect to their exploitation in the segmentation of 3D data varying in time, as well as further improvement of the applications in anthropometry. The conclusion reached is that machine learning approach tends to be the most suitable solution for future 4D applications. Another foreseeable direction of development in the field of segmentation algorithms is the classification of the points on the borders between segments and maintaining fluent and consistent edges of the segments between the subsequent frames.}
}
@article{CHEN2023109467,
title = {An Interpretable Channelwise Attention Mechanism based on Asymmetric and Skewed Gaussian Distribution},
journal = {Pattern Recognition},
volume = {139},
pages = {109467},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109467},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300167X},
author = {Cheng Chen and Bo Li},
keywords = {Channelwise attention, Interpretable modeling, Skewness distribution, Asymmetric distribution},
abstract = {Channelwise attention mechanisms have recently been demonstrated to boost the performance of deep convolutional neural networks (CNNs). The hypothesis on the negative correlation between channelwise responses and their importance levels has been verified. Therefore, according to the shifted means and unbalanced responses that are observed in attention distribution, two empirical hypotheses are proposed in this paper. Then, as an interpretable attention module, bilateral asymmetric skewed Gaussian attention (bi-SGA) is utilized to combine skewed and asymmetric properties into the Gaussian context transformer (GCT), which is the state-of-the-art. Finally, extensive experiments on the different benchmarks validate the rationality of the hypotheses. The proposed bi-SGA improves the overall performance of GCT attention mechanisms with only two extra parameters to be learned. Moreover, our attention mechanism also provides a perspective on analyzing the channelwise importance levels in deep neural networks in an interpretable and logical manner.}
}
@article{ZHAO2023109374,
title = {Weight-guided class complementing for long-tailed image recognition},
journal = {Pattern Recognition},
volume = {138},
pages = {109374},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109374},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000754},
author = {Xinqiao Zhao and Jimin Xiao and Siyue Yu and Hui Li and Bingfeng Zhang},
keywords = {Image recognition, Long-tailed distribution, Gradient shift, Weight-guided method},
abstract = {Real-world data are often long-tailed distributed and have plenty classes. This characteristic leads to a significant performance drop for various models. One reason behind that is the gradient shift caused by unsampled classes in each training iteration. In this paper, we propose a Weight-Guided Class Complementing framework to address this issue. Specifically, this framework first complements the unsampled classes in each training iteration by using a dynamic updated data slot. Then, considering the over-fitting issue caused by class complementing, we utilize the classifier weights as learned knowledge and encourage the model to discover more class specific characteristics. Finally, we design a weight refining scheme to deal with the long-tailed bias existing in classifier weights. Experimental results show that our framework can be implemented upon different existing approaches effectively, achieving consistent improvements on various benchmarks with new state-of-the-art performances. Codes will be released.}
}
@article{WANG2023109472,
title = {Sparse feature selection via fast embedding spectral analysis},
journal = {Pattern Recognition},
volume = {139},
pages = {109472},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109472},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001723},
author = {Jingyu Wang and Hongmei Wang and Feiping Nie and Xuelong Li},
keywords = {Unsupervised learning, Feature selection, Spectral analysis, Sparse subspace, -Norm},
abstract = {Feature selection has been a research hotspot in many fields. Models based on graph learning are currently the most popular approaches. However, the sparsity of most models is not strong, and graph learning for pair-sample evaluation takes a lot of time. ℓ2,1-norm regularization is the sparsity strategy adopted in most sparse models at present since the convex function is easy to solve. Nevertheless, the sparsity of ℓ2,1-norm is insufficient, and there exist parameter adjustment problems. ℓ2,0-norm is a better choice, which can strengthen the sparse constraints of the subspace. In this paper, the Sparse feature selection via Fast Embedding Spectral Analysis (SFESA) is proposed. Firstly, an adaptive anchor nearest neighbor graph is constructed to avoid the high time cost of learning pairwise nearest neighbor graphs to a certain extent. The low-dimensional embedding of data manifold structure is maintained by performing spectral analysis for the constructed graph. Secondly, the projected data is approximated to the low-dimensional embedding structure via a regularization term. Finally, ℓ2,0-norm is employed to constrain the projection matrix to enhance the subspace sparsity. Furthermore, a fast iterative algorithm is presented to solve this non-convex optimization problem. Extensive experiments on multiple public datasets show that SFESA can obtain excellent performance in less time.}
}
@article{BAKKALI2023109419,
title = {VLCDoC: Vision-Language contrastive pre-training model for cross-Modal document classification},
journal = {Pattern Recognition},
volume = {139},
pages = {109419},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109419},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001206},
author = {Souhail Bakkali and Zuheng Ming and Mickael Coustaty and Marçal Rusiñol and Oriol Ramos Terrades},
keywords = {Multimodal document representation learning, Document classification, Contrastive learning, Self-Attention, Transformers},
abstract = {Multimodal learning from document data has achieved great success lately as it allows to pre-train semantically meaningful features as a prior into a learnable downstream task. In this paper, we approach the document classification problem by learning cross-modal representations through language and vision cues, considering intra- and inter-modality relationships. Instead of merging features from different modalities into a joint representation space, the proposed method exploits high-level interactions and learns relevant semantic information from effective attention flows within and across modalities. The proposed learning objective is devised between intra- and inter-modality alignment tasks, where the similarity distribution per task is computed by contracting positive sample pairs while simultaneously contrasting negative ones in the joint representation space. Extensive experiments on public benchmark datasets demonstrate the effectiveness and the generality of our model both on low-scale and large-scale datasets.}
}
@article{LI2023109398,
title = {Human-related anomalous event detection via memory-augmented Wasserstein generative adversarial network with gradient penalty},
journal = {Pattern Recognition},
volume = {138},
pages = {109398},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109398},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000997},
author = {Nanjun Li and Faliang Chang and Chunsheng Liu},
keywords = {Human-related anomalous event detection, Video surveillance, Human skeleton trajectories, Wasserstein generative adversarial network with gradient penalty, Memory module},
abstract = {Timely detection of human-related anomaly in surveillance videos is a challenging task. Generally, the irregular human motion and action patterns can be regarded as abnormal human-related events. In this paper, we utilize the skeleton trajectories to learn the regularities of human motion and action in videos for anomaly detection. The skeleton trajectories are decomposed into global and local feature sequences, which are utilized to provide human motion and action information, respectively. Then, the global and local sequences are modeled as two separate sub-processes with our proposed Memory-augmented Wasserstein Generative Adversarial Network with Gradient Penalty (MemWGAN-GP). In each sub-process, the pre-trained MemWGAN-GP is employed to predict future feature sequences from corresponding input past sequences and reconstruct the input sequences simultaneously. The predicted and reconstructed feature sequences are compared with their groundtruth to identify anomalous sequences. The MemWGAN-GP integrates the autoencoder with a WGAN model to boost the reconstruction and prediction ability of the autoencoder. Besides, a memory module is employed in MemWGAN-GP to overcome high capacity of the autoencoder for anomalies reconstruction and prediction. Experimental results on four challenging datasets demonstrate advantages of the proposed method over other state-of-the-art algorithms.}
}
@article{PENG2023109370,
title = {Learning efficient facial landmark model for human attractiveness analysis},
journal = {Pattern Recognition},
volume = {138},
pages = {109370},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109370},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000717},
author = {Tianhao Peng and Mu Li and Fangmei Chen and Yong Xu and David Zhang},
keywords = {Human attractiveness, Landmark model, Geometric feature, Genetic algorithm},
abstract = {Existing geometric features on facial attractiveness analysis only focus on the ratios and distances, which is incomplete to represent all the information of a face. In this paper, we introduce a new category of feature, i.e., the angle features, to describe the angle of different organs such as the chin and eyes, which help boost the analysis performance in experiment. In addition, existing facial beauty analysis papers usually apply existing landmark models and extract their own different geometric feature sets on the landmarks. On the one hand, the geometric features are quite chaotic between different papers. On the other hand, most of the landmarks in the existing landmark model are useless for geometric feature extraction which wastes a lot of computational resources. To tackle these issues, we suggest to define a common geometric feature set and learn a special landmark model for attractiveness analysis. Specially, we collect all the available geometric features from the previous jobs and introduce a genetic feature selection algorithm to select the most effective geometric features. Furthermore, we introduce a special landmark model which exactly covers all the extracted geometric features. The experiments show that our method with the introduced angle features and the common feature set can outperform state-of-art facial beauty estimation methods with geometric features.}
}
@article{WU2023109449,
title = {ECM-EFS: An ensemble feature selection based on enhanced co-association matrix},
journal = {Pattern Recognition},
volume = {139},
pages = {109449},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109449},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001498},
author = {Ting Wu and Yihang Hao and Bo Yang and Lizhi Peng},
keywords = {Ensemble feature selection, Machine learning, Feature kernel, Relative-co-association matrix (RCM)},
abstract = {Currently, feature selection faces a huge challenge that no single feature selection method can effectively deal with various data sets for all real cases. Ensemble learning is a potential promising solution to address this problem. We propose an ensemble feature selection method based on enhanced co-association matrix (ECM-EFS). Positive-co-association matrix (PCM), negative-co-association matrix (NCM), and relative-co-association matrix (RCM) are first introduced to discover the relationship among features by ensembling the results in multiple feature selection methods. To further produce a more stable feature selection result, “Feature Kernel” is also introduced and used as a starting point for feature selection. Comparative experiments with four state-of-the-art methods have confirmed that the ECM-EFS can provide more robust results. Moreover, compared with traditional ensemble feature selection methods, our method can compensate information loss and reduce computational cost significantly.}
}
@article{ZHANG2023109400,
title = {Learning from multiple annotators for medical image segmentation},
journal = {Pattern Recognition},
volume = {138},
pages = {109400},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109400},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001012},
author = {Le Zhang and Ryutaro Tanno and Moucheng Xu and Yawen Huang and Kevin Bronik and Chen Jin and Joseph Jacob and Yefeng Zheng and Ling Shao and Olga Ciccarelli and Frederik Barkhof and Daniel C. Alexander},
keywords = {Multi-Annotator, Label fusion, Segmentation},
abstract = {Supervised machine learning methods have been widely developed for segmentation tasks in recent years. However, the quality of labels has high impact on the predictive performance of these algorithms. This issue is particularly acute in the medical image domain, where both the cost of annotation and the inter-observer variability are high. Different human experts contribute estimates of the ”actual” segmentation labels in a typical label acquisition process, influenced by their personal biases and competency levels. The performance of automatic segmentation algorithms is limited when these noisy labels are used as the expert consensus label. In this work, we use two coupled CNNs to jointly learn, from purely noisy observations alone, the reliability of individual annotators and the expert consensus label distributions. The separation of the two is achieved by maximally describing the annotator’s “unreliable behavior” (we call it “maximally unreliable”) while achieving high fidelity with the noisy training data. We first create a toy segmentation dataset using MNIST and investigate the properties of the proposed algorithm. We then use three public medical imaging segmentation datasets to demonstrate our method’s efficacy, including both simulated (where necessary) and real-world annotations: 1) ISBI2015 (multiple-sclerosis lesions); 2) BraTS (brain tumors); 3) LIDC-IDRI (lung abnormalities). Finally, we create a real-world multiple sclerosis lesion dataset (QSMSC at UCL: Queen Square Multiple Sclerosis Center at UCL, UK) with manual segmentations from 4 different annotators (3 radiologists with different level skills and 1 expert to generate the expert consensus label). In all datasets, our method consistently outperforms competing methods and relevant baselines, especially when the number of annotations is small and the amount of disagreement is large. The studies also reveal that the system is capable of capturing the complicated spatial characteristics of annotators’ mistakes.}
}
@article{HOU2023109477,
title = {Deep generative image priors for semantic face manipulation},
journal = {Pattern Recognition},
volume = {139},
pages = {109477},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109477},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001772},
author = {Xianxu Hou and Linlin Shen and Zhong Ming and Guoping Qiu},
keywords = {GANs, Face attribute prediction, Semantic face manipulation},
abstract = {Previous works on generative adversarial networks (GANs) mainly focus on how to synthesize high-fidelity images. In this paper, we present a framework to leverage the knowledge learned by GANs for semantic face manipulation. In particular, we propose to control the semantics of synthesized faces by adapting the latent codes with an attribute prediction model. Moreover, in order to achieve a more accurate estimation of different facial attributes, we propose to pretrain the attribute prediction model by inverting the synthesized face images back to the GAN latent space. As a result, our method explicitly considers the semantics encoded in the latent space of a pretrained GAN and is able to faithfully edit various attributes like eyeglasses, smiling, bald, age, mustache and gender for high-resolution face images. Extensive experiments show that our method has superior performance compared to state of the art for both face attribute prediction and semantic face manipulation.}
}
@article{ZHANG2023109424,
title = {ThumbDet: One thumbnail image is enough for object detection},
journal = {Pattern Recognition},
volume = {138},
pages = {109424},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109424},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001255},
author = {Yongqiang Zhang and Yin Zhang and Rui Tian and Zian Zhang and Yancheng Bai and Wangmeng Zuo and Mingli Ding},
keywords = {Object detection, Down-sampling network, Knowledge distillation},
abstract = {Computer vision fields have witnessed great success thanks to deep convolutional neural networks (CNNs). However, state-of-the-art methods often benefit from large models and datasets, which introduce heavy parameters and computational requirements. Deploying such large models in real-world applications is very difficult because of the limited computing resources. Although many researchers focus on designing efficient block structures to compress model parameters, they ignore that the role of large-scale input images is also an important factor for algorithm efficiency. Reducing input resolution is a useful method to boost runtime efficiency, however, traditional interpolation methods assume a fixed degradation criterion that greatly hurts performance. To solve the above problems, in this paper, we propose a novel framework named ThumbDet for reducing model computation while maintaining detection accuracy. In our framework, we first design an image down-sampling module to learn a small-scale image that looks realistic and contains discriminative properties. Furthermore, we propose a distillation-boost supervision strategy to maintain the detection performance of small-scaled images as the original-size inputs. Extensive experiments conducted on a standard object detection dataset MS COCO demonstrate the effectiveness of the proposed method when using very low-resolution images (i.e. 4× down-sampling) as inputs. In particular, ThumbDet achieves satisfactory detection performance (i.e. 32.3% in mAP) while drastically reducing computation and memory requirements (i.e. speed up of 1.26×), outperforming the traditional interpolation methods (e.g. bicubic) by +3.2% absolutely in terms of mAP.}
}
@article{DING2023109436,
title = {Multi-agent dueling Q-learning with mean field and value decomposition},
journal = {Pattern Recognition},
volume = {139},
pages = {109436},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109436},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001371},
author = {Shifei Ding and Wei Du and Ling Ding and Lili Guo and Jian Zhang and Bo An},
keywords = {Multi-agent, value decomposition, mixed cooperative-competitive task, mean filed},
abstract = {A great deal of multi agent reinforcement learning(MARL) work has investigated how multiple agents effectively accomplish cooperative tasks utilizing value function decomposition methods. However, existing value decomposition methods can only handle cooperative tasks with shared reward, due to these methods factorize the value function from a global perspective. To tackle the competitive tasks and mixed cooperative-competitive tasks with differing individual reward setting, we design the Multi-agent Dueling Q-learning (MDQ) method based on mean-filed theory and individual value decomposition. Specifically, we integrate the mean-field theory with the value decomposition to factorize the value function at the individual level, which can deal with mixed cooperative-competitive tasks. Besides, we take a dueling network architecture to distinguish which states are valuable, eliminating the need to learn the impact of each action on each state, therefore enabling efficient learning and leading to better policy evaluation. The proposed method MDQ is applicable not only to cooperative tasks with shared rewards setting, but also to mixed cooperative-competitive tasks with individualized reward settings. In this end, it is flexible and generically applicable enough to most multi-agent tasks. Empirical experiments on various mixed cooperative-competitive tasks demonstrate that MDQ significantly outperforms existing multi agent reinforcement learning methods.}
}
@article{DORNAIKA2023109481,
title = {Object-centric Contour-aware Data Augmentation Using Superpixels of Varying Granularity},
journal = {Pattern Recognition},
volume = {139},
pages = {109481},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109481},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001814},
author = {F. Dornaika and D. Sun and K. Hammoudi and J. Charafeddine and A. Cabani and C. Zhang},
keywords = {Data augmentation, Cutmix, Object-centric Contour-aware, Discriminative regions, Attention, Superpixels},
abstract = {Regional dropout strategies have demonstrated to be very effective in improving both the performance and the generalization capability of deep learning models. However, when such strategies are performed in a totally random manner, the background noise and label mismatch problems arise. To tackle such problems, existing approaches typically focus on regions with the highest distinctiveness. Yet, there are two main drawbacks of existing approaches: (I) Many existing region-based augmentation methods can only use rectangular regions, resulting in the loss of object contour information; (II) Deterministic selection of the most discriminative regions leads to poor diversification in data augmentation. In fact, a trade-off is needed between diversification and concentration, which can decrease the undesirable noise. In this paper, we propose a novel object-centric contour-aware CutMix data augmentation strategy with arbitrary- shape and size superpixel supports, which is hereafter referred to as OcCaMix for short. It not only captures the most discriminative regions, but also effectively preserves the contour details of the objects. Moreover, it enables the search of natural object parts of different sizes. Extensive experiments on a large number of benchmark datasets show that OcCaMix significantly outperforms state-of-the-art CutMix based data augmentation methods in classification tasks. The source codes and trained models are available at https://github.com/DanielaPlusPlus/OcCaMix.}
}
@article{QIU2023109383,
title = {SATS: Self-attention transfer for continual semantic segmentation},
journal = {Pattern Recognition},
volume = {138},
pages = {109383},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109383},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000845},
author = {Yiqiao Qiu and Yixing Shen and Zhuohao Sun and Yanchong Zheng and Xiaobin Chang and Weishi Zheng and Ruixuan Wang},
keywords = {Continual learning, Semantic segmentation, Self-attention transfer, Class-specific region pooling},
abstract = {Continually learning to segment more and more types of image regions is a desired capability for many intelligent systems. However, such continual semantic segmentation exhibits catastrophic forgetting issues similar to those of continual classification learning. Unlike the existing knowledge distillation strategies for alleviating this problem, transferring a new type of information, namely, the relationships between elements (e.g., pixels) within each image that can capture both within-class and between-class knowledge, is proposed in this study. Such information can be effectively obtained from self-attention maps in a Transformer-style segmentation model. Considering that pixels belonging to the same class in each image typically share similar visual properties, a class-specific region pooling operator is novelly applied to provide reliable relationship information for knowledge transfer. Extensive evaluations on multiple public benchmarks reveal that the proposed self-attention transfer method can effectively alleviate the catastrophic forgetting issue. Furthermore, flexible combinations of the proposed method with widely adopted strategies considerably outperform state-of-the-art solutions.}
}
@article{ZHANG2023109435,
title = {Boosting transferability of physical attack against detectors by redistributing separable attention},
journal = {Pattern Recognition},
volume = {138},
pages = {109435},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109435},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300136X},
author = {Yu Zhang and Zhiqiang Gong and Yichuang Zhang and Kangcheng Bin and Yongqian Li and Jiahao Qi and Hao Wen and Ping Zhong},
keywords = {Physical attack, Transferability, Multi-layer attention, Object detection, Black-box models},
abstract = {The research on attack transferability is of great importance as it can guide how to conduct an adversarial attack without knowing any information about target models. However, it remains challenging for adversarial examples to maintain a good attack transferability performance, especially for the black-box attack implemented in the physical world. To enhance black-box transferability of physical attacks on object detectors, we present a novel adversarial learning method to produce adversarial patches by redistributing separable attention maps. Concretely, we first develop smoothed multilayer attention maps by introducing serial composite transformations, which could suppress model-specific noise on the one hand, and cover objects to be concealed at various resolutions on the other hand. Besides, our method resorts to a scalable mask to separate object attention from the background and adjust their distribution with a novel loss function. Extensive experiments show that our approach outperforms state-of-the-art methods in both the digital space and the physical world. Our code is available at https://github.com/zhangyu13a/transPhyAtt.}
}
@article{WANG2023109441,
title = {Dynamic dual graph networks for textbook question answering},
journal = {Pattern Recognition},
volume = {139},
pages = {109441},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109441},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000961},
author = {Yaxian Wang and Jun Liu and Jie Ma and Hongwei Zeng and Lingling Zhang and Junjun Li},
keywords = {Textbook question answering, Multi-modal machine comprehension, Diagram understanding},
abstract = {Textbook Question Answering (TQA) is a complex task oriented to multi-modal context, which requires reasoning on a diagram and a long essay to get the correct answer. There are mainly two related issues for the task. First, diagrams are mostly abstract expressions of real world and some constituents with similar appearance may have different semantics, which makes it difficult to understand them effectively. Secondly, a long essay contains abundant and useful information for question answering, which shows that it is vital to extract the relevant information from the abundant text and then perform reasoning on it. To address the two issues, we propose a new model, Dynamic Dual Graph Networks (DDGNet), which performs question-guided multi-step reasoning on the dynamic directed Diagram Graph Network (DGN) for the diagram and Textual Graph Network (TGN) for the most related paragraph extracted from a long essay. Specifically, DGN combines text features with positional features of text boxes in the diagram as the node feature to avoid the ambiguity of visual features for the abstract constituents and help express explicit semantics. TGN uses the representation of each sentence in the most related paragraph as the node feature to learn the contextualized interaction of the useful information in the graph reasoning process. For the reasoning strategy, we propose a question-guided multi-step graph reasoning method to update both DGN and TGN dynamically under the question guidance in every step. Experimental results show that our proposed model outperforms the baselines on the TQA dataset. Moreover, extensive ablation studies are also conducted to analyze the effectiveness of our proposed model.}
}
@article{DISSANAYAKE2023109440,
title = {Multi-stage stacked temporal convolution neural networks (MS-S-TCNs) for biosignal segmentation and anomaly localization},
journal = {Pattern Recognition},
volume = {139},
pages = {109440},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109440},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001413},
author = {Theekshana Dissanayake and Tharindu Fernando and Simon Denman and Sridha Sridharan and Clinton Fookes},
keywords = {Deep learning, Electrocardiogram, Heart sounds, Lung sounds, Segmentation, Model interpretation},
abstract = {In the computer vision domain, temporal convolution networks (TCN) have gained traction due to their lightweight, robust architectures for sequence-to-sequence prediction tasks. With that insight, in this study, we propose a novel deep learning architecture for biosignal segmentation and anomaly localization based on TCNs, named the multi-stage stacked TCN, which employs multiple TCN modules with varying dilation factors. More precisely, for each stage, our architecture uses TCN modules with multiple dilation factors, and we use convolution-based fusion to combine predictions returned from each stage. Furthermore, aiming smoothed predictions, we introduce a novel loss function based on the first-order derivative. To demonstrate the robustness of our architecture, we evaluate our model on five different tasks related to three 1D biosignal modalities (heart sounds, lung sounds and electrocardiogram). Our proposed framework achieves state-of-the-art performance for all tasks, significantly outperforming the respective state-of-the-art models having F1 score gains up to ≈9%. Furthermore, the framework demonstrates competitive performance gains compared to traditional multi-stage TCN models with similar configurations yielding F1 score gains up to ≈5%. Our model is also interpretable. Using neural conductance, we demonstrate the effectiveness of having TCNs with varying dilation factors. Our visualizations show that the model benefits from feature maps captured at multiple dilation factors, and the information is effectively propagated through the network such that the final stage produces the most accurate result.}
}
@article{LIU2023109371,
title = {Capturing the few-shot class distribution: Transductive distribution optimization},
journal = {Pattern Recognition},
volume = {138},
pages = {109371},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109371},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000729},
author = {Xinyue Liu and Ligang Liu and Han Liu and Xiaotong Zhang},
keywords = {Few-shot learning, Transductive learning, Distribution estimation},
abstract = {Few-shot learning is challenging since only a few labeled samples are available for training a learning model. To alleviate the data limitation problem in few-shot learning, several works try to generate samples or features by learning a model or distribution. But complex models and biased estimation of class distribution hamper their interpretability and generalization ability, respectively. In this work, we propose a generation based Transductive Distribution Optimization (TDO) method, which introduces neither extra parameters nor complex models. We use a few labeled samples and some high-confident unlabeled samples of the target set to capture the distributions of the few-shot classes, and then generate sufficient samples from them to augment the labeled inputs. Our method can work with most pre-trained feature extractors and outperforms state-of-the-art methods with a simple linear classifier. The visualization of the generated samples shows that our method can capture an accurate distribution even though the few labeled samples deviate from the ground-truth distribution.}
}
@article{YU2023109478,
title = {Fast support vector machine training via three-term conjugate-like SMO algorithm},
journal = {Pattern Recognition},
volume = {139},
pages = {109478},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109478},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001784},
author = {Lang Yu and Shengjie Li and Siyi Liu},
keywords = {Support vector machine, Sequential minimal optimization, Three-term conjugate direction},
abstract = {Support vector machine (SVM) is an important class of methods in pattern recognition, and the sequential minimal optimization (SMO) algorithm is one of the most popular methods for training SVM at present. Based on the conjugate sequential minimal optimization algorithm (CSMO), we propose a novel three-term conjugate-like sequential minimal optimization algorithm (TCSMO) for classification and regression tasks. Compared with the CSMO, although the three-term conjugate-like SMO slightly increases the amount of arithmetic operations in each iteration, it significantly reduces the number of iterations required to converge to the specified accuracy and shortens the training time of the SVM. Additionally, we give a convergence proof of the three-term conjugate-like SMO algorithm and four new conjugate parameters. Numerical experiments show that the three-term conjugate-like SMO algorithm performs better numerically in both classification and regression tasks.}
}
@article{HUANG2023109425,
title = {NPDN-3D: A 3D neural partial differential network for spatiotemporal prediction},
journal = {Pattern Recognition},
volume = {138},
pages = {109425},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109425},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001267},
author = {Xu Huang and Shanshan Feng and Yunming Ye and Xutao Li and Bowen Zhang and Shidong Chen},
keywords = {Spatiotemporal prediction, Neural partial differential network, Machine learning, Physical dynamics},
abstract = {Neural network-based methods have been widely applied to spatiotemporal prediction tasks, such as video prediction and weather forecasting. However, most existing works are designed for prediction in 2D space, and 3D prediction has not been extensively studied. In this paper, we propose to leverage 3D partial differential equations (PDEs) for spatiotemporal prediction in 3D space, and further develop a novel 3D neural partial differential network. This is inspired by that 3D PDEs can model both horizontal and vertical information interactions by various partial derivatives. Moreover, they can also formulate physical knowledge by equations. To integrate 3D PDEs in neural networks, we first develop the theory of approximating 3D partial derivatives by 3D convolutions, and further present an effective strategy to utilize the theory in practice. Then based on the theory and strategy, we propose a novel 3D Neural Partial Differential Network for prediction, named NPDN-3D. Specifically, NPDN-3D consists of two pivotal modules: (1) a neural partial differential module for capturing low-order spatiotemporal dynamics. This module is the key for prediction, where the dynamics are formulated by commonly-used low-order 3D PDEs. (2) A residual module for capturing the remaining non-low-order dynamics. This module performs as an extensible plug-in to enhance the expressiveness of our model. Extensive experiments on two simulated datasets and two real datasets show that our method not only achieves better prediction but also learns the correct PDEs.}
}
@article{REZAEI2023109454,
title = {K-sets and k-swaps algorithms for clustering sets},
journal = {Pattern Recognition},
volume = {139},
pages = {109454},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109454},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001541},
author = {Mohammad Rezaei and Pasi Fränti},
keywords = {Clustering sets, Similarity of sets, -means, -medoids, Random swap, K-swaps, Customer segmentation, Clustering healthcare records},
abstract = {We present two new clustering algorithms called k-sets and k-swaps for data where each object is a set. First, we define the mean of the sets in a cluster, and the distance between a set and the mean. We then derive the k-sets algorithm from the principles of classical k-means so that it repeats the assignment and update steps until convergence. To the best of our knowledge, the proposed algorithm is the first k-means based algorithm for this kind of data. We adopt the idea also into random swap algorithm, which is a wrapper around the k-means that avoids local minima. This variant is called k-swaps. We show by experiments that this algorithm provides more accurate clustering results than k-medoids and other competitive methods.}
}
@article{VIRTA2023109401,
title = {Poisson PCA for matrix count data},
journal = {Pattern Recognition},
volume = {138},
pages = {109401},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109401},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001024},
author = {Joni Virta and Andreas Artemiou},
keywords = {Discrete data, Kronecker model, Matrix normal distribution, Poisson log-normal distribution},
abstract = {We develop a dimension reduction framework for data consisting of matrices of counts. Our model is based on the assumption of existence of a small amount of independent normal latent variables that drive the dependency structure of the observed data, and can be seen as the exact discrete analogue of a contaminated low-rank matrix normal model. We derive estimators for the model parameters and establish their limiting normality. An extension of a recent proposal from the literature is used to estimate the latent dimension of the model. The method is shown to outperform both its vectorization-based competitors and matrix methods assuming the continuity of the data distribution in analysing simulated data and real world abundance data.}
}
@article{WEI2023109437,
title = {Exploring the diversity and invariance in yourself for visual pre-training task},
journal = {Pattern Recognition},
volume = {139},
pages = {109437},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109437},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001383},
author = {Longhui Wei and Lingxi Xie and Wengang Zhou and Houqiang Li and Qi Tian},
keywords = {Visual pre-training, Self-supervised learning, Multi-grained visual information},
abstract = {Recently, self-supervised learning methods have achieved remarkable success in the visual pre-training task. By simply pulling the different augmented views of each image together or other novel mechanisms, they can learn much unsupervised knowledge and significantly improve the transfer performance of pre-training models. However, these works still cannot avoid the representation collapse problem, i.e., they only focus on limited regions or the extracted features on totally different regions inside each image are nearly the same. Generally, this problem makes the pre-training models cannot sufficiently describe the multi-grained information inside images, which further limits the upper bound of their transfer performance. To alleviate this issue, this paper introduces a simple but effective mechanism, called Exploring the Diversity and Invariance in Yourself (E-DIY). By simply pushing the most different regions inside each augmented view away, E-DIY can preserve the diversity of extracted region-level features. By pulling the most similar regions from different augmented views of the same image together, E-DIY can ensure the robustness of extracted region-level features. Benefiting from the above diversity and invariance exploring mechanism, E-DIY better extracts the multi-grained visual information inside each image compared with previous self-supervised learning approaches. Extensive experiments on various downstream tasks have demonstrated the superiority of our method, e.g., there is 1.9% improvement (compared with the recent state-of-the-art method, BYOL) on AP50 metric of detection task on VOC.}
}
@article{WANG2023109384,
title = {How to Reduce Change Detection to Semantic Segmentation},
journal = {Pattern Recognition},
volume = {138},
pages = {109384},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109384},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000857},
author = {Guo-Hua Wang and Bin-Bin Gao and Chengjie Wang},
keywords = {Change detection, Semantic segmentation, Feature fusion},
abstract = {Change detection (CD) aims to identify changes that occur in an image pair taken different times. Prior methods devise specific networks from scratch to predict change masks in pixel-level, and struggle with general segmentation problems. In this paper, we propose a new paradigm that reduces CD to semantic segmentation which means tailoring an existing and powerful semantic segmentation network to solve CD. This new paradigm conveniently enjoys the mainstream semantic segmentation techniques to deal with general segmentation problems in CD. Hence we can concentrate on studying how to detect changes. We propose a novel and importance insight that different change types exist in CD and they should be learned separately. Based on it, we devise a module named MTF to extract the change information and fuse temporal features. MTF enjoys high interpretability and reveals the essential characteristic of CD. And most segmentation networks can be adapted to solve the CD problems with our MTF module. Finally, we propose C-3PO, a network to detect changes at pixel-level. C-3PO achieves state-of-the-art performance without bells and whistles. It is simple but effective and can be considered as a new baseline in this field. Our code for C-3PO is available at https://github.com/DoctorKey/C-3PO.}
}
@article{GHALYAN2023109482,
title = {Capacitive empirical risk function-based bag-of-words and pattern classification processes},
journal = {Pattern Recognition},
volume = {139},
pages = {109482},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109482},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001826},
author = {Ibrahim F. Ghalyan},
keywords = {Bag-of-words, Classification, Fuzzy measure, Generalization, Statistical modeling},
abstract = {This paper proposes capacitive bag-of-words (Cap BoW) modeling and capacitive pattern classification processes for improved generalization ability. The Cap BoW and capacitive pattern classification are realized by introducing the notion of the capacitive empirical risk function (CERF). The CERF is used as a cost function to build a capacitive pattern classification process. The resulted capacitive pattern classification is used in the classification stage of the bag-of-words process, realizing thereby the Cap BoW process. The use of the CERF in building the capacitive pattern classification and Cap BoW processes is demonstrated to achieve simultaneous reduction to the empirical risk function (ERF) and confidence interval, reducing potential overfitting and enhancing the generalization ability of considered models. To have a thorough evaluation, two groups of experiments are carried out. The first group addresses the capacitive pattern classification process for 4 datasets. The second group addresses a more holistic impact of the CERF by using Cap Bow model to build capacitive dual ergodicity limits-based bag-of-words (Cap DEL-BoW) process, which is applied to 5 image datasets. In both groups of experiments, remarkable enhancement is demonstrated with the use of CERF-based pattern classification and Cap BoW processes. Comparison with corresponding conventional non-capacitive models demonstrates the tangible enhancement with the use of CERF-based models.}
}
@article{JIANG2023109429,
title = {Aggregated pyramid gating network for human pose estimation without pre-training},
journal = {Pattern Recognition},
volume = {138},
pages = {109429},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109429},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001309},
author = {Chenru Jiang and Kaizhu Huang and Shufei Zhang and Xinheng Wang and Jimin Xiao and Yannis Goulermas},
keywords = {Pyramid gating system, Stabilization, Human pose estimation},
abstract = {In this work, we propose a comprehensive aggregated residual gating structure, the Pyramid GAting Network (PGA-Net) for human pose estimation which can select, distill, and fuse semantic level and natural level information from multiple scales. In comparison, through utilizing multi-scale features, most existing state-of-the-art pose estimation methods are still limited in three aspects. First, multi-scale features contain massively redundant information, which is unfortunately not distilled by most existing approaches. Second, preferring deeper network structures to extract strong semantic features, the conventional methods often ignore original texture information fusion. Third, to attain a good parameter initialization, the current methods heavily rely on pre-training, which is very time-consuming or even unavailable. While better coping with the above problems, our proposed PGA-Net distills high-level semantic features and replenishes low-level original information to reinforce module representation capability. Meanwhile, PGA-Net demonstrates notable training stability and superior performance even without pre-training. Extensive experiments demonstrate that our method consistently outperforms previous approaches even without pre-training, enabling thus an end-to-end model training from scratch. In COCO benchmark, PGA-Net consistently achieves over 3% improvements than the baseline (without pre-training) under various model configurations.11The code is released at https://github.com/ssr0512/PGA-Net}
}
@article{GUPTA2023109453,
title = {A survey of human-computer interaction (HCI) & natural habits-based behavioural biometric modalities for user recognition schemes},
journal = {Pattern Recognition},
volume = {139},
pages = {109453},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109453},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300153X},
author = {Sandeep Gupta and Carsten Maple and Bruno Crispo and Kiran Raja and Artsiom Yautsiukhin and Fabio Martinelli},
keywords = {Internet of Things (IoT), User recognition, Behavioural biometrics, Secutity, Privacy, Usability},
abstract = {The proliferation of Internet of Things (IoT) systems is having a profound impact across all aspects of life. Recognising and identifying particular users is central to delivering the personalised experience that citizens want to experience, and that organisations wish to deliver. This article presents a survey of human-computer interaction-based (HCI-based) and natural habits-based behavioural biometrics that can be acquired unobtrusively through smart devices or IoT sensors for user recognition purposes. Robust and usable user recognition is also a security requirement for emerging IoT ecosystems to protect them from adversaries. Typically, it can be specified as a fundamental building block for most types of human-to-things accountability principles and access-control methods. However, end-users are facing numerous security and usability challenges in using currently available knowledge- and token-based recognition (i.e., authentication and identification) schemes. To address the limitations of conventional recognition schemes, biometrics, naturally come as a first choice to supporting sophisticated user recognition solutions. We perform a comprehensive review of touch-stroke, swipe, touch signature, hand-movements, voice, gait and footstep behavioural biometrics modalities. This survey analyzes the recent state-of-the-art research of these behavioural biometrics with a goal to identify their attributes and features for generating unique identification signatures. Finally, we present security, privacy, and usability evaluations that can strengthen the designing of robust and usable user recognition schemes for IoT applications.}
}
@article{ZHANG2023109442,
title = {Efficient large-scale oblique image matching based on cascade hashing and match data scheduling},
journal = {Pattern Recognition},
volume = {138},
pages = {109442},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109442},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001425},
author = {Qiyuan Zhang and Shunyi Zheng and Ce Zhang and Xiqi Wang and Rui Li},
keywords = {Oblique image matching, Feature point matching, SIFT, Cascade hashing, Match data scheduling, Structure from motion},
abstract = {In this paper, we design an efficient large-scale oblique image matching method. First, to reduce the number of redundant transmissions of match data, we propose a novel three-level buffer data scheduling (TLBDS) algorithm that considers the adjacency between images for match data scheduling from disk to graphics memory. Second, we adopt the epipolar constraint to filter the initial candidate points of cascade hashing matching, thereby significantly increasing the robustness of matching feature points. Comprehensive experiments are conducted on three oblique image datasets to test the efficiency and effectiveness of the proposed method. The experimental results show that our method can complete a match pair within 2.50∼2.64 ms, which not only is much faster than two open benchmark pipelines (i.e., OpenMVG and COLMAP) by 20.4∼97.0 times but also have higher efficiency than two state-of-the-art commercial software (i.e., Agisoft Metashape and Pix4Dmapper) by 10.4∼50.0 times.}
}
@article{WEI2023109466,
title = {Adversarial pan-sharpening attacks for object detection in remote sensing},
journal = {Pattern Recognition},
volume = {139},
pages = {109466},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109466},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001668},
author = {Xingxing Wei and Maoxun Yuan},
keywords = {Adversarial pan-sharpening, Remote sensing, Object detection},
abstract = {Pan-sharpening, as one of the most commonly used techniques in remote sensing systems, aims to fuse texture-rich PAN images and multi-spectral MS images to obtain texture-rich MS images. With the development of deep learning, CNN based Pan-sharpening methods have received more and more attention in recent years. Since Pan-sharpening technique can integrate the complementary information of Pan and MS images, researchers usually apply object detectors on these pan-sharpened images to achieve reliable detection results. However, recent studies have shown that Deep Learning-based object detection methods are vulnerable to adversarial examples, i.e., adding imperceptible noise to clean images can fool well-trained deep neural networks. It is interesting to combine the pan-sharpening technique with adversarial examples to attack object detectors in remote sensing. In this paper, we propose a framework to generate adversarial pan-sharpened images. Specifically, we propose a two-stream network to generate the pan-sharpened images, and then utilize the shape loss and label loss to perform the attack task. To guarantee the quality of pan-sharpened images, a perceptual loss is utilized to balance spectral preservation and attacking performance. Experimental results demonstrate that the proposed method can generate effective adversarial pan-sharpened images that maintain a high success rate for white-box attacks and achieve transferability for black-box attacks.}
}
@article{FERNANDEZGARCIA2023109396,
title = {Assessing polygonal approximations: A new measurement and a comparative study},
journal = {Pattern Recognition},
volume = {138},
pages = {109396},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109396},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000973},
author = {Nicolás Luis Fernández-García and Luis Del-Moral Martínez and Ángel Carmona-Poyato and Francisco José Madrid-Cuevas and Rafael Medina-Carnicer},
keywords = {2D Closed curve, Contour, Polygonal approximation, Performance evaluation},
abstract = {Two proposals related to the evaluation of polygonal approximations are presented in this document. First, a new measurement, called normalized compression ratio and adjustment error (NCA), to provide a fair evaluation of the performance of the polygonal approximations of 2D closed curves is proposed. Second, a new methodology for evaluation of measurements for assessing polygonal approximations is also proposed. This methodology is based on the optimal quality curve concept, which can characterize the performance of the measurements. A simple visual analysis of the optimal quality curve allows possible drawbacks or weaknesses of the measurement to be detected. The new evaluation methodology is used to compare the performance of the proposed NCA and the most popular measurements, such as Rosin’s Merit, FOM or versions of FOM. Experiments show that NCA obtains the best results and, therefore, may be used to fairly evaluate the performance of polygonal approximations.}
}
@article{ZHA2023109459,
title = {Conditional invertible image re-scaling},
journal = {Pattern Recognition},
volume = {139},
pages = {109459},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109459},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001590},
author = {Yufei Zha and Fan Li and Peng Zhang and Wei Huang},
keywords = {Invertible neural network, Image re-scaling, Information theory},
abstract = {One of the key issue for image re-scaling is modeling the down-sampling loss. The loss can be approached by a prior distribution through an invertible network in recent works. However, this assumption of independence between images and down-sampling losses is not always satisfied in practice. To address the above problem, we design a module to learn a latent variable representing the down-scaling loss conditional on low-resolution(LR) images according to the information entropy theory. Unlike previous methods, the down-scaling loss is recovered through the proposed module whose inputs are both LR images and predefined distributions. The difference between the high-resolution(HR) image and the low-resolution(LR) image is represented with a latent variable through a lossless invertible neural network. In addition, a conditional entropy loss is proposed to train the invertible neural network to suppress the conditional entropy between LR images and latent variables. It helps to decrease the effect of the latent variable to generate the HR image from LR images during the up-scaling procedure. We evaluate the proposed method on widely used data sets, and the experimental results demonstrate that our proposed method performs favorably against SOTA methods in terms of PSNR and SSIM metrics.}
}
@article{ZHAO2023109406,
title = {Density peaks clustering algorithm based on fuzzy and weighted shared neighbor for uneven density datasets},
journal = {Pattern Recognition},
volume = {139},
pages = {109406},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109406},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001073},
author = {Jia Zhao and Gang Wang and Jeng-Shyang Pan and Tanghuai Fan and Ivan Lee},
keywords = {Uneven density data, Density peaks clustering, Fuzzy neighborhood, K-nearest neighbor, Weighted shared neighbor},
abstract = {Uneven density data refers to data with a certain difference in sample density between clusters. The local density of density peaks clustering algorithm (DPC) does not consider the effect of sample density difference between clusters of uneven density data, which may lead to wrong selection of cluster centers; the algorithm allocation strategy makes it easy to incorrectly allocate samples originally belonging to sparse clusters to dense clusters, which reduces clustering efficiency. In this study, we proposed the density peaks clustering algorithm based on fuzzy and weighted shared neighbor for uneven density datasets (DPC-FWSN). First, a nearest neighbor fuzzy kernel function is obtained by combining K-nearest neighbor and fuzzy neighborhood. Then, local density is redefined by the nearest neighbor fuzzy kernel function. The local density can better characterize the distribution characteristics of the sample by balancing the contribution of sample density in dense and sparse areas, in order to avoid the situation that the sparse cluster does not have a cluster center. Finally, the allocation strategy for weighted shared neighbor similarity is proposed to optimize the sample allocation at the boundary of the sparse cluster. Experiments are performed on IDPC-FA, FKNN-DPC, FNDPC, DPCSA and DPC for uneven density datasets, complex morphologies datasets and real datasets. The clustering results demonstrate that DPC-FWSN effectively handles datasets with uneven density distribution.}
}
@article{BORAL2023109447,
title = {MEQA: Manifold embedding quality assessment via anisotropic scaling and Kolmogorov-Smirnov test},
journal = {Pattern Recognition},
volume = {139},
pages = {109447},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109447},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001474},
author = {Subhadip Boral and Mainak Sarkar and Ashish Ghosh},
keywords = {Manifold learning, Anisotropic scaling, Gradient descent, Global scaling, Singular value decomposition, Kolmogorov-Smirnov test},
abstract = {Manifold learning methods unfold the manifold structures and embed them in a lower-dimensional space. The quality of such an embedding should be measured both qualitatively and quantitatively. The proposed manifold embedding quality assessment (MEQA) method does so by taking into account of local and global structure preservation as both are important traits of an embedding. To measure the local structure preservation MEQA uses two transformations. Initially anisotropic scaling, rotation and translation are incorporated to measure the closeness between the original and the embedded data points. In the next stage, rigid transformation is incorporated to quantify the previous transformation which involved anisotropic scaling. For quantifying the global structure preservation, the Kolmogorov-Smirnov test is applied in a distributed manner over each dimension and averaged over them. To establish the superiority of MEQA we conducted several studies over standard synthetic and real-life datasets across separate existing feature extraction techniques.}
}
@article{YULIN2023109422,
title = {BEST: Building evidences from scattered templates for accurate contactless palmprint recognition},
journal = {Pattern Recognition},
volume = {138},
pages = {109422},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109422},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001231},
author = {Feng Yulin and Ajay Kumar},
keywords = {Contactless palmprint matching, Cross-sensor palmprint matching, Personal identification, Lightweight Convolutional Neural Networks,, Information security, Biometrics},
abstract = {Contactless palmprint identification offers significantly improved hygiene and user convenience, making it highly attractive for a range of civilian applications, especially during the current pandemic. However, the accurate recognition of contactless palmprint images can be highly challenging, attributed to the significant variations in the intra-class similarity and limitations of conventional palmprint feature descriptors under involuntary or contactless imaging variations. State-of-the-art completely contactless palmprint matching algorithms in the literature cannot adequately address these challenges and are not sufficiently accurate and fast enough for such real-world applications. This paper proposes a novel approach that adaptively locates the local palmprint regions with high similarities between their corresponding feature representations or templates to address these challenges. We consider spatial localization of such highly similar feature representations from multiple local regions and consolidate them to generate a more reliable match score. This paper presents reproducible and comparative experimental results, using within-database, cross-database, and cross-sensor performance evaluation, on four publicly available contactless palmprint datasets, including a sizeable contactless palmprint database from 600 different subjects. The proposed method achieves outperforming results compared with three state-of-the-art deep learning-based methods and five widely used conventional methods. In addition, the proposed method is also significantly faster than all state-of-the-art baseline methods.}
}
@article{SHU2023109475,
title = {Localized curvature-based combinatorial subgraph sampling for large-scale graphs},
journal = {Pattern Recognition},
volume = {139},
pages = {109475},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109475},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001759},
author = {Dong Wook Shu and Youjin Kim and Junseok Kwon},
keywords = {Subgraph sampling method, Discrete Ricci curvature},
abstract = {This paper introduces a subgraph sampling method based on curvature to train large-scale graphs via mini-batch training. Owing to the difficulty in sampling globally optimal subgraphs from large graphs, we sample the subgraphs to minimize the distributional metric with combinatorial sampling. In particular, we define a combinatorial metric that distributionally measures the similarity between an original graph and all possible node and edge combinations of the subgraphs. Further, we prove that the subgraphs sampled using the probability model proportional to the discrete Ricci curvature (i.e., Ollivier-Ricci curvatures) of the edges can minimize the proposed metric. Moreover, as accurate calculation of the curvature on a large graph is challenging, we propose to use a localized curvature considering only 3-cycles on the graph, suggesting that this is a sufficiently approximated curvature on a sparse graph. We also show that the probability models of conventional sampling methods are related to coarsely approximated curvatures with no cycles, implying that the curvature is closely related to subgraph sampling. The experimental results confirm the feasibility of integrating the proposed curvature-based sampling method into existing graph neural networks to improve performance.}
}
@article{SUNKARA2023109451,
title = {YOGA: Deep object detection in the wild with lightweight feature learning and multiscale attention},
journal = {Pattern Recognition},
volume = {139},
pages = {109451},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109451},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001516},
author = {Raja Sunkara and Tie Luo},
abstract = {We introduce YOGA, a deep learning based yet lightweight object detection model that can operate on low-end edge devices while still achieving competitive accuracy. The YOGA architecture consists of a two-phase feature learning pipeline with a cheap linear transformation, which learns feature maps using only half of the convolution filters required by conventional convolutional neural networks. In addition, it performs multi-scale feature fusion in its neck using an attention mechanism instead of the naive concatenation used by conventional detectors. YOGA is a flexible model that can be easily scaled up or down by several orders of magnitude to fit a broad range of hardware constraints. We evaluate YOGA on COCO-val and COCO-testdev datasets with over 10 state-of-the-art object detectors. The results show that YOGA strikes the best trade-off between model size and accuracy (up to 22% increase of AP and 23–34% reduction of parameters and FLOPs), making it an ideal choice for deployment in the wild on low-end edge devices. This is further affirmed by our hardware implementation and evaluation on NVIDIA Jetson Nano.}
}
@article{LI2023109421,
title = {Non-contact PPG signal and heart rate estimation with multi-hierarchical convolutional network},
journal = {Pattern Recognition},
volume = {139},
pages = {109421},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109421},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300122X},
author = {Bin Li and Panpan Zhang and Jinye Peng and Hong Fu},
keywords = {Remote heart rate measurement, Multi-hierarchical feature fusion, Spatio-temporal convolution},
abstract = {Heartbeat rhythm and heart rate (HR) are important physiological parameters of the human body. This study presents an efficient multi-hierarchical spatio-temporal convolutional network that can quickly estimate remote physiological (rPPG) signal and HR from face video clips. First, the facial color distribution characteristics are extracted using a low-level face feature generation (LFFG) module. Then, the three-dimensional (3D) spatio-temporal stack convolution module (STSC) and multi-hierarchical feature fusion module (MHFF) are used to strengthen the spatio-temporal correlation of multi-channel features. In the MHFF, sparse optical flow is used to capture the tiny motion information of faces between frames and generate a self-adaptive region of interest (ROI) skin mask. Finally, the signal prediction module (SP) is used to extract the estimated rPPG signal. The heart rate estimation results show that the proposed network overperforms the state-of-the-art methods on three datasets, 1) UBFC-RPPG, 2) COHFACE, 3) our dataset, with the mean absolute error (MAE) of 2.15, 5.57, 1.75 beats per minute (bpm) respectively.}
}
@article{LI2023109381,
title = {Deep metric learning for few-shot image classification: A Review of recent developments},
journal = {Pattern Recognition},
volume = {138},
pages = {109381},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109381},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000821},
author = {Xiaoxu Li and Xiaochen Yang and Zhanyu Ma and Jing-Hao Xue},
keywords = {Few-shot learning, Metric learning, Image classification, Deep neural networks},
abstract = {Few-shot image classification is a challenging problem that aims to achieve the human level of recognition based only on a small number of training images. One main solution to few-shot image classification is deep metric learning. These methods, by classifying unseen samples according to their distances to few seen samples in an embedding space learned by powerful deep neural networks, can avoid overfitting to few training images in few-shot image classification and have achieved the state-of-the-art performance. In this paper, we provide an up-to-date review of deep metric learning methods for few-shot image classification from 2018 to 2022 and categorize them into three groups according to three stages of metric learning, namely learning feature embeddings, learning class representations, and learning distance measures. Under this taxonomy, we identify the trends of transitioning from learning task-agnostic features to task-specific features, from simple computation of prototypes to computing task-dependent prototypes or learning prototypes, from using analytical distance or similarity measures to learning similarities through convolutional or graph neural networks. Finally, we discuss the current challenges and future directions of few-shot deep metric learning from the perspectives of effectiveness, optimization and applicability, and summarize their applications to real-world computer vision tasks.}
}
@article{CHIEN2023109463,
title = {Bayesian asymmetric quantized neural networks},
journal = {Pattern Recognition},
volume = {139},
pages = {109463},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109463},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001632},
author = {Jen-Tzung Chien and Su-Ting Chang},
keywords = {Quantized neural network, Model compression, Binary neural network, Bayesian asymmetric quantization},
abstract = {This paper develops a robust model compression for neural networks via parameter quantization. Traditionally, quantized neural networks (QNN) were constructed by binary or ternary weights where the weights were deterministic. This paper generalizes QNN in two directions. First, M-ary QNN is developed to adjust the balance between memory storage and model capacity. The representation values and the quantization partitions in M-ary quantization are mutually estimated to enhance the resolution of gradients in neural network training. A flexible quantization with asymmetric partitions is formulated. Second, the variational inference is incorporated to implement the Bayesian asymmetric QNN. The uncertainty of weights is faithfully represented to enhance the robustness of the trained model in presence of heterogeneous data. Importantly, the multiple spike-and-slab prior is proposed to represent the quantization levels in Bayesian asymmetric learning. M-ary quantization is then optimized by maximizing the evidence lower bound of classification network. An adaptive parameter space is built to implement Bayesian quantization and neural representation. The experiments on various image recognition tasks show that M-ary QNN achieves similar performance as the full-precision neural network (FPNN), but the memory cost and the test time are significantly reduced relative to FPNN. The merit of Bayesian M-ary QNN using multiple spike-and-slab prior is investigated.}
}
@article{LU2023109434,
title = {Intensity mixture and band-adaptive detail fusion for pansharpening},
journal = {Pattern Recognition},
volume = {139},
pages = {109434},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109434},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001358},
author = {Hangyuan Lu and Yong Yang and Shuying Huang and Xiaolong Chen and Hongfu Su and Wei Tu},
keywords = {Pansharpening, Intensity mixture, Band-adaptive detail fusion, Point spread function},
abstract = {Pansharpening aims to sharpen a low-resolution multispectral (MS) image through a high-resolution single-channel panchromatic (PAN) image to obtain a high-resolution multi-spectral (HRMS) image. However, low correlation between the PAN and MS images, as well as the inaccurate detail injection for each band of MS image are the key problems causing spectral and spatial distortions in pansharpening. To address these issues, a new pansharpening method based on the intensity mixture and band-adaptive detail fusion is proposed. To obtain a mixed-intensity image (T) that has a high correlation with the MS image and maintain the gradient information of the PAN image, the intensity mixture model is constructed by establishing the intensity and gradient constraints between T and the source images. As it is hard to obtain a proper degradation filter in the model, a filter estimation algorithm is designed by the distribution alignment. To inject the details that match the point spread function of the sensor, a band-adaptive detail fusion algorithm is presented to fuse the details extracted from T with those from the MS image for each band. Furthermore, as there are far fewer details in the MS image than in T, a detail enhancement algorithm is proposed to enhance the details proportionally. The final HRMS image is obtained by injecting the fused details into the upsampled MS image. Extensive experiments show that the proposed method can efficiently achieve the best results in fusion quality compared to state-of-the-art methods. The code is availabe at https://github.com/yotick/IMBD.}
}
@article{LEE2023109380,
title = {Robust spherical principal curves},
journal = {Pattern Recognition},
volume = {138},
pages = {109380},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109380},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300081X},
author = {Jongmin Lee and Hee-Seok Oh},
keywords = {Dimension reduction, Robustness, Measure of central tendency, Spherical domain},
abstract = {Principal curves are a nonlinear generalization of principal components and go through the mean of data lying in Euclidean space. In this paper, we propose L1-type and Huber-type principal curves through the median of data to robustify the principal curves for a dataset that may contain outliers. We further investigate the stationarity of the proposed robust principal curves on S2. Results from numerical experiments on S2 and S4, including real data analysis, manifest promising empirical features of the proposed method.}
}
@article{XI2023109476,
title = {TreeNet: Structure preserving multi-class 3D point cloud completion},
journal = {Pattern Recognition},
volume = {139},
pages = {109476},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109476},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001760},
author = {Long Xi and Wen Tang and TaoRuan Wan},
keywords = {3D Point cloud completion, Multi-class training, Hierarchical tree, Computer vision, Artificial intelligence, Deep learning},
abstract = {Generating the missing data of 3D object point clouds from partial observations is a challenging task. Existing state-of-the-art learning-based 3D point cloud completion methods tend to use a limited number of categories/classes of training data and regenerate the entire point cloud based on the training datasets. As a result, output 3D point clouds generated by such methods may lose details (i.e. sharp edges and topology changes) due to the lack of multi-class training. These methods also lose the structural and spatial details of partial inputs due to the models do not separate the reconstructed partial input from missing points in the output. In this paper, we propose a novel deep learning network - TreeNet for 3D point cloud completion. TreeNet has two networks in hierarchical tree-based structures: TreeNet-multiclass focuses on multi-class training with a specific class of the completion task on each sub-tree to improve the quality of point cloud output; TreeNet-binary focuses on generating points in missing areas and fully preserving the original partial input. TreeNet-multiclass and TreeNet-binary are both network decoders and can be trained independently. TreeNet decoder is the combination of TreeNet-multiclass and TreeNet-binary and is trained with an encoder from existing methods (i.e. PointNet encoder). We compare the proposed TreeNet with five state-of-the-art learning-based methods on fifty classes of the public Shapenet dataset and unknown classes, which shows that TreeNet provides a significant improvement in the overall quality and exhibits strong generalization to unknown classes that are not trained.}
}
@article{LI2023109423,
title = {Dynamic graph structure learning for multivariate time series forecasting},
journal = {Pattern Recognition},
volume = {138},
pages = {109423},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109423},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001243},
author = {Zhuo Lin Li and Gao Wei Zhang and Jie Yu and Ling Yu Xu},
keywords = {Multivariate time series forecasting, Dynamic spatio-temporal dependencies, Graph neural networks, Long- and short-term patterns, Graph structure learning},
abstract = {Multivariate time series forecasting is a challenging task because the dynamic spatio-temporal dependencies between variables are a combination of multiple unknown association patterns. Existing graph neural networks typically model multivariate relationships with a predefined spatial graph or a learned fixed adjacency graph, which fails to handle the aforementioned challenges. In this study, we decompose association patterns into stable long-term and dynamic short-term patterns and propose a novel framework, named the static and dynamic graph learning network (SDGL), for modeling unknown patterns. Our approach infers two types of graph structures, from the data simultaneously: static and dynamic graphs. A static graph is developed to capture the fixed long-term pattern via node embedding, and we leverage graph regularity to control its learning direction. Dynamic graphs, which are time-varying matrices based on changing node-level features, are used to model dynamic dependencies over the short term. To effectively capture local dynamic patterns, we integrate the learned long-term pattern as an inductive bias. Experiments on six benchmark datasets show the state-of-the-art performance of our method. Analysis of the learned graphs reveals that the model succeeds in modeling dynamic spatio-temporal dependencies.}
}
@article{LUO2023109448,
title = {Dual-channel graph contrastive learning for self-supervised graph-level representation learning},
journal = {Pattern Recognition},
volume = {139},
pages = {109448},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109448},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001486},
author = {Zhenfei Luo and Yixiang Dong and Qinghua Zheng and Huan Liu and Minnan Luo},
keywords = {Contrastive learning, Graph representation learning, Graph neural networks, Graph classification},
abstract = {Self-supervised graph-level representation learning aims to learn discriminative representations for subgraphs or entire graphs without human-curated labels. Recently, graph contrastive learning (GCL) methods have revolutionized this field and achieved state-of-the-art results in various downstream tasks. Nonetheless, current GCL models are mostly based on simple node-level information aggregation operations and fail to reveal various substructures from input graphs. Moreover, to perform graph-graph contrastive training, they often involve well-designed graph augmentation, which is expensive and requires extensive expert efforts. Here, we propose a novel GCL framework, namely DualGCL, for self-supervised graph-level representation learning. For fine-grained local information incorporation, we first present an adaptive hierarchical aggregation process with a differentiable Transformer-based aggregator. Then, to efficiently learn graph-level discriminative representations, we introduce a dual-channel contrastive learning process in a multi-granularity and augmentation-free contrasting mode. When tested empirically on six popular graph classification benchmarks, our DualGCL achieves better or comparable performance than various strong baselines.}
}
@article{LIU2023109418,
title = {Mitigate the classification ambiguity via localization-classification sequence in object detection},
journal = {Pattern Recognition},
volume = {138},
pages = {109418},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109418},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300119X},
author = {Chang Liu and Shaorong Xie and Xiaomao Li and Jiantao Gao and Weiping Xiao and Baojie Fan and Yan Peng},
keywords = {Object detection, Classification ambiguity, Refinement-aware classification},
abstract = {In anchor-based detectors, the confidence scores and label-assignment results for the classification task are determined by the unrefined anchors rather than the final-refined boxes, which causes classification ambiguity due to the lack of correlation between the classification and localization tasks. In this paper, we investigate the classification ambiguity thoroughly via extensive experiments, and present the localization-classification sequence detector (LCSDet) that performs localization and classification in order, bridging the gap between them. To achieve this, the refinement-aware (RA) classification branch and RA assignment are proposed in LCSDet. In inference, the RA classification branch rectifies the feature misalignment and directly classifies the refined anchors. During training, the RA assignment tackles the training instability, narrows the location-quality gap and assigns the refined anchors to ground-truth objects. Comprehensive experiments indicate that the LCSDet can effectively mitigate the classification ambiguity and achieve stable improvement across different baselines.}
}
@article{SUN2023109399,
title = {MUNet: Motion uncertainty-aware semi-supervised video object segmentation},
journal = {Pattern Recognition},
volume = {138},
pages = {109399},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109399},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001000},
author = {Jiadai Sun and Yuxin Mao and Yuchao Dai and Yiran Zhong and Jianyuan Wang},
keywords = {Video object segmentation, Uncertainty, Motion estimation, Self-supervised},
abstract = {The task of semi-supervised video object segmentation (VOS) has been greatly advanced and state-of-the-art performance has been made by dense matching-based methods. The recent methods leverage space-time memory (STM) networks and learn to retrieve relevant information from all available sources, where the past frames with object masks form an external memory and the current frame as the query is segmented using the mask information in the memory. However, when forming the memory and performing matching, these methods only exploit the appearance information while ignoring the motion information. In this paper, we advocate for the return of the motion information and propose a motion uncertainty-aware framework (MUNet) for semi-supervised VOS. First, we propose an implicit method to learn the spatial correspondences between neighboring frames, building upon a correlation cost volume. To handle the challenging cases of occlusion and textureless regions during constructing dense correspondences, we incorporate the uncertainty in dense matching and achieve motion uncertainty-aware feature representation. Second, we introduce a motion-aware spatial attention module to effectively fuse the motion features with the semantic features. Comprehensive experiments on challenging benchmarks show that using a small amount of data and combining it with powerful motion information can bring a significant performance boost. We achieve 76.5% J&F only using DAVIS17 for training22This result is initialized by the Mask-RCNN-ResNet50 weights pre-trained on COCO dataset. By initialization from ResNet50 pre-trained on ImageNet dataset, we can achieve 75.0% J&F, which is still the SOTA performance., which significantly outperforms the SOTA methods under the low-data protocol. The code and supplementary materials will be available at https://npucvr.github.io/MUNet.}
}
@article{ZHANG2023109378,
title = {MFSJMI: Multi-label feature selection considering join mutual information and interaction weight},
journal = {Pattern Recognition},
volume = {138},
pages = {109378},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109378},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000791},
author = {Ping Zhang and Guixia Liu and Jiazhi Song},
keywords = {Multi-label learning, Multi-label feature selection, Information theory, Underlying assumptions},
abstract = {Multi-label feature selection captures a reliable and informative feature subset from high-dimensional multi-label data, which plays an important role in pattern recognition. In conventional information-theoretical based multi-label feature selection methods, the high-order feature relevance between feature and label set is evaluated using low-order mutual information. However, existing methods do not establish the theoretical basis for the low-order approximation. To fill this gap, we first identify two underlying assumptions based on high-order label distribution: Label Independence Assumption (LIA) and Paired-label Independence Assumption (PIA). Second, we systematically analyze the strengths and weaknesses of two assumptions and introduce joint mutual information to satisfy more realistic label distribution. Furthermore, by decomposing joint mutual information, an interaction weight is proposed to consider multiple label correlations. Finally, a new method considering join mutual information and interaction weight is proposed. Comprehensive experiments demonstrate the effectiveness of the proposed method on various evaluation metrics.}
}
@article{LI2023109452,
title = {Semantic-based conditional generative adversarial hashing with pairwise labels},
journal = {Pattern Recognition},
volume = {139},
pages = {109452},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109452},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001528},
author = {Qi Li and Weining Wang and Yuanyan Tang and Chengzhong Xu and Zhenan Sun},
keywords = {Generative adversarial networks, Semantic-based conditional information, Hashing with pairwise labels},
abstract = {Hashing has been widely exploited in recent years due to the rapid growth of image and video data on the web. Benefiting from recent advances in deep learning, deep hashing methods have achieved promising results with supervised information. However, it is usually expensive to collect the supervised information. In order to utilize both labeled and unlabeled data samples, many semi-supervised hashing methods based on Generative Adversarial Networks (GANs) have been proposed. Most of them still need the conditional information, which is usually generated by the pre-trained neural networks or leveraging random binary vectors. One natural question about these methods is that how can we generate a better conditional information given the semantic similarity information? In this paper, we propose a general two-stage conditional GANs hashing framework based on the pairwise label information. Both the labeled and unlabeled data samples are exploited to learn hash codes under our framework. In the first stage, the conditional information is generated via a general Bayesian approach, which has a much lower dimensional representation and maintains the semantic information of original data samples. In the second stage, a semi-supervised approach is presented to learn hash codes based on the conditional information. Both pairwise based cross entropy loss and adversarial loss are introduced to make full use of labeled and unlabeled data samples. Extensive experiments have shown that the propose algorithm outperforms current state-of-the-art methods on three benchmark image datasets, which demonstrates the effectiveness of our method.}
}
@article{NIU2023109382,
title = {Defense Against Adversarial Attacks with Efficient Frequency-Adaptive Compression and Reconstruction},
journal = {Pattern Recognition},
volume = {138},
pages = {109382},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109382},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000833},
author = {Zhong-Han Niu and Yu-Bin Yang},
keywords = {Deep neural networks, Adversarial defense, Adversarial robustness, Closed-set attack, Open-set attack},
abstract = {The increasing use of deep neural networks exposes themselves to adversarial attacks in the real world drawn from closed-set and open-set, which poses great threats to their application in safety-critical systems. Since adversarial attacks tend to mislead an original model by adding small perturbations into clean images, an intuitive idea of defensing adversarial attacks is eliminating perturbations as much as possible to mitigate attacking effects. However, such elimination-based strategies unfortunately fail to achieve satisfactory robustness. Aiming to investigate the intrinsic reasons for this phenomenon, systematic experiments are carried out in this paper to indicate that even a 20% residual perturbation can still preserve and exhibit attacking effects as strong as a full one. Our study also indicates that there are strong correlations between perturbations and legitimate images. Thus, breaking the correlation across multiple bands is more effective in mitigating attacking effects. Based on these findings, this paper proposes an efficient defense strategy called “Frequency-Adaptive Compression and rEconstruction (FACE)” to improve the robustness of the model to adversarial attacks. Specifically, low-frequency bands containing semantic information are compressed by a down-sampling operation, while the channel width of high-frequency bands is squeezed and further compressed by adding noise before the Tanh activating function. Meanwhile, attachment spaces of perturbations are also squeezed to the extent as much as possible. Finally, a clean output is obtained by upsampling together with expanded reconstruction. Experiments are extensively conducted on widely used datasets to demonstrate the effectiveness of the proposed method. For closed-set attacks, FACE outperforms the STOA elimination-based methods on ImageNet, achieving a 27.9% improvement. For the MNIST open-set attacks, it not only reduces the success rate of targeted attack by a large margin (from 100% to 24.7%), but also mitigates attacking effects with an FPR-95 value of 0.3.}
}
@article{LU2023109480,
title = {A survey on machine learning from few samples},
journal = {Pattern Recognition},
volume = {139},
pages = {109480},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109480},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001802},
author = {Jiang Lu and Pinghua Gong and Jieping Ye and Jianwei Zhang and Changshui Zhang},
keywords = {Few sample learning, Learn to learn, Survey, Few-shot learning, Meta learning},
abstract = {The capability of learning and generalizing from very few samples successfully is a noticeable demarcation separating artificial intelligence and human intelligence. Despite the long history dated back to the early 2000s and the widespread attention in recent years with booming deep learning, few surveys for few sample learning (FSL) are available. We extensively study almost all papers of FSL spanning from the 2000s to now and provide a timely and comprehensive survey for FSL. In this survey, we review the evolution history and current progress on FSL, categorize FSL approaches into the generative model based and discriminative model based kinds in principle, and emphasize particularly on the meta learning based FSL approaches. We also summarize several recently emerging extensional topics of FSL and review their latest advances. Furthermore, we highlight the important FSL applications covering many research hotspots in computer vision, natural language processing, audio and speech, reinforcement learning and robotic, data analysis, etc. Finally, we conclude the survey with a discussion on promising trends in the hope of providing guidance and insights to follow-up researches.}
}
@article{DONG2023109488,
title = {Class-incremental object detection},
journal = {Pattern Recognition},
volume = {139},
pages = {109488},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109488},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001887},
author = {Na Dong and Yongqiang Zhang and Mingli Ding and Yancheng Bai},
keywords = {Class-incremental learning, Object detection, Information asymmetry, Non-affection distillation, Deep learning},
abstract = {Deep learning architectures have shown remarkable results in the object detection task. However, they experience a critical performance drop when they are required to learn new classes incrementally without forgetting old ones. This catastrophic forgetting phenomenon impedes the deployment of artificial intelligence in real word scenarios where systems need to learn new and different representations over time. Recently, many incremental learning methods have been proposed to avoid the catastrophic forgetting problem. However, current state-of-the-art class-incremental learning strategies aim at preserving the knowledge of old classes while learning new ones sequentially, which would encounter other problems as follows: (1) In the process of preserving information of old classes, only a small portion of data in the previous tasks are kept and replayed during training, which inevitably incurs bias that is favorable for the new classes but malicious to the old classes. (2) With the knowledge of previous classes distilled into the new model, a sub-optimal solution for the new task is obtained since the preserving process of previous classes sabotages the training of new classes. To address these issues, termed as Information Asymmetry (IA), we propose a double-head framework which preserves the knowledge of old classes and learns the knowledge of new classes separately. Specifically, we transfer the knowledge of the previous model to the current learned one for overcoming the catastrophic forgetting problem. Furthermore, considering that IA would introduce impacts on the training of the new model, we propose a Non-Affection mask to distill the knowledge of the interested regions at the feature level. Comprehensive experimental results demonstrate that our proposed method significantly outperforms other state-of-the-art class-incremental object detection methods on PASCAL VOC and MS COCO datasets.}
}
@article{WANG2023109377,
title = {Cascaded feature fusion with multi-level self-attention mechanism for object detection},
journal = {Pattern Recognition},
volume = {138},
pages = {109377},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109377},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300078X},
author = {Chuanxu Wang and Huiru Wang},
keywords = {Cascaded feature fusion, Multi-level self-attention mechanism, Space-channel feature correlation, Object detection},
abstract = {Object detection has been a challenging task due to the complexity and diversity of objects. The emergence of self-attention mechanism provides a new clue for feature fusion in object detection task. Most existing self-attention mechanisms focus on extracting the correlation between global and local information in space or among channels, however it remains problematic issues of how to effectively fuse all those features. To address the above problems, we propose a Pooling and Global feature Fusion Self-attention Mechanism (PGFSM) to capture multi-level correlations among a variety of features, so as to perform cascaded aggregations upon them. PGFSM consists of three parts: Spatial Self-attention Pooling Fusion Module (SSPFM), Channel Self-attention Pooling Fusion Module (CSPFM), and Spatial and Channel Global Self-attention Fusion Module (SCGSFM). SSPFM and CSPFM respectively carried out in space and channel, extract the global maximum pooling and global average pooling self-attention features; SCGSFM extracts the spatial and channel fused characteristic relationship in the global. Finally, the three fused feature relations are added on the original feature to achieve an enhanced trait representation. In test, our PGFSM is embedded into YOLOv4, YOLOv5, and EfficientDet network respectively, and evaluated in PASCAL VOC and MS COCO datasets. The experiment results show that the feature fusion self-attention mechanism improves the performance of object detection compared to each original framework and also the state-of-the-art modules, which proves the effectiveness of our method.}
}
@article{ZHANG2023109439,
title = {Kernel-based feature aggregation framework in point cloud networks},
journal = {Pattern Recognition},
volume = {139},
pages = {109439},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109439},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001401},
author = {Jianjia Zhang and Zhenxi Zhang and Lei Wang and Luping Zhou and Xiaocai Zhang and Mengting Liu and Weiwen Wu},
keywords = {Point cloud, Kernel, Feature aggregation, Deep learning, Pooling},
abstract = {Various effective deep networks have been developed for analysis of 3D point clouds. One key step in these networks is to aggregate the features of orderless points into a compact representation for the cloud. As a typical order-invariant aggregation method, max-pooling has been widely applied. However, while enjoying simplicity and high efficiency, max-pooling does not fully exploit the feature information since it not only ignores the non-maximum elements in each feature dimension but also neglects the interactions between different dimensions. These drawbacks of max-pooling motivate us to explore advanced feature aggregation methods for 3D point cloud analysis. The desired advanced method should be capable of modeling richer information between the point features than max-pooling, and, at the same time, it can readily replace max-pooling without the need to modify other parts of the existing network architecture. To this end, this paper proposes a novel kernel-based feature aggregation framework for 3D point cloud analysis for the first time. The proposed method effectively considers all the elements in each dimension and models the nonlinear interactions between feature dimensions as complementary information to max-pooling. In addition, it is a plug-in module that can be integrated to many common networks as a replacement of max-pooling. Comprehensive experiments are conducted to demonstrate the consistently superior performance and high generality of the proposed method over max-pooling. Specifically, the proposed kernel-based feature aggregation framework consistently improves the max-pooling with three representative backbones of PointNet, DGCNN and PCT across four 3D point cloud based analysis tasks, including supervised 3D object classification, 3D part segmentation, indoor semantic segmentation and one additional unsupervised place retrieval task. Especially, it shows remarkable performance improvement over max-pooling in the unsupervised retrieval task, demonstrating its advantage in forming 3D point cloud representation.}
}
@article{LI2023109366,
title = {A single-stage point cloud cleaning network for outlier removal and denoising},
journal = {Pattern Recognition},
volume = {138},
pages = {109366},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109366},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000675},
author = {Ying Li and Huankun Sheng},
keywords = {Point cloud cleaning, Denoising, Outlier removal, Neural networks},
abstract = {As a simple, flexible and effective representation for objects, 3D point cloud has attracted more and more attention in recent years. However, raw point clouds obtained from 3D scanners or image-based reconstruction techniques are often contaminated with noise and outliers, which hinders downstream tasks such as object classification, surface reconstruction, and so on. Therefore, point cloud cleaning, i.e., removing noisy points and outliers from raw point cloud, is a prior step of most geometry processing workflows. The exiting techniques for point cloud cleaning usually include two stages, that is, discarding outliers at first, and then denoising the resulting point cloud. This two-stage process usually requires two different models, which is cumbersome to train and use. To solve this problem, a novel data driven method, named SSPCN (single-stage point cloud cleaning network), is proposed in this paper. SSPCN can simultaneously remove outliers and denoise a point cloud in a single model. Specifically, SSPCN is consisted of adaptive downsampling module, feature compensation module, upsampling module and coordinate reconstruction module. Given a raw point cloud as input, the downsampling module is first used to obtain a prefiltered point cloud subset and learn initial features of the subset. The feature compensation module is then utilized to learn accurate features from initial features. Next, the upsampling module upsamples the features to restore the original size of the point cloud. Last, the coordinate reconstruction module generates a cleaned point cloud from upsampled features. SSPCN is validated both on synthetic and real scanned data. Extensive experiments demonstrate that SSPCN outperforms state-of-the-art point cloud cleaning techniques in terms of quantitative metric and visual quality.}
}
@article{SUN2023109464,
title = {Hyperspectral subpixel target detection based on interaction subspace model},
journal = {Pattern Recognition},
volume = {139},
pages = {109464},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109464},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001644},
author = {Shengyin Sun and Jun Liu and Siyu Sun},
keywords = {Subpixel target detection, Hyperspectral images, Subspace model, Generalized likelihood ratio test (GLRT), Spectral variability},
abstract = {In this paper, we examine the problem of detecting subpixel targets in hyperspectral images. The so-called subpixel target refers to a target that only occupies a part of a pixel due to the low spatial resolution of hyperspectral sensors. Considering that the subpixel target spectrum is not always reliable (e.g., due to spectral variability), an interaction subspace model is designed to deal with this problem. In this subspace model, the second-order interaction terms are introduced to better describe the spectral variability, thereby improving the robustness. Specifically, the subspace model uses a hyperplane in a high-dimensional space to model spectral variability, while traditional models (e.g., the additive model and the replacement model) use a line in the high-dimensional space to model spectral variability. Obviously, the stronger description ability of the hyperplane makes the subspace model more tolerant to the mismatch of the target spectrum. Based on this interaction subspace model, we derive adaptive detectors according to the one-step generalized likelihood ratio test and its two-step variant. Experiments conducted on hyperspectral data demonstrate that the proposed two-step detector exhibits the strongest robustness in cases where the target spectrum is not very reliable.}
}