@article{LI2023109024,
title = {SaberNet: Self-attention based effective relation network for few-shot learning},
journal = {Pattern Recognition},
volume = {133},
pages = {109024},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109024},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005040},
author = {Zijun Li and Zhengping Hu and Weiwei Luo and Xiao Hu},
keywords = {Few-shot learning, Feature representation, Task analysis, Transformers},
abstract = {Few-shot learning is an essential and challenging field in machine learning since the agent needs to learn novel concepts with a few data. Recent methods aim to learn comparison or relation between query and support samples to tackle few-shot tasks but have not exceeded human performance and made full use of relations in few-shot tasks. Humans can recognize multiple variants of objects located anywhere in images and compare the relation among learned instances. Inspired by the human learning mechanism, we explore the definition of relations in relation networks and propose self-attention relation modules for feature and learning ability. First, we introduce vision self-attention to generate and purify features in few-shot learning. The comparison of different patches leads the backbone to infer relations between local features, which enforces feature extraction focus on more details. Second, we propose task-specific feature augmentation modules to infer relations and weight different contributions of components in few-shot tasks. The proposed SaberNet is conceptually simple and empirically powerful. Its performance surpasses the baseline a great margin, including pushing 5-way 1-shot CUB accuracy to 89.75% (12.73% absolute improvement), Cars to 76.71% (12.99% absolute improvement) and Flowers to 84.33% (7.67% absolute improvement).}
}
@article{CHEN2022108926,
title = {SWIPENET: Object detection in noisy underwater scenes},
journal = {Pattern Recognition},
volume = {132},
pages = {108926},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108926},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004071},
author = {Long Chen and Feixiang Zhou and Shengke Wang and Junyu Dong and Ning Li and Haiping Ma and Xin Wang and Huiyu Zhou},
keywords = {Underwater object detection, Curriculum Multi-Class Adaboost, Sample-weighted detection loss, Noisy data},
abstract = {Deep learning based object detection methods have achieved promising performance in controlled environments. However, these methods lack sufficient capabilities to handle underwater object detection due to these challenges: (1) images in the underwater datasets and real applications are blurry whilst accompanying severe noise that confuses the detectors and (2) objects in real applications are usually small. In this paper, we propose a Sample-WeIghted hyPEr Network (SWIPENET), and a novel training paradigm named Curriculum Multi-Class Adaboost (CMA), to address these two problems at the same time. Firstly, the backbone of SWIPENET produces multiple high resolution and semantic-rich Hyper Feature Maps, which significantly improve small object detection. Secondly, inspired by the human education process that drives the learning from easy to hard concepts, we propose the noise-robust CMA training paradigm that learns the clean data first and then move on to learns the diverse noisy data. Experiments on four underwater object detection datasets show that the proposed SWIPENET+CMA framework achieves better or competitive accuracy in object detection against several state-of-the-art approaches.}
}
@article{GUAN2022108967,
title = {MonoPoly: A practical monocular 3D object detector},
journal = {Pattern Recognition},
volume = {132},
pages = {108967},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108967},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004472},
author = {He Guan and Chunfeng Song and Zhaoxiang Zhang and Tieniu Tan},
keywords = {Object detection, Monocular 3D, Real-time, Light-weight},
abstract = {3D object detection plays a pivotal role in driver assistance systems and has practical requirements for small storage and fast inference. Monocular 3D detection alternatives abandon the complexity of LiDAR setup and pursues the effectiveness and efficiency of the vision scheme. In this work, we propose a set of anchor-free monocular 3D detectors called MonoPoly based on the keypoint paradigm. Specifically, we design a polynomial feature aggregation sampling module to extract multi-scale context features for auxiliary training and alleviate classification and localization misalignment through an attention-aware loss. Extensive experiments show that the proposed MonoPoly series achieves an excellent trade-off between performance and model size while maintaining real-time efficiency on KITTI and nuScenes datasets.}
}
@article{WANG2022108925,
title = {Learning pseudo labels for semi-and-weakly supervised semantic segmentation},
journal = {Pattern Recognition},
volume = {132},
pages = {108925},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108925},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200406X},
author = {Yude Wang and Jie Zhang and Meina Kan and Shiguang Shan},
keywords = {Semi-supervised, Weakly supervised, Semi-and-weakly supervised, Semantic segmentation, Pseudo label, Self-training},
abstract = {In this paper, we aim to tackle semi-and-weakly supervised semantic segmentation (SWSSS), where many image-level classification labels and a few pixel-level annotations are available. We believe the most crucial point for solving SWSSS is to produce high-quality pseudo labels, and our method deals with it from two perspectives. Firstly, we introduce a class-aware cross entropy (CCE) loss for network training. Compared to conventional cross entropy loss, CCE loss encourages the model to distinguish concurrent classes only and simplifies the learning target of pseudo label generation. Secondly, we propose a progressive cross training (PCT) method to build cross supervision between two networks with a dynamic evaluation mechanism, which progressively introduces high-quality predictions as additional supervision for network training. Our method significantly improves the quality of generated pseudo labels in the regime with extremely limited annotations. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods significantly. The code is released for public access11https://github.com/YudeWang/Learning-Pseudo-Label.}
}
@article{ESKANDARI2023109007,
title = {Online and offline streaming feature selection methods with bat algorithm for redundancy analysis},
journal = {Pattern Recognition},
volume = {133},
pages = {109007},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109007},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004873},
author = {S. Eskandari and M. Seifaddini},
keywords = {Feature selection, Online feature selection, Streamwise feature selection, Dimension reduction, Bat algorithm},
abstract = {Streaming feature selection (SFS), is the task of selecting the most informative features in dealing with high-dimensional or incrementally growing problems. Several SFS algorithms have been proposed in the literature. However, they do not consider all feature subsets at the redundancy analysis step due to computational concerns. Moreover, they do not reconsider previously removed features which leads to losing most of the useful information. In this paper, the redundancy analysis step is defined as a binary optimization problem. Then, a binary bat algorithm (BBA) is adopted to find the minimal informative subsets. In this way, a large number of feature subsets can be considered effectively at the redundancy analysis step. In addition, an effective priority list is used to maintain previously removed redundant features. Such a list allows the re-examination of informative features. As a result, it is possible to consider the mutual information between features that are not streamed in an small time interval. Experimental studies on fifteen different types of datasets show that our approach is superior to state-of-the-art online and offline streaming feature selection methods in terms of classification accuracy.}
}
@article{LUO2022108955,
title = {Domain consistency regularization for unsupervised multi-source domain adaptive classification},
journal = {Pattern Recognition},
volume = {132},
pages = {108955},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108955},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004356},
author = {Zhipeng Luo and Xiaobing Zhang and Shijian Lu and Shuai Yi},
keywords = {Domain adaptation, Transfer learning, Adversarial learning, Feature alignment},
abstract = {Deep learning-based multi-source unsupervised domain adaptation (MUDA) has been actively studied in recent years. Compared with single-source unsupervised domain adaptation (SUDA), domain shift in MUDA exists not only between the source and target domains but also among multiple source domains. Most existing MUDA algorithms focus on extracting domain-invariant representations among all domains whereas the task-specific decision boundaries among classes are largely neglected. In this paper, we propose an end-to-end trainable network that exploits domain Consistency Regularization for unsupervised Multi-source domain Adaptive classification (CRMA). CRMA aligns not only the distributions of each pair of source and target domains but also that of all domains. For each pair of source and target domains, we employ an intra-domain consistency to regularize a pair of domain-specific classifiers to achieve intra-domain alignment. In addition, we design an inter-domain consistency that targets joint inter-domain alignment among all domains. To address different similarities between multiple source domains and the target domain, we design an authorization strategy that assigns different authorities to domain-specific classifiers adaptively for optimal pseudo label prediction and self-training. Extensive experiments show that CRMA tackles unsupervised domain adaptation effectively under a multi-source setup and achieves superior adaptation consistently across multiple MUDA datasets.}
}
@article{2024110270,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {148},
pages = {110270},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(24)00021-9},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000219}
}
@article{ZHAO2022108983,
title = {Motion-blurred image restoration framework based on parameter estimation and fuzzy radial basis function neural networks},
journal = {Pattern Recognition},
volume = {132},
pages = {108983},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108983},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004630},
author = {Shengmin Zhao and Sung-Kwun Oh and Jin-Yul Kim and Zunwei Fu and Witold Pedrycz},
keywords = {Motion-blurred image restoration framework, Point spread function, Blur parameter estimation based on the particle swarm optimization, Polynomial-based radial basis function neural network, Image Quality Assessment},
abstract = {The restoration of motion-blurred images has always been a complex problem in image restoration. The current single blurred image algorithm cannot very well solve the estimation error of motion blur parameters. A comprehensive motion-blurred image restoration framework is proposed, which includes motion-blurred data generation, blur parameter estimation, and image quality assessment of restored images. First, we designed and used four image data sets with different degrees of blurring. We innovatively propose a blur parameter estimation algorithm based on the particle swarm optimization (B-PSO) algorithm. The Naturalness Image Quality Evaluator (NIQE) is used as the fitness function of the PSO algorithm. The framework also introduces a polynomial-based radial basis function neural network (P-RBFNN) as a new image quality assessment (IQA) method, with good image classification performance. Test results from public datasets show that the proposed framework can accurately estimate blur parameters. The peak signal-to-noise ratio (PSNR) reaches 29.976 dB, the structural similarity (SSIM) reaches 0.9044, and the classification rate is 96%. The proposed restoration framework produces the best image restoration results.}
}
@article{RAHIMZADEHARASHLOO2022108930,
title = {ℓp-Norm Support Vector Data Description},
journal = {Pattern Recognition},
volume = {132},
pages = {108930},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108930},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004113},
author = {Shervin {Rahimzadeh Arashloo}},
keywords = {One-class classification, Kernel methods, Support vector data description, -norm penalty},
abstract = {The support vector data description (SVDD) approach serves as a de facto standard for one-class classification where the learning task entails inferring the smallest hyper-sphere to enclose target objects while linearly penalising the errors/slacks via an ℓ1-norm penalty term. In this study, we generalise this modelling formalism to a general ℓp-norm (p≥1) penalty function on slacks. By virtue of an ℓp-norm function, in the primal space, the proposed approach enables formulating a non-linear cost for slacks. From a dual problem perspective, the proposed method introduces a dual norm into the objective function, thus, proving a controlling mechanism to tune into the intrinsic sparsity/uniformity of the problem for enhanced descriptive capability. A theoretical analysis based on Rademacher complexities characterises the generalisation performance of the proposed approach while the experimental results on several datasets confirm the merits of the proposed method compared to other alternatives.}
}
@article{LI2023108979,
title = {Adaptive momentum variance for attention-guided sparse adversarial attacks},
journal = {Pattern Recognition},
volume = {133},
pages = {108979},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108979},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004599},
author = {Chao Li and Wen Yao and Handing Wang and Tingsong Jiang},
keywords = {Deep neural networks, Black-box adversarial attacks, Transferability, Momentum variances},
abstract = {The phenomenon that deep neural networks are vulnerable to adversarial examples has been found for several years. Under the black-box setting, transfer-based methods usually produce the adversarial examples on a white-box model, which serves as the surrogate model in the black-box attack, and hope that the same adversarial examples can also fool the black-box model. However, these methods have high success rates for the surrogate model and exhibit weak transferability for the black-box model. In addition, some studies have shown that deep neural networks are also vulnerable to sparse alterations of the input, but existing sparse attacks mainly focus on the number of attacked pixels without restricting the size of the perturbations, which is perceptible to human eyes. To address the above problems, we propose a transfer-based sparse attack method, called adaptive momentum variance based iterative gradient method with a class activation map, where the method considers a simple adaptive momentum variance and a refining perturbation mechanism to improve the transferability of adversarial examples. Also, a class activation map, which is also known as attention mechanism, is employed to explore the relationship between the number of the perturbed pixels and the attack performance in the case of limiting the intensity of perturbation. The proposed method is compared with a number of the state-of-the-art transfer-based adversarial attack methods on the ImageNet dataset, and the empirical results demonstrate that our method achieves a significant increase in transferability with only attacking about 50% of the pixels.}
}
@article{FAN2023108971,
title = {A landmark-free approach for automatic, dense and robust correspondence of 3D faces},
journal = {Pattern Recognition},
volume = {133},
pages = {108971},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108971},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004514},
author = {Zhenfeng Fan and Xiyuan Hu and Chen Chen and Xiaolian Wang and Silong Peng},
keywords = {3D face, Dense correspondence, Non-rigid registration},
abstract = {Global dense registration of 3D faces commonly prioritizes correspondences of facial landmarks which are fiducial points for the anatomical structures. However, it is not always easy to pre-annotate the landmarks accurately in raw scans of 3D faces. Contrary to the current state-of-the-art in dense 3D face correspondence, we propose a general framework without pre-annotated landmarks, which promotes its robustness and allows the meshes to deform in a uniform manner. The proposed framework includes two stages: first the correspondences are established using a template face; and then we select some well-reconstructed samples to build a prior model and leverage it into the correspondence process of other samples. In both stages, the dense registration is revisited in two perspectives: semantic and topological correspondence. In the latter stage, we further incorporate shape and normal statistics of 3D faces to regularize the correspondence process for more robust results. This provides a feasible way to handle data with noises and occlusions, as well as large deformation caused by facial expressions. Our basic idea is to gradually refine the correspondence of individual points in a way global-to-local. At the same time, we solve the local-to-global deformation based on the refined correspondences. The two processes are alternated, and aided by some confidence checks for each individual points. In the experiments, the proposed method is evaluated both qualitatively and quantitatively on three datasets including two publicly available ones: FRGC v2.0 and BU-3DFE datasets, demonstrating its effectiveness.}
}
@article{SONG2023108995,
title = {Decoupling multi-task causality for improved skin lesion segmentation and classification},
journal = {Pattern Recognition},
volume = {133},
pages = {108995},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108995},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004757},
author = {Lei Song and Haoqian Wang and Z. Jane Wang},
keywords = {Skin lesion analysis, Multi-task decoupled, Deep learning, Task causality},
abstract = {Multi-task learning has been used widely in many computer aided diagnosis applications recently, while the trade-off between different tasks remains challenging. Also, the inherent causality is less studied. In this paper, we focus on skin lesion analysis, including lesion classification, detection and segmentation. By defining the chain relationship (i.e., lesion detection boosts contour segmentation, and segmentation boosts lesion classification in turn), and further decoupling each pair-wise causality (e.g., detection to segmentation) from the Pareto efficiency view, we can solve the common trade-off issue between multi-task. On this basis, we propose a novel paradigm to improve the skin lesion segmentation and classification separately, and favourable feature fusion ways for each task are explored. Moreover, to address the huge model size problem, we design an effective model compression scheme (MCS). Extensive experiments on the ISIC2017 and PH2 datasets are conducted to evaluate the proposed paradigm. The results demonstrate that the popular models such as ResNet, DenseNet and UNet for lesion analysis can be boosted by applying the proposed paradigm, and the designed MCS reduces the amount of model parameters efficiently. We achieve performance improvements on skin lesion segmentation and classification without strenuous network design and soaring model complexity. This proposed approach is promising for the multi-task diagnosis setting in other medical applications.}
}
@article{SAMBATURU2023109011,
title = {ScribbleNet: Efficient interactive annotation of urban city scenes for semantic segmentation},
journal = {Pattern Recognition},
volume = {133},
pages = {109011},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109011},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004915},
author = {Bhavani Sambaturu and Ashutosh Gupta and C.V. Jawahar and Chetan Arora},
abstract = {Annotation is a crucial first step in the semantic segmentation of urban images that facilitates the development of autonomous navigation systems. However, annotating complex urban images is time-consuming and challenging. It requires significant human effort making it expensive and error-prone. To reduce human effort during annotation, multiple images need to be annotated in a short time-span. In this paper, we introduce ScribbleNet, an interactive image segmentation algorithm to address this issue. Our approach provides users with a pre-segmented image that iteratively improves the segmentation using scribble as an annotation input. This method is based on conditional inference and exploits the learnt correlations in a deep neural network (DNN). ScribbleNet can: (1) work with urban city scenes captured in unseen environments, (2) annotate new classes not present in the training set, and (3) correct several labels at once. We compare this method with other interactive segmentation approaches on multiple datasets such as CityScapes, BDD, Mapillary Vistas, KITTI, and IDD. ScribbleNet reduces the annotation time of an image by up to 14.7 × over manual annotation and up to 5.4× over the current approaches. The algorithm is integrated into the publicly available LabelMe image annotation tool and will be released as an open-source software.}
}
@article{SHANG2022108966,
title = {Uncorrelated feature selection via sparse latent representation and extended OLSDA},
journal = {Pattern Recognition},
volume = {132},
pages = {108966},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108966},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004460},
author = {Ronghua Shang and Jiarui Kong and Weitong Zhang and Jie Feng and Licheng Jiao and Rustam Stolkin},
keywords = {Unsupervised feature selection, Sparse latent representation, OLSDA, Pseudo-labels, Uncorrelated constraints},
abstract = {Modern unsupervised feature selection methods predominantly obtain the cluster structure and pseudo-labels information through spectral clustering. However, the pseudo-labels obtained by spectral clustering are usually mixed between positive and negative. Moreover, the Laplacian matrix in spectral clustering typically affects feature selection. Additionally, spectral clustering does not consider the interconnection information between data. To address these problems, this paper proposes uncorrelated feature selection via sparse latent representation and extended orthogonal least square discriminant analysis (OLSDA), which we term SLREO). Firstly, SLREO retains the interconnection between data by latent representation learning, and preserves the internal information between the data. In order to remove redundant interconnection information, an l2,1-norm constraint is applied to the residual matrix of potential representation learning. Secondly, SLREO obtains non-negative pseudo-labels through orthogonal least square discriminant analysis (OLSDA) of embedded non-negative manifold structure. It not only avoids the appearance of negative pseudo-labels, but also eliminates the effect of the Laplacian matrix on feature selection. The manifold information of the data is also preserved. Furthermore, the matrix of the learned latent representation and OLSDA is used as pseudo-labels information. It not only ensures that the generated pseudo-labels are non-negative, but also makes the pseudo-labels closer to the true class labels. Finally, in order to avoid trivial solutions, an uncorrelated constraint and l2,1-norm constraint are imposed on the feature transformation matrix. These constraints ensure row sparsity of the feature transformation matrix, select low-redundant and discriminative features, and improve the effect of feature selection. Experimental results show that the Clustering Accuracy (ACC) and Normalized Mutual Information (NMI) of SLREO are significantly improved, as compared with six other published algorithms, tested on 11 benchmark datasets.}
}
@article{HOU2023109035,
title = {Game-theoretic hypergraph matching with density enhancement},
journal = {Pattern Recognition},
volume = {133},
pages = {109035},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109035},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005155},
author = {Jian Hou and Huaqiang Yuan and Marcello Pelillo},
keywords = {Feature matching, Hypergraph matching, Game-theoretic, Density enhancement},
abstract = {Feature matching plays a fundamental role in computer vision and pattern recognition. As straightforward comparison of feature descriptors is not enough to provide reliable matching results in many situations, graph matching makes use of the pairwise relationship between features to improve matching accuracy. Hypergraph matching further employs the relationship among multiple features to provide more invariance between feature correspondences. Existing hypergraph matching algorithms usually solve an assignment problem, where outliers may result in a large number of false matches. In this paper we cast the hypergraph matching problem as a non-cooperative multi-player game, and obtain the matches by extracting the evolutionary stable strategies. Our algorithm exerts a strong constraint on the consistency of obtained matches, and false matches are excluded effectively. In order to increase the number of matches without increasing the computation load evidently, we present a density enhancement method to improve the matching results. We further propose two methods to enforce the one-to-one constraint, thereby removing false matches and maintaining a high matching accuracy. Experiments with both synthetic and real datasets validate the effectiveness of our algorithm.}
}
@article{XU2022108929,
title = {Infrared and visible image fusion via parallel scene and texture learning},
journal = {Pattern Recognition},
volume = {132},
pages = {108929},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108929},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004101},
author = {Meilong Xu and Linfeng Tang and Hao Zhang and Jiayi Ma},
keywords = {Image fusion, Infrared, Scene and texture learning, Recurrent neural network},
abstract = {Image fusion plays a pivotal role in numerous high-level computer vision tasks. Existing deep learning-based image fusion methods usually leverage an implicit manner to achieve feature extraction, which would cause some characteristics of source images, e.g., contrast and structural information, are unable to be fully extracted and integrated into the fused images. In this work, we propose an infrared and visible image fusion method via parallel scene and texture learning. Our key objective is to deploy two branches of deep neural networks, namely the content branch and detail branch, to synchronously extract different characteristics from source images and then reconstruct the fused image. The content branch focuses primarily on coarse-grained information and is deployed to estimate the global content of source images. The detail branch primarily pays attention to fine-grained information, and we design an omni-directional spatially variant recurrent neural networks in this branch to model the internal structure of source images more accurately and extract texture-related features in an explicit manner. Extensive experiments show that our approach achieves significant improvements over state-of-the-arts on qualitative and quantitative evaluations with comparatively less running time consumption. Meanwhile, we also demonstrate the superiority of our fused results in the object detection task. Our code is available at: https://github.com/Melon-Xu/PSTLFusion.}
}
@article{SHU2022108978,
title = {Wasserstein distributional harvesting for highly dense 3D point clouds},
journal = {Pattern Recognition},
volume = {132},
pages = {108978},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108978},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004587},
author = {Dong Wook Shu and Sung Woo Park and Junseok Kwon},
keywords = {3D point cloud harvesting, Progressive sampling, Stochastic instance normalization},
abstract = {In this paper, we present a novel 3D point cloud harvesting method, which can harvest 3D points from an estimated surface distribution in an unsupervised manner (i.e., an input is a prior distribution). Our method outputs the surface distribution of a 3D object and samples 3D points from the distribution based on the proposed progressive random sampling strategy. The progressive sampling regards a prior distribution itself as a network input and uses a progressively increasing number of latent variables for training, which can diversify the coordinates of 3D points with fast convergence. Subsequently, our stochastic instance normalization transforms the implicit distribution into other distributions, which enables diverse shapes of 3D objects. Experimental results show that our method is competitive with other state-of-the-art methods. Our method can harvest an arbitrary number of 3D points, wherein the 3D object is represented in detail with highly dense 3D points or a part of it is described with partial sampling.}
}
@article{MA2023109006,
title = {Robust Table Detection and Structure Recognition from Heterogeneous Document Images},
journal = {Pattern Recognition},
volume = {133},
pages = {109006},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109006},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004861},
author = {Chixiang Ma and Weihong Lin and Lei Sun and Qiang Huo},
keywords = {Table detection, Table structure recognition, Corner detection, Spatial CNN, Grid CNN, Split-and-merge},
abstract = {We introduce a new table detection and structure recognition approach named RobusTabNet to detect the boundaries of tables and reconstruct the cellular structure of each table from heterogeneous document images. For table detection, we propose to use CornerNet as a new region proposal network to generate higher quality table proposals for Faster R-CNN, which has significantly improved the localization accuracy of Faster R-CNN for table detection. Consequently, our table detection approach achieves state-of-the-art performance on three public table detection benchmarks, namely cTDaR TrackA, PubLayNet and IIIT-AR-13K, by only using a lightweight ResNet-18 backbone network. Furthermore, we propose a new split-and-merge based table structure recognition approach, in which a novel spatial CNN based separation line prediction module is proposed to split each detected table into a grid of cells, and a Grid CNN based cell merging module is applied to recover the spanning cells. As the spatial CNN module can effectively propagate contextual information across the whole table image, our table structure recognizer can robustly recognize tables with large blank spaces and geometrically distorted (even curved) tables. Thanks to these two techniques, our table structure recognition approach achieves state-of-the-art performance on three public benchmarks, including SciTSR, PubTabNet and cTDaR TrackB2-Modern. Moreover, we have further demonstrated the advantages of our approach in recognizing tables with complex structures, large blank spaces, as well as geometrically distorted or even curved shapes on a more challenging in-house dataset.}
}
@article{XU2022108954,
title = {Towards generalizable person re-identification with a bi-stream generative model},
journal = {Pattern Recognition},
volume = {132},
pages = {108954},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108954},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004344},
author = {Xin Xu and Wei Liu and Zheng Wang and Ruimin Hu and Qi Tian},
keywords = {Person re-identification, Generalizable re-ID, Camera-Camera problem, Camera-Person problem},
abstract = {Generalizable person re-identification (re-ID) has attracted growing attention due to its powerful adaptation capability in the unseen data domain. However, existing solutions often neglect either crossing cameras (e.g., illumination and resolution differences) or pedestrian misalignments (e.g., viewpoint and pose discrepancies), which easily leads to poor generalization capability when adapted to the new domain. In this paper, we formulate these difficulties as: 1) Camera-Camera (CC) problem, which denotes the various human appearance changes caused by different cameras; 2) Camera-Person (CP) problem, which indicates the pedestrian misalignments caused by the same identity person under different camera viewpoints or changing pose. To solve the above issues, we propose a Bi-stream Generative Model (BGM) to learn the fine-grained representations fused with camera-invariant global feature and pedestrian-aligned local feature, which contains an encoding network and two stream decoding sub-network. Guided by original pedestrian images, one stream is employed to learn a camera-invariant global feature for the CC problem via filtering cross-camera interference factors. For the CP problem, another stream learns a pedestrian-aligned local feature for pedestrian alignment using information-complete densely semantically aligned part maps. Moreover, a part-weighted loss function is presented to reduce the influence of missing parts on pedestrian alignment. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods on the large-scale generalizable re-ID benchmarks, involving domain generalization setting and cross-domain setting.}
}
@article{YOU2023109023,
title = {Dynamic dense CRF inference for video segmentation and semantic SLAM},
journal = {Pattern Recognition},
volume = {133},
pages = {109023},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109023},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005039},
author = {Mingyu You and Chaoxian Luo and Hongjun Zhou and Shaoqing Zhu},
keywords = {Incremental multi-class video segmentation, Semantic robotSimultaneous Localization and mMapping, Dynamic dense conditional random field},
abstract = {The dense conditional random field (dense CRF) is an effective post-processing tool for image/video segmentation and semantic SLAM. In this paper, we extend the traditional dense CRF inference algorithm to incremental sensor data modelling. The algorithm efficiently infers the maximum a posteriori probability (MAP) solution for a dynamically changing dense CRF model that is applied to incremental multi-class video segmentation and semantic SLAM. The computational cost is roughly proportional to the total change in the Gaussian pairwise edges of the dense CRF. In our system, with an increase in the number of frames of the sensor data, MAP calculations take approximately the same time to compute the overall three-dimensional dense CRF modelled for the entire video. Compared with the traditional dense CRF for video segmentation, this method is more suitable for incremental (in-line) video segmentation and robot semantic SLAM. The results of experiments show that if part of a pairwise edge is altered, our dynamic algorithm is significantly faster than the widely known standard dense CRF algorithm. In addition, the accuracy of its inference does not change. Several multi-class video segmentation tests confirmed the efficiency of inference of the algorithm. In another application, we used the dynamic dense CRF to incrementally integrate robot SLAM and video segmentation. The results show that an accurate SLAM can improve the accuracy of video segmentation, and the computational cost of the dense CRF MAP can be constrained over a constant range. The application of our algorithm is not limited to video segmentation: It is generic, and can be used to yield similar improvements in many optimization solutions for MAP in dynamically changing models.}
}
@article{MELNYKOV2023108994,
title = {Conditional mixture modeling and model-based clustering},
journal = {Pattern Recognition},
volume = {133},
pages = {108994},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108994},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004745},
author = {Volodymyr Melnykov and Yang Wang},
keywords = {finite mixture model, model-based clustering, non-compact clusters, regression, variable selection},
abstract = {Due to a potentially high number of parameters, finite mixture models are often at the risk of overparameterization even for a moderate number of components. This can lead to overfitting individual components and result in mixture order underestimation. One of the most popular approaches to address this issue is to reduce the number of parameters by considering parsimonious models. The vast majority of techniques in this direction focuses on the reparameterization of covariance matrices associated with mixture components. We propose an alternative approach based on the parsimonious parameterization of location parameters that enjoys remarkable modeling flexibility especially in the presence of non-compact clusters. Due to an attractive closed form formulation, speedy parameter estimation is available by means of the EM algorithm. The utility of the proposed method is illustrated on synthetic and well-known classification data sets.}
}
@article{SHEN2022108942,
title = {Distribution alignment for cross-device palmprint recognition},
journal = {Pattern Recognition},
volume = {132},
pages = {108942},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108942},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004228},
author = {Lei Shen and Yingyi Zhang and Kai Zhao and Ruixin Zhang and Wei Shen},
keywords = {Palmprint recognition, Deep learning, Loss function, Biometric recognition, Person Reidentification},
abstract = {With the development of IoT and mobile devices, cross-device palmprint recognition is becoming an emerging research topic in multimedia for its great application potential. Due to the diverse characteristics of different devices, e.g.resolution or artifacts caused by post-processing, cross-device palmprint recognition remains a challenging problem. In this paper, we make efforts to improve cross-device palmprint recognition in two aspects: (1) we put forward a novel distribution-based loss to narrow the representation gap across devices, and (2) we establish a new cross-device benchmark based on existing palmprint recognition datasets. Different from many recent studies that only utilize instance-level or pairwise-level information between devices, the proposed progressive target distribution loss (PTD loss) uses the distributional information. Moreover, we establish a progressive target mechanism that will be dynamically updated during training, making the optimization easier and smoother. The newly established benchmark contains more samples and more types of IoT devices than previous benchmarks, which can facilitate cross-device palmprint research. Extensive comparisons on several benchmarks reveal that: (1) our method outperforms other cross-device biometric recognition approaches significantly; (2) our method presents superior performance compared to SOTA competitors on several general palmprint recognition benchmarks; Code and data are openly available at https://kaizhao.net/palmprint.}
}
@article{ZHOU2023108970,
title = {GCM: Efficient video recognition with glance and combine module},
journal = {Pattern Recognition},
volume = {133},
pages = {108970},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108970},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004502},
author = {Yichen Zhou and Ziyuan Huang and Xulei Yang and Marcelo Ang and Teck Khim Ng},
keywords = {Glance and combine module, Video action recognition, Spatio-temporal convolution, Action recognition datasets},
abstract = {In this work, we present an efficient and powerful building block for video action recognition, dubbed Glance and Combine Module (GCM). In order to obtain a broader perspective of the video features, GCM introduces an extra glancing operation with a larger receptive field over both the spatial and temporal dimensions, and combines features with different receptive fields for further processing. We show in our ablation studies that the proposed GCM is much more efficient than other forms of 3D spatio-temporal convolutional blocks. We build a series of GCM networks by stacking GCM repeatedly, and train them from scratch on the target datasets directly. On the Kinetics-400 dataset which focuses more on appearance rather than action, our GCM networks can achieve similar accuracy as others without pre-training on ImageNet. For the more action-centric recognition datasets such as Something-Something (V1 & V2) and Multi-Moments in Time, the GCM networks achieve state-of-the-art performance with less than two thirds the computational complexity of other models. With only 19.2 GFLOPs of computation, our GCMNet15 can obtain 63.9% top-1 classification accuracy on Something-Something-V2 validation set under single-crop testing. On the fine-grained action recognition dataset FineGym, we beat the previous state-of-the-art accuracy achieved with 2-stream methods by more than 6% using only RGB input.}
}
@article{DING2023109018,
title = {Self-regularized prototypical network for few-shot semantic segmentation},
journal = {Pattern Recognition},
volume = {133},
pages = {109018},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109018},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004988},
author = {Henghui Ding and Hui Zhang and Xudong Jiang},
keywords = {Few-shot segmentation, Prototype, Prototypical network, Self-regularized, Non-parametric distance fidelity, Iterative query inference, SRPNet, CNN},
abstract = {The deep CNNs in image semantic segmentation typically require a large number of densely-annotated images for training and have difficulties in generalizing to unseen object categories. Therefore, few-shot segmentation has been developed to perform segmentation with just a few annotated examples. In this work, we tackle the few-shot segmentation using a self-regularized prototypical network (SRPNet) based on prototype extraction for better utilization of the support information. The proposed SRPNet extracts class-specific prototype representations from support images and generates segmentation masks for query images by a distance metric - the fidelity. A direct yet effective prototype regularization on support set is proposed in SRPNet, in which the generated prototypes are evaluated and regularized on the support set itself. The extent to which the generated prototypes restore the support mask imposes an upper limit on performance. The performance on the query set should never exceed the upper limit no matter how complete the knowledge is generalized from support set to query set. With the specific prototype regularization, SRPNet fully exploits knowledge from the support and offers high-quality prototypes that are representative for each semantic class and meanwhile discriminative for different classes. The query performance is further improved by an iterative query inference (IQI) module that combines a set of regularized prototypes. Our proposed SRPNet achieves new state-of-art performance on 1-shot and 5-shot segmentation benchmarks.}
}
@article{XU2023109010,
title = {BH2I-GAN: Bidirectional Hash_code-to-Image Translation using Multi-Generative Multi-Adversarial Nets},
journal = {Pattern Recognition},
volume = {133},
pages = {109010},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109010},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004903},
author = {Liming Xu and Xianhua Zeng and Weisheng Li and Yicai Xie},
keywords = {Deep hashing, Generative adversarial nets, Low storage cost, Hash_code-to-image, Supervised manifold similarity},
abstract = {Given the benefits of high retrieval efficiency and low storage cost, hashing method has received an increasing attention. In particular, deep learning-based hashing has been widely used in data mining and information retrieval. However, almost all the existing methods only achieve the goal of high retrieval precision, and limit the evaluation of hashing methods to objective aspect. In this paper, we propose a novel bidirectional hash_code-to-image translation model by using multi-generative multi-adversarial nets to reduce storage cost truly and obtain satisfactory user acceptance on the basis of high retrieval precision. Firstly, we propose supervised manifold metric to reduce Hamming distance between similar instances while increasing the Hamming distance between dissimilar instances, which have been proved to be helpful for high retrieval precision and good user acceptance. Then, we utilize multi-generative and multi-adversarial networks to construct hash mapping and inverse hash generation. During inverse generation, theoretical analysis is conducted to show that inverse hash network can avoid unstable training and mode collapse. Besides, we prove that Poisson distribution induced by hash codes can be initialized as generative distribution to fit real distribution. Experimental results show that our method outperforms several state-of-the-art approaches on three popular datasets. Specifically, ours yields average about 9.3% increment in Mean Average Precision(MAP) on three datasets, and achieves over 90% user satisfaction. Besides, it successfully reduces storage cost by 1,634 times in COCO 2017 large-scale dataset.}
}
@article{CHEN2023108982,
title = {Incremental learning for transductive support vector machine},
journal = {Pattern Recognition},
volume = {133},
pages = {108982},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108982},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004629},
author = {Haiyan Chen and Ying Yu and Yizhen Jia and Bin Gu},
keywords = {Transductive support vector machine, Incremental learning, Non-convex optimization, Infinitesimal annealing},
abstract = {Semi-supervised learning is ubiquitous in real-world machine learning applications due to its good performance for handling the data where only a few number of samples are labeled while most of then are unlabeled. Transductive support vector machine (TSVM) is an important semi-supervised learning method which formulates the problem as a nonconvex combinatorial optimization problem. The infinitesimal annealing algorithm is a novel training method of TSVM which can alleviate the impact of the combinatorial and non-convex natures in TSVM and achieve a fast training of TSVM. However, it is still a challenging problem to handle large-scale data for TSVM even using the infinitesimal annealing algorithm. To mitigate this problem, in this paper, we propose an incremental learning algorithm for TSVM (ILTSVM) based on the path following technique under the framework of infinitesimal annealing. Specifically, for new samples, we call CP-Step to change the solution and partition by increasing the size of the penalty coefficient. The difference between training labeled samples and training unlabeled samples is that the variation range of the penalty coefficient of labeled samples is larger than that of unlabeled samples. If in the process of CP-Step, pseudo-labels of unlabeled samples are classified incorrectly, call DJ-Step to flip the pseudo-labels, and use incremental and decremental algorithms to make the KKT condition satisfied. We also analyze the time complexity and convergence of ILTSVM. The experimental results show that compared with other incremental or batch learning algorithms, our algorithm is the most effective and fastest method for training TSVM.}
}
@article{YANG2022108949,
title = {Center Prediction Loss for Re-identification},
journal = {Pattern Recognition},
volume = {132},
pages = {108949},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108949},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004290},
author = {Lu Yang and Yunlong Wang and Lingqiao Liu and Peng Wang and Yanning Zhang},
keywords = {Person re-identification, Loss, Deep metric learning},
abstract = {The training loss function that enforces certain training sample distribution patterns plays a critical role in building a re-identification (ReID) system. Besides the basic requirement of discrimination, i.e., the features corresponding to different identities should not be mixed, additional intra-class distribution constraints, such as features from the same identities should be close to their centers, have been adopted to construct losses. Despite the advances of various new loss functions, it is still challenging to strike the balance between the need of reducing the intra-class variation and allowing certain distribution freedom. Traditional intra-class losses try to shrink samples of the same class into one point in the feature space and may easily drop their intra-class similarity structure. In this paper, we propose a new loss based on center predictivity, that is, a sample must be positioned in a location of the feature space such that from it we can roughly predict the location of the center of same-class samples. The prediction error is then regarded as a loss called Center Prediction Loss (CPL). Unlike most existing metric learning loss functions, CPL involves learnable parameters, i.e., the center predictor, which brings a remarkable change in the properties of the loss. In particular, it allows higher freedom in intra-class distributions. And the parameters in CPL will be discarded after training. Extensive experiments on various real-world ReID datasets show that the proposed loss can achieve superior performance and can also be complementary to existing losses.}
}
@article{WANG2023108989,
title = {A new algorithm for support vector regression with automatic selection of hyperparameters},
journal = {Pattern Recognition},
volume = {133},
pages = {108989},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108989},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004691},
author = {You-Gan Wang and Jinran Wu and Zhi-Hua Hu and Geoffrey J. McLachlan},
keywords = {Automatic selection, Loss functions, Noise models, Parameter estimation, Probability regularization},
abstract = {The hyperparameters in support vector regression (SVR) determine the effectiveness of the support vectors with fitting and predictions. However, the choice of these hyperparameters has always been challenging in both theory and practice. The ν-support vector regression eliminates the need to specify an ϵ value elegantly, but at the cost of specifying or postulating a ν value. We propose an extended primal objective function arising from probability regularization leading to an automatic selection of ϵ, and we can express ν as an explicit function of ϵ. The resultant hyperparameter values can be interpreted as ‘working’ values required only in training but not testing or prediction. This regularized algorithm, namely ϵ*-SVR, automatically provides a data-dependent ϵ and is found to have a close connection to the ν-support vector regression in the sense that ν as a fraction is a sensible function of ϵ. The ϵ*-SVR automatically selects both ν and ϵ values. We illustrate these findings with some public benchmark datasets.}
}
@article{SUN2023109029,
title = {Multi-scale multi-hierarchy attention convolutional neural network for fetal brain extraction},
journal = {Pattern Recognition},
volume = {133},
pages = {109029},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109029},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200509X},
author = {Liang Sun and Wei Shao and Qi Zhu and Meiling Wang and Gang Li and Daoqiang Zhang},
keywords = {Fetal brain extraction, In utero MR images, Multi-scale, Multi-hierarchy, 3D convolutional neural network},
abstract = {Fetal brain extraction from in utero magnetic resonance imaging (MRI) scans is a key step for fetal brain development analysis. As the unpredicted fetal motion and maternal breathing generally result in blurring and ghosting in the slices of phase encoding direction, using the conventional 3D convolutional neural networks for fetal brain extraction with pseudo 3D fetal brain MR scans will lead to sub-optimal extraction performance. To address this issue, in this paper, we propose a novel multi-scale multi-hierarchy attention convolutional neural network (MSMHA-CNN) for fetal brain extraction in MR images. Specifically, to effectively utilize the 3D contextual information of the in utero MR image for fetal brain extraction, we employ multiple convolutional operations with different local receptive fields (i.e., with different kernel sizes) in each layer to learn the multi-scale feature representation for fetal brain extraction. To effectively use the learned multi-scale feature maps, we introduce a channel-wise spatial attention architecture to adaptively fuse those multi-scale feature maps derived from convolutional operations with different kernel sizes. In this way, the learned multi-scale features can be explicitly used to fetal brain extraction process. Besides, to take advantage of high-level feature maps at all spatial resolutions, we adopt the feature pyramid architecture to learn multi-hierarchy features for boosting the performance. We compare our proposed method with several state-of-the-art methods on two in utero MRI scan datasets (a total of 180 scans) for fetal brain extraction. The experimental results suggest the superior performance of the proposed MSMHA-CNN in comparison with its competitors.}
}
@article{KHO2022108953,
title = {Exploiting shape cues for weakly supervised semantic segmentation},
journal = {Pattern Recognition},
volume = {132},
pages = {108953},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108953},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004332},
author = {Sungpil Kho and Pilhyeon Lee and Wonyoung Lee and Minsong Ki and Hyeran Byun},
keywords = {Semantic segmentation, Weakly supervised learning, Texture biases, Shape cues},
abstract = {Weakly supervised semantic segmentation (WSSS) aims to produce pixel-wise class predictions with only image-level labels for training. To this end, previous methods adopt the common pipeline: they generate pseudo masks from class activation maps (CAMs) and use such masks to supervise segmentation networks. However, it is challenging to derive comprehensive pseudo masks that cover the whole extent of objects due to the local property of CAMs, i.e., they tend to focus solely on small discriminative object parts. In this paper, we associate the locality of CAMs with the texture-biased property of convolutional neural networks (CNNs). Accordingly, we propose to exploit shape information to supplement the texture-biased CNN features, thereby encouraging mask predictions to be not only comprehensive but also well-aligned with object boundaries. We further refine the predictions in an online fashion with a novel refinement method that takes into account both the class and the color affinities, in order to generate reliable pseudo masks to supervise the model. Importantly, our model is end-to-end trained within a single-stage framework and therefore efficient in terms of the training cost. Through extensive experiments on PASCAL VOC 2012, we validate the effectiveness of our method in producing precise and shape-aligned segmentation results. Specifically, our model surpasses the existing state-of-the-art single-stage approaches by large margins. What is more, it also achieves a new state-of-the-art performance over multi-stage approaches, when adopted in a simple two-stage pipeline without bells and whistles.}
}
@article{FERRARI2023109022,
title = {Online change-point detection with kernels},
journal = {Pattern Recognition},
volume = {133},
pages = {109022},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109022},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005027},
author = {André Ferrari and Cédric Richard and Anthony Bourrier and Ikram Bouchikhi},
keywords = {Non-parametric change-point detection, Reproducing kernel Hilbert space, Kernel least-mean-square algorithm, Online algorithm, Convergence analysis},
abstract = {Change-points in time series data are usually defined as the time instants at which changes in their properties occur. Detecting change-points is critical in a number of applications as diverse as detecting credit card and insurance frauds, or intrusions into networks. Recently the authors introduced an online kernel-based change-point detection method built upon direct estimation of the density ratio on consecutive time intervals. This paper further investigates this algorithm, making improvements and analyzing its behavior in the mean and mean square sense, in the absence and presence of a change point. These theoretical analyses are validated with Monte Carlo simulations. The detection performance of the algorithm is illustrated through experiments on real-world data and compared to state of the art methodologies.}
}
@article{AGIBETOV2023108977,
title = {Neural graph embeddings as explicit low-rank matrix factorization for link prediction},
journal = {Pattern Recognition},
volume = {133},
pages = {108977},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108977},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004575},
author = {Asan Agibetov},
keywords = {Graph embedding, Random walks, Matrix factorization, Information theory, Link prediction},
abstract = {Learning good quality neural graph embeddings has long been achieved by minimzing the pointwise mutual information (PMI) for co-occuring nodes in simulated random walks. This design choice has been mostly popularized by the direct application of the highly-successful word embedding algorithm word2vec to predicting the formation of new links in social, co-citation, and biological networks. However, such a skeuomorphic design of graph embedding methods entails a truncation of information coming from pairs of nodes with low PMI. To circumvent this issue, we propose an improved approach to learning low-rank factorization embeddings that incorporate information from such unlikely pairs of nodes and show that it can improve the link prediction performance of baseline methods from 1.2% to 24.2%. Based on our results and observations, we outline further steps that could improve the design of next graph embedding algorithms that are based on matrix factorizaion.}
}
@article{IBANEZ2022108933,
title = {Generalized discriminant analysis via kernel exponential families},
journal = {Pattern Recognition},
volume = {132},
pages = {108933},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108933},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004058},
author = {Isaías Ibañez and Liliana Forzani and Diego Tomassi},
keywords = {Discriminant analysis, Sufficient dimension reduction, Reproducing kernel Hilbert spaces, Support vector machine},
abstract = {This paper introduces a novel supervised dimension reduction method for classification and regression problems using reproducing kernel Hilbert spaces. The proposed approach takes advantage of the modeling power of kernel exponential families to extract nonlinear summary statistics of the data that are sufficient to preserve information about the target response. For the special case of finite dimensional exponential family distributions, the proposed method is shown to simplify the known solutions for sufficient dimension reduction. A connection with support vector machines is shown and exploited to obtain efficient estimation procedures. Experiments with simulated and real data illustrate the potential of the proposed approach.}
}
@article{CUI2022108988,
title = {Progressive downsampling and adaptive guidance networks for dynamic scene deblurring},
journal = {Pattern Recognition},
volume = {132},
pages = {108988},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108988},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200468X},
author = {Jinkai Cui and Weihong Li and Wei Guo and Weiguo Gong},
keywords = {Progressive downsampling, Adaptive guidance, Blended activation, Multisupervision, Dynamic scene deblurring},
abstract = {The existing learning-based dynamic scene deblurring methods have made good progress to some extent. However, these methods are usually based on multiscale strategy, which has the following shortcomings: (1) The bilinear downsampling operation will cause some loss of important high-frequency information, e.g., strong edges, which also further affects the network learning a better deblurring mapping. (2) Existing methods only use a single activation function, which limits the ability of the network model to fit data and causes the network performance to be easily saturated. Therefore, we propose an end-to-end progressive downsampling and adaptive guidance network called PDAG-Net for solving above problems. The proposed PDAG-Net can retain more strong edges and other high-frequency information of a blurry image so as to make the network learn a more effective deblurring mapping between the input and label images. In the proposed PDAG-Net, we implement a multiscale blended activation residual block called MSBA-ResBlock for learning the nonlinear characteristics of dynamic scene blur, which can also alleviate the performance saturation problem caused by a single activation function and improve multiscale feature extraction ability. Finally, we propose a multisupervision strategy for obtaining more robust and effective features and making the network possess more stable trainging and faster convergence. Extensive experimental results on a public dataset indicate that the proposed network outperforms the state-of-the-art image deblurring methods.}
}
@article{JIANG2022108965,
title = {JSL3d: Joint subspace learning with implicit structure supervision for 3D pose estimation},
journal = {Pattern Recognition},
volume = {132},
pages = {108965},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108965},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004459},
author = {Mengxi Jiang and Shihao Zhou and Cuihua Li and Yunqi Lei},
keywords = {, , , },
abstract = {Estimating 3D human poses from a single image is an important task in computer graphics. Most model-based estimation methods represent the labeled/detected 2D poses and the projection of approximated 3D poses using vector representations of body joints. However, such lower-dimensional vector representations fail to maintain the spatial relations of original body joints, because the representations do not consider the inherent structure of body joints. In this paper, we propose JSL3d, a novel joint subspace learning approach with implicit structure supervision based on Sparse Representation (SR) model, capturing the latent spatial relations of 2D body joints by an end-to-end autoencoder network. JSL3djointly combines the learned latent spatial relations and 2D joints as inputs for the standard SR inference frame. The optimization is simultaneously processed via geometric priors in both latent and original feature spaces. We have evaluated JSL3dusing four large-scale and well-recognized benchmarks, including Human3.6M, HumanEva-I, CMU MoCap and MPII. The experiment results demonstrate the effectiveness of JSL3d.}
}
@article{WANG2023108993,
title = {BP-triplet net for unsupervised domain adaptation: A Bayesian perspective},
journal = {Pattern Recognition},
volume = {133},
pages = {108993},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108993},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004733},
author = {Shanshan Wang and Lei Zhang and Pichao Wang and MengZhu Wang and Xingyi Zhang},
keywords = {Cross domain class alignment, Unsupervised domain adaptation, Metric learning, Bayesian perspective},
abstract = {Triplet loss, one of the deep metric learning (DML) methods, is to learn the embeddings where examples from the same class are closer than examples from different classes. Motivated by DML, we propose an effective BP-triplet Loss for unsupervised domain adaption (UDA) from the perspective of Bayesian learning and we name the model as BP-Triplet Net. In previous metric learning based methods for UDA, sample pairs across domains are treated equally, which is not appropriate due to the domain bias. In our work, considering the different importance of pair-wise samples for both feature learning and domain alignment, we deduce our BP-triplet loss for effective UDA from the perspective of Bayesian learning. Our BP-triplet loss adjusts the weights of pair-wise samples in intra-domain and inter-domain. Especially, it can self attend to the hard pairs (including hard positive pair and hard negative pair). Together with the commonly used adversarial loss for domain alignment, the quality of target pseudo labels is progressively improved. Our method achieved low joint error of the ideal source and target hypothesis. The expected target error can then be upper bounded following Ben-David’s theorem. Comprehensive evaluations on four benchmark datasets demonstrate the effectiveness of the proposed approach for UDA. Code is available at https://github.com/wangshanshanAHU/BP-Triplet-Net.}
}
@article{ZHENG2022108941,
title = {Unsupervised domain adaptation in homogeneous distance space for person re-identification},
journal = {Pattern Recognition},
volume = {132},
pages = {108941},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108941},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004216},
author = {Dingyuan Zheng and Jimin Xiao and Yunchao Wei and Qiufeng Wang and Kaizhu Huang and Yao Zhao},
keywords = {Person re-identification, Unsupervised domain adaptation, Distribution alignment, Clustering, Pseudo label},
abstract = {Data distribution alignment and clustering-based self-training are two feasible solutions to tackle unsupervised domain adaptation (UDA) on person re-identification (re-ID). Most existing alignment-based methods solely learn the source domain decision boundaries and align the data distribution of the target domain to the source domain, thus the re-ID performance on the target domain completely depends on the shared decision boundaries and how well the alignment is performed. However, two domains can hardly be precisely aligned because of the label space discrepancy of two domains, resulting in poor target domain re-ID performance. Although clustering-based self-training approaches could learn independent decision boundaries on the pseudo-labelled target domain data, they ignore both the accurate ID-related information of the labelled source domain data and the underlying relations between two domains. To fully exploit the source domain data to learn discriminative target domain ID-related features, in this paper, we propose a novel cross-domain alignment method in the homogeneous distance space, which is constructed by the newly designed stair-stepping alignment (SSA) matcher. Such alignment method can be integrated into both alignment-based framework and clustering-based framework. Extensive experiments validate the effectiveness of our proposed alignment method in these two frameworks. We achieve superior performance when the proposed alignment module is integrated into the clustering-based framework. Codes will be available at: http://github.com/Dingyuan-Zheng/HDS.}
}
@article{BENITOALTAMIRANO2023108981,
title = {Back-compatible Color QR Codes for colorimetric applications},
journal = {Pattern Recognition},
volume = {133},
pages = {108981},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108981},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004617},
author = {Ismael Benito-Altamirano and David Martínez-Carpena and Olga Casals and Cristian Fábrega and Andreas Waag and Joan Daniel Prades},
keywords = {Barcodes, QR codes, Color correction, Color calibration, Colorchecker, Colorimetry},
abstract = {Color correction techniques in digital photography often rely on the use of color correction charts, which require including this relatively large object in the field of view. We propose here to use QR Codes to pack these color charts in a compact form factor, in a fully compatible manner with conventional black and white QR Codes; this is, without losing any of their easy location, sampling and digital data storage features. First, we present an algorithm to build these new colored QR Codes that preserves the original QR Code functionality - much more than other coloring proposals based on the random substitution of black and white pixels by colors - that relies on the ability of the native CRC code to correct and counteract these alterations. Second, we demonstrate that, as a result, these QR Codes can allocate far many more colors than the conventional color correction charts, enabling much more accurate color correction schemes in a more convenient and usable format.}
}
@article{LAN2023109033,
title = {AEDNet: Adaptive Edge-Deleting Network For Subgraph Matching},
journal = {Pattern Recognition},
volume = {133},
pages = {109033},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109033},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005131},
author = {Zixun Lan and Ye Ma and Limin Yu and Linglong Yuan and Fei Ma},
keywords = {Subgraph matching, Graph neural network, Neural matching},
abstract = {Subgraph matching is to find all subgraphs in a data graph that are isomorphic to an existing query graph. Subgraph matching is an NP-hard problem, yet has found its applications in many areas. Many learning-based methods have been proposed for graph matching, whereas few have been designed for subgraph matching. The subgraph matching problem is generally more challenging, mainly due to the different sizes between the two graphs, resulting in considerable large space of solutions. Also the extra edges existing in the data graph connecting to the matched nodes may lead to two matched nodes of two graphs having different adjacency structures and often being identified as distinct objects. Due to the extra edges, the existing learning based methods often fail to generate sufficiently similar node-level embeddings for matched nodes. This study proposes a novel Adaptive Edge-Deleting Network (AEDNet) for subgraph matching. The proposed method is trained in an end-to-end fashion. In AEDNet, a novel sample-wise adaptive edge-deleting mechanism removes extra edges to ensure consistency of adjacency structure of matched nodes, while a unidirectional cross-propagation mechanism ensures consistency of features of matched nodes. We applied the proposed method on six datasets with graph sizes varying from 20 to 2300. Our evaluations on six open datasets demonstrate that the proposed AEDNet outperforms six state-of-the-arts and is much faster than the exact methods on large graphs.}
}
@article{LI2023108976,
title = {A unified model for the sparse optimal scoring problem},
journal = {Pattern Recognition},
volume = {133},
pages = {108976},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108976},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004563},
author = {Guoquan Li and Linxi Yang and Kequan Zhao},
keywords = {Optimal scoring, Linear discriminant analysis, Feature selection, norm, Sparseness},
abstract = {Optimal scoring (OS), an equivalent form of linear discriminant analysis (LDA), is an important supervised learning method and dimensionality reduction tool. However, it is still a challenge for the classical OS on small sample size (SSS) datasets. In this paper, to find sparse discriminant vectors, we propose a unified model for sparse optimal scoring (SOS) by virtue of the generalized ℓq-norm (0≤q≤1). To overcome the difficulty in treating the generalized ℓq-norm, we propose an efficient alternative direction method of multipliers (ADMM), where proximity operator of ℓq-norm is employed for different q values. Meanwhile, the convergence results of our method are also established. Numerical experiments on artificial and benchmark datasets demonstrate the effectiveness and feasibility of our proposed method.}
}
@article{LIU2022108952,
title = {Non-rigid point set registration based on local neighborhood information support},
journal = {Pattern Recognition},
volume = {132},
pages = {108952},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108952},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004320},
author = {Chuanju Liu and Dongmei Niu and Peng Wang and Xiuyang Zhao and Bo Yang and Caiming Zhang},
keywords = {Non-rigid point set registration, Gaussian mixture model, Expectation–Maximization method, Local neighborhood information},
abstract = {Non-rigid point set registration is a crucial task and an unsolved problem in the field of computer vision. One commonly used method for solving the problem is based on the Gaussian mixture model (GMM). In this method, the point set registration is formalized as a probability density estimation problem. Most GMM-based methods achieve registration by maintaining global and local structures of points. However, the previous methods did not filter the neighborhood information in the local structure, and the quality of local neighborhood information directly affects the accuracy of registration. Therefore, extracting effective local neighborhood information is still a challenge. We propose a novel point set registration method based on GMM by extracting local neighborhood information. The two point sets X and Y are regarded as the centroids of GMM and data points produced by GMM, respectively. Our method computes initial correspondences by comparing the feature descriptors of point sets, and the initial correspondences are updated by considering the neighborhood information. Our method then uses the Expectation–Maximization method to solve the GMM. In the experimental results, the efficiency and advantages of our method relative to the current methods are verified by applying five commonly used datasets.}
}
@article{LI2022108948,
title = {Automatically classifying non-functional requirements using deep neural network},
journal = {Pattern Recognition},
volume = {132},
pages = {108948},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108948},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004289},
author = {Bing Li and Xiuwen Nong},
keywords = {Non-functional requirements, Non-functional requirements classification, BERT, N-gram, Bi-LSTM, Multi-sample dropout},
abstract = {Non-functional requirements are property that software products must have in order to meet the user’s business requirements, and are additional constraints on the quality and characteristics of software systems. They are generally written by software designers and documented in various parts of requirements documentation. When developing systems, developers need to classify non-functional requirements from requirements documents, and classifying these non-functional requirements requires professional skills, experience, and domain knowledge, which is challenging and time-consuming for developers. It would be beneficial to implement automatic classification of non-functional requirements from requirements documents, which could reduce the manual, time, and mental fatigue involved in identifying specific non-functional requirements from a large number of requirements. In this paper, a deep neural network model called NFRNet is designed to automatically classify non-functional requirements from software requirement documents. The network consists of two parts. One is an improved BERT word embedding model based on N-gram masking for learning context representation of the requirement descriptions, and the other is a Bi-LSTM classification network for capture context information of the requirement descriptions. We use a Softmax classifier in the end to classify the requirement descriptions. At the same time, in order to accelerate the training and improve the generalization ability of the model, the network uses multi-sample dropout regularization technology. This new regularization technology can reduce the number of iterations needed for training, accelerate the training of deep neural networks, and the networks trained achieved lower error rates. In addition, we expanded the original non-functional requirements dataset (PROMISE dataset) and designed a new dataset called SOFTWARE NFR. The new dataset far exceeds the original dataset in terms of the number of requirement description sentences and the number of non-functional requirements categories. It can be taken as a new testbed for non-functional requirements classification. Through cross-validation on the new dataset, the experimental results show that the network designed in this paper is significantly better than the other 17 classification methods in terms of Precision, Recall, and F1-score. At the same time, for the training set and the validation set, using the multi-sample dropout regularization technology can accelerate the training speed, reduce the number of iterations, and achieve lower error rates and loss.}
}
@article{REN2023108992,
title = {Grouping-based Oversampling in Kernel Space for Imbalanced Data Classification},
journal = {Pattern Recognition},
volume = {133},
pages = {108992},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108992},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004721},
author = {Jinjun Ren and Yuping Wang and Yiu-ming Cheung and Xiao-Zhi Gao and Xiaofang Guo},
keywords = {Imbalanced data classification, Kernel method, Support vector machine, Oversampling},
abstract = {The class-imbalanced classification is a difficult problem because not only traditional classifiers are more biased towards the majority classes and inclined to generate incorrect predictions, but also the existing algorithms often have difficulty tackling this kind of problem with the class overlapping. Oversampling is a widely used and effective method to obtain balanced samples for imbalanced data, but the existing oversampling methods usually result in more serious class overlapping due to improper choice of the reference samples. To circumvent this shortcoming, according to the different possibilities of minority class samples appearing in the overlapping regions in the feature space, a grouping scheme for the minority class samples is first designed to identify the overlapping region samples. Then, a new oversampling method based on this grouping scheme is proposed to make the new samples far away from the overlapping region and rectify the decision boundary properly. Subsequently, a new effective classification algorithm is developed for imbalanced data. Extensive experiments show that the proposed algorithm is superior to the seventeen benchmark algorithms in terms of three performance metrics, especially on high imbalance ratio data sets.}
}
@article{LI2022108918,
title = {Unsupervised domain adaptation with progressive adaptation of subspaces},
journal = {Pattern Recognition},
volume = {132},
pages = {108918},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108918},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003995},
author = {Weikai Li and Songcan Chen},
keywords = {Unsupervised domain adaptation, Partial domain adaptation, Subspace learning, Pseudo label},
abstract = {Unsupervised Domain Adaptation (UDA) aims to classify unlabeled target domain by transferring knowledge from labeled source domain with domain shift. Most of the existing UDA methods try to mitigate the adverse impact induced by the shift via reducing domain discrepancy. However, such approaches easily suffer a notorious mode collapse issue due to the lack of labels in target domain. Naturally, one of the effective ways to mitigate this issue is to reliably estimate the pseudo labels for target domain, which itself is hard. To overcome this, we propose a novel UDA method named Progressive Adaptation of Subspaces approach (PAS) in which we utilize such an intuition that appears much reasonable to gradually obtain reliable pseudo labels. Specifically, we progressively and steadily refine the shared subspaces as bridge of knowledge transfer by adaptively anchoring/selecting and leveraging those target samples with reliable pseudo labels. Subsequently, the refined subspaces can in turn provide more reliable pseudo-labels of the target domain, making the mode collapse highly mitigated. Our thorough evaluation demonstrates that PAS is not only effective for common UDA, but also outperforms the state-of-the arts for more challenging Partial Domain Adaptation (PDA) situation, where the source label set subsumes the target one.}
}
@article{CHEN2022108964,
title = {Enhancement of DNN-based multilabel classification by grouping labels based on data imbalance and label correlation},
journal = {Pattern Recognition},
volume = {132},
pages = {108964},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108964},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004447},
author = {Ling Chen and Yuhong Wang and Hao Li},
keywords = {Multilabel classification, data imbalance, label correlation, neural network},
abstract = {Multilabel classification (MLC) is a challenging task in real-world applications, such as project document classification which led us to conduct this research. In the past decade, deep neural networks (DNNs) have been explored in MLC due to their flexibility in dealing with annotated data. However, DNN-based MLC still suffers many problems. Two critical problems are data imbalance and label correlation. These two problems will become more prominent when a training dataset is limited and with a large label set. In this study, special neural network configurations were developed to enhance the performance of DNN-based MLC based on data imbalance and label correlation. The classification accuracy of minority labels and users-preferred labels was increased using customized label groups. The proposed method was evaluated using river restoration project documents and other fifteen datasets. The results show that the proposed method generally increases f1-score for minority labels up to 10%. Adding label dependence into label groups improves the f1-score of user-preferred majority labels up to 5%. The accuracy increase varies in different datasets.}
}
@article{CHEN2022108980,
title = {CAAN: Context-Aware attention network for visual question answering},
journal = {Pattern Recognition},
volume = {132},
pages = {108980},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108980},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004605},
author = {Chongqing Chen and Dezhi Han and Chin-Chen Chang},
keywords = {Visual question answering, Attention mechanism, Understanding bias, Absolute position, Contextual information},
abstract = {Understanding multimodal information is the key to visual question answering (VQA) tasks. Most existing approaches use attention mechanisms to acquire fine-grained information understanding. However, these approaches with merely attention mechanisms do not solve the potential understanding bias problem. Hence, this paper introduces contextual information into VQA for the first time and presents a context-aware attention network (CAAN) to tackle the case. By improving the modular co-attention network (MCAN) framework, CAAN’s main work includes: designing a novel absolute position calculation method based on the coordinates of each image region in the image and the image’s actual size, the position information of all image regions are integrated as contextual information to enhance the visual representation; based on the question itself, several internal contextual information representations are introduced to participate in the modeling of the question words, solving the understanding bias caused by the similarity of the question. Additionally, we also designed two models of different scales, namely CAAN-base and CAAN-large, to explore the effect of the field of view on interaction. Finally, extensive experimental results show that CAAN significantly outperforms MCAN and achieves comparable or even better performance than other state-of-the-art approaches, proving our method can tackle the understanding bias.}
}
@article{ZHENG2023108991,
title = {Margin embedding net for robust margin collaborative representation-based classification},
journal = {Pattern Recognition},
volume = {133},
pages = {108991},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108991},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200471X},
author = {Zhichao Zheng and Huaijiang Sun and Ying Zhou},
keywords = {Collaborative representation, Feature extraction, Marginal sample, Image classification},
abstract = {Collaborative Representation-based Classification method (CRC) shows great potential in classification task. However, redundancies in both features and samples limit the application of CRC seriously. The existing works only solve one of them and ignore the other, which leads to performance degradation. To address this problem, we explore collaborative representation mechanism and propose a classification method termed Robust Margin Collaborative Representation-based Classification (RMCRC) which uses a few but more representative robust marginal samples to eliminate redundancy between samples. As the performance of RMCRC is related to robust marginal samples and class separability assumption closely, we further propose a feature extraction method termed Margin Embedding Net (MEN) for RMCRC. In MEN, virtual samples are generated by a generative model to enhance effectiveness of robust marginal samples and generalizability of RMCRC. Then, an embedding network with triplet loss is used to eliminate the redundancy in features and ensure the assumption is satisfied. Specifically, we construct triplet according to the collaborative representation. Hence, MEN fits RMCRC very well. Extensive experimental results validate effectiveness of proposed method.}
}
@article{HE2023109028,
title = {Temporal sparse adversarial attack on sequence-based gait recognition},
journal = {Pattern Recognition},
volume = {133},
pages = {109028},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109028},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005088},
author = {Ziwen He and Wei Wang and Jing Dong and Tieniu Tan},
keywords = {Adversarial attack, Gait recognition, Temporal sparsity},
abstract = {Gait recognition is widely used in social security applications due to its advantages in long-distance human identification. Recently, sequence-based methods have achieved high accuracy by learning abundant temporal and spatial information. However, their robustness under adversarial attacks in an open world has not been clearly explored. In this paper, we demonstrate that the state-of-the-art gait recognition model is vulnerable to such attacks. To this end, we propose a novel temporal sparse adversarial attack method. Different from previous additive noise models which add perturbations on original samples, we employ a generative adversarial network based architecture to semantically generate adversarial high-quality gait silhouettes or video frames. Moreover, by sparsely substituting or inserting a few adversarial gait silhouettes, the proposed method ensures its imperceptibility and achieves a strong attack ability. The experimental results show that if only one-fortieth of the frames are attacked, the accuracy of the target model drops dramatically.}
}
@article{LIU2022108959,
title = {Dynamic self-attention with vision synchronization networks for video question answering},
journal = {Pattern Recognition},
volume = {132},
pages = {108959},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108959},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004393},
author = {Yun Liu and Xiaoming Zhang and Feiran Huang and Shixun Shen and Peng Tian and Lang Li and Zhoujun Li},
keywords = {Video question answering, Dynamic self-attention, Vision synchronization},
abstract = {Video Question Answering (VideoQA) has gained increasing attention as an important task in understanding the rich spatio-temporal contents, i.e., the appearance and motion in the video. However, existing approaches mainly use the question to learn attentions over all the sampled appearance and motion features separately, which neglect two properties of VideoQA: (1) the answer to the question is often reflected on a few frames and video clips, and most video contents are superfluous; (2) appearance and motion features are usually concomitant and complementary to each other in time series. In this paper, we propose a novel VideoQA model, i.e., Dynamic Self-Attention with Vision Synchronization Networks (DSAVS), to address these problems. Specifically, a gated token selection mechanism is proposed to dynamically select the important tokens from appearance and motion sequences. These chosen tokens are fed into a self-attention mechanism to model the internal dependencies for more effective representation learning. To capture the correlation between the appearance and motion features, a vision synchronization block is proposed to synchronize the two types of vision features at the time slice level. Then, the visual objects can be correlated with their corresponding activities and the performance is further improved. Extensive experiments conducted on three public VideoQA data sets confirm the effectivity and superiority of our model compared with state-of-the-art methods.}
}
@article{SOHRAB2023108999,
title = {Graph-embedded subspace support vector data description},
journal = {Pattern Recognition},
volume = {133},
pages = {108999},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108999},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004794},
author = {Fahad Sohrab and Alexandros Iosifidis and Moncef Gabbouj and Jenni Raitoharju},
keywords = {One-Class classification, Support vector data description, Subspace learning, Spectral regression},
abstract = {In this paper, we propose a novel subspace learning framework for one-class classification. The proposed framework presents the problem in the form of graph embedding. It includes the previously proposed subspace one-class techniques as its special cases and provides further insight on what these techniques actually optimize. The framework allows to incorporate other meaningful optimization goals via the graph preserving criterion and reveals a spectral solution and a spectral regression-based solution as alternatives to the previously used gradient-based technique. We combine the subspace learning framework iteratively with Support Vector Data Description applied in the subspace to formulate Graph-Embedded Subspace Support Vector Data Description. We experimentally analyzed the performance of newly proposed different variants. We demonstrate improved performance against the baselines and the recently proposed subspace learning methods for one-class classification.}
}
@article{ZHAO2022108947,
title = {Siamese networks with an online reweighted example for imbalanced data learning},
journal = {Pattern Recognition},
volume = {132},
pages = {108947},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108947},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004277},
author = {Linchang Zhao and Zhaowei Shang and Jin Tan and Mingliang Zhou and Mu Zhang and Dagang Gu and Taiping Zhang and Yuan Yan Tang},
keywords = {Few-shot learning, Reweighted example learning, Data mining, Imbalanced learning},
abstract = {One key challenging problem in data mining and decision-making is to establish a decision support system based on unbalanced datasets. In this study, we propose a novel algorithm to handle unbalanced learning problems that integrates the advantages of Siamese convolutional neural networks (SCNN) and the online reweighted example (ORE) algorithm into a unified method. First, the SCNN model is established for learning and extracting deep feature features at different levels. Second, the ORE algorithm is used to address the problem of data with a class-imbalanced distribution. Compared with baseline approaches, the experimental results show that our proposed method substantially enhances the performance of both within-project defect prediction and cross-project defect prediction.}
}
@article{BAI2022108975,
title = {Self-supervised spectral clustering with exemplar constraints},
journal = {Pattern Recognition},
volume = {132},
pages = {108975},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108975},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004551},
author = {Liang Bai and Yunxiao Zhao and Jiye Liang},
keywords = {Spectral clustering, Self-supervised algorithm, Exemplar constraint, Optimization model},
abstract = {As a leading graph clustering technique, spectral clustering is one of the most widely used clustering methods that captures complex clusters in data. However, some of its deficiencies, such as the high computational complexity in eigen decomposition and the guidance without supervised information, limit its real applications. To get rid of the deficiencies, we propose a self-supervised spectral clustering algorithm. In this algorithm, we define an exemplar constraint which reflects the relations between objects and exemplars. We provide the related analysis to show that it is more suitable for unsupervised learning. Based on the exemplar constraint, we build an optimization model for self-supervised spectral clustering so that we can simultaneously learn clustering results and exemplar constraints. Furthermore, we propose an iterative method to solve the new optimization problem. Compared to other existing versions of spectral clustering algorithms, the new algorithm can use the low computational costs to discover a high-quality cluster structure of a data set without prior information. Furthermore, we did a number of experiments of algorithm comparison and parameter analysis on benchmark data sets to illustrate that the proposed algorithm is very effective and efficient.}
}
@article{LI2022108922,
title = {Kernel dependence regularizers and Gaussian processes with applications to algorithmic fairness},
journal = {Pattern Recognition},
volume = {132},
pages = {108922},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108922},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004034},
author = {Zhu Li and Adrián Pérez-Suay and Gustau Camps-Valls and Dino Sejdinovic},
keywords = {Fairness, Kernel methods, Gaussian processes, Regularization, Hilbert-Schmidt independence criterion},
abstract = {Current adoption of machine learning in industrial, societal and economical activities has raised concerns about the fairness, equity and ethics of automated decisions. Predictive models are often developed using biased datasets and thus retain or even exacerbate biases in their decisions and recommendations. Removing the sensitive covariates, such as gender or race, is insufficient to remedy this issue since the biases may be retained due to other related covariates. We present a regularization approach to this problem that trades off predictive accuracy of the learned models (with respect to biased labels) for the fairness in terms of statistical parity, i.e. independence of the decisions from the sensitive covariates. In particular, we consider a general framework of regularized empirical risk minimization over reproducing kernel Hilbert spaces and impose an additional regularizer of dependence between predictors and sensitive covariates using kernel-based measures of dependence, namely the Hilbert-Schmidt Independence Criterion (HSIC) and its normalized version. This approach leads to a closed-form solution in the case of squared loss, i.e. ridge regression. We also provide statistical consistency results for both risk and fairness bound for our approach. Moreover, we show that the dependence regularizer has an interpretation as modifying the corresponding Gaussian process (GP) prior. As a consequence, a GP model with a prior that encourages fairness to sensitive variables can be derived, allowing principled hyperparameter selection and studying of the relative relevance of covariates under fairness constraints. Experimental results in synthetic examples and in real problems of income and crime prediction illustrate the potential of the approach to improve fairness of automated decisions.}
}
@article{ZHANG2023109020,
title = {Pyramid Geometric Consistency Learning For Semantic Segmentation},
journal = {Pattern Recognition},
volume = {133},
pages = {109020},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109020},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005003},
author = {Xian Zhang and Qiang Li and Zhibin Quan and Wankou Yang},
keywords = {Semantic segmentation, Consistency learning, Supervised contrastive learning},
abstract = {Semantic segmentation is a critical in vision fields. Randomly transforms each image into different augmented samples and supervise the views with transformed semantics labels. However, even if the views are expanded from the same sample, the prediction results obtained by the same network will be very different. Therefore, we argue that between the augmented samples, the transformation-equivariance and the representational consistency also need to be supervised. Motivated by this, we propose a simple cross-data augmentation for semantic segmentation, in which we also leverage the pixel-level consistency constraint learning between pairs of augmented samples. As a result, our scheme significantly can improve the performances of existing semantic segmentation models without additional computation overhead. We verified the effectiveness of this method on Deeplab V3 Plus. Experiments show that our method can achieve stable performance improvement on mainstream data sets such as Pascal VOC 2012, Camvid, Cityscapes, etc.}
}
@article{SONG2023109015,
title = {Answering knowledge-based visual questions via the exploration of Question Purpose},
journal = {Pattern Recognition},
volume = {133},
pages = {109015},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109015},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004952},
author = {Lingyun Song and Jianao Li and Jun Liu and Yang Yang and Xuequn Shang and Mingxuan Sun},
keywords = {Visual question answering, DNN, Question Purpose},
abstract = {Visual question answering has been greatly advanced by deep learning technologies, but still remains an open problem subjected to two aspects of factors. First, previous works estimate the correctness of each candidate answer mainly by its semantic correlations with visual questions, overlooking the fact that some questions and their answers are semantically inconsistent. Second, previous works that require external knowledge mainly uses the knowledge facts retrieved by key words or visual objects. However, the retrieved knowledge facts may only be related to the semantics of the question, but are useless or even misleading for answer prediction. To address these issues, we investigate how to capture the purpose of visual questions and propose a Purpose Guided Visual Question Answering model, called PGVQA. It mainly has two appealing properties: (1) It can estimate the correctness of candidate answers based on the Question Purpose (QP) that reveals which aspects of the concept are examined by visual questions. This is helpful for avoiding the negative effect of the semantic inconsistency between answers and questions. (2) It can incorporate the knowledge facts accordant with the QP into answer prediction, which helps to improve the probability of answering visual questions correctly. Empirical studies on benchmark datasets show that PGVQA achieves state-of-the-art performance.}
}
@article{FAN2022108963,
title = {GFNet: Automatic segmentation of COVID-19 lung infection regions using CT images based on boundary features},
journal = {Pattern Recognition},
volume = {132},
pages = {108963},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108963},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004435},
author = {Chaodong Fan and Zhenhuan Zeng and Leyi Xiao and Xilong Qu},
keywords = {Image segmentation, COVID-19, Edge-guidance, Convolutional neural network, CT image},
abstract = {In early 2020, the global spread of the COVID-19 has presented the world with a serious health crisis. Due to the large number of infected patients, automatic segmentation of lung infections using computed tomography (CT) images has great potential to enhance traditional medical strategies. However, the segmentation of infected regions in CT slices still faces many challenges. Specially, the most core problem is the high variability of infection characteristics and the low contrast between the infected and the normal regions. This problem leads to fuzzy regions in lung CT segmentation. To address this problem, we have designed a novel global feature network(GFNet) for COVID-19 lung infections: VGG16 as backbone, we design a Edge-guidance module(Eg) that fuses the features of each layer. First, features are extracted by reverse attention module and Eg is combined with it. This series of steps enables each layer to fully extract boundary details that are difficult to be noticed by previous models, thus solving the fuzzy problem of infected regions. The multi-layer output features are fused into the final output to finally achieve automatic and accurate segmentation of infected areas. We compared the traditional medical segmentation networks, UNet, UNet++, the latest model Inf-Net, and methods of few shot learning field. Experiments show that our model is superior to the above models in Dice, Sensitivity, Specificity and other evaluation metrics, and our segmentation results are clear and accurate from the visual effect, which proves the effectiveness of GFNet. In addition, we verify the generalization ability of GFNet on another “never seen” dataset, and the results prove that our model still has better generalization ability than the above model. Our code has been shared at https://github.com/zengzhenhuan/GFNet.}
}
@article{WANG2023108987,
title = {Better pseudo-label: Joint domain-aware label and dual-classifier for semi-supervised domain generalization},
journal = {Pattern Recognition},
volume = {133},
pages = {108987},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108987},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004678},
author = {Ruiqi Wang and Lei Qi and Yinghuan Shi and Yang Gao},
keywords = {Semi-supervised learning, Domain generalization, Image recognition, Feature representation},
abstract = {With the goal of directly generalizing trained model to unseen target domains, domain generalization (DG), a newly proposed learning paradigm, has attracted considerable attention. Previous DG models usually require a sufficient quantity of annotated samples from observed source domains during training. In this paper, we relax this requirement about full annotation and investigate semi-supervised domain generalization (SSDG) where only one source domain is fully annotated along with the other domains totally unlabeled in the training process. With the challenges of tackling the domain gap between observed source domains and predicting unseen target domains, we propose a novel deep framework via joint domain-aware labels and dual-classifier to produce high-quality pseudo-labels. Concretely, to predict accurate pseudo-labels under domain shift, a domain-aware pseudo-labeling module is developed. Also, considering inconsistent goals between generalization and pseudo-labeling: former prevents overfitting on all source domains while latter might overfit the unlabeled source domains for high accuracy, we employ a dual-classifier to independently perform pseudo-labeling and domain generalization in the training process. When accurate pseudo-labels are generated for unlabeled source domains, the domain mixup operation is applied to augment new domains between labeled and unlabeled domains, which is beneficial for boosting the generalization capability of the model. Extensive results on publicly available DG benchmark datasets show the efficacy of our proposed SSDG method.}
}
@article{XIE2023108974,
title = {WITS: Weakly-supervised individual tooth segmentation model trained on box-level labels},
journal = {Pattern Recognition},
volume = {133},
pages = {108974},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108974},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200454X},
author = {Ruicheng Xie and Yunyun Yang and Zhaoyang Chen},
keywords = {Tooth detection, Deep learning, Active contour, Oral CBCT images, Level set},
abstract = {Accurately and automatically segmenting teeth from cone-beam computed tomography (CBCT) images plays an essential role in dental disease diagnosis and treatment. This paper presents an automatic tooth segmentation model that combines deep learning methods and level-set approaches. The proposed model uses a deep learning method to detect each tooth’s location and size and generates prior ellipses from those detected boundary boxes. Calculating each point’s signed distance to the prior edge and using them as prior weights, the restriction term can constrain the evolution of level set functions according to the distance to the prior ellipses. Then, we use the curvature direction to find out joint points of teeth and employ a variational model to separate them to get individual results. By quantitative evaluation, we show that the proposed model can accurately segment teeth. The performance is more accurate and stable than those of classical level-set models and deep-learning models. For example, the Dice coefficient is increased by 7% than that of the U-Net model. Besides, we will release the code on https://github.com/ruicx/Individual-Tooth-Segmentation-with-Rectangle-Labels.}
}
@article{LIU2022108951,
title = {Alleviating the over-smoothing of graph neural computing by a data augmentation strategy with entropy preservation},
journal = {Pattern Recognition},
volume = {132},
pages = {108951},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108951},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004319},
author = {Xue Liu and Dan Sun and Wei Wei},
keywords = {Graph representation, Graph convolutional networks, Information theory, Graph entropy},
abstract = {The Graph Convolutional Networks (GCN) proposed by Kipf and Welling is an effective model to improve semi-supervised learning of pattern recognition, but faces the obstacle of over-smoothing, which will weaken the representation ability of GCN. Recently some works are proposed to tackle above limitation by randomly perturbing graph topology or feature matrix to generate data augmentations as input for training. However, these operations inevitably do damage to the integrity of information structures and have to sacrifice the smoothness of feature manifold. In this paper, we first introduce a novel graph entropy definition as a measure to quantitatively evaluate the smoothness of a data manifold and then point out that this graph entropy is controlled by triangle motif-based information structures. Considering the preservation of graph entropy, we propose an effective strategy to generate randomly perturbed training data but maintain both graph topology and graph entropy. Extensive experiments have been conducted on real-world datasets and the results verify the effectiveness of our proposed method in improving semi-supervised node classification accuracy compared with a surge of baselines. Beyond that, our proposed approach could significantly enhance the robustness of training process for GCN.}
}
@article{LIU2023109039,
title = {LAE-Net: A locally-adaptive embedding network for low-light image enhancement},
journal = {Pattern Recognition},
volume = {133},
pages = {109039},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109039},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005192},
author = {Xiaokai Liu and Weihao Ma and Xiaorui Ma and Jie Wang},
keywords = {Locally-adaptive, Image enhancement, Multi-distribution, Image entropy, Kernel selection},
abstract = {In the low-light enhancement task, one of the major challenges lies in how to balance the image enhancement properties of light intensity, detail presentation and color fidelity. In natural scenes, the multi-distribution of frequency and illumination characteristics in the spatial domain makes the balance more difficult. To solve this problem, we propose a Locally-Adaptive Embedding Network, namely LAE-Net, to realize high-quality low-light image enhancement with locally-adaptive kernel selection and feature adaptation for multi-distribution issues. Specifically, for the frequency multi-distribution, we rethink the spatial-frequency characteristic of human eyes, experimentally explore the relationship among the receptive field size, the image spatial frequency and the light enhancement properties, and propose an Entropy-Inspired Kernel-Selection Convolution, where each neuron can adaptively adjust the receptive field size according to its spatial frequency characterized by information entropy. For the illumination multi-distribution, we propose an Illumination Attentive Transfer subnet, where the neurons can simultaneously sense global consistency and local details, and accordingly hint where to focus the efforts on, thereby adjusting the refined features. Extensive experiments with ablation analysis show the effectiveness of our method and the proposed method outperforms many related state-of-the-art techniques on four benchmark datasets: MEF, LIME, NPE and DICM.}
}
@article{JUNG2022108958,
title = {Counterfactual explanation based on gradual construction for deep networks},
journal = {Pattern Recognition},
volume = {132},
pages = {108958},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108958},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004381},
author = {Hong-Gyu Jung and Sin-Han Kang and Hee-Dong Kim and Dong-Ok Won and Seong-Whan Lee},
keywords = {Explainable AI, Counterfactual explanation, Interpretability, Model-agnostics, Generative model},
abstract = {To understand the black-box characteristics of deep networks, counterfactual explanation that deduces not only the important features of an input space but also how those features should be modified to classify input as a target class has gained an increasing interest. The patterns that deep networks have learned from a training dataset can be grasped by observing the feature variation among various classes. However, current approaches perform the feature modification to increase the classification probability for the target class irrespective of the internal characteristics of deep networks. This often leads to unclear explanations that deviate from real-world data distributions. To address this problem, we propose a counterfactual explanation method that exploits the statistics learned from a training dataset. Especially, we gradually construct an explanation by iterating over masking and composition steps. The masking step aims to select an important feature from the input data to be classified as a target class. Meanwhile, the composition step aims to optimize the previously selected feature by ensuring that its output score is close to the logit space of the training data that are classified as the target class. Experimental results show that our method produces human-friendly interpretations on various classification datasets and verify that such interpretations can be achieved with fewer feature modification.}
}
@article{ZHANG2023109027,
title = {OW-TAL: Learning Unknown Human Activities for Open-World Temporal Action Localization},
journal = {Pattern Recognition},
volume = {133},
pages = {109027},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109027},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005076},
author = {Yaru Zhang and Xiao-Yu Zhang and Haichao Shi},
keywords = {Temporal action localization, Open-world learning, Self-paced learning},
abstract = {Current temporal action localization methods work well on a closed-world assumption, in which all action categories to be localized are known as a priori. However, this assumption doesn’t apply to open-world scenarios, as novel categories that never appeared in the training stage will be encountered without explicit supervision. Distinct from the closed-world setting, localizing actions under the open-world setup poses two significant challenges: 1) identifying unknown actions from diverse knowns and localizing their temporal boundaries. 2) defying forgetting of previous actions when incrementally updating knowledge of identified unknown actions. To address the aforementioned challenges, we develop a two-branch framework with Unknown and Known action modeling Networks, a.k.a. UK-Net, for the problem of Open-World Temporal Action Localization (OW-TAL). The potential patterns underlying unknown and known actions, as well as their dynamic transformation, are modeled in a unified pipeline. Specifically, a self-attention based position-sensitive module is designed to produce actionness scores for unknown actions in a class-agnostic way. Besides, an iterative optimization strategy is developed to enable knowledge derived from known categories to be shared with the unknowns. In addition, a self-paced learning strategy is proposed to instructionally guide class-incremental learning while defying catastrophic forgetting. Benefiting from the above components, our UK-Net yields superior performance on three challenging datasets, i.e., THUMOS14, ActivityNet1.2, and MUSES. Experimental results also demonstrate the competitive performance of our method when compared with traditional closed-world counterparts.}
}
@article{BOSQUET2023108998,
title = {A full data augmentation pipeline for small object detection based on generative adversarial networks},
journal = {Pattern Recognition},
volume = {133},
pages = {108998},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108998},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004782},
author = {Brais Bosquet and Daniel Cores and Lorenzo Seidenari and Víctor M. Brea and Manuel Mucientes and Alberto Del Bimbo},
keywords = {Small object detection, Data augmentation, Generative adversarial network},
abstract = {Object detection accuracy on small objects, i.e., objects under 32 × 32 pixels, lags behind that of large ones. To address this issue, innovative architectures have been designed and new datasets have been released. Still, the number of small objects in many datasets does not suffice for training. The advent of the generative adversarial networks (GANs) opens up a new data augmentation possibility for training architectures without the costly task of annotating huge datasets for small objects. In this paper, we propose a full pipeline for data augmentation for small object detection which combines a GAN-based object generator with techniques of object segmentation, image inpainting, and image blending to achieve high-quality synthetic data. The main component of our pipeline is DS-GAN, a novel GAN-based architecture that generates realistic small objects from larger ones. Experimental results show that our overall data augmentation method improves the performance of state-of-the-art models up to 11.9% APs@.5 on UAVDT and by 4.7% APs@.5 on iSAID, both for the small objects subset and for a scenario where the number of training instances is limited.}
}
@article{LI2022108946,
title = {Table Structure Recognition and Form Parsing by End-to-End Object Detection and Relation Parsing},
journal = {Pattern Recognition},
volume = {132},
pages = {108946},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108946},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004265},
author = {Xiao-Hui Li and Fei Yin and He-Sen Dai and Cheng-Lin Liu},
keywords = {Table detection, Table structure recognition, Template-free form parsing, Graph neural network, End-to-end training},
abstract = {The recognition of two-dimensional structure of tables and forms from document images is a challenge due to the complexity of document structures and the diversity of layouts. In this paper, we propose a graph neural network (GNN) based unified framework named Table Structure Recognition Network (TSRNet) to jointly detect and recognize the structures of various tables and forms. First, a multi-task fully convolutional network (FCN) is used to segment primitive regions such as text segments and ruling lines from document images, then a GNN is used to classify and group these primitive regions into page objects such as tables and cells. At last, the relationships between neighboring page objects are analyzed using another GNN based parsing module. The parameters of all the modules in the system can be trained end-to-end to optimize the overall performance. Experiments of table detection and structure recognition for modern documents on the POD 2017, cTDaR 2019 and PubTabNet datasets and template-free form parsing for historical documents on the NAF dataset show that the proposed method can handle various table/form structures and achieve superior performance.}
}
@article{WU2022108957,
title = {Covered Style Mining via Generative Adversarial Networks for Face Anti-spoofing},
journal = {Pattern Recognition},
volume = {132},
pages = {108957},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108957},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200437X},
author = {Yiqiang Wu and Dapeng Tao and Yong Luo and Jun Cheng and Xuelong Li},
keywords = {Face anti-spoofing, Generative adversarial networks, Deep learning},
abstract = {Face anti-spoofing, a biometric authentication method, is a central part of automatic face recognition. Recently, two sets of approaches have performed particularly well against presentation attacks: 1) pixel-wise supervision-based methods, which intend to provide fine-grained pixel information to learn specific auxiliary maps; and 2) anomaly detection-based methods, which regard face anti-spoofing as an open-set training task and learn spoof detectors using only bona fide data, where the detectors are shown to generalize well to unknown attacks. However, these approaches depend on handcrafted prior information to control the generation of intermediate difference maps and easily fall into local optima. In this paper, we propose a novel frame-level face anti-spoofing method, Covered Style Mining-GAN (CSM-GAN), which converts face anti-spoofing detection into a style transfer process without any prior information. Specifically, CSM-GAN has four main components: the Covered Style Encoder (CSE), responsible for mining the difference map containing the photography style and discriminative clues; the Auxiliary Style Classifier (ASC), consisting of several stacked Difference Capture Blocks (DCB) responsible for distinguishing bona fide faces from spoofing faces; and the Style Transfer Generator (STG) and Style Adversarial Discriminator (SAD), which form generative adversarial networks to achieve style transfer. Comprehensive experiments on several benchmark datasets show that the proposed method not only outperforms current state-of-the-art but also produces better visual diversity in difference maps.}
}
@article{QASIMABBAS2023109031,
title = {Transformed domain convolutional neural network for Alzheimer's disease diagnosis using structural MRI},
journal = {Pattern Recognition},
volume = {133},
pages = {109031},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109031},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005118},
author = {S. {Qasim Abbas} and Lianhua Chi and Yi-Ping Phoebe Chen},
keywords = {Alzheimer disease (AD) detection, Brain disease, Convolutional neural network (CNN), Supervised learning, Structural magnetic resonance imaging (sMRI), Transform domain AD classification, AD diagnosis},
abstract = {Structural magnetic resonance imaging (sMRI) has become a prevalent and potent imaging modality for the computer-aided diagnosis (CAD) of neurological diseases like dementia. Recently, a handful of deep learning techniques such as convolutional neural networks (CNNs) have been proposed to diagnose Alzheimer's disease (AD) by learning the atrophy patterns available in sMRIs. Although CNN-based techniques have demonstrated superior performance and characteristics compared to conventional learning-based classifiers, their diagnostic performance still needs to be improved for reliable classification results. The drawback of current CNN-based approaches is the requirement to locate discriminative landmark (LM) locations by identifying regions of interest (ROIs) in sMRIs, thus the performance of the whole framework is highly influenced by the LM detection step. To overcome this issue, we propose a novel three-dimensional Jacobian domain convolutional neural network (JD-CNN) to diagnose AD subjects and achieve excellent classification performance without the involvement of the LM detection framework. We train the proposed JD-CNN model on the basis of features generated by transforming the sMRI from the spatial domain to the Jacobian domain. The proposed JD-CNN is evaluated on baseline T1-weighted sMRI data collected from 154 healthy control (HC) and 84 Alzheimer's disease (AD) subjects in the Alzheimer's disease neuroimaging initiative (ADNI) database. The proposed JD-CNN exhibits superior classification performance to previously reported state-of-the-art techniques.}
}
@article{CHEN2023108986,
title = {Few-shot learning with unsupervised part discovery and part-aligned similarity},
journal = {Pattern Recognition},
volume = {133},
pages = {108986},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108986},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004666},
author = {Wentao Chen and Zhang Zhang and Wei Wang and Liang Wang and Zilei Wang and Tieniu Tan},
keywords = {Few-shot learning, Self-supervised learning, Part discovery network, Part-aligned similarity},
abstract = {Few-shot learning aims to recognize novel concepts with only a few examples. To this end, previous studies resort to acquiring a strong inductive bias via meta-learning on a group of similar tasks, which however needs a large labeled base dataset to sample training tasks. In this paper, we show that such inductive bias can be learned from a flat collection of unlabeled images, and instantiated as transferable representations among seen and unseen classes. Specifically, we propose a novel unsupervised Part Discovery Network (PDN) to learn transferable representations from unlabeled images, which automatically selects the most discriminative part from an input image and then maximizes its similarities to the global view of the input and other neighbors with similar semantics. To better leverage the learned representations for few-shot learning, we further propose Part-Aligned Similarity (PAS), the key of which is to measure image similarities based on a set of discriminative and aligned parts. We conduct extensive studies on five popular few-shot learning datasets to evaluate our approach. The experimental results show that our approach outperforms previous unsupervised methods by a large margin and is even comparable with state-of-the-art supervised methods.}
}
@article{HAN2022108934,
title = {Single image based 3D human pose estimation via uncertainty learning},
journal = {Pattern Recognition},
volume = {132},
pages = {108934},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108934},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004149},
author = {Chuchu Han and Xin Yu and Changxin Gao and Nong Sang and Yi Yang},
keywords = {Uncertainty, 3D pose estimation, Graph convolutional network},
abstract = {In monocular image scenes, 3D human pose estimation exhibits inherent ambiguity due to the loss of depth information and occlusions. Simply regressing body joints with high uncertainties will lead to model overfitting and poor generalization. In this paper, we propose an uncertainty-based framework to jointly learn 3D human poses and the uncertainty of each joint. Our proposed joint estimation framework aims to mitigate the adverse effects of training samples with high uncertainties and facilitate the training procedure. To be specific, we model each body joint as a Laplace distribution for uncertainty representation. Since visual joints often exhibit low uncertainties while occluded ones have high uncertainties, we develop an adaptive scaling factor, named the uncertainty-aware scaling factor, to ease the network optimization in accordance with the joint uncertainties. By doing so, our network is able to converge faster and significantly reduce the adverse effects caused by those ambiguous joints. Furthermore, we present an uncertainty-aware graph convolutional network by exploiting the learned joint uncertainties and the relationships among joints to refine the initial joint localization. Extensive experiments on single-person (Human3.6M) and multi-person (MuCo-3DHP & MuPoTS-3D) 3D human pose estimation datasets demonstrate the effectiveness of our method.}
}
@article{MORENOPINO2023109014,
title = {Deep autoregressive models with spectral attention},
journal = {Pattern Recognition},
volume = {133},
pages = {109014},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109014},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004940},
author = {Fernando Moreno-Pino and Pablo M. Olmos and Antonio Artés-Rodríguez},
keywords = {Attention models, Deep learning, Filtering, Global-local contexts, Signal processing, Spectral domain attention, Time series forecasting},
abstract = {Time series forecasting is an important problem across many domains, playing a crucial role in multiple real-world applications. In this paper, we propose a forecasting architecture that combines deep autoregressive models with a Spectral Attention (SA) module, which merges global and local frequency domain information in the model’s embedded space. By characterizing in the spectral domain the embedding of the time series as occurrences of a random process, our method can identify global trends and seasonality patterns. Two spectral attention models, global and local to the time series, integrate this information within the forecast and perform spectral filtering to remove time series’s noise. The proposed architecture has a number of useful properties: it can be effectively incorporated into well-known forecast architectures, requiring a low number of parameters and producing explainable results that improve forecasting accuracy. We test the Spectral Attention Autoregressive Model (SAAM) on several well-known forecast datasets, consistently demonstrating that our model compares favorably to state-of-the-art approaches.}
}
@article{TONG2022108962,
title = {Neural architecture search via reference point based multi‐objective evolutionary algorithm},
journal = {Pattern Recognition},
volume = {132},
pages = {108962},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108962},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004423},
author = {Lyuyang Tong and Bo Du},
keywords = {Neural architecture search, Multi-objective evolutionary algorithm, The image classification},
abstract = {For neural architecture search, NSGA-Net has searched a representative neural architecture set of Pareto-optimal solutions to consider both accuracy and computation complexity simultaneously. However, some decision-makers only concentrate on such neural architectures in the subpart regions of Pareto-optimal Frontier that they have interests in. Under the above circumstances, certain uninterested neural architectures may cost many computing resources. In order to consider the preference of decision-makers, we propose the reference point based NSGA-Net (RNSGA-Net) for neural architecture search. The core of RNSGA-Net adopts the reference point approach to guarantee the Pareto-optimal region close to the reference points and also combines the advantage of NSGAII with the fast nondominated sorting approach to split the Pareto front. Moreover, we augment an extra bit value of the original encoding to represent two types of residual block and one type of dense block for residual connection and dense connection in the RNSGA-Net. In order to satisfy the decision-maker preference, the multi-objective is measured to search competitive neural architecture by minimizing an error metric and FLOPs of computational complexity. Experiment results on the CIFAR-10 dataset demonstrate that RNSGA-Net can improve NSGA-Net in terms of the more structured representation space and the preference of decision-makers.}
}
@article{HE2023108990,
title = {Co-Attention Fusion Network for Multimodal Skin Cancer Diagnosis},
journal = {Pattern Recognition},
volume = {133},
pages = {108990},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108990},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004708},
author = {Xiaoyu He and Yong Wang and Shuang Zhao and Xiang Chen},
keywords = {Skin cancer diagnosis, Convolutional neural networks, Multimodal fusion, Attention mechanism},
abstract = {Recently, multimodal image-based methods have shown great performance in skin cancer diagnosis. These methods usually use convolutional neural networks (CNNs) to extract the features of two modalities (i.e., dermoscopy and clinical images), and fuse these features for classification. However, they commonly have the following two shortcomings: 1) the feature extraction processes of the two modalities are independent and lack cooperation, which may lead to limited representation ability of the extracted features, and 2) the multimodal fusion operation is a simple concatenation followed by convolutions, thus causing rough fusion features. To address these two issues, we propose a co-attention fusion network (CAFNet), which uses two branches to extract the features of dermoscopy and clinical images and a hyper-branch to refine and fuse these features at all stages of the network. Specifically, the hyper-branch is composed of multiple co-attention fusion (CAF) modules. In each CAF module, we first design a co-attention (CA) block with a cross-modal attention mechanism to achieve the cooperation of two modalities, which enhances the representation ability of the extracted features through mutual guidance between the two modalities. Following the CA block, we further propose an attention fusion (AF) block that dynamically selects appropriate fusion ratios to conduct the pixel-wise multimodal fusion, which can generate fine-grained fusion features. In addition, we propose a deep-supervised loss and a combined prediction method to obtain a more robust prediction result. The results show that CAFNet achieves the average accuracy of 76.8% on the seven-point checklist dataset and outperforms state-of-the-art methods.}
}
@article{HE2023109038,
title = {Single image super‐resolution based on progressive fusion of orientation‐aware features},
journal = {Pattern Recognition},
volume = {133},
pages = {109038},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109038},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005180},
author = {Zewei He and Du Chen and Yanpeng Cao and Jiangxin Yang and Yanlong Cao and Xin Li and Siliang Tang and Yueting Zhuang and Zhe-ming Lu},
keywords = {Single image super-resolution, Channel attention, Orientation-aware, Feature extraction, Feature fusion},
abstract = {Single image super-resolution (SISR) is an active research topic in the fields of image processing, computer vision and pattern recognition, restoring high-frequency details and textures based on the low-resolution input image. In this paper, we aim to build more accurate and faster SISR models via developing better-performing feature extraction and fusion techniques. Firstly, we proposed a novel Orientation-Aware feature extraction/selection Module (OAM), which contains a mixture of 1D and 2D convolutional kernels (i.e., 3×1, 1×3, and 3×3) for extracting orientation-aware features. The channel attention mechanism is deployed within each OAM, performing scene-specific selection of informative outputs of the orientation-dependent kernels (e.g., horizontal, vertical, and diagonal). Secondly, we present an effective fusion architecture to progressively integrate multi-scale features extracted in different convolutional stages. Instead of directly combining low-level and high-level features, similar outputs of adjacent feature extraction modules are grouped and further compressed to generate a more concise representation of a specific convolutional stage for high-accuracy SISR task. Based on the above two important improvements, we present a compact but effective CNN-based model for high-quality SISR via Progressive Fusion of Orientation-Aware features (SISR-PF-OA). Extensive experimental results verify the superiority of the proposed SISR-PF-OA model, performing favorably against the state-of-the-art models in terms of both restoration accuracy and computational efficiency (e.g., SISR-PF-OA outperforms RCAN model, achieving higher PSNR 31.25 dB vs. 31.21 dB and using fewer FLOPs 764.41 G vs. 1020.28 G on the Manga109 dataset for scale factor ×4 SISR task.). The source codes will be made publicly available.}
}
@article{BEHJATI2023108997,
title = {Single image super-resolution based on directional variance attention network},
journal = {Pattern Recognition},
volume = {133},
pages = {108997},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108997},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004770},
author = {Parichehr Behjati and Pau Rodriguez and Carles Fernández and Isabelle Hupont and Armin Mehri and Jordi Gonzàlez},
keywords = {Single image super-resolution, Efficient network, Attention mechanism},
abstract = {Recent advances in single image super-resolution (SISR) explore the power of deep convolutional neural networks (CNNs) to achieve better performance. However, most of the progress has been made by scaling CNN architectures, which usually raise computational demands and memory consumption. This makes modern architectures less applicable in practice. In addition, most CNN-based SR methods do not fully utilize the informative hierarchical features that are helpful for final image recovery. In order to address these issues, we propose a directional variance attention network (DiVANet), a computationally efficient yet accurate network for SISR. Specifically, we introduce a novel directional variance attention (DiVA) mechanism to capture long-range spatial dependencies and exploit inter-channel dependencies simultaneously for more discriminative representations. Furthermore, we propose a residual attention feature group (RAFG) for parallelizing attention and residual block computation. The output of each residual block is linearly fused at the RAFG output to provide access to the whole feature hierarchy. In parallel, DiVA extracts most relevant features from the network for improving the final output and preventing information loss along the successive operations inside the network. Experimental results demonstrate the superiority of DiVANet over the state of the art in several datasets, while maintaining relatively low computation and memory footprint. The code is available at https://github.com/pbehjatii/DiVANet.}
}
@article{LIN2023109026,
title = {Image manipulation detection by multiple tampering traces and edge artifact enhancement},
journal = {Pattern Recognition},
volume = {133},
pages = {109026},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109026},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005064},
author = {Xun Lin and Shuai Wang and Jiahao Deng and Ying Fu and Xiao Bai and Xinlei Chen and Xiaolei Qu and Wenzhong Tang},
keywords = {Image manipulation detection, Transformer, Edge artifact enhancement, Edge supervision},
abstract = {Image manipulation detection has attracted considerable attention owing to the increasing security risks posed by fake images. Previous studies have proven that tampering traces hidden in images are essential for detecting manipulated regions. However, existing methods have limitations in generalization and the ability to tackle post-processing methods. This paper presents a novel Network to learn and Enhance Multiple tampering Traces (EMT-Net), including noise distribution and visual artifacts. For better generalization, EMT-Net extracts global and local noise features from noise maps using transformers and captures local visual artifacts from original RGB images using convolutional neural networks. Moreover, we enhance fused tampering traces using the proposed edge artifacts enhancement modules and edge supervision strategy to discover subtle edge artifacts hidden in images. Thus, EMT-Net can prevent the risks of losing slight visual clues against well-designed post-processing methods. Experimental results indicate that the proposed method can detect manipulated regions and outperform state-of-the-art approaches under comprehensive quantitative metrics and visual qualities. In addition, EMT-Net shows robustness when various post-processing methods further manipulate images.}
}
@article{AUDIBERT2022108945,
title = {Do deep neural networks contribute to multivariate time series anomaly detection?},
journal = {Pattern Recognition},
volume = {132},
pages = {108945},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108945},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004253},
author = {Julien Audibert and Pietro Michiardi and Frédéric Guyard and Sébastien Marti and Maria A. Zuluaga},
keywords = {Anomaly detection, Multivariate time series, Neural networks},
abstract = {Anomaly detection in time series is a complex task that has been widely studied. In recent years, the ability of unsupervised anomaly detection algorithms has received much attention. This trend has led researchers to compare only learning-based methods in their articles, abandoning some more conventional approaches. As a result, the community in this field has been encouraged to propose increasingly complex learning-based models mainly based on deep neural networks. To our knowledge, there are no comparative studies between conventional, machine learning-based and, deep neural network methods for the detection of anomalies in multivariate time series. In this work, we study the anomaly detection performance of sixteen conventional, machine learning-based and, deep neural network approaches on five real-world open datasets. By analyzing and comparing the performance of each of the sixteen methods, we show that no family of methods outperforms the others. Therefore, we encourage the community to reincorporate the three categories of methods in the anomaly detection in multivariate time series benchmarks.}
}
@article{SUN2022108969,
title = {Stochastic gate-based autoencoder for unsupervised hyperspectral band selection},
journal = {Pattern Recognition},
volume = {132},
pages = {108969},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108969},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004496},
author = {He Sun and Lei Zhang and Lizhi Wang and Hua Huang},
keywords = {Hyperspectral data, Unsupervised band selection, Autoencoder, Stochastic gate},
abstract = {Due to its strong feature representation ability, the deep learning (DL)-based method is preferable for the unsupervised band selection task of hyperspectral image (HSI). However, the current DL-based UBS methods have not further investigated the nonlinear relationship between spectral bands, a more robust DL model with effective loss function is desired. To solve the above problem, a novel stochastic gate-based autoencoder (SGAE) has been proposed for the UBS task. With the proposed stochastic gate layer, the desired band subset with learnable parameters can be directly obtained. For obtaining better UBS results, a nonlinear regularization term is added with the loss function to supervise the training process of SGAE. Furthermore, an early stopping criteria with a regularization term-based threshold is developed. Experimental results on four publicly available remote sensing datasets prove the effectiveness of our SGAE.}
}
@article{WANG2022108961,
title = {Learnable dynamic margin in deep metric learning},
journal = {Pattern Recognition},
volume = {132},
pages = {108961},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108961},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004411},
author = {Yifan Wang and Pingping Liu and Yijun Lang and Qiuzhan Zhou and Xue Shan},
keywords = {Deep metric learning, Proxy-based loss, Adaptive margin, Image retrieval, Fine-grained images},
abstract = {With the deepening of deep neural network research, deep metric learning has been further developed and achieved good results in many computer vision tasks. Deep metric learning trains the deep neural network by designing appropriate loss functions, and the deep neural network projects the training samples into an embedding space, where similar samples are very close, while dissimilar samples are far away. In the past two years, the proxy-based loss achieves remarkable improvements, boosts the speed of convergence and is robust against noisy labels and outliers due to the introduction of proxies. In the previous proxy-based losses, fixed margins were used to achieve the goal of metric learning, but the intra-class variance of fine-grained images were not fully considered. In this paper, a new proxy-based loss is proposed, which aims to set a learnable margin for each class, so that the intra-class variance can be better maintained in the final embedding space. Moreover, we also add a loss between proxies, so as to improve the discrimination between classes and further maintain the intra-class distribution. Our method is evaluated on fine-grained image retrieval, person re-identification and remote sensing image retrieval common benchmarks. The standard network trained by our loss achieves state-of-the-art performance. Thus, the possibility of extending our method to different fields of pattern recognition is confirmed.}
}
@article{ZHENG2023109009,
title = {Robust Physical-World Attacks on Face Recognition},
journal = {Pattern Recognition},
volume = {133},
pages = {109009},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109009},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004897},
author = {Xin Zheng and Yanbo Fan and Baoyuan Wu and Yong Zhang and Jue Wang and Shirui Pan},
keywords = {Physical-world adversarial attack, Face recognition, Environmental variations, Curriculum learning},
abstract = {Face recognition has been greatly facilitated by the development of deep neural networks (DNNs) and has been widely applied to many safety-critical applications. However, recent studies have shown that DNNs are very vulnerable to adversarial examples, raising severe concerns on the security of real-world face recognition. In this work, we study sticker-based physical attacks on face recognition for better understanding its adversarial robustness. To this end, we first analyze in-depth the complicated physical-world conditions confronted by attacking face recognition, including the different variations of stickers, faces, and environmental conditions. Then, we propose a novel robust physical attack framework, dubbed PadvFace, to model these challenging variations specifically. Furthermore, we reveal that the attack complexities vary under different physical-world conditions and propose an efficient Curriculum Adversarial Attack (CAA) algorithm that gradually adapts adversarial stickers to environmental variations from easy to complex. Finally, we construct a standardized testing protocol to facilitate the fair evaluation of physical attacks on face recognition, and extensive experiments on both physical dodging and impersonation attacks demonstrate the superior performance of the proposed method.}
}
@article{GIULIVI2023108985,
title = {Adversarial scratches: Deployable attacks to CNN classifiers},
journal = {Pattern Recognition},
volume = {133},
pages = {108985},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108985},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004654},
author = {Loris Giulivi and Malhar Jere and Loris Rossi and Farinaz Koushanfar and Gabriela Ciocarlie and Briland Hitaj and Giacomo Boracchi},
keywords = {Adversarial perturbations, Adversarial attacks, Deep learning, Convolutional neural networks, Bézier curves},
abstract = {A growing body of work has shown that deep neural networks are susceptible to adversarial examples. These take the form of small perturbations applied to the model’s input which lead to incorrect predictions. Unfortunately, most literature focuses on visually imperceivable perturbations to be applied to digital images that often are, by design, impossible to be deployed to physical targets. We present Adversarial Scratches: a novel L0 black-box attack, which takes the form of scratches in images, and which possesses much greater deployability than other state-of-the-art attacks. Adversarial Scratches leverage Bézier Curves to reduce the dimension of the search space and possibly constrain the attack to a specific location. We test Adversarial Scratches in several scenarios, including a publicly available API and images of traffic signs. Results show that our attack achieves higher fooling rate than other deployable state-of-the-art methods, while requiring significantly fewer queries and modifying very few pixels.}
}
@article{FAN2022108932,
title = {Riemannian dynamic generalized space quantization learning},
journal = {Pattern Recognition},
volume = {132},
pages = {108932},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108932},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004137},
author = {MengLing Fan and Fengzhen Tang and Yinan Guo and Xingang Zhao},
keywords = {Learning vector quantization, Dynamic learning vector quantization, Riemannian manifold, Short-term memory},
abstract = {Many existing works represent signals by covariance matrices and then develop learning methods on the Riemannian symmetric positive-definite (SPD) manifold to deal with such data. However, they summarize each instance with a single covariance matrix, omitting some potential important information, such as the time evolution of the correlation in signals. In this paper, we represent each instance by a sequence of covariance matrices and develop a novel dynamic generalized learning Riemannian space quantization (DGLRSQ) method to deal with such data representations. The proposed DGLRSQ method incorporates short-term memory mechanism in generalized learning Riemannian space quantization (GLRSQ), which is an extension of Euclidean generalized learning vector quantization to deal with SPD matrix-valued data. The proposed method can capture the temporal evolution of the correlation in signals and thus provides better performance to its the counterpart – GLRSQ, which treats each instance as a signal covariance matrix. Empirical investigations on synthetic data and motor imagery EEG data show the superior performance of the proposed method.}
}
@article{ZHOU2023109030,
title = {CSR: Cascade Conditional Variational Auto Encoder with Socially-aware Regression for Pedestrian Trajectory Prediction},
journal = {Pattern Recognition},
volume = {133},
pages = {109030},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109030},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005106},
author = {Hao Zhou and Dongchun Ren and Xu Yang and Mingyu Fan and Hai Huang},
keywords = {Pedestrian trajectory prediction, Socially-aware model, Conditional variational autoencoder (CVAE)},
abstract = {Pedestrian trajectory prediction is a key technology in many real applications such as video surveillance, social robot navigation, and autonomous driving, and significant progress has been made in this research topic. However, there remain two limitations of previous studies. First, the losses of the last time steps are heavier weighted than that of the beginning time steps in the objective function at the learning stage, causing the prediction errors generated at the beginning to accumulate to large errors at the last time steps at the inference stage. Second, the prediction results of multiple pedestrians in the prediction horizon might be socially incompatible with the interactions modeled by past trajectories. To overcome these limitations, this work proposes a novel trajectory prediction method called CSR, which consists of a cascaded conditional variational autoencoder (CVAE) module and a socially-aware regression module. The CVAE module estimates the future trajectories in a cascaded sequential manner. Specifically, each CVAE concatenates the past trajectories and the predicted location points so far as the input and predicts the adjacent location at the following time step. The socially-aware regression module generates offsets from the estimated future trajectories to produce the corrected predictions, which are more reasonable and accurate than the estimated trajectories. Experiments results demonstrate that the proposed method exhibits significant improvements over state-of-the-art methods on the Stanford Drone Dataset (SDD) and the ETH/UCY dataset of approximately 38.0% and 22.2%, respectively. The code is available at https://github.com/zhouhao94/CSR.}
}
@article{PATRO2022108898,
title = {Explanation vs. attention: A two-player game to obtain attention for VQA and visual dialog},
journal = {Pattern Recognition},
volume = {132},
pages = {108898},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108898},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200379X},
author = {Badri N. Patro and  Anupriy and Vinay P. Namboodiri},
keywords = {CNN, LSTM, Explanation, Attention, Grad-CAM, MMD, CORAL, GAN, VQA, Visual Dialog, Deep learning},
abstract = {In this paper, we aim to obtain improved attention for a visual question answering (VQA) task. It is challenging to provide supervision for attention. An observation we make is that visual explanations as obtained through class activation mappings (specifically Grad-CAM) that are meant to explain the performance of various networks could form a means of supervision. However, as the distributions of attention maps and that of Grad-CAMs differ, it would not be suitable to directly use these as a form of supervision. Rather, we propose the use of a discriminator that aims to distinguish samples of visual explanation and attention maps. The use of adversarial training of the attention regions as a two-player game between attention and explanation serves to bring the distributions of attention maps and visual explanations closer. Significantly, we observe that providing such a means of supervision also results in attention maps that are more closely related to human attention resulting in a substantial improvement over baseline stacked attention network (SAN) models. It also results in a good improvement in rank correlation metric on the VQA task. This method can also be combined with recent MCB based methods and results in consistent improvement. We also provide comparisons with other means for learning distributions such as based on Correlation Alignment (Coral), Maximum Mean Discrepancy (MMD) and Mean Square Error (MSE) losses and observe that the adversarial loss outperforms the other forms of learning the attention maps. A generalization of the work is also provided by extending our approach to the task of ‘Visual Dialog’ where the attention is more contextual. Thorough evaluation for this task is also provided. Visualization of the results confirms our hypothesis that attention maps improve using the proposed form of supervision.}
}
@article{XU2023108973,
title = {GripNet: Graph information propagation on supergraph for heterogeneous graphs},
journal = {Pattern Recognition},
volume = {133},
pages = {108973},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108973},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004538},
author = {Hao Xu and Shengqi Sang and Peizhen Bai and Ruike Li and Laurence Yang and Haiping Lu},
keywords = {Graph representation learning, Heterogeneous graph, Data integration, Multi-relational link prediction, Node classification},
abstract = {Heterogeneous graph representation learning aims to learn low-dimensional vector representations of different types of entities and relations to empower downstream tasks. Existing popular methods either capture semantic relationships but indirectly leverage node/edge attributes in a complex way, or leverage node/edge attributes directly without taking semantic relationships into account. When involving multiple convolution operations, they also have poor scalability. To overcome these limitations, this paper proposes a flexible and efficient Graph information propagation Network (GripNet) framework. Specifically, we introduce a new supergraph data structure consisting of supervertices and superedges. A supervertex is a semantically-coherent subgraph. A superedge defines an information propagation path between two supervertices. GripNet learns new representations for the supervertex of interest by propagating information along the defined path using multiple layers. We construct multiple large-scale graphs and evaluate GripNet against competing methods to show its superiority in link prediction, node classification, and data integration. The code and data are available at https://github.com/nyxflower/GripNet.}
}
@article{LIU2022108960,
title = {An End-to-end Supervised Domain Adaptation Framework for Cross-Domain Change Detection},
journal = {Pattern Recognition},
volume = {132},
pages = {108960},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108960},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200440X},
author = {Jia Liu and Wenjie Xuan and Yuhang Gan and Yibing Zhan and Juhua Liu and Bo Du},
keywords = {Change Detection, Supervised Domain Adaptation, Image Adaptation, Feature Adaptation},
abstract = {Change detection is a crucial but extremely challenging task in remote sensing image analysis, and much progress has been made with the rapid development of deep learning. However, most existing deep learning-based change detection methods try to elaborately design complicated neural networks with powerful feature representations. However, they ignore the universal domain shift induced by time-varying land cover changes, including luminance fluctuations and seasonal changes between pre-event and post-event images, thereby producing suboptimal results. In this paper, we propose an end-to-end supervised domain adaptation framework for cross-domain change detection named SDACD, to effectively alleviate the domain shift between bi-temporal images for better change predictions. Specifically, our SDACD presents collaborative adaptations from both image and feature perspectives with supervised learning. Image adaptation exploits generative adversarial learning with cycle-consistency constraints to perform cross-domain style transformation, which effectively narrows the domain gap in a two-side generation fashion. As for feature adaptation, we extract domain-invariant features to align different feature distributions in the feature space, which could further reduce the domain gap of cross-domain images. To further improve the performance, we combine three types of bi-temporal images for the final change prediction, including the initial input bi-temporal images and two generated bi-temporal images from the pre-event and post-event domains. Extensive experiments and analyses conducted on two benchmarks demonstrate the effectiveness and generalizability of our proposed framework. Notably, our framework pushes several representative baseline models up to new State-Of-The-Art records, achieving 97.34% and 92.36% on the CDD and WHU building datasets, respectively. The source code and models are publicly available at https://github.com/Perfect-You/SDACD.}
}
@article{CORDEIRO2023109013,
title = {LongReMix: Robust learning with high confidence samples in a noisy label environment},
journal = {Pattern Recognition},
volume = {133},
pages = {109013},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109013},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004939},
author = {Filipe R. Cordeiro and Ragav Sachdeva and Vasileios Belagiannis and Ian Reid and Gustavo Carneiro},
keywords = {Noisy label learning, Deep learning, Empirical vicinal risk, Semi-supervised learning},
abstract = {State-of-the-art noisy-label learning algorithms rely on an unsupervised learning to classify training samples as clean or noisy, followed by a semi-supervised learning (SSL) that minimises the empirical vicinal risk using a labelled set formed by samples classified as clean, and an unlabelled set with samples classified as noisy. The classification accuracy of such noisy-label learning methods depends on the precision of the unsupervised classification of clean and noisy samples, and the robustness of SSL to small clean sets. We address these points with a new noisy-label training algorithm, called LongReMix, which improves the precision of the unsupervised classification of clean and noisy samples and the robustness of SSL to small clean sets with a two-stage learning process. The stage one of LongReMix finds a small but precise high-confidence clean set, and stage two augments this high-confidence clean set with new clean samples and oversamples the clean data to increase the robustness of SSL to small clean sets. We test LongReMix on CIFAR-10 and CIFAR-100 with introduced synthetic noisy labels, and the real-world noisy-label benchmarks CNWL (Red Mini-ImageNet), WebVision, Clothing1M, and Food101-N. The results show that our LongReMix produces significantly better classification accuracy than competing approaches, particularly in high noise rate problems. Furthermore, our approach achieves state-of-the-art performance in most datasets. The code is available at https://github.com/filipe-research/LongReMix.}
}
@article{MOHAIMENUZZAMAN2023109025,
title = {Environmental Sound Classiﬁcation on the Edge: A Pipeline for Deep Acoustic Networks on Extremely Resource-Constrained Devices},
journal = {Pattern Recognition},
volume = {133},
pages = {109025},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109025},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005052},
author = {Md Mohaimenuzzaman and Christoph Bergmeir and Ian West and Bernd Meyer},
keywords = {Deep learning, Audio classification, Environmental sound classification, Acoustics, Intelligent sound recognition, Micro-Controller, IoT, Edge-AI},
abstract = {Significant efforts are being invested to bring state-of-the-art classification and recognition to edge devices with extreme resource constraints (memory, speed, and lack of GPU support). Here, we demonstrate the first deep network for acoustic recognition that is small, flexible and compression-friendly yet achieves state-of-the-art performance for raw audio classification. Rather than handcrafting a once-off solution, we present a generic pipeline that automatically converts a large deep convolutional network via compression and quantization into a network for resource-impoverished edge devices. After introducing ACDNet, which produces above state-of-the-art accuracy on ESC-10 (96.65%), ESC-50 (87.10%), UrbanSound8K (84.45%) and AudioEvent (92.57%), we describe the compression pipeline and show that it allows us to achieve 97.22% size reduction and 97.28% FLOP reduction while maintaining close to state-of-the-art accuracy 96.25%, 83.65%, 78.27% and 89.69% on these datasets. We describe a successful implementation on a standard off-the-shelf microcontroller and, beyond laboratory benchmarks, report successful tests on real-world datasets.}
}
@article{GOMEZ2022108927,
title = {BR-NPA: A non-parametric high-resolution attention model to improve the interpretability of attention},
journal = {Pattern Recognition},
volume = {132},
pages = {108927},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108927},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004083},
author = {Tristan Gomez and Suiyi Ling and Thomas Fréour and Harold Mouchère},
keywords = {Deep learning, Interpretability, Spatial attention, Resolution, Non-parametric},
abstract = {The prevalence of employing attention mechanisms has brought along concerns about the interpretability of attention distributions. Although it provides insights into how a model is operating, utilizing attention as the explanation of model predictions is still highly dubious. The community is still seeking more interpretable strategies for better identifying local active regions that contribute the most to the final decision. To improve the interpretability of existing attention models, we propose a novel Bilinear Representative Non-Parametric Attention (BR-NPA) strategy that captures the task-relevant human-interpretable information. The target model is first distilled to have higher-resolution intermediate feature maps. From which, representative features are then grouped based on local pairwise feature similarity, to produce finer-grained, more precise attention maps highlighting task-relevant parts of the input. The obtained attention maps are ranked according to the activity level of the compound feature, which provides information regarding the important level of the highlighted regions. The proposed model can be easily adapted in a wide variety of modern deep models, where classification is involved. Extensive quantitative and qualitative experiments showcase more comprehensive and accurate visual explanations compared to state-of-the-art attention models and visualization methods across multiple tasks including fine-grained image classification, few-shot classification, and person re-identification, without compromising the classification accuracy. The proposed visualization model sheds imperative light on how neural networks ‘pay their attention’ differently in different tasks.}
}
@article{YANG2023108968,
title = {Retinal image enhancement with artifact reduction and structure retention},
journal = {Pattern Recognition},
volume = {133},
pages = {108968},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108968},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004484},
author = {Bingyu Yang and He Zhao and Lvchen Cao and Hanruo Liu and Ningli Wang and Huiqi Li},
keywords = {Retinal image enhancement, Generative adversarial networks, High frequency},
abstract = {Enhancement of low-quality retinal fundus images is beneficial to clinical diagnosis of ophthalmic diseases and computer-aided analysis. Enhancement accuracy is a challenge for image generation models, especially when there is no supervision by paired images. To reduce artifacts and retain structural consistency for accuracy improvement, we develop an unpaired image generation method for fundus image enhancement with the proposed high-frequency extractor and feature descriptor. Specifically, we summarize three causes of tiny vessel-like artifacts which always appear in other image generation methods. A high frequency prior is incorporated into our model to reduce artifacts by the proposed high-frequency extractor. In addition, the feature descriptor is trained alternately with the generator using segmentation datasets and generated image pairs to ensure the fidelity of the image structure. Pseudo-label loss is proposed to improve the performance of the feature descriptor. Experimental results show that the proposed method performs better than other methods both qualitatively and quantitatively. The enhancement can improve the performance of segmentation and classification in retinal images.}
}
@article{LIU2022108944,
title = {In the eye of the beholder: A survey of gaze tracking techniques},
journal = {Pattern Recognition},
volume = {132},
pages = {108944},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108944},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004241},
author = {Jiahui Liu and Jiannan Chi and Huijie Yang and Xucheng Yin},
keywords = {Gaze estimation, eye features, appearance-based, personal calibration, head motion},
abstract = {Gaze tracking estimates and tracks the user’s gaze by analyzing facial or eye features, it is an important way to realize automated vision-based interaction. This paper introduces the visual information used in gaze tracking, and discusses the commonly used gaze estimation methods and their research dynamics, including: 2D mapping-based methods, 3D model-based methods, and appearance-based methods. In this way, some key issues that need to be solved in these methods are considered, and their research trends are discussed. Their characteristics in system configuration, personal calibration, head motion, gaze accuracy and robustness are also compared. Finally, the applications of gaze tracking techniques are analyzed from various application factors and fields. This paper reviews the latest development of gaze tracking, focuses more on various gaze tracking algorithms and their existing challenges. The development trends of gaze tracking are prospected, which provides ideas for future theoretical research and practical applications.}
}
@article{ZHAO2022108984,
title = {Progressive Deep Non-Negative Matrix Factorization Architecture with Graph Convolution-based Basis Image Reorganization},
journal = {Pattern Recognition},
volume = {132},
pages = {108984},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108984},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004642},
author = {Yang Zhao and Furong Deng and Jihong Pei and Xuan Yang},
keywords = {Deep non-negative matrix factorization, Graph convolution, Basis image reconstruction, Basis image factorization, Face recognition},
abstract = {Deep non-negative matrix factorization is committed to using multi-layer structure to extract underlying parts-based representation. However, the basis images obtained by continuous depth factorization is too sparse, resulting in too fragmented parts reflected by the basis image. This makes the number of factorization layers limited and the underlying local feature representation is inaccurate. Therefore, we propose a novel progressive deep non-negative matrix factorization (PDNMF) architecture that adds a basis image reconstruction step to the successive basis image factorization steps. This helps the basis image in depth factorization to maintain better robustness of feature representation. In the reconstruction step, the attribute similarity graph (ASG) is constructed to describe the semantic expression ability of each basis image. With the help of the ASG, the basis image enhances its own semantic integrity through graph convolution without drastically destroying its representation. The evaluation in image recognition shows that the recognition accuracy of the proposed PDNMF improves with the increase of layers. Our method outperforms the state-of-the-art deep factorization methods in image recognition.}
}
@article{DIETTERICH2022108931,
title = {The familiarity hypothesis: Explaining the behavior of deep open set methods},
journal = {Pattern Recognition},
volume = {132},
pages = {108931},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108931},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004125},
author = {Thomas G. Dietterich and Alex Guyer},
keywords = {Anomaly detection, Open set learning, Computer vision, Object recognition, Novel category detection, Representation learning, Deep learning},
abstract = {In many object recognition applications, the set of possible categories is an open set, and the deployed recognition system will encounter novel objects belonging to categories unseen during training. Detecting such “novel category” objects is usually formulated as an anomaly detection problem. Anomaly detection algorithms for feature-vector data identify anomalies as outliers, but outlier detection has not worked well in deep learning. Instead, methods based on the computed logits of visual object classifiers give state-of-the-art performance. This paper proposes the Familiarity Hypothesis that these methods succeed because they are detecting the absence of familiar learned features rather than the presence of novelty. This distinction is important, because familiarity-based detection will fail in many situations where novelty is present. For example when an image contains both a novel object and a familiar one, the familiarity score will be high, so the novel object will not be noticed. The paper reviews evidence from the literature and presents additional evidence from our own experiments that provide strong support for this hypothesis. The paper concludes with a discussion of whether familiarity-based detection is an inevitable consequence of representation learning.}
}
@article{BAI2023109037,
title = {Query efficient black-box adversarial attack on deep neural networks},
journal = {Pattern Recognition},
volume = {133},
pages = {109037},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109037},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005179},
author = {Yang Bai and Yisen Wang and Yuyuan Zeng and Yong Jiang and Shu-Tao Xia},
keywords = {Black-box adversarial attack, Adversarial distribution, Query efficiency, Neural process},
abstract = {Deep neural networks (DNNs) have demonstrated excellent performance on various tasks, yet they are under the risk of adversarial examples that can be easily generated when the target model is accessible to an attacker (white-box setting). As plenty of machine learning models have been deployed via online services that only provide query outputs from inaccessible models (e.g., Google Cloud Vision API2), black-box adversarial attacks raise critical security concerns in practice rather than white-box ones. However, existing query-based black-box adversarial attacks often require excessive model queries to maintain a high attack success rate. Therefore, in order to improve query efficiency, we explore the distribution of adversarial examples around benign inputs with the help of image structure information characterized by a Neural Process, and propose a Neural Process based black-box adversarial attack (NP-Attack) in this paper. Our proposed NP-Attack could be further boosted when applied with surrogate models or tiling tricks. Extensive experiments show that NP-Attack could greatly decrease the query counts under the black-box setting.}
}
@article{LIU2023109008,
title = {Noise-robust oversampling for imbalanced data classification},
journal = {Pattern Recognition},
volume = {133},
pages = {109008},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109008},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004885},
author = {Yongxu Liu and Yan Liu and Bruce X.B. Yu and Shenghua Zhong and Zhejing Hu},
keywords = {Imbalanced learning, Classification, Clustering},
abstract = {The class imbalance problem is characterized by an unequal data distribution in which majority classes have a greater number of data samples than minority classes. Oversampling methods generate samples for minority classes to balance the data distribution. However, the generated minority samples may overlap with majority samples, resulting in noise. In this paper, we propose a noise-robust oversampling algorithm for mixed-type and multi-class imbalanced data. Our proposed noise-robust designs include an algorithm to eliminate noise within clusters of data samples, adaptive embedding to generate samples safely, and a safe boundary for enlarging class boundaries. The heterogeneous distance metric and adapted decomposition strategy render our noise-robust algorithm suitable for mixed-type and multi-class imbalanced data. Experimental results on 20 benchmark datasets demonstrate the effectiveness of the proposed algorithm.}
}
@article{LV2022108956,
title = {Memory‐augmented neural networks based dynamic complex image segmentation in digital twins for self‐driving vehicle},
journal = {Pattern Recognition},
volume = {132},
pages = {108956},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108956},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004368},
author = {Zhihan Lv and Liang Qiao and Shuo Yang and Jinhua Li and Haibin Lv and Francesco Piccialli},
keywords = {Deep learning, Image segmentation, Memory-augmented neural networks, LSTM, Self-driving, Digital twins},
abstract = {With the continuous increase of the amount of information, people urgently need to identify the information in the image in more detail in order to obtain richer information from the image. This work explores the dynamic complex image segmentation of self-driving vehicle under Digital Twins (DTs) based on Memory-augmented Neural Networks (MANNs), so as to further improve the performance of self-driving in intelligent transportation. In view of the complexity of the environment and the dynamic changes of the scene in intelligent transportation, this work constructs a segmentation model for dynamic complex image of self-driving vehicle under DTs based on MANNs by optimizing the Deep Learning algorithm and further combining with the DTs technology, so as to recognize the information in the environment image during the self-driving. Finally, the performance of the constructed model is analyzed by experimenting with different image datasets (PASCALVOC 2012, NYUDv2, PASCAL CONTEXT, and real self-driving complex traffic image data). The results show that compared with other classical algorithms, the established MANN-based model has an accuracy of about 85.80%, the training time is shortened to 107.00 s, the test time is 0.70 s, and the speedup ratio is high. In addition, the average algorithm parameter of the given energy function α=0.06 reaches the maximum value. Therefore, it is found that the proposed model shows high accuracy and short training time, which can provide experimental reference for future image visual computing and intelligent information processing.}
}
@article{YI2023109019,
title = {UAVformer: A Composite Transformer Network for Urban Scene Segmentation of UAV Images},
journal = {Pattern Recognition},
volume = {133},
pages = {109019},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109019},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200499X},
author = {Shi Yi and Xi Liu and Junjie Li and Ling Chen},
keywords = {Urban scenes segmentation, UAV image, Composite backbone, Aggregation windows multi-head self-attention transformer block, V-shaped decoder},
abstract = {Urban scenes segmentation based on UAV (Unmanned aerial vehicle) view is a fundamental task for the applications of smart city such as city planning, land use monitoring, traffic monitoring, and crowd estimation. While urban scenes in UAV image characteristic by large scale variation of objects size and complexity background, which posed challenges to urban scenes segmentation of UAV image. The feature extracting backbone of existing networks cannot extract complex features of UAV image effectively, which limits the performance of urban scenes segmentation. To design segmentation network capable of extracting features of large scale variation urban ground scenes, this study proposed a novel composite transformer network for urban scenes segmentation of UAV image. A composite backbone with aggregation windows multi-head self-attention transformer blocks is proposed to make the extracted features more representatives by adaptive multi-level features fusion, and the full utilisation of contextual information and local information. Position attention modules are inserted in each stage between encoder and decoder to further enhance the spatial attention of extracted feature maps. Finally, a V-shaped decoder which is capable of utilising multi-level features is designed to get accurately dense prediction. The accuracy of urban scenes segmentation could significantly be enhanced in this way and successfully segmented the large scale variation objects from UAV views. Extensive ablation experiments and comparative experiments for the proposed network have been conducted on the public available urban scenes segmentation datasets for UAV imagery. Experimental results have demonstrated the effectiveness of designed network structure and the superiority of proposed network over state-of-the-art methods. Specifically, reached 53.2% mIoU on the UAVid dataset and 77.6% mIoU on the UDD6 dataset, respectively.}
}
@article{ZHAO2022108943,
title = {Towards a category-extended object detector with limited data},
journal = {Pattern Recognition},
volume = {132},
pages = {108943},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108943},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200423X},
author = {Bowen Zhao and Chen Chen and Xi Xiao and Shutao Xia},
keywords = {Object detector, Category-extended, Limited data, Multi-dataset},
abstract = {Object detectors are typically learned on fully-annotated training data with fixed predefined categories. However, categories are often required to be increased progressively. Usually, only the original training set annotated with old classes and some new training data labeled with new classes are available in such scenarios. Based on the limited datasets, a unified detector that can handle all categories is strongly needed. We propose a practical scheme to achieve it in this work. A conflict-free loss is designed to avoid label ambiguity, leading to an acceptable detector in one training round. To further improve performance, we propose a retraining phase in which Monte Carlo Dropout is employed to calculate the localization confidence to mine more accurate bounding boxes, and an overlap-weighted method is proposed for making better use of pseudo annotations during retraining. Extensive experiments demonstrate the effectiveness of our method.}
}
@article{WEI2023108996,
title = {An accurate stereo matching method based on color segments and edges},
journal = {Pattern Recognition},
volume = {133},
pages = {108996},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108996},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004769},
author = {Hui Wei and Lingjiang Meng},
keywords = {Binocular vision, Stereo matching, Industrial robot},
abstract = {Stereo matching algorithms of binocular vision suffer from low accuracy when dealing with natural scenes (such as industrial robot scenes). Biological vision is sensitive to object edges; it divides objects by their edges, and then perceives their distances. Similar to the biological eye mechanism, this study proposes a matching algorithm that combines segment- and edge-matching to obtain the disparity. In segment matching, pixel strings from the same row of the left and right images are divided into pixel segments, whose colors and lengths are used as clues to determine several types of matching pixel segment pairs according to non-crossing mapping. The analysis of the spatial state yields several types of stimulus bars. Disparities can be obtained from the relation between pixel segment pairs and stimulus bars. In edge matching, the DTW (Dynamic Time Warping) algorithm and the gradient are used to determine the initial edge pixel matching results. The remaining edge point disparity is obtained by fitting a fill to the existing edge point disparity. Finally, segment and edge matching results are combined to check and fill and post-processing. This new matching method transforms pixel matching to pixel segment matching and edge matching, which can reduces the time complexity. The algorithm can be implemented in an industrial robot environment for high-precision needle threading guidance, which neither traditional binocular matching nor deep learning matching algorithms can do.}
}
@article{EELAHI2022108972,
title = {Online temporal classification of human action using action inference graph},
journal = {Pattern Recognition},
volume = {132},
pages = {108972},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108972},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004526},
author = {G M Mashrur {E Elahi} and Yee-Hong Yang},
keywords = {Online temporal classification, Action recognition, Action detection, Action inference graph},
abstract = {Nowadays, deep learning methods have achieved state-of-the-art results in human action recognition. These methods process a full video sequence to recognize an action, which is unnecessary because many frames are similar. Recently, keyframe-based methods are proposed to overcome this issue. Though keyframe based methods have shown competitive performance in action recognition, both methods still process all the required frames of a video clip and average the results of individual clips/frames to recognize the action of the video. We argue that by simply using the average of the results of the video clips, deep models are not using the motion information of the video and thus leads to an inaccurate recognition of the action. To cope with the aforementioned issue, we propose a new online temporal classification model (OTCM) that classifies an action from a video in an online fashion and addresses the issue of averaging by making decision of each frame of a video sequence. As well, we propose a new action inference graph (AIG) that enables early recognition. Hence, the proposed model can recognize an action early before using all the keyframes or the whole video sequence and thus, requires less computation for recognizing human actions. Moreover, our OTCM can perform online action detection. To the best of our knowledge, this is the first time that the OTCM model along with the AIG is proposed. The experimental results of the benchmark datasets show that the proposed OTCM model has achieved and set a new record of the SOTA results, in particular, without using full video sequences.}
}
@article{ZHANG2023109012,
title = {Towards prior gap and representation gap for long-tailed recognition},
journal = {Pattern Recognition},
volume = {133},
pages = {109012},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109012},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004927},
author = {Ming-Liang Zhang and Xu-Yao Zhang and Chuang Wang and Cheng-Lin Liu},
keywords = {Long-tailed learning, Prior gap, Representation gap, Image recognition},
abstract = {Most deep learning models are elaborately designed for balanced datasets, and thus they inevitably suffer performance degradation in practical long-tailed recognition tasks, especially to the minority classes. There are two crucial issues in learning from imbalanced datasets: skew decision boundary and unrepresentative feature space. In this work, we establish a theoretical framework to analyze the sources of these two issues from Bayesian perspective, and find that they are closely related to the prior gap and the representation gap, respectively. Under this framework, we show that existing long-tailed recognition methods manage to remove either the prior gap or the presentation gap. Different from these methods, we propose to simultaneously remove the two gaps to achieve more accurate long-tailed recognition. Specifically, we propose the prior calibration strategy to remove the prior gap and introduce three strategies (representative feature extraction, optimization strategy adjustment and effective sample modeling) to mitigate the representation gap. Extensive experiments on five benchmark datasets validate the superiority of our method against the state-of-the-art competitors.}
}
@article{LUO2022108901,
title = {Scale-selective and noise-robust extended local binary pattern for texture classification},
journal = {Pattern Recognition},
volume = {132},
pages = {108901},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108901},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200382X},
author = {Qiwu Luo and Jiaojiao Su and Chunhua Yang and Olli Silven and Li Liu},
keywords = {Local binary pattern (LBP), Texture descriptor, Feature extraction, Texture classification},
abstract = {As one of the most successful local feature descriptors, the local binary pattern (LBP) estimates the texture distribution rule of an image based on the signs of differences between neighboring pixels to obtain intensity- and rotation- invariance. In this paper, we propose a novel image descriptor to address scale transformation and noise interference simultaneously. We name it scale-selective and noise-robust extended LBP (SNELBP). First, each image in training sets is transformed into different scale spaces by a Gaussian filter. Second, noise-robust pattern histograms are obtained from each scale space by using our previously proposed median robust extended LBP (MRELBP). Then, scale-invariant histograms are determined by selecting the maximum among all scale levels for a certain image. Finally, the most informative patterns are selected from the dictionary pretrained by the two-stage compact dominant feature selection method (CDFS), maintaining the descriptor more lightweight with sufficiently low time cost. Extensive experiments on five public databases (Outex_TC_00011, TC_00012, KTH-TIPS, UMD and NEU) and one fresh texture database (JoJo) under two kinds of interferences (Gaussian and salt pepper) indicate that our SNELBP yields more competitive results than thirty classical LPB variants as well as eight typical deep learning methods.}
}