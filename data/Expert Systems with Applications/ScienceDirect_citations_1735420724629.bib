@article{QIU2024110807,
title = {PPM: A boolean optimizer for data association in multi-view pedestrian detection},
journal = {Pattern Recognition},
volume = {156},
pages = {110807},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110807},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005582},
author = {Rui Qiu and Ming Xu and Yuyao Yan and Jeremy S. Smith and Yuchen Ling},
keywords = {Multi-camera, Data fusion, Video surveillance, Logic minimization, Pedestrian detection},
abstract = {To accurately localize occluded people in a crowd is a challenging problem in video surveillance. Existing end-to-end deep multi-camera detectors rely heavily on pre-training with the same multiview datasets used for testing, which compromises their real-world applications. An alternative approach presented here is to project the torso lines of the instance segmentation masks from multiple views to the ground plane and propose pedestrian candidates at the intersection points. The candidate selection process is, for the first time, formulated as a logic minimization problem in Boolean algebra. A probabilistic Petrick’s method (PPM) is proposed to seek the minimum number of candidates to account for all the foreground masks while maximizing the joint occupancy likelihoods in multiple views. Experiments on benchmark video datasets have demonstrated the much improved performance of this approach in comparison with the benchmark deep or non-deep algorithms for multiview pedestrian detection.}
}
@article{LU2025110864,
title = {Measuring generalized divergence for multiple distributions with application to deep clustering},
journal = {Pattern Recognition},
volume = {157},
pages = {110864},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110864},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006150},
author = {Mingfei Lu and Lei Xing and Badong Chen},
keywords = {Generalized divergence measures, Sample-based estimation, Jensen–Rényi divergence, Deep clustering},
abstract = {In machine learning scenarios involving multiple sources (distributions) of data, such as multi-view learning, domain adaptation or generalization, and clustering, it is crucial to efficiently handle such multi-source data simultaneously while effectively identifying their dissimilarity. Traditional approaches for measuring the overall divergence among multiple distributions involve calculating the divergence between each pair of distributions using a two-distribution-compare measure and then quantifying their average. However, this approach risks disregarding the collective synergy or overlap that may exist among more than two distributions and also incurs substantial computational costs. To address these limitations, we propose the use of the generalized Jensen–Rényi divergence (GJRD), to facilitate the simultaneous handling of multiple distributions. We derive a closed-form non-parametric empirical estimator for the GJRD based on kernel density estimation, making it convenient for data-driven machine learning applications. Further, we develop the GJRD-based deep clustering framework (GJRD-DC) and the corresponding algorithms, leveraging the derived estimator. Experimental results on various benchmarks demonstrate that the proposed GJRD-DC method achieves state-of-the-art performance on challenging datasets and comparable results on others. Code is available at https://github.com/LMFLRB/GJRD.git}
}
@article{LIAO2024110767,
title = {Finding score-based representative samples for cancer risk prediction},
journal = {Pattern Recognition},
volume = {156},
pages = {110767},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110767},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005181},
author = {Jun Liao and Hao Luo and Xuewen Yan and Ting Ye and Shanshan Huang and Li Liu},
keywords = {Representative sampling, Score-based estimation, Explainable model, Cancer risk prediction},
abstract = {Finding representative samples is important for predicting cancer risk. In particular, it is crucial to identify each representative sample as responsible for the prediction performance. In this article, we present a general framework for finding representative samples by explicitly estimating their inherit contribution levels (or scores). By leveraging explainable models as our score functions such as Shapley value, LIME and influence function, our framework can quantitatively identify the representative level of each sample in cancer risk prediction. Furthermore, a score ensembler is introduced to integrate these scores obtained from various score functions with an additional vector of weight variables optimized by the Fast Iterative Shrinkage-Thresholding Algorithm. Empirical evaluations on four cancer risk datasets with different challenges by using five classifiers suggest that our approach significantly outperforms the state-of-the-art methods.}
}
@article{PANAGIOTOU2024110799,
title = {Denoising diffusion post-processing for low-light image enhancement},
journal = {Pattern Recognition},
volume = {156},
pages = {110799},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110799},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005508},
author = {Savvas Panagiotou and Anna S. Bosman},
keywords = {Diffusion model, Denoising, Low-light image enhancement, Post-processing},
abstract = {Low-light image enhancement (LLIE) techniques attempt to increase the visibility of images captured in low-light scenarios. However, as a result of enhancement, a variety of image degradations such as noise and color bias are revealed. Furthermore, each particular LLIE approach may introduce a different form of flaw within its enhanced results. To combat these image degradations, post-processing denoisers have widely been used, which often yield oversmoothed results lacking detail. We propose using a diffusion model as a post-processing approach, and we introduce Low-light Post-processing Diffusion Model (LPDM) in order to model the conditional distribution between under-exposed and normally-exposed images. We apply LPDM in a manner which avoids the computationally expensive generative reverse process of typical diffusion models, and post-process images in one pass through LPDM. Extensive experiments demonstrate that our approach outperforms competing post-processing denoisers by increasing the perceptual quality of enhanced low-light images on a variety of challenging low-light datasets. Source code is available at https://github.com/savvaki/LPDM.}
}
@article{YE2024110811,
title = {Deep generative domain adaptation with temporal relation attention mechanism for cross-user activity recognition},
journal = {Pattern Recognition},
volume = {156},
pages = {110811},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110811},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005624},
author = {Xiaozhou Ye and Kevin I-Kai Wang},
keywords = {Human activity recognition, Deep domain adaptation, Data out-of-distribution, Temporal relation knowledge, Time series classification},
abstract = {In sensor-based Human Activity Recognition (HAR), a predominant assumption is that the data utilized for training and evaluation purposes are drawn from the same distribution. It is also assumed that all data samples are independent and identically distributed (i.i.d.). Contrarily, practical implementations often challenge this notion, manifesting data distribution discrepancies, especially in scenarios such as cross-user HAR. Domain adaptation is the promising approach to address these challenges inherent in cross-user HAR tasks. However, a clear gap in domain adaptation techniques is the neglect of the temporal dependency relation embedded within time series data during the phase of aligning data distributions. Addressing this oversight, our research presents the Deep Generative Domain Adaptation with Temporal Attention (DGDATA) method. This novel method uniquely recognizes and integrates temporal dependency relations during the domain adaptation process. By synergizing the capabilities of generative models with the Temporal Relation Attention mechanism, our method improves the classification performance in cross-user HAR. The evaluation has been conducted on three public sensor-based HAR datasets targeting daily living activity and sports fitness activity scenarios to demonstrate the efficacy of the proposed DGDATA method.}
}
@article{BAI2024110759,
title = {Incorporating texture and silhouette for video-based person re-identification},
journal = {Pattern Recognition},
volume = {156},
pages = {110759},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110759},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005107},
author = {Shutao Bai and Hong Chang and Bingpeng Ma},
keywords = {Video-based person re-identification, Silhouette, Relational modeling, Decomposition},
abstract = {Silhouette is an effective modality in video-based person re-identification (ReID) since it contains features (e.g., stature and gait) complementary to the RGB modality. However, recent silhouette-assisted methods have not fully explored the spatial–temporal relations within each modality or considered the cross-modal complementarity in fusion. To address these two issues, we propose a Complete Relational Framework that includes two key components. The first component, Spatial-Temporal Relational Module (STRM), explores the spatiotemporal relations. STRM decomposes the video’s spatiotemporal context into local/fine-grained and global/semantic aspects, modeling them sequentially to enhance the representation of each modality. The second component, Modality-Channel Relational Module (MCRM), explores the complementarity between RGB and silhouette videos. MCRM aligns two modalities semantically and multiplies them to capture complementary interrelations. With these two modules focusing on intra- and cross-modal relationships, our method achieves superior results across multiple benchmarks with minimal additional parameters and FLOPs. Code and models are available at https://github.com/baist/crf.}
}
@article{LIU2024110847,
title = {Sentiment analysis based on text information enhancement and multimodal feature fusion},
journal = {Pattern Recognition},
volume = {156},
pages = {110847},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110847},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005983},
author = {Zijun Liu and Li Cai and Wenjie Yang and Junhui Liu},
keywords = {Sentiment analysis, Text information enhancement, Multimodal data fusion, Cross-modal attention mechanism, Sentiment lexicons},
abstract = {Rapid advancements in multimedia technology have created explosive growth in sentiment data generated across various social media platforms. While previous research on sentiment analysis has shifted from analyzing single data types to incorporating multimodal data, current studies face certain limitations. These include overlooking the impact of redundant information within feature sequences of each modality, failing to account for the complementarity between modality data, and neglecting the varying significance of different modalities in conveying sentiments. This paper introduces a sentiment analysis framework designed for text information enhancement and multimodal feature fusion. The text modality is central to this framework, around which an attention mechanism augments emotional correlations between modalities. An expanded sentiment lexicon refines the representation of multimodal features, thus capturing emotional information more accurately. Experimental evaluations conducted on two standard datasets, CMU-MOSI and CMU-MOSEI, show that the accuracy of the proposed method in multimodal emotion recognition tasks reaches 85.7% and 85.8% respectively, at 1.6% and 1.8% higher than the baseline methods. Thus, it demonstrates robust regression and classification performance.}
}
@article{SUN2024110795,
title = {High-frequency and low-frequency dual-channel graph attention network},
journal = {Pattern Recognition},
volume = {156},
pages = {110795},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110795},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005466},
author = {Yukuan Sun and Yutai Duan and Haoran Ma and Yuelong Li and Jianming Wang},
keywords = {Graph convolution, Graph attention, High-frequency information, Heterophilic graphs},
abstract = {Most existing graph convolution layers use learnable or fixed weights to sum up neighbor features to aggregate neighbor information. Since the attention values are always positive, these graph convolution layers perform as low-pass filters, which may result in their poor performance on heterophilic graphs. In this paper, two graph convolutional layers are proposed, HLGAT and NGAT. NGAT is a convolution network using only negative attention values, which only make the aggregation of high-frequency information of neighbor nodes. HLGAT makes the aggregation of low-frequency and high-frequency information by two channels, respectively, and fuses two outputs by using a learnable way. On node-classification task, both NGAT and HLGAT offer significant performance improvement compared to existing methods. The results clearly show that: (1) High-frequency information of neighborhoods plays a decisive role in heterophilic graphs. (2) The aggregation of low-frequency and high-frequency information of neighbor nodes can significantly improve the performance on heterophilic graphs.}
}
@article{YANG2024110766,
title = {A Neuroinspired Contrast Mechanism enables Few-Shot Object Detection},
journal = {Pattern Recognition},
volume = {156},
pages = {110766},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110766},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400517X},
author = {Lingxiao Yang and Dapeng Chen and Yifei Chen and Wei Peng and Xiaohua Xie},
keywords = {Few-Shot Object Detection, Contrast Blocks, Faster R-CNN, Perceptual Learning},
abstract = {Deep learning-based object detectors often demand abundant annotated data for training. However, in practice, only limited training data are available, making Few-Shot Object Detection (FSOD) an attractive research topic. Existing two-stage proposal-based Faster R-CNN detectors for FSOD struggle to match the performance of models trained on large datasets. We argue that detectors trained with limited samples cannot establish robust comparison-based relationships. Additionally, FSOD methods only explore these relationships during the training phase. To address these issues, we draw inspiration from neuroscience studies and propose Residual Contrast Faster R-CNN (RcFRCN). RcFRCN incorporates two novel customized contrast blocks: a Residual Spatial Contrast Block and a Residual Proposal Contrast Block. These blocks capture cross-spatial and cross-proposal contrast information, enhancing both training and testing phases. We conduct comprehensive experiments on two FSOD benchmarks: PASCAL VOC and MS-COCO. Our RcFRCN achieves a mAP of 21.9 under a 30-shot setting on MS-COCO. It also achieves AP scores of 69.1, 55.8, and 64.0 under 10-shot settings of different splits on PASCAL VOC, respectively. Moreover, we apply RcFRCN on remote sensing and use our contrast blocks for open-vocabulary detection. Experiment results on these tasks also demonstrate the robustness and generalization ability of our methods.}
}
@article{SUN2024110823,
title = {Learning real-world heterogeneous noise models with a benchmark dataset},
journal = {Pattern Recognition},
volume = {156},
pages = {110823},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110823},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005740},
author = {Lu Sun and Jie Lin and Weisheng Dong and Xin Li and Jinjian Wu and Guangming Shi},
keywords = {Benchmark dataset, Noise modeling, Deep convolutional neural network, Real denoising},
abstract = {Noise modeling is an important research field in computer vision; the design of an accurate model for imaging sensor noise depends on not only a comprehensive benchmark dataset of the real world, but also a precise design of the noise modeling algorithm. However, due to the inaccurate estimation method of noise-free images and limited shooting scenes, the current realistic datasets could not describe the diverse noise properties sufficiently. Moreover, popular parametric noise models are not sophisticated enough to characterize the real-world noise exactly. In this work, we first construct a more comprehensive dataset of the real world by capturing more indoor and outdoor scenes under different lighting conditions using diverse smartphones, then we propose a non-parametric noise estimation method capable of modeling the spatial heterogeneity of real-world noise patterns. Specifically, in order to characterize the spatial heterogeneity of real-world noise, we assume a non-i.i.d Gaussian distribution and propose a deep convolutional neural network (DCNN)-based approach for learning pixel-wise noise variance maps. To learn the pixel-wise variance map, we have constructed a variance estimation network mapping from the conditional signals (clean image, ISO, and camera model) to surrogate labels obtained from the nonlocal search of similar patches from the clean-noisy image pair. Finally, we conducted denoising and classification experiments using different kinds of simulated noisy images, compared to the Poisson-Gaussian and Noise Flow noise models, the proposed method achieves denoising performance improvements (PSNR) of 1.13 dB and 2.51 dB respectively on the proposed real-world test dataset, denoising and classification results on the real noisy data captured by mobile phones have verified that our approach is more accurate than current noise modeling methods.}
}
@article{WANG2024110771,
title = {Shadow-aware decomposed transformer network for shadow detection and removal},
journal = {Pattern Recognition},
volume = {156},
pages = {110771},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110771},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005223},
author = {Xiao Wang and Siyuan Yao and Yong Tang and Sili Yang and Zhenbao Liu},
keywords = {Shadow detection, Shadow removal, Shadow-aware transformer, Image decomposition},
abstract = {Shadow detection and removal are important yet challenging computer vision tasks. Existing methods simultaneously contend with the brightness and color information of input image while treating different regions of input images equally. We argue that these operations fail to effectively extract the relationship between shadow and non-shadow regions. To relieve these problems, this paper proposes a shadow-aware decomposed transformer network for shadow detection and removal. The network decomposes the input image into brightness and color maps using its bright channel, which are concatenated with the original image as the input data, enabling the network to pay balanced attention to the brightness and color information when calculating the relationship between regions. Additionally, given that the correlation matrix of the transformer measures the relative dependency between regions, the proposed shadow-aware transformer block can extract the relationship between shadow and non-shadow regions more effectively by retaining the specific elements of the correlation matrix. We conduct extensive experiments on three shadow detection benchmark datasets and two shadow removal benchmark datasets. Experimental results show that the proposed method performs favorably against state-of-the-art methods. Codes have been made available at https://github.com/XIAOWANG914/SADT.}
}
@article{ZHANG2024110806,
title = {Few-shot learning with long-tailed labels},
journal = {Pattern Recognition},
volume = {156},
pages = {110806},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110806},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005570},
author = {Hongliang Zhang and Shuo Chen and Lei Luo and Jiang Yang},
keywords = {Few-shot learning, Long-tailed labels, Contrastive learning, Semi-supervised learning, Weighted random sampling},
abstract = {Few-Shot Learning (FSL) is a challenging classification task in machine learning, and it aims to recognize unseen examples of new classes with only a few labeled reference examples (i.e., the support set). The training phase of FSL typically requires a large amount of labeled examples (i.e., the base set) to effectively learn transferable knowledge, but it is usually difficult to obtain sufficient data annotation in practical applications. Existing semi-supervised FSL approaches can learn generalizable representations from partly labeled data, yet they do not sufficiently consider the real distribution of those labeled data. In this paper, we propose a new problem setting termed Few-Shot Learning with Long-Tailed Labels (FSL-LTL) to further consider a more practical semi-supervised scenario where the labeled examples are long-tailed. To effectively address this new problem, we build a novel two-stage training framework dubbed Reweighted Contrastive Embedding (RCE). In the first stage of RCE, we adopt the popular contrastive learning framework to pre-train a reliable network in a self-supervised manner. In the second stage, we integrate the semi-supervised empirical risk into a Weighted Random Sampling (WRS) strategy to fine-tune the pre-trained backbone with the aid of a consistency regularization. Experimental results demonstrate the feasibility of the proposed FSL-LTL problem setting and the superiority of our new RCE method over existing FSL approaches and semi-supervised learning methods. These results also suggest that the RCE approach is a promising solution for addressing the new FSL-LTL problem.}
}
@article{LEE2024110754,
title = {Text-guided distillation learning to diversify video embeddings for text-video retrieval},
journal = {Pattern Recognition},
volume = {156},
pages = {110754},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110754},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005053},
author = {Sangmin Lee and Hyung-Il Kim and Yong Man Ro},
keywords = {text-video retrieval, One-to-many correspondence, Diverse video embedding, Text-guided distillation learning, Text-agnostic},
abstract = {Conventional text-video retrieval methods typically match a video with a text on a one-to-one manner. However, a single video can contain diverse semantics, and text descriptions can vary significantly. Therefore, such methods fail to match a video with multiple texts simultaneously. In this paper, we propose a novel approach to tackle this one-to-many correspondence problem in text-video retrieval. We devise diverse temporal aggregation and a multi-key memory to address temporal and semantic diversity, consequently constructing multiple video embedding paths from a single video. Additionally, we introduce text-guided distillation learning that enables each video path to acquire meaningful distinct competencies in representing varied semantics. Our video embedding approach is text-agnostic, allowing the prepared video embeddings to be used continuously for any new text query. Experiments show our method outperforms existing methods on four datasets. We further validate the effectiveness of our designs with ablation studies and analyses on diverse video embeddings.}
}
@article{GOLDBRAIKH2024110778,
title = {MS-TCRNet: Multi-Stage Temporal Convolutional Recurrent Networks for action segmentation using sensor-augmented kinematics},
journal = {Pattern Recognition},
volume = {156},
pages = {110778},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110778},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005296},
author = {Adam Goldbraikh and Omer Shubi and Or Rubin and Carla M. Pugh and Shlomi Laufer},
keywords = {Action segmentation, Kinematic data, Deep learning, Data augmentation},
abstract = {Action segmentation is a challenging task in high-level process analysis, typically performed on video or kinematic data obtained from various sensors. This work presents two contributions related to action segmentation on kinematic data. Firstly, we introduce two versions of Multi-Stage Temporal Convolutional Recurrent Networks (MS-TCRNet), specifically designed for kinematic data. The architectures consist of a prediction generator with intra-stage regularization and Bidirectional LSTM or GRU-based refinement stages. Secondly, we propose two new data augmentation techniques, World Frame Rotation and Hand Inversion, which utilize the strong geometric structure of kinematic data to improve algorithm performance and robustness. We evaluate our models on three datasets of surgical suturing tasks: the Variable Tissue Simulation (VTS) Dataset and the newly introduced Bowel Repair Simulation (BRS) Dataset, both of which are open surgery simulation datasets collected by us, as well as the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), a well-known benchmark in robotic surgery. Our methods achieved state-of-the-art performance. code: https://github.com/AdamGoldbraikh/MS-TCRNet.}
}
@article{TANG2024110822,
title = {ITFuse: An interactive transformer for infrared and visible image fusion},
journal = {Pattern Recognition},
volume = {156},
pages = {110822},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110822},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005739},
author = {Wei Tang and Fazhi He and Yu Liu},
keywords = {Image fusion, Transformer, Interactive network, Infrared image, Deep learning},
abstract = {Infrared and visible image fusion (IVIF) has attracted increasing attention from the community because of its pleasing results in downstream applications. However, most existing deep fusion models are either feature–level fusion or image–level fusion, leading to information loss. In this paper, we propose an interactive transformer for IVIF, termed ITFuse. In contrast to previous algorithms, ITFuse consists of feature interactive modules (FIMs) and a feature reconstruction module (FRM) to alternatively extract and integrate important features. Specifically, to adequately exploit the common properties of different source images, we design a residual attention block (RAB) for mutual feature representation. To aggregate the distinct characteristics that existed in the corresponding input images, we leverage interactive attention (ITA) to incorporate the complementary information for comprehensive feature preservation and interaction. In addition, cross-modal attention (CMA) and transformer block (TRB) are presented to fully merge the capitalized features and construct long-range relationships. Furthermore, we devise a pixel loss and a structural loss to train the proposed deep fusion model in an unsupervised manner for excess performance amelioration. Massive experiments on popular databases illustrate that our ITFuse performs better than other representative state-of-the-art methods in terms of both qualitative and quantitative assessments. The source code of the proposed method is available at https://github.com/tthinking/ITFuse.}
}
@article{YIN2024110769,
title = {Scalable compressive sampling network with progressive hierarchical subspace learning},
journal = {Pattern Recognition},
volume = {156},
pages = {110769},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110769},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400520X},
author = {Zhu Yin and Zhongcheng Wu and Wuzhen Shi},
keywords = {Compressive sensing, Hierarchical subspace learning, Band-separated sampling, Image reconstruction},
abstract = {Traditional compressive sampling does not sufficiently exploit the sparsity of signals to learn the sampling matrix adaptively. Moreover, they do not independently sample different frequency bands, which makes them ineffective in utilizing information from specific frequency bands. The existing deep learning-based compressive sensing methods achieve good performance with high model complexity, which limits their application to devices with low computing resources or small storage space. To address the above issues and improve the compressive sensing performance of natural images, we propose a novel scalable compressive sampling network with progressive hierarchical subspace learning (called SPHSL-CSNet) in an end-to-end mode. Specifically, the progressive hierarchical sampling strategy based on a three-level wavelet transform is presented, achieving band-separated sampling by extracting the low frequency, low-medium frequency, low-mid-second high frequency and the whole wavelet frequency band of the wavelet transform. This enables our model to obtain more image information with fewer sampling measurements and pay more attention to the reconstruction of texture details. The independent sampling of specific frequency bands is realized through the band-aware mask, which effectively reduces the parameter quantity of the sampling matrix and easier to deploy terminal devices in resource-limited scenarios. Extensive experiments on widely used benchmark datasets not only demonstrate that the proposed SPHSL-CSNet outperforms state-of-the-art performance under the premise of being lightweight, but also effective for the multispectral image compression. Furthermore, SPHSL-CSNet achieves excellent performance on antinoise performance with respect to the existing deep learning-based image CS method in most cases.}
}
@article{LIU2024110845,
title = {A sparse transformer generation network for brain imaging genetic association},
journal = {Pattern Recognition},
volume = {156},
pages = {110845},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110845},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400596X},
author = {Hongrui Liu and Yuanyuan Gui and Hui Lu and Manhua Liu},
keywords = {Imaging genetics, Sparse transformer, Alzheimer’s disease, Brain aging},
abstract = {Brain imaging and genetic data are commonly utilized to investigate brain aging and diseases, particularly Alzheimer’s Disease (AD). Imaging genetics analyze the associations between neuroimaging and genetic data to reveal potential pathological mechanisms for more accurate diagnosis. Many existing methods have limitations in performing fine-grained association analysis between genetic data and phenotypic features extracted from predefined regions of interest in imaging data. To address this issue, this paper proposes a sparse transformer association analysis (STAA) framework that integrates phenotype and genotype feature extraction, identification, and association analysis into a unified model. A key component of the framework is a cross-modal generation network that connects genetic variants with imaging data, enhancing the understanding of the genetic associations underlying imaging patterns in AD and brain aging. Validated using the simulated data and real ADNI dataset, STAA shows superior performance in age regression and AD diagnosis, identifying key genetic and imaging biomarkers and providing association analysis of genetic imaging expression patterns at the voxel level.}
}
@article{CAI2024110782,
title = {Multi-schema prompting powered token-feature woven attention network for short text classification},
journal = {Pattern Recognition},
volume = {156},
pages = {110782},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110782},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005338},
author = {Zijing Cai and Hua Zhang and Peiqian Zhan and Xiaohui Jia and Yongjian Yan and Xiawen Song and Bo Xie},
keywords = {Short text classification, Self-attention mechanism, Token-feature woven attention, Prompt learning, Machine reading comprehension},
abstract = {Short text classification task poses challenges in natural language processing due to insufficient contextual information. This task is typically approached by extracting rich semantic features in the text and encoding it as a sentence-level representation using deep neural networks. The self-attention mechanism has emerged as one of the primary methods to tackle this problem. However, traditional attention methods only focus on the interactions between tokens, neglecting the semantic relationships between features. We propose a novel attention-based module, called token-feature woven attention fusion (TFWAF) network for sentence-level representation information aggregation, which leverages the self-attention mechanism from both token and feature perspectives. Moreover, we design a multi-schema prompting approach within machine reading comprehension and prompt learning paradigms to better utilize prior knowledge in a pre-trained language model and recognize enhanced textual semantic representation. Experimental results show our model achieves state-of-the-art performance compared to existing baselines on eight benchmark datasets in the context of short text classification. The source code is available in https://github.com/Aaronzijingcai/MP-TFWA.}
}
@article{YAN2024110810,
title = {DM-GAN: CNN hybrid vits for training GANs under limited data},
journal = {Pattern Recognition},
volume = {156},
pages = {110810},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110810},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005612},
author = {Longquan Yan and Ruixiang Yan and Bosong Chai and Guohua Geng and Pengbo Zhou and Jian Gao},
keywords = {GAN, Few-shot, Vision transformer, Proprietary artifact image},
abstract = {Generative adversarial network (GAN) training demands substantial data and computational resources. This paper aims to explore an economical approach for generating novel images with limited image data, addressing the challenge of data scarcity. Our contributions involve resolving the few-shot image generation challenge through the development of an unsupervised hybrid generative adversarial network named DM-GAN. We introduce a lightweight hybrid module (DC-Vit) comprising convolution and visual transformation, merging local and global features to enhance image perception, expressiveness, and ensure stable image generation. Additionally, a multi-scale adaptive skip connection module is incorporated to effectively mitigate the feature loss problem arising from inter-layer jumps, thereby producing more complete and regular images. To enhance the texture learning process and improve the quality and realism of synthesized images, we integrate the gray conjugate matrix into the loss function. Empirical evaluations are conducted on small sample datasets at various resolutions, including publicly accessible collections of art paintings, real-life photographs, and proprietary artifact image datasets. The experimental results unequivocally demonstrate the qualitative and quantitative superiority of our model over existing methods, underscoring its efficacy and robustness.}
}
@article{LAI2025110863,
title = {Fast radiance field reconstruction from sparse inputs},
journal = {Pattern Recognition},
volume = {157},
pages = {110863},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110863},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006149},
author = {Song Lai and Linyan Cui and Jihao Yin},
keywords = {3D reconstruction, Neural radiance field, Shape from silhouette, Novel view synthesis},
abstract = {Neural Radiance Field (NeRF) has emerged as a powerful method in data-driven 3D reconstruction because of its simplicity and state-of-the-art performance. However, NeRF requires densely captured calibrated images and lengthy training and rendering time to realize high-resolution reconstruction. Thus, we propose a fast radiance field reconstruction method from a sparse set of images with silhouettes. Our approach integrates NeRF with Shape from Silhouette, a traditional 3D reconstruction method that uses silhouette information to fit the shape of an object. To combine NeRF’s implicit representation with Shape from Silhouette’s explicit representation, we propose a novel explicit–implicit radiance field representation consisting of voxel grids with confidence and feature embedding for geometry and a multilayer perceptron network to decode view-dependent color emission for appearance. We propose to make the reconstructed geometry compact by taking advantage of silhouette images, which can avoid the majority of artifacts in sparse input scenarios and speed up training and rendering. We also apply voxel dilating and pruning to refine the geometry prediction. In addition, we impose a total variation regularization on our model to encourage a smooth radiance field. Experiments on the DTU and the NeRF-Synthetic datasets show that our algorithm surpasses the existing baselines in terms of efficiency and accuracy.}
}
@article{WU2025110818,
title = {A large cross-modal video retrieval dataset with reading comprehension},
journal = {Pattern Recognition},
volume = {157},
pages = {110818},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110818},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005697},
author = {Weijia Wu and Yuzhong Zhao and Zhuang Li and Jiahong Li and Hong Zhou and Mike Zheng Shou and Xiang Bai},
keywords = {Cross-modal, Retrieval, Text reading, Contrastive learning},
abstract = {Most existing cross-modal language-to-video retrieval (VR) research focuses on single-modal input from video, i.e., visual representation, while the text is omnipresent in human environments and frequently critical to understand video. To study how to retrieve video with both modal inputs, i.e., visual and text semantic representations, we firstly introduce a large-scale and cross-modal Video Retrieval dataset with reading comprehension, TextVR , which contains 42.2k sentence queries for 10.5k videos of 8 scenario domains, i.e., Street View (indoor), Street View (outdoor), Game, Sports, Driving, Activity, TV Show, and Cooking. The proposed TextVR requires one unified cross-modal model to recognize and comprehend texts, relate them to the visual context, and decide what text semantic information is vital for the video retrieval task. Besides, we present a detailed analysis of TextVR in comparison to the existing datasets and design a novel multimodal video retrieval baseline for the text-based video retrieval task. The dataset analysis and extensive experiments show that our TextVR benchmark provides many new technical challenges and insights over previous datasets for the video-and-language community. The project website and github repo can be found at CVPR 23 LOVEU and TextVR, respectively.}
}
@article{ZHANG2024110770,
title = {Graph semantic information for self-supervised monocular depth estimation},
journal = {Pattern Recognition},
volume = {156},
pages = {110770},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110770},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005211},
author = {Dongdong Zhang and Chunping Wang and Huiying Wang and Qiang Fu},
keywords = {Monocular depth estimation, Self-supervision, Convolutional neural networks, Graph learning},
abstract = {Self-supervised monocular depth estimation has garnered significant attention in recent years due to its practical value in applications, as it eliminates the need for ground truth depth maps during training. However, its performance usually drops when estimating weakly textured regions and boundary regions, primarily due to the limited depth representation capability of traditional Convolutional Neural Networks (CNNs) that do not support topology. To address these issues, we propose a Graph Semantic Model (GSM) to improve self-supervised monocular depth estimation by utilizing graph learning and semantic information. Our focus is on improving feature representation through graph semantic information. Therefore, we incorporate semantic segmentation and depth estimation into one framework and enhance the interaction of different modal information through the Inter-Directed Graph Reasoning (IDGR) module. In addition, we design the Semantic-Guided Edge Graph Reasoning (SGEGR) module, aiming to boost the network's ability to perceive local depth. Extensive experiments on the KITTI dataset show that our method outperforms the state-of-the-art methods, particularly in accurately estimating depth within weakly textured regions and boundary regions.}
}
@article{KIM2024110846,
title = {Unsupervised outlier detection using random subspace and subsampling ensembles of Dirichlet process mixtures},
journal = {Pattern Recognition},
volume = {156},
pages = {110846},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110846},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005971},
author = {Dongwook Kim and Juyeon Park and Hee Cheol Chung and Seonghyun Jeong},
keywords = {Anomaly detection, Gaussian mixture models, Outlier ensembles, Random projection, Variational inference},
abstract = {Probabilistic mixture models are recognized as effective tools for unsupervised outlier detection owing to their interpretability and global characteristics. Among these, Dirichlet process mixture models stand out as a strong alternative to conventional finite mixture models for both clustering and outlier detection tasks. Unlike finite mixture models, Dirichlet process mixtures are infinite mixture models that automatically determine the number of mixture components based on the data. Despite their advantages, the adoption of Dirichlet process mixture models for unsupervised outlier detection has been limited by challenges related to computational inefficiency and sensitivity to outliers in the construction of outlier detectors. Additionally, Dirichlet process Gaussian mixtures struggle to effectively model non-Gaussian data with discrete or binary features. To address these challenges, we propose a novel outlier detection method that utilizes ensembles of Dirichlet process Gaussian mixtures. This unsupervised algorithm employs random subspace and subsampling ensembles to ensure efficient computation and improve the robustness of the outlier detector. The ensemble approach further improves the suitability of the proposed method for detecting outliers in non-Gaussian data. Furthermore, our method uses variational inference for Dirichlet process mixtures, which ensures both efficient and rapid computation. Empirical analyses using benchmark datasets demonstrate that our method outperforms existing approaches in unsupervised outlier detection.}
}
@article{GAN2024110794,
title = {A survey of dialogic emotion analysis: Developments, approaches and perspectives},
journal = {Pattern Recognition},
volume = {156},
pages = {110794},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110794},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005454},
author = {Chenquan Gan and Jiahao Zheng and Qingyi Zhu and Yang Cao and Ye Zhu},
keywords = {Dialogic emotion analysis, Natural language process, Dialogic artificial intelligence},
abstract = {Dialogic emotion analysis is an emerging and important research field in natural language processing. It aims to understand and process emotions in various forms of dialogue, such as human-human conversations, human–machine interactions, and chatbot responses. However, dialogic emotion analysis faces many challenges, such as the diversity of dialogue genres, the complexity of emotional expressions, and the difficulty of capturing the emotional needs of dialogue participants. Moreover, the current dialogue systems lack the ability to analyze emotions effectively and appropriately in different dialogue contexts. Therefore, a comprehensive review of the existing research on dialogic emotion analysis is needed. This survey aims to review dialogic emotion analysis methods based on natural language processing from 2017 to 2024. The review process follows the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA). We summarize the research methods and emphasize their main research contributions. In addition, we also discuss current research trends and possible future research directions, as well as the impact of personal traits on emotions and potential ethical issues.}
}
@article{CAO2024110761,
title = {Complementary pseudo multimodal feature for point cloud anomaly detection},
journal = {Pattern Recognition},
volume = {156},
pages = {110761},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110761},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005120},
author = {Yunkang Cao and Xiaohao Xu and Weiming Shen},
keywords = {Point cloud, Anomaly detection, Pre-trained representation, Multimodal learning, Rendering},
abstract = {Point cloud anomaly detection is steadily emerging as a promising research area. Recognizing the importance of feature descriptiveness in this task, this study introduces the Complementary Pseudo Multimodal Feature (CPMF), which combines local geometrical information extracted by 3D handcrafted descriptors with global semantic information extracted from 2D pre-trained neural networks. Specifically, to leverage 2D pre-trained neural networks for point-wise feature extraction, this study projects original point clouds into multi-view images. These images are then fed into a pre-trained 2D neural network for informative 2D modality feature extraction. Following the 2D–3D correspondence, the multi-view 2D modality features are projected back to 3D space and aggregated to obtain point-wise 2D modality features. Finally, the point-wise 3D and 2D modality features are fused to derive the CPMF for point cloud anomaly detection. Extensive experiments conducted on MVTec 3D and Real3D datasets demonstrate the complementary capacity between 2D and 3D modality features and the effectiveness of CPMF. Notably, CPMF achieves a significantly higher object-level AUROC of 95.15% compared to other methods on the MVTec 3D benchmark. Code is available at https://github.com/caoyunkang/CPMF.}
}
@article{HONEINE2024110800,
title = {Theoretical insights on the pre-image resolution in machine learning},
journal = {Pattern Recognition},
volume = {156},
pages = {110800},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110800},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400551X},
author = {Paul Honeine},
keywords = {Machine learning, Pattern recognition, Pre-image problem, Fixed-point iteration, Newton’s method, Majorize-minimization algorithm},
abstract = {While many nonlinear pattern recognition and data mining tasks rely on embedding the data into a latent space, one often needs to extract the patterns in the input space. Estimating the inverse of the nonlinear embedding is the so-called pre-image problem. Several strategies have been proposed to address the estimation of the pre-image; However, there are no theoretical results so far to understand the pre-image problem and its resolution. In this paper, we provide theoretical underpinnings of the resolution of the pre-image problem in Machine Learning. These theoretical results are on the gradient descent optimization, the fixed-point iteration algorithm and Newton’s method. We provide sufficient conditions on the convexity/nonconvexity of the pre-image problem. Moreover, we show that the fixed-point iteration is a Newton update and prove that it is a Majorize-Minimization (MM) algorithm where the surrogate function is a quadratic function. These theoretical results are derived for the wide classes of radial kernels and projective kernels. We also provide other insights by connecting the resolution of this problem to the gradient density estimation problem with the so-called mean shift algorithm.}
}
@article{LIU2024110813,
title = {IDSSI: Image Deturbulence with Semantic and Spatial–Temporal Information},
journal = {Pattern Recognition},
volume = {156},
pages = {110813},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110813},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005648},
author = {Xiangqing Liu and Li Tang and Gang Li and Zijun Zhang and Shaoan Yan and Yongguang Xiao and Jianbin Xie and Minghua Tang},
keywords = {Mitigate turbulence effects, Multi-scale perception, Edge cues, Spatial–Temporal feature learning},
abstract = {The propagation of light through turbulent media results in refractive index fluctuations, causing distortion and blurring of images captured by imaging equipment. Consequently, it is of paramount importance to mitigate turbulence effects. Deep convolutional neural networks incorporating attention mechanisms have demonstrated remarkable success in dynamic video restoration. However, in most networks, the attention mechanism is limited to capturing simple contextual features, failing to adequately exploit multi-level image features. This paper introduces IDSSI, an effective model that utilizes semantic and spatio–temporal information for turbulence mitigation. A novel two-branch feature extraction structure is proposed, capable of extracting multi-scale enhanced features with semantic information under limited supervision. These perceived semantic features are subsequently fused into global features, enhancing multi-scale perception for turbulence repair. Furthermore, a new Spatial–Temporal feature learning strategy is proposed. This approach facilitates the extraction and modulation of temporal information by obtaining edge cues, effectively concatenating and merging with spatial features. This strategy serves as an efficient alternative to 3D convolution. Experimental results on relevant datasets demonstrate the superiority of the proposed model over current state-of-the-art turbulence repair methods.}
}
@article{ZHANG2024110841,
title = {BMPCN: A Bigraph Mutual Prototype Calibration Net for few-shot classification},
journal = {Pattern Recognition},
volume = {156},
pages = {110841},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110841},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005922},
author = {Jing Zhang and Mingzhe Chen and Yunzuo Hu and Xinzhou Zhang and Zhe Wang},
keywords = {Few-shot learning, Bigraph Mutual Prototype Calibration Net, Bigraph Mutual Promotion, Proto-level similarity},
abstract = {In recent studies on few-shot classification, most of the existing methods utilized word embeddings as prior knowledge to adjust the distribution of visual prototypes. However, this straightforward fusion of visual and semantic features profoundly alters the feature distribution in the original feature space, rendering it unable to effectively calibrate feature distribution through mutual guidance of cross-modal information. To address this problem, we propose a novel Bigraph Mutual Prototype Calibration Network (BMPCN) for few-shot learning in this paper, in which we not only update the distribution of class features based on prototype-level similarity in both visual and semantic spaces but also facilitate the mutual guidance of visual and semantic feature updates through instance-level similarity. In the BMPCN, a bigraph mutual promotion structure is proposed, wherein a visual graph is constructed with visual features as nodes and the similarity between visual features as edges. Simultaneously, the semantic feature nodes are automatically generated from images, and the class-level prior knowledge is leveraged to correct these automatically generated semantic nodes. To better update the bigraph mutual promotion structure, we propose a Bigraph Interactive Augmentation Module (BIAM), a Nearest Neighbor Proto-level Similarity Promotion Module (NN-PSP), and a Proto-level Similarity Promotion Module (PK-PSP) based on original knowledge augmentation to perform the bigraph update. For inter-graph updating, we use the prototype-level similarity obtained from the NN-PSP and PK-PSP modules to fully learn task-level information, thus enabling task-specific prototype updates. For intra-graph updating, our visual and semantic graphs use instance-level similarity analysis to extract potential correlations between different feature domains and implement mutual guidance in the BIAM module to correct the feature distribution of visual and semantic features. Experiments on three widely used benchmarks illustrated that our proposed method obtains excellent performance based on the backbone Conv-4, and the results outperform state-of-the-art methods by about 8% on miniImageNet, tieredImageNet, and CUB-200-2011. Code has been available at https://github.com/cmzHome/BMPCN-MASTER.}
}
@article{TOGBAN2024110789,
title = {Hierarchical mixture of discriminative Generalized Dirichlet classifiers},
journal = {Pattern Recognition},
volume = {156},
pages = {110789},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110789},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005405},
author = {Elvis Togban and Djemel Ziou},
keywords = {Compositional data, Generalized Dirichlet, Hierarchical mixture of experts, Variational approximation, Upper-bound of generalized Dirichlet mixture},
abstract = {This paper presents a discriminative classifier for compositional data. This classifier is based on the posterior distribution of the Generalized Dirichlet which is the discriminative counterpart of Generalized Dirichlet mixture model. Moreover, following the mixture of experts paradigm, we proposed a hierarchical mixture of this classifier. In order to learn the models parameters, we use a variational approximation by deriving an upper-bound for the Generalized Dirichlet mixture. To the best of our knownledge, this is the first time this bound is proposed in the literature. Experimental results are presented for spam detection and color space identification.}
}
@article{FANG2024110801,
title = {HFGN: High-Frequency residual Feature Guided Network for fast MRI reconstruction},
journal = {Pattern Recognition},
volume = {156},
pages = {110801},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110801},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005521},
author = {Faming Fang and Le Hu and Jinhao Liu and Qiaosi Yi and Tieyong Zeng and Guixu Zhang},
keywords = {Complex convolutional neural network, Deep learning, Fourier convolution, Image reconstruction, Magnetic resonance imaging},
abstract = {Magnetic Resonance Imaging (MRI) is a valuable medical imaging technology, while it suffers from a long acquisition time. Various methods have been proposed to reconstruct sharp images from undersampled k-space data to reduce imaging time. However, these methods hardly reconstruct high-quality aliasing-free Magnetic Resonance (MR) images with clear structures, especially in high-frequency components. To address this problem, we propose a High-Frequency residual feature Guided Network (HFGN) for fast MRI reconstruction. HFGN uses a sub-network, High-Frequency Extraction Network (HFEN), to learn the difference between the U-Net reconstruction result and the ground truth, then uses the learned features to guide the reconstruction of the network. In the reconstruction network, we propose Residual Channel and Spatial Attention block (RCSA), which uses frequency domain and image domain convolution branching to learn the global and local features of the image simultaneously. The experiment results under different acceleration rates on different datasets demonstrate that our proposed method surpasses the existing state-of-the-art methods.}
}
@article{GUARRASI2024110825,
title = {Multimodal explainability via latent shift applied to COVID-19 stratification},
journal = {Pattern Recognition},
volume = {156},
pages = {110825},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110825},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005764},
author = {Valerio Guarrasi and Lorenzo Tronchin and Domenico Albano and Eliodoro Faiella and Deborah Fazzini and Domiziana Santucci and Paolo Soda},
keywords = {XAI, Multimodal deep learning, Joint fusion, Classification, COVID-19},
abstract = {We are witnessing a widespread adoption of artificial intelligence in healthcare. However, most of the advancements in deep learning in this area consider only unimodal data, neglecting other modalities. Their multimodal interpretation necessary for supporting diagnosis, prognosis and treatment decisions. In this work we present a deep architecture, which jointly learns modality reconstructions and sample classifications using tabular and imaging data. The explanation of the decision taken is computed by applying a latent shift that, simulates a counterfactual prediction revealing the features of each modality that contribute the most to the decision and a quantitative score indicating the modality importance. We validate our approach in the context of COVID-19 pandemic using the AIforCOVID dataset, which contains multimodal data for the early identification of patients at risk of severe outcome. The results show that the proposed method provides meaningful explanations without degrading the classification performance.}
}
@article{YUAN2024110773,
title = {Confidence correction for trained graph convolutional networks},
journal = {Pattern Recognition},
volume = {156},
pages = {110773},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110773},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005247},
author = {Junqing Yuan and Huanlei Guo and Chenyi Zhou and Jiajun Ding and Zhenzhong Kuang and Zhou Yu and Yuan Liu},
keywords = {Graph convolutional network, Node classification, Under-confident, Confidence correction mechanism},
abstract = {Adopting Graph Convolutional Networks (GCNs) for transductive node classification is a hot research direction in artificial intelligence. Vanilla GCNs are primarily under-confident and struggle to clarify the final classification results explicitly due to the lack of supervision. Existing works mainly alleviated this issue by improving annotation deficiency and introducing addition regularization terms. However, these methods need to re-train the model from the beginning, which is computationally expensive for large dataset and model. To deal with this problem, a novel confidence correction mechanism (CCM) for trained GCNs is proposed in this work. Such mechanism aims at calibrating the confidence output of each node in the inference stage by jointly inferring the feature and predicted pseudo label. Specifically, in the inference stage, it uses the predicted pseudo label to select target-related features over all network to obtain a more confident and better result. Such selectivity is formulated as an optimization problem to maximize the category score of each node. In addition, the greedy optimization strategy is utilized to solve this problem and we have mathematically proven that the proposed mechanism can reach the local optimum by mathematical induction. Note that such mechanism is flexible and can be introduced to most GCN-based model. Extensive experimental results on benchmark datasets show that the proposed method can promote the confidence of the final target category and improve the performance of GCNs in the inference stage.}
}
@article{LI2025110854,
title = {SANet: Face super-resolution based on self-similarity prior and attention integration},
journal = {Pattern Recognition},
volume = {157},
pages = {110854},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110854},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006058},
author = {Ling Li and Yan Zhang and Lin Yuan and Xinbo Gao},
keywords = {Face super-resolution (FSR), Facial self-similarity, Non-local correlation, Attention integration},
abstract = {Recent deep learning techniques, especially CNN (Convolutional Neural Network), have been driving advancements in face super-resolution (FSR) technologies, achieving unprecedented breakthroughs. However, most existing FSR approaches fail to effectively explore and exploit the inherent self-similarity information of face images, deteriorating the FSR performance. In this paper, we propose a novel attention integration network (SANet) incorporating self-similarity information to model non-local pixel-level dependencies of features. The SANet mainly consists of Hybrid Attention Integration Modules (HAIMs), Self-similarity Information Mining Modules (SIMMs), and a CycleMLP-based Reconstruction Unit (CRU). The HAIM is designed to adaptively bootstrap features relevant to informative facial regions through the customized attention aggregation mechanism, enabling more discriminative feature extraction. The SIMM is dedicated to constructing enhanced features by thoroughly mining the self-similarity information and modeling feature-wise correlations. This is achieved with the help of the clever implementation of the well-designed Symmetric Nearest Neighbor Sampling (SNNS) strategy and Non-local Aggregated Sparse Attention (NASA) mechanism. Based on the iterative interaction between HAMIs and SIMMs, crucial facial feature information can be progressively aggregated. The CRU-based reconstruction module is crafted to restore facial details with greater pixel-wise precision more efficiently. Comprehensive experimental results on three face benchmark datasets demonstrate the superiority of the proposed SANet over current state-of-the-art methods.}
}
@article{CHEN2024110837,
title = {Feature selections based on two-type overlap degrees and three-view granulation measures for k-nearest-neighbor rough sets},
journal = {Pattern Recognition},
volume = {156},
pages = {110837},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110837},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005880},
author = {Jiang Chen and Xianyong Zhang and Zhong Yuan},
keywords = {-nearest-neighbor rough set, Neighborhood rough set, Feature selection, Overlap degree, Dependency degree, Information measure},
abstract = {Feature selections facilitate data learning, and they effectively develop by combining the overlap degree and dependency degree related to k-nearest-neighbor (KNN) rough sets. However, the overlap degree neglects the fusion centrality while the dependency degree adopts only the algebraic perspective, so the corresponding feature selection has advance space. In this paper, the overlap degree is improved by fusion priority while the dependency degree is enriched by informational and dual viewpoints, so systematic feature selections are two-dimensionally established to generate multiple improved algorithms. At first, an improved type of overlap degrees is proposed by operationally exchanging the integration summation and fusion division, thus better motivating feature sorting. Then based on KNN granulation, informational, joint, conditional entropies are constructed, and they derive relative entropies by combining the dependency degree; corresponding size relationships, system equations, and granulation monotonicity are acquired. Furthermore, three-view granulation measures (i.e., the dependency degree, conditional entropy, relative conditional entropy) determine three-view attribute reducts based on feature significances; after pre-sorting deletion features based on two-type overlap degrees, 2×3=6 heuristic reduction algorithms are systematically established to extend recent algorithm OD&KNN. Finally, relevant uncertainty measures and feature selections are validated through data experiments, and five improved selection algorithms achieve better classification performance.}
}
@article{VANMA2024110785,
title = {Visual multi-object tracking with re-identification and occlusion handling using labeled random finite sets},
journal = {Pattern Recognition},
volume = {156},
pages = {110785},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110785},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005363},
author = {Linh {Van Ma} and Tran Thien Dat Nguyen and Changbeom Shim and Du Yong Kim and Namkoo Ha and Moongu Jeon},
keywords = {Visual multi-object tracking, Track reappearance, Re-ID feature, Occlusion handling, Labeled random finite set},
abstract = {This paper proposes an online visual multi-object tracking (MOT) algorithm that resolves object appearance–reappearance and occlusion. Our solution is based on the labeled random finite set (LRFS) filtering approach, which in principle, addresses disappearance, appearance, reappearance, and occlusion via a single Bayesian recursion. However, in practice, existing numerical approximations cause reappearing objects to be initialized as new tracks, especially after long periods of being undetected. In occlusion handling, the filter’s efficacy is dictated by trade-offs between the sophistication of the occlusion model and computational demand. Our contribution is a novel modeling method that exploits object features to address reappearing objects whilst maintaining a linear complexity in the number of detections. Moreover, to improve the filter’s occlusion handling, we propose a fuzzy detection model that takes into consideration the overlapping areas between tracks and their sizes. We also develop a fast version of the filter to further reduce the computational time.}
}
@article{WANG2024110757,
title = {Discriminative atoms embedding relation dual network for classification of choroidal neovascularization in OCT images},
journal = {Pattern Recognition},
volume = {156},
pages = {110757},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110757},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005089},
author = {Ruifeng Wang and Guang Zhang and Xiaoming Xi and Longsheng Xu and Xiushan Nie and Jianhua Nie and Xianjing Meng and Yanwei Zhang and Xinjian Chen and Yilong Yin},
keywords = {Choroidal neovascularization, Optical coherence tomography, CNV classification, Semi-supervised learning},
abstract = {Choroidal neovascularization (CNV) is an eye disease that can cause vision loss. Automatic CNV classification in OCT images is crucial in the treatment of CNV. However, two problems arise for CNV classification in OCT images. The subtle visual differences between different CNV types render classification difficult. Additionally, it is difficult to obtain sufficient labeled data, which results in performance degradation. In order to solve these two problems, a discriminative atom-embedding relation dual network is proposed in this paper. Considering that semi-supervised learning (SSL) is an effective machine learning framework to make full use of limited labeled data and a large amount of unlabeled data, the proposed network is developed within an SSL framework. To capture the visual differences, novel discriminative atoms are first introduced to mine discriminative information between different CNV types. Subsequently, a relation module is incorporated to embed the learned discriminative atom information into the features. This makes the learned features capable of distinguishing between different CNV types. Moreover, a novel relation consistency loss is proposed to further improve the robustness of the learned features. Experimental results on private and public datasets demonstrate the effectiveness of the proposed method.}
}
@article{SHI2024110784,
title = {Learning to match features with discriminative sparse graph neural network},
journal = {Pattern Recognition},
volume = {156},
pages = {110784},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110784},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005351},
author = {Yan Shi and Jun-Xiong Cai and Mingyu Fan and Wensen Feng and Kai Zhang},
keywords = {Feature matching, Graph neural network, Visual localization, Attention},
abstract = {We propose a cluster-based sparse graph network to improve the efficiency of image feature matching. This architecture clusters keypoints with high correlations into the same subgraphs, where each keypoint interacts only with others within the same subgraph. This strategy effectively reduces the spread of redundant messages and boosts the efficiency of message transmission. A unique coarse-to-fine paradigm is proposed for the incremental construction of sparse graphs, facilitating the evolution of subgraphs from coarse to fine, which enhances keypoint correlation and reduces misclassification. Additionally, the introduction of global tokens within each subgraph enables the learning of global information through interactions with a limited number of global tokens, further minimizing the impact of misclassification by broadening the scope of learning beyond the limits of individual subgraphs. The methodology demonstrates competitive performance in a range of vision tasks, including pose estimation, visual localization, and homography estimation. Compared to complete graph networks, it reduces time and memory consumption by 91% and 46%, respectively, during dense matching. Moreover, building on this foundational architecture, we introduce a novel hierarchical approach for visual localization, utilizing a two-stage sparse-to-dense matching process, achieves a substantial 31.8% decrease in time consumption while maintains competitive accuracy.}
}
@article{LI2024110768,
title = {D2GL: Dual-level dual-scale graph learning for sketch-based 3D shape retrieval},
journal = {Pattern Recognition},
volume = {156},
pages = {110768},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110768},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005193},
author = {Wenjing Li and Jing Bai and Hu Zheng},
keywords = {3D shape retrieval, Sketch-based 3D shape retrieval, Structural information, Graph learning-based feature extraction, Dual-level dual-scale graph learning (DGL)},
abstract = {Sketch-based 3D shape retrieval (SBSR) is an active research area in the computer vision community, but it is still very challenging. One main reason is that existing deep learning-based methods usually treat sketches as 2D images, neglecting the sparsity and diversity. In this paper, we propose a novel Dual-level Dual-scale Graph Learning (D2GL) method to effectively enhance structural information and produce robust representations for sparse and diverse hand-drawn sketches. Specifically, in addition to the traditional branches for SBSR, we introduce a Dual-level Dual-scale Graph Self-attention (DLDS-GSA) as an auxiliary branch. DLDS-GSA further consists of two levels of encoders, i.e., a local structural encoder and a dual-scale global structural encoder, to capture both local discriminative and multi-scale global structures while minimizing the impact of various sketch drawing details. Comprehensive experiments on SHREC’13 and SHREC’14 datasets demonstrate the superiority of D2GL for SBSR, with extended experiments on PART-SHREC’14 confirming its generalization for unseen classes in SBSR.}
}
@article{ZHANG2024110839,
title = {Learning latent disentangled embeddings and graphs for multi-view clustering},
journal = {Pattern Recognition},
volume = {156},
pages = {110839},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110839},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005909},
author = {Chao Zhang and Haoxing Chen and Huaxiong Li and Chunlin Chen},
keywords = {Multi-view clustering, Embedding disentanglement, Graph learning, Low-rank tensor},
abstract = {Graph based methods have recently attracted much attention for multi-view clustering. Most existing methods seek the latent shared embeddings to learn a unified similarity graph or fuse multiple view-specific graphs to a consensus one for clustering, which may not sufficiently explore the common and complementary information among views. Besides, the high-order inter-view correlations are not fully investigated. To address these issues, this paper proposes a latent Disentangled Embeddings and GRaphs based multi-viEw clustEring (DEGREE) method, which considers the common and view-specific information in a latent subspace by explicit embedding disentanglement and multiple graphs learning. We assume that each view can be generated from a shared latent embedding and a corresponding view-specific embedding, which model the common information and exclusive complementary information among views, respectively. The intra-view and inter-view exclusivities among embeddings are encouraged by an orthogonality regularizer. To fully use the underlying information, we excavate the pairwise instance relations in both shared embedding and diverse view-specific embeddings by learning multiple graphs. Besides, a tensor singular value decomposition (t-SVD) based tensor nuclear norm regularizer is imposed on view-specific graphs, which helps to explore the high-order inter-view correlations. An alternative optimization algorithm is designed to solve the proposed model. Experimental evaluations on several popular datasets demonstrate that our DEGREE method outperforms the state-of-the-art methods.}
}
@article{WANG2024110836,
title = {Detect-order-construct: A tree construction based approach for hierarchical document structure analysis},
journal = {Pattern Recognition},
volume = {156},
pages = {110836},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110836},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005879},
author = {Jiawei Wang and Kai Hu and Zhuoyao Zhong and Lei Sun and Qiang Huo},
keywords = {Document layout analysis, Table of contents, Reading order prediction, Page object detection},
abstract = {Document structure analysis (aka document layout analysis) is crucial for understanding the physical layout and logical structure of documents, with applications in information retrieval, document summarization, knowledge extraction, etc. In this paper, we concentrate on Hierarchical Document Structure Analysis (HDSA) to explore hierarchical relationships within structured documents created using authoring software employing hierarchical schemas, such as LaTeX, Microsoft Word, and HTML. To comprehensively analyze hierarchical document structures, we propose a tree construction based approach that addresses multiple subtasks concurrently, including page object detection (Detect), reading order prediction of identified objects (Order), and the construction of intended hierarchical structure (Construct). We present an effective end-to-end solution based on this framework to demonstrate its performance. To assess our approach, we develop a comprehensive benchmark called Comp-HRDoc, which evaluates the above subtasks simultaneously. Our end-to-end system achieves state-of-the-art performance on two large-scale document layout analysis datasets (PubLayNet and DocLayNet), a high-quality hierarchical document structure reconstruction dataset (HRDoc), and our Comp-HRDoc benchmark. The Comp-HRDoc benchmark is publicly available at .}
}
@article{WANG2024110840,
title = {Mask-DerainGAN: Learning to remove rain streaks by learning to generate rainy images},
journal = {Pattern Recognition},
volume = {156},
pages = {110840},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110840},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005910},
author = {Pengjie Wang and Pei Wang and Miaomiao Chen and Rynson W.H. Lau},
keywords = {Generative adversarial networks, Rain removal and generation, Mask guidance, Contrast learning},
abstract = {Image deraining with unpaired data has been a challenging problem. Previous methods suffer from either the color distortion artifacts, due to the pixel-level cycle consistency loss, or the time-consuming training process. To address these problems, in this paper, we propose a novel method for rain removal based on using unpaired data. First, we obtain a rain streak mask from the derained result, which serves as a guidance for generating rainy images. Both the mask and the rain-free image are then fed into the proposed generator to obtain a high-quality rainy image, which implicitly helps improve the rain removal performance. In this way, the proposed learning framework simultaneously learns rain removal and rain generation in order to produce high-quality rain-free images and rainy images. Second, we propose a contrastive learning generator to preserve background texture details and ensure semantic consistency between the generated rain-free image and the original input. Experimental results demonstrate that our method surpasses most state-of-the-art unsupervised methods on multiple benchmark synthetic and real datasets.}
}
@article{FIAZ2024110812,
title = {Guided-attention and gated-aggregation network for medical image segmentation},
journal = {Pattern Recognition},
volume = {156},
pages = {110812},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110812},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005636},
author = {Mustansar Fiaz and Mubashir Noman and Hisham Cholakkal and Rao Muhammad Anwer and Jacob Hanna and Fahad Shahbaz Khan},
keywords = {Medical image segmentation, Multi-scale feature aggregation, Mask-guided feature attention, Deep supervision, Transformers, Convolutional neural networks},
abstract = {Recently, transformers have been widely used in medical image segmentation to capture long-range and global dependencies using self-attention. However, they often struggle to learn the local details which limit their ability to capture irregular shapes and sizes of the tissues and indistinct boundaries between the tissues, which are critical for accurate segmentation. To alleviate this issue, we propose a network named GA2Net, which comprises an encoder, a bottleneck, and a decoder. The encoder computes multi-scale features. In the bottleneck, we propose a hierarchical-gated features aggregation (HGFA) which introduces a novel spatial gating mechanism to enrich the multi-scale features. To effectively learn the shapes and sizes of the tissues, we apply deep supervision in the bottleneck. GA2Net proposes to use adaptive aggregation (AA) within the decoder, to adjust the receptive fields for each location in the feature map, by replacing the traditional concatenation/summation operations in skip connections in U-Net like architecture. Furthermore, we propose mask-guided feature attention (MGFA) modules within the decoder which strives to learn the salient features using foreground priors to adequately grasp the intricate structural and contour information of the tissues. We also apply intermediate supervision for each stage of the decoder, which further improves the capability of the model to better locate the boundaries of the tissues. Our extensive experimental results illustrate that our GA2-Net significantly outperforms the existing state-of-the-art methods over eight medical image segmentation datasets i.e., five polyps, a skin lesion, a multiple myeloma cell segmentation, and a cardiac MRI scan datasets. We then perform an extensive ablation study to validate the capabilities of our method. Code is available at https://github.com/mustansarfiaz/ga2net.}
}
@article{GOU2024110760,
title = {Cascaded learning with transformer for simultaneous eye landmark, eye state and gaze estimation},
journal = {Pattern Recognition},
volume = {156},
pages = {110760},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110760},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005119},
author = {Chao Gou and Yuezhao Yu and Zipeng Guo and Chen Xiong and Ming Cai},
keywords = {Eye tracking, Cascaded learning, Multi-task learning, Diffusion model, Deep learning},
abstract = {Eye tracking have garnered attention in human–machine interaction, disease monitoring, biometrics, etc. Existing investigations for eye tracking have predominantly concentrated on individual task for pupil detection or gaze estimation, overlooking the implicit relationships that exist among different tasks for eye tracking. In this work, we introduce a cascaded framework with transformer to collaboratively realize eye landmark detection, eye state detection and gaze estimation. Within our framework, we leverage Transformer to capture long dependencies with explicit eye-related structural information and implicit correlation among different tasks. Furthermore, the proposed cascade iteration framework alternatively optimize each task and boost the overall performance for pupil center, eye state and gaze estimation simultaneously. To address the problem of manual annotation, we further introduce the Control-Eye Diffusion Model (CEDM), a controllable eye image generation method conditioned on a simple contour with structure information. The proposed methods are evaluated on challenging datasets such as GI4E, BioID and MPIIGaze, and the results show that our methods outperform state-of-the-art methods in several tasks.}
}
@article{HU2024110783,
title = {A knowledge graph completion model based on triple level interaction and contrastive learning},
journal = {Pattern Recognition},
volume = {156},
pages = {110783},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110783},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400534X},
author = {Jie Hu and Hongqun Yang and Fei Teng and Shengdong Du and Tianrui Li},
keywords = {Knowledge graph completion, Link prediction, Semantic matching, Contrastive learning, Negative sampling},
abstract = {Knowledge graphs provide credible and structured knowledge for downstream tasks such as information retrieval. Nevertheless, the ubiquitous incompleteness of knowledge graphs often limits the performance of applications. To address the incompleteness, people have proposed the knowledge graph completion task to supplement the facts of incomplete triplets. Recently, researchers have proposed introducing text descriptions to enrich entity representations. Existing methods based on triple decoupling with text description solve the combinatorial explosion problem well. Nevertheless, they still suffer from a lack of global characteristics of factual triples. In addition, the success of contrastive learning research has improved such methods, but they are still limited by existing negative sampling, which is usually more costly than embedding-based methods. In order to solve these limitations, this paper proposes an innovative triple-level interaction model for knowledge graph completion named InCL-KGC. Concretely, the proposed model employs an on-verge interaction method to reduce text redundancy information for entity representation and capture the global semantics of factual triplets. Furthermore, we design an effective hard negative sampling strategy to improve contrast learning. Additionally, we perform an improved Harbsort algorithm for the purpose of reducing the adverse impact of candidate entity sparsity on inference. Extensive experiment consequences exhibit that our model transcends recent baselines with MRR, Hit@3, and Hits@10 increased by 1.2%, 3.2%, and 6.8% on WN18RR, while the index MRR, Hit@1, Hit@3, and Hits@10 were enhanced by 2.8%, 1%, 3.3%, 4.3% on FB15K-237.}
}
@article{SUN2024110824,
title = {Federated zero-shot learning with mid-level semantic knowledge transfer},
journal = {Pattern Recognition},
volume = {156},
pages = {110824},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110824},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005752},
author = {Shitong Sun and Chenyang Si and Guile Wu and Shaogang Gong},
keywords = {Federated learning, Knowledge transfer},
abstract = {Conventional centralized deep learning paradigms are not feasible when data from different sources cannot be shared due to data privacy or transmission limitation. To resolve this problem, federated learning has been introduced to transfer knowledge across multiple sources (clients) with non-shared data while optimizing a globally generalized central model (server). Existing federated learning paradigms mostly focus on transmitting image encoders that take instance-sensitive images as input, making them less generalizable and vulnerable to privacy inference attacks. In contrast, in this work, we consider transferring mid-level semantic knowledge (such as attribute) which is not sensitive to specific objects of interest and therefore is more privacy-preserving and general. To this end, we formulate a new Federated Zero-Shot Learning (FZSL) paradigm to learn mid-level semantic knowledge at multiple local clients with non-shared local data and cumulatively aggregate a globally generalized central model for deployment. To improve model discriminative ability, we explore semantic knowledge available from either a language or a vision-language foundation model in order to enrich the mid-level semantic space in FZSL. Extensive experiments on five zero-shot learning benchmark datasets validate the effectiveness of our approach for optimizing a generalizable federated learning model with mid-level semantic knowledge transfer.}
}
@article{HOU2024110772,
title = {Flexible density peak clustering for real-world data},
journal = {Pattern Recognition},
volume = {156},
pages = {110772},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110772},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005235},
author = {Jian Hou and Houshen Lin and Huaqiang Yuan and Marcello Pelillo},
keywords = {Clustering, Density peak, Real-world data, Number of clusters},
abstract = {In density based clustering, the density peak algorithm has attracted much attention due to its effectiveness and simplicity, and a vast amount of clustering approaches have been proposed based on this algorithm. Some of these works require manual selection of cluster centers with a decision graph, where human involvement leads to uncertainty in clustering results. In order to avoid human involvement, some other algorithms depend on user-specified number of clusters to determine cluster centers automatically. However, it is well known that accurate estimation of number of clusters is a long-standing difficulty in data clustering. In this paper we present a sequential density peak clustering algorithm to extract clusters one by one, thereby determining the number of clusters automatically and avoiding manual selection of cluster centers in the meanwhile. Starting from a density peak, our algorithm generates an initial cluster surrounding the density peak in the first step, and then obtains the final cluster by expanding the initial cluster based on the relative density relationship among neighboring data points. With a peeling-off strategy, we obtain all the clusters sequentially. Our algorithm works well with clusters of Gaussian distribution and is therefore potential for clustering of real-world data. Experiments with a large number of synthetic and real datasets and comparisons with existing algorithms demonstrate the effectiveness of the proposed algorithm.}
}
@article{ZHANG2024110848,
title = {LiDARCapV2: 3D human pose estimation with human–object interaction from LiDAR point clouds},
journal = {Pattern Recognition},
volume = {156},
pages = {110848},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110848},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005995},
author = {Jingyi Zhang and Qihong Mao and Siqi Shen and Chenglu Wen and Lan Xu and Cheng Wang},
keywords = {3D human pose estimation, Human–object interaction, LiDAR point clouds},
abstract = {Human–object interactions in open environments are common in the real world. Estimating 3D human pose from data where objects occlude the human is a challenging task in biometrics. However, existing LiDAR-based human motion capture datasets lack occlusion scenarios between humans and objects. To overcome this limitation, we propose LiDARHuman51M, a new human–object interaction dataset captured by LiDAR in long-range outdoor scene. It includes human motion labels acquired by an IMU system and synchronous RGB images. Additionally, we present an occlusion-aware method, LiDARCapV2, for capturing human motion from LiDAR point clouds under human–object interaction settings. Our key insight is to overcome object interference in human feature extraction by introducing a module called AgNoise-Segment. A noise augmentation strategy introduced in the AgNoise-Segment module alleviates the dependency of the segmentation accuracy on the effectiveness of 3D human pose estimations. Furthermore, we propose a skeleton extraction module that integrates features learned from the AgNoise-Segment module and predicts the skeleton locations. Quantitative and qualitative experiments demonstrate that LiDARCapV2 can capture high-quality 3D human motion under human–object interaction settings. Experiments on the KITTI and Waymo datasets demonstrate that our method can be generalized to real-world open scenarios.}
}
@article{JIANG2024110796,
title = {Exposure difference network for low-light image enhancement},
journal = {Pattern Recognition},
volume = {156},
pages = {110796},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110796},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005478},
author = {Shengqin Jiang and Yongyue Mei and Peng Wang and Qingshan Liu},
keywords = {Low-light image, Image enhancement, Exposure difference, Neural network},
abstract = {Low-light image enhancement aims to simultaneously improve the brightness and contrast of low-light images and recover the details of the visual content. This is a challenging task that makes typical data-driven methods suffer, especially when faced with severe information loss in extreme low-light conditions. In this work, we approach this task by proposing a novel exposure difference network. The proposed network generates a set of possible exposure corrections derived from the differences between synthesized images under different exposure levels, which are fused and adaptively combined with the raw input for light compensation. By modeling the intermediate exposure differences, our model effectively eliminates the redundancy existing in the synthesized data and offers the flexibility to handle image quality degradation resulting from varying levels of inadequate illumination. To further enhance the naturalness of the output image, we propose a global-aware color calibration module to derive low-frequency global information from inputs, which is further converted into a projection matrix to calibrate the RGB output. Extensive experiments show that our method can achieve competitive light enhancement performance both quantitatively and qualitatively.}
}
@article{DIKO2024110853,
title = {ReViT: Enhancing vision transformers feature diversity with attention residual connections},
journal = {Pattern Recognition},
volume = {156},
pages = {110853},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110853},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006046},
author = {Anxhelo Diko and Danilo Avola and Marco Cascio and Luigi Cinque},
keywords = {Vision transformer, Feature collapse, Self-attention mechanism, Residual attention learning, Visual recognition},
abstract = {Vision Transformer (ViT) self-attention mechanism is characterized by feature collapse in deeper layers, resulting in the vanishing of low-level visual features. However, such features can be helpful to accurately represent and identify elements within an image and increase the accuracy and robustness of vision-based recognition systems. Following this rationale, we propose a novel residual attention learning method for improving ViT-based architectures, increasing their visual feature diversity and model robustness. In this way, the proposed network can capture and preserve significant low-level features, providing more details about the elements within the scene being analyzed. The effectiveness and robustness of the presented method are evaluated on five image classification benchmarks, including ImageNet1k, CIFAR10, CIFAR100, Oxford Flowers-102, and Oxford-IIIT Pet, achieving improved performances. Additionally, experiments on the COCO2017 dataset show that the devised approach discovers and incorporates semantic and spatial relationships for object detection and instance segmentation when implemented into spatial-aware transformer models.}
}
@article{ALFASLY2024110808,
title = {Auxiliary audio–textual modalities for better action recognition on vision-specific annotated videos},
journal = {Pattern Recognition},
volume = {156},
pages = {110808},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110808},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005594},
author = {Saghir Alfasly and Jian Lu and Chen Xu and Yu Li and Yuru Zou},
keywords = {Action recognition, Multimodal training, Large language models, Video transformer, Audio–visual training},
abstract = {Most current audio–visual datasets are class-relevant, where audio and visual modalities are annotated. Thus, current audio–visual recognition methods apply cross-modality attention or modality fusion. However, leveraging the audio modality effectively in vision-specific videos for human activity recognition is of particular challenge. We address this challenge by proposing a novel audio–visual recognition framework that effectively leverages audio modality in any vision-specific annotated dataset. The proposed framework employs language models (e.g., GPT-3, CPT-text, BERT) for building a semantic audio–video label dictionary (SAVLD) that serves as a bridge between audio and video datasets by mapping each video label to its most K-relevant audio labels. Then, SAVLD along with a pre-trained audio multi-label model are used to estimate the audio–visual modality relevance. Accordingly, we propose a novel learnable irrelevant modality dropout (IMD) to completely drop the irrelevant audio modality and fuse only the relevant modalities. Finally, for the efficiency of the proposed multimodal framework, we present an efficient two-stream video Transformer to process the visual modalities (i.e., RGB frames and optical flow). The final predictions are re-ranked with GPT-3 recommendations of the human activity classes. GPT-3 provides high-level recommendations using the labels of the detected visual objects and the audio predictions of the input video. Our framework demonstrated a remarkable performance on the vision-specific annotated datasets Kinetics400 and UCF-101 by outperforming most relevant human activity recognition methods.}
}
@article{ZHONG2024110756,
title = {Dehazing & Reasoning YOLO: Prior knowledge-guided network for object detection in foggy weather},
journal = {Pattern Recognition},
volume = {156},
pages = {110756},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110756},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005077},
author = {Fujin Zhong and Wenxin Shen and Hong Yu and Guoyin Wang and Jun Hu},
keywords = {Object detection, Foggy weather, Prior knowledge, End-to-end network},
abstract = {Fast and accurate object detection in foggy weather is crucial for visual tasks such as autonomous driving and video surveillance. Existing methods typically preprocess images with enhancement techniques before the object detector, so that the real-time performance of object detection decreases to some extent. Meanwhile, many popular object detection models rely solely on visual features for localization and classification. When fog is present, visual features would be so adversely impacted that the detection accuracy sharply decreases. Therefore, we propose an end-to-end prior knowledge-guided network called DR-YOLO for object detection in foggy weather. DR-YOLO integrates the atmospheric scattering model and the co-occurrence relation graph as prior knowledge into the entire training process of the detector. Firstly, Restoration Subnet Module (RSM) is designed to employ the atmospheric scattering model to guide the learning direction of the detector for dehazing features. Specifically, it is only adopted during the training process and does not increase the time cost of detection process. Secondly, for guiding the detector to pay more attention to potential co-occurring objects in the same scene, we introduce Relation Reasoning Attention Module (RRAM) that utilizes the co-occurrence relation graph to supplement deficient visual features in foggy weather. In addition, DR-YOLO employs Adaptive Feature Fusion Module (AFFM) to effectively merge the key features from the backbone and neck for the needs of RRAM and RSM. Finally, we conduct experiments on clear, synthetic and real-world foggy datasets to demonstrate the effectiveness of DR-YOLO. The source code is available at https://github.com/wenxinss/DR-YOLO.}
}
@article{YE2024110775,
title = {Application of Tswin-F network based on multi-scale feature fusion in tomato leaf lesion recognition},
journal = {Pattern Recognition},
volume = {156},
pages = {110775},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110775},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005260},
author = {Yuanbo Ye and Houkui Zhou and Huimin Yu and Haoji Hu and Guangqun Zhang and Junguo Hu and Tao He},
keywords = {Plant leaf disease identification, Bilateral attention mechanism, Belf-supervised learning, Feature fuse local attention},
abstract = {Tomato leaf lesion identification can greatly help the detection and analysis of plant lesions. This study proposes Tswin-F network, a new network structure based on Transformer, to detect tomato leaf diseases. This Tswin-F network would obtain position information on images by implementing the bilateral local attention module and the self-supervised learning module. Specifically, the bilateral local attention mechanism focuses on the connection with certain continuous tokens, while the self-supervised learning module pays attention to the connection with random token positions. Then the information learned from the above two modules approaches will be combined to create the spatial connection between the final tokens. The combination of the above two modules can enhance the ability to communicate information between the windows of the input images and improve the accuracy of the models. In addition, a Feature Fuse Local Attention (FFLCA) structure is designed to solve the problem that attention distances would increase with the number of layers in the transformer network model. Furthermore, all the feature information is fused through the adaptive fusion strategy and is inputted into the classification network as the final global information of the model. Finally, an accuracy of 99.64% is obtained on 10 types of datasets, reaching the state-of-the-art level of CNN-based methods in terms of accuracy. The accuracy rate of identifying 13 types of tomato leaf lesions reaches 90.81% on average. Code is available at: https://github.com/fightpotato.}
}
@article{ZHAO2025110856,
title = {Balanced feature fusion collaborative training for semi-supervised medical image segmentation},
journal = {Pattern Recognition},
volume = {157},
pages = {110856},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110856},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006071},
author = {Zhongda Zhao and Haiyan Wang and Tao Lei and Xuan Wang and Xiaohong Shen and Haiyang Yao},
keywords = {Medical image segmentation, Semi-supervised learning, Collaborative training},
abstract = {Collaborative learning is a fundamental component of consistency learning. It has been extensively utilized in semi-supervised medical image segmentation, primarily based on the learning of multiple models from each other. However, existing semi-supervised collaborative image segmentation methods face two primary issues. Firstly, these methods fail to fully leverage the hidden knowledge within the models during a knowledge exchange, resulting in inefficient knowledge sharing and limited generalization capabilities. To address this, we propose a novel approach, termed ‘fusion teacher’, which merges the knowledge of two models at the feature-level. This enhances the efficiency of knowledge exchange between models and generates more accurate pseudo-labels for consistency learning. Secondly, the initial and intermediate stages of collaborative learning are hindered by a significant performance gap between the fusion teacher and student models, impairs effective knowledge transfer. Our approach advocates a gradual increase in the dropout rate. This strategy enhances the transfer efficiency of knowledge from a fusion teacher to a student model. To demonstrate the efficacy of our method, we conduct experiments on the ISIC, ACDC, and AbdomenCT-1K datasets. Our approach achieves Dice scores of 87.4%, 84.8%, and 84.5%, respectively, with 10% labelled data. Compared with the current state-of-the-art (SOTA) methods, our method demonstrates strong competitiveness.}
}
@article{LIU2024110819,
title = {Triadic temporal-semantic alignment for weakly-supervised video moment retrieval},
journal = {Pattern Recognition},
volume = {156},
pages = {110819},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110819},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005703},
author = {Jin Liu and JiaLong Xie and Fengyu Zhou and Shengfeng He},
keywords = {Weakly supervised learning, Video moment retrieval, Temporal-semantic alignment},
abstract = {Video Moment Retrieval (VMR) aims to identify specific event moments within untrimmed videos based on natural language queries. Existing VMR methods have been criticized for relying heavily on moment annotation bias rather than true multi-modal alignment reasoning. Weakly supervised VMR approaches inherently overcome this issue by training without precise temporal location information. However, they struggle with fine-grained semantic alignment and often yield multiple speculative predictions with prolonged video spans. In this paper, we take a step forward in the context of weakly supervised VMR by proposing a triadic temporal-semantic alignment model. Our proposed approach augments weak supervision by comprehensively addressing the multi-modal semantic alignment between query sentences and videos from both fine-grained and coarse-grained perspectives. To capture fine-grained cross-modal semantic correlations, we introduce a concept-aspect alignment strategy that leverages nouns to select relevant video clips. Additionally, an action-aspect alignment strategy with verbs is employed to capture temporal information. Furthermore, we propose an event-aspect alignment strategy that focuses on event information within coarse-grained video clips, thus mitigating the tendency towards long video span predictions during coarse-grained cross-modal semantic alignment. Extensive experiments conducted on the Charades-CD and ActivityNet-CD datasets demonstrate the superior performance of our proposed method.}
}
@article{LIU2024110843,
title = {Multi-order graph clustering with adaptive node-level weight learning},
journal = {Pattern Recognition},
volume = {156},
pages = {110843},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110843},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005946},
author = {Ye Liu and Xuelei Lin and Yejia Chen and Reynold Cheng},
keywords = {Graph clustering, Motifs, Higher-order structure, Spectral clustering, Optimization},
abstract = {Current graph clustering methods emphasize individual node and edge connections, while ignoring higher-order organization at the level of motif. Recently, higher-order graph clustering approaches have been designed by motif-based hypergraphs. However, these approaches often suffer from hypergraph fragmentation issue seriously, which degrades the clustering performance greatly. Moreover, real-world graphs usually contain diverse motifs, with nodes participating in multiple motifs. A key challenge is how to achieve precise clustering results by integrating information from multiple motifs at the node level. In this paper, we propose a multi-order graph clustering model (MOGC) to integrate multiple higher-order structures and edge connections at node level. MOGC employs an adaptive weight learning mechanism to automatically adjust the contributions of different motifs for each node. This not only tackles hypergraph fragmentation issue but enhances clustering accuracy. MOGC is efficiently solved by an alternating minimization algorithm. Experiments on seven real-world datasets illustrate the effectiveness of MOGC.}
}
@article{QIAN2024110791,
title = {Partial label feature selection based on noisy manifold and label distribution},
journal = {Pattern Recognition},
volume = {156},
pages = {110791},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110791},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005429},
author = {Wenbin Qian and Jiale Liu and Wenji Yang and Jintao Huang and Weiping Ding},
keywords = {Feature selection, Label distribution, Manifold learning, Feature dependency, Partial label learning},
abstract = {In partial label learning, each training object is assigned a valid label and pseudo-labels, and a multi-class classifier is derived with inaccurate supervision. However, ambiguous labeling information adversely affects the performance of the classifier. Partial label feature selection has been shown efficiently improve the generalization performance of classifiers. Traditional manifold learning can employ intrinsic geometric information to identify discriminative features, while it is challenging due to the noisy manifold caused by pseudo-labels. Consequently, this paper proposes an embedding partial label feature selection based on noisy manifold and label distribution, which exploits feature dependency, label correlation, and instance relevance. Specifically, a linear regression function projects the feature space to the low-dimensional manifold space, which can avoid the influence of pseudo-labels affected by direct projection to the label space. The feature dependency and label correlation are obtained by manifold regularization in the feature and label space to reflect the feature significance. During optimization, instance similarity constraints variable iteration. Label distribution obtained through feature significance and instance relevance guides label space updates and reduces the impact of noise in the manifold. The effectiveness and robustness of the proposed algorithm are corroborated through experiments with three classifiers and five comparison methods on twelve datasets.}
}
@article{SHUANG2025110814,
title = {Visual primitives as words: Alignment and interaction for compositional zero-shot learning},
journal = {Pattern Recognition},
volume = {157},
pages = {110814},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110814},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400565X},
author = {Feng Shuang and Jiahuan Li and Qingbao Huang and Wenye Zhao and Dongsheng Xu and Chao Han and Haonan Cheng},
keywords = {Compositional zero-shot learning, Attribute-object composition, Vision-language model, Prompt tuning},
abstract = {Compositional Zero-Shot Learning (CZSL) aims to recognize seen and unseen attribute-object compositions. Recently, some researchers apply vision-language models to CZSL task. However, they only roughly match the image embedding and composition embedding on the image level, which can be a barrier to further improvement. With observation and analysis, we believe that a visual primitive is worth a word. To make full use of visual primitives to achieve fine-grained alignment and bridging modal gap, we propose VisPrompt for interacting visual primitives with sub-concepts in a prompt. Specifically, VisPrompt aligns the visual primitives (i.e., visual attribute and visual object) with the sub-concepts (i.e., text attribute and text object) at a fine-grained level. It consists of two steps: (1) First, we extract the visual attribute embedding by an attribute extraction module, and the visual object embedding by an object extraction module; (2) Second, we design an attribute-wise prompt, an object-wise prompt, and a visual reconstructed prompt to be encoded, where a visual primitive plays the role of corresponding sub-concept to interact. Therefore, our model is capable of applying fine-grained alignment and bridging the gap between vision and text. Sufficient experiments on widely-used MIT-States, UT-Zappos, CGQA, and VAW-CZSL datasets show that our VisPromt achieves state-of-the-art on the core metric AUC.}
}
@article{YANG2024110803,
title = {Dynamic selection for reconstructing instance-dependent noisy labels},
journal = {Pattern Recognition},
volume = {156},
pages = {110803},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110803},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005545},
author = {Jie Yang and Xiaoguang Niu and Yuanzhuo Xu and Zejun Zhang and Guangyi Guo and Steve Drew and Ruizhi Chen},
keywords = {Instance-dependent noise, Label reconstruction, Sample selection, Identity mapping},
abstract = {As an inevitable issue in annotating large-scale datasets, instance-dependent label noise (IDN) can cause serious overfitting in neural networks. To combat IDN, label reconstruction methods have been developed with noise transition matrices or DNNs to simulate the transition from clean labels to noisy labels. Nevertheless, the absence of correct supervisions will lead to learning wrong noise transitions. This motivates us to select samples with clean labels to fetch the correct supervisions. However, the difficulty in obtaining prior knowledge of the noise rate prohibits the use of existing sample selection methods. To this end, we propose a dynamic sample selection method, namely Identity Mapping (IdMap), to overcome this limitation. Inspired by the feature-dependent characteristic of IDN, we first introduce the extracted instance features and pseudo-ground-truth labels to reconstruct noisy labels. A partial identity mapping between two labels is then established and samples with consistent identity mapping output are selected as clean data to update the classifier. Extensive experiments on both artificial and real-world noisy datasets demonstrate the superiority of IdMap compared with other state-of-the-art methods.}
}
@article{WAN2024110827,
title = {Precise facial landmark detection by Dynamic Semantic Aggregation Transformer},
journal = {Pattern Recognition},
volume = {156},
pages = {110827},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110827},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005788},
author = {Jun Wan and He Liu and Yujia Wu and Zhihui Lai and Wenwen Min and Jun Liu},
keywords = {Facial landmark detection, Dynamic network, Multi-scale feature, Heavy occlusions, Heatmap regression},
abstract = {At present, deep neural network methods have played a dominant role in face alignment field. However, they generally use predefined network structures to predict landmarks, which tends to learn general features and leads to mediocre performance, e.g., they perform well on neutral samples but struggle with faces exhibiting large poses or occlusions. Moreover, they cannot effectively deal with semantic gaps and ambiguities among features at different scales, which may hinder them from learning efficient features. To address the above issues, in this paper, we propose a Dynamic Semantic-Aggregation Transformer (DSAT) for more discriminative and representative feature (i.e., specialized feature) learning. Specifically, a Dynamic Semantic-Aware (DSA) model is first proposed to partition samples into subsets and activate the specific pathways for them by estimating the semantic correlations of feature channels, making it possible to learn specialized features from each subset. Then, a novel Dynamic Semantic Specialization (DSS) model is designed to mine the homogeneous information from features at different scales for eliminating the semantic gap and ambiguities and enhancing the representation ability. Finally, by integrating the DSA model and DSS model into our proposed DSAT in both dynamic architecture and dynamic parameter manners, more specialized features can be learned for achieving more precise face alignment. It is interesting to show that harder samples can be handled by activating more feature channels. Extensive experiments on popular face alignment datasets demonstrate that our proposed DSAT outperforms state-of-the-art models in the literature. Our code is available at https://github.com/GERMINO-LiuHe/DSAT.}
}
@article{GAO2024110779,
title = {Few-shot relational triple extraction with hierarchical prototype optimization},
journal = {Pattern Recognition},
volume = {156},
pages = {110779},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110779},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005302},
author = {Chen Gao and Xuan Zhang and Zhi Jin and Weiyi Shang and Yubin Ma and Linyu Li and Zishuo Ding and Yuqin Liang},
keywords = {Few-shot learning, Relational triple extraction, Hierarchical prototype optimization, Contrastive learning, Prompt learning, Prototype network},
abstract = {Relational Triple Extraction (RTE) aims to extract relations and entities from unstructured text. Current RTE models using supervised learning require a large amount of labeled data, which presents a challenge for real-world applications. Therefore, the research work on Few-Shot Relational Triple Extraction (FS-RTE) has been proposed. However, the existing work cannot effectively construct accurate prototypes from a small number of samples, and it is difficult to model the dependencies between entities and relations, resulting in poor performance in relational triple extraction. In this paper, we propose a Hierarchical Prototype Optimized FS-RTE method (HPO). In particular, to mitigate prototype bias built on a small number of samples, HPO uses prompt learning to merge the information of relational labels into the text. Then, the entity-level prototypes are constructed using a span encoder to avoid label dependency between entity tokens. Finally, the hierarchical contrastive learning (HCL) method is introduced to improve the metric space between the prototypes of entities and relations, respectively. Experiments conducted on two public datasets show that HPO can significantly outperform previous state-of-the-art methods.}
}
@article{ZHANG2024110787,
title = {Enhanced online CAM: Single-stage weakly supervised semantic segmentation via collaborative guidance},
journal = {Pattern Recognition},
volume = {156},
pages = {110787},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110787},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005387},
author = {Bingfeng Zhang and Xuru Gao and Siyue Yu and Weifeng Liu},
keywords = {Semantic segmentation, Weakly supervised learning, CAM, Single-stage},
abstract = {Weakly supervised semantic segmentation with image-level annotations usually adopts multi-stage approaches, where high-quality offline CAM is generated as pseudo labels for further training, leading to a complex training process. Instead, current single-stage approaches, directly learning to segment objects with online CAM from image-level supervision, are more elegant. The quality of CAM critically determines the final segmentation performance. However, how to generate high-quality online CAM has not been deeply studied in existing single-stage methods. In this paper, we propose a new single-stage framework to mine more relative target features for enhanced online CAM. Specifically, we design a novel Collaborative Guidance Mechanism, where a prior guidance block uses the original CAM to produce class-specific feature representations, improving the quality of online CAM. However, such a prior is sensitive to discriminative regions of objects. Thus, we further propose a prior fusion block, in which the online segmentation prediction and the original CAM are fused to strengthen the prior guidance. Extensive experiments show that our approach achieves new state-of-the-art performance on both PASCAL VOC 2012 and MS COCO 2014 datasets, outperforming recent single-stage methods by a clear margin. Code is available at https://github.com/1rua11/CGM}
}
@article{LIU2024110860,
title = {Tensorial bipartite graph clustering based on logarithmic coupled penalty},
journal = {Pattern Recognition},
volume = {156},
pages = {110860},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110860},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006113},
author = {Chang Liu and Hongbing Zhang and Hongtao Fan and Yajing Li},
keywords = {Multi-view clustering, Bipartite graph, Logarithmic coupled penalty, Theoretical convergence},
abstract = {The graph-based multi-view clustering method has gained considerable attention in recent years. However, due to its large time complexity, it is limited to handling small-scale clustering datasets. Moreover, most existing models only consider the similarity within views and do not leverage the correlation between views and use the tensor nuclear norm (TNN) as a convex approximation to the tensor rank function. The TNN treats each singular value equally, leading to suboptimal results. To address this issue, this paper proposes a tensorial multi-view clustering model based on bipartite graphs. This paper first introduces a new non-convex logarithmic coupled penalty (LCP) function that treats different singular values differently and preserves the useful structural information required. Additionally, a tensorial bipartite graph clustering model based on logarithmic coupled penalty (LCP-TBGC) is proposed along with a corresponding solution algorithm. The paper also presents a theoretical proof that the obtained resulting sequence converges to the Karush–Kuhn–Tucker (KKT) point. Finally, to validate the effectiveness and superiority of the proposed model, experiments were conducted on eight datasets.}
}
@article{ZHAO2024110855,
title = {Multi-scale task-aware structure graph modeling for few-shot image recognition},
journal = {Pattern Recognition},
volume = {156},
pages = {110855},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110855},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400606X},
author = {Peng Zhao and Zilong Ye and Liang Wang and Huiting Liu and Xia Ji},
keywords = {Few-shot learning, Multi-scale representation, Task-aware, Graph attention network},
abstract = {The Few-shot image recognition attempts to recognize images from a novel class with only a limited number of labeled training images, which is a few-shot learning (FSL) task. FSL is very challenging. Limited labeled training samples cannot adequately represent the distribution of classes, and the base and novel classes in the training and testing stages do not intersect and have different distributions, leading to a domain shift problem in generalizing the learned model to the novel class dataset. In this paper, we propose multi-scale task-aware structure graph modeling for few-shot image recognition. We train a meta-filter learner to generate task-aware local structure filters for each scale and adaptively capture the local structures at each scale. Moreover, we introduce a novel multi-scale graph attention network (MGAT) module to model the multi-scale local structures of an image, fully exploring the correlations between different local structures at all scales of the image. Finally, we leverage the attention mechanism of graph attention network to achieve information aggregation and propagation, aiming to obtain more representative and discriminative local structure features that integrate both local and global information. We conducted comprehensive experiments on four benchmark datasets widely adopted in FSL tasks. The experimental results demonstrate that the MTSGM obtains state-of-the-art performance, which validates the effectiveness of MTSGM.}
}
@article{NIU2024110815,
title = {GR-GAN: A unified adversarial framework for single image glare removal and denoising},
journal = {Pattern Recognition},
volume = {156},
pages = {110815},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110815},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005661},
author = {Cong Niu and Ke Li and Di Wang and Wenxuan Zhu and Haojie Xu and Jinhui Dong},
keywords = {Single image glare removal, Generative adversarial network},
abstract = {In this paper, we introduce the single image glare removal (SIGR) task. SIGR aims to eliminate glare or light caused by external environment in lighting scene. To efficiently improve natural image quality and usability, many research tasks, such as image deraining and shadow removal, have been investigated a lot. However, SIGR is still underexplored. Therefore, we propose to construct a dataset and explore deep learning-based models for SIGR. Our contributions can be summarized as follows: (1) We establish a new benchmark dataset for SIGR, termed De-Glare, aiming to propel research of SIGR. This dataset comprises pairs of {glare, glare-free} images sourced from both real-world and synthetic data, utilized for training and evaluating models. (2) We conduct a comprehensive benchmarking of extensive state-of-the-art (SoTA) methods on the constructed De-Glare dataset and provide insightful analyses based on the results. (3) An innovative approach for single image glare removal employing a multi-scale generative adversarial network (GR-GAN) model is proposed. Glare images typically exhibit irregular glare shapes and cluttered backgrounds. To address these irregular glare patterns, we introduce a deformable convolution-based glare attention detector (GAD) designed to generate an attention map which specifics glare spots or rays in the input image. In pursuit of enhancing the perceptual quality of output image, GR-GAN adaptively filters out irrelevant noises and enhances salient features through a generator with cascaded pyramid neck (CPN) network. This work can provide useful insights for developing better SIGR models. Without specific tuning, our method achieves the SoTA results on multiple computer vision tasks, including the image deraining and image shadow removal.}
}
@article{SUN2024110763,
title = {Unsupervised multi-branch network with high-frequency enhancement for image dehazing},
journal = {Pattern Recognition},
volume = {156},
pages = {110763},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110763},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005144},
author = {Hang Sun and Zhiming Luo and Dong Ren and Bo Du and Laibin Chang and Jun Wan},
keywords = {Image dehazing, Physical model, Unsupervised learning, Multi-branch network, High-frequency enhancement},
abstract = {Recently, CycleGAN-based methods have been widely applied to the unsupervised image dehazing and achieved significant results. However, most existing CycleGAN-based methods ignore that the input of the generator contains two different distributions of data which can often lead to confusion in the learning process of the generator, consequently limiting the final dehazing performance. Moreover, reconstructing clear images through model architecture design and loss functions is an indirect constraint, making it difficult to compensate for the missing high-frequency information, such as textures and structures in the extracted features from hazy images. To address these issues, in this paper, we propose an Unsupervised Multi-Branch with High-Frequency Enhancement Network (UME-Net) which contain an Multi-Branch Dehazing Network (MBDN) and a High-Frequency Components Enhancement Module (HFEM). Specifically, MBDN constructs a single unsupervised dehazing network with Shared Encoding Module (SEM) and Multi-Branch Decoding Module (MDM). SEM enhance the consistency of feature representation and MDM effectively addresses the confusion during the generator learning process in CycleGAN-based methods. Furthermore, based on a key observation that hazy images and their corresponding clear images exhibit only subtle differences in high-frequency information, the HFEM is designed to compensates for the missing high-frequency information in the network which further enhances the reconstruction capability of the UME-Net for restore edge and texture information in obscured by dense haze. Experimental results on challenging benchmark datasets demonstrate the superiority of our UME-Net over SOTA unsupervised image dehazing methods. The source code is available at https://www.github.com/thislzm/UME-Net.}
}
@article{ZHOU2024110798,
title = {Semantic segmentation for large-scale point clouds based on hybrid attention and dynamic fusion},
journal = {Pattern Recognition},
volume = {156},
pages = {110798},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110798},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005491},
author = {Ce Zhou and Zhaokun Shu and Li Shi and Qiang Ling},
keywords = {Hybrid attention, Dynamic fusion, Point cloud, Semantic segmentation},
abstract = {This paper investigates the semantic segmentation problem for large-scale point clouds. Recent segmentation methods usually employ an encoder–decoder architecture. However, these methods may not effectively extract neighboring information in the encoder. Additionally, they typically use nearest neighbor interpolation and skip connections in the decoder, overlooking the semantic gap between encoder and decoder features. To resolve these issues, we propose HADF-Net, which consists of a Hybrid Attention Encoder (HAE), an Edge Dynamic Fusion module (EDF), and a Dynamic Cross-attention Decoder (DCD). HAE leverages the distinctive properties of geometric and semantic relations to aggregate local features at different stages. EDF aims to alleviate information loss during decoder upsampling by dynamically integrating the neighboring information. DCD employs an enhanced fusion mechanism with spatial-wise cross-attention to bridge the semantic gap between encoder and decoder features. Experimental results on 4 datasets demonstrate that our HADF-Net achieves superior performance.}
}
@article{HUANG2024110758,
title = {Efficient neural implicit representation for 3D human reconstruction},
journal = {Pattern Recognition},
volume = {156},
pages = {110758},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110758},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005090},
author = {Zexu Huang and Sarah Monazam Erfani and Siying Lu and Mingming Gong},
keywords = {3D reconstruction, Neural rendering, Human pose estimation, Human motion model, Neural implicit representation},
abstract = {High-fidelity digital human representations are increasingly in demand in the digital world, particularly for interactive telepresence, AR/VR, 3D graphics, and the rapidly evolving metaverse. Even though they work well in small spaces, conventional methods for reconstructing 3D human motion frequently require the use of expensive hardware and have high processing costs. This study presents HumanAvatar, an innovative approach that efficiently reconstructs precise human avatars from monocular video sources. At the core of our methodology, we integrate the pre-trained HuMoR, a model celebrated for its proficiency in human motion estimation. This is adeptly fused with the cutting-edge neural radiance field technology, Instant-NGP, and the state-of-the-art articulated model, Fast-SNARF, to enhance the reconstruction fidelity and speed. By combining these two technologies, a system is created that can render quickly and effectively while also providing estimation of human pose parameters that are unmatched in accuracy. We have enhanced our system with an advanced posture-sensitive space reduction technique, which optimally balances rendering quality with computational efficiency. In our detailed experimental analysis using both artificial and real-world monocular videos, we establish the advanced performance of our approach. HumanAvatar consistently equals or surpasses contemporary leading-edge reconstruction techniques in quality. Furthermore, it achieves these complex reconstructions in minutes, a fraction of the time typically required by existing methods. Our models achieve a training speed that is 110× faster than that of State-of-The-Art (SoTA) NeRF-based models. Our technique performs noticeably better than SoTA dynamic human NeRF methods if given an identical runtime limit. HumanAvatar can provide effective visuals after only 30 s of training. Please visit https://github.com/HZXu-526/Human-Avatar for further demo results and code.}
}
@article{XING2024110842,
title = {Deep multi-sphere support vector data description based on disentangled representation learning},
journal = {Pattern Recognition},
volume = {156},
pages = {110842},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110842},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005934},
author = {Hong-Jie Xing and Hui-Nan Wu and Ping-Ping Zhang},
keywords = {Deep support vector data description, Disentangled representation learning, Variational autoencoder, Hypersphere collapse, Anomaly detection},
abstract = {Deep support vector data description (Deep SVDD) combines deep mapping network and support vector data description (SVDD) to jointly optimize network connection weights and hypersphere volume. However, when the parameters of deep mapping network are set improperly, Deep SVDD may face the problem of hypersphere collapse, where all input data are mapped as the hypersphere center. To overcome the hypersphere collapse problem of Deep SVDD and improve the feature learning ability of deep mapping network, deep multi-sphere SVDD based on disentangled representation learning (DMSVDD-DRL) is proposed. DMSVDD-DRL consists of a variational autoencoder (VAE) and multiple hyperspheres. The feature representations obtained by VAE are disentangled into discriminative representations and generative representations that obey mixture t-distribution and Gaussian distribution, respectively. In the pre-training phase of DMSVDD-DRL, the network parameters and the hypersphere centers are initialized. In the training phase, the augmented data are added into the training set. The discriminative representations of both the input and augmented data are generated through the mapping network. Furthermore, multiple hyperspheres are constructed by the obtained discriminative representations in the feature space. Finally, the VAE loss of the input data, the reconstruction error of the augmented data, the augmentation loss between the input and augmented data, the average radius of the multiple hyperspheres, and the average distance from discriminative representations to their corresponding hypersphere centers are jointly minimized to obtain the optimal network connection weights and the multiple minimum volume hyperspheres. The effectiveness of the proposed DMSVDD-DRL is validated through the comparative and ablation experiments on the benchmark data sets. In addition, it is verified that DMSVDD-DRL is more robust against outliers in comparison with its related methods.}
}
@article{ZHOU2024110790,
title = {Meta-collaborative comparison for effective cross-domain few-shot learning},
journal = {Pattern Recognition},
volume = {156},
pages = {110790},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110790},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005417},
author = {Fei Zhou and Peng Wang and Lei Zhang and Wei Wei and Yanning Zhang},
keywords = {Cross-domain few-shot learning, Meta-learning, Deep neural network},
abstract = {Recent advancements in cross-domain few-shot learning (CD-FSL) primarily focus on learning to compare global representations between query and support images for classification. However, due to the notorious cross-domain semantic gap, the ideal global representations can be totally different across domains, thereby solely learning to compare global representations is not sufficient to achieve effective generalization in challenging cases. To mitigate this problem, we present a Meta-collaborative Comparison Network (MeCo-Net) for CD-FSL, which imitates humans to recognize unfamiliar objects through collaborative comparison on both global and local representations. Following this idea, paralleling with a conventional global comparison branch, we additionally feed random crops of both query and support images into a feature encoder to separately extract their local representations. Subsequently, we associate these local representations across images through bipartite graph matching for local comparison. Thanks to the complementary global and local comparisons, we can obtain a more generalizable classifier for CD-FSL by meta-integrating them for final prediction. Experimental results on eight benchmarks demonstrate that the proposed model generalizes to multiple target domains with state-of-the-art performance without the need for fine-tuning.}
}
@article{LEE2024110826,
title = {Explainable time series anomaly detection using masked latent generative modeling},
journal = {Pattern Recognition},
volume = {156},
pages = {110826},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110826},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005776},
author = {Daesoo Lee and Sara Malacarne and Erlend Aune},
keywords = {Time series anomaly detection (TSAD), TimeVQVAE-AD, TimeVQVAE, Masked generative modeling, Explainable AI (XAI), Explainable anomaly detection},
abstract = {We present a novel time series anomaly detection method that achieves excellent detection accuracy while offering a superior level of explainability. Our proposed method, TimeVQVAE-AD, leverages masked generative modeling adapted from the cutting-edge time series generation method known as TimeVQVAE. The prior model is trained on the discrete latent space of a time–frequency domain. Notably, the dimensional semantics of the time–frequency domain are preserved in the latent space, enabling us to compute anomaly scores across different frequency bands, which provides a better insight into the detected anomalies. Additionally, the generative nature of the prior model allows for sampling likely normal states for detected anomalies, enhancing the explainability of the detected anomalies through counterfactuals. Our experimental evaluation on the UCR Time Series Anomaly archive demonstrates that TimeVQVAE-AD significantly surpasses the existing methods in terms of detection accuracy and explainability. We provide our implementation on GitHub: https://github.com/ML4ITS/TimeVQVAE-AnomalyDetection.}
}
@article{XING2024110774,
title = {CFNet: An infrared and visible image compression fusion network},
journal = {Pattern Recognition},
volume = {156},
pages = {110774},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110774},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005259},
author = {Mengliang Xing and Gang Liu and Haojie Tang and Yao Qian and Jun Zhang},
keywords = {Image fusion, Image compression, Variational autoencoder, Transformer, Region of interest},
abstract = {Image fusion aims to acquire a more complete image representation within a limited physical space to more effectively support practical vision applications. Although the currently popular infrared and visible image fusion algorithms take practical applications into consideration. However, they did not fully consider the redundancy and transmission efficiency of image data. To address this limitation, this paper proposes a compression fusion network for infrared and visible images based on joint CNN and Transformer, termed CFNet. First of all, the idea of variational autoencoder image compression is introduced into the image fusion framework, achieving data compression while maintaining image fusion quality and reducing redundancy. Moreover, a joint CNN and Transformer network structure is proposed, which comprehensively considers the local information extracted by CNN and the global long-distance dependencies emphasized by Transformer. Finally, multi-channel loss based on region of interest is used to guide network training. Not only can color visible and infrared images be fused directly but more bits can be allocated to the foreground region of interest, resulting in a superior compression ratio. Extensive qualitative and quantitative analyses affirm that the proposed compression fusion algorithm achieves state-of-the-art performance. In particular, rate–distortion performance experiments demonstrate the great advantages of the proposed algorithm for data storage and transmission. The source code is available at https://github.com/Xiaoxing0503/CFNet.}
}
@article{GE2025110797,
title = {Iterative active learning strategies for subgraph matching},
journal = {Pattern Recognition},
volume = {158},
pages = {110797},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110797},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400548X},
author = {Yurun Ge and Dominic Yang and Andrea L. Bertozzi},
keywords = {Subgraph matching, Active learning, Multiplex networks},
abstract = {The subgraph matching problem arises in a variety of domains including pattern recognition for segmented images, meshes of 3D objects, biochemical reactions, and security applications. Large and complex solution spaces are common for this graph-based problem, especially when the world graph contains many more nodes and edges in comparison to the template graph. Researchers have focused on the task of finding one match or many matches, however a real use-case scenario can necessitate identifying specific matches from a combinatorially complex solution space. Our work directly addresses this challenge. We propose to introduce additional queries to the subgraph that iteratively reduce the size of the solution space, and consider the optimal strategy for doing so. We formalize this problem and demonstrate that it is NP-complete. We compare different quantitative criteria for choosing nodes to query. We introduce a new method based on a spanning tree that outperforms other graph-based criteria for the multichannel datasets. Finally, we present numerical experiments for single channel and multichannel subgraph matching problems created from both synthetic and real world datasets.}
}
@article{YAN2024110802,
title = {Uncertainty estimation in HDR imaging with Bayesian neural networks},
journal = {Pattern Recognition},
volume = {156},
pages = {110802},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110802},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005533},
author = {Qingsen Yan and Haishen Wang and Yifan Ma and Yuhang Liu and Wei Dong and Marcin Woźniak and Yanning Zhang},
keywords = {High dynamic range imaging, Uncertainty, Deep learning, Semantic information, Exposure fusion, Ghosting artifacts},
abstract = {The goal of high dynamic range (HDR) imaging is to estimate potential high-quality images from multi-exposed low dynamic range (LDR) inputs. Intuitively, there exist various possible HDR images corresponding to given LDR inputs, which results in uncertainty in the estimated results. However, most existing HDR imaging methods employ l1 or l2 loss only to provide one possible estimation from various possible solutions, which fails to model the uncertainty, and thus lacks high-frequency details. In this work, we design Bayesian neural networks to capture the uncertainty, which can model one-to-many relations and provide various possible solutions. Concretely, we propose a Variational Bayesian Layer by leveraging a hierarchical prior on the network weights and inferring a new joint posterior, which is utilized to model uncertainty in high-frequency details (e.g., textures), and model uncertainty in semantic information (e.g., ghost areas), respectively. By leveraging Bayesian framework, the proposed method can provide various potential high-quality estimations, especially in high-frequency details. Experiments on different datasets show that the proposed method enables sampling possible HDR imaging, and the consensus estimate achieves state-of-the-art quantitative and qualitative results.}
}
@article{CHENG2025110859,
title = {DMANet: Dual-modality alignment network for visible–infrared person re-identification},
journal = {Pattern Recognition},
volume = {157},
pages = {110859},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110859},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006101},
author = {Xu Cheng and Shuya Deng and Hao Yu and Guoying Zhao},
keywords = {Visible–infrared person re-identification, Transformer, Attention mechanism, Modality alignment},
abstract = {Visible–infrared person re-identification (VI-ReID) is a challenging retrieval task, which aims to match the same pedestrian between visible and infrared modalities. Most existing works achieve performance gains by solving the problem of the inherent cross-modality discrepancies. However, they cannot fully mine the modality information and lead to a poor generalization. In addition, the pedestrian images are unable to align well due to the large inter- and intra- class variations. To tackle the above limitations, we propose a novel dual-modality alignment network (DMANet) for VI-ReID. The core idea of our work is to develop multi-granularity features mutual learning (MGFML) for inadequate perception of modalities information, and to solve modality difference by proposing inter- and intra- modality alignment module (IIMA). Specifically, firstly, an effective multi-granularity features mutual learning module is proposed to mine the multi-granularity features, which combines the domain alignment and self-distillation to relieve modality discrepancy. Further, the maximum mean discrepancy loss and mutual learning loss are presented to enhance the identity-aware ability of the DMANet. Secondly, an effective inter- and intra- modality alignment module is presented to explore the potential alignment relation of inter- and intra- modalities. Finally, joint learning mechanism of multi-granularity features and modality alignment is utilized to improve the VI-ReID accuracy. Extensive experiments on mainstream benchmarks demonstrate that our method is superior to the state-of-the-art methods.}
}
@article{ANGARANO2024110762,
title = {Back-to-Bones: Rediscovering the role of backbones in domain generalization},
journal = {Pattern Recognition},
volume = {156},
pages = {110762},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110762},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005132},
author = {Simone Angarano and Mauro Martini and Francesco Salvetti and Vittorio Mazzia and Marcello Chiaberge},
keywords = {Deep learning, Computer vision, Image classification, Domain generalization, Backbone},
abstract = {Domain Generalization (DG) studies the capability of a deep learning model to generalize to out-of-training distributions. In the last decade, literature has been massively filled with training methodologies that claim to obtain more abstract and robust data representations to tackle domain shifts. Recent research has provided a reproducible benchmark for DG, pointing out the effectiveness of naive empirical risk minimization (ERM) over existing algorithms. Nevertheless, researchers persist in using the same outdated feature extractors, and little to no attention has been given to the effects of different backbones yet. In this paper, we go “back to the backbones”, proposing a comprehensive analysis of their intrinsic generalization capabilities, which so far have been overlooked by the research community. We evaluate a wide variety of feature extractors, from standard residual solutions to transformer-based architectures, finding an evident linear correlation between large-scale single-domain classification accuracy and DG capability. Our extensive experimentation shows that by adopting competitive backbones in conjunction with effective data augmentation, plain ERM outperforms recent DG solutions and achieves state-of-the-art accuracy. Moreover, our additional qualitative studies reveal that novel backbones give more similar representations to same-class samples, separating different domains in the feature space. This boost in generalization capabilities leaves marginal room for DG algorithms. It suggests a new paradigm for investigating the problem, placing backbones in the spotlight and encouraging the development of consistent algorithms on top of them. The code is available at https://github.com/PIC4SeR/Back-to-Bones.}
}
@article{LEE2024110838,
title = {Discovering an inference recipe for weakly-supervised object localization},
journal = {Pattern Recognition},
volume = {156},
pages = {110838},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110838},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005892},
author = {Sanghuk Lee and Cheolhyun Mun and Youngjung Uh and Junsuk Choe and Hyeran Byun},
keywords = {Object localization, Weakly-supervised learning, Semantic segmentation, Test-time processing, Binarization},
abstract = {Most existing weakly-supervised object localization (WSOL) methods have improved training procedures for better localization performance. However, the inference procedure has been overlooked. We observe that the useful information for localization is missed by the current inference practice of WSOL. To address this limitation, we propose a new test-time ingredient for WSOL: binarizing the penultimate feature map and their corresponding weights of the last linear layer. With this simple remedy, the proposed method consistently improves the localization performance of the existing training methods for WSOL. Extensive evaluation including with three different backbone networks on three different WSOL benchmarks validates its effectiveness. In addition, we demonstrate our method is also able to improve weakly-supervised semantic segmentation performances on PASCAL VOC dataset. Lastly, since our method is only applied during the testing phase, our performance gain comes with negligible computational overheads.}
}
@article{LI2024110786,
title = {STNet: Structure and texture-guided network for image inpainting},
journal = {Pattern Recognition},
volume = {156},
pages = {110786},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110786},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005375},
author = {Zhan Li and Yanan Zhang and Yingfei Du and Xiaofeng Wang and Chao Wen and Yongqin Zhang and Guohua Geng and Fan Jia},
keywords = {Image inpainting, Generative adversarial network, Structure and texture, Ancient murals},
abstract = {Generative Adversarial Network (GAN) has made great progress in image inpainting due to its strong generation capability. However, the previous methods based on GAN cannot understand the structure and texture of damaged images simultaneously, which leads to inconsistent structure and blurred details of inpainting results. To address this issue, we propose a Structure and Texture-guided Network for image inpainting (STNet), which consists of three networks, i.e. structure reconstruction network, texture reconstruction network, and image refinement network. STNet can restore the coherent structure and fine textures of damaged images in the first two networks, and fuse them to complete image inpainting in the last network. Experiments on three standard datasets CelebA, Places2, and Pairs StreetView show STNet is effective and outperforms other related methods. In particular, we also conducted experiments on the damaged images from tomb murals of Tang Dynasty in China, whose results also verify that our method can be applied to the restoration of historical relics. Our code is available at https://github.com/nwuAI/STNet.}
}
@article{XIE2024110777,
title = {Multi-query and multi-level enhanced network for semantic segmentation},
journal = {Pattern Recognition},
volume = {156},
pages = {110777},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110777},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005284},
author = {Bin Xie and Jiale Cao and Rao Muhammad Anwer and Jin Xie and Jing Nie and Aiping Yang and Yanwei Pang},
keywords = {Semantic segmentation, Transformer, Multi-query, Multi-level},
abstract = {Plain transformer-based methods have achieved promising performance on semantic segmentation recently. These methods adopt a single set of class queries to predict masks of different semantic categories based on multi-level feature maps. We argue that this single-query design cannot fully exploit diverse information of different levels for improved semantic segmentation. To address this issue, we propose a multi-query and multi-level enhanced network for semantic segmentation (named QLSeg). Our QLSeg first performs multi-level feature enhancement on plain transformer to improve feature discriminability. Afterwards, we introduce multi-query decoder to respectively extract feature embeddings and predict mask logits at different levels, where feature embeddings are adaptively merged for classification and mask logits are summed for output masks. In addition, we introduce masked attention-to-mask to focus on local regions with the same class. We perform the experiments on three widely-used semantic segmentation datasets: ADE20K, COCO-Stuff-10K, and PASCAL-Context. Our proposed QLSeg achieves competitive results on all these three datasets.}
}
@article{LU2024110753,
title = {Multi-view hypergraph regularized Lp norm least squares twin support vector machines for semi-supervised learning},
journal = {Pattern Recognition},
volume = {156},
pages = {110753},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110753},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005041},
author = {Junqi Lu and Xijiong Xie and Yujie Xiong},
keywords = {Multi-view semi-supervised learning, Twin support vector machines, Lp norm graph regularization, Hypergraph regularized},
abstract = {In recent years, multi-view semi-supervised learning has gradually become a popular research direction. The classic binary classification methods in this field are multi-view Laplacian support vector machines (MvLapSVM) and multi-view Laplacian twin support vector machines (MvLapTSVM), which extend semi-supervised support vector machine to multi-view learning. Nevertheless, similar to the majority of SVM-based multi-view methods, the above methods are two-view methods that cannot fully leverage the information from all views and are constructed based on the L2 norm. Additionally, in semi-supervised graph learning, the quality of the graph often has a significant impact on the results. Therefore, we propose a novel multi-view hypergraph regularized Lp norm least squares twin support vector machines (MvHGLpLSTSVM) that can handle general multi-view data for semi-supervised learning. It extends hypergraph learning to multi-view learning and combines Lp norm to further explore the manifold structure and embedded geometric information of multi-view data. By using equality constraints, we design a simple and effective iterative algorithm. In the classification of six multi-view datasets, we compare the proposed method with some other state-of-the-art methods, and the results show that the proposed method is effective.}
}
@article{YANG2024110858,
title = {PointSurFace: Discriminative point cloud surface feature extraction for 3D face recognition},
journal = {Pattern Recognition},
volume = {156},
pages = {110858},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110858},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006095},
author = {Junpeng Yang and Qiufu Li and Linlin Shen},
keywords = {3D face recognition, Point cloud, Surface feature extraction},
abstract = {Due to the geometric information in the 3D face data, the 3D face recognition methods exhibit better robustness against the physical attacks compared to the 2D recognition methods. In this paper, we propose PointSurFace, a 3D face point cloud recognition method integrated with a specially designed facial surface feature extraction (SurF) module. In PointSurFace, SurF comprehensively combines the coordinate positions, normal vectors, and angle relationships, etc., of each patch in the 3D face point cloud, to explore the explicit local geometric information of 3D faces and generate facial surface features. Subsequently, the surface features are fed into an encoder consisting of four set abstraction (SA) modules and three Inverted Residual MLP (InvResMLP) modules to extract more discriminative 3D facial features, where InvResMLP could suppress the overfitting and reduce information loss in the model training. In addition, PointSurFace utilizes channel fusion operations to integrate position information into the extracted 3D facial features, further enhancing the model performance. Experimental results in 3D facial recognition and verification across multiple datasets demonstrate that PointSurFace achieves the best 3D face recognition performance to date. For example, on Lock3DFace, it achieves the state-of-the-art recognition and verification accuracy of 90.03% and 80.01%, respectively, surpassing the previous methods by 2.02% and 3.19%. The ablation studies evaluate each module of PointSurFace, suggesting that the proposed surface feature extraction module significantly enhances the model performance.}
}
@article{XIAN2025110821,
title = {Distilling consistent relations for multi-source domain adaptive person re-identification},
journal = {Pattern Recognition},
volume = {157},
pages = {110821},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110821},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005727},
author = {Yuqiao Xian and Yi-Xing Peng and Xing Sun and Wei-Shi Zheng},
keywords = {Person re-identification, Multiple domains, Visual surveillance},
abstract = {Although unsupervised person re-identification has made great progress by employing domain adaptation, existing methods rarely make use of multiple source datasets. These works neglect a practical scenario where annotated data from multiple domains are available. To address this issue, we propose a novel method to exploit the diversity and consistency of multiple sources to improve domain adaptation for person re-identification. In the proposed method, multiple expert models pre-trained in different source domains are adapted to the target domain by self-training with expert-specific clustering-based pseudo labels. To improve the consistency of multiple experts, we transfer knowledge between experts through dual similarity distillation between experts. Finally, to further encourage experts to learn complementary features, we maintain the feature diversity of multiple experts by representation decorrelation, preventing them from being too similar. Extensive experiments conducted on three benchmarks, Market-1501, DukeMTMC-reID, and MSMT17, demonstrate the effectiveness of the proposed method.}
}
@article{SUN2024110805,
title = {Few-shot classification with Fork Attention Adapter},
journal = {Pattern Recognition},
volume = {156},
pages = {110805},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110805},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005569},
author = {Jieqi Sun and Jian Li},
keywords = {Few-shot Classification, Meta-Learning, Attention mechanism, Dense feature similarity},
abstract = {Few-shot learning aims to transfer the knowledge learned from seen categories to unseen categories with a few references. It is also an essential challenge to bridge the gap between humans and deep learning models in real-world applications. Despite extensive previous efforts to tackle this problem by finding an appropriate similarity function, we emphasize that most existing methods have merely considered a single low-resolution representation pair utilized in similarity calculations between support and query samples. Such representational limitations could induce the instability of category predictions. To better achieve metric learning stabilities, we present a novel method dubbed Fork Attention Adapter (FA-adapter), which can seamlessly establish the dense feature similarity with the newly generated nuanced features. The utility of the proposed method is more performant and efficient via the two-stage training phase. Extensive experiments demonstrate consistent and substantial accuracy gains on the fine-grained CUB, Aircraft, non-fine-grained mini-ImageNet, and tiered-ImageNet benchmarks. By comprehensively studying and visualizing the learned knowledge from different source domains, we further present an extension version termed FA-adapter++ to boost the performance in fine-grained scenarios.}
}
@article{LUO2025110828,
title = {Toward real text manipulation detection: New dataset and new solution},
journal = {Pattern Recognition},
volume = {157},
pages = {110828},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110828},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400579X},
author = {Dongliang Luo and Yuliang Liu and Rui Yang and Xianjin Liu and Jishen Zeng and Yu Zhou and Xiang Bai},
keywords = {Tampered text detection, Document forgery, Image forensics, Image manipulation detection, Multi-modal},
abstract = {With the surge in realistic text tampering, detecting fraudulent text in images has gained prominence for maintaining information security. However, the high costs associated with professional text manipulation and annotation limit the availability of real-world datasets. With most relying on synthetic tampering, they inadequately replicate real-world tampering attributes. To address this issue, we present the Real Text Manipulation (RTM) dataset, encompassing 9,000 text images, which include 6,000 manually tampered images, created using a variety of techniques, alongside 3,000 unaltered text images for evaluating solution stability. Our evaluations indicate that existing methods falter in text forgery detection on the RTM dataset. We propose a robust baseline solution featuring a Consistency-aware Aggregation Hub and a Gated Cross Neighborhood-attention Fusion module for efficient multi-modal information fusion, supplemented by a Tampered-Authentic Contrastive Learning module during training, enriching feature representation distinction. This framework, extendable to other dual-stream architectures, demonstrated notable localization performance improvements of 3.99% and 5.76% on IoU and F1-measure, respectively. Our contributions aim to propel advancements in real-world text tampering detection. Code and dataset will be made available at https://github.com/DrLuo/RTM.}
}
@article{CHENG2024110809,
title = {Vision-language pre-training via modal interaction},
journal = {Pattern Recognition},
volume = {156},
pages = {110809},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110809},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005600},
author = {Hang Cheng and Hehui Ye and Xiaofei Zhou and Ximeng Liu and Fei Chen and Meiqing Wang},
keywords = {Cross-modal, Pre-training, Partial auxiliary, Image captioning},
abstract = {Existing vision-language pre-training models typically extract region features and conduct fine-grained local alignment based on masked image/text completion or object detection methods. However, these models often design independent subtasks for different modalities, which may not adequately leverage interactions between modalities, requiring large datasets to achieve optimal performance. To address these limitations, this paper introduces a novel pre-training approach that facilitates fine-grained vision-language interaction. We propose two new subtasks — image filling and text filling — that utilize data from one modality to complete missing parts in another, enhancing the model’s ability to integrate multi-modal information. A selector mechanism is also developed to minimize semantic overlap between modalities, thereby improving the efficiency and effectiveness of the pre-trained model. Our comprehensive experimental results demonstrate that our approach not only fosters better semantic associations among different modalities but also achieves state-of-the-art performance on downstream vision-language tasks with significantly smaller datasets.}
}
@article{HUANG2024110781,
title = {Feature-semantic augmentation network for few-shot open-set recognition},
journal = {Pattern Recognition},
volume = {156},
pages = {110781},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110781},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005326},
author = {Xilang Huang and Seon Han Choi},
keywords = {Few-shot learning, Open-set recognition, Image classification, Semantic knowledge},
abstract = {Few-shot open-set recognition (FSOR) represents a relatively underexplored area of research. The primary challenge encountered by FSOR methods lies in recognizing known classes while simultaneously rejecting unknown classes utilizing only limited samples. Current FSOR methods predominantly rely on the visual information extracted from images to establish class representations, aiming to derive distinguishable classification scores for both known and unknown classes. However, these methods often overlook the benefits of leveraging semantic information derived from class names associated with images, which could provide valuable auxiliary learning insights. This study introduces a feature-semantic augmentation network to improve FSOR performance utilizing multimodal information. Specifically, we augment the class-specific features of closed-set prototypes by integrating visual and textual features from known class names across both local and global feature spaces. To facilitate prototype learning, We introduce a refinement and fusion module. Among these, the former leverages the similarity between prototype and target features at both channel and spatial dimensions to calibrate targets relative to their relevant prototypes. Meanwhile, the latter employs additional classification targets generated by the fusion module to provide learning sources from different classes. Experimental results on various few-shot learning benchmarks show that the proposed method significantly outperforms current state-of-the-art methods across both closed- and open-set scenarios.}
}
@article{MA2024110793,
title = {Source-free domain adaptation via dynamic pseudo labeling and Self-supervision},
journal = {Pattern Recognition},
volume = {156},
pages = {110793},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110793},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005442},
author = {Qiankun Ma and Jie Zeng and Jianjia Zhang and Chen Zu and Xi Wu and Jiliu Zhou and Jie Chen and Yan Wang},
keywords = {Source-free domain adaptation, Self-supervised learning, Image classification},
abstract = {Recently, unsupervised domain adaptation (UDA) has attracted extensive interest in relieving the greedy requirement of vanilla deep learning for labeled data. It seeks for a solution to adapt the knowledge from a well-labeled training dataset (source domain) to another unlabeled target dataset (target domain). However, in some practical scenarios, the source domain data is inaccessible for a variety of reasons, and only a model trained on it can be provided, thus deriving a more challenging task, i.e., source-free unsupervised domain adaptation (SFUDA). Some pseudo labeling-based methods have been proposed to solve it by predicting pseudo labels for the unlabeled target domain data. Nevertheless, incorrectly designated pseudo labels will impose an adverse impact on the network adaptation. To alleviate this issue, we propose a dynamic confidence-based pseudo labeling strategy for SFUDA in this paper. Unlike those methods that first rigidly assign pseudo labels to all target domain data and then try to weaken the effect of incorrect pseudo labels in training, we proactively label the target samples with higher confidence in a dynamic manner. To further relieve the impact of incorrect pseudo labels, we harness the collaborative learning to constrain the consistency of the network and impose an additional soft supervision. Besides, we also investigate the possible problem brought by our labeling strategy, i.e., the neglect of wavering samples near the decision boundary, and solve it by injecting the self-supervised learning into our model. Experiments on three UDA benchmark datasets demonstrate the state-of-the-art performance of our proposed method. The code is publicly available at https://github.com/meowpass/DPLS.}
}
@article{FLABOREA2024110817,
title = {Contracting skeletal kinematics for human-related video anomaly detection},
journal = {Pattern Recognition},
volume = {156},
pages = {110817},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110817},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005685},
author = {Alessandro Flaborea and Guido Maria {D’Amely di Melendugno} and Stefano D’Arrigo and Marco Aurelio Sterpa and Alessio Sampieri and Fabio Galasso},
keywords = {Anomaly detection, Open set recognition, Hyperbolic geometry, Kinematic skeleton, Graph convolutional networks},
abstract = {Detecting the anomaly of human behavior is paramount to timely recognizing endangering situations, such as street fights or elderly falls. However, anomaly detection is complex since anomalous events are rare and because it is an open set recognition task, i.e., what is anomalous at inference has not been observed at training. We propose COSKAD, a novel model that encodes skeletal human motion by a graph convolutional network and learns to COntract SKeletal kinematic embeddings onto a latent hypersphere of minimum volume for Video Anomaly Detection. We propose three latent spaces: the commonly-adopted Euclidean and the novel spherical and hyperbolic. All variants outperform the state-of-the-art on the most recent UBnormal dataset, for which we contribute a human-related version with annotated skeletons. COSKAD sets a new state-of-the-art on the human-related versions of ShanghaiTech Campus and CUHK Avenue, with performance comparable to video-based methods. Source code and dataset will be released upon acceptance.}
}
@article{AHISHALI2024110765,
title = {R2C-GAN: Restore-to-Classify Generative Adversarial Networks for blind X-ray restoration and COVID-19 classification},
journal = {Pattern Recognition},
volume = {156},
pages = {110765},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110765},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005168},
author = {Mete Ahishali and Aysen Degerli and Serkan Kiranyaz and Tahir Hamid and Rashid Mazhar and Moncef Gabbouj},
keywords = {COVID-19 classification, Generative adversarial networks, Machine learning, X-ray images, X-ray image restoration},
abstract = {Restoration of poor-quality medical images with a blended set of artifacts plays a vital role in a reliable diagnosis. As a pioneer study in blind X-ray restoration, we propose a joint model for generic image restoration and classification: Restore-to-Classify Generative Adversarial Networks (R2C-GANs). This is the first generic restoration approach forming an Image-to-Image translation task from poor-quality having noisy, blurry, or over/under-exposed images to high-quality image domain where forward and inverse transformations are learned using unpaired training samples. Simultaneously, the joint classification preserves the diagnostic-related label during restoration. Each R2C-GAN is equipped with operational layers/neurons in a compact architecture. The proposed joint model successfully restores images while achieving state-of-the-art Coronavirus Disease 2019 (COVID-19) classification with above 90% in F1-Score. In qualitative analysis, the restoration performance is confirmed by medical doctors where 68% of the restored images are selected against the original images. We share the software implementation at https://github.com/meteahishali/R2C-GAN.}
}
@article{WANG2024110776,
title = {LightCM-PNet: A lightweight pyramid network for real-time prostate segmentation in transrectal ultrasound},
journal = {Pattern Recognition},
volume = {156},
pages = {110776},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110776},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005272},
author = {Weirong Wang and Bo Pan and Yue Ai and Gonghui Li and Yili Fu and Yanjie Liu},
keywords = {Robotic prostate biopsy, Real-time segmentation, Ultrasound image, Tokenized MLP, Pyramid network},
abstract = {The accurate and real-time automatic segmentations of prostate transrectal ultrasound (TRUS) images enable the fusion of magnetic resonance imaging (MRI) and TRUS image to guide robotic prostate biopsy systems. This segmentation, applied to intraoperative TRUS images, is crucial for accurate image registration and automatic localization of the biopsy target. However, the blurred imaging and uneven intensity distribution in TRUS make accurate prostate segmentation still challenging. Most deep learning-based image segmentation methods, like convolutional neural network (CNN) and transformer, are often characterized by large parameters, computational complexity, and slow inference speed. Therefore, we propose a lightweight and accurate segmentation network. We combine CNN and tokenized multilayer perceptron (MLP) as the backbone of feature extraction, and build a pyramid structure network for feature encoding and decoding. This structure can effectively reduce the number of parameters and computational complexity, and effectively use global context information. Additionally, we introduce an interactive hybrid attention module that sequentially derives attention maps along the channel and spatial dimensions. The attention module interacts with the encoded to focus on the prostate region in the output features. We then incorporate a feature fusion module to construct reverse attention features, which enhances areas with weak responses in the foreground. To further improve the accuracy and smoothness of the segmentation result, we include a boundary constraint term in the loss function. Experimental results demonstrate that the proposed network achieves better performance in prostate TRUS image segmentation, with fewer parameters and faster inference speed.}
}
@article{LU2024110857,
title = {Heterogeneous domain adaptation via incremental discriminative knowledge consistency},
journal = {Pattern Recognition},
volume = {156},
pages = {110857},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110857},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006083},
author = {Yuwu Lu and Dewei Lin and Jiajun Wen and Linlin Shen and Xuelong Li and Zhenkun Wen},
keywords = {Heterogeneous domain adaptation, Distribution alignment, Pseudo labeling, Feature correlation, Transfer learning},
abstract = {Heterogeneous domain adaptation is a challenging problem in transfer learning since samples from the source and target domains reside in different feature spaces with different feature dimensions. The key problem is how to minimize some gaps (e.g., data distribution mismatch) presented in two heterogeneous domains and produce highly discriminative representations for the target domain. In this paper, we attempt to address these challenges with the proposed incremental discriminative knowledge consistency (IDKC) method, which integrates cross-domain mapping, distribution matching, discriminative knowledge preservation, and domain-specific geometry structure consistency into a unified learning model. Specifically, we attempt to learn a domain-specific projection to project original samples into a common subspace in which the marginal distribution is well aligned and the discriminative knowledge consistency is preserved by leveraging the labeled samples from both domains. Moreover, domain-specific structure consistency is enforced to preserve the data manifold from the original space to the common feature space in each domain. Meanwhile, we further apply pseudo labeling to unlabeled target samples based on the feature correlation and retain pseudo labels with high feature correlation coefficients for the next iterative learning. Our pseudo-labeling strategy expands the number of labeled target samples in each category and thus enforces class-discriminative knowledge consistency to produce more discriminative feature representations for the target domain. Extensive experiments on several standard benchmarks for object recognition, cross-language text classification, and digit classification tasks verify the effectiveness of our method.}
}
@article{CHEN2024110844,
title = {SiSe: Simultaneous and Sequential Transformers for multi-label activity recognition},
journal = {Pattern Recognition},
volume = {156},
pages = {110844},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110844},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005958},
author = {Zhao-Min Chen and Xin Jin and Sixian Chan},
keywords = {Multi-label, Activity recognition, Sequential transformer, Hierarchical structure},
abstract = {Multi-label activity recognition is extremely challenging, where multiple activities may appear simultaneously or sequentially in a video. While previous works have realized the temporal co-occurrence of activities, the sequential order of activities have been largely overlooked. However, we argue that the sequential order of activities should also be preserved in correlation modeling, because shuffling the order might not form a semantically meaningful video. In this work, we present plug-and-play Simultaneous and Sequential Transformer (SiSe) modules for multi-label activity recognition. Upon frame features of all time steps, SiSe enhances spatio-temporal feature learning for multi-label activity recognition, by capturing the simultaneous and sequential activity correlations. Specifically, we employ a Simultaneous Transformer module to connect multiple activities that probably appear at each frame, and a hierarchical Sequential Transformer module to efficiently capture the sequential activity correlations in an order-preserved manner. Despite the straightforward and class-agnostic design of SiSe, it can outperform state-of-the-art approaches on three multi-label activity recognition benchmarks. In particular, we verify the significance of preserving the sequential order of activities with our Sequential Transformer in correlation modeling. We also conduct ablation studies and visual analysis for better understanding of our SiSe.}
}
@article{PAN2024110820,
title = {Category-agnostic semantic edge detection by measuring neural representation randomness},
journal = {Pattern Recognition},
volume = {156},
pages = {110820},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110820},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005715},
author = {Zhiyi Pan and Peng Jiang and Qiong Zeng and Ge Li and Changhe Tu},
keywords = {Edge detection, Category-agnostic edge, Randomness metric},
abstract = {Edge detection plays a fundamental role in computer vision tasks and gains wide applications. In particular, semantic edge detection recently draws more attention due to the high demand for a fine-grained understanding of visual scenes. However, detecting high-level semantic edges hidden in visual scenes is quite challenging. Existing semantic edge detection methods focus on category-aware semantic edges and require elaborate category annotations. Instead, we first propose the category-agnostic semantic edge detection task without additional semantic category annotations. To achieve this goal, we propose to utilize only edge position annotations and leverage the information randomness of semantic edges. Specifically, we align semantic edge positions to the ground truth by maximizing randomness on edge regions and minimizing randomness on non-edge regions in the training process. In the inference process, we first obtain neural representations by the trained network, and then generate semantic edges by measuring neural randomness. We evaluate our method by comparisons with alternative methods on two well-known datasets: Cityscapes (Cordts et al., 2016) and SBD (Hariharan et al., 2014). The results demonstrate our superiority over the alternatives, which is more significant under weak annotations. We also provide comprehensive mechanism studies to verify the generalizability, rationality, and validity of our working mechanism.}
}
@article{KANG2024110849,
title = {Fine-grained recognition via submodular optimization regulated progressive training},
journal = {Pattern Recognition},
volume = {156},
pages = {110849},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110849},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006009},
author = {Bin Kang and Songlin Du and Dong Liang and Fan Wu and Xin Li},
keywords = {Fine-grained recognition, Progressive training, Submodular optimization},
abstract = {Progressive training has unfolded its superiority on a wide range of downstream tasks. However, it may fail in fine-grained recognition (FGR) due to special challenges with high intra-class and low inter-class variances. In this paper, we propose an active self-pace learning method to exploit the full potential of progressive training strategy in FGR. The key innovation of our design is to integrate submodular optimization and self-pace learning into a maximum–minimum optimization framework. The submodular optimization is regarded as a dynamic regularization to select active sample groups in each training round for restricting the search space of self-pace optimization. This can overcome the limitation of traditional self-pace learning that is easily trapped into local minimums when facing challenging samples. Extensive experiments on three public FGR datasets show that the proposed method can win at least 1.5% performance gain in various kinds of network backbones including swin-transformer.}
}
@article{LI2024110780,
title = {CarvingNet: Point cloud completion by stepwise refining multi-resolution features},
journal = {Pattern Recognition},
volume = {156},
pages = {110780},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110780},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005314},
author = {Liangliang Li and Guihua Liu and Feng Xu and Lei Deng},
keywords = {3D computer vision, Point cloud completion, Transformer, U-Net},
abstract = {In the field of 3D vision, 3D point cloud completion is a crucial task in many practical applications. Current methods use Transformer's Encoder-Decoder framework to predict the missing part of the point cloud features at low resolution, which does not fully utilize the feature information at multiple resolutions and can result in the loss of the object's geometric details. In this paper, we present a novel point cloud completion method, CarvingNet, which, to the best of our knowledge, is the first to apply the U-Net architecture to the point cloud completion task by operating directly on unordered point cloud features at multiple resolutions. Firstly, we gradually expand the receptive field and use cross-attention to purify the features of the missing part of the point cloud at each resolution and to generate the contour features of the complete point cloud at the last obtained resolution. Then, we gradually reduce the receptive field and use cross-attention to refine the features of the complete point cloud at each resolution and to generate the features of the complete point cloud with rich details at the last obtained resolution. To obtain point cloud features at different resolutions, we specifically design the up-sampling module and down-sampling module for disordered point cloud features. Furthermore, we improve the FoldingNet network to make it more suitable for generating high-quality dense point clouds. The experimental results demonstrate that our proposed CarvingNet achieves the performance of existing state-of-the-art methods on the ShapeNet-55, ShapeNet-34, and KITTI benchmarks.}
}
@article{ALTABRAWEE2024110804,
title = {Repeat and learn: Self-supervised visual representations learning by Repeated Scene Localization},
journal = {Pattern Recognition},
volume = {156},
pages = {110804},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110804},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005557},
author = {Hussein Altabrawee and Mohd Halim {Mohd Noor}},
keywords = {Visual representations learning, Action recognition, Self-supervised learning},
abstract = {Large labeled datasets are crucial for video understanding progress. However, the labeling process is time-consuming, expensive, and tiresome. To overcome this impediment, various pretexts use the temporal coherence in videos to learn visual representations in a self-supervised manner. However, these pretexts (order verification and sequence sorting) struggle when encountering cyclic actions due to the label ambiguity problem. To overcome these limitations, we present a novel temporal pretext task to address self-supervised learning of visual representations from unlabeled videos. Repeated Scene Localization (RSL) is a multi-class classification pretext that involves changing the temporal order of the frames in a video by repeating a scene. Then, the network is trained to identify the modified video, localize the location of the repeated scene, and identify the unmodified original videos that do not have repeated scenes. We evaluated the proposed pretext on two benchmark datasets, UCF-101 and HMDB-51. The experimental results show that the proposed pretext achieves state-of-the-art results in action recognition and video retrieval tasks. In action recognition, our S3D model achieves 88.15% and 56.86% on UCF-101 and HMDB-51, respectively. It outperforms the current state-of-the-art by 1.05% and 3.26%. Our R(2+1)D-Adjacent model achieves 83.52% and 54.50% on UCF-101 and HMDB-51, respectively. It outperforms the single pretext tasks by 8.7% and 13.9%. In video retrieval, our R(2+1)D-Offset model outperforms the single pretext tasks by 4.68% and 1.1% Top 1 accuracies on UCF-101 and HMDB-51, respectively. The source code and the trained models are publicly available at https://github.com/Hussein-A-Hassan/RSL-Pretext.}
}
@article{DONG2024110861,
title = {Cluster prototype earth mover’s distance adapters and alignment-guided prompt learning for vision–language models},
journal = {Pattern Recognition},
volume = {156},
pages = {110861},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110861},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006125},
author = {Mengping Dong and Fei Li and Zhenbo Li and Xue Liu},
keywords = {Cluster prototype, Earth mover’s distance, Adapter, Prompt learning, Vision–language models},
abstract = {Vision–language pre-trained models have focused on fine-tuning methods to enhance generalization in downstream tasks, such as CLIP. Recent research proposes fine-tuning these models using prompting and adapter techniques. However, prompting methods tend to overfit class-specific data distributions, and most adapters ignore the prototype representations as the weights, resulting in poor generalization. To tackle these challenges, we propose a novel method that integrates the cluster prototype Earth Mover’s Distance adapters and alignment-guided prompt learning (CPAAP) for vision–language models. Specifically, our adapter designs three components: graph convolutional network-based clusters, hierarchy prototype representations, and Earth Mover’s Distance similarity, providing a robust metric for measuring feature representations. Additionally, alignment-guided prompt learning enforces a constraint in the prediction of trainable and frozen models, enabling effective adaptation to downstream tasks. Extensive experiments are conducted on few-shot classification, base-to-novel generalization, cross-dataset evaluation, and domain generalization. In practice, CPAAP outperforms state-of-the-art methods on zero-shot tasks, achieving an absolute gain of 0.46% on the harmonic mean across 11 popular datasets.}
}
@article{LONG2025110816,
title = {LORE++: Logical location regression network for table structure recognition with pre-training},
journal = {Pattern Recognition},
volume = {157},
pages = {110816},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110816},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005673},
author = {Rujiao Long and Hangdi Xing and Zhibo Yang and Qi Zheng and Zhi Yu and Fei Huang and Cong Yao},
keywords = {Visual document understanding, Table structure recognition, OCR},
abstract = {Table structure recognition (TSR) aims at extracting tables in images into machine-understandable formats. Current approaches address this issue by either predicting the adjacency of detected cells or direct generation of structural sequences. Nonetheless, these approaches either count on additional heuristic rules for post-processing, or involve the generation of extremely long-range sequences that lead to computational intricacy. In this paper, We redefine TSR as a LOgical location REgression paradigm, which effectively captures inherent logical dependencies and constraints among table cells. Correspondingly, we propose LORE, a novel approach for TSR. LORE simultaneously predicts accurate geometric coordinates of table cells and the logical structures of the entire table. Our proposed LORE is conceptually simpler, easier to train, and more accurate than other TSR paradigms. Moreover, to enhance the model’s spatial and logical representation capabilities, we propose two pre-training tasks, resulting in an upgraded version named LORE++. The incorporation of pre-training is proven to enjoy significant advantages, leading to a substantial enhancement in terms of accuracy, generalization, and few-shot capability compared to its predecessor. Experiments on standard benchmarks demonstrate the superiority of LORE++, which highlights the potential and promising prospect of the logical location regression paradigm for TSR.}
}
@article{AI2024110764,
title = {A2GCN: Graph Convolutional Networks with Adaptive Frequency and Arbitrary Order},
journal = {Pattern Recognition},
volume = {156},
pages = {110764},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110764},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005156},
author = {Guoguo Ai and Hui Yan and Huan Wang and Xin Li},
keywords = {Graph convolutional networks, Graph spectral filter, Adaptive frequency, Arbitrary order},
abstract = {Graph Neural Networks (GNNs) gain remarkable success in various graph learning tasks under homophily graph assumption. This assumption is extremely fragile since real-world graphs with heterophily are ubiquitous. Under this circumstance, existing GNNs attempt to design or learn graph spectral filters for observed graphs. The representation ability of them, however, is limited due to: (1) Constant frequency response is incapable of simulating complicated filters that real-world applications require. (2) Fixed polynomial order fails to effectively uncover the node label patterns concealing different order neighborhoods. To this end, we propose a novel Graph Convolutional Networks with Adaptive Frequency and Arbitrary Order (A2GCN) to learn various graph spectral filters suitable for distinct networks. Specifically, a simple but elegant filter with adaptive frequency response is designed to span across multiple layers for capturing different frequency components hiding in varying orders, producing A2GCN filter bases. Afterward, the coefficients of the A2GCN basis for each node are learned to achieve A2GCN filters with arbitrary polynomial order. The resulting A2GCN filters possess flexible frequency response that can automatically adapt to the node label pattern, as such, it empowers A2GCN with stronger expressiveness and naturally alleviates the over-smoothing problem. Theoretical analysis is provided to show the superiority of the proposed A2GCN. Additionally, extensive experiments on both node-level and graph-level tasks validate that the proposed A2GCN accomplishes highly competitive performance and improves classification accuracy. Codes are available at https://github.com/AIG22/A2GCN.}
}
@article{LI2024110788,
title = {HTQ: Exploring the High-Dimensional Trade-Off of mixed-precision quantization},
journal = {Pattern Recognition},
volume = {156},
pages = {110788},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110788},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005399},
author = {Zhikai Li and Xianlei Long and Junrui Xiao and Qingyi Gu},
keywords = {Model compression, Quantized neural networks, Mixed-precision},
abstract = {Mixed-precision quantization, where more sensitive layers are kept at higher precision, can achieve the trade-off between accuracy and complexity of neural networks. However, the search space for mixed-precision grows exponentially with the number of layers, making the brute force approach infeasible on deep networks. To reduce this exponential search space, recent efforts use Pareto frontier or integer linear programming to select the bit-precision of each layer. Unfortunately, we find that these prior works rely on a single constraint. In practice, model complexity includes space complexity and time complexity, and the two are weakly correlated, thus using simply one as a constraint leads to sub-optimal results. Besides this, they require manually set constraints, making them only pseudo-automatic. To address the above issues, we propose High-dimensional Trade-off Quantization (HTQ), which automatically determines the bit-precision in the high-dimensional space of model accuracy, space complexity, and time complexity without any manual intervention. Specifically, we use the saliency criterion based on connection sensitivity to indicate the accuracy perturbation after quantization, which performs similarly to Hessian information but can be calculated quickly (more than 1000× speedup). The bit-precision is then automatically selected according to the three-dimensional (3D) Pareto frontier of the total perturbation, model size, and bit operations (BOPs) without manual constraints. Moreover, HTQ allows for the joint optimization of weights and activations, and thus the bit-precisions of both can be computed concurrently. Compared to state-of-the-art methods, HTQ achieves higher accuracy and lower space/time complexity on various model architectures for image classification and object detection tasks. Code is available at: https://github.com/zkkli/HTQ.}
}
@article{BAYKAL2024110792,
title = {EdVAE: Mitigating codebook collapse with evidential discrete variational autoencoders},
journal = {Pattern Recognition},
volume = {156},
pages = {110792},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110792},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005430},
author = {Gulcin Baykal and Melih Kandemir and Gozde Unal},
keywords = {Vector quantized variational autoencoders, Discrete variational autoencoders, Evidential deep learning, Codebook collapse},
abstract = {Codebook collapse is a common problem in training deep generative models with discrete representation spaces like Vector Quantized Variational Autoencoders (VQ-VAEs). We observe that the same problem arises for the alternatively designed discrete variational autoencoders (dVAEs) whose encoder directly learns a distribution over the codebook embeddings to represent the data. We hypothesize that using the softmax function to obtain a probability distribution causes the codebook collapse by assigning overconfident probabilities to the best matching codebook elements. In this paper, we propose a novel way to incorporate evidential deep learning (EDL) through a hierarchical Bayesian modeling instead of softmax to combat the codebook collapse problem of dVAE. We evidentially monitor the significance of attaining the probability distribution over the codebook embeddings, in contrast to softmax usage. Our experiments using various datasets show that our model, called EdVAE, mitigates codebook collapse while improving the reconstruction performance, and enhances the codebook usage compared to dVAE and VQ-VAE based models. Our code can be found at https://github.com/ituvisionlab/EdVAE.}
}