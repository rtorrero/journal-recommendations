@article{WANG2024110236,
title = {A Max-Relevance-Min-Divergence criterion for data discretization with applications on naive Bayes},
journal = {Pattern Recognition},
volume = {149},
pages = {110236},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110236},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009330},
author = {Shihe Wang and Jianfeng Ren and Ruibin Bai and Yuan Yao and Xudong Jiang},
keywords = {Data discretization, Maximal dependency, Maximal relevance, Minimal divergence, Naive Bayes classification},
abstract = {In many classification models, data is discretized to better estimate its distribution. Existing discretization methods often target at maximizing the discriminant power of discretized data, while overlooking the fact that the primary target of data discretization in classification is to improve the generalization performance. As a result, the data tend to be over-split into many small bins since the data without discretization retain the maximal discriminant information. Thus, we propose a Max-Dependency-Min-Divergence (MDmD) criterion that maximizes both the discriminant information and generalization ability of the discretized data. More specifically, the Max-Dependency criterion maximizes the statistical dependency between the discretized data and the classification variable while the Min-Divergence criterion explicitly minimizes the JS-divergence between the training data and the validation data for a given discretization scheme. The proposed MDmD criterion is technically appealing, but it is difficult to reliably estimate the high-order joint distributions of attributes and the classification variable. We hence further propose a more practical solution, Max-Relevance-Min-Divergence (MRmD) discretization scheme, where each attribute is discretized separately, by simultaneously maximizing the discriminant information and the generalization ability of the discretized data. The proposed MRmD is compared with the state-of-the-art discretization algorithms under the naive Bayes classification framework on 45 benchmark datasets. It significantly outperforms all the compared methods on most of the datasets.}
}
@article{LI2024110208,
title = {SideNet: Learning representations from interactive side information for zero-shot Chinese character recognition},
journal = {Pattern Recognition},
volume = {148},
pages = {110208},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110208},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009056},
author = {Ziyan Li and Yuhao Huang and Dezhi Peng and Mengchao He and Lianwen Jin},
keywords = {Optical character recognition, Chinese character recognition, Zero shot learning, Open set recognition},
abstract = {Existing methods for zero-shot Chinese character recognition usually exploit a single type of side information such as radicals, glyphs, or strokes to establish a mapping with the input characters for the recognition of unseen categories. However, these approaches have two limitations. Firstly, the mappings are inefficient owing to their complexity. Some existing methods design radical-level mappings using a non-differentiable dictionary-matching strategy, whereas others construct sophisticated embeddings to map seen and unseen characters into a unified latent space. Although the latter approach is straightforward, it lacks a learnable scheme for explicit structure construction. Secondly, the complementarity within multiple types of side information has not been effectively explored. For example, the radicals provide structural knowledge at an abstract level, whereas glyphs offer detailed information on their figurative counterparts. To this end, we propose a new method called SideNet that jointly learns character-level representations assisted by two types of interactive side information: radicals and glyphs. SideNet contains a structural conversion module that extracts radical knowledge via dimensional decomposition, and a spatial conversion module that encodes the radical counting map to produce an interactive outcome between radicals and glyph. Finally, we propose a new classifier that integrates the converted features by a similarity-guided fusion mechanism. To the best of our knowledge, this study represents the first attempt to integrate these two types of side information and explore a joint representation for zero-shot learning. Experiments show that SideNet consistently outperforms existing methods by a significant margin in diverse scenarios, including handwriting, printed art, natural scenes, and ancient Chinese characters, which demonstrates the potential of joint learning with multiple types of side information.}
}
@article{DENG2024110232,
title = {A diagnostic report supervised deep learning model training strategy for diagnosis of COVID-19},
journal = {Pattern Recognition},
volume = {149},
pages = {110232},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110232},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009299},
author = {Shiqi Deng and Xing Zhang and Shancheng Jiang},
keywords = {Deep learning, COVID-19, Image classification, Diagnosis, Chest CT image, Multimodal},
abstract = {COVID-19 is a highly contagious infectious disease that necessitates timely assessment and effective diagnosis, although it is no longer a health emergency. Most existing computer-aided diagnosis systems for COVID-19 can achieve high accuracy, but they show insufficient generalization performance and weak interpretability. To address these issues, we propose diagnostic report supervised contrastive learning (DRSCL), a model training strategy that incorporates textual information from medical reports into model pretraining and then only transfers the pretrained image encoder into model inference. Due to the issue of data recurrence in medical diagnosis reports, which is common in the medical domain and can cause nonconvergence of the pretraining stage of DRSCL, we improve the loss function calculation of contrastive learning by integrating an operation to merge identical text or image features. In addition, for the fine-tuning and inference stage of DRSCL, we design a hierarchical fine-tuning strategy to better evaluate the pretraining performance and importance of each module. In case study, we build a medical image-text pair dataset of lung diseases for pretraining, with samples collected from hospitals in East China, and then conduct the fine-tuning and inference operation of DRSCL with a publicly available SARS-CoV-2 dataset. The comparative experimental results show that DRSCL helps all involved image encoders obtain better classification accuracy and superior generalization performance in the given COVID-19-related diagnostic application. This finding indicates that DRSCL enhances deep models to learn more deep information with supervision of medical textual information. Furthermore, we adopt the Grad-CAM method to visualize pretrained models, and the results demonstrate that the DRSCL strategy is advantageous for improving model interpretability.}
}
@article{LI2024110220,
title = {A tree-based model with branch parallel decoding for handwritten mathematical expression recognition},
journal = {Pattern Recognition},
volume = {149},
pages = {110220},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110220},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009172},
author = {Zhe Li and Wentao Yang and Hengnian Qi and Lianwen Jin and Yichao Huang and Kai Ding},
keywords = {Handwritten mathematical expression recognition, Tree-based model, Parallel decoding, Attention mechanism},
abstract = {Handwritten mathematical expression recognition (HMER) is a challenging task in the field of computer vision due to the complex two-dimensional spatial structure and diverse handwriting styles of mathematical expressions (MEs). Recent mainstream approach treats MEs as objects with tree structures, modeled by sequence decoders or tree decoders. These decoders recognize the symbols and relationships between symbols in MEs in depth-first order, resulting in long decoding steps that can harm their performance, particularly for MEs with complex structures. In this paper, we propose a novel tree-based model with branch parallel decoding for HMER, which parses the structures of ME trees by explicitly predicting the relationships between symbols. In addition, a query constructing module is proposed to assist the decoder in decoding the branches of ME trees in parallel, thus reducing the number of decoding time steps and alleviating the problem of long sequence attention decoding. As a result, our model outperforms existing models on three widely-used benchmarks and demonstrates significant improvements in HMER performance.}
}
@article{HE2024110293,
title = {Local topology similarity guided probabilistic sampling for mismatch removal},
journal = {Pattern Recognition},
volume = {150},
pages = {110293},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110293},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400044X},
author = {Zaixing He and Chentao Shen and Xinyue Zhao},
keywords = {Topological stability, Local topology similarity, Probability sampling consensus, Mismatch removal},
abstract = {Feature point matching between two images is a fundamental and important process in machine vision. In many cases, mismatches are inevitable, and removing mismatches is an indispensable task. The existing methods attempt to find comprehensive constraints or sampling model to achieve better performance, which results in the increasingly complexity and may cause the weakness of the generality and scalability. To address this issue, a method called Local Topology similarity guided probabilistic Sampling consensus (LTS) is proposed. It constructs a topological network, then quantifies the mismatch probability in a concise approach based on comparing the topological relationship with neighbourhoods. Then, it detects and removes the mismatches by sampling guided by the mismatch probability. Compared with the state-of-the-art methods, LTS has an excellent performance in accuracy and robustness.}
}
@article{ZHOU2024110282,
title = {Multi-modal brain tumor segmentation via disentangled representation learning and region-aware contrastive learning},
journal = {Pattern Recognition},
volume = {149},
pages = {110282},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110282},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000335},
author = {Tongxue Zhou},
keywords = {Brain tumor segmentation, Multi-modal feature fusion, Disentangled representation learning, Contrastive learning},
abstract = {Brain tumors are threatening the life and health of people in the world. Automatic brain tumor segmentation using multiple MR images is challenging in medical image analysis. It is known that accurate segmentation relies on effective feature learning. Existing methods address the multi-modal MR brain tumor segmentation by explicitly learning a shared feature representation. However, these methods fail to capture the relationship between MR modalities and the feature correlation between different target tumor regions. In this paper, I propose a multi-modal brain tumor segmentation network via disentangled representation learning and region-aware contrastive learning. Specifically, a feature fusion module is first designed to learn the valuable multi-modal feature representation. Subsequently, a novel disentangled representation learning is proposed to decouple the fused feature representation into multiple factors corresponding to the target tumor regions. Furthermore, contrastive learning is presented to help the network extract tumor region-related feature representations. Finally, the segmentation results are obtained using the segmentation decoders. Quantitative and qualitative experiments conducted on the public datasets, BraTS 2018 and BraTS 2019, justify the importance of the proposed strategies, and the proposed approach can achieve better performance than other state-of-the-art approaches. In addition, the proposed strategies can be extended to other deep neural networks.}
}
@article{GUO2024110294,
title = {Progressive modality-complement aggregative multitransformer for domain multi-modal neural machine translation},
journal = {Pattern Recognition},
volume = {149},
pages = {110294},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110294},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000451},
author = {Junjun Guo and Zhenyu Hou and Yantuan Xian and Zhengtao Yu},
keywords = {Domain multi-modal neural machine translation, Multi-modal transformer, Progressive modality-complement, Modality-specific mask},
abstract = {Domain-specific Multi-modal Neural Machine Translation (DMNMT) aims to translate domain-specific sentences from a source language to a target language by incorporating text-related visual information. Generally, domain-specific text-image data often complement each other and have the potential to collaboratively enhance the representation of domain-specific information. Unfortunately, there is a considerable modality gap between image and text in data format and semantic expression, which leads to distinctive challenges in domain-text translation tasks. Narrowing the modality gap and improving domain-aware representation are two critical challenges in DMNMT. To this end, this paper proposes a progressive modality-complement aggregative MultiTransformer, which aims to simultaneously narrow the modality gap and capture domain-specific multi-modal representation. We first adopt a bidirectional progressive cross-modal interactive strategy to effectively narrow the text-to-text, text-to-visual, and visual-to-text semantics in the multi-modal representation space by integrating visual and text information layer-by-layer. Subsequently, we introduce a modality-complement MultiTransformer based on progressive cross-modal interaction to extract the domain-related multi-modal representation, thereby enhancing machine translation performance. Experiment results on the Fashion-MMT and Multi-30k datasets are conducted, and the results show that the proposed approach outperforms the compared state-of-the-art (SOTA) methods on the En-Zh task in E-commerce domain, En-De, En-Fr and En-Cs tasks of Multi-30k in general domain. The in-depth analysis confirms the validity of the proposed modality-complement MultiTransformer and bidirectional progressive cross-modal interactive strategy for DMNMT.}
}
@article{CHEN2024110266,
title = {Learning self-target knowledge for few-shot segmentation},
journal = {Pattern Recognition},
volume = {149},
pages = {110266},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110266},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000177},
author = {Yadang Chen and Sihan Chen and Zhi-Xin Yang and Enhua Wu},
keywords = {Few-shot segmentation, Two-level similarity matching, Step-by-step mining, Attention mechanism},
abstract = {Few-shot semantic segmentation uses a few annotated data of a specific class in the support set to segment the target of the same class in the query set. Most existing approaches fail to perform well when there are significant intra-class variances. This paper alleviates the problem by concentrating on mining the query image and using the support set as supplementary information. First, it proposes a Query Prototype Generation Module to generate a query foreground prototype from the query features. Specifically, we use both prototype-level and pixel-level similarity matching to generate two complementary initial prototypes, which we then integrate to create a discriminative query foreground prototype. Second, we propose a Support Auxiliary Refinement Module to further guide the final precise prediction of the query image by leveraging the target category information of the support set through step-by-step mining. Specifically, we generate a query-support mixture prototype based on the support prototype representation obtained using the attention mechanism. Then we generate a support supplement prototype to complement the missing information by encoding over the foreground regions that the query-support mixture prototype fails to segment out. Extensive experiments on PASCAL-5i and COCO-20i demonstrate that our model outperforms the prior works of few-shot segmentation.}
}
@article{HAYASHI2024110224,
title = {Gaussian process classification bandits},
journal = {Pattern Recognition},
volume = {149},
pages = {110224},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110224},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009214},
author = {Tatsuya Hayashi and Naoki Ito and Koji Tabata and Atsuyoshi Nakamura and Katsumasa Fujita and Yoshinori Harada and Tamiki Komatsuzaki},
keywords = {Bandit problem, Gaussian process, Classification bandits, Level set estimation},
abstract = {Classification bandits are multi-armed bandit problems whose task is to classify a given set of arms into either positive or negative class depending on whether the rate of the arms with the expected reward of at least h is not less than w for given thresholds h and w. We study a special classification bandit problem in which arms correspond to points x in d-dimensional real space with expected rewards f(x) which are generated according to a Gaussian process prior. We develop a framework algorithm for the problem using various arm selection policies and propose policies called FCB (Farthest Confidence Bound) and FTSV (Farthest Thompson Sampling Value). We show a smaller sample complexity upper bound for FCB than that for the existing algorithm of the level set estimation, in which whether f(x) is at least h or not must be decided for every arm’s x. Arm selection policies depending on an estimated rate of arms with mean rewards of at least h are also proposed and shown to improve empirical sample complexity. According to our experimental results, the rate-estimation versions of FCB and FTSV, together with that of the popular active learning policy which selects the point with the maximum variance, outperform other policies for synthetic functions, and the rate-estimation version of FTSV is also the best performer for our real-world dataset.}
}
@article{YU2024110248,
title = {Gradient aggregation based fine-grained image retrieval: A unified viewpoint for CNN and Transformer},
journal = {Pattern Recognition},
volume = {149},
pages = {110248},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110248},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009457},
author = {Han Yu and Huibin Lu and Min Zhao and Zhuoyi Li and Guanghua Gu},
keywords = {Fine-grained image retrieval, Convolution filters gradient aggregation, CFGA feature, Transformer parameter gradients aggregation, Deep metric learning},
abstract = {The gradients of CNN are traditionally utilized for optimization and visualization. In this paper, we find that a discriminative representation hides in the gradients of convolution filters. Based on this, we propose a corresponding feature extraction and aggregation method for fine-grained image retrieval (FGIR). Firstly, we propose a metric to evaluate manually-designed loss functions and design a loss function originating from Grad-CAM in the testing phase based on it to extract the gradients of the convolution filters. Secondly, we take the gradients as the new features and design a succinct approach to aggregate them into a compact vector, which is named as Convolution Filters Gradient Aggregation (CFGA) feature. CFGA features can be extracted from pre-trained and fine-tuned CNN models. Extensive experiments are conducted on FGIR to verify the effectiveness of our proposed CFGA approach, compared with five supervised state-of-the-art methods and two unsupervised methods on two standard fine-grained retrieval datasets. Moreover, we generalize the CFGA method designed for CNN to Swin Transformer, and propose the Transformer parameter gradients aggregation (TPGA) method, which proves the applicability of the core idea of CFGA/TPGA to mainstream feature extraction models. We achieve state-of-the-art FGIR performance on CUB-200-2011 dataset and CARS196 dataset.}
}
@article{LU2024110254,
title = {HRNet: 3D object detection network for point cloud with hierarchical refinement},
journal = {Pattern Recognition},
volume = {149},
pages = {110254},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110254},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000050},
author = {Bin Lu and Yang Sun and Zhenyu Yang and Ran Song and Haiyan Jiang and Yonghuai Liu},
keywords = {3D object detection, LiDAR point clouds, Multi-scale features, Transformer, Dynamic sample selection, Hierarchical refinement},
abstract = {Recently, 3D object detection from LiDAR point clouds has advanced rapidly. Although the second stage can improve the detection performance significantly, prior works concern little about the essential differences among different stages for the performance enhancement. To address this, this paper proposes a Hierarchical Refinement Network (HRNet) with two novel strategies. Firstly, we build the detection head on multi-scale voxel features to optimize the regression branch progressively with an effective Scale-aware Attentive Propagation (SAP) module. Then, we propose a Dynamic Sample Selection (DSS) module for the recalculation of the IoU during each stage to obtain more balanced positive and negative sample selections. Experiments over benchmark datasets show the effectiveness of our HRNet, particularly for car detection in the sparse point clouds.}
}
@article{ZHAO2024110221,
title = {Class correlation correction for unbiased scene graph generation},
journal = {Pattern Recognition},
volume = {149},
pages = {110221},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110221},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009184},
author = {Mengnan Zhao and Yuqiu Kong and Lihe Zhang and Baocai Yin},
keywords = {Unbiased scene graph generation, Class correlation correction, Debiasing transformation, Biasing transformation},
abstract = {The long-tail distribution in the scene graph generation (SGG) task has spurred immense interest in unbiased SGG. However, current state-of-the-art debiasing techniques extract statistics from the independent category, while ignoring the correlation between categories. To address this issue, we propose a simple but effective method, class correlation correction, to aggregate dependency knowledge among various classes. Specifically, given biased predictions, two kinds of debiasing transformations are developed employing the class correlation aware label to recover the unbiased estimates. We also propose to retrain SGG models with the biasing transformations adapted to the biased data distribution. The proposed debiasing method is evaluated using several biased datasets that are constructed from CIFAR-10 and Fashion-MNIST, as well as the commonly used SGG dataset and Caltech 101 dataset. Multiple evaluation metrics are used to assess the debiasing performance, and the results of extensive experiments show the effectiveness of the proposed method. The Pytorch® implementations can be downloaded from an open-source GitHub project https://github.com/Dlut-lab-zmn/class-correlation-correction.}
}
@article{ZHU2024110237,
title = {Unsupervised spatial self-similarity difference-based change detection method for multi-source heterogeneous images},
journal = {Pattern Recognition},
volume = {149},
pages = {110237},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110237},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009342},
author = {Linye Zhu and Wenbin Sun and Deqin Fan and Huaqiao Xing and Xiaoqi Liu},
keywords = {Heterogeneous images, multi-source, change detection, unsupervised method},
abstract = {Multi-source heterogeneous change detection has been widely used in dynamic disaster monitoring, land cover updating, etc. Various methods have been proposed to make heterogeneous data comparable. However, heterogeneous images are difficult to compare directly and may be affected by noise. Most existing methods obtain change information through mapping and regression, lacking the utilisation of image spatial information and a comprehensive portrayal of the changes, which may affect change detection results. To address these challenges, we propose an unsupervised spatial self-similarity difference-based change detection (USSD) method for multi-source heterogeneous images to evaluate the similarity of spatial relationships in heterogeneous images. First, the images are divided into image blocks to construct spatial self-difference images between individual image blocks aiming to make the data comparable. Second, the change information is portrayed in terms of both the magnitude differences and similarity differences to obtain a more comprehensive spatial self-difference change magnitude map. Then, the spatial neighbourhood information of the spatial self-difference change magnitude map is considered to avoid noise. Experimental results on six open datasets indicate that the overall accuracy of the USSD method was approximately 85%–95%. This method improves the change magnitude map discrimination, better detects the change region, and avoids noise in synthetic aperture radar images.}
}
@article{MRABAH2024110209,
title = {A contrastive variational graph auto-encoder for node clustering},
journal = {Pattern Recognition},
volume = {149},
pages = {110209},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110209},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009068},
author = {Nairouz Mrabah and Mohamed Bouguessa and Riadh Ksantini},
keywords = {Unsupervised learning, Contrastive learning, Graph variational auto-encoders, Node clustering},
abstract = {Variational Graph Auto-Encoders (VGAEs) have been widely used to solve the node clustering task. However, the state-of-the-art methods have numerous challenges. First, existing VGAEs do not account for the discrepancy between the inference and generative models after incorporating the clustering inductive bias. Second, current models are prone to degenerate solutions that make the latent codes match the prior independently of the input signal (i.e., Posterior Collapse). Third, existing VGAEs overlook the effect of the noisy clustering assignments (i.e., Feature Randomness) and the impact of the strong trade-off between clustering and reconstruction (i.e., Feature Drift). To address these problems, we formulate a variational lower bound in a contrastive setting. Our lower bound is a tighter approximation of the log-likelihood function than the corresponding Evidence Lower BOund (ELBO). Thanks to a newly identified term, our lower bound can escape Posterior Collapse and has more flexibility to account for the difference between the inference and generative models. Additionally, our solution has two mechanisms to control the trade-off between Feature Randomness and Feature Drift. Extensive experiments show that the proposed method achieves state-of-the-art clustering results on several datasets. We provide strong evidence that this improvement is attributed to four aspects: integrating contrastive learning and alleviating Feature Randomness, Feature Drift, and Posterior Collapse.}
}
@article{ZHANG2024110279,
title = {SEMv2: Table separation line detection based on instance segmentation},
journal = {Pattern Recognition},
volume = {149},
pages = {110279},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110279},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400030X},
author = {Zhenrong Zhang and Pengfei Hu and Jiefeng Ma and Jun Du and Jianshu Zhang and Baocai Yin and Bing Yin and Cong Liu},
keywords = {Table structure recognition, Table separation line detection, Instance segmentation, Conditional convolution, Table structure dataset},
abstract = {Table structure recognition is an indispensable element for enabling machines to comprehend tables. Its primary purpose is to identify the internal structure of a table. Nevertheless, due to the complexity and diversity of their structure and style, it is highly challenging to parse the tabular data into a structured format that machines can comprehend. In this work, we adhere to the principle of the split-and-merge based methods and propose an accurate table structure recognizer, termed SEMv2 (SEM: Split, Embed and Merge). Unlike the previous works in the “split” stage, we aim to address the table separation line instance-level discrimination problem and introduce a table separation line detection strategy based on conditional convolution. Specifically, we design the “split” in a top-down manner that detects the table separation line instance first and then dynamically predicts the table separation line mask for each instance. The final table separation line shape can be accurately obtained by processing the table separation line mask in a row-wise/column-wise manner. To comprehensively evaluate the SEMv2, we also present a more challenging dataset for table structure recognition, dubbed iFLYTAB, which encompasses multiple style tables in various scenarios such as photos, scanned documents, etc. Extensive experiments on publicly available datasets (e.g. SciTSR, PubTabNet and iFLYTAB) demonstrate the efficacy of our proposed approach. The code and iFLYTAB dataset are available at https://github.com/ZZR8066/SEMv2}
}
@article{FANG2024110281,
title = {Towards robust neural networks via orthogonal diversity},
journal = {Pattern Recognition},
volume = {149},
pages = {110281},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110281},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000323},
author = {Kun Fang and Qinghua Tao and Yingwen Wu and Tao Li and Jia Cai and Feipeng Cai and Xiaolin Huang and Jie Yang},
keywords = {Model augmentation, Multi-head, Orthogonality, Margin-maximization, Data augmentation, Adversarial robustness},
abstract = {Deep Neural Networks (DNNs) are vulnerable to invisible perturbations on the images generated by adversarial attacks, which raises researches on the adversarial robustness of DNNs. A series of methods represented by the adversarial training and its variants have proven as one of the most effective techniques in enhancing the DNN robustness. Generally, adversarial training focuses on enriching the training data by involving perturbed data. Such data augmentation effect of the involved perturbed data in adversarial training does not contribute to the robustness of DNN itself and usually suffers from clean accuracy drop. Towards the robustness of DNN itself, we in this paper propose a novel defense that aims at augmenting the model in order to learn features that are adaptive to diverse inputs, including adversarial examples. More specifically, to augment the model, multiple paths are embedded into the network, and an orthogonality constraint is imposed on these paths to guarantee the diversity among them. A margin-maximization loss is then designed to further boost such DIversity via Orthogonality (DIO). In this way, the proposed DIO augments the model and enhances the robustness of DNN itself as the learned features can be corrected by these mutually-orthogonal paths. Extensive empirical results on various data sets, structures and attacks verify the stronger adversarial robustness of the proposed DIO utilizing model augmentation. Besides, DIO can also be flexibly combined with different data augmentation techniques (e.g., TRADES and DDPM), further promoting robustness gains.}
}
@article{LI2024110305,
title = {A transformer-CNN parallel network for image guided depth completion},
journal = {Pattern Recognition},
volume = {150},
pages = {110305},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110305},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000566},
author = {Tao Li and Xiucheng Dong and Jie Lin and Yonghong Peng},
keywords = {Depth completion, Convolutional neural network, Transformer, Token correlation, Conditional random field},
abstract = {Image guided depth completion aims to predict a dense depth map from sparse depth measurements and the corresponding single color image. However, most state-of-the-art methods only rely on convolutional neural network (CNN) or transformer. In this paper, we propose a transformer-CNN parallel network (TCPNet) to integrate the advantages of CNN in local detail recovery and transformer in long-range semantic modeling. Specifically, our CNN branch adopts dense connection to strengthen feature propagation. Since the common transformer computes self-attention based on all the tokens in the window, no matter if they are relevant or not, this will inevitably introduce interferences and noises. To improve the self-attention accuracy, we propose a correlation-based transformer to only allow nearest neighbor tokens to participate in the self-attention computation. We also design a multi-scale conditional random field (CRF) module to implement multi-scale high-dimensional filtering for depth refinement. The comprehensive experimental results on KITTI and NYUv2 demonstrate that our method outperforms the state-of-the-art methods.}
}
@article{ZHANG2024110253,
title = {Tensor recovery based on Bivariate Equivalent Minimax-Concave Penalty},
journal = {Pattern Recognition},
volume = {149},
pages = {110253},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110253},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000049},
author = {Hongbing Zhang and Hongtao Fan and Yajing Li},
keywords = {Tensor recovery, Bivariate equivalent minimax-concave penalty (BEMCP), Low-rank tensor completion (LRTC), Tensor robust principal component analysis (TRPCA)},
abstract = {In tensor recovery problem, larger singular values often correspond to the primary information of the image, such as contours, sharp edges and smooth areas. The minimum–maximum concave penalty (MCP) function has been effective in preserving larger singular values and achieving good tensor recovery results. However, in the process of solving this problem, singular values may vary during iterations, and the fixed parameters of the MCP function may not sufficiently preserve all the larger singular values, which may hinder the attainment of optimal results in tensor recovery. To overcome this challenge, we propose the Bivariate Equivalent Minimax-Concave Penalty (BEMCP) theorem, which allows the parameters to adapt to the changes in singular values and more comprehensively preserve the larger singular values. For low-rank tensor completion and tensor robust principal component analysis problems, we propose BEMCP-based models. Finally, experiments with various real-world data demonstrate that the proposed methods outperform state-of-the-art methods.}
}
@article{ZHANG2024110249,
title = {SCGTracker: Spatio-temporal correlation and graph neural networks for multiple object tracking},
journal = {Pattern Recognition},
volume = {149},
pages = {110249},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110249},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009469},
author = {Yajuan Zhang and Yongquan Liang and Jiaxu Leng and Zhihui Wang},
keywords = {Multiple object tracking, Graph neural networks, Spatio-temporal correlation, Historical trajectory},
abstract = {Multiple Object Tracking is an important vitally important fundamental task in computer vision. Visual tracking becomes challenging when objects move in groups and are obscured from each other. There are two mainstream solution strategies for these group models. One is to transform the data association problem into a graph matching problem for solving, while the other is to apply the social power model as an advanced constraint for group tracking. In the former case, the solving difficulty geometric growth as the number of tracked objects increases, and the computing efficiency for real-time tracking demand cannot be met. The latter strategy tends to set up fixed-size groups or offline training rules, resulting in a lack of flexibility that limits their scenario generalization. According to the shortcomings of existing methods, this paper proposes a novel multiple object tracking method with spatio-temporal correlation and graph neural networks. Firstly, the relational features of the historical trajectories are extracted through the spatio-temporal relationship learning module, which models the spatio-temporal correlations of the objects and dynamically constructs the group structure online. Then, the graph neural network is combined with appearance and motion information, and the similarity between each detection and tracklet is used as a weight in node feature aggregation to make powerful distinctions between node features. Meanwhile, the spatio-temporal correlation method is also used to solve target loss issues caused by occlusion. Even collocated with linearly assigned data association method, good tracking results are still achieved, with a low computational complexity. Experiments on three challenging public datasets, namely MOT16, MOT17, and MOT20, validated the accuracy and efficiency of the proposed tracking method.}
}
@article{CHEN2024110265,
title = {FET-FGVC: Feature-enhanced transformer for fine-grained visual classification},
journal = {Pattern Recognition},
volume = {149},
pages = {110265},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110265},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000165},
author = {Huazhen Chen and Haimiao Zhang and Chang Liu and Jianpeng An and Zhongke Gao and Jun Qiu},
keywords = {Fine-grained visual classification (FGVC), Transformer, Graph convolutional network (GCN), Feature enhancement},
abstract = {The challenge of Fine-grained visual classification (FGVC) comes from the small variations between classes and the large variations within classes. Inspired by the fact that identifying bird species focuses not only on the global features of the subject area but also on the subtle details of the local area, we propose a feature-enhanced Transformer to improve the performance of FGVC. Our proposed method consists of a Dynamic Swin Transformer backbone for extracting comprehensive global image features through continuous attention aggregation, a GCN-based local branch for separating and enhancing local features in different regions, and a pairwise feature interaction (PFI) module for enhancing global features through interactions between image pairs. We conducted extensive experiments on five FGVC datasets to demonstrate the superiority of our method. By fusing the enhanced global and local features, our method achieves the best accuracy compared to existing methods. Our method has an advantage in terms of computational efficiency.}
}
@article{ZHUANG2024110225,
title = {LAFED: Towards robust ensemble models via Latent Feature Diversification},
journal = {Pattern Recognition},
volume = {150},
pages = {110225},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110225},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009226},
author = {Wenzi Zhuang and Lifeng Huang and Chengying Gao and Ning Liu},
keywords = {Adversarial example, Adversarial defense, Ensemble model, Robustness},
abstract = {Adversarial examples pose a significant challenge to the security of deep neural networks (DNNs). In order to defend against malicious attacks, adversarial training forces DNNs to learn more robust features by suppressing generalizable but non-robust features, which boosts the robustness while suffering from significant accuracy drops on clean images. Ensemble training, on the other hand, trains multiple sub-models to predict data for improved robustness and still achieves desirable accuracy on clean data. Despite these efforts, previous ensemble methods are still susceptible to attacks and fail to increase model diversity as the size of the ensemble group increases. In this work, we revisit the model diversity from the perspective of data and discover that high similarity between training batches decreases feature diversity and weakens ensemble robustness. To this end, we propose Latent Feature Diversification (LAFED), which reconstructs training sets with diverse features during the optimization, enhancing the overall robustness of an ensemble. For each sub-model, LAFED treats the vulnerability extracted from other sub-models as raw data, which is then combined with round-changed weights with a stochastic manner in the latent space. This results in the formation of new features, remarkably reducing the similarity of learned representations between the sub-models. Furthermore, LAFED enhances feature diversity within the ensemble model by utilizing hierarchical smoothed labels. Extensive experiments illustrate that LAFED significantly improves diversity among sub-models and enhances robustness against adversarial attacks compared to current methods. The code is publicly available at https://github.com/zhuangwz/LAFED.}
}
@article{ZHANG2024110196,
title = {SED: Searching Enhanced Decoder with switchable skip connection for semantic segmentation},
journal = {Pattern Recognition},
volume = {149},
pages = {110196},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110196},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008932},
author = {Xian Zhang and Zhibin Quan and Qiang Li and Dejun Zhu and Wankou Yang},
keywords = {NAS, Semantic segmentation, Encoder-decoder model},
abstract = {Neural architecture search (NAS) has shown excellent performance. However, existing semantic segmentation models rely heavily on pre-training on Image-Net or COCO and mainly focus on the designing of decoders. Directly training the encoder–decoder architecture search models from scratch to SOTA for semantic segmentation requires even thousands GPU days, which greatly limits the application of NAS. To address this issue, we propose a novel neural architecture Search framework for Enhanced Decoder (SED). Utilizing the pre-trained hand-designing backbone and the searching space composed of light-weight cells, SED searches for a decoder which can perform high-quality segmentation. Furthermore, we attach switchable skip connection operations to search space, expanding the diversity of possible network structure. The parameters of backbone and operations selected in searching phrase are copied to retraining process. As a result, searching, pruning and retraining can be done in just 1 day. The experimental results show that the SED proposed in this paper only needs 1/4 of the parameters and calculation in contrast to hand-designing decoder, and obtains higher segmentation accuracy on Cityscapes. Transferring the same decoder architecture to other datasets, such as: Pascal VOC 2012, Camvid, ADE20K proves the robustness of SED.}
}
@article{BANERJEE2024110206,
title = {An end-to-end model for multi-view scene text recognition},
journal = {Pattern Recognition},
volume = {149},
pages = {110206},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110206},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009032},
author = {Ayan Banerjee and Palaiahnakote Shivakumara and Saumik Bhattacharya and Umapada Pal and Cheng-Lin Liu},
keywords = {Text detection,, Scene text recognition, Siamese network, Natural language model, Genetic algorithm, Multi-view text detection},
abstract = {Due to the increasing applications of surveillance and monitoring such as person re-identification, vehicle re-identification and sports events tracking, the necessity of text detection and end-to-end recognition is also growing. Although the past deep learning-based models have addressed several challenges such as arbitrary-shaped text, multiple scripts, and variations in the geometric structure of characters, the scope of the models is limited to a single view. This paper presents an end-to-end model for text recognition through refining the multi-views of the same scene, which is called E2EMVSTR (End-to-End Model for Multi-View Scene Text Recognition). Considering the common characteristics shared in multi-view texts, we propose a cycle consistency pairwise similarity-based deep learning model to find texts more efficiently in three input views. Further, the extracted texts are supplied to a Siamese network and semi-supervised attention embedding combinational network for obtaining recognition results. The proposed model combines natural language processing and genetic algorithm models to restore missing character information and correct wrong recognition results. In experiments on our multi-view dataset and several benchmark datasets, the proposed method is proven effective compared to the state-of-the-art methods. The dataset and codes will be made available to the public upon acceptance.}
}
@article{NIE2025111084,
title = {Corrigendum to “Fast adaptively balanced min-cut clustering” [Pattern Recognition 158 (2025) 111027]},
journal = {Pattern Recognition},
volume = {159},
pages = {111084},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111084},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008355},
author = {Feiping Nie and Fangyuan Xie and Jingyu Wang and Xuelong Li}
}
@article{KANG2024110230,
title = {FedNN: Federated learning on concept drift data using weight and adaptive group normalizations},
journal = {Pattern Recognition},
volume = {149},
pages = {110230},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110230},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009275},
author = {Myeongkyun Kang and Soopil Kim and Kyong Hwan Jin and Ehsan Adeli and Kilian M. Pohl and Sang Hyun Park},
keywords = {Federated learning, Concept drift, Weight normalization, Adaptive group normalization},
abstract = {Federated Learning (FL) allows a global model to be trained without sharing private raw data. The major challenge in FL is client-wise data heterogeneity leading to different model convergence speed and accuracy. Despite the recent progress of FL, most methods verify their accuracy on prior probability shift (label distribution skew) dataset, while the concept drift problem (i.e., where each client has distinct styles of input while sharing the same labels) has not been explored. In real scenarios, concept drift is of paramount concern in FL since the client’s data is collected under extremely different conditions making FL optimization more challenging. Significant differences in inputs among clients exacerbate the heterogeneity of clients’ parameters compared to prior probability shift, ultimately resulting in failures for previous FL approaches. To address the challenge of concept drift, we use Weight Normalization (WN) and Adaptive Group Normalization (AGN) to alleviate conflicts during global model updates. WN re-parameterizes weights to have zero mean and unit variance while AGN adaptively selects the optimal mean and standard deviation for feature normalization based on the dataset. These two components significantly contribute to having consistent activations after global model updates reducing heterogeneity in concept drift data. Comprehensive experiments on seven datasets (with concept drift) demonstrate that our method outperforms five state-of-the-art FL methods and shows faster convergence speed compared to the previous FL methods.}
}
@article{SONG2024110241,
title = {Robust low tubal rank tensor recovery via L2E criterion},
journal = {Pattern Recognition},
volume = {149},
pages = {110241},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110241},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300938X},
author = {Zihao Song and Xiangjian Xu and Heng Lian and Weihua Zhao},
keywords = {Robust tensor recovery, Tensor factor Frobenius norm, Low tubal rank,  criterion, Nonconvex},
abstract = {Tensor analysis has received enormous attention as the increasing prevalence of high-dimensional data in science research and engineering application. Tensor recovery is an important and meaningful problem for tensor analysis. It aims to complete a tensor from an observed subset of its entries disturbed by noise. However, classical methods either develop on the second-order statistics or Lasso-type penalty, leading to them not effectively dealing with gross or dense noise effectively. To address such issues, we propose a robust tensor recovery model for simultaneously completing a low tubal rank tensor with complex noise and missing data. Based on tensor–tensor product (t-product), we first develop a tensor factor Frobenius norm to exploit the low tubal rank property which is closely related to tensor nuclear norm and tubal rank. By utilizing the robust L2 criterion, we derive the nonconvex objective function to accommodate the low tubal rank tensor. An implementable alternating minimization algorithm has been developed to estimate the low tubal rank tensor. It is worth noting that our method is able to jointly estimate the precision parameter to capture the hidden complex noise pattern. Furthermore, some convergence properties of the proposed algorithm are presented. A series of numerical experiments are conducted on both synthetic and real-world data to demonstrate the effectiveness and robustness of the proposed approach in comparison with the state-of-the-art methods.}
}
@article{HE2024110246,
title = {Source-free domain adaptation with unrestricted source hypothesis},
journal = {Pattern Recognition},
volume = {149},
pages = {110246},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110246},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009433},
author = {Jiujun He and Liang Wu and Chaofan Tao and Fengmao Lv},
keywords = {Domain adaptation, Privacy protection, Transfer learning, Deep learning},
abstract = {Domain adaptation aims to bridge the distribution discrepancy across different domains and improve the generalization ability of learning models on the target domain. The existing domain adaptation approaches align the distribution shift via adversarial training on the source and target data. In practice, however, the source data is usually unavailable due to the privacy factor. In this work, we mainly focus on the source-free domain adaptation setting, in which we are only accessible to the model trained on the source data and the unlabeled target data. To this end, we propose the Source-Free Adversarial Domain Adaptation (SFADA) approach to align the distribution of the target domain data in the absence of source domain data. In particular, we develop an effective metric to measure the domain discrepancy by introducing the proxy data of the source domain. To generate the proxy data, our approach retrieves target data which lie over the intersection of the supports of the source and target domains. We also derive the learning bound of the source-free domain adaptation theoretically and show that our proposed SFADA approach is capable of reducing the bound effectively. Additionally, instead of modifying the source model in previous source-free approaches, our SFADA does not require training the source model with specific restrictions (i.e., normalizing the classifier weight) for practice and privacy-related concerns. State-of-the-art results are achieved for different standard domain adaptation benchmarks. The code can be available from https://github.com/tiggers23/SFADA-main.}
}
@article{XIA2024110242,
title = {Multi-scale architectures matter: Examining the adversarial robustness of flow-based lossless compression},
journal = {Pattern Recognition},
volume = {149},
pages = {110242},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110242},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009391},
author = {Yichong Xia and Bin Chen and Yan Feng and Tianshuo Ge and Yujun Huang and Haoqian Wang and Yaowei Wang},
keywords = {Flow models, Lossless compression, Adversarial attack, Lipschitz property},
abstract = {Exact likelihood estimation on entropy coding makes flow-based models appealing for lossless image compression. However, the high fidelity storage cost is affected by the lossless compression ratio. The trade-off between efficiency and robustness of flow-based deep lossless compression models has not been fully explored. This paper characterizes the trade-off theoretically and empirically, revealing that flow-based models are susceptible to adversarial examples resulting in a significant change in compression ratio. The fragile robustness of flow-based models is due to their intrinsic multi-scale architectures lacking the Lipschitz property. Based on this insight, a stronger white-box attack, Auto-Weighted Projected Gradient Descent (AW-PGD), is developed to generate more universal adversarial examples. Additionally, a novel flow-based lossless compression model, Robust Integer Discrete Flow (R-IDF), is proposed to achieve comparable robustness to adversarial training without sacrificing compression efficiency. Experiments demonstrate that the PGD algorithm falls into local extreme values when attacking the compression model, but the proposed attack and defense methods effectively improve the invulnerability of the flow-based compression model.}
}
@article{LIU2024110284,
title = {PVConvNet: Pixel-Voxel Sparse Convolution for multimodal 3D object detection},
journal = {Pattern Recognition},
volume = {149},
pages = {110284},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110284},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000359},
author = {Huaijin Liu and Jixiang Du and Yong Zhang and Hongbo Zhang and Jiandian Zeng},
keywords = {3D object detection, LiDAR points, Virtual points, Image pixels, Multi-modal fusion},
abstract = {Current LiDAR-only 3D detection methods inevitably suffer from the sparsity of point clouds and insufficient semantic information. To alleviate this difficulty, recent proposals densify LiDAR points by depth completion and then perform feature fusion with image pixels at the data-level or result-level. However, these methods often suffer from poor fusion effects and insufficient use of image information for voxel feature-level fusion. Meanwhile, noises brought by inaccurate depth completion significantly degrade detection accuracy. In this paper, we propose PVConvNet, a unified framework for multi-modal feature fusion that cleverly combines LiDAR points, virtual points and image pixels. Firstly, we develop an efficient Pixel-Voxel Sparse Convolution (PVConv) to perform voxel-wise feature-level fusion of point clouds and images. Secondly, we design a Noise-Resistant Dilated Sparse Convolution (NRDConv) to encode the voxel features of virtual points, which effectively reduces the impact of noise. Finally, we propose a unified RoI pooling strategy, namely Multimodal Voxel-RoI Pooling, for improving proposal refinement accuracy. We evaluate PVConvNet on the widely used KITTI dataset and the more challenging nuScenes dataset. Experimental results show that our method outperforms state-of-the-art multi-modal based methods, achieving a moderate 3D AP of 86.92% on the KITTI test set.}
}
@article{WU2024110177,
title = {DSText V2: A comprehensive video text spotting dataset for dense and small text},
journal = {Pattern Recognition},
volume = {149},
pages = {110177},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110177},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008749},
author = {Weijia Wu and Yiming Zhang and Yefei He and Luoming Zhang and Zhenyu Lou and Hong Zhou and Xiang Bai},
keywords = {Video text spotting, Small text, Text tracking, Dense text},
abstract = {Recently, video text detection, tracking, and recognition in natural scenes are becoming very popular in the computer vision community. However, most existing algorithms and benchmarks focus on common text cases (e.g.,normal size, density) and single scenario, while ignoring extreme video text challenges, i.e., dense and small text in various scenarios. In this paper, we establish a video text reading benchmark, named DSText V2, which focuses on Dense and Small text reading challenges in the video with various scenarios. Compared with the previous datasets, the proposed dataset mainly include three new challenges: (1) Dense video texts, a new challenge for video text spotters to track and read. (2) High-proportioned small texts, coupled with the blurriness and distortion in the video, will bring further challenges. (3) Various new scenarios, e.g., ‘Game’, ‘Sports’, etc. The proposed DSText V2 includes 140 video clips from 7 open scenarios, supporting three tasks, i.e., video text detection (Task 1), video text tracking (Task 2), and end-to-end video text spotting (Task 3). In this article, we describe detailed statistical information of the dataset, tasks, evaluation protocols, and the results summaries. Most importantly, a thorough investigation and analysis targeting three unique challenges derived from our dataset are provided, aiming to provide new insights. Moreover, we hope the benchmark will promise video text research in the community. DSText v2 is built upon DSText v1, which was previously introduced to organize the ICDAR 2023 competition for dense and small video text. The dataset website can be found at RRC or Zenodo.}
}
@article{SU2024110307,
title = {Kernel correlation–dissimilarity for Multiple Kernel k-Means clustering},
journal = {Pattern Recognition},
volume = {150},
pages = {110307},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110307},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400058X},
author = {Rina Su and Yu Guo and Caiying Wu and Qiyu Jin and Tieyong Zeng},
keywords = {k-means, Multiple kernel learning, Consistency, Frobenius inner product, Manhattan distance},
abstract = {The main objective of the Multiple Kernel k-Means (MKKM) algorithm is to extract non-linear information and achieve optimal clustering by optimizing base kernel matrices. Current methods enhance information diversity and reduce redundancy by exploiting interdependencies among multiple kernels based on correlations or dissimilarities. Nevertheless, relying solely on a single metric, such as correlation or dissimilarity, to define kernel relationships introduces bias and incomplete characterization. Consequently, this limitation hinders efficient information extraction, ultimately compromising clustering performance. To tackle this challenge, we introduce a novel method that systematically integrates both kernel correlation and dissimilarity. Our approach comprehensively captures kernel relationships, facilitating more efficient classification information extraction and improving clustering performance. By emphasizing the coherence between kernel correlation and dissimilarity, our method offers a more objective and transparent strategy for extracting non-linear information and significantly improving clustering precision, supported by theoretical rationale. We assess the performance of our algorithm on 13 challenging benchmark datasets, demonstrating its superiority over contemporary state-of-the-art MKKM techniques.}
}
@article{CIOCARLAN2024110312,
title = {Deep-NFA: A deep a contrario framework for tiny object detection},
journal = {Pattern Recognition},
volume = {150},
pages = {110312},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110312},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000633},
author = {Alina Ciocarlan and Sylvie {Le Hégarat-Mascle} and Sidonie Lefebvre and Arnaud Woiselle},
keywords = {A contrario reasoning, One-class semantic segmentation, Deep learning, Convolutional neural networks, Small target detection},
abstract = {The detection of tiny objects is a challenging task in computer vision. Conventional object detection methods have difficulties in finding the balance between high detection rate and low false alarm rate. In the literature, some methods have addressed this issue by enhancing the feature map responses for small objects, but without guaranteeing robustness with respect to the number of false alarms induced by background elements. To tackle this problem, we introduce an a contrario decision criterion into the learning process to take into account the unexpectedness of tiny objects. This statistic criterion enhances the feature map responses while controlling the number of false alarms (NFA) and can be integrated as an add-on into any semantic segmentation neural network. Our add-on NFA module not only allows us to obtain competitive results for small target, road crack and ship detection tasks respectively, but also leads to more robust and interpretable results.}
}
@article{LI2024110260,
title = {Multi-scale hypergraph-based feature alignment network for cell localization},
journal = {Pattern Recognition},
volume = {149},
pages = {110260},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110260},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000116},
author = {Bo Li and Yong Zhang and Chengyang Zhang and Xinglin Piao and Yongli Hu and Baocai Yin},
keywords = {Cell localization, Feature alignment, Hypergraph neural network, Multi-scale hypergraph attention, Stepwise adaptive fusion},
abstract = {Cell localization in medical image analysis is a challenging task due to the significant variation in cell shape, size and color. Existing localization methods continue to tackle these challenges separately, frequently facing complications where these difficulties intersect and adversely impact model performance. In this paper, these challenges are first reframed as issues of feature misalignment between cell images and location maps, which are then collectively addressed. Specifically, we propose a feature alignment model based on a multi-scale hypergraph attention network. The model considers local regions in the feature map as nodes and utilizes a learnable similarity metric to construct hypergraphs at various scales. We then utilize a hypergraph convolutional network to aggregate the features associated with the nodes and achieve feature alignment between the cell images and location maps. Furthermore, we introduce a stepwise adaptive fusion module to fuse features at different levels effectively and adaptively. The comprehensive experimental results demonstrate the effectiveness of our proposed multi-scale hypergraph attention module in addressing the issue of feature misalignment, and our model achieves state-of-the-art performance across various cell localization datasets.}
}
@article{ZHOU2024110218,
title = {Latent Linear Discriminant Analysis for feature extraction via Isometric Structural Learning},
journal = {Pattern Recognition},
volume = {149},
pages = {110218},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110218},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009159},
author = {Jianhang Zhou and Qi Zhang and Shaoning Zeng and Bob Zhang and Leyuan Fang},
keywords = {Feature extraction, Linear discriminant analysis, Latent space, Structure learning, Pattern classification},
abstract = {Linear discriminant analysis (LDA) is one of the most successful feature extraction methods, which projects high-dimensional data to a low-dimensional space with discriminative features. However, there are problems in the existing LDAs: (1) the effect of hidden data is not exploited in LDA, (2) the LDAs cannot preserve the local isometric structure, (3) there is no consideration for structural consistency that unifies the supervised global and unsupervised local information. In this paper, we propose a brand-new LDA method, namely, Latent Linear Discriminant Analysis with Isometric Structural Learning (L2DA-ISL). We formulate LDA to a latent representation framework that considers both the discriminability from observed data and hidden data. Then, we propose isometric structural learning to capture the intrinsic local structural information. Lastly, we establish the concept of structural consistency in LDA framework. Extensive experiments and comparisons show that L2DA-ISL achieves a promising performance with structural consistency and stronger robustness in feature extraction.}
}
@article{CHANG2024110200,
title = {IIOF: Intra- and Inter-feature orthogonal fusion of local and global features for music emotion recognition},
journal = {Pattern Recognition},
volume = {148},
pages = {110200},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110200},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300897X},
author = {Pei-Chun Chang and Yong-Sheng Chen and Chang-Hsing Lee},
keywords = {Music emotion recognition, SincNet, ResNet, Orthogonal fusion},
abstract = {In this paper, we propose intra- and inter-feature orthogonal fusion (IIOF) of local and global features obtained from MS-SincResNet or MS-SSincResNet (a variant of MS-SincResNet) for music emotion recognition (MER). Given a raw waveform of music signal, MS-SincResNet/MS-SSincResNet is first used to learn several 2D representations having different receptive fields and obtain embeddings with time-frequency information from different layers. Then, local and global features are extracted from these embeddings. IIOF consisting of intra-feature OF and inter-feature OF is further employed to integrate both local and global features to obtain a discriminative descriptor for MER. The intra-feature OF is used to enhance the diversity of the global feature, and the inter-feature OF is utilized to reduce redundancies and produce complementary information between local and global features. The experimental results have demonstrated that the representation discriminability can be enhanced by IIOF considering the feature orthogonality. Furthermore, extensive experimental results have shown that the proposed method outperforms other state-of-the-art methods in terms of regression and classification tasks on the well-known MER datasets, including the DEAM dataset and the PMEmo dataset. The codes are available at https://github.com/PeiChunChang/MS-SSincResNet_with_IIOF.}
}
@article{YANG2024110308,
title = {Robust spectral embedded bilateral orthogonal concept factorization for clustering},
journal = {Pattern Recognition},
volume = {150},
pages = {110308},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110308},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000591},
author = {Ben Yang and Jinghan Wu and Yu Zhou and Xuetao Zhang and Zhiping Lin and Feiping Nie and Badong Chen},
keywords = {Concept factorization, Spectral embedding, Correntropy, Clustering},
abstract = {Concept factorization (CF), unlike nonnegative matrix factorization (NMF), can handle data with negative values by approximating the original data with two low-dimensional nonnegative matrices and itself. Nevertheless, existing CF-based methods continue to suffer from the two issues specified as follows: (1) Their effectiveness is reduced by the high degree of factorization freedom and the two-stage mismatch between factorization and category acquisition, and (2) their robustness drops significantly when dealing with complex noise. In response to the aforementioned issues, we propose a robust spectral-embedded bilateral orthogonal concept factorization (RSOCF) model for clustering. It constrains the factor matrices as orthogonal matrices to decrease the freedom and obtain samples’ categories directly after factorization, which can significantly improve clustering effectiveness. Moreover, correntropy is introduced into RSOCF to improve its robustness to complex noise. To optimize the non-convex RSOCF model, a half-quadratic-based algorithm is devised. Numerous experiments demonstrate that RSOCF surpasses other state-of-the-art methods in terms of clustering effectiveness and robustness.}
}
@article{XU2024110256,
title = {A robust circle detector with regionalized radius aid},
journal = {Pattern Recognition},
volume = {149},
pages = {110256},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110256},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000074},
author = {Xianguang Xu and Ronggang Yang and Naige Wang},
keywords = {Circle detection, Least squares fit, Regionalized radius aid, Edge pruning},
abstract = {The detection of circles in geometric shapes is highly valued in computer vision and pattern recognition. Conventionally, the least-squares fitting is sensitive to occlusion or noise and prone to false circles. Therefore, this paper proposes a novel algorithm for robust circle detection using the least-squares fitting method combined with regionalized radius aid on the arc and chord lengths. To reduce edge noise impact, we present an edge pruning method to prune non-curve edge branch ports. Furthermore, we extract arcs based on the inflection points and sharp corners of the approximate line segment. Next, curves that belong to the arcs obtain circle parameters according to the regionalized radius aided the least-squares method. Then, valid circles are obtained by considering two distance deviations to verify the candidate circles. Finally, valid arcs that belong to the same circle are combined and refitted, wherein the most representative arc is used as the basis for the refitting of all arcs. All experiments are conducted on real images from four publicly diverse datasets (one of them is the one we built). The detection results are compared with those of representative state-of-the-art algorithms, and the proposed algorithm has several advantages based on the comparison results: more robust to noise, effective rejection of false circles, and better performance.}
}
@article{WANG2024110234,
title = {Towards fair and personalized federated recommendation},
journal = {Pattern Recognition},
volume = {149},
pages = {110234},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110234},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009317},
author = {Shanfeng Wang and Hao Tao and Jianzhao Li and Xinyuan Ji and Yuan Gao and Maoguo Gong},
keywords = {Federated learning, Fairness, Graph neural network, Personalized recommendation},
abstract = {Recommender systems have gained immense popularity in recent years for predicting users’ interests by learning embeddings. The majority of existing recommendation approaches, represented by graph neural network-based recommendation algorithms, rely on centralized storage of user-item graphs for model learning, which raises privacy issues in the process of collecting and sharing user data. Federated recommendation can mitigate privacy concerns by preventing the server from collecting sensitive data from clients while there still exist unfairness and personalization issues. To address these challenges, we propose a novel framework named Fair and Personalized Federated Recommendation (FPFR). On the client-side, the soft attention mechanism is designed to learn the representation of user/item by combining interaction and attribute information, and the filter network is combined to better characterize user preferences. On the server-side, we cluster users into different groups and learn personalized models for each user. Then, we select representative users from each group to participate in the global model parameters update. Finally, the fairness of federated recommendation is implemented by adding the fairness constraint to recommendation loss. We conduct experiments on five real-world recommendation datasets, and the results demonstrate that the proposed FPFR not only balances group fairness and recommendation accuracy but also improves personalization.}
}
@article{LIU2024110207,
title = {Non-negative Tucker decomposition with graph regularization and smooth constraint for clustering},
journal = {Pattern Recognition},
volume = {148},
pages = {110207},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110207},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009044},
author = {Qilong Liu and Linzhang Lu and Zhen Chen},
keywords = {Non-negative Tucker decomposition, Graph regularization, Randomized algorithm, Clustering},
abstract = {Non-negative Tucker decomposition (NTD) and its graph regularized extensions are the most popular techniques for representing high-dimensional non-negative data, which are typically found in a low-dimensional sub-manifold of ambient space, from a geometric perspective. Therefore, the performance of the graph-based NTD methods relies heavily on the low-dimensional representation of the original data. However, most existing approaches treat the last factor matrix in NTD as a low-dimensional representation of the original data. This treatment leads to the loss of the original data’s multi-linear structure in the low-dimensional subspace. To remedy this defect, we propose a novel graph regularized Lp smooth NTD (GSNTD) method for high-dimensional data representation by incorporating graph regularization and an Lp smoothing constraint into NTD. The new graph regularization term constructed by the product of the core tensor and the last factor matrix in NTD, and it is used to uncover hidden semantics while maintaining the intrinsic multi-linear geometric structure of the data. The addition of the Lp smoothing constraint to NTD may produce a more accurate and smoother solution to the optimization problem. The update rules and the convergence of the GSNTD method are proposed. In addition, a randomized variant of the GSNTD algorithm based on fiber sampling is proposed. Finally, the experimental results on four standard image databases show that the proposed method and its randomized variant have better performance than some other state-of-the-art graph-based regularization methods for image clustering.}
}
@article{YANG2024110223,
title = {Semantic perceptive infrared and visible image fusion Transformer},
journal = {Pattern Recognition},
volume = {149},
pages = {110223},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110223},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009202},
author = {Xin Yang and Hongtao Huo and Chang Li and Xiaowen Liu and Wenxi Wang and Cheng Wang},
keywords = {Infrared image, Visible image, Transformer, Long-range dependency, Local feature, Semantic perceptive, Image fusion},
abstract = {Deep learning based fusion mechanisms have achieved sophisticated performance in the field of image fusion. However, most existing approaches focus on learning global and local features but seldom consider to modeling semantic information, which might result in inadequate source information preservation. In this work, we propose a semantic perceptive infrared and visible image fusion Transformer (SePT). The proposed SePT extracts local feature through convolutional neural network (CNN) based module and learns long-range dependency through Transformer based modules, and meanwhile designs two semantic modeling modules based on Transformer architecture to manage high-level semantic information. One semantic modeling module maps the shallow features of source images into deep semantic, the other learns the deep semantic information in different receptive fields. The final fused results are recovered from the combination of local feature, long-range dependency and semantic feature. Extensive comparison experiments demonstrate the superiority of SePT compare to other advanced fusion approaches.}
}
@article{SHAO2024110202,
title = {A learnable support selection scheme for boosting few-shot segmentation},
journal = {Pattern Recognition},
volume = {148},
pages = {110202},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110202},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008993},
author = {Wenxuan Shao and Hao Qi and Xinghui Dong},
keywords = {Image segmentation, Few-shot segmentation, Support selection, Few-shot learning, Meta learning},
abstract = {Upon reevaluating recent studies of Few-Shot Segmentation (FSS), a key observation is that the random selection of support images is not always the optimal. In this situation, the support images cannot provide the useful guidance for the segmentation task. Therefore, we argue that a similarity-based support selection scheme, which selects support images according to the similarity between the query and candidate support images, is able to boost the performance of an FSS network. To this end, we propose a Siamese Support Selection Network (SSSN) which can be end-to-end trained along with an FSS network. We also leverage the joint utilization of a Convolutional Neural Network (CNN) and a Transformer network on top of a new feature fusion method to further improve the performance. To our knowledge, none of the similarity-based support selection scheme and the dual-stream network have been utilized for the FSS task before. Experimental results show that our FSS approach outperforms its counterparts on three data sets. In particular, the SSSN is able to boost the performance of an FSS network. We believe that these promising results should be due to the ability of the SSSN to select the top similar support images, which are useful for the FSS task.11Code is available at https://indtlab.github.io/projects/SSSN.}
}
@article{ALEXANDRE2024110231,
title = {TriSig: Evaluating the statistical significance of triclusters},
journal = {Pattern Recognition},
volume = {149},
pages = {110231},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110231},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009287},
author = {Leonardo Alexandre and Rafael S. Costa and Rui Henriques},
keywords = {Triclustering, Pattern discovery, Statistical significance, Temporal pattern mining, Multivariate time series data},
abstract = {Tensor data analysis allows researchers to uncover novel patterns and relationships that cannot be obtained from tabular data alone. The information inferred from multi-way patterns can offer valuable insights into disease progression, bioproduction processes, behavioral responses, weather fluctuations, or social dynamics. However, spurious patterns often hamper this process. This work aims at proposing a statistical frame to assess the probability of patterns in tensor data to deviate from null expectations, extending well-established principles for assessing the statistical significance of patterns in tabular data. A principled discussion on binomial testing to mitigate false positive discoveries is entailed at the light of: variable dependencies, temporal associations and misalignments, and multi-hypothesis correction. Results gathered from the application of triclustering algorithms over distinct real-world case studies in biotechnological domains confer validity to the proposed statistical frame while revealing vulnerabilities of reference triclustering searches. The proposed assessment can be incorporated into existing triclustering algorithms to minimize spurious occurrences, rank patterns, and further prune the search space, reducing their computational complexity.}
}
@article{LAI2024110219,
title = {Multi-view robust regression for feature extraction},
journal = {Pattern Recognition},
volume = {149},
pages = {110219},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110219},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009160},
author = {Zhihui Lai and Foping Chen and Jiajun Wen},
keywords = {Image classification, Small-class problem, Linear regression (LR)},
abstract = {Recently, Multi-view Discriminant Analysis (MVDA) has been proposed and achieves good performance in multi-view recognition tasks. However, as an extension of LDA, this method still suffers from the small-class problem and has the sensitivity to outliers. In order to address these drawbacks and achieve better performance on multi-view recognition tasks, we proposed Multi-view Robust Regression (MVRR) for multi-view feature extraction. MVRR is a regression based method that imposes L2,1 norm as the metric of the loss function and the regularization term to improve robustness and obtain jointly sparse projection matrices for effective feature extraction. Moreover, we incorporate an orthogonal matrix to regress the extracted features to their scaled label to avoid the small-class problem. Therefore, MVRR guarantees the projection matrix to break through the restriction of the number of class for solving the small-class problem. We also propose an iterative algorithm to compute the optimal solution of MVRR and the convergence of MVRR is proved. Experiments are conducted on four databases to verify the performance of MVRR and the result illustrates that MVRR is robust on multi-view feature extraction.}
}
@article{KHODAYARISAMGHABADI2024110299,
title = {A Fast Multi-Network K-Dependence Bayesian Classifier for Continuous Features},
journal = {Pattern Recognition},
volume = {150},
pages = {110299},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110299},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000505},
author = {Imaneh Khodayari-Samghabadi and Leyli Mohammad-Khanli and Jafar Tanha},
keywords = {Kernel density estimation, B-spline functions, Probability density estimation, Continuous features, Bayesian network classifier},
abstract = {One of the Bayesian network classifiers widely used in the classification is K-dependence Bayesian (KDB). However, most of the KDB classifiers build a single network on a class variable without considering dependencies between features in each class. Moreover, many KDB classifiers need the discretization process to handle continuous features. This paper aims to propose a fast Multi-Network K-Dependence Bayesian (MNKDB) classifier for continuous features. According to this aim, we propose a non-parametric approach that efficiently identifies dependencies between continuous features in each class with a low computational cost and without discretizing continuous features. The results indicate that the MNKDB classifier is more accurate than the state-of-the-art KDB classifiers, especially for datasets with more than three classes. The MNKDB classifier not only decreases the classification time but also deals with continuous variables without discretizing them. The results for K=2 show that the MNKDB classifier is 36.5, 31.8, and 14.2 times faster and 4.13%, 5.15%, and 5.48% more accurate than the state-of-the-art FKDB (Flexible KDB), KMM-KDB (Kernel Mixture Model based on KDB), and SKDB (Scalable KDB) classifiers, respectively.}
}
@article{WANG2024110311,
title = {Pose-robust personalized facial expression recognition through unsupervised multi-source domain adaptation},
journal = {Pattern Recognition},
volume = {150},
pages = {110311},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110311},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000621},
author = {Shangfei Wang and Yanan Chang and Qiong Li and Can Wang and Guoming Li and Meng Mao},
keywords = {Facial expression recognition, Pose-robust, Personalized, Multi-source domain adaptation},
abstract = {Pose-robust personalized facial expression recognition is rather challenging, as facial expressions are subject-related and pose-dependent. Multi-source domain adaptation tries to leverage knowledge from multiple source domains to boost the performance of the target domain. For pose-robust personalized facial expression recognition, the images of the source domain are from multiple sources since the images are under different poses. Thus, in this paper, we propose a novel unsupervised multi-source domain adaptation framework for pose-robust personalized facial expression recognition. The proposed framework consists of five components: a source encoder, a target encoder, an expression classifier, a view discriminator, and a domain discriminator. The source encoder and target encoder learn facial representations from facial images in the training and testing sets, respectively. The expression classifier recognizes expressions from the learned representations. The view discriminator classifies poses. The domain discriminator distinguishes the learned representations of the source domain from those of the target domain. The source encoder works cooperatively with the expression classifier and adversarially with the view discriminator. The target encoder aims to learn domain robust representations and fool the domain discriminator, while the domain discriminator tries to distinguish between the source and target domains. Through adversarial learning, the distribution of the learned representations from the source domain converges to that from the target domain. Thus, the feature representation extracted by the target encoder is pose-invariant and target subject-specific. Experimental results demonstrate the superiority of the proposed method compared to related works.}
}
@article{ZHAO2024110259,
title = {Promote knowledge mining towards open-world semi-supervised learning},
journal = {Pattern Recognition},
volume = {149},
pages = {110259},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110259},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000104},
author = {Tianhao Zhao and Yutian Lin and Yu Wu and Bo Du},
keywords = {Open world semi-supervised learning, Representation learning, Novel class discovery},
abstract = {Deep learning models often rely on a large number of labeled data to achieve good performance. However, labeling such a large number of data requires exhaustive labor efforts. In recent years, a pivotal research direction is to generalize deep learning models to learn from not only unlabeled data of seen classes but also data of novel classes which are not predefined, known as open-world semi-supervised learning (open-world SSL). Existing works tackled this challenging task by manually designing different optimizations for labeled/unlabeled data and seen/novel classes. In this paper, we propose a simple unified framework that can be applied to all images and all classes in the same form. In this framework, we exploit the Sinkhorn–Knopp algorithm to overcome the overconfidence issue of pseudo labels on seen classes and thus lead to a more balanced distribution of seen and novel classes. To reduce the intra-class variance and avoid model collapse, we take as input two different views of an image and regard one’s prediction as the other’s pseudo label. However, in a unified framework, the model converges much faster on the seen classes than those novel classes. To balance them and encourage knowledge transfer from seen classes to novel classes, we further propose mixing up any two training images during our unified optimization. Extensive experiments on three benchmarks (i.e., CIFAR-10, CIFAR-100, and ImageNet-100) show that our unified framework achieved comparable performance with existing state-of-the-art methods. Our code is available on https://github.com/happytianhao/OWSSL.}
}
@article{XU2024110283,
title = {Large-scale continual learning for ancient Chinese character recognition},
journal = {Pattern Recognition},
volume = {150},
pages = {110283},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110283},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000347},
author = {Yue Xu and Xu-Yao Zhang and Zhaoxiang Zhang and Cheng-Lin Liu},
keywords = {Continual learning, Class-incremental learning, Convolutional prototype network, Character recognition, Ancient Chinese characters},
abstract = {Ancient Chinese character recognition is a challenging problem in the field of pattern recognition. It is difficult to collect all character classes during the training stage due to the numerous classes of ancient Chinese characters and the likelihood of discovering new characters over time. A solution to address this problem is continual learning. However, most continual learning methods are not well-suited for large-scale applications, making them insufficient for solving the problem of ancient Chinese character recognition. Although saving raw data for old classes is a good approach for continual learning to address large-scale problems, it is often infeasible due to the lack of data accessibility in reality. To solve these problems, we propose a large-scale continual learning framework based on the convolutional prototype network (CPN), which does not save raw data for old classes. In this paper, several basic strategies have been proposed for the initial training stage to enhance the feature extraction ability and robustness of the network, which can improve the performance of the model in continual learning. In addition, we propose two practical methods in varying feature space (parameters of feature extractor are changeable) and fixed feature space (parameters of feature extractor are fixed), which enable the model to carry out large-scale continual learning. The proposed method does not save the raw data of old classes and enables simultaneous classification of all existing classes without knowing the incremental batch number. Experiments on the CASIA-AHCDB dataset with 5000 character classes demonstrate the effectiveness and superiority of the proposed method.}
}
@article{SHEN2024110194,
title = {SSPNet: Scale and spatial priors guided generalizable and interpretable pedestrian attribute recognition},
journal = {Pattern Recognition},
volume = {148},
pages = {110194},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110194},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008919},
author = {Jifeng Shen and Teng Guo and Xin Zuo and Heng Fan and Wankou Yang},
keywords = {Pedestrian attribute recognition, Attribute localization, Prior knowledge, Generalizability, Interpretability},
abstract = {Global feature based Pedestrian Attribute Recognition (PAR) models are often poorly localized when using Grad-CAM for attribute response analysis, which has a significant impact on the interpretability, generalizability and performance. Previous researches have attempted to improve generalization and interpretation through meticulous model design, yet they often have neglected or underutilized effective prior information crucial for PAR. To this end, a novel Scale and Spatial Priors Guided Network (SSPNet) is proposed for PAR, which is mainly composed of the Adaptive Feature Scale Selection (AFSS) and Prior Location Extraction (PLE) modules. The AFSS module learns to provide reasonable scale prior information for different attribute groups, allowing the model to focus on different levels of feature maps with varying semantic granularity. The PLE module reveals potential attribute spatial prior information, which avoids unnecessary attention on irrelevant areas and lowers the risk of model over-fitting. More specifically, the scale prior in AFSS is adaptively learned from different layers of feature pyramid with maximum accuracy, while the spatial priors in PLE can be revealed from part feature with different granularity (such as image blocks, human pose keypoint and sparse sampling points). Besides, a novel IoU based attribute localization metric is proposed for Weakly-supervised Pedestrian Attribute Localization (WPAL) based on the improved Grad-CAM for attribute response mask. The experimental results on the intra-dataset and cross-dataset evaluations demonstrate the effectiveness of our proposed method in terms of mean accuracy (mA). Furthermore, it also achieves superior performance on the PCS dataset for attribute localization in terms of IoU. Code will be released at https://github.com/guotengg/SSPNet.}
}
@article{CHEN2024110295,
title = {Multi-Source Domain Adaptation with Mixture of Joint Distributions},
journal = {Pattern Recognition},
volume = {149},
pages = {110295},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110295},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000463},
author = {Sentao Chen},
keywords = {Statistical machine learning, Multi-source domain adaptation, Mixture joint distribution, Kernel method},
abstract = {The goal of Multi-Source Domain Adaptation (MSDA) is to train a model (e.g., neural network) with minimal target loss, utilizing training data from multiple source domains (source joint distributions) and a target domain (target joint distribution). The challenge in this problem is that the multiple source joint distributions are different from the target joint distribution. In this paper, we develop a theory that shows a neural network’s target loss is upper bounded by both its source mixture loss (i.e., the loss concerning the source mixture joint distribution) and the Pearson χ2 divergence between the source mixture joint distribution and the target joint distribution. Here, the source mixture joint distribution is the mixture of multiple source joint distributions with mixing weights. Accordingly, we propose an algorithm that optimizes both the mixing weights and the neural network to minimize the estimated source mixture loss and the estimated Pearson χ2 divergence. To estimate the Pearson χ2 divergence, we rewrite it as the maximal value of a quadratic functional, exploit a linear-in-parameter function as the functional’s input, and solve the resultant optimization problem with an analytic solution. This analytic solution allows us to explicitly express the estimated divergence as a loss of the mixing weights and the network’s feature extractor. Finally, we conduct experiments on popular image classification datasets, and the results show that our algorithm statistically outperforms the comparison algorithms. PyTorch code is available at https://github.com/sentaochen/Mixture-of-Joint-Distributions.}
}
@article{WANG2024110247,
title = {Pan-sharpening via intrinsic decomposition knowledge distillation},
journal = {Pattern Recognition},
volume = {149},
pages = {110247},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110247},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009445},
author = {Jiaming Wang and Qiang Zhou and Xiao Huang and Ruiqian Zhang and Xitong Chen and Tao Lu},
keywords = {Knowledge distillation, Pan-sharpening, Intrinsic decomposition, Image fusion},
abstract = {Existing deep-learning-based pan-sharpening strategies mainly involve the fusion of panchromatic and multispectral (MS) information at both the pixel and feature levels. In this paper, we hypothesize that the MS image can be expressed as the multiplication of reflectance and illumination components, and that the reflection components of low-resolution (LR) MS and high-resolution (HR) MS images are invariant. Here, the spectral reflection component can effectively describe the spectral response of an object, while the illumination component can effectively describe its texture. Based on this hypothesis, we propose a novel and concise pan-sharpening framework called intrinsic decomposition knowledge distillation. Specifically, the teacher network decomposes the HR MS image into reflectance and illumination components, which are then combined in the student network with the reflectance component and the enhanced illumination component from LR MS to reconstruct the pan-sharpened image. To approximate the component distributions from the teacher network, we introduce a novel three-stage knowledge distillation strategy that can transfer knowledge about the relationships between components and constrain the student network. Our quantitative and qualitative comparisons demonstrate the reasonableness of our hypothesis and the effectiveness of our proposed method in significantly improving perception quality.}
}
@article{THEODON2024110255,
title = {A stochastic model based on Gaussian random fields to characterize the morphology of granular objects},
journal = {Pattern Recognition},
volume = {149},
pages = {110255},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110255},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000062},
author = {L. Théodon and C. Coufort-Saudejaud and J. Debayle},
keywords = {3D modeling, Aggregate, Agglomerate, Fourier descriptor, Gaussian random fields, Image analysis, Morphological characterization},
abstract = {The geometrical modeling of granular objects is a complex challenge that exists in many scientific fields, such as the modeling of granular materials or rocks and coarse aggregates with applications in civil, mechanical, and chemical engineering. In this paper, a model called SPHERE (Stochastic Process for Highly Effective Radial Expansion) is proposed, which is based on the deformation of an ellipsoid mesh using multiple 3D Gaussian random fields. The model is designed to be flexible (full control over 2D and 3D morphological properties of granular objects), ultra-fast (over 1000 aggregates in less than 5 s), and independent of the mesh and base shape used (as long as it is a star-shaped object). The flexibility of the model and its ability to reflect real data is illustrated using images of latex nanoparticle aggregates. Using 2D measurements on images from a morphogranulometer, a method based on the SPHERE model is proposed to estimate the 3D morphological properties of aggregates. A multiscale optimization process is applied, in particular using a partial reconstruction of 2D shapes from elliptic Fourier descriptors, in order to best reproduce the shape, angularity and texture of the aggregates using the SPHERE model. Validation of the method on 3D printed data shows relative errors of less than 3% for all measured 2D and 3D morphological characteristics, and validation on a population of synthetic objects shows relative errors of less than 6%. The results are compared and discussed with those obtained using other models based on overlapping spheres and show consistency with previous work. Finally, suggestions for improvement are given.}
}
@article{SONG2024110235,
title = {Closed-loop unified knowledge distillation for dense object detection},
journal = {Pattern Recognition},
volume = {149},
pages = {110235},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110235},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009329},
author = {Yaoye Song and Peng Zhang and Wei Huang and Yufei Zha and Tao You and Yanning Zhang},
keywords = {Triple parallel distillation, Hierarchical re-weighting attention distillation, Dense object detection, Closed-loop unified},
abstract = {Most of knowledge distillation methods for object detection are feature-based and have achieved competitive results. However, only distillating in feature imitation part does not take full advantage of more sophisticated detection head design for object detection, especially dense object detection. In this paper, a triple parallel distillation (TPD) is proposed which can efficiently transfer all the output response in detection head from teacher to student. Moreover, to overcome the drawback of simply combining the feature-based with the response-based distillation with limited effect enhancement. A hierarchical re-weighting attention distillation (HRAD) is proposed to make student learn more than the teacher in feature information, as well as reciprocal feedback between the classification-IoU joint representation of detection head and the attention-based feature. By jointly interacting the benefits of TPD and HRAD, a closed-loop unified knowledge distillation for dense object detection is proposed, which makes the feature-based and response-based distillation unified and complementary. Experiments on different benchmark datasets have shown that the proposed work is able to outperform other state-of-the-art distillation methods for dense object detection on both accuracy and robustness.}
}
@article{CAI2024110228,
title = {Self-adaptive subspace representation from a geometric intuition},
journal = {Pattern Recognition},
volume = {149},
pages = {110228},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110228},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009251},
author = {Lipeng Cai and Jun Shi and Shaoyi Du and Yue Gao and Shihui Ying},
keywords = {Subspace learning, Grassmannian manifold, Geometric model, Intrinsic algorithm},
abstract = {In this paper, we address the unsupervised subspace learning task from a geometric intuition, which offers a direct understanding of the data representation. First, we consider all subspaces of the Euclidean space as a series of graded Grassmannian manifolds, and represent them by orbits of rotation group action. Then, we reformulate the unsupervised subspace learning by a least square problem with respect to rotation and projection operator. Second, we introduce a low-rank regularization to obtain a low-dimensional and robust subspace representation. Then, the model is translated into a minimization problem on the Grassmannian manifold. By dividing the model into two subproblems of solving the optimal rotation and subspace dimension, we design an alternating iteration strategy, where the locally geodesic structure of the rotation group and the unconstrained quadratic 0-1 programming are utilized. Finally, we apply the proposed method to the image classification problem on five benchmark datasets and compare it with nine state-of-the-art methods. Numerical results show that our proposed method has better feature representation ability and almost achieves the best performance in terms of classification accuracy and robustness.}
}
@article{LIANG2024110216,
title = {Dynamic semantic structure distillation for low-resolution fine-grained recognition},
journal = {Pattern Recognition},
volume = {148},
pages = {110216},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110216},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009135},
author = {Mingjiang Liang and Shaoli Huang and Wei Liu},
keywords = {Low-resolution, Fine-grained recognition, Image classification, Distillation},
abstract = {Low-resolution images are ubiquitous in real applications such as surveillance and mobile photography. However, existing fine-grained approaches usually suffer catastrophic failures when dealing with low-resolution inputs. This is because their learning strategy inherently depends on the semantic structure of the pre-trained model, resulting in poor robustness and generalization. To mitigate this limitation, we propose a dynamic semantic structure distillation learning framework. Our method first facilitates knowledge distillation of diverse semantic structures by perturbing the composition of semantic components and then utilizes a decoupled distillation objective to prevent the loss of primary semantic part relation knowledge. We evaluate our proposed approach on two knowledge distillation tasks: high-to-low resolution and large-to-small model. The experimental results show that our proposed approach significantly outperforms existing methods in low-resolution fine-grained image classification tasks. This indicates that it can effectively distill knowledge from high-resolution teacher models to low-resolution student models. Furthermore, we demonstrate the effectiveness of our approach in general image classification and standard knowledge distillation tasks.}
}
@article{XIE2024110240,
title = {Network characteristics adaption and hierarchical feature exploration for robust object recognition},
journal = {Pattern Recognition},
volume = {149},
pages = {110240},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110240},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009378},
author = {Weicheng Xie and Cheng Luo and Gui Wang and Linlin Shen and Zhihui Lai and Siyang Song},
keywords = {Robust object recognition, Attention-based dropout, Adaptive characteristics, Hierarchically-salient features},
abstract = {Recent advances in deep networks have achieved appealing performances on object recognition tasks, due to their robust feature learning abilities. Besides the generated deep features, other network characteristics, e.g., inter-layer weight matrix and their back-propagated derivatives, may behave complementarily in feature learning in terms of generalization and robustness performances. However, characteristics adaptivity to different databases is not well studied. Meanwhile, current algorithms are apt to explore the most salient features for better generalization performance, while the hierarchically-salient features that may be beneficial for network robustness are not fully explored. Thus, we propose an attention module to make network characteristics adaptive to different training tasks, which can be further combined with the dynamic dropout algorithm to suppress salient neurons to explore more SndMS (Second Most Salient) features for robust recognition. The proposed algorithm has two main merits. First, the complementarity of network characteristics is taken into account when conducting training on different databases; Second, with the exploration of more SndMS neurons for hierarchically-salient feature representation learning, the network robustness against adversarial perturbations or fine-grained differences can be enhanced. The extensive experiments on seven public databases show that the proposed attention-based dropout largely improves the network robustness, without compromising the generalization performance, compared with related variants and state-of-the-art (SOTA) algorithms. Algorithm codes are available at https://github.com/lingjivoo/ACAD.}
}
@article{HU2024110212,
title = {Mathematical formula detection in document images: A new dataset and a new approach},
journal = {Pattern Recognition},
volume = {148},
pages = {110212},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110212},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009093},
author = {Kai Hu and Zhuoyao Zhong and Lei Sun and Qiang Huo},
keywords = {Mathematical formula detection, Dataset, Instance segmentation, Relation extraction},
abstract = {Most of existing mathematical formula detectors focus on detecting formula entities through object detection or instance segmentation techniques. However, these methods often fail to convey complete messages due to the absence of the contextual and layout information of mathematical formulas. For a more comprehensive understanding of mathematical formulas in document images, it is preferable to detect logical formula blocks that include one or multiple formula entities arranged in their natural reading order. These logical formula blocks enable the transmission of complete contextual messages of mathematical formulas and aid in the reconstruction of layout information of the document images, resulting in a more accurate mathematical formula detection. In this paper, we present a novel perspective on mathematical formula detection by framing it as a joint task of formula entity detection and formula relation extraction for identifying logical formula blocks. To this end, we introduce a new, large-scale dataset, called ArxivFormula, that includes well-annotated formula entity bounding boxes and formula relationships. We also propose a new approach, called FormulaDet, to address these two sub-tasks simultaneously. FormulaDet first employs a dynamic convolution-based formula entity detector, named DynFormula, to detect formula entities. It then uses a multi-modal transformer-based relation extraction method, named RelFormer, to group these detected formula entities into logical formula blocks. Extensive experiments on standard benchmarks in this field and the proposed dataset demonstrate that our FormulaDet can achieve significantly improved performance on formula entity detection and formula relation extraction compared to previous state-of-the-art methods. The joint detection and relation extraction approach provides a more thorough understanding of mathematical formulas in document images and effectively supports downstream tasks such as document layout analysis and scientific document digitization. The ArxivFormula dataset is publicly available at https://github.com/microsoft/ArxivFormula.}
}
@article{LYU2024110290,
title = {MCPNet: Multi-space color correction and features prior fusion for single-image dehazing in non-homogeneous haze scenarios},
journal = {Pattern Recognition},
volume = {150},
pages = {110290},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110290},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000414},
author = {Zhiyu Lyu and Yan Chen and Yimin Hou},
keywords = {Image dehazing, Image prior, CNN, Color space feature, Color correction},
abstract = {When capturing images in non-homogeneous haze conditions, the degree of haze impact varies across the scene, and the information in certain parts of the scene are lost, making the scene nearly invisible. However, most existing dehazing convolutional neural networks (CNNs) are designed for homogeneous haze and do not consider the challenge of feature extraction arising from non-homogeneous haze distribution. Moreover, these networks primarily rely on the RGB color space for feature extraction, often fail to extract color and detail feature effectively, resulting in color distortion and loss of details in the output. To tackle this problem, we introduce image priors in the YCbCr and HSV color spaces, proposing a novel Multiple Color Space Prior Network (MCPNet) to enhance the dehazing performance specifically for non-homogeneous hazy images, while simultaneously correcting the color, preserving the visual quality of the output. Leveraging image priors, we designed two parallel sub-networks to extract color and detail features from the YCbCr and HSV color spaces. Moreover, to capitalize on these features and incorporate them effectively into the dehazed image, we introduce a Comprehensive Fusion Module (CFM). This module judiciously takes into account both the fusion of multiscale features and the interrelation among channels to optimize feature fusion. By employing a dual network architecture coupled with the CFM, our model proficiently amalgamates and exploits the mined features, accurately restoring the color and detail information of the image, especially for images containing non-homogeneous haze. Extensive experimental highlight the effectiveness of our model in addressing homogeneous and non-homogeneous hazy images, concurrently preserving the visual appeal of the dehazed outcomes. When compared with other SOTA models, our MCPNet demonstrates superior results in dehazing.}
}
@article{LIU2024110274,
title = {Unbiased and augmentation-free self-supervised graph representation learning},
journal = {Pattern Recognition},
volume = {149},
pages = {110274},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110274},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000256},
author = {Ruyue Liu and Rong Yin and Yong Liu and Weiping Wang},
keywords = {Self-supervised, Representation learning, GNN, Pseudo-homology},
abstract = {Graph Contrastive Learning (GCL) is a promising self-supervised method for learning node representations that combines graph convolutional networks (GCN) and contrastive learning. However, existing GCL methods heavily rely on graph structure data and augmentation schemes to learn invariant representations between different augmentation views. This can be problematic as the performance of GCNs may deteriorate when noisy connections are present in the original graph structure. Additionally, there is limited knowledge on how to significantly augment graphs without altering their labels. To address these issues, we propose a novel method called Unbiased and Augmentation-Free Self-Supervised Graph Contrastive Learning (USAF-GCL). We design graph learners and post-processing schemes to improve the structure of the original graph. Instead of using augmentation schemes, we generate contrastive views using global and local semantics. To ensure consistency between embedding similarity and original feature similarity, we introduce pseudo-homology to maximize the mutual information between predicted and true labels. Furthermore, we theoretically demonstrate that pseudo-homology maximization can enhance the upper bound of mutual information between predicted and true labels. USAF-GCL offers several advantages over existing GCL methods. Firstly, it uses an unbiased graph structure to reduce the impact of noise on model performance. Secondly, it saves computational resources by eliminating complex data expansion. Lastly, it integrates structural information, neighborhood information, and the consistency of embeddings and features in graph representation learning, effectively improving model performance. Extensive experiments on eight benchmark datasets confirm the remarkable effectiveness and efficiency of USAF-GCL.}
}
@article{LI2024110258,
title = {Learning adversarial semantic embeddings for zero-shot recognition in open worlds},
journal = {Pattern Recognition},
volume = {149},
pages = {110258},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110258},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000098},
author = {Tianqi Li and Guansong Pang and Xiao Bai and Jin Zheng and Lei Zhou and Xin Ning},
keywords = {Zero-Shot Learning (ZSL), Open-Set Recognition (OSR), Zero-Shot Open-Set Recognition (ZS-OSR)},
abstract = {Zero-Shot Learning (ZSL) focuses on classifying samples of unseen classes with only their side semantic information presented during training. It cannot handle real-life, open-world scenarios where there are test samples of unknown classes for which neither samples (e.g., images) nor their side semantic information is known during training. Open-Set Recognition (OSR) is dedicated to addressing the unknown class issue, but existing OSR methods are not designed to model the semantic information of the unseen classes. To tackle this combined ZSL and OSR problem, we consider the case of “Zero-Shot Open-Set Recognition” (ZS-OSR), where a model is trained under the ZSL setting but it is required to accurately classify samples from the unseen classes while being able to reject samples from the unknown classes during inference. We perform large experiments on combining existing state-of-the-art ZSL and OSR models for the ZS-OSR task on four widely used datasets adapted from the ZSL task, and reveal that ZS-OSR is a non-trivial task as the simply combined solutions perform badly in distinguishing the unseen-class and unknown-class samples. We further introduce a novel approach specifically designed for ZS-OSR, in which our model learns to generate adversarial semantic embeddings of the unknown classes to train an unknowns-informed ZS-OSR classifier. Extensive empirical results show that our method 1) substantially outperforms the combined solutions in detecting the unknown classes while retaining the classification accuracy on the unseen classes and 2) achieves similar superiority under generalized ZS-OSR settings. Our code is available at https://github.com/lhrst/ASE.}
}
@article{YANG2024110204,
title = {Investigating the effectiveness of data augmentation from similarity and diversity: An empirical study},
journal = {Pattern Recognition},
volume = {148},
pages = {110204},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110204},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009019},
author = {Suorong Yang and Suhan Guo and Jian Zhao and Furao Shen},
keywords = {Data augmentation, Interpretability, Generalization, Deep learning, Image classification},
abstract = {Data augmentation has emerged as a widely adopted technique for improving the generalization capabilities of deep neural networks. However, evaluating the effectiveness of data augmentation methods solely based on model training is computationally demanding and lacks interpretability. Moreover, the absence of quantitative standards hinders our understanding of the underlying mechanisms of data augmentation approaches and the development of novel techniques. To this end, we propose interpretable quantitative measures that decompose the effectiveness of data augmentation methods into two key dimensions: similarity and diversity. The proposed similarity measure describes the overall similarity between the original and augmented datasets, while the diversity measure quantifies the divergence in inherent complexity between the original and augmented datasets in terms of categories. Importantly, our proposed measures are model training-agnostic, ensuring efficiency in their calculation. Through experiments on several benchmark datasets, including MNIST, CIFAR-10, CIFAR-100, and ImageNet, we demonstrate the efficacy of our measures in evaluating the effectiveness of various data augmentation methods. Furthermore, although the proposed measures are straightforward, they have the potential to guide the design and parameter tuning of data augmentation techniques and enable the validation of data augmentation methods’ efficacy before embarking on large-scale model training.}
}
@article{ZHENG2024110250,
title = {Exploring low-resource medical image classification with weakly supervised prompt learning},
journal = {Pattern Recognition},
volume = {149},
pages = {110250},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110250},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000013},
author = {Fudan Zheng and Jindong Cao and Weijiang Yu and Zhiguang Chen and Nong Xiao and Yutong Lu},
keywords = {Medical image classification, Weakly supervised learning, Prompt learning, Few-shot learning, Zero-shot learning},
abstract = {Most advances in medical image recognition supporting clinical auxiliary diagnosis meet challenges due to the low-resource situation in the medical field, where annotations are highly expensive and professional. This low-resource problem can be alleviated by leveraging the transferable representations of large-scale pre-trained vision-language models like CLIP. After being pre-trained using large-scale unlabeled medical images and texts (such as medical reports), the vision-language models can learn transferable representations and support flexible downstream clinical tasks such as medical image classification via relevant medical text prompts. However, existing pre-trained vision-language models require domain experts (clinicians) to carefully design the medical text prompts based on different datasets when applied to specific medical image tasks, which is extremely time-consuming and greatly increases the burden on clinicians. To address this problem, we propose a weakly supervised prompt learning method MedPrompt for automatically generating medical prompts, which includes an unsupervised pre-trained vision-language model and a weakly supervised prompt learning model. The unsupervised pre-trained vision-language model adopts large-scale medical images and texts for pre-training, utilizing the natural correlation between medical images and corresponding medical texts without manual annotations. The weakly supervised prompt learning model only utilizes the classes of images in the dataset to guide the learning of the specific class vector in the prompt, while the learning of other context vectors in the prompt does not require any manual annotations for guidance. To the best of our knowledge, this is the first model to automatically generate medical prompts. With the assistance of these prompts, the pre-trained vision-language model can be freed from the strong expert dependency of manual annotation and manual prompt design, thus achieving end-to-end, low-cost medical image classification. Experimental results show that the model using our automatically generated prompts outperforms all its hand-crafted prompts counterparts in full-shot learning on all four datasets, and achieves superior accuracy on zero-shot image classification and few-shot learning in three of the four medical benchmark datasets and comparable accuracy in the remaining one. In addition, the proposed prompt generator is lightweight and therefore has the potential to be embedded into any network architecture.}
}
@article{ZHANG2024110227,
title = {Confidence-based dynamic cross-modal memory network for image aesthetic assessment},
journal = {Pattern Recognition},
volume = {149},
pages = {110227},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110227},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300924X},
author = {Xiaodan Zhang and Yuan Xiao and Jinye Peng and Xinbo Gao and Bo Hu},
keywords = {Image aesthetic assessment (IAA), Memory-based network, Dynamical multi-modal fusion},
abstract = {Image aesthetic assessment (IAA) aims to design algorithms that can make human-like aesthetic decisions. Due to its high subjectivity and complexity, visual information alone is limited to fully predict the aesthetic quality of an image. More and more researchers try to use complementary information from user comments. However, user comments are not always available due to various technical and practical reasons. Therefore, it is necessary to find a way to reconstruct the missing textual information for aesthetic prediction with visual information only. This paper solves this problem by proposing a Confidence-based Dynamic Cross-modal Memory Network (CDCM-Net). Specifically, the proposed CDCM-Net consists of two key components: Visual and Textual Memory (VTM) network and Confidence-based Dynamical Multi-modal Fusion module (CDMF). VTM is based on the key–value memory network. It consists of a visual key memory and a textual value memory. The visual key memory learns the visual information. While the textual value memory learns to remember the textual feature and align them with the corresponding visual features. During inference, textual information can be reconstructed using only visual features. Furthermore, a CDMF module is introduced to perform trustworthy fusion. CDMF evaluates modality-level informativeness and then dynamically integrates reliable information. Extensive experiments are performed to demonstrate the superiority of the proposed method.}
}
@article{KE2024110199,
title = {U-Transformer-based multi-levels refinement for weakly supervised action segmentation},
journal = {Pattern Recognition},
volume = {149},
pages = {110199},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110199},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008968},
author = {Xiao Ke and Xin Miao and Wenzhong Guo},
keywords = {Action segmentation, U-Transformer, Timestamp supervision, Multi-stages refinement},
abstract = {Action segmentation is a research hotspot in human action analysis, which aims to split videos into segments of different actions. Recent algorithms have achieved great success in modeling based on temporal convolution, but these methods weight local or global timing information through additional modules, ignoring the existing long-term and short-term information connections between actions. This paper proposes a U-Transformer structure based on multi-level refinement, introduces neighborhood attention to learn the neighborhood information of adjacent frames, and aggregates video frame features to effectively process long-term sequence information. Then a loss optimization strategy is proposed to smooth the original classification effect and generate a more accurate calibration sequence by introducing a pairing similarity optimization method based on deep feature learning. In addition, we propose a timestamp supervised training method to generate complete information for actions based on pseudo-label predictions for action boundary predictions. Experiments on three challenging action segmentation datasets, 50Salads, GTEA, and Breakfast, show that our model performs state-of-the-art models, and our weakly supervised model also performs comparably to fully supervised performance.}
}
@article{WANG2024110286,
title = {MPG-LSD: A high-quality line segment detector based on multi-scale perceptual grouping},
journal = {Pattern Recognition},
volume = {149},
pages = {110286},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110286},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000372},
author = {Zikai Wang and Baojiang Zhong and Xueyuan Chen and Hangjia Zheng},
keywords = {Line segment detection, Perceptual grouping, Multi-scale, Optimal scale, Line segment refinement, Line segment validation},
abstract = {Existing methods that rely on a single Gaussian scale to detect line segments could yield poor line continuity and inferior orientation and position accuracy due to insufficient suppression of quantization noise inherent in digital images. To address this fundamental issue, a novel multi-scale perceptual grouping-based line segment detector (MPG-LSD) is proposed in this paper. Our multi-scale perceptual grouping is developed to identify and aggregate collinear pixels that have similar gradient orientations for producing line segment candidates, not only over the input image but also across a set of Gaussian scales. Multi-scale line segment refinement and validation are then further developed and implemented to produce the final detection result, which delivers high quality in terms of line continuity, orientation and position accuracy. To enrich the evaluation of line segment detection performance, a new dataset consisting of high-resolution and natural noise-corrupted images with line segment annotations is constructed. Extensive experimental results show that our proposed MPG-LSD can outperform the current state-of-the-arts by a large margin.}
}
@article{LIU2024110175,
title = {3D human pose estimation with single image and inertial measurement unit (IMU) sequence},
journal = {Pattern Recognition},
volume = {149},
pages = {110175},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110175},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008725},
author = {Liujun Liu and Jiewen Yang and Ye Lin and Peixuan Zhang and Lihua Zhang},
keywords = {Artificial intelligence, 3D human pose estimation, Cross modals fusion, Light weight model},
abstract = {Three-dimensional human pose estimation plays an important role in the field of computer vision, such as in healthcare, sports, activity recognition, motion capture, and augmented reality. However, monocular image or video based methods are sensitive to occlusions, while multi-view methods usually require enormous computation resources. Currently, inertial measurement unit (IMU)-based methods have begun to overcome the occlusion problem and can potentially achieve real-time inference. Yet, they still suffer from insufficient precision and scale drift error over time. In this paper, we propose a novel, efficient framework to fuse a single image with temporal sequence from IMU sensors to estimate human poses and reconstruct human shapes. Our method achieves 46 mm Mean Per Joint Positional Error (MPJPE) on the Total Capture dataset with 30 frames time segment, and surpasses state-of-the-art pure IMU-based methods. Moreover, in comparison with other vision-based methods, the proposed method shows great advantage in reducing computing floating point operations per second (FLOPS) quota while still achieving competitive estimation precision. Our method achieves 74 FPS on an IPhone 12 for offline processing. In addition, our method can easily be generalized for outdoor cases.}
}
@article{TROSTEN2024110229,
title = {Leveraging tensor kernels to reduce objective function mismatch in deep clustering},
journal = {Pattern Recognition},
volume = {149},
pages = {110229},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110229},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009263},
author = {Daniel J. Trosten and Sigurd Løkse and Robert Jenssen and Michael Kampffmeyer},
keywords = {Tensor kernels, Unsupervised companion objectives, Objective function mismatch, Deep clustering},
abstract = {Objective Function Mismatch (OFM) occurs when the optimization of one objective has a negative impact on the optimization of another objective. In this work we study OFM in deep clustering, and find that the popular autoencoder-based approach to deep clustering can lead to both reduced clustering performance, and a significant amount of OFM between the reconstruction and clustering objectives. To reduce the mismatch, while maintaining the structure-preserving property of an auxiliary objective, we propose a set of new auxiliary objectives for deep clustering, referred to as the Unsupervised Companion Objectives (UCOs). The UCOs rely on a kernel function to formulate a clustering objective on intermediate representations in the network. Generally, intermediate representations can include other dimensions, for instance spatial or temporal, in addition to the feature dimension. We therefore argue that the naïve approach of vectorizing and applying a vector kernel is suboptimal for such representations, as it ignores the information contained in the other dimensions. To address this drawback, we equip the UCOs with structure-exploiting tensor kernels, designed for tensors of arbitrary rank. The UCOs can thus be adapted to a broad class of network architectures. We also propose a novel, regression-based measure of OFM, allowing us to accurately quantify the amount of OFM observed during training. Our experiments show that the OFM between the UCOs and the main clustering objective is lower, compared to a similar autoencoder-based model. Further, we illustrate that the UCOs improve the clustering performance of the model, in contrast to the autoencoder-based approach. The code for our experiments is available at https://github.com/danieltrosten/tk-uco.}
}
@article{WEN2024110205,
title = {From heavy rain removal to detail restoration: A faster and better network},
journal = {Pattern Recognition},
volume = {148},
pages = {110205},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110205},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009020},
author = {Yuanbo Wen and Tao Gao and Jing Zhang and Kaihao Zhang and Ting Chen},
keywords = {Single image deraining, Detail reconstruction, Dual-stage network, Dilated convolution, Pixel-wise attention},
abstract = {The profound accumulation of precipitation during intense rainfall events can markedly degrade the quality of images, leading to the erosion of textural details. Despite the improvements observed in existing learning-based methods specialized for heavy rain removal, it is discerned that a significant proportion of these methods tend to overlook the precise reconstruction of the intricate details. In this work, we introduce a simple dual-stage progressive enhancement network, denoted as DPENet, aiming to achieve effective deraining while preserving the structural accuracy of rain-free images. This approach comprises two key modules, a rain streaks removal network (R2Net) focusing on accurate rain removal, and a details reconstruction network (DRNet) designed to recover the textural details of rain-free images. Firstly, we introduce a dilated dense residual block (DDRB) within R2Net, enabling the aggregation of high-level and low-level features. Secondly, an enhanced residual pixel-wise attention block (ERPAB) is integrated into DRNet to facilitate the incorporation of contextual information. To further enhance the fidelity of our approach, we employ a comprehensive loss function that accentuates both the marginal and regional accuracy of rain-free images. Extensive experiments conducted on publicly available benchmarks demonstrates the noteworthy efficiency and effectiveness of our proposed DPENet. The source code and pre-trained models are currently available at https://github.com/chdwyb/DPENet.}
}
@article{GAN2024110245,
title = {Content Temporal Relation Network for temporal action proposal generation},
journal = {Pattern Recognition},
volume = {149},
pages = {110245},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110245},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009421},
author = {Ming-Gang Gan and Yan Zhang},
keywords = {Temporal action proposal generation, Temporal action detection, Untrimmed video analysis, Proposal–proposal relations},
abstract = {Temporal action proposal generation is an essential step for untrimmed video analysis and gains much attention from academia. However, most of the prior works predict the confidence score of each proposal separately and neglect the relations between proposals, limiting their performance. In this work, we design a novel Content Temporal Relation Network (CTRNet) to generate temporal action proposals by exploring the content and temporal semantic relations between proposals simultaneously. Specifically, we design a proposal feature map generation layer to convert the temporal semantic relations of proposals into spatial relations. Based on the proposal feature map, we propose a content-temporal relation module, which applies a novel adaptive-dilated convolution to model the temporal semantic relations between proposals and designs a content-adaptive convolution operation to explore the content semantic relation between proposals. Considering the temporal and content semantic relations between proposals, CTRNet has learned discriminative proposal features to improve performance. Extensive experiments are performed on two mainstream temporal action detection datasets, and CTRNet significantly outperforms the previous state-of-the-art methods. The codes are available at https://github.com/YanZhang-bit/CTRNet.}
}
@article{DUTTA2024110261,
title = {EmoComicNet: A multi-task model for comic emotion recognition},
journal = {Pattern Recognition},
volume = {150},
pages = {110261},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110261},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000128},
author = {Arpita Dutta and Samit Biswas and Amit Kumar Das},
keywords = {Comic analysis, Multi-modal emotion recognition, Document image processing, Deep learning, Multi-task learning},
abstract = {The emotion and sentiment associated with comic scenes can provide potential information for inferring the context of comic stories, which is an essential pre-requisite for developing comics’ automatic content understanding tools. Here, we address this open area of comic research by exploiting the multi-modal nature of comics. The general assumptions for multi-modal sentiment analysis methods are that both image and text modalities are always present at the test phase. However, this assumption is not always satisfied for comics since comic characters’ facial expressions, gestures, etc., are not always clearly visible. Also, the dialogues between comic characters are often challenging to comprehend the underlying context. To deal with these constraints of comic emotion analysis, we propose a multi-task-based framework, namely EmoComicNet, to fuse multi-modal information (i.e., both image and text) if it is available. However, the proposed EmoComicNet is designed to perform even when any modality is weak or completely missing. The proposed method potentially improves the overall performance. Besides, EmoComicNet can also deal with the problem of weak or absent modality during the training phase.}
}
@article{CHEN2024110289,
title = {Single image super-resolution based on trainable feature matching attention network},
journal = {Pattern Recognition},
volume = {149},
pages = {110289},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110289},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000402},
author = {Qizhou Chen and Qing Shao},
keywords = {Super-resolution, Feature matching, Non-local, Recurrent convolutional neural network, Deep learning},
abstract = {Convolutional Neural Networks (CNNs) have been widely employed for image Super-Resolution (SR) in recent years. Various techniques enhance SR performance by altering CNN structures or incorporating improved self-attention mechanisms. Interestingly, these advancements share a common trait. Instead of explicitly learning high-frequency details, they learn an implicit feature processing mode that utilizes weighted sums of a feature map’s own elements for reconstruction, akin to convolution and non-local. In contrast, early dictionary-based approaches learn feature decompositions explicitly to match and rebuild Low-Resolution (LR) features. Building on this analysis, we introduce Trainable Feature Matching (TFM) to amalgamate this explicit feature learning into CNNs, augmenting their representation capabilities. Within TFM, trainable feature sets are integrated to explicitly learn features from training images through feature matching. Furthermore, we integrate non-local and channel attention into our proposed Trainable Feature Matching Attention Network (TFMAN) to further enhance SR performance. To alleviate the computational demands of non-local operations, we propose a streamlined variant called Same-size-divided Region-level Non-Local (SRNL). SRNL conducts non-local computations in parallel on blocks uniformly divided from the input feature map. The efficacy of TFM and SRNL is validated through ablation studies and module explorations. We employ a recurrent convolutional network as the backbone of our TFMAN to optimize parameter utilization. Comprehensive experiments on benchmark datasets demonstrate that TFMAN achieves superior results in most comparisons while using fewer parameters. The code is available at https://github.com/qizhou000/tfman.}
}
@article{NI2024110233,
title = {DA-Tran: Multiphase liver tumor segmentation with a domain-adaptive transformer network},
journal = {Pattern Recognition},
volume = {149},
pages = {110233},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110233},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009305},
author = {Yangfan Ni and Geng Chen and Zhan Feng and Heng Cui and Dimitris Metaxas and Shaoting Zhang and Wentao Zhu},
keywords = {Multiphase CT, Liver tumor segmentation, Domain adaption, Transformer},
abstract = {Accurate liver tumor segmentation from multiphase CT images is a prerequisite for data-driven tumor analysis. This study presents a domain-adaptive transformer (DA-Tran) network to segment liver tumors from each CT phase. First, a DA module is designed to produce domain-adapted feature maps from noncontrast-enhanced (NC)-phase, arterial (ART)-phase, portal venous (PV)-phase, and delay-phase (DP) images. Then, these domain-adapted feature maps are integrated using 3D transformer blocks to catch patch-structured similarity information and global context attention. Finally, the attention fusion decoder (AFD) integrates features from different branches to generate a more refined prediction. Extensive experimental results demonstrate that DA-Tran achieves state-of-the-art tumor segmentation results, i.e., a Dice similarity coefficient (DSC) of 87.00% and a 95% Hausdorff distance (HD95) of 5.10 mm on a clinical dataset (DB1). Additionally, DA-Tran consistently outperforms other cutting-edge methods on another multiphase liver tumor dataset (DB2). The DA module and transformer blocks can boost the co-segmentation performance and make DA-Tran an ideal solution for multiphase liver tumor segmentation.}
}
@article{YANG2024110273,
title = {Continual learning for cross-modal image-text retrieval based on domain-selective attention},
journal = {Pattern Recognition},
volume = {149},
pages = {110273},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110273},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000244},
author = {Rui Yang and Shuang Wang and Yu Gu and Jihui Wang and Yingzhi Sun and Huan Zhang and Yu Liao and Licheng Jiao},
keywords = {Cross-modal retrieval, Continual learning, Attention, Weight regularization},
abstract = {Cross-modal image-text retrieval (CMITR) has been a high-value research topic for more than a decade. In most of the previous studies, the data for all tasks are trained as a single set. However, in reality, a more likely scenario is that the dataset has multiple tasks and trains them in sequence. The consequence is the limited ability to memorize the old task once a new task arrives; in other words, catastrophic forgetting. To solve this issue, this paper proposes a novel continual learning for cross-modal image-text retrieval (CLCMR) method to alleviate catastrophic forgetting. We construct a multilayer domain-selective attention (MDSA) based network to obtain knowledge from task-relevant and domain-specific attention levels. Moreover, a memory factor has been designed to achieve weight regularization, and a novel memory loss function is utilized to constrain MDSA. The extensive experimental results from multiple datasets (Wikipedia, Pascal Sentence, and PKU XMedianet datasets) demonstrate that CLCMR can effectively alleviate catastrophic forgetting and achieve a superior continual learning ability compared with the state-of-the-art methods.}
}
@article{YANG2024110244,
title = {Class-Aware Mask-guided feature refinement for scene text recognition},
journal = {Pattern Recognition},
volume = {149},
pages = {110244},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110244},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300941X},
author = {Mingkun Yang and Biao Yang and Minghui Liao and Yingying Zhu and Xiang Bai},
keywords = {Text recognition, Text segmentation, Multi-task learning, Feature fusion},
abstract = {Scene text recognition is a rapidly developing field that faces numerous challenges due to the complexity and diversity of scene text, including complex backgrounds, diverse fonts, flexible arrangements, and accidental occlusions. In this paper, we propose a novel approach called Class-Aware Mask-guided feature refinement (CAM) to address these challenges. Our approach introduces canonical class-aware glyph masks generated from a standard font to effectively suppress background and text style noise, thereby enhancing feature discrimination. Additionally, we design a feature alignment and fusion module to incorporate the canonical mask guidance for further feature refinement for text recognition. By enhancing the alignment between the canonical mask feature and the text feature, the module ensures more effective fusion, ultimately leading to improved recognition performance. We first evaluate CAM on six standard text recognition benchmarks to demonstrate its effectiveness. Furthermore, CAM exhibits superiority over the state-of-the-art method by an average performance gain of 4.1% across six more challenging datasets, despite utilizing a smaller model size. Our study highlights the importance of incorporating canonical mask guidance and aligned feature refinement techniques for robust scene text recognition. Code will be available at https://github.com/MelosY/CAM.}
}
@article{MA2024110285,
title = {Fusion-competition framework of local topology and global texture for head pose estimation},
journal = {Pattern Recognition},
volume = {149},
pages = {110285},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110285},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000360},
author = {Dongsheng Ma and Tianyu Fu and Yifei Yang and Kaibin Cao and Jingfan Fan and Deqiang Xiao and Hong Song and Ying Gu and Jian Yang},
keywords = {Head pose estimation, Feature fusion, Local regions competition, Feature channel attention, Point cloud, RGB image},
abstract = {RGB image and point cloud involve texture and geometric structure, which are widely used for head pose estimation. However, images lack of spatial information, and the quality of point cloud is easily affected by sensor noise. In this paper, a novel fusion-competition framework (FCF) is proposed to overcome the limitations of a single modality. The global texture information is extracted from image and the local topology information is extracted from point cloud to project heterogeneous data into a common feature subspace. The projected texture feature weighted by the channel attention mechanism is embedded into each local point cloud region with different topological features for fusion. The scoring mechanism creates competition among the regions involving local-global fused features to predict final pose with the highest score. According to the evaluation results on the public and our constructed datasets, the FCF improves the estimation accuracy and stability by an average of 13.6 % and 12.7 %, which is compared to nine state-of-the-art methods.}
}
@article{2025111166,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {158},
pages = {111166},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(24)00917-8},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009178}
}
@article{HUANG2024110257,
title = {Superpixel-based multi-scale multi-instance learning for hyperspectral image classification},
journal = {Pattern Recognition},
volume = {149},
pages = {110257},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110257},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000086},
author = {Shiluo Huang and Zheng Liu and Wei Jin and Ying Mu},
keywords = {Multi-instance learning (MIL), Hyperspectral image (HSI) classification, Superpixel},
abstract = {Superpixels can define meaningful local regions within a hyperspectral image (HSI) and have become the building blocks of various HSI classification methods. The superpixels in HSIs consist of multiple spectral pixels, sharing a similar structure with the data in multi-instance learning (MIL). However, the potential of MIL methods in the field of HSI classification has been rarely explored. In this paper, we propose the superpixel-based multi-scale multi-instance learning (MSMIL) framework, enhancing the superpixel representation with MIL for the first time. Segmenting the HSIs with superpixels, MSMIL converts the HSI classification into MIL problems and extracts superpixel representations via the MIL method, namely multi-instance factor analysis (MIFA). Compared with the existing methods focusing exclusively on the local information, MIFA utilizes the deviations from an overall generative model to describe the superpixels, retaining both the local and the global information. Moreover, MSMIL introduces multi-scale superpixels and a spectral–spatial decision fusion strategy for further refinement, where the results of multi-scale superpixel maps are weighted according to prediction certainty and spatial consistency. The proposed method is evaluated on four benchmark datasets and achieves competitive results. For example, MSMIL outperforms the comparison methods with a margin of 5% overall accuracy on the Indian Pines dataset, when 2% pixels are selected as the training set.}
}
@article{DAI2024110222,
title = {A gated cross-domain collaborative network for underwater object detection},
journal = {Pattern Recognition},
volume = {149},
pages = {110222},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110222},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009196},
author = {Linhui Dai and Hong Liu and Pinhao Song and Mengyuan Liu},
keywords = {Underwater object detection, Underwater image enhancement, Cross-domain},
abstract = {Underwater object detection (UOD) plays a significant role in aquaculture and marine environmental protection. Considering the challenges posed by low contrast and low-light conditions in underwater environments, several underwater image enhancement (UIE) methods have been proposed to improve the quality of underwater images. However, only using the enhanced images does not improve the performance of UOD, since it may unavoidably remove or alter critical patterns and details of underwater objects. In contrast, we believe that exploring the complementary information from the two domains is beneficial for UOD. The raw image preserves the natural characteristics of the scene and texture information of the objects, while the enhanced image improves the visibility of underwater objects. Based on this perspective, we propose a Gated Cross-domain Collaborative Network (GCC-Net) to address the challenges of poor visibility and low contrast in underwater environments, which comprises three dedicated components. Firstly, a real-time UIE method is employed to generate enhanced images, which can improve the visibility of objects in low-contrast areas. Secondly, a cross-domain feature interaction module is introduced to facilitate the interaction and mine complementary information between raw and enhanced image features. Thirdly, to prevent the contamination of unreliable generated results, a gated feature fusion module is proposed to adaptively control the fusion ratio of cross-domain information. Our method presents a new UOD paradigm from the perspective of cross-domain information interaction and fusion. Experimental results demonstrate that the proposed GCC-Net achieves state-of-the-art performance on four underwater datasets.}
}
@article{ZHOU2024110214,
title = {GaFL: Geometric-aware Feature Learning for universal 3D models recognition},
journal = {Pattern Recognition},
volume = {149},
pages = {110214},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110214},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009111},
author = {Yan Zhou and Huajie Sun and Huaidong Zhang and Xuemiao Xu and Chang’an Yi and Dewang Ye and Yuexia Zhou and Xiangyu Liu},
keywords = {Universal 3D models recognition, Geometric feature, Spherical convolution, Inactivation fusion},
abstract = {Existing methods of 3D model recognition mainly depend on deep learning algorithms. However, the extracted deep feature lacks the geometric information of the models. Thus, these methods fail to identify rigid and non-rigid 3D models simultaneously, i.e., universal 3D models recognition. In this work, we propose a novel method, Geometric-aware Feature Learning (GaFL), to further investigate the combination mechanism of geometric feature and deep learning in universal 3D models recognition. In GaFL, we design the Layer-Projected and Ray-Projected feature extraction policies to obtain depth values, which contain rich geometric information. Furthermore, the sphere convolution is proposed to guarantee the continuity and integrity of the ray feature when feeding into a deep network and the feature inactivation fusion module is designed to achieve the complementarity between layer and ray feature. Finally, the final merged feature vector contains enough geometric information as well as high-level semantic information, which are critical to universal 3D models recognition. In the experiments, GaFL achieves 95.0% and 95.2% classification accuracy in the rigid 3D models dataset ModelNet40 and the non-rigid 3D models dataset SHREC16, respectively, indicating that GaFL is powerful in universal 3D models recognition. Moreover, its significant advantage over state-of-the-art methods has also been validated on three other datasets, i.e., ShapeNet Core55, ScanObjectNN and SHREC15.}
}
@article{HAN2024110238,
title = {Adaptive instance similarity embedding for online continual learning},
journal = {Pattern Recognition},
volume = {149},
pages = {110238},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110238},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009354},
author = {Ya-nan Han and Jian-wei Liu},
keywords = {Continual learning, Catastrophic forgetting, Experience replay, Adaptive similarity embedding},
abstract = {We study the online continual learning (CL) paradigm, where the learner must continually learn a sequence of tasks. In this setting, improving the learning ability of the model and mitigating catastrophic forgetting are two pivotal factors. Note that most existing approaches for online continual learning are based on the experience replay strategy. In this type of method, a memory buffer is applied to store a subset of previous tasks to prevent catastrophic forgetting. The samples from between the current task and the memory buffer are jointly trained to update the network parameters. Consider that most methods only generate the feature embeddings via a shared feature extractor and then train the network via cross-entropy loss. We argue that such methods fail to explore the feature embedding in its entirety and neglect the similar relations between samples, thus leading to lower discriminant performance, especially in an online learning setting. To this end, we propose the Adaptive Instance Similarity Embedding for Online Continual Learning (AISEOCL) framework, which further takes all the sample relations in a given batch into account. In detail, firstly, the experience replay strategy is used to avoid catastrophic forgetting. Then, during training, we apply the adaptive similar embedding to obtain additional valuable similar information from the current training samples composed of the current and previous tasks. Since not all samples are equally important to make a prediction, we further weigh the importance of each instance accordingly resorting to the attention mechanism. Importantly, we further impose a similarity distillation loss on the distributions of the similarity relationship between current and previous models. Such operation can transfer the similarity relationship between different samples from the old model to the current model to alleviate catastrophic forgetting. With this strategy, AISEOCL can further improve the learning ability of the model while enhancing the discriminant power, which is also beneficial to stably resist forgetting. The experiments on several existing benchmarks validate the effectiveness of our proposed approach.}
}
@article{CHAI2024110292,
title = {Hypergraph modeling and hypergraph multi-view attention neural network for link prediction},
journal = {Pattern Recognition},
volume = {149},
pages = {110292},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110292},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000438},
author = {Lang Chai and Lilan Tu and Xianjia Wang and Qingqing Su},
keywords = {Link prediction, Network structure representation, Hypergraph modeling, Hypergraph learning, Hypergraph neural network},
abstract = {Hypergraph neural networks are widely used in link prediction because of their ability to learn the high-order structure relationship. However, most existing hypergraph modeling relies on the attribute information of nodes. And as for the link prediction, missing links are not utilized when training link predictors, so conventional transductive hypergraph learning are generally not consistent with link prediction tasks. To address these limitations, we propose the Network Structure Linear Representation (NSLR) method to model hypergraph for general networks without node attribute information and the inductive hypergraph learning method Hypergraph Multi-view Attention Neural Network (HMANN) that learns the rich high-order structure information from node-level and hyperedge-level. Also, this paper put forwards a novel NSLR-HMANN link prediction algorithm based on NSLR and HMANN methods. Extensive comparison and ablation experiments show that the NSLR-HMANN link prediction algorithm achieves state-of-the-art performance on link prediction and has better performance on robustness.}
}
@article{LIN2024110280,
title = {Semi-supervised domain generalization with evolving intermediate domain},
journal = {Pattern Recognition},
volume = {149},
pages = {110280},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110280},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000311},
author = {Luojun Lin and Han Xie and Zhishu Sun and Weijie Chen and Wenxi Liu and Yuanlong Yu and Lei Zhang},
keywords = {Domain generalization, Unsupervised domain adaptation, Semi-supervised learning, Transfer learning},
abstract = {Domain Generalization (DG) aims to generalize a model trained on multiple source domains to an unseen target domain. The source domains always require precise annotations, which can be cumbersome or even infeasible to obtain in practice due to the vast amount of data involved. Web data, namely web-crawled images, offers an opportunity to access large amounts of unlabeled images with rich style information, which can be leveraged to improve DG. From this perspective, we introduce a novel paradigm of DG, termed as Semi-Supervised Domain Generalization (SSDG), to explore how the labeled and unlabeled source domains can interact, and establish two settings, including the close-set and open-set SSDG. The close-set SSDG is based on existing public DG datasets, while the open-set SSDG, built on the newly-collected web-crawled datasets, presents a novel yet realistic challenge that pushes the limits of current technologies. A natural approach of SSDG is to transfer knowledge from labeled data to unlabeled data via pseudo labeling, and train the model on both labeled and pseudo-labeled data for generalization. Since there are conflicting goals between domain-oriented pseudo labeling and out-of-domain generalization, we develop a pseudo labeling phase and a generalization phase independently for SSDG. Unfortunately, due to the large domain gap, the pseudo labels provided in the pseudo labeling phase inevitably contain noise, which has negative affect on the subsequent generalization phase. Therefore, to improve the quality of pseudo labels and further enhance generalizability, we propose a cyclic learning framework to encourage a positive feedback between these two phases, utilizing an evolving intermediate domain that bridges the labeled and unlabeled domains in a curriculum learning manner. Extensive experiments are conducted to validate the effectiveness of our method. It is worth highlighting that web-crawled images can promote domain generalization as demonstrated by the experimental results.}
}
@article{WANG2024110252,
title = {AdvOps: Decoupling adversarial examples},
journal = {Pattern Recognition},
volume = {149},
pages = {110252},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110252},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000037},
author = {Donghua Wang and Wen Yao and Tingsong Jiang and Xiaoqian Chen},
keywords = {Adversarial attack, Analysis of adversarial examples, Analysis of neural network},
abstract = {Adversarial examples have a simple additive structure that the clean sample is added with delicate devised noise. Inspired by such an observation, we find that the prediction of the network on adversarial examples can also be decoupled into a simple additive structure, which is the sum of clean samples and adversarial perturbations in terms of the model prediction (called the decoupling principle). Thus, our findings can be served as a useful tool to gain insight into the underlying relationship between the inputs and the outputs of the model. However, although the adversarial examples generated by existing attack methods can satisfy the decoupling principle, the proportion is small. In this paper, we formulate the above issues as an optimization problem with multi-constrains, and we propose a generative model to generate adversarial examples that satisfy the decoupling principle and simultaneously obtain high attack performance. Specifically, we first adopt the adversarial loss to ensure the attack performance. Then, we devise a decouple loss to guarantee the decoupling principle. Moreover, we treat the Euclidean distances of perturbation as regularization terms to maintain visual quality. Extensive experiments against various networks on ImageNet and CIFAR10 show that the proposed method performs better than comparison methods in the comprehensive metric. Furthermore, transferability results suggested that adversarial examples that satisfy the decoupling principle show better transferability.}
}
@article{ZHENG2024110264,
title = {Edge-labeling based modified gated graph network for few-shot learning},
journal = {Pattern Recognition},
volume = {150},
pages = {110264},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110264},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000153},
author = {Peixiao Zheng and Xin Guo and Enqing Chen and Lin Qi and Ling Guan},
keywords = {Graph network, Few-shot learning, Gated recurrent unit, Edge-labeling},
abstract = {Accurate determination of similarity between samples is fundamental and critical for graph network based few-shot learning tasks. Previous approaches typically employ convolutional neural networks to obtain relations between nodes. However, these networks are not adept at handling node features in vector form. To overcome this limitation, we proposed a modified gated graph network (MGGN) that uniquely integrates graph networks and modified gated recurrent units (M-GRU) for few-shot classification. The introduced M-GRU mitigates the loss of label information from the initial graph and reduces computational complexity. The MGGN contains two modules that alternately update node and edge features. The node update module leverages a gating mechanism to integrate edge features into node update weights, fostering a learnable node aggregation process. The edge update component perceives the trend in edge feature changes and establishes long-term dependencies. Experimental results on two benchmark datasets demonstrate that our MGGN achieves comparable performance to state-of-the-art methods. The code is available at https://github.com/zpx16900/MGGN.}
}
@article{LIU2024110226,
title = {ASFFuse: Infrared and visible image fusion model based on adaptive selection feature maps},
journal = {Pattern Recognition},
volume = {149},
pages = {110226},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110226},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009238},
author = {Kuizhuang Liu and Min Li and Enguang Zuo and Chen Chen and Cheng Chen and Bo Wang and Yunling Wang and Xiaoyi Lv},
keywords = {Image fusion, Adaptive selection feature maps, Feature enhancement, Texture loss},
abstract = {Researchers continuously modify deep learning network architecture for improved fusion results. However, little attention is given to the influence of noise feature maps generated during the convolution process on the fusion outcomes. Here, we aim to minimize the influence of noisy feature maps on fusion results and propose a fusion model, the infrared and visible image fusion model based on adaptive selection feature maps (ASFFuse). We propose an adaptive selection feature maps module (ASFM). ASFM measures the amount of information contained in each feature map and filters out feature maps that contain more noise information. Additionally, we introduce a feature enhancement module (FEM) to enrich the fusion image with more source image information. For unsupervised training of the proposed model, we propose a texture loss function based on contrast learning. This loss function preserves the texture information of the image in a better way and makes the fusion image have a better visual effect. Our ASFFuse model has been shown to outperform state-of-the-art models in both quantitative and qualitative evaluations in extensive experiments on the TNO and RoadScene datasets. The code is available at https://github.com/LKZ1584905069/ASFFuse.}
}
@article{SARKER2024110288,
title = {Enhanced visible–infrared person re-identification based on cross-attention multiscale residual vision transformer},
journal = {Pattern Recognition},
volume = {149},
pages = {110288},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110288},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000396},
author = {Prodip Kumar Sarker and Qingjie Zhao},
keywords = {Cross-modality, Vision transformer, Weighted fused features, Datasets, Evaluation protocols},
abstract = {Visible–infrared (VI) person re-identification (Re-ID) is a critical identification task that involves retrieving and matching images of an individual using both infrared and visible imaging modalities. To improve the performance, researchers have developed methods to obtain implicit feature information; however, this degrades with fewer discriminative features. To address this issue, we propose a weighted fused cross-attention multi-scale residual vision transformer (WF-CAMReViT) approach to re-identify the appropriate person from visible–infrared modality images by integrating the cross-attention multi-scale residual vision transformer architecture with Opposition-based Dove Swarm Optimization (ODSO). The proposed framework aims to bridge the domain gap between the visible and infrared modalities and significantly improve the re-identification performance. RGB (visible) and infrared (IR) images of persons are gathered from standard datasets, subjected to a cross-attention multi-scale residual vision transformer network to collect features, and then fuse using minimal weight. We also propose Opposition-based DSO to find the minimal weight. The weighted fused features are then subjected to the final decoder layer of CAMReViT to perceive the characteristics of each modality. In this study, model-aware enhancement (MAE) loss is develop to improve the modality information capacity of modality-shared features. Then, the experimental results on the SYSU-MM01 and RegDB datasets are compared with state-of-the-art transformer-based visible–infrared person Re-ID tasks to verify the efficacy of the proposed model.}
}
@article{NOORI2024110213,
title = {TFS-ViT: Token-level feature stylization for domain generalization},
journal = {Pattern Recognition},
volume = {149},
pages = {110213},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110213},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300910X},
author = {Mehrdad Noori and Milad Cheraghalikhani and Ali Bahri and Gustavo A. {Vargas Hakim} and David Osowiechi and Ismail Ben Ayed and Christian Desrosiers},
keywords = {Deep learning, Domain generalization, Vision transformer, Feature stylization},
abstract = {Standard deep learning models such as convolutional neural networks (CNNs) lack the ability of generalizing to domains which have not been seen during training. This problem is mainly due to the common but often wrong assumption of such models that the source and target data come from the same i.i.d. distribution. Recently, Vision Transformers (ViTs) have shown outstanding performance for a broad range of computer vision tasks. However, very few studies have investigated their ability to generalize to new domains. This paper presents a first Token-level Feature Stylization (TFS-ViT) approach for domain generalization, which improves the performance of ViTs to unseen data by synthesizing new domains. Our approach transforms token features by mixing the normalization statistics of images from different domains. We further improve this approach with a novel strategy for attention-aware stylization, which uses the attention maps of class (CLS) tokens to compute and mix normalization statistics of tokens corresponding to different image regions. The proposed method is flexible to the choice of backbone model and can be easily applied to any ViT-based architecture with a negligible increase in computational complexity. Comprehensive experiments show that our approach is able to achieve state-of-the-art performance on five challenging benchmarks for domain generalization, and demonstrate its ability to deal with different types of domain shifts. The implementation is available at this repository.}
}
@article{SUN2024110197,
title = {Devil in the details: Delving into accurate quality scoring for DensePose},
journal = {Pattern Recognition},
volume = {148},
pages = {110197},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110197},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008944},
author = {Junyao Sun and Qiong Liu},
keywords = {Dense human pose estimation, Quality estimation, Instance-aware feature},
abstract = {How to score the quality of the network output is an essential but long-neglected problem in DensePose, which dramatically limits the potential of the existing methods. To fill the blank in the quality estimation of DensePose, we conduct rigorous experiments to clarify the key factors that accurately reflect the quality of DensePose results. We find that the accurate results already exist in the candidate pool but are mistakenly removed due to the inappropriate quality scores. To solve this problem, we proposed DensePose Scoring RCNN (DS RCNN), a simple and comprehensive quality estimation framework to learn the calibrated quality score and select high-quality results from the pool. DS RCNN introduces a quality scoring module (QSM) and a quality perception module (QPM) into the existing high-performance pipeline. The QSM scores the quality of DensePose results by fusing diverse quality information, and the QPM enhances the ability of quality perception by extracting instance-aware quality features guided by the predicted IUV maps. Benefiting from the superiority of QSM and QPM, DS RCNN outperforms baselines by up to 4.8 AP on the DensePose-COCO dataset.}
}
@article{WEI2024110239,
title = {Multi-label contrastive hashing},
journal = {Pattern Recognition},
volume = {149},
pages = {110239},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110239},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009366},
author = {Zeqiang Wei and Kai Jin and Zheng Zhang and Xiuzhuang Zhou},
keywords = {Multi-label hashing, Multi-level similarity, Supervised contrastive learning, Curriculum learning, Quantization deviation},
abstract = {Joint learning of image representation and hash encoding represents a dominant solution to approximate nearest neighbor search for large-scale image retrieval. Despite significant advances in deep learning to hash in multi-label setting, optimization of semantic similarity-preserving representations with minimal quantization error remains challenging. Motivated by the recent success of contrastive representation learning in various vision tasks, this article introduces a Multi-label Contrastive Hashing (MCH) method for large-scale multi-label image retrieval. We extend the image similarity modeling in existing supervised contrastive loss from binary to multi-level structure, by which multi-level semantic similarity between multi-label images can be well modeled and captured in learning to hash. Despite the appealing properties of contrastive learning, directly adapting it into multi-label hashing is non-trivial, as the quantization loss may restricts the optimization of the multi-level contrastive loss, degrading the multi-level similarity-preserving hashing. To this end, we design a curriculum strategy to adaptively adjust the weight of quantization loss by leveraging the historical quantization deviations during training, such that the multi-level semantic similarity can be well preserved with progressively reduced quantization deviation. We conduct extensive experiments on three benchmark datasets including MirFlickr25k, NUS-WIDE, and IAPRTC-12. The results indicate the effectiveness of our approach, outperforming several state-of-the-art solutions for hashing-based multi-label image retrieval.}
}
@article{BICCIATO2024110210,
title = {GNN-LoFI: A novel graph neural network through localized feature-based histogram intersection},
journal = {Pattern Recognition},
volume = {148},
pages = {110210},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110210},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300907X},
author = {Alessandro Bicciato and Luca Cosmo and Giorgia Minello and Luca Rossi and Andrea Torsello},
keywords = {Graph neural network, Deep learning},
abstract = {Graph neural networks are increasingly becoming the framework of choice for graph-based machine learning. In this paper, we propose a new graph neural network architecture that substitutes classical message passing with an analysis of the local distribution of node features. To this end, we extract the distribution of features in the egonet for each local neighbourhood and compare them against a set of learned label distributions by taking the histogram intersection kernel. The similarity information is then propagated to other nodes in the network, effectively creating a message passing-like mechanism where the message is determined by the ensemble of the features. We perform an ablation study to evaluate the network’s performance under different choices of its hyper-parameters. Finally, we test our model on standard graph classification and regression benchmarks, and we find that it outperforms widely used alternative approaches, including both graph kernels and graph neural networks.}
}
@article{KACHOLE2024110215,
title = {Bimodal SegNet: Fused instance segmentation using events and RGB frames},
journal = {Pattern Recognition},
volume = {149},
pages = {110215},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110215},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009123},
author = {Sanket Kachole and Xiaoqian Huang and Fariborz Baghaei Naeini and Rajkumar Muthusamy and Dimitrios Makris and Yahya Zweiri},
keywords = {Robotics, Grasping, Event vision, Deep learning, Cross attention},
abstract = {Object segmentation enhances robotic grasping by aiding object identification. Complex environments and dynamic conditions pose challenges such as occlusion, low light conditions, motion blur and object size variance. To address these challenges, we propose a Bimodal SegNet that fuses two types of visual signals, event-based data and RGB frame data. The proposed Bimodal SegNet network has two distinct encoders — one for RGB signal input and another for Event signal input, in addition to an Atrous Pyramidal Feature Amplification module. Encoders capture and fuse the rich contextual information from different resolutions via a Cross-Domain Contextual Attention layer while the decoder obtains sharp object boundaries. The evaluation of the proposed method undertakes five unique image degradation challenges including occlusion, blur, brightness, trajectory and scale variance on the Event-based Segmentation (ESD) Dataset. The results show a 4%–6% MIOU score improvement over state-of-the-art methods in terms of mean intersection over the union and pixel accuracy. The source code, dataset and model are publicly available at: https://github.com/sanket0707/Bimodal-SegNet.}
}
@article{LIU2024110211,
title = {Multimodal multiscale dynamic graph convolution networks for stock price prediction},
journal = {Pattern Recognition},
volume = {149},
pages = {110211},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110211},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009081},
author = {Ruirui Liu and Haoxian Liu and Huichou Huang and Bo Song and Qingyao Wu},
keywords = {Stock movement prediction, Multimodal feature fusing, Multiscale architecture, Graph convolutional network},
abstract = {Predicting directional future stock price movements is very challenging due to the complex, stochastic, and evolving nature of the financial markets. Existing literature either neglects other timely and granular alternative data, such as media text data, or fails to extract and distill predictive multimodal features from the data. Moreover, the time-varying cross-sectional relations beyond sequential dependencies of stock prices are informative for forecasting price fluctuations, for which the modelling flexibility, however, is not adequate in most of the previous studies. In this paper, we propose a novel Multiscale Multimodal Dynamic Graph Convolution Network (Melody-GCN) to address these issues in stock price prediction. It contains three core modules: (1) multimodal fusing-diffusing blocks that effectively integrate and align the numerical and textual features; (2) a multiscale architecture that extracts and refines temporal features via a fine-to-coarse descending path and a coarse-to-fine ascending path progressively; and (3) dynamic spatio-temporal graph convolutional layers that learn the complex and evolving stock relations not only in between industries and individual companies but also across time horizons. Extensive experimental results and trading simulations on two real-world datasets demonstrate the superior performance of our proposed approach beyond other state-of-the-art models.}
}
@article{WU2024110291,
title = {Dual residual attention network for image denoising},
journal = {Pattern Recognition},
volume = {149},
pages = {110291},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110291},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000426},
author = {Wencong Wu and Shijie Liu and Yuelong Xia and Yungang Zhang},
keywords = {Image denoising, Dual deep convolutional network, Residual attention learning, Hybrid residual attention learning},
abstract = {In image denoising, deep convolutional neural networks (CNNs) can obtain favorable performance on removing spatially invariant noise. However, many of these networks cannot perform well on removing the real noise (i.e. spatially variant noise) that is generated during image acquisition or transmission, which severely impedes their application in practical image denoising tasks. In this paper, we propose a novel Dual-branch Residual Attention Network (DRANet) for image denoising, which has both the merits of a wide model architecture and the attention-guided feature learning. The proposed DRANet includes two different parallel branches, which can capture complementary features to enhance the learning ability of the model. We designed a new residual attention block (RAB) and a novel hybrid dilated residual attention block (HDRAB) for the upper and lower branches, respectively. The RAB and HDRAB can capture rich local features through multiple skip connections between different convolutional layers, and the unimportant features can be dropped. Meanwhile, the long skip connections in each branch and the global feature fusion between the two parallel branches can effectively capture the global features as well. Extensive experiments demonstrate that compared with other state-of-the-art denoising methods, our DRANet can produce competitive denoising performance both on the synthetic and real-world noise removal. The code for DRANet is accessible at https://github.com/WenCongWu/DRANet.}
}
@article{SOARES2024110303,
title = {Comprehensive assessment of triclustering algorithms for three-way temporal data analysis},
journal = {Pattern Recognition},
volume = {150},
pages = {110303},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110303},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000542},
author = {Diogo F. Soares and Rui Henriques and Sara {C. Madeira}},
keywords = {Triclustering, Temporal data, Three-way data},
abstract = {The analysis of temporal data has gained increasing attention in recent years, aiming to identify patterns and trends that change over time. Temporal triclustering is a promising approach for this purpose, as it allows for the simultaneous clustering of three dimensions of data: objects, attributes, and time. In this work, we present a comparative study and experimental evaluation of state-of-the-art temporal triclustering algorithms. Our study provides a comprehensive quantitative assessment of several triclustering algorithms to unravel their strengths and limitations. To this end, we consider synthetic data with varying sizes and regularities, where true solutions are planted with different coherence and quality criteria, in order to assess the algorithms’ performance in datasets with diverse characteristics and assess their capacity to retrieve specific types of hidden patterns. This provides a more comprehensive evaluation of the algorithms and allows for a better understanding of their capabilities and limitations. This study is the first to compare state-of-the-art triclustering algorithms inherently prepared to deal with temporal data and provides new benchmark results for the Temporal Triclustering task. Our results on the algorithms’ performance can guide practitioners in selecting the most appropriate algorithm for their specific application.}
}
@article{LI2024110251,
title = {CR-CAM: Generating explanations for deep neural networks by contrasting and ranking features},
journal = {Pattern Recognition},
volume = {149},
pages = {110251},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110251},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000025},
author = {Yanshan Li and Huajie Liang and Hongfang Zheng and Rui Yu},
keywords = {Class Activation Mapping (CAM), Manifold space, Interpretation},
abstract = {The class activation mapping (CAM) algorithm is a visual interpretation algorithm that identifies the most discriminative regions for the target class in a classification task. However, existing CAM algorithms do not consider the differences between different categories and regions that are irrelevant to the target class during the feature extraction process. This results in interference from similar categories and irrelevant regions on the edges of the target class, leading to distorted saliency maps. To address this issue, we propose the Contrast-Ranking Class Activation Mapping (CR-CAM), including Inter-class Mapping Contract Block (IMCB) and Ranking Block. To gradually eliminate the interference regions that are irrelevant to the target class, IMCB is designed to compare distances between features in manifold space in order to generate more accurate saliency maps. To account for the similarity between different categories, ranking blocks adopt a comparative approach to measure the distances of feature mappings in the manifold space, thereby reducing the weights of surrounding regions. CR-CAM can be used on both CNN and GCN without modifying the algorithm to generate the class activation map. Experiments on both ImageNet and NTU RGB+D 60 datasets show highly competitive performance. In particular, CR-CAM outperforms alternative approaches by an average decrease of 0.0811 in Ave Drop, while exhibiting an average improvement of 0.011 in Ave Inc for CNN.}
}
@article{YIN2024110262,
title = {Spatiotemporal Progressive Inward-Outward Aggregation Network for skeleton-based action recognition},
journal = {Pattern Recognition},
volume = {150},
pages = {110262},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110262},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400013X},
author = {Xinpeng Yin and Jianqi Zhong and Deliang Lian and Wenming Cao},
keywords = {Action recognition, Graph convolutional networks, Progressive aggregation, Self-attention mechanism},
abstract = {Previous works have realized that spatio-temporal entanglement features can not be ignored in skeleton-based motion recognition tasks, then they have not broken away from the barriers of traditional GCN (The entanglement feature is still modeled by the extended single-frame adjacency matrix). We introduce a new joint-correlations determination mechanism that uses a non-linear transformation of the distance between joints in multiple frames to construct the connection relationship. The proposed method results in improved accuracy while significantly reducing the number of parameters. Meanwhile, recent works have alleviated the problem of most actions being only related to the dynamic characteristics of local joints by aggregating features of different parts of the human body in parallel, while interacting with different features still remains at a lower level of concatenation or addition. We propose a progressive inward-outward structure (PIS) that allows joint features corresponding to the action to be extracted while taking into account the lightweight link between this part of the joints and the rest. Integrating the above two designs, we propose a Spatiotemporal Progressive Inward-Outward Aggregation Network (SPIANet) to model the complex spatiotemporal entanglement between joints in the process of human motion, which is validated on three public datasets (NTU-RGB+D60, NTU-RGB+D120, and UESTC varying-view) and outperforms state-of-the-art methods.}
}
@article{DANG2024110287,
title = {Kinematics modeling network for video-based human pose estimation},
journal = {Pattern Recognition},
volume = {150},
pages = {110287},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110287},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000384},
author = {Yonghao Dang and Jianqin Yin and Shaojie Zhang and Jiping Liu and Yanzhu Hu},
keywords = {Human pose estimation, Relational modeling, Keypoint detection},
abstract = {Estimating human poses from videos is critical in human–computer interaction. Joints cooperate rather than move independently during human movement. There are both spatial and temporal correlations between joints. Despite the positive results of previous approaches, most of them focus on modeling the spatial correlation between joints while only straightforwardly integrating features along the temporal dimension, which ignores the temporal correlation between joints. In this work, we propose a plug-and-play kinematics modeling module (KMM) to explicitly model temporal correlations between joints across different frames by calculating their temporal similarity. In this way, KMM can capture motion cues of the current joint relative to all joints in different time. Besides, we formulate video-based human pose estimation as a Markov Decision Process and design a novel kinematics modeling network (KIMNet) to simulate the Markov Chain, allowing KIMNet to locate joints recursively. Our approach achieves state-of-the-art results on two challenging benchmarks. In particular, KIMNet shows robustness to the occlusion. Code will be released at https://github.com/YHDang/KIMNet.}
}
@article{PENG2024110203,
title = {Prototype Guided Pseudo Labeling and Perturbation-based Active Learning for domain adaptive semantic segmentation},
journal = {Pattern Recognition},
volume = {148},
pages = {110203},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110203},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009007},
author = {Junkun Peng and Mingjie Sun and Eng Gee Lim and Qiufeng Wang and Jimin Xiao},
keywords = {Domain adaptation, Active learning, Prototype, Perturbation},
abstract = {This work aims at active domain adaptation to transfer knowledge from a fully-labeled source domain to an entirely unlabeled target domain. During the active learning period, some pixels in the target domain are selected and annotated as active labels through several selection rounds. Such active labels can improve the target domain model performance greatly. However, existing approaches solely rely on pseudo labels, highly-confident classifier predictions on target images, to train the initial target domain model, resulting in a sub-optimal solution for model training. This initial model will be used for active label selection. Meanwhile, previous methods use entropy-based measurement to select pixels for annotation, which fails to detect high-confidence errors in earlier selection rounds due to the absence of target information. To address these issues, we propose a prototype-guided pseudo-label generating approach that leverages the relationships between source prototypes and target features. It generates target pseudo labels based on diverse source prototypes, thereby alleviating the issue of classifier predictions. Furthermore, perturbation-based uncertainty measurement, calculating the discrepancy between the target image and the augmented one, is introduced to find the areas with unstable predictions. Extensive experiments demonstrate that our approach outperforms state-of-the-art active domain adaptation methods on two benchmarks, GTAV → Cityscapes, and SYNTHIA → Cityscapes. Comparable performance is also achieved when compared to fully-supervised methods.}
}
@article{CHEN2024110198,
title = {PDRLRR: A novel low-rank representation with projection distance regularization via manifold optimization for clustering},
journal = {Pattern Recognition},
volume = {149},
pages = {110198},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110198},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008956},
author = {Haoran Chen and Xu Chen and Hongwei Tao and Zuhe Li and Boyue Wang},
keywords = {Low-rank representation, Schatten- norm, Projection distance regularization, Manifold optimization, Data clustering},
abstract = {The low-rank representation (LRR) method has attracted widespread attention due to its excellent performance in pattern recognition and machine learning. LRR-based variants have been proposed to solve the three existing problems in LRR: (1) the projection matrix is permanently fixed when dimensionality reduction techniques are adopted; (2) LRR fails to capture the local geometric structure; and (3) the solution deviates from the real low-rank solution. To address these problems, this paper proposes a low-rank representation with projection distance regularization (PDRLRR) via manifold optimization for clustering. In detail, we first introduce a low-dimensional projection matrix and a projection distance regularization term to fit the projected data automatically and capture the local structure of the data, respectively. Consequently, the projection matrix and representation matrix are obtained jointly. Then, we obtain a more accurate low-rank solution by minimizing the Schatten-p norm instead of the nuclear norm. Next, the projection matrix is optimized through a generalized Stiefel manifold. Extensive experiments demonstrate that our proposed method outperforms the state-of-the-art methods.}
}
@article{YANG2024110243,
title = {iCausalOSR: invertible Causal Disentanglement for Open-set Recognition},
journal = {Pattern Recognition},
volume = {149},
pages = {110243},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110243},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009408},
author = {Fenglei Yang and Baomin Li and Jingling Han},
keywords = {Open-set recognition, Invertible model, Causal priors},
abstract = {Despite recent developments that allowed neural networks to achieve impressive performance in a wide range of recognition, these models are intrinsically challenged by real-world applications. Open-set recognition is introduced to facilitate the development of recognition systems towards real-world applications, for this it has to deal with the issue caused by discriminative features. Such features arise from closed-set training and tend to partition the full input space into the closed set of target classes. To reduce the issue, we present an invertible model, iCausalOSR, to learn invertible causal disentanglement to reveal the essence of classes for open-set recognition. The invertible model consists of an encoder and class functions, wherein the class functions are responsible to model the known classes, and the encoder is responsible for progressive signal separation and contraction. A contrast strategy is designed to couple the encoder and class functions to learn invertible causal disentanglement. The dual properties of the model, causal disentanglement and invertibility, constitute the key elements in revealing the class essence. Experiments on widely-used standard datasets in open-set recognition demonstrate the superior performance of our model.}
}
@article{COBO2024110263,
title = {On the representation and methodology for wide and short range head pose estimation},
journal = {Pattern Recognition},
volume = {149},
pages = {110263},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110263},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000141},
author = {Alejandro Cobo and Roberto Valle and José M. Buenaposada and Luis Baumela},
keywords = {Short- and wide-range head pose estimation, Orientation representation, Error metrics, Cross-data set evaluation methodology},
abstract = {Head pose estimation (HPE) is a problem of interest in computer vision to improve the performance of face processing tasks in semi-frontal or profile settings. Recent applications require the analysis of faces in the full 360° rotation range. Traditional approaches to solve the semi-frontal and profile cases are not directly amenable for the full rotation case. In this paper we analyze the methodology for short- and wide-range HPE and discuss which representations and metrics are adequate for each case. We show that the popular Euler angles representation is a good choice for short-range HPE, but not at extreme rotations. However, the Euler angles’ gimbal lock problem prevents them from being used as a valid metric in any setting. We also revisit the current cross-data set evaluation methodology and note that the lack of alignment between the reference systems of the training and test data sets negatively biases the results of all articles in the literature. We introduce a procedure to quantify this misalignment and a new methodology for cross-data set HPE that establishes new, more accurate, SOTA for the 300W-LP/Biwi benchmark. We also propose a generalization of the geodesic angular distance metric that enables the construction of a loss that controls the contribution of each training sample to the optimization of the model. Finally, we introduce a wide range HPE benchmark based on the CMU Panoptic data set. code:https://github.com/pcr-upm/opal23_headpose}
}