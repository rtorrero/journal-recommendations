@article{HE2025111151,
title = {Two-stage Rule-induction visual reasoning on RPMs with an application to video prediction},
journal = {Pattern Recognition},
volume = {160},
pages = {111151},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111151},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009026},
author = {Wentao He and Jianfeng Ren and Ruibin Bai and Xudong Jiang},
keywords = {Visual reasoning, Video prediction, Raven’s progressive matrices},
abstract = {Raven’s Progressive Matrices (RPMs) are frequently used in evaluating human’s visual reasoning ability. Researchers have made considerable efforts in developing systems to automatically solve the RPM problem, often through a black-box end-to-end convolutional neural network for both visual recognition and logical reasoning tasks. Based on the intrinsic natures of RPM problem, we propose a Two-stage Rule-Induction Visual Reasoner (TRIVR), which consists of a perception module and a reasoning module, to tackle the challenges of real-world visual recognition and subsequent logical reasoning tasks, respectively. For the reasoning module, we further propose a “2+1” formulation that models human’s thinking in solving RPMs and significantly reduces the model complexity. It derives a reasoning rule from each RPM sample, which is not feasible for existing methods. As a result, the proposed reasoning module is capable of yielding a set of reasoning rules modeling human in solving the RPM problems. To validate the proposed method on real-world applications, an RPM-like Video Prediction (RVP) dataset is constructed, where visual reasoning is conducted on RPMs constructed using real-world video frames. Experimental results on various RPM-like datasets demonstrate that the proposed TRIVR achieves a significant and consistent performance gain compared with state-of-the-art models.}
}
@article{BUZZELLI2025111175,
title = {Uncertainty estimation in color constancy},
journal = {Pattern Recognition},
volume = {160},
pages = {111175},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111175},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009269},
author = {Marco Buzzelli and Simone Bianco},
keywords = {Uncertainty estimation, Color constancy, Automatic white balance, Illuminant estimation},
abstract = {Computational color constancy is an under-determined problem. As such, a key objective is to assign a level of uncertainty to the output illuminant estimations, which can significantly impact the reliability of the corrected images for downstream computer vision tasks. In this paper we present a formalization of uncertainty estimation in color constancy, and we define three forms of uncertainty that require at most one inference run to be estimated. The defined uncertainty estimators are applied to five different categories of color constancy algorithms. The experimental results on two standard datasets show a strong correlation between the estimated uncertainty and the illuminant estimation error. Furthermore, we show how color constancy algorithms can be cascaded leveraging the estimated uncertainty to provide more accurate illuminant estimates.}
}
@article{DONG2025111122,
title = {PRSN: Prototype resynthesis network with cross-image semantic alignment for few-shot image classification},
journal = {Pattern Recognition},
volume = {159},
pages = {111122},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111122},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008732},
author = {Mengping Dong and Fei Li and Zhenbo Li and Xue Liu},
keywords = {Few-shot learning, Prototypical network, Semantic alignment, Image classification},
abstract = {Few-shot image classification aims to learn novel classes with limited labeled samples for each class. Recent research mainly focuses on reconstructing a query image from a support set. However, most methods overlook the nearest semantic base parts of support samples, leading to higher intra-class semantic variation. To address this issue, we propose a novel prototype resynthesis network (PRSN) for few-shot image classification that includes global-level and local-level branches. Firstly, the prototype is compounded from semantically similar base parts to enhance the representation. Then, the query set is used to reconstruct the prototypes, further reducing intra-class variations. Additionally, we design a cross-image semantic alignment to enforce global-level and local-level semantic consistency between different query images of the same class. Our empirical results demonstrate that PRSN achieves remarkable performance across a range of widely recognized benchmarks. For instance, our method outperforms the second-best by 0.69% under 5-way 1-shot settings with ResNet-12 backbone on the miniImageNet dataset.}
}
@article{ZHENG2025111146,
title = {HTCSigNet: A Hybrid Transformer and Convolution Signature Network for offline signature verification},
journal = {Pattern Recognition},
volume = {159},
pages = {111146},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111146},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008975},
author = {Lidong Zheng and Da Wu and Shengjie Xu and Yuchen Zheng},
keywords = {Offline signature verification, Convolutional neural network, Transformer},
abstract = {For Offline Handwritten Signature Verification (OHSV) tasks, traditional Convolutional Neural Networks (CNNs) and transformers are hard to individually capture global and local features from signatures, and single-depth models often suffer from overfitting and poor generalization problems. To overcome those difficulties, in this paper, a novel Hybrid Transformer and Convolution Signature Network (HTCSigNet) is proposed to capture multi-scale features from signatures. Specifically, the HTCSigNet is an innovative framework that consists of two parts: transformer and CNN-based blocks which are used to respectively extract global and local features from signatures. The CNN-based block comprises a Space-to-depth Convolution (SPD-Conv) module which improves the feature learning capability by precisely focusing on signature strokes, a Spatial and Channel Reconstruction Convolution (SCConv) module which enhances model generalization by focusing on more distinctive micro-deformation features while reducing attention to common features, and convolution module that extracts the shape, morphology of specific strokes, and other local features from signatures. In the transformer-based block, there is a Vision Transformer (ViT) which is used to extract overall shape, layout, general direction, and other global features from signatures. After the feature learning stage, Writer-Dependent (WD) and Writer-Independent (WI) verification systems are constructed to evaluate the performance of the proposed HTCSigNet. Extensive experiments on four public signature datasets, GPDSsynthetic, CEDAR, UTSig, and BHSig260 (Bengali and Hindi) demonstrate that the proposed HTCSigNet learns discriminative representations between genuine and skilled forged signatures and achieves state-of-the-art or competitive performance compared with advanced verification systems. Furthermore, the proposed HTCSigNet is easy to transfer to different language datasets in OHSV tasks.22The code is available at https://github.com/copycpp/HTCSigNet-Master.}
}
@article{ZHAO2025111094,
title = {Improving stability and performance of spiking neural networks through enhancing temporal consistency},
journal = {Pattern Recognition},
volume = {159},
pages = {111094},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111094},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008458},
author = {Dongcheng Zhao and Guobin Shen and Yiting Dong and Yang Li and Yi Zeng},
keywords = {Spiking neural networks, Enhancing temporal consistency, Low latency},
abstract = {Spiking neural networks have gained significant attention due to their brain-like information processing capabilities. The use of surrogate gradients has made it possible to train spiking neural networks with backpropagation, leading to impressive performance in various tasks. However, spiking neural networks trained with backpropagation typically approximate actual labels using the average output, often necessitating a larger simulation timestep to enhance the network’s performance. This delay constraint poses a challenge to the further advancement of spiking neural networks. Current training algorithms tend to overlook the differences in output distribution at various timesteps. Particularly for neuromorphic datasets, inputs at different timesteps can cause inconsistencies in output distribution, leading to a significant deviation from the optimal direction when combining optimization directions from different moments. To tackle this issue, we have designed a method to enhance the temporal consistency of outputs at different timesteps. We have conducted experiments on static datasets such as CIFAR10, CIFAR100, and ImageNet. The results demonstrate that our algorithm can achieve comparable performance to other optimal SNN algorithms. Notably, our algorithm has achieved state-of-the-art performance on neuromorphic datasets DVS-CIFAR10 and N-Caltech101, and can achieve superior performance in the test phase with timestep T = 1.}
}
@article{HU2025111110,
title = {Count, decompose and correct: A new approach to handwritten Chinese character error correction},
journal = {Pattern Recognition},
volume = {160},
pages = {111110},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111110},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008616},
author = {Pengfei Hu and Jiefeng Ma and Zhenrong Zhang and Jun Du and Jianshu Zhang},
keywords = {Handwritten Chinese character error correction, Radical, Zero-shot learning, Transfer learning},
abstract = {Recently, handwritten Chinese character error correction has been greatly improved by employing encoder–decoder methods to decompose a Chinese character into an ideographic description sequence (IDS). However, existing methods implicitly capture and encode linguistic information inherent in IDS sequences, leading to a tendency to generate IDS sequences that match seen characters. This poses a challenge when dealing with an unseen misspelled character, as the decoder may generate an IDS sequence that matches a seen character instead. Therefore, we introduce Count, Decompose and Correct (CDC), a novel approach that exhibits better generalization towards unseen misspelled characters. CDC is mainly composed of three parts: the Counter, the Decomposer, and the Corrector. In the first stage, the Counter predicts the number of each radical class without the symbol-level position annotations. In the second stage, the Decomposer employs the counting information and generates the IDS sequence step by step. Moreover, by updating the counting information at each time step, the Decomposer becomes aware of the existence of each radical. With the decomposed IDS sequence, we can determine whether the given character is misspelled. If it is misspelled, the Corrector under the transductive transfer learning strategy predicts the ideal character that the user originally intended to write. We integrate our method into existing encoder–decoder models and significantly enhance their performance.}
}
@article{MOMANI2025111134,
title = {A robust fingerprint identification approach using a fuzzy system and novel rotation method},
journal = {Pattern Recognition},
volume = {159},
pages = {111134},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111134},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008859},
author = {Ahmad A. Momani and László T. Kóczy},
keywords = {Forensic science, Biometrics, Fingerprint, Distortion, Partial cuts, Binary pattern, Vagueness, Histograms},
abstract = {Forensic science has developed significantly in the last few decades. Its key role is to provide crime investigators with processed data obtained from the crime scene to achieve more accurate results presented in court. Biometrics has proved its robustness against various critical crimes encountered by forensics experts. Fingerprints are the most important biometric used until now due to their uniqueness and production low cost. The automated fingerprint identification system (AFIS) came into existence in the early 1960s through the cooperation of the countries: USA, UK, France, and Japan. Ever since it started to develop gradually because of the challenges found at the crime scenes such as fingerprints distortions and partial cuts which in turn can severely affect the final calculations made by experts. The vagueness of the results was the main motivation to build a robust fingerprint identification system that introduces new and enhanced methods in its stages to help experts make more accurate decisions. The proposed fingerprint identification system uses Fourier domain analysis for image enhancement, then the system cuts the image around the core point after applying the rotation and core point detection methods. After that, it calculates the similarity based on the distance between fingerprint histograms extracted using the Local Binary Pattern (LBP). The system's last step is to translate the results into a sensible form where it utilizes fuzziness to provide more possibilities for the answer. The proposed identification system showed high efficiency on FVC 2002 and FVC 2000 databases. For instance, the results of applying our system on FVC 2002 provided a set of three ordered matching candidates such that 97.5 % of the results provided the correct candidate as the first order, and the rest of 2.5 % provided the correct candidate as the second order.}
}
@article{KIM2025111118,
title = {A unified framework for unsupervised action learning via global-to-local motion transformer},
journal = {Pattern Recognition},
volume = {159},
pages = {111118},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111118},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008690},
author = {Boeun Kim and Jungho Kim and Hyung Jin Chang and Tae-Hyun Oh},
keywords = {Unsupervised learning, Action recognition, Group activity recognition, Transformer},
abstract = {Human action recognition remains challenging due to the inherent complexity arising from the combination of diverse granularity of semantics, ranging from the local motion of body joints to high-level relationships across multiple people. To learn this multi-level characteristic of human action in an unsupervised manner, we propose a novel pretraining strategy along with a transformer-based model architecture named GL-Transformer++. Prior methods in unsupervised action recognition or unsupervised group activity recognition (GAR) have shown limitations, often focusing solely on capturing a partial scope of the action, such as the local movements of each individual or the broader context of the overall motion. To tackle this problem, we introduce a novel pretraining strategy named multi-interval pose displacement prediction (MPDP) that enables the model to learn the diverse extents of the action. In the architectural aspect, we incorporate the global and local attention (GLA) mechanism within the transformer blocks to learn local dynamics between joints, global context of each individual, as well as high-level interpersonal relationships in both spatial and temporal manner. In fact, the proposed method is a unified approach that demonstrates efficacy in both action recognition and GAR. Particularly, our method presents a new and strong baseline, surpassing the current SOTA GAR method by significant margins: 29.6% in Volleyball and 60.3% and 59.9% on the xsub and xset settings of the Mutual NTU dataset, respectively.}
}
@article{YUAN2025111082,
title = {Instant pose extraction based on mask transformer for occluded person re-identification},
journal = {Pattern Recognition},
volume = {159},
pages = {111082},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111082},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008331},
author = {Ting-Ting Yuan and Qing-Ling Shu and Si-Bao Chen and Li-Li Huang and Bin Luo},
keywords = {Transformer, Person re-identification, Instant pose extraction, Mask aware module, Local prototype},
abstract = {Re-Identification (Re-ID) of obscured pedestrians is a daunting task, primarily due to the frequent occlusion caused by various obstacles like buildings, vehicles, and even other pedestrians. To address this challenge, we propose a novel approach named Instant Pose Extraction based on Mask Transformer (MTIPE), tailored specifically for occluded person Re-ID. MTIPE consists of several new modules: a Mask Aware Module (MAM) for alignment between the overall prototype and the occluded image; a Multi-headed Attention Constraint Module (MACM) to enrich the feature representation; a Pose Aggregation Module (PAM) to separate useful human information from the occlusion noise; a Feature Matching Module (FMM) in matching non-occluded parts; introduction of learnable local prototypes in the defined local prototype-based transformer decoder; a Pooling Attention Module (PAM) instead of traditional self-attention module to better extract and propagate local contextual information; and Pose Key-points Loss to better match non-occluded body parts. Through comprehensive experimental evaluations and comparisons, MTIPE demonstrates encouraging performance improvements in both occluded and holistic person Re-ID tasks. Its results surpass or at least match those of current state-of-the-art methods in various aspects, highlighting its potential advantages and promising application prospects.}
}
@article{WANG2025111179,
title = {Adaptive learning rate algorithms based on the improved Barzilai–Borwein method},
journal = {Pattern Recognition},
volume = {160},
pages = {111179},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111179},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009300},
author = {Zhi-Jun Wang and Hong Li and Zhou-Xiang Xu and Shuai-Ye Zhao and Peng-Jun Wang and He-Bei Gao},
keywords = {Barzilai–Borwein step size, Momentum method, Unconstrained optimization, Deep learning},
abstract = {Objective:
The Barzilai–Borwein(BB) method is essential in solving unconstrained optimization problems. The momentum method accelerates optimization algorithms with exponentially weighted moving average. In order to design reliable deep learning optimization algorithms, this paper proposes applying the BB method in four variants to the optimization algorithm of deep learning.
Findings:
The momentum method generates the BB step size under different step range limits. We also apply the momentum method and its variants to the stochastic gradient descent with the BB step size.
Novelty:
The algorithm’s robustness has been demonstrated through experiments on the initial learning rate and random seeds. The algorithm’s sensitivity is tested by choosing different momentum factors until a suitable momentum factor is found. Moreover, we compare our algorithms with popular algorithms in various neural networks. The results show that the new algorithms improve the efficiency of the BB step size in deep learning and provide a variety of optimization algorithm choices.}
}
@article{WU2025111106,
title = {Local and global self-attention enhanced graph convolutional network for skeleton-based action recognition},
journal = {Pattern Recognition},
volume = {159},
pages = {111106},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111106},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008574},
author = {Zhize Wu and Yue Ding and Long Wan and Teng Li and Fudong Nian},
keywords = {Graph convolutional networks, Self-attention, Skeleton-based action recognition},
abstract = {The current successful paradigm for skeleton-based action recognition is the combination of Graph Convolutional Networks (GCNs) modeling spatial correlations, and Temporal Convolution Networks (TCNs), extracting motion features. Such GCN-TCN-based approaches usually rely on local graph convolution operations, which limits their ability to capture complicated correlations among distant joints, as well as represent long-range dependencies. Although the self-attention originated from Transformers shows great potential in correlation modeling of global joints, the Transformer-based methods are usually computationally expensive and ignore the physical connectivity structure of the human skeleton. To address these issues, we propose a novel Local-Global Self-Attention Enhanced Graph Convolutional Network (LG-SGNet) to simultaneously learn both local and global representations in the spatial–temporal dimension. Our approach consists of three components: The Local-Global Graph Convolutional Network (LG-GCN) module extracts local and global spatial feature representations by parallel channel-specific global and local spatial modeling. The Local-Global Temporal Convolutional Network (LG-TCN) module performs a joint-wise global temporal modeling using multi-head self-attention in parallel with local temporal modeling. This constitutes a new multi-branch temporal convolution structure that effectively captures both long-range dependencies and subtle temporal structures. Finally, the Dynamic Frame Weighting Module (DFWM) adjusts the weights of skeleton action sequence frames, allowing the model to adaptively focus on the features of representative frames for more efficient action recognition. Extensive experiments demonstrate that our LG-SGNet performs very competitively compared to the state-of-the-art methods. Our project website is available at https://github.com/DingYyue/LG-SGNet.}
}
@article{GUO2025111174,
title = {Deep learning-enhanced environment perception for autonomous driving: MDNet with CSP-DarkNet53},
journal = {Pattern Recognition},
volume = {160},
pages = {111174},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111174},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009257},
author = {Xuyao Guo and Feng Jiang and Quanzhen Chen and Yuxuan Wang and Kaiyue Sha and Jing Chen},
keywords = {Multitasking, driverless, Environment awareness, Traffic target and lane line detection},
abstract = {Implementing environmental perception in intelligent vehicles is a crucial application, but the parallel processing of numerous algorithms on the vehicle side is complex, and their integration remains a critical challenge. To address this problem, this paper proposes a multitask detection algorithm Multitask Detection Network (MDNet) based on Cross Stage Partial Networks with Darknet53 Backbone (CSP-DarkNet53) with high feature extraction capability, which can simultaneously detect vehicles, pedestrians, traffic lights, traffic signs, and bicycles as well as lane lines. MDNet obtains exceptional results in multitask scenarios by employing innovative architectural designs consisting of a Feature Extraction Module, Target-level Branches, and Pixel-level Branches. The feature extraction module proposes an improved CSPPF structure to extract features more efficiently for three tasks, facilitating MDNet's capacity. The target-level branch suggests PFPN, which combines features from the backbone network, and the pixel-level branch utilizes a primary feature fusion network and an enhanced C2F_Faster method to spot lane lines more precisely. By incorporating these designs, MDNet's performance in complex environments is enhanced significantly. The algorithm underwent testing on the Berkeley DeepDrive 100K (BDD100K) and Cityscapes datasets, in which it could identify traffic targets and lane lines in numerous challenging settings, resulting in a 9.8 % measure of improvement in detection accuracy map for all three tasks relative to You Only Look Once for Panoptic Driving Perception (YOLOP, a multitask detection network), an 8.9 % improvement in IoU, a 22.1 % improvement in accuracy. It reached a speed of 46fps, which serves the practical applications' requirements more effectively.}
}
@article{GUAN2025111121,
title = {S2Match: Self-paced sampling for data-limited semi-supervised learning},
journal = {Pattern Recognition},
volume = {159},
pages = {111121},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111121},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008720},
author = {Dayan Guan and Yun Xing and Jiaxing Huang and Aoran Xiao and Abdulmotaleb {El Saddik} and Shijian Lu},
keywords = {Semi-supervised learning, Self-paced learning, Miscalibration, Image classification, Limited data},
abstract = {Data-limited semi-supervised learning tends to be severely degraded by miscalibration (i.e., misalignment between confidence and correctness of predicted pseudo labels) and stuck at poor local minima while learning from the same set of over-confident yet incorrect pseudo labels repeatedly. We design a simple and effective self-paced sampling technique that can greatly alleviate the impact of miscalibration and learn more accurate semi-supervised models from limited training data. Instead of employing static or dynamic confidence thresholds which is sensitive to miscalibration, the proposed self-paced sampling follows a simple linear policy to select pseudo labels which eases repeated learning from the same set of falsely predicted pseudo labels at the early training stage and lowers the chance of being stuck at local minima effectively. Despite its simplicity, extensive evaluations over multiple data-limited semi-supervised tasks show the proposed self-paced sampling outperforms the state-of-the-art consistently by large margins.}
}
@article{GUO2025111145,
title = {Accelerating the convergence of concept drift based on knowledge transfer},
journal = {Pattern Recognition},
volume = {159},
pages = {111145},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111145},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008963},
author = {Husheng Guo and Zhijie Wu and Qiaoyan Ren and Wenjian Wang},
keywords = {Streaming data, Concept drift, Knowledge transfer, Accelerating convergence},
abstract = {Concept drift detection and processing is an important issue in streaming data mining. When concept drift occurs, online learning model often cannot quickly adapt to the new data distribution due to the insufficient newly distributed data, which may lead to poor model performance. Currently, most online learning methods adapt to new data distributions after concept drift through autonomous adjustment of the model, but they may often fail to update the model to a stable state quickly. To solve these problems, this paper proposes an accelerating convergence method of concept drift based on knowledge transfer (ACC_KT). It extracts the most valuable information from the source domain (pre-drift data), and transfers it to the target domain (post-drift data), to realize the update of the ensemble model by knowledge transfer. Besides, different knowledge transfer patterns are adopted to accelerate convergence of model performance when different types concept drift occur. Experimental results show that the proposed method has an obvious acceleration effect on the online learning model after concept drift.}
}
@article{WANG2025111093,
title = {Continual learning with high-order experience replay for dynamic network embedding},
journal = {Pattern Recognition},
volume = {159},
pages = {111093},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111093},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008446},
author = {Zhizheng Wang and Yuanyuan Sun and Xiaokun Zhang and Bo Xu and Zhihao Yang and Hongfei Lin},
keywords = {Continual learning, Dynamic pattern recognition, High-order experience replay, Graph auto-encoder},
abstract = {Dynamic network embedding (DNE) poses a tough challenge in graph representation learning, especially when confronted with the frequent updates of streaming data. Conventional DNEs primarily resort to parameter updating but perform inadequately on historical networks, resulting in the problem of catastrophic forgetting. To tackle such issues, recent advancements in graph neural networks (GNNs) have explored matrix factorization techniques. However, these approaches encounter difficulties in preserving the global patterns of incremental data. In this paper, we propose CLDNE, a Continual Learning framework specifically designed for Dynamic Network Embedding. At the core of CLDNE lies a streaming graph auto-encoder that effectively captures both global and local patterns of the input graph. To further overcome catastrophic forgetting, CLDNE is equipped with an experience replay buffer and a knowledge distillation module, which preserve high-order historical topology and static historical patterns. We conduct experiments on four dynamic networks using link prediction and node classification tasks to evaluate the effectiveness of CLDNE. The outcomes demonstrate that CLDNE successfully mitigates the catastrophic forgetting problem and reduces training time by 80% without a significant loss in learning new patterns.}
}
@article{DONG2025111117,
title = {SAR target augmentation and recognition via cross-domain reconstruction},
journal = {Pattern Recognition},
volume = {159},
pages = {111117},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111117},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008689},
author = {Ganggang Dong and Yafei Song},
keywords = {Target recognition, Deep learning, Data augmentation, Coherent re-imaging, SAR image},
abstract = {The deep learning-based target recognition methods have achieved great performance in the preceding works. Large amounts of training data with label were collected to train a deep architecture, by which the inference can be obtained. For radar sensors, the data could be collected easily, yet the prior knowledge on label was difficult to be accessed. To solve the problem, a cross-domain re-imaging target augmentation method was proposed in this paper. The original image was first recast into the frequency domain. The frequency were then randomly filtered by a randomly generated mask. The size and the shape of mask was randomly determined. The filtering results were finally used for re-imaging. The original target can be then reconstructed accordingly. A series of new samples can be generated freely. The amounts and the diversity of dataset can be therefore improved. The proposed augmentation method can be implemented on-line or off-line, making it adaptable to various downstream tasks. Multiple comparative studies throw the light on the superiority of proposed method over the standard and recent techniques. It served to generate the images that would aid the downstream tasks.}
}
@article{NIRMALA2025111201,
title = {Automatic cervical cancer classification using adaptive vision transformer encoder with CNN for medical application},
journal = {Pattern Recognition},
volume = {160},
pages = {111201},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111201},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400952X},
author = {G. Nirmala and P. Prathap Nayudu and A. Ranjith Kumar and Renuka Sagar},
keywords = {Adaptive vision transformer encoder, Cervical cancer, Adaptive cat swarm optimization, Segmentation, Deep learning},
abstract = {Accurate and early cervical cancer screening can reduce the mortality rate of cervical cancer patients. The Pap test, often known as a Pap smear, is one of the frequently used methods for the early diagnosis of cervical cancer. However, manual analysis can be time-consuming. Previous approaches have faced challenges such as low accuracy, increased computing complexity, larger feature dimensionality, poor reliability, and increased time consumption due to subpar hyper-parameter optimization. This paper proposes an automatic cervical cancer classification system using a deep learning algorithm to address these issues. The proposed system consists of three stages: pre-processing, segmentation, and classification. Initially, images are collected and pre-processed through normalization, smoothing, and resizing. The pre-processed images are then passed to the segmentation stage, where an Adaptive Deep Residual Aggregation Network is utilized (ADRAN). After segmentation, the images are classified into seven categories: Carcinoma_in_situ, Light_dysplastic, Moderate_dysplastic, Normal_columnar, Normal_Intermediate, Normal_superficial, and Severe_dysplastic using an Adaptive Vision Transformer Encoder (AVTE) with CNN. To improve the efficiency of the transformer learning network, the hyperparameters of AVTE with CNN are optimized using an Adaptive Cat Swarm Optimization algorithm (ACSO). The efficiency of the presented technique is evaluated based on various metrics, and experimentation is conducted using the Herlev dataset.}
}
@article{CHEN2025111109,
title = {Spcolor: Semantic prior guided exemplar-based image colorization},
journal = {Pattern Recognition},
volume = {159},
pages = {111109},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111109},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008604},
author = {Siqi Chen and Xianlin Zhang and Mingdao Wang and Xueming Li and Yu Zhang and Yue Zhang},
keywords = {Image colorization, Exemplar-based, Semantic prior, Mismatch},
abstract = {Exemplar-based image colorization aims to colorize a target grayscale image based on a color reference image, and the key is to establish accurate pixel-level semantic correspondence between these two images. Previous methods directly search for correspondence over the entire reference image, and this type of global matching is prone to mismatch. Intuitively, a reasonable correspondence should be established between objects which are semantically similar. Motivated by this, we introduce the idea of semantic prior and propose SPColor, a semantic prior guided exemplar-based image colorization framework. Several novel components are systematically designed in SPColor, including a semantic prior guided correspondence network (SPC), a category reduction algorithm (CRA), and a similarity masked perceptual loss (SMP loss). Different from previous methods, SPColor establishes the correspondence between the pixels in the same semantic class locally. In this way, improper correspondence between different semantic classes is explicitly excluded, and the mismatch is obviously alleviated. In addition, SPColor supports region-level class assignments before SPC in the pipeline. With this feature, a category manipulation process (CMP) is proposed as an interactive interface to control colorization, which can also produce more varied colorization results and improve the flexibility of reference selection. Experiments demonstrate that our model outperforms recent state-of-the-art methods both quantitatively and qualitatively on public dataset. Our code is available at https://github.com/viector/spcolor.}
}
@article{ZHANG2025111133,
title = {A Structured Bipartite Graph Learning method for ensemble clustering},
journal = {Pattern Recognition},
volume = {160},
pages = {111133},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111133},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008847},
author = {Zitong Zhang and Xiaojun Chen and Chen Wang and Ruili Wang and Wei Song and Feiping Nie},
keywords = {Clustering, Ensemble clustering, Structure learning},
abstract = {Given a set of base clustering results, conventional bipartite graph-based ensemble clustering methods typically require computing a sample-cluster similarity matrix from each base clustering result. These matrices are then either concatenated or averaged to form a bipartite weight matrix, which is used to create a bipartite graph. Graph-based partition techniques are subsequently applied to this graph to obtain the final clustering result. However, these methods often suffer from unreliable base clustering results, making it challenging to identify a clear cluster structure due to the variations in cluster structures across the base results. In this paper, we propose a novel Structured Bipartite Graph Learning (SBGL) method. Our approach begins by computing a sample-cluster similarity matrix from each base clustering result and constructing a base bipartite graph from each of these matrices. We assume these base bipartite graphs contain a set of latent clusters and project them into a set of sample-latent-cluster bipartite graphs. These new graphs are then ensembled into a bipartite graph with a distinct cluster structure, from which the final set of clusters is derived. Our method allows for different numbers of clusters across base clusterings, leading to improved performance. Experimental results on both synthetic and real-world datasets demonstrate the superior performance of our new method.}
}
@article{WARGNIERDAUCHELLE2025111186,
title = {Explainable monotonic networks and constrained learning for interpretable classification and weakly supervised anomaly detection},
journal = {Pattern Recognition},
volume = {160},
pages = {111186},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111186},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009373},
author = {Valentine Wargnier-Dauchelle and Thomas Grenier and Françoise Durand-Dubief and François Cotton and Michaël Sdika},
keywords = {Monotonic network, Constrained learning, Weights initialization, Interpretability, Anomaly detection},
abstract = {Deep networks interpretability is fundamental in critical domains like medicine: using easily explainable networks with decisions based on radiological signs and not on spurious confounders would reassure the clinicians. Confidence is reinforced by the integration of intrinsic properties and characteristics of monotonic networks could be used to design such intrinsically explainable networks. As they are considered as too constrained and difficult to train, they are often very shallow and rarely used for image applications. In this work, we propose a procedure to transform any architecture into a trainable monotonic network, identifying the critical importance of weights initialization, and highlight the interest of such networks for explicability and interpretability. By constraining the features and gradients of a healthy vs pathological images classifier, we show, using counterfactual examples, that the network decision is more based on radiological signs of the pathology and outperform state-of-the-art weakly supervised anomaly detection methods.}
}
@article{WONG2025111105,
title = {Approximate geometric structure transfer for cross-domain image classification},
journal = {Pattern Recognition},
volume = {159},
pages = {111105},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111105},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008562},
author = {Wai Keung Wong and Yuwu Lu and Junyi Wu and Zhihui Lai and Xuelong Li},
keywords = {Domain adaptation, Transfer learning, Cross-domain feature extraction, Distribution matching, Image classification},
abstract = {The main purpose of domain adaptation (DA) is to conduct cross-domain related knowledge transfer. Considering the issue of unsupervised DA (UDA), learning a transformation that reduces the differences between domains is the primary goal. In addition to minimizing both the marginal and conditional distributions between the source and target domains, many methods explore potential factors that show the commonalities among domains to yield improved learning efficiency. However, geometric structure information is overlooked by most existing approaches, indicating that the shared information between domains has not been fully exploited. On account of this finding, by taking advantage of more potential shared factors to further enhance the results of DA, we propose an approximate geometric structure transfer (AGST) method for cross-domain image classification in this paper. By combining structural consistency and sample reweighting techniques, AGST encodes the geometric structure information taken from the samples in both domains, enabling it to easily obtain richer interdomain features and effectively facilitate knowledge transfer. Extensive experiments are conducted on several cross-domain data benchmarks. The experimental results indicate that our AGST method can outperform many state-of-the-art algorithms.}
}
@article{CHEN2025111150,
title = {Online indoor visual odometry with semantic assistance under implicit epipolar constraints},
journal = {Pattern Recognition},
volume = {159},
pages = {111150},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111150},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009014},
author = {Yang Chen and Lin Zhang and Shengjie Zhao and Yicong Zhou},
keywords = {Indoor visual odometry, Self-supervised learning, Unsupervised semantic segmentation, Epipolar geometric constraint, Online learning},
abstract = {Among solutions to the tasks of indoor localization and reconstruction, compared with traditional SLAM (Simultaneous Localization And Mapping), learning-based VO (Visual Odometry) has gained more and more popularity due to its robustness and low cost. However, the performance of existing indoor deep VOs is still limited in comparison with their outdoor counterparts mainly owing to large areas of textureless regions and complex indoor motions containing much more rotations. In this paper, the above two challenges are carefully tackled with the proposed SEOVO (Semantic Epipolar-constrained Online VO). On the one hand, as far as we know, SEOVO is the first semantic-aided VO under an online adaptive framework, which adaptively reconstructs low-texture planes without any supervision. On the other hand, we introduce the epipolar geometric constraint in an implicit way for improving the accuracy of pose estimation without destroying the global scale consistency. The efficiency and efficacy of SEOVO have been corroborated by extensive experiments conducted on both public datasets and our collected video sequences.}
}
@article{BAI2025111136,
title = {PIM-Net: Progressive Inconsistency Mining Network for image manipulation localization},
journal = {Pattern Recognition},
volume = {159},
pages = {111136},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111136},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008872},
author = {Ningning Bai and Xiaofeng Wang and Ruidong Han and Jianpeng Hou and Yihang Wang and Shanmin Pang},
keywords = {Image manipulation localization, Digital image forensics, Inconsistency learning},
abstract = {The content authenticity and reliability of digital images have promoted the research on image manipulation localization (IML). Most current deep learning-based methods focus on extracting global or local tampering features for identifying forged regions. These features usually contain semantic information and lead to inaccurate detection results for non-object or incomplete semantic tampered regions. In this study, we propose a novel Progressive Inconsistency Mining Network (PIM-Net) for effective IML. Specifically, PIM-Net consists of two core modules, the Inconsistency Mining Module (ICMM) and the Progressive Fusion Refinement module (PFR). ICMM models the inconsistency between authentic and forged regions at two levels, i.e., pixel correlation inconsistency and region attribute incongruity, while avoiding the interference of semantic information. Then PFR progressively aggregates and refines extracted inconsistent features, which in turn yields finer and pure localization responses. Extensive qualitative and quantitative experiments on five benchmarks demonstrate PIM-Net’s superiority to current state-of-the-art IML methods. Code: https://github.com/ningnbai/PIM-Net.}
}
@article{XUE2025111181,
title = {Interpretable tumor cell detection by domain adaptive pixel space encoding},
journal = {Pattern Recognition},
volume = {161},
pages = {111181},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111181},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009324},
author = {Yao Xue and Luyao Wang and Haipeng Zhou and Dun Ding and Yuelang Zhang},
keywords = {Tumor cell detection, Histopathology images},
abstract = {Identifying tumor from histopathology images is a significant task in clinical practice and research. The small size and high density of tumor, together with the domain disparity between distinct histopathology images, make existing approaches not quite reliable. In this research, we present a Compressive Sensing based Pixel space Encoding (CSPE) strategy that seeks a different route to fulfill this task. CSPE transforms the issue of detecting tumor cells in pixel space into an interpretable signal regression task in encoding signal space. We also develop a domain invariant learning module for global and center-aware domain alignment. Under the framework of CSPE, and with the support of domain adaptation modules, our approach becomes more robust to deep network training error, which is particularly detrimental for tumor detection in highly congested scenes. Our approach obtains state-of-the-art performance over three mainstream datasets. A range of experiments support our opinion that regression in CSPE space is superior to localizing coordinates of tumor cells in pixel space facing highly congested circumstances.}
}
@article{ZHOU2025111108,
title = {MvWECM: Multi-view Weighted Evidential C-Means clustering},
journal = {Pattern Recognition},
volume = {159},
pages = {111108},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111108},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008598},
author = {Kuang Zhou and Yuchen Zhu and Mei Guo and Ming Jiang},
keywords = {Multi-view clustering, Credal partitions, Uncertainty, View weights},
abstract = {Traditional multi-view clustering algorithms, designed to produce hard or fuzzy partitions, often neglect the inherent ambiguity and uncertainty in the cluster assignment of objects. This oversight may lead to performance degradation. To address these issues, this paper introduces a novel multi-view clustering method, termed MvWECM, capable of generating credal partitions within the framework of belief functions. The objective function of MvWECM is introduced considering the uncertainty in the cluster structure included in the multi-view dataset. We take into account inter-view conflict to effectively leverage coherent information across different views. Moreover, the effectiveness is heightened through the incorporation of adaptive view weights, which are customized to modulate their smoothness in accordance with their entropy. The optimization method to get the optimal credal membership and class prototypes is derived. The view wights can be also provided as a by-product. Experimental results on several real-word datasets demonstrate the effectiveness and superiority of MvWECM by comparing with some state-of-the-art methods.}
}
@article{SUN2025111095,
title = {Explainability-based knowledge distillation},
journal = {Pattern Recognition},
volume = {159},
pages = {111095},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111095},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400846X},
author = {Tianli Sun and Haonan Chen and Guosheng Hu and Cairong Zhao},
keywords = {Explainability, Knowledge distillation, CAM},
abstract = {Knowledge distillation (KD) is a popular approach for deep model acceleration. Based on the knowledge distilled, we categorize KD methods as label-related and structure-related. The former distills the very abstract (high-level) knowledge, e.g., logits; and the latter uses the spatial (low- or medium-level feature) knowledge. However, existing KD methods are usually not explainable, i.e., we do not know what knowledge is transferred during distillation. In this work, we propose a new KD method, Explainability-based Knowledge Distillation (Exp-KD). Specifically, we propose to use class activation map (CAM) as the explainable knowledge which can effectively capture both label- and structure-related information during the distillation. We conduct extensive experiments, including image classification tasks on CIFAR-10, CIFAR-100 and ImageNet datasets, and explainability tests on ImageNet and ImageNet-Segmentation. The results show the great effectiveness and explainability of Exp-KD compared with the state-of-the-art. Code is available at https://github.com/Blenderama/Exp-KD.}
}
@article{ZHANG2025111100,
title = {MWVOS: Mask-Free Weakly Supervised Video Object Segmentation via promptable foundation model},
journal = {Pattern Recognition},
volume = {159},
pages = {111100},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111100},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008513},
author = {Zhenghao Zhang and Shengfan Zhang and Zuozhuo Dai and Zilong Dong and Siyu Zhu},
keywords = {Vision foundation model, Video instance segmentation, Deep learning},
abstract = {The current state-of-the-art techniques for video object segmentation necessitate extensive training on video datasets with mask annotations, thereby constraining their ability to transfer zero-shot learning to new image distributions and tasks. However, recent advancements in foundation models, particularly in the domain of image segmentation, have showcased robust generalization capabilities, introducing a novel prompt-driven paradigm for a variety of downstream segmentation challenges on new data distributions. This study delves into the potential of vision foundation models using diverse prompt strategies and proposes a mask-free approach for unsupervised video object segmentation. To further improve the efficacy of prompt learning in diverse and complex video scenes, we introduce a spatial–temporal decoupled deformable attention mechanism to establish an effective correlation between intra- and inter-frame features. Extensive experiments conducted on the DAVIS2017-unsupervised and YoutubeVIS19&21 and OIVS datasets demonstrate the superior performance of the proposed approach without mask supervision when compared to existing mask-supervised methods, as well as its capacity to generalize to weakly-annotated video datasets.}
}
@article{HAI2025111124,
title = {L2T-DFM: Learning to Teach with Dynamic Fused Metric},
journal = {Pattern Recognition},
volume = {159},
pages = {111124},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111124},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008756},
author = {Zhaoyang Hai and Liyuan Pan and Xiabi Liu and Mengqiao Han},
keywords = {Learning to teach, Dynamic loss function, Optimization},
abstract = {The loss function plays a crucial role in the construction of machine learning algorithms. Employing a teacher model to set loss functions dynamically for student models has attracted attention. In existing works, (1) the characterization of the dynamic loss suffers from some inherent limitations, ie, the computational cost of loss networks and the restricted similarity measurement handcrafted loss functions; and (2) the states of the student model are provided to the teacher model directly without integration, causing the teacher model to underperform when trained on insufficient amounts of data. To alleviate the above-mentioned issues, in this paper, we select and weigh a set of similarity metrics by a confidence-based selection algorithm and a temporal teacher model to enhance the dynamic loss functions. Subsequently, to integrate the states of the student model, we employ statistics to quantify the information loss of the student model. Extensive experiments demonstrate that our approach can enhance student learning and improve the performance of various deep models on real-world tasks, including classification, object detection, and semantic segmentation scenarios.}
}
@article{MIAH2025111169,
title = {Learning data association for multi-object tracking using only coordinates},
journal = {Pattern Recognition},
volume = {160},
pages = {111169},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111169},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009208},
author = {Mehdi Miah and Guillaume-Alexandre Bilodeau and Nicolas Saunier},
keywords = {Tracking, Transformer, Data association, Motion, Multi-object tracking},
abstract = {We propose a novel Transformer-based module to address the data association problem for multi-object tracking. From detections obtained by a pretrained detector, this module uses only coordinates from bounding boxes to estimate an affinity score between pairs of tracks extracted from two distinct temporal windows. This module, named TWiX, is trained on sets of tracks with the objective of discriminating pairs of tracks coming from the same object from those which are not. Our module does not use the intersection over union measure, nor does it requires any motion priors or any camera motion compensation technique. By inserting TWiX within an online cascade matching pipeline, our tracker C-TWiX achieves state-of-the-art performance on the DanceTrack and KITTIMOT datasets, and gets competitive results on the MOT17 dataset. The code will be made available upon publication on the website https://mehdimiah.com/twix.}
}
@article{HOU2025111148,
title = {Self-supervised multimodal change detection based on difference contrast learning for remote sensing imagery},
journal = {Pattern Recognition},
volume = {159},
pages = {111148},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111148},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008999},
author = {Xuan Hou and Yunpeng Bai and Yefan Xie and Yunfeng Zhang and Lei Fu and Ying Li and Changjing Shang and Qiang Shen},
keywords = {Self-supervised learning, Change detection, Multimodal image, Remote sensing},
abstract = {Most existing change detection (CD) methods target homogeneous images. However, in real-world scenarios like disaster management, where CD is urgent and pre-changed and post-changed images are typical of different modalities, significant challenges arise for multimodal change detection (MCD). One challenge is that bi-temporal image pairs, sourced from distinct sensors, may cause an image domain gap. Another issue surfaces when multimodal bi-temporal image pairs require collaborative input from domain experts who are specialised among different image fields for pixel-level annotation, resulting in scarce annotated samples. To address these challenges, this paper proposes a novel self-supervised difference contrast learning framework (Self-DCF). This framework facilitates networks training without labelled samples by automatically exploiting the feature information inherent in bi-temporal imagery to supervise each other mutually. Additionally, a Unified Mapping Unit reduces the domain gap between different modal images. The efficiency and robustness of Self-DCF are validated on five popular datasets, outperforming state-of-the-art algorithms.}
}
@article{SUN2025111140,
title = {Joint Intra-view and Inter-view Enhanced Tensor Low-rank Induced Affinity Graph Learning},
journal = {Pattern Recognition},
volume = {159},
pages = {111140},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111140},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008914},
author = {Weijun Sun and Chaoye Li and Qiaoyun Li and Xiaozhao Fang and Jiakai He and Lei Liu},
keywords = {Multi-view clustering, Graph learning, Tensor, Low-rank},
abstract = {Graph-based and tensor-based multi-view clustering have gained popularity in recent years due to their ability to explore the relationship between samples. However, there are still several shortcomings in the current multi-view graph clustering algorithms. (1) Most previous methods only focus on the inter-view correlation, while ignoring the intra-view correlation. (2) They usually use the Tensor Nuclear Norm (TNN) to approximate the rank of tensors. However, while it has the same penalty for different singular values, the model cannot approximate the true rank of tensors well. To solve these problems in a unified way, we propose a new tensor-based multi-view graph clustering method. Specifically, we introduce the Enhanced Tensor Rank (ETR) minimization of intra-view and inter-view in the process of learning the affinity graph of each view. Compared with 10 state-of-the-art methods on 8 real datasets, the experimental results demonstrate the superiority of our method.}
}
@article{DOHERTY2025111209,
title = {BiFPN-YOLO: One-stage object detection integrating Bi-Directional Feature Pyramid Networks},
journal = {Pattern Recognition},
volume = {160},
pages = {111209},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111209},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009609},
author = {John Doherty and Bryan Gardiner and Emmett Kerr and Nazmul Siddique},
keywords = {Object detection, Computer vision, Neural networks, YOLO},
abstract = {Object detection is a key component in computer vision research, allowing a system to determine the location and type of object within any given scene. YOLOv5 is a modern object detection model, which utilises the advantages of the original YOLO implementation while being built from scratch in Python. In this paper, BiFPN-YOLO is proposed, featuring clear improvements over the existing range of YOLOv5 object detection models; these include replacing the traditional Path-Aggregation Network (PANet) with a higher performing Bi-Directional Feature Pyramid Network (BiFPN), requiring complex adaptation from its original implementation to function with YOLOv5, as well as exploring a replacement to the standard Swish activation function by evaluating the performance against a number of other activation functions. The proposed model showcases state-of-the-art performance, benchmarking against well-known datasets such as the German Traffic Sign Detection Benchmark (GTSDB), improving mAP by 3.1 %, and the RoboFEI@Home dataset, where Mean Average Precision (mAP) is improved by 2 % compared to the base YOLOv5 model. Performance was also improved on MSCOCO by 1.1 % and a custom subset of the OpenImagesV6 dataset by 2.4 %.}
}
@article{LI2025111177,
title = {An effective multi-scale interactive fusion network with hybrid Transformer and CNN for smoke image segmentation},
journal = {Pattern Recognition},
volume = {159},
pages = {111177},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111177},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009282},
author = {Kang Li and Feiniu Yuan and Chunmei Wang},
keywords = {Smoke segmentation, Attention coupled module, Hybrid network, Foreground enhancement},
abstract = {Smoke has visually elusive appearances, especially in low-light conditions, so it is quite difficult to quickly and accurately detect smoke from images. To address these challenges, we design a dual-encoder structure of Transformer and Convolutional Neural Network (CNN) to propose an effective Multi-scale Interactive Fusion Network (MIFNet) for smoke image segmentation. To improve the presentation of features, we propose a Local Feature Enhancement Propagation (LFEP) module to enhance spatial details. To optimize global and local features for efficient fusion, we integrate LFEP into the original Transformer to replace the traditional multi-head self-attention mechanism. Then, we propose a Multi-level Attention Coupled Module (MACM) to fuse Transformer and CNN features of the dual-encoder. MACM can flexibly focus on information interaction between different levels of two encoding paths. Finally, we design a Prior-guided Multi-scale Fusion Decoder (PMFD), which combines prior knowledge with a multi-scale feature fusion strategy to improve the performance of segmentation. Experimental results demonstrate that MIFNet substantially outperforms the state-of-the-art methods. MIFNet achieves a mean Intersection over Union (mIoU) of 81.6 % on the synthetic smoke (SYN70 K) dataset, and a remarkable accuracy of 98.3 % on the forest smoke dataset.}
}
@article{KE2025111096,
title = {Cross-modal independent matching network for image-text retrieval},
journal = {Pattern Recognition},
volume = {159},
pages = {111096},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111096},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008471},
author = {Xiao Ke and Baitao Chen and Xiong Yang and Yuhang Cai and Hao Liu and Wenzhong Guo},
keywords = {Image-text retrieval, Relationship reasoning, Cross-modal matching, Cross-modal representation learning},
abstract = {Image-text retrieval serves as a bridge connecting vision and language. Mainstream modal cross matching methods can effectively perform cross-modal interactions with high theoretical performance. However, there is a deficiency in efficiency. Modal independent matching methods exhibit superior efficiency but lack in performance. Therefore, achieving a balance between matching efficiency and performance becomes a challenge in the field of image-text retrieval. In this paper, we propose a new Cross-modal Independent Matching Network (CIMN) for image-text retrieval. Specifically, we first use the proposed Feature Relationship Reasoning (FRR) to infer neighborhood and potential relations of modal features. Then, we introduce Graph Pooling (GP) based on graph convolutional networks to perform modal global semantic aggregation. Finally, we introduce the Gravitation Loss (GL) by incorporating sample mass into the learning process. This loss can correct the matching relationship between and within each modality, avoiding the problem of equal treatment of all samples in the traditional triplet loss. Extensive experiments on Flickr30K and MSCOCO datasets demonstrate the superiority of the proposed method. It achieves a good balance between matching efficiency and performance, surpasses other similar independent matching methods in performance, and can obtain retrieval accuracy comparable to some mainstream cross matching methods with an order of magnitude lower inference time.}
}
@article{LING2025111193,
title = {RNDiff: Rainfall nowcasting with Condition Diffusion Model},
journal = {Pattern Recognition},
volume = {160},
pages = {111193},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111193},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009440},
author = {Xudong Ling and Chaorong Li and Fengqing Qin and Peng Yang and Yuanyuan Huang},
keywords = {Diffusion models, Rainfall prediction, Condition encoder},
abstract = {The Diffusion Models are widely used in image generation because they can generate high-quality and realistic samples. In contrast, generative adversarial networks (GANs) and variational autoencoders (VAEs) have some limitations in terms of image quality. We introduce a diffusion model to the precipitation forecasting task and propose a short-term precipitation nowcasting with condition diffusion model based on historical observational data, which is referred to as Rainfall nowcasting with Condition Diffusion Model(RNDiff). By incorporating an additional conditional decoder module in the denoising process, RNDiff achieves end-to-end conditional rainfall prediction. RNDiff is composed of two networks: a denoising network and a conditional encoder network. The conditional network is composed of multiple independent UNet networks. These networks extract conditional feature maps at different resolutions, providing accurate conditional information that guides the diffusion model for conditional generation. RNDiff surpasses GANs in terms of prediction accuracy, although it requires more computational resources. The RNDiff model exhibits higher stability and efficiency during training than GANs-based approaches, and generates high-quality precipitation distribution samples that better reflect future actual precipitation conditions. Compared to the current state-of-the-art GAN-based methods, our proposed approach achieves significant improvements on key evaluation metrics. Specifically, our method leads to improvements in the CSI, HSS, and FSS, which are increased by around 8%, 5%, and 6%, respectively. The experiment fully verified the advantages and potential of RNdiff in precipitation forecasting and provided new insights for improving rainfall forecasting. Our project is open source and available on GitHub at: https://github.com/ybu-lxd/RNDiff.}
}
@article{WU2025111180,
title = {STARNet: Low-light video enhancement using spatio-temporal consistency aggregation},
journal = {Pattern Recognition},
volume = {160},
pages = {111180},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111180},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009312},
author = {Zhe Wu and Zehua Sheng and Xue Zhang and Si-Yuan Cao and Runmin Zhang and Beinan Yu and Chenghao Zhang and Bailin Yang and Hui-Liang Shen},
keywords = {Low-light enhancement, Image processing, Video enhancement, Spatio-temporal aggregation},
abstract = {In low-light environments, capturing high-quality videos is an imaging challenge due to the limited number of photons. Previous low-light enhancement approaches usually result in over-smoothed details, temporal flickers, and color deviation. We propose STARNet, an end-to-end video enhancement network that leverages temporal consistency aggregation to address these issues. We introduce a spatio-temporal consistency aggregator, which extracts structures from multiple frames in hidden space to overcome detail corruption and temporal flickers. It parameterizes neighboring frames to extract and align consistent features, and then selectively fuses consistent features to restore clear structures. To further enhance temporal consistency, we develop a local temporal consistency constraint with robustness against the warping error from motion estimation. Furthermore, we employ a normalized low-frequency color constraint to regularize the color as the normal-light condition. Extensive experimental results on real datasets show that the proposed method achieves better detail fidelity, color accuracy, and temporal consistency, outperforming state-of-the-art approaches.}
}
@article{NGUYENLE2025111107,
title = {Privacy-preserving speaker verification system using Ranking-of-Element hashing},
journal = {Pattern Recognition},
volume = {159},
pages = {111107},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111107},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008586},
author = {Hong-Hanh Nguyen-Le and Lam Tran and Dinh Song An Nguyen and Nhien-An Le-Khac and Thuc Nguyen},
keywords = {Voice biometrics, Biometric template protection, And cancellable biometrics},
abstract = {The advancements in automatic speaker recognition have led to the exploration of voice data for verification systems. This raises concerns about the security of storing voice templates in plaintext. In this paper, we propose a novel cancellable biometrics that does not require users to manage random matrices or tokens. First, we pre-process the raw voice data and feed it into a deep feature extraction module to obtain embeddings. Next, we propose a hashing scheme, Ranking-of-Elements, which generates compact hashed codes by recording the number of elements whose values are lower than that of a random element. This approach captures more information from smaller-valued elements and prevents the adversary from guessing the ranking value through Attacks via Record Multiplicity. Lastly, we introduce a fuzzy matching method, to mitigate the variations in templates resulting from environmental noise. We evaluate the performance and security of our method on two datasets: TIMIT and VoxCeleb1.}
}
@article{YANG2025111152,
title = {Associative graph convolution network for point cloud analysis},
journal = {Pattern Recognition},
volume = {159},
pages = {111152},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111152},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009038},
author = {Xi Yang and Xingyilang Yin and Nannan Wang and Xinbo Gao},
keywords = {Point cloud analysis, GCN, Classification, Segmentation},
abstract = {Since point cloud is the raw output of most 3D sensors, its effective analysis is in huge demand in the field of autonomous driving and robotic manipulation. However, directly processing point clouds is challenging because point clouds are a kind of disordered and unstructured geometric data. Recently, numerous graph convolution neural networks are proposed for introducing graph structure to point clouds yet far from perfect. Specially, DGCNN tries to learn local geometric of points in semantic space and recomputes the graph using nearest neighbors in the feature space in each layer. However, it discards all the information of the previous graph after each graph update, which neglects the relations between each dynamic update. To this end, we propose an associative graph convolution neural network (AGCN) which mainly consists of associative graph convolution (AGConv) and two kinds of residual connections. AGConv additionally considers the information from the previous graph when computing the edge function on current local neighborhoods in each layer, and it can precisely and continuously capture the local geometric features on point clouds. Residual connections further explore the semantic relations between layers for effective learning on point clouds. Extensive experiments on several benchmark datasets show that our network achieves competitive classification and segmentation results.}
}
@article{DAI2025111099,
title = {Text–video retrieval re-ranking via multi-grained cross attention and frozen image encoders},
journal = {Pattern Recognition},
volume = {159},
pages = {111099},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111099},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008501},
author = {Zuozhuo Dai and Kaihui Cheng and Fangtao Shao and Zilong Dong and Siyu Zhu},
keywords = {Multi-grained cross attention, Text–video retrieval, Re-ranking},
abstract = {State-of-the-art methods for text–video retrieval generally leverage CLIP embeddings and cosine similarity for efficient retrieval. Meanwhile, recent advancements in cross-attention techniques introduce transformer decoders to facilitate attention computation between text queries and visual tokens extracted from video frames, enabling a more comprehensive interaction between textual and visual information. In this study, we combine the advantages of both approaches and propose a fine-grained re-ranking approach incorporating a multi-grained text–video cross attention module. Specifically, the re-ranker enhances the top K similar candidates identified by the cosine similarity network. To explore video and text interactions efficiently, we introduce frame and video token selectors to obtain salient visual tokens at both frame and video levels. Then, a multi-grained cross-attention mechanism is applied between text and visual tokens at these levels to capture multimodal information. To reduce the training overhead associated with the multi-grained cross-attention module, we freeze the vision backbone and only train the multi-grained cross attention module. This frozen strategy allows for scalability to larger pre-trained vision models such as ViT-G, leading to enhanced retrieval performance. Experimental evaluations on text–video retrieval datasets showcase the effectiveness and scalability of our proposed re-ranker combined with existing state-of-the-art methodologies.}
}
@article{CAO2025111123,
title = {Multi-task OCTA image segmentation with innovative dimension compression},
journal = {Pattern Recognition},
volume = {159},
pages = {111123},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111123},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008744},
author = {Guogang Cao and Zeyu Peng and Zhilin Zhou and Yan Wu and Yunqing Zhang and Rugang Yan},
keywords = {Image segmentation, U-shaped structure network, Dimension compression, Multitask segmentation, Optical Coherence Tomography Angiography (OCTA)},
abstract = {Optical Coherence Tomography Angiography (OCTA) plays a crucial role in the early detection and continuous monitoring of ocular diseases, which relies on accurate multi-tissue segmentation of retinal images. Existing OCTA segmentation methods typically focus on single-task designs that do not fully utilize the information of volume data in these images. To bridge this gap, our study introduces H2C-Net, a novel network architecture engineered for simultaneous and precise segmentation of various retinal structures, including capillaries, arteries, veins, and the fovea avascular zone (FAZ). At its core, H2C-Net consists of a plug-and-play Height-Channel Module (H2C) and an Enhanced U-shaped Network (GPC-Net). The H2C module cleverly converts the height information of the OCTA volume data into channel information through the Squeeze operation, realizes the lossless dimensionality reduction from 3D to 2D, and provides the "Soft layering" information by unidirectional pooling. Meanwhile, in order to guide the network to focus on channels for training, U-Net is enhanced with group normalization, channel attention mechanism, and Parametric Rectified Linear Unit (PReLU), which reduces the dependence on batch size and enhances the network's ability to extract salient features. Extensive experiments on two subsets of the publicly available OCTA-500 dataset have shown that H2C-Net outperforms existing state-of-the-art methods. It achieves average Intersection over Union (IoU) scores of 82.84 % and 88.48 %, marking improvements of 0.81 % and 1.59 %, respectively. Similarly, the average Dice scores are elevated to 90.40 % and 93.76 %, exceeding previous benchmarks by 0.42 % and 0.94 %. The proposed H2C-Net exhibits excellent performance in OCTA image segmentation, providing an efficient and accurate multi-task segmentation solution in ophthalmic diagnostics. The code is publicly available at: https://github.com/IAAI-SIT/H2C-Net.}
}
@article{LV2025111147,
title = {Joint utilization of positive and negative pseudo-labels in semi-supervised facial expression recognition},
journal = {Pattern Recognition},
volume = {159},
pages = {111147},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111147},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008987},
author = {Jinwei Lv and Yanli Ren and Guorui Feng},
keywords = {Semi-supervised learning, Pseudo-labels, Facial expression, Margin, Stable-positive and negative},
abstract = {Facial expression recognition has obtained significant attention due to the abundance of unlabeled expressions, and semi-supervised learning aims to leverage unlabeled samples sufficiently. Recent approaches primarily focus on combining an adaptive margin and pseudo-labels to extract hard samples and boost performance. However, the instability of pseudo-labels and the utilization of the rest unlabeled samples remain critical challenges. We introduce a stable-positive-single and negative-multiple pseudo-labels (SPS-NM) method to solve the above two challenges. All unlabeled samples are categorized into three groups properly by adaptive confidence margins. When the maximum confidence scores are high and stable enough, the unlabeled samples are attached with positive pseudo-labels. On the contrary, when the confidence scores of unlabeled samples are low enough, the related negative-multi pseudo-labels are attached to these samples. The quality and quantity of classes in negative pseudo-labels are balanced by top-k. Eventually, the remaining unlabeled samples are ambiguous and fail to match their pseudo-labels, but they can still be used to extract valuable features by contrastive learning. We conduct comparative experiments and ablation study on RAF-DB, AffectNet and SFEW datasets to demonstrate that SPS-NM achieves improvement and becomes the state-of-the-art method in facial expression recognition.}
}
@article{CAI2025111192,
title = {Tensorized latent representation with automatic dimensionality selection for multi-view clustering},
journal = {Pattern Recognition},
volume = {160},
pages = {111192},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111192},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009439},
author = {Bing Cai and Gui-Fu Lu and Xiaoxing Guo and Tong Wu},
keywords = {Multi-view clustering, Latent representation, Singular value decomposition, Tensor subspace learning},
abstract = {Latent representation has garnered significant attention in the field of multi-view learning due to its ability to capture the underlying structures of raw data and achieve promising results. However, latent representation-based methods often encounter challenges in selecting the dimensionality of the latent view, which limits their applicability. To address this problem, we propose a novel method called Tensorized Latent Representation with Automatic Dimensionality Selection (TLRADS), which can automatically determine the optimal dimensions. In TLRADS, we leverage the cumulative contribution rate of singular values to determine the number of dimensions for each view-specific latent representation. This approach ensures that the chosen dimensions capture a significant portion of the data’s variability while discarding less relevant information. After obtaining the latent representation views, we incorporate the tensor subspace learning technique to capture the underlying structural information more comprehensively. Finally, an efficient iterative algorithm is designed to solve the TLRADS model. Through experimental validation, we demonstrate the effectiveness of the automatic dimensionality selection strategy in TLRADS. Meanwhile, the experimental results on real-life datasets indicate that TLRADS outperforms state-of-the-art multi-view clustering methods.}
}
@article{HASSAN2025111139,
title = {Forget to Learn (F2L): Circumventing plasticity–stability trade-off in continuous unsupervised domain adaptation},
journal = {Pattern Recognition},
volume = {159},
pages = {111139},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111139},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008902},
author = {Mohamed Abubakr Hassan and Chi-Guhn Lee},
keywords = {Plasticity–stability dilemma, Continuous unsupervised domain adaptation, Knowledge distillation, Forgetting},
abstract = {In continuous unsupervised domain adaptation (CUDA), deep learning models struggle with the stability-plasticity trade-off—where the model must forget old knowledge to acquire new one. This paper introduces the “Forget to Learn” (F2L), a novel framework that circumvents such a trade-off. In contrast to state-of-the-art methods that aim to balance the two conflicting objectives, stability and plasticity, F2L utilizes active forgetting and knowledge distillation to circumvent the conflict’s root causes. In F2L, dual-encoders are trained, where the first encoder – the ‘Specialist’ – is designed to actively forget, thereby boosting adaptability (i.e., plasticity) and generating high-accuracy pseudo labels on the new domains. Such pseudo labels are then used to transfer/accumulate the specialist knowledge to the second encoder—the ‘Generalist’ through conflict-free knowledge distillation. Empirical and ablation studies confirmed F2L’s superiority on different datasets and against different SOTAs. Furthermore, F2L minimizes the need for hyperparameter tuning, enhances computational and sample efficiency, and excels in problems with long domain sequences—key advantages for practical systems constrained by hardware limitations.}
}
@article{HUA2025111111,
title = {Weakly Supervised Underwater Object Real-time Detection Based on High-resolution Attention Class Activation Mapping and Category Hierarchy},
journal = {Pattern Recognition},
volume = {159},
pages = {111111},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111111},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008628},
author = {Xia Hua and Xiaopeng Cui and Xinghua Xu and Shaohua Qiu and Zhong Li},
keywords = {Underwater image, weakly supervised object detection, hierarchical network, Class Activation Mapping, network attention},
abstract = {Recently, deep learning-based underwater object detection technology has achieved remarkable success. However, the accuracy and completeness of dataset instance annotation are crucial for its success. The quality of underwater images is low, severe objects clustering, and occlusion, acquiring object's annotations demands substantial time and labor costs, while mis annotation and missed annotation can also degrade model performance and limit their application in practical scenarios. To address this issue, this paper presents a novel weakly supervised underwater object real-time detection method, which is divided into two subtasks: weakly supervised object localization and real-time object detection. In the weakly supervised object localization task, we design a novel category hierarchy structure network that integrates the high-resolution attention-class activation mapping algorithm to obtain high-quality object class activation maps, weaken background interference, and obtain more complete object regions. The parameterized spatial loss module is devised to enable the model to escape from local optimal solutions, thus accurately and efficiently obtaining object pseudo-detection annotation boxes. For the real-time object detection task, the single-stage detector YOLOv7 is selected as the basic detection model, and an object perception loss function is designed based on the class activation map to jointly supervise the training process. A method for filtering noisy pseudo-supervision information is proposed to enhance the pseudo-supervision information involved in training. Ablation experiments and multi-method comparison experiments were conducted on the URPC and RUOD datasets, and the results verify the effectiveness of the proposed strategy, and our model exhibits significant advantages in detection performance and detection efficiency compared to current mainstream and advanced models.}
}
@article{LI2025111176,
title = {Pseudo-labeling with keyword refining for few-supervised video captioning},
journal = {Pattern Recognition},
volume = {159},
pages = {111176},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111176},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009270},
author = {Ping Li and Tao Wang and Xinkui Zhao and Xianghua Xu and Mingli Song},
keywords = {Video captioning, Few supervision, Pseudo-labeling, Keyword refiner, Gated fusion},
abstract = {Video captioning generate a sentence that describes the video content. Existing methods always require a number of captions (e.g., 10 or 20) per video to train the model, which is quite costly. In this work, we explore the possibility of using only one or very few ground-truth sentences, and introduce a new task named few-supervised video captioning. Specifically, we propose a few-supervised video captioning framework that consists of lexically constrained pseudo-labeling module and keyword-refined captioning module. Unlike the random sampling in natural language processing that may cause invalid modifications (i.e., edit words), the former module guides the model to edit words using some actions (e.g., copy, replace, insert, and delete) by a pretrained token-level classifier, and then fine-tunes candidate sentences by a pretrained language model. Meanwhile, the former employs the repetition penalized sampling to encourage the model to yield concise pseudo-labeled sentences with less repetition, and selects the most relevant sentences upon a pretrained video-text model. Moreover, to keep semantic consistency between pseudo-labeled sentences and video content, we develop the transformer-based keyword refiner with the video-keyword gated fusion strategy to emphasize more on relevant words. Extensive experiments on several benchmarks demonstrate the advantages of the proposed approach in both few-supervised and fully-supervised scenarios.}
}
@article{ZOHAIB2025111188,
title = {CDHN: Cross-domain hallucination network for 3D keypoints estimation},
journal = {Pattern Recognition},
volume = {160},
pages = {111188},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111188},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009397},
author = {Mohammad Zohaib and Milind Gajanan Padalkar and Pietro Morerio and Matteo Taiana and Alessio {Del Bue}},
keywords = {3D keypoints, Pose estimation, Hallucination, Single-view RGB},
abstract = {This paper presents a novel method to estimate sparse 3D keypoints from single-view RGB images. Our network is trained in two steps using a knowledge distillation framework. In the first step, the teacher is trained to extract 3D features from point cloud data, which are used in combination with 2D features to estimate the 3D keypoints. In the second step, the teacher teaches the student module to hallucinate the 3D features from RGB images that are similar to those extracted from the point clouds. This procedure helps the network during inference to extract 2D and 3D features directly from images, without requiring point clouds as input. Moreover, the network also predicts a confidence score for every keypoint, which is used to select the valid ones from a set of N predicted keypoints. This allows the prediction of different number of keypoints depending on the object’s geometry. We use the estimated keypoints for computing the relative pose between two views of an object. The results are compared with those of KP-Net and StarMap , which are the state-of-the-art for estimating 3D keypoints from a single-view RGB image. The average angular distance error of our approach (5.94°) is 8.46° and 55.26° lower than that of KP-Net (14.40°) and StarMap (61.20°), respectively.}
}
@article{TANG2025111135,
title = {Riding feeling recognition based on multi-head self-attention LSTM for driverless automobile},
journal = {Pattern Recognition},
volume = {159},
pages = {111135},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111135},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008860},
author = {Xianzhi Tang and Yongjia Xie and Xinlong Li and Bo Wang},
keywords = {Electroencephalography (EEG), Attention, Feature extraction, Driving experience},
abstract = {With the emergence of driverless technology, passenger ride comfort has become an issue of concern. In recent years, driving fatigue detection and braking sensation evaluation based on EEG signals have received more attention, and analyzing ride comfort using EEG signals is also a more intuitive method. However, it is still a challenge to find an effective method or model to evaluate passenger comfort. In this paper, we propose a long- and short-term memory network model based on a multiple self-attention mechanism for passenger comfort detection. By applying the multiple attention mechanism to the feature extraction process, more efficient classification results are obtained. The results show that the long- and short-term memory network using the multi-head self-attention mechanism is efficient in decision making along with higher classification accuracy. In conclusion, the classifier based on the multi-head attention mechanism proposed in this paper has excellent performance in EEG classification of different emotional states, and has a broad development prospect in brain-computer interaction.}
}
@article{HUA2025111090,
title = {MSCMNet: Multi-scale Semantic Correlation Mining for Visible-Infrared Person Re-Identification},
journal = {Pattern Recognition},
volume = {159},
pages = {111090},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111090},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008410},
author = {Xuecheng Hua and Ke Cheng and Hu Lu and Juanjuan Tu and Yuanquan Wang and Shitong Wang},
keywords = {Visible-infrared person re-identification, Person re-identification, Semantic correlation, Multi-scales},
abstract = {The main challenge in the Visible-Infrared Person Re-Identification (VI-ReID) task lies in extracting discriminative features from different modalities for matching purposes. While existing studies primarily focus on reducing modal discrepancies, the modality information fails to be thoroughly exploited. To solve this problem, the Multi-scale Semantic Correlation Mining network (MSCMNet) is proposed to comprehensively exploit semantic features at multiple scales. The network fuses shallow-level features into the deep network through dimensionality reduction and mapping, and the fused features are utilized to minimize modality information loss in feature extraction. Firstly, considering the effective utilization of modality information, the Multi-scale Information Correlation Mining Block (MIMB) is designed to fuse features at different scales and explore the semantic correlation of fusion features. Secondly, in order to enrich the semantic information that MIMB can utilize, the Quadruple-stream Feature Extractor (QFE) with non-shared parameters is specifically designed to extract information from different dimensions of the dataset. Finally, the Quadruple Center Triplet Loss (QCT) is further proposed to address the information discrepancy in the comprehensive features. Extensive experiments on the SYSU-MM01, RegDB, and LLCM datasets demonstrate that the proposed MSCMNet achieves the greatest accuracy. We release the source code on https://github.com/Hua-XC/MSCMNet.}
}
@article{LI2025111114,
title = {Self-distillation with beta label smoothing-based cross-subject transfer learning for P300 classification},
journal = {Pattern Recognition},
volume = {159},
pages = {111114},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111114},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008653},
author = {Shurui Li and Liming Zhao and Chang Liu and Jing Jin and Cuntai Guan},
keywords = {Brain-computer interface, P300 classification, Cross-subject, Self-distillation},
abstract = {Background:
The P300 speller is one of the most well-known brain-computer interface (BCI) systems, offering users a novel way to communicate with their environment by decoding brain activity.
Problem:
However, most P300-based BCI systems require a longer calibration phase to develop a subject-specific model, which can be inconvenient and time-consuming. Additionally, it is challenging to implement cross-subject P300 classification due to significant inter-individual variations.
Method:
To address these issues, this study proposes a calibration-free approach for P300 signal detection. Specifically, we incorporate self-distillation along with a beta label smoothing method to enhance model generalization and overall system performance, which can not only enable the distillation of informative knowledge from the electroencephalogram (EEG) data of other subjects but effectively reduce individual variability.
Experimental results:
The results conducted on the publicly available OpenBMI dataset demonstrate that the proposed method achieves statistically significantly higher performance compared to state-of-the-art approaches. Notably, the average character recognition accuracy of our method reaches up to 97.37% without the need for calibration. And information transfer rate and visualization further confirm its effectiveness.
Significance:
This method holds great promise for future developments in BCI applications.}
}
@article{JIANG2025111159,
title = {Semi-supervised multi-view feature selection with adaptive similarity fusion and learning},
journal = {Pattern Recognition},
volume = {159},
pages = {111159},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111159},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009105},
author = {Bingbing Jiang and Jun Liu and Zidong Wang and Chenglong Zhang and Jie Yang and Yadi Wang and Weiguo Sheng and Weiping Ding},
keywords = {Multi-view feature selection, Semi-supervised learning, Similarity fusion, Graph learning},
abstract = {Existing multi-view semi-supervised feature selection methods typically need to calculate the inversion of high-order dense matrices, rendering them impractical for large-scale applications. Meanwhile, traditional works construct similarity graphs on different views and directly fuse these graphs from the view level, ignoring the differences among samples in various views and the interplay between graph learning and feature selection. Consequently, both the reliability of graphs and the discrimination of selected features are compromised. To address these issues, we propose a novel multi-view semi-supervised feature selection with Adaptive Similarity Fusion and Learning (ASFL) for large-scale tasks. Specifically, ASFL constructs bipartite graphs for each view and then leverages the relationships between samples and anchors to align anchors and graphs across different views, preserving the complementarity and consistency among views. Moreover, an effective view-to-sample fusion manner is designed to coalesce the aligned graphs while simultaneously exploiting the neighbor structures in projection subspaces to construct the joint graph compatible across views, reducing the adverse effects of noisy features and outliers. By incorporating bipartite graph fusion and learning, label propagation, and l2,0-norm multi-view feature selection into a unified framework, ASFL not only avoids the expensive computation in the solution procedures but also enhances the quality of selected features. An effective optimization strategy with fast convergence is developed to solve the objective function, and experimental results validate its efficiency and effectiveness over state-of-the-art methods.}
}
@article{YUAN2025111138,
title = {Fully exploring object relation interaction and hidden state attention for video captioning},
journal = {Pattern Recognition},
volume = {159},
pages = {111138},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111138},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008896},
author = {Feiniu Yuan and Sipei Gu and Xiangfen Zhang and Zhijun Fang},
keywords = {Video Captioning, Knowledge Graph, Encoder, Decoder, Convolutional Neural Network},
abstract = {Video Captioning (VC) is a challenging task of automatically generating natural language sentences for describing video contents. As a video often contains multiple objects, it is comprehensively crucial to identify multiple objects and model relationships between them. Previous models usually adopt Graph Convolutional Networks (GCN) to infer relational information via object nodes, but there exist uncertainty and over-smoothing issues of relational reasoning. To tackle these issues, we propose a Knowledge Graph based Video Captioning Network (KG-VCN) by fully exploring object relation interaction, hidden state and attention enhancement. In encoding stages, we present a Graph and Convolution Hybrid Encoder (GCHE), which uses an object detector to find visual objects with bounding boxes for Knowledge Graph (KG) and Convolutional Neural Network (CNN). To model intrinsic relations between detected objects, we propose a knowledge graph based Object Relation Graph Interaction (ORGI) module. In ORGI, we design triplets (head, relation, tail) to efficiently mine object relations, and create a global node to enable adequate information flow among all graph nodes for avoiding possibly missed relations. To produce accurate and rich captions, we propose a hidden State and Attention Enhanced Decoder (SAED) by integrating hidden states and dynamically updated attention features. Our SAED accepts both relational and visual features, adopts Long Short-Term Memory (LSTM) to produce hidden states, and dynamically update attention features. Unlike existing methods, we concatenate state and attention features to predict next word sequentially. To demonstrate the effectiveness of our model, we conduct experiments on three well-known datasets (MSVD, MSR-VTT, VaTeX), and our model achieves impressive results significantly outperforming existing state-of-the-art models.}
}
@article{LIN2025111086,
title = {NAS-BNN: Neural Architecture Search for Binary Neural Networks},
journal = {Pattern Recognition},
volume = {159},
pages = {111086},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111086},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008379},
author = {Zhihao Lin and Yongtao Wang and Jinhe Zhang and Xiaojie Chu and Haibin Ling},
keywords = {Neural architecture search, Binary neural network, Deep learning},
abstract = {Binary Neural Networks (BNNs) have gained extensive attention for their superior inferencing efficiency and compression ratio compared to traditional full-precision networks. However, due to the unique characteristics of BNNs, designing a powerful binary architecture is challenging and often requires significant manpower. A promising solution is to utilize Neural Architecture Search (NAS) to assist in designing BNNs, but current NAS methods for BNNs are relatively straightforward and leave a performance gap between the searched models and manually designed ones. To address this gap, we propose a novel neural architecture search scheme for binary neural networks, named NAS-BNN. We first carefully design a search space based on the unique characteristics of BNNs. Then, we present three training strategies, which significantly enhance the training of supernet and boost the performance of all subnets. Our discovered binary model family outperforms previous BNNs for a wide range of operations (OPs) from 20M to 200M. For instance, we achieve 68.20% top-1 accuracy on ImageNet with only 57M OPs. In addition, we validate the transferability of these searched BNNs on the object detection task, and our binary detectors with the searched BNNs achieve a novel state-of-the-art result, e.g., 31.6% mAP with 370M OPs, on MS COCO dataset. The source code and models will be released at https://github.com/VDIGPKU/NAS-BNN.}
}
@article{GUAN2025111183,
title = {Dual Contrastive Label Enhancement},
journal = {Pattern Recognition},
volume = {160},
pages = {111183},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111183},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009348},
author = {Ren Guan and Yifei Wang and Xinyuan Liu and Bin Chen and Jihua Zhu},
keywords = {Label distribution learning, Label enhancement, Contrastive learning},
abstract = {Label Enhancement (LE) strives to convert logical labels of instances into label distributions to provide data preparation for label distribution learning (LDL). Existing LE methods ordinarily neglect to consider original features and logical labels as two complementary descriptive views of instances for extracting implicit related information across views, resulting in insufficient utilization of the feature and logical label information of the instances. To address this issue, we propose a novel method named Dual Contrastive Label Enhancement (DCLE). This method regards original features and logical labels as two view-specific descriptions and encodes them into a unified projection space. We employ dual contrastive learning strategy at both instance-level and class-level to excavate cross-view consensus information and distinguish instance representations by exploring inherent correlations among features, thereby generating high-level representations of the instances. Subsequently, to recover label distributions from obtained high-level representations, we design a distance-minimized and margin-penalized training strategy and preserve the consistency of label attributes. Extensive experiments conducted on 13 benchmark datasets of LDL validate the efficacy and competitiveness of DCLE.}
}
@article{GAO2025111170,
title = {Learning accurate and enriched features for stereo image super-resolution},
journal = {Pattern Recognition},
volume = {159},
pages = {111170},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111170},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400921X},
author = {Hu Gao and Depeng Dang},
keywords = {Stereo image super-resolution, Mixed-scale feature representation, Selective fusion attention module, Fast fourier convolution},
abstract = {Stereo image super-resolution (stereoSR) aims to enhance the quality of super-resolution results by incorporating complementary information from an alternative view. Although current methods have shown significant advancements, they typically operate on representations at full resolution to preserve spatial details, facing challenges in accurately capturing contextual information. Simultaneously, they utilize all feature similarities to cross-fuse information from the two views, potentially disregarding the impact of irrelevant information. To overcome this problem, we propose a mixed-scale selective fusion network (MSSFNet) to preserve precise spatial details and incorporate abundant contextual information, and adaptively select and fuse most accurate features from two views to enhance the promotion of high-quality stereoSR. Specifically, we develop a mixed-scale block (MSB) that obtains contextually enriched feature representations across multiple spatial scales while preserving precise spatial details. Furthermore, to dynamically retain the most essential cross-view information, we design a selective fusion attention module (SFAM) that searches and transfers the most accurate features from another view. To learn an enriched set of local and non-local features, we introduce a fast fourier convolution block (FFCB) to explicitly integrate frequency domain knowledge. Extensive experiments show that MSSFNet achieves significant improvements over state-of-the-art approaches on both quantitative and qualitative evaluations. The code and the pre-trained models will be released at https://github.com/Tombs98/MSSFNet.}
}
@article{LI2025111130,
title = {EGO-LM: An efficient, generic, and out-of-the-box language model for handwritten text recognition},
journal = {Pattern Recognition},
volume = {159},
pages = {111130},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111130},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008811},
author = {Hongliang Li and Dezhi Peng and Lianwen Jin},
keywords = {Handwritten text recognition, Online and offline text recognition, Language model, Optical character recognition},
abstract = {The language model (LM) plays a crucial role in post-processing handwritten text recognition (HTR) by capturing linguistic patterns. However, traditional rule-based LMs are inefficient, and recent end-to-end LMs require customized training for each HTR model. To address these limitations, we propose an Efficient, Generic, and Out-of-the-box Language Model (EGO-LM) for HTR. To unlock the out-of-the-box capability of the end-to-end LM, we introduce a vision-limited proxy task that focuses on visual-pattern-agnostic linguistic dependencies during training, enhancing the robustness and generality of the LM. The enhanced capabilities also enable EGO-LM to iteratively refine its output for a further accuracy boost without additional tuning. Moreover, we introduce a Diverse-Corpus Online Handwriting dataset (DCOH-120K) with more diverse corpus types and more samples than existing datasets, including 83,142 Chinese and 39,398 English text lines. Extensive experiments demonstrate that EGO-LM can attain state-of-the-art performance while achieving up to 613× acceleration. The DCOH-120K dataset is available at .}
}
@article{YANG2025111102,
title = {Improving the sparse coding model via hybrid Gaussian priors},
journal = {Pattern Recognition},
volume = {159},
pages = {111102},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111102},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008537},
author = {Lijian Yang and Jianxun Mi and Weisheng Li and Guofen Wang and Bin Xiao},
keywords = {Sparse coding, Denoising, Deep unfolding, Elastic net},
abstract = {Sparse Coding (SC) imposes a sparse prior on the representation coefficients under a dictionary or a sensing matrix. However, the sparse regularization, approximately expressed as the L1-norm, is not strongly convex. The uniqueness of the optimal solution requires the dictionary to be of low mutual coherence. As a specialized form of SC, Convolutional Sparse Coding (CSC) encounters the same issue. Inspired by the Elastic Net, this paper proposes to learn an additional anisotropic Gaussian prior for the sparse codes, thus improving the convexity of the SC problem and enabling the modeling of feature correlation. As a result, the SC problem is modified by the proposed elastic projection. We thereby analyze the effectiveness of the proposed method under the framework of LISTA and demonstrate that this simple technique has the potential to correct bad codes and reduce the error bound, especially in noisy scenarios. Furthermore, we extend this technique to the CSC model for the vision practice of image denoising. Extensive experimental results show that the learned Gaussian prior significantly improves the performance of both the SC and CSC models. Source codes are available at https://github.com/eeejyang/EPCSCNet.}
}
@article{SAJID2025111142,
title = {GB-RVFL: Fusion of randomized neural network and granular ball computing},
journal = {Pattern Recognition},
volume = {159},
pages = {111142},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111142},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008938},
author = {M. Sajid and A. Quadir and M. Tanveer},
keywords = {Random Vector Functional Link (RVFL), Granular computation, Scalability, Noise, Graph embedding, Interpretability},
abstract = {The random vector functional link (RVFL) network is a prominent classification model with strong generalization ability. However, RVFL treats all samples uniformly, ignoring whether they are pure or noisy, and its scalability is limited due to the need for inverting the entire training matrix. To address these issues, we propose granular ball RVFL (GB-RVFL) model, which uses granular balls (GBs) as inputs instead of training samples. This approach enhances scalability by requiring only the inverse of the matrix of GBs’ centers and improves robustness against noise and outliers through the coarse granularity of GBs. Furthermore, RVFL overlooks the dataset’s geometric structure. To address this, we propose graph embedding GB-RVFL (GE-GB-RVFL) model, which fuses granular computing and graph embedding (GE) to preserve the topological structure of GBs and enhances the interpretability of the proposed model. The proposed GB-RVFL and GE-GB-RVFL models are evaluated on KEEL, UCI, NDC and biomedical datasets such as Alzheimer’s disease diagnosis and breast cancer prediction. The experimental evaluation demonstrates that the proposed models outperform baseline models in efficacy, robustness, scalability, and interpretability. The source codes of the proposed GB-RVFL and GE-GB-RVFL models are available at https://github.com/mtanveer1/GB-RVFL.}
}
@article{CAO2025111129,
title = {Enhancing robust VQA via contrastive and self-supervised learning},
journal = {Pattern Recognition},
volume = {159},
pages = {111129},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111129},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400880X},
author = {Runlin Cao and Zhixin Li and Zhenjun Tang and Canlong Zhang and Huifang Ma},
keywords = {Visual question answering, Language priors, Robust VQA, Contrastive learning, Self-supervised learning},
abstract = {Visual Question Answering (VQA) aims to evaluate the reasoning abilities of an intelligent agent using visual and textual information. However, recent research indicates that many VQA models rely primarily on learning the correlation between questions and answers in the training dataset rather than demonstrating actual reasoning ability. To address this limitation, we propose a novel training approach called Enhancing Robust VQA via Contrastive and Self-supervised Learning (CSL-VQA) to construct a more robust VQA model. Our approach involves generating two types of negative samples to balance the biased data, using self-supervised auxiliary tasks to help the base VQA model overcome language priors, and filtering out biased training samples. In addition, we construct positive samples by removing spurious correlations in biased samples and perform auxiliary training through contrastive learning. Our approach does not require additional annotations and is compatible with different VQA backbones. Experimental results demonstrate that CSL-VQA significantly outperforms current state-of-the-art approaches, achieving an accuracy of 62.30% on the VQA-CP v2 dataset, while maintaining robust performance on the in-distribution VQA v2 dataset. Moreover, our method shows superior generalization capabilities on challenging datasets such as GQA-OOD and VQA-CE, proving its effectiveness in reducing language bias and enhancing the overall robustness of VQA models.}
}
@article{BLEY2025111171,
title = {Explaining predictive uncertainty by exposing second-order effects},
journal = {Pattern Recognition},
volume = {160},
pages = {111171},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111171},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009221},
author = {Florian Bley and Sebastian Lapuschkin and Wojciech Samek and Grégoire Montavon},
keywords = {Explainable AI, Predictive uncertainty, Ensemble models, Second-order attribution},
abstract = {Explainable AI has brought transparency to complex ML black boxes, enabling us, in particular, to identify which features these models use to make predictions. So far, the question of how to explain predictive uncertainty, i.e., why a model ‘doubts’, has been scarcely studied. Our investigation reveals that predictive uncertainty is dominated by second-order effects, involving single features or product interactions between them. We contribute a new method for explaining predictive uncertainty based on these second-order effects. Computationally, our method reduces to a simple covariance computation over a collection of first-order explanations. Our method is generally applicable, allowing for turning common attribution techniques (LRP, Gradient×Input, etc.) into powerful second-order uncertainty explainers, which we call CovLRP, CovGI, etc. The accuracy of the explanations our method produces is demonstrated through systematic quantitative evaluations, and the overall usefulness of our method is demonstrated through two practical showcases.}
}
@article{CHEN2025111098,
title = {ACFNet: An adaptive cross-fusion network for infrared and visible image fusion},
journal = {Pattern Recognition},
volume = {159},
pages = {111098},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111098},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008495},
author = {Xiaoxuan Chen and Shuwen Xu and Shaohai Hu and Xiaole Ma},
keywords = {Image fusion, Object detection, Auto-encoder, Global features, Adaptive fusion method},
abstract = {Considering the prospects for image fusion, it is necessary to guide the fusion to adapt to downstream vision tasks. In this paper, we propose an Adaptive Cross-Fusion Network (ACFNet) that utilizes an adaptive approach to fuse infrared and visible images, addressing cross-modal differences to enhance object detection performance. In ACFNet, a hierarchical cross-fusion module is designed to enrich the features at each level of the reconstructed images. In addition, a special adaptive gating selection module is proposed to realize feature fusion in an adaptive manner so as to obtain fused images without the interference of manual design. Extensive qualitative and quantitative experiments have demonstrated that ACFNet is superior to current state-of-the-art fusion methods and achieves excellent results in preserving target information and texture details. The fusion framework, when combined with the object detection framework, has the potential to significantly improve the precision of object detection in low-light conditions.}
}
@article{HUANG2025111195,
title = {Debiasing weighted multi-view k-means clustering based on causal regularization},
journal = {Pattern Recognition},
volume = {160},
pages = {111195},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111195},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009464},
author = {Xiuqi Huang and Hong Tao and Haotian Ni and Chenping Hou},
keywords = {Multi-view, Clustering, Covariate balance, Causal regularization},
abstract = {In the field of unsupervised learning, many methods such as clustering rely on exploring the correlations among features. However, considering these correlations is not always advantageous for learning models. The biased selection of data may lead to redundant and unstable correlations among features, adversely affecting the performance of learning models. Multi-view data presents more complex feature correlations with potential redundancy and varying distributions across views, necessitating detailed analysis. This paper proposes a causal regularized debiased multi-view k-means clustering (DMKC) method to counteract redundant feature correlations stemming from sample selection bias. This method introduces a covariate weighted balance method from causal inference to mitigate redundant bias in multi-view clustering by adjusting sample weights. The approach combines sample and view weights within a k-means loss framework, effectively eliminating feature redundancy and enhancing clustering performance amidst sample selection bias. The optimization process of the relevant parameters is detailed in this paper, and comprehensive experiments demonstrate the effectiveness of the method.}
}
@article{WANG2025111153,
title = {CAST: An innovative framework for Cross-dimensional Attention Structure in Transformers},
journal = {Pattern Recognition},
volume = {159},
pages = {111153},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111153},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400904X},
author = {Dezheng Wang and Xiaoyi Wei and Congyan Chen},
keywords = {Cross-dimensional attention structure, Static attention mechanism, Time series forecasting},
abstract = {Dominant Transformer-based approaches rely solely on attention mechanisms and their variations, primarily emphasizing capturing crucial information within the temporal dimension. For enhanced performance, we introduce a novel architecture for Cross-dimensional Attention Structure in Transformers (CAST), which presents an innovative approach in Transformer-based models, emphasizing attention mechanisms across both temporal and spatial dimensions. The core component of CAST, the cross-dimensional attention structure (CAS), captures dependencies among multivariable time series in both temporal and spatial dimensions. The Static Attention Mechanism (SAM) is incorporated to simplify and enhance multivariate time series forecasting performance. This integration effectively reduces complexity, leading to a more efficient model. CAST demonstrates robust and efficient capabilities in predicting multivariate time series, with the simplicity of SAM broadening its applicability to various tasks. Beyond time series forecasting, CAST also shows promise in CV classification tasks. By integrating CAS into pre-trained image models, CAST facilitates spatiotemporal reasoning. Experimental results highlight the superior performance of CAST in time series forecasting and its competitive edge in CV classification tasks.}
}
@article{ZHANG2025111113,
title = {Structured multi-view k-means clustering},
journal = {Pattern Recognition},
volume = {160},
pages = {111113},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111113},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008641},
author = {Zitong Zhang and Xiaojun Chen and Chen Wang and Ruili Wang and Wei Song and Feiping Nie},
keywords = {Clustering, Multi-view clustering, -means, Structure learning},
abstract = {K-means is a very efficient clustering method and many multi-view k-means clustering methods have been proposed for multi-view clustering during the past decade. However, since k-means have trouble uncovering clusters of varying sizes and densities, these methods suffer from the same performance issues as k-means. Improving the clustering performance of multi-view k-means has become a challenging problem. In this paper, we propose a new multi-view k-means clustering method that is able to uncover clusters in arbitrary sizes and densities. The new method simultaneously performs three tasks, i.e., sparse connection probability matrices learning, prototypes aligning, and cluster structure learning. We evaluate the proposed new method by 5 benchmark datasets and compare it with 11 multi-view clustering methods. The experimental results on both synthetic and real-world experiments show the superiority of our proposed method.}
}
@article{WEN2025111137,
title = {Highly realistic synthetic dataset for pixel-level DensePose estimation via diffusion model},
journal = {Pattern Recognition},
volume = {159},
pages = {111137},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111137},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008884},
author = {Jiaxiao Wen and Tao Chu and Qiong Liu},
keywords = {Dense pose estimation, Synthetic data, Image generation, Diffusion model},
abstract = {Generating training data with pixel-level annotations for DensePose is a labor-intensive task, resulting in sparse labeling in real-world datasets. Prior solutions have relied on specialized data generation systems to synthesize datasets. However, these synthetic datasets often lack realism and rely on expensive resources such as human body models and texture mappings. In this paper, we address these challenges by introducing a novel data generation method based on the diffusion model, effectively producing highly realistic data without the need for expensive resources. Specifically, our method comprises annotation generation and image generation. Utilizing graphic renderers and SMPL models, we produce synthetic annotations solely based on human poses and shapes. Subsequently, guided by these annotations, we employ simple yet effective textual prompts to generate a wide range of realistic images using the diffusion model. Our experiments conducted on DensePose-COCO dataset demonstrate the superiority of our method compared to existing methods. Code and benchmarks will be released.}
}
@article{WANG2025111085,
title = {Global-aware Fragment Representation Aggregation Network for image–text retrieval},
journal = {Pattern Recognition},
volume = {159},
pages = {111085},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111085},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008367},
author = {Di Wang and Jiabo Tian and Xiao Liang and Yumin Tian and Lihuo He},
keywords = {Image–text retrieval, Aggregation, Attention},
abstract = {Image–text retrieval is an important kind of cross-modal retrieval method and has recently attracted much attention. Existing image–text retrieval methods often ignore the relative importance of each fragment (region in an image or word in a sentence) on the global semantic of image or text when aggregating features of image or text fragments, resulting in the ineffectiveness of the learned image and text representations. To address this problem, we propose an image–text retrieval method named Global-aware Fragment Representation Aggregation Network (GFRAN). Specifically, it first designs a fine-grained multimodal information interaction module based on the self-attention mechanism to model both the intra-modality and inter-modality relationships between image regions and words. Then, with the guidance of the global image or text feature, it aggregates image or text fragment features conditioned on their attention weights over global feature, to highlight fragments that contribute more to the overall semantics of images and texts. Extensive experiments on two benchmark datasets Flickr30K and MS-COCO demonstrate the superiority of the proposed GFRAN model over several state-of-the-art baselines.}
}
@article{REIS2025111182,
title = {DSCIMABNet: A novel multi-head attention depthwise separable CNN model for skin cancer detection},
journal = {Pattern Recognition},
volume = {159},
pages = {111182},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111182},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009336},
author = {Hatice Catal Reis and Veysel Turk},
keywords = {Deep features, Dermoscopic images, Depthwise separable convolution, DSCIMABNet deep learning model, Deep learning-ensemble learning fusion, Skin cancer identification, Vision transformer},
abstract = {Skin cancer is a common type of cancer worldwide. Early diagnosis of skin cancer can reduce the risk of death by increasing treatment success. However, it is challenging for dermatologists or specialists because the symptoms are vague in the early stages and cannot be noticed by the naked eye. This study examines digital diagnostic techniques supported by artificial intelligence, focusing on early skin cancer detection and two methods have been proposed. In the first method, DSCIMABNet deep learning architecture was developed by combining multi-head attention and depthwise separable convolution techniques. This model provides flexibility in learning the dataset's local features, abstract concepts, and long-term relationships. The DSCIMABNet model and modern deep learning models trained on ImageNet are proposed to be combined with the ensemble learning method in the second method. This approach provides a comprehensive feature extraction process that will increase the performance of the classification process with ensemble learning. The proposed approaches are trained and evaluated on the ISIC 2018 dataset with image enhancement applied in preprocessing. In the experimental results, DSCIMABNet achieved 84.28% accuracy, while the proposed hybrid method achieved 99.40% accuracy. Moreover, on the Mendeley dataset (CNN for Melanoma Detection Data), DSCIMABNet achieved 92.58% accuracy, while the hybrid method achieved 99.37% accuracy. This study may significantly contribute to developing new and effective methods for the early diagnosis and treatment of skin cancer.}
}
@article{HE2025111101,
title = {Radar gait recognition using Dual-branch Swin Transformer with Asymmetric Attention Fusion},
journal = {Pattern Recognition},
volume = {159},
pages = {111101},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111101},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008525},
author = {Wentao He and Jianfeng Ren and Ruibin Bai and Xudong Jiang},
keywords = {Micro-Doppler signature, Radar gait recognition, Spectrogram, Cadence velocity diagram, Asymmetric Attention Fusion},
abstract = {Video-based gait recognition suffers from potential privacy issues and performance degradation due to dim environments, partial occlusions, or camera view changes. Radar has recently become increasingly popular and overcome various challenges presented by vision sensors. To capture tiny differences in radar gait signatures of different people, a dual-branch Swin Transformer is proposed, where one branch captures the time variations of the radar micro-Doppler signature and the other captures the repetitive frequency patterns in the spectrogram. Unlike natural images where objects can be translated, rotated, or scaled, the spatial coordinates of spectrograms and CVDs have unique physical meanings, and there is no affine transformation for radar targets in these synthetic images. The patch splitting mechanism in Vision Transformer makes it ideal to extract discriminant information from patches, and learn the attentive information across patches, as each patch carries some unique physical properties of radar targets. Swin Transformer consists of a set of cascaded Swin blocks to extract semantic features from shallow to deep representations, further improving the classification performance. Lastly, to highlight the branch with larger discriminant power, an Asymmetric Attention Fusion is proposed to optimally fuse the discriminant features from the two branches. To enrich the research on radar gait recognition, a large-scale NTU-RGR dataset is constructed, containing 45,768 radar frames of 98 subjects. The proposed method is evaluated on the NTU-RGR dataset and the MMRGait-1.0 database. It consistently and significantly outperforms all the compared methods on both datasets. The codes are available at: https://github.com/wentaoheunnc/NTU-RGR.}
}
@article{ZHANG2025111125,
title = {Jointly stochastic fully symmetric interpolatory rules and local approximation for scalable Gaussian process regression},
journal = {Pattern Recognition},
volume = {159},
pages = {111125},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111125},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008768},
author = {Hongli Zhang and Jinglei Liu},
keywords = {Gaussian process regression, Local approximation, Stochastic fully symmetric interpolatory rules, Stochastic spherical-radial rules, Tsallis mutual information},
abstract = {When exploring the broad application prospects of large-scale Gaussian process regression (GPR), three core challenges significantly constrain its full effectiveness: firstly, the O(n3) time complexity of computing the inverse covariance matrix of n training points becomes an insurmountable performance bottleneck when processing large-scale datasets; Secondly, although traditional local approximation methods are widely used, they are often limited by the inconsistency of prediction results; The third issue is that many aggregation strategies lack discrimination when evaluating the importance of experts (i.e. local models), resulting in a loss of overall prediction accuracy. In response to the above challenges, this article innovatively proposes a comprehensive method that integrates third-degree stochastic fully symmetric interpolatory rules (TDSFSI), local approximation, and Tsallis mutual information (TDSFSIRLA), aiming to fundamentally break through existing limitations. Specifically, TDSFSIRLA first introduces an efficient third-degree stochastic fully symmetric interpolatory rules, which achieves accurate approximation of Gaussian kernel functions by generating adaptive dimensional feature maps. This innovation not only significantly reduces the number of required orthogonal nodes and effectively lowers computational costs, but also maintains extremely high approximation accuracy, providing a solid theoretical foundation for processing large-scale datasets. Furthermore, in order to overcome the inconsistency of local approximation methods, this paper adopts the Generalized Robust Bayesian Committee Machine (GRBCM) as the aggregation framework for local experts. GRBCM ensures the harmonious unity of the prediction results of each local model through its inherent consistency and robustness, significantly improving the stability and reliability of the overall prediction. More importantly, in response to the issue of uneven distribution of expert weights, this article creatively introduces Tsallis mutual information as a metric for weight allocation. Tsallis mutual information, with its sensitive ability to capture information complexity, assigns weights to different local experts that match their contribution, effectively solving the problem of prediction bias caused by uneven weight distribution and further improving prediction accuracy. In the experimental verification phase, this article conducted comprehensive testing on multiple synthetic datasets and seven representative real datasets. The results show that the TDSFSIRLA method not only achieves significant reduction in time complexity, but also demonstrates excellent performance in prediction accuracy, fully verifying its significant advantages and broad application prospects in the field of large-scale Gaussian process regression.}
}
@article{LIAO2025111112,
title = {Self-supervised random mask attention GAN in tackling pose-invariant face recognition},
journal = {Pattern Recognition},
volume = {159},
pages = {111112},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111112},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400863X},
author = {Jiashu Liao and Tanaya Guha and Victor Sanchez},
keywords = {Face image frontalization, Pose, Face, Adversarial forensics, GAN, Self-supervision},
abstract = {Pose Invariant Face Recognition (PIFR) has significantly advanced with Generative Adversarial Networks (GANs), which rotate face images acquired at any angle to a frontal view for enhanced recognition. However, such frontalization methods typically need ground-truth frontal-view images, often collected under strict laboratory conditions, making it challenging and costly to acquire the necessary training data. Additionally, traditional self-supervised PIFR methods rely on external rendering models for training, further complicating the overall training process. To tackle these two issues, we propose a new framework called Mask Rotate. Our framework introduces a novel training approach that requires no paired ground truth data for the face image frontalization task. Moreover, it eliminates the need for an external rendering model during training. Specifically, our framework simplifies the face image frontalization task by transforming it into a face image completion task. During the inference or testing stage, it employs a reliable pre-trained rendering model to obtain a frontal-view face image, which may have several regions with missing texture due to pose variations and occlusion. Our framework then uses a novel self-supervised Random Mask Attention Generative Adversarial Network (RMAGAN) to fill in these missing regions by considering them as randomly masked regions. Furthermore, our proposed Mask Rotate framework uses a reliable post-processing model designed to improve the visual quality of the face images after frontalization. In comprehensive experiments, the Mask Rotate framework eliminates the requirement for complex computations during training and achieves strong results, both qualitative and quantitative, compared to the state-of-the-art.}
}
@article{XU2025111141,
title = {Incremental feature selection: Parallel approach with local neighborhood rough sets and composite entropy},
journal = {Pattern Recognition},
volume = {159},
pages = {111141},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111141},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008926},
author = {Weihua Xu and Weirui Ye},
keywords = {Composite entropy, Feature selection, Incremental algorithm, Rough set},
abstract = {Rough set theory is a powerful mathematical framework for managing uncertainty and is widely utilized in feature selection. However, traditional rough set-based feature selection algorithms encounter significant challenges, especially when processing large-scale incremental data and adapting to the dynamic nature of real-world scenarios, where both data volume and feature sets are continuously changing. To overcome these limitations, this study proposes an innovative algorithm that integrates local neighborhood rough sets with composite entropy to measure uncertainty in information systems more accurately. By incorporating decision distribution, composite entropy enhances the precision of uncertainty quantification, thereby improving the effectiveness of the algorithm in feature selection. To further improve performance in handling large-scale incremental data, matrix operations are employed in place of traditional set-based methods, allowing the algorithm to fully utilize modern hardware capabilities for accelerated processing. Additionally, parallel computing technology is integrated to further enhance computational speed. An incremental version of the algorithm is also introduced to better adapt to dynamic data environments, increasing its flexibility and practicality. Comprehensive experimental evaluations demonstrate that the proposed algorithm significantly surpasses existing methods in both effectiveness and efficiency.}
}
@article{CHEN2025111089,
title = {Exploring sample relationship for few-shot classification},
journal = {Pattern Recognition},
volume = {159},
pages = {111089},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111089},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008409},
author = {Xingye Chen and Wenxiao Wu and Li Ma and Xinge You and Changxin Gao and Nong Sang and Yuanjie Shao},
keywords = {Few-shot classification, Sample Relationship Exploration, Meta-learning, Transfer learning},
abstract = {Few-shot classification (FSC) is a challenging problem, which aims to identify novel classes with limited samples. Most existing methods employ vanilla transfer learning or episodic meta-training to learn a feature extractor, and then measure the similarity between the query image and the few support examples of novel classes. However, these approaches merely learn feature representations from individual images, overlooking the exploration of the interrelationships among images. This neglect can hinder the attainment of more discriminative feature representations, thus limiting the potential improvement of few-shot classification performance. To address this issue, we propose a Sample Relationship Exploration (SRE) module comprising the Sample-level Attention (SA), Explicit Guidance (EG) and Channel-wise Adaptive Fusion (CAF) components, to learn discriminative category-related features. Specifically, we first employ the SA component to explore the similarity relationships among samples and obtain aggregated features of similar samples. Furthermore, to enhance the robustness of these features, we introduce the EG component to explicitly guide the learning of sample relationships by providing an ideal affinity map among samples. Finally, the CAF component is adopted to perform weighted fusion of the original features and the aggregated features, yielding category-related embeddings. The proposed method is a plug-and-play module which can be embedded into both transfer learning and meta-learning based few-shot classification frameworks. Extensive experiments on benchmark datasets show that the proposed module can effectively improve the performance over baseline models, and also perform competitively against the state-of-the-art algorithms. The source code is available at https://github.com/Chenguoz/SRE.}
}
@article{CHEN2025111154,
title = {DyConfidMatch: Dynamic thresholding and re-sampling for 3D semi-supervised learning},
journal = {Pattern Recognition},
volume = {159},
pages = {111154},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111154},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009051},
author = {Zhimin Chen and Bing Li},
keywords = {Semi-supervised learning, 3D detection, 3D classification, Data imbalance, Point clouds},
abstract = {Semi-supervised learning (SSL) leverages limited labeled and abundant unlabeled data but often faces challenges with data imbalance, especially in 3D contexts. This study investigates class-level confidence as an indicator of learning status in 3D SSL, proposing a novel method that utilizes dynamic thresholding to better use unlabeled data, particularly from underrepresented classes. A re-sampling strategy is also introduced to mitigate bias towards well-represented classes, ensuring equitable class representation. Through extensive experiments in 3D SSL, our method surpasses state-of-the-art counterparts in classification and detection tasks, highlighting its effectiveness in tackling data imbalance. This approach presents a significant advancement in SSL for 3D datasets, providing a robust solution for data imbalance issues.}
}
@article{WANG2025111178,
title = {Lightweight remote sensing super-resolution with multi-scale graph attention network},
journal = {Pattern Recognition},
volume = {160},
pages = {111178},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111178},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009294},
author = {Yu Wang and Zhenfeng Shao and Tao Lu and Xiao Huang and Jiaming Wang and Zhizheng Zhang and Xiaolong Zuo},
keywords = {Remote sensing, Multi-scale network, Lightweight network, Super-resolution, Graph attention network},
abstract = {Remote Sensing Super-Resolution (RS-SR) constitutes a pivotal component in the domain of remote sensing image analysis, aimed at enhancing the spatial resolution of low-resolution imagery. Recent advancements have seen deep learning techniques achieving substantial progress in the RS-SR field. Notably, Graph Neural Networks (GNNs) have emerged as a potent mechanism for processing remote sensing images, adept at elucidating the intricate inter-pixel relationships within images. Nevertheless, a prevalent limitation among existing GNN-based methodologies is their disregard for the high computational demands, which circumscribes their applicability in environments with limited computational resources. This paper introduces a streamlined RS-SR framework, leveraging a Multi-Scale Graph Attention Network (MSGAN), designed to effectively balance computational efficiency with high performance. The core of MSGAN is a novel multi-scale graph attention module, integrating graph attention block and multi-scale lattice block structures, engineered to comprehensively assimilate both localized and extensive spatial information in remote sensing images. This enhances the framework’s overall efficacy and resilience in RS-SR tasks. Comparative experimental analyses demonstrate that MSGAN delivers competitive results against state-of-the-art methods while reducing parameter count and computational overhead, presenting a promising avenue for deployment in scenarios with limited computational resources.}
}
@article{YANG2025111097,
title = {Eye-movement-prompted large image captioning model},
journal = {Pattern Recognition},
volume = {159},
pages = {111097},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111097},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008483},
author = {Zheng Yang and Bing Han and Xinbo Gao and Zhi-Hui Zhan},
keywords = {Image captioning, Large model, Eye-movement, Dataset, Interpretability},
abstract = {Pretrained large vision-language models have shown outstanding performance on the task of image captioning. However, owing to the insufficient decoding of image features, existing large models sometimes lose important information, such as objects, scenes, and their relationships. In addition, the complex “black-box” nature of these models makes their mechanisms difficult to explain. Research shows that humans learn richer representations than machines do, which inspires us to improve the accuracy and interpretability of large image captioning models by combining human observation patterns. We built a new dataset, called saliency in image captioning (SIC), to explore relationships between human vision and language representation. One thousand images with rich context information were selected as image data of SIC. Each image was annotated with five caption labels and five eye-movement labels. Through analysis of the eye-movement data, we found that humans efficiently captured comprehensive information for image captioning during their observations. Therefore, we propose an eye-movement-prompted large image captioning model, which is embedded with two carefully designed modules: the eye-movement simulation module (EMS) and the eye-movement analyzing module (EMA). EMS combines the human observation pattern to simulate eye-movement features, including the positions and scan paths of eye fixations. EMA is a graph neural network (GNN) based module, which decodes graphical eye-movement data and abstracts image features as a directed graph. More accurate descriptions can be predicted by decoding the generated graph. Extensive experiments were conducted on the MS-COCO and NoCaps datasets to validate our model. The experimental results showed that our network was interpretable, and could achieve superior results compared with state-of-the-art methods, i.e., 84.2% BLEU-4 and 145.1% CIDEr-D on MS-COCO Karpathy test split, indicating its strong potential for use in image captioning.}
}
@article{HU2025111194,
title = {Optimizing reinforcement learning for large action spaces via generative models: Battery pattern selection},
journal = {Pattern Recognition},
volume = {160},
pages = {111194},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111194},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009452},
author = {Jingwei Hu and Xinjie Li and Xiaodong Li and Zhensong Hou and Zhihong Zhang},
keywords = {Active equalization, Generative model, Reinforcement learning, Dynamic reconfigurable battery},
abstract = {Intrinsic and environmental factors contribute to variability in the performance of cells within a battery pack, affecting the lifespan and safety of battery systems. To solve this problem, active and passive equalization methods are proposed. However, existing passive equalization methods suffer from energy loss and low efficiency among batteries, while existing active equalization methods necessitate complex expert knowledge and control algorithms. We propose an active equalization model that leverages a generative model (GM) to assist in pattern selection for a reinforcement learning (RL) scheme, tailored for Dynamic Reconfigurable Battery (DRB) systems. The proposed model overcomes the pattern selection challenge in large-scale discrete action spaces by employing a Variational Autoencoder (VAE) for dimensionality reduction and latent space mapping, actively balancing DRB systems. Moreover, the use of pattern subgraphs diminishes dependence on expert knowledge, enabling the model to recognize structural information and adjust the system’s stability. The experimental setup adheres to the laws of physics and tests the model’s functionality on a simulation system. Results show that the proposed Generative Model-based Reinforcement Learning (GMRL) approach effectively addresses decision-making challenges in large-scale spaces. It can learn the structured features of the battery network, thus balancing the energy storage system and maximizing discharge efficiency gains.}
}
@article{JIANG2025111144,
title = {Cross-modal adapter for vision–language retrieval},
journal = {Pattern Recognition},
volume = {159},
pages = {111144},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111144},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008951},
author = {Haojun Jiang and Jianke Zhang and Rui Huang and Chunjiang Ge and Zanlin Ni and Shiji Song and Gao Huang},
keywords = {Adapter, Cross-modal interaction, Cross-modal retrieval, Parameter-efficient training, Multi-modal learning},
abstract = {Vision–language retrieval is an important multi-modal learning topic, where the goal is to retrieve the most relevant visual candidate for a given text query. Recently, pre-trained models, e.g., CLIP, show great potential on retrieval tasks. However, as pre-trained models are scaling up, fully fine-tuning them on donwstream retrieval datasets has a high risk of overfitting. Moreover, in practice, it would be costly to train and store a large model for each task. To overcome the above issues, we present a novel Cross-Modal Adapter for parameter-efficient transfer learning. Inspired by adapter-based methods, we adjust the pre-trained model with a few parameterization layers. However, there are two notable differences. First, our method is designed for the multi-modal domain. Secondly, it allows encoder-level implicit cross-modal interactions between vision and language encoders. Although surprisingly simple, our approach has three notable benefits: (1) reduces the vast majority of fine-tuned parameters, (2) saves training time, and (3) allows all the pre-trained parameters to be fixed, enabling the pre-trained model to be shared across datasets. Extensive experiments demonstrate that, without bells and whistles, our approach outperforms adapter-based methods on image–text retrieval datasets (MSCOCO, Flickr30K) and video–text retrieval datasets (MSR-VTT, DiDeMo, and ActivityNet).}
}
@article{SA2025111092,
title = {ECTFormer: An efficient Conv-Transformer model design for image recognition},
journal = {Pattern Recognition},
volume = {159},
pages = {111092},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111092},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008434},
author = {Jaewon Sa and Junhwan Ryu and Heegon Kim},
keywords = {Conv-Transformer network, Lightweight architecture, Dynamic kernel sizes, Efficient overlapping patchify, Efficient self-attention mechanism},
abstract = {Since the success of Vision Transformers (ViTs), there has been growing interest in combining ConvNets and Transformers in the computer vision community. While the hybrid models have demonstrated state-of-the-art performance, many of these models are too large and complex to be applied to edge devices for real-world applications. To address this challenge, we propose an efficient hybrid network called ECTFormer that leverages the strengths of ConvNets and Transformers while considering both model performance and inference speed. Specifically, our approach involves: (1) optimizing the combination of convolution kernels by dynamically adjusting kernel sizes based on the scale of feature tensors; (2) revisiting existing overlapping patchify to not only reduce the model size but also propagate fine-grained patches for the performance enhancement; and (3) introducing an efficient single-head self-attention mechanism, rather than multi-head self-attention in the base Transformer, to minimize the increase in model size and boost inference speed, overcoming bottlenecks of ViTs. In experimental results on ImageNet-1K, ECTFormer not only demonstrates comparable or higher top-1 accuracy but also faster inference speed on both GPUs and edge devices compared to other efficient networks.}
}
@article{CHEN2025111189,
title = {Percept, Chat, Adapt: Knowledge transfer of foundation models for open-world video recognition},
journal = {Pattern Recognition},
volume = {160},
pages = {111189},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111189},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009403},
author = {Boyu Chen and Siran Chen and Kunchang Li and Qinglin Xu and Yu Qiao and Yali Wang},
keywords = {Open-world video recognition, Foundation models, Knowledge adaptation, Multi-modality},
abstract = {Open-world video recognition is challenging since traditional networks are not generalized well on complex environment variations. Alternatively, foundation models with rich knowledge have recently shown their generalization power. However, how to apply such knowledge has not been fully explored for open-world video recognition. To this end, we propose a generic knowledge transfer pipeline, which progressively exploits and integrates external multimodal knowledge from foundation models to boost open-world video recognition. We name it PCA, based on three stages of Percept, Chat, and Adapt. First, we perform Percept process to reduce the video domain gap and obtain external visual knowledge. Second, we generate rich linguistic semantics as external textual knowledge in Chat stage. Finally, we blend external multimodal knowledge in Adapt stage, by inserting multimodal knowledge adaptation modules into networks. We conduct extensive experiments on three challenging open-world video benchmarks, i.e., TinyVIRAT, ARID, and QV-Pipe. Our approach achieves state-of-the-art performance on all three datasets.}
}
@article{WANG2025111116,
title = {Data augmentation strategies for semi-supervised medical image segmentation},
journal = {Pattern Recognition},
volume = {159},
pages = {111116},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111116},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008677},
author = {Jiahui Wang and Dongsheng Ruan and Yang Li and Zefeng Wang and Yongquan Wu and Tao Tan and Guang Yang and Mingfeng Jiang},
keywords = {Semi-supervised segmentation, Cropping and stitching, Laplace pyramid fusion, Mutual consistency},
abstract = {Exploiting unlabeled and labeled data augmentations has become considerably important for semi-supervised medical image segmentation tasks. However, existing data augmentation methods, such as Cut-mix and generative models, typically dependent on consistency regularization or ignore data correlation between slices. To address cognitive biases problems, we propose two novel data augmentation strategies and a Dual Attention-guided Consistency network (DACNet) to improve semi-supervised medical image segmentation performance significantly. For labeled data augmentation, we randomly crop and stitch annotated data rather than unlabeled data to create mixed annotated data, which breaks the anatomical structures and introduces voxel-level uncertainty in limited annotated data. For unlabeled data augmentation, we combine the diffusion model with the Laplacian pyramid fusion strategy to generate unlabeled data with higher slice correlation. To enhance the decoders to learn different semantic but discriminative features, we propose the DACNet to achieve structural differentiation by introducing spatial and channel attention into the decoders. Extensive experiments are conducted to show the effectiveness and generalization of our approach. Specifically, our proposed labeled and unlabeled data augmentation strategies improved accuracy by 0.3% to 16.49% and 0.22% to 1.72%, respectively, when compared with various state-of-the-art semi-supervised methods. Furthermore, our DACNet outperforms existing methods on three medical datasets (91.72% dice score with 20% labeled data on the LA dataset). Source code will be publicly available at https://github.com/Oubit1/DACNet.}
}
@article{YU2025111161,
title = {Adaptive representation learning and sample weighting for low-quality 3D face recognition},
journal = {Pattern Recognition},
volume = {159},
pages = {111161},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111161},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009129},
author = {Cuican Yu and Fengxun Sun and Zihui Zhang and Huibin Li and Liming Chen and Jian Sun and Zongben Xu},
keywords = {Low-quality 3D face recognition, Adaptive representation learning, Adaptive sample weighting},
abstract = {3D face recognition (3DFR) algorithms have advanced significantly in the past two decades by leveraging facial geometric information, but they mostly focus on high-quality 3D face scans, thus limiting their practicality in real-world scenarios. Recently, with the development of affordable consumer-level depth cameras, the focus has shifted towards low-quality 3D face scans. In this paper, we propose a method for low-quality 3DFR. On one hand, our approach employs the normalizing flow to model an adaptive-form distribution for any given 3D face scan. This adaptive distributional representation learning strategy allows for more robust representations of low-quality 3D face scans (which may be caused by the scan noises, pose or occlusion variations, etc.). On the other hand, we introduce an adaptive sample weighting strategy to adjust the importance of each training sample by measuring both the difficulty of being recognized and the data quality. This adaptive sample weighting strategy can further enhance the robustness of the deep model and meanwhile improve its performance on low-quality 3DFR. Through comprehensive experiments, we demonstrate that our method can significantly improve the performance of low-quality 3DFR. For example, our method achieves competitive results on both the IIIT-D database and the Lock3DFace datasets, underscoring its effectiveness in addressing the challenges associated with low-quality 3D faces.}
}
@article{CORDEIRO2025111132,
title = {ANNE: Adaptive Nearest Neighbours and Eigenvector-based sample selection for robust learning with noisy labels},
journal = {Pattern Recognition},
volume = {159},
pages = {111132},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111132},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008835},
author = {Filipe R. Cordeiro and Gustavo Carneiro},
keywords = {Noisy label learning, Deep learning, Sample selection},
abstract = {An important stage of most state-of-the-art (SOTA) noisy-label learning methods consists of a sample selection procedure that classifies samples from the noisy-label training set into noisy-label or clean-label subsets. The process of sample selection typically consists of one of the two approaches: loss-based sampling, where high-loss samples are considered to have noisy labels, or feature-based sampling, where samples from the same class tend to cluster together in the feature space and noisy-label samples are identified as anomalies within those clusters. Empirically, loss-based sampling is robust to a wide range of noise rates, while feature-based sampling tends to work effectively in particular scenarios, e.g., the filtering of noisy instances via their eigenvectors (FINE) sampling exhibits greater robustness in scenarios with low noise rates, and the K nearest neighbour (KNN) sampling mitigates better high noise-rate problems. This paper introduces the Adaptive Nearest Neighbours and Eigenvector-based (ANNE) sample selection methodology, a novel approach that integrates loss-based sampling with the feature-based sampling methods FINE and Adaptive KNN to optimize performance across a wide range of noise rate scenarios. ANNE achieves this integration by first partitioning the training set into high-loss and low-loss sub-groups using loss-based sampling. Subsequently, within the low-loss subset, sample selection is performed using FINE, while the high-loss subset employs Adaptive KNN for effective sample selection. We integrate ANNE into the noisy-label learning state of the art (SOTA) method SSR+, and test it on CIFAR-10/-100 (with symmetric, asymmetric and instance-dependent noise), Webvision and ANIMAL-10, where our method shows better accuracy than the SOTA in most experiments, with a competitive training time. The code is available at https://github.com/filipe-research/anne.}
}
@article{MAO2025111143,
title = {FedKT: Federated learning with knowledge transfer for non-IID data},
journal = {Pattern Recognition},
volume = {159},
pages = {111143},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111143},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400894X},
author = {Wenjie Mao and Bin Yu and Chen Zhang and A.K. Qin and Yu Xie},
keywords = {Federated learning, Knowledge distillation, Knowledge transfer, Statistical heterogeneity, Non-IID data, Generalization, Personalization},
abstract = {Federated Learning enables clients to train a joint model collaboratively without disclosing raw data. However, learning over non-IID data may raise performance degeneration, which has become a fundamental bottleneck. Despite numerous efforts to address this issue, challenges such as excessive local computational burdens and reliance on shared data persist, rendering them impractical in real-world scenarios. In this paper, we propose a novel federated knowledge transfer framework to overcome data heterogeneity issues. Specifically, a model segmentation distillation method and a learnable aggregation network are developed for server-side knowledge ensemble and transfer, while a client-side consistency-constrained loss is devised to rectify local updates, thereby enhancing both global and client models. The framework considers both diversity and consistency among clients and can serve as a general solution for extracting knowledge from distributed nodes. Extensive experiments on four datasets demonstrate our framework’s effectiveness, achieving superior performance compared to advanced competitors in high-heterogeneity settings.}
}
@article{LIU2025111120,
title = {TransMatch: Transformer-based correspondence pruning via local and global consensus},
journal = {Pattern Recognition},
volume = {159},
pages = {111120},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111120},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008719},
author = {Yizhang Liu and Yanping Li and Shengjie Zhao},
keywords = {Correspondence pruning, Transformer, Local and global consensus, Camera pose estimation},
abstract = {Correspondence pruning aims to filter out false correspondences (a.k.a. outliers) from the initial feature correspondence set, which is pivotal to matching-based vision tasks, such as image registration. To solve this problem, most existing learning-based methods typically use a multilayer perceptron framework and several well-designed modules to capture local and global contexts. However, few studies have explored how local and global consensuses interact to form cohesive feature representations. This paper proposes a novel framework called TransMatch, which leverages the full power of Transformer structure to extract richer features and facilitate progressive local and global consensus learning. In addition to enhancing feature learning, Transformer is used as a powerful tool to connect the above two consensuses. Benefiting from Transformer, our TransMatch is surprisingly effective for differentiating correspondences. Experimental results on correspondence pruning and camera pose estimation demonstrate that the proposed TransMatch outperforms other state-of-the-art methods by a large margin. The code will be available at https://github.com/lyz8023lyp/TransMatch/.}
}
@article{LI2025111088,
title = {Multi-view visual semantic embedding for cross-modal image–text retrieval},
journal = {Pattern Recognition},
volume = {159},
pages = {111088},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111088},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008392},
author = {Zheng Li and Caili Guo and Xin Wang and Hao Zhang and Lin Hu},
keywords = {Image–text retrieval, Cross-modal retrieval, Visual semantic embedding, Multi-view learning},
abstract = {Visual Semantic Embedding (VSE) is a dominant method for cross-modal image–text retrieval. The purpose of VSE is to learn an embedding space where images can be embedded close to the corresponding captions. However, there are large intra-class variations in image–text data. Multiple captions describing the same image may be described from different views, and descriptions of different views are often dissimilar. The VSE method embeds samples from the same class in similar positions, which suppresses intra-class variations and leads to inferior generalization. This paper proposes a Multi-View Visual Semantic Embedding (MV-VSE) framework that learns multiple embeddings for an image, explicitly modeling intra-class variation. To optimize the MV-VSE framework, a multi-view triplet loss is proposed, which jointly optimizes multi-view embeddings while retaining intra-class variation. Recently, large-scale Vision-Language Pre-training (VLP) has become a new paradigm for cross-modal image–text retrieval. To allow our framework to be flexibly applied to the traditional VSE models and VSE-based VLP models, we incorporate the contrastive loss commonly used in VLP and the triplet loss into a unified loss, and further propose a multi-view unified loss. Our framework can be applied plug-and-play to traditional VSE models and VSE-based VLP models without excessively increasing model complexity. Experimental results on the image–text retrieval benchmark datasets demonstrate that applying our framework can boost the retrieval performance of current VSE models. The code is available at https://github.com/AAA-Zheng/MV-VSE.}
}
@article{LIANG2025111185,
title = {A masking, linkage and guidance framework for online class incremental learning},
journal = {Pattern Recognition},
volume = {160},
pages = {111185},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111185},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009361},
author = {Guoqiang Liang and Zhaojie Chen and Shibin Su and Shizhou Zhang and Yanning Zhang},
keywords = {Class incremental learning, Logit mask, Feature distillation},
abstract = {Due to the powerful ability to acquire new knowledge and preserve previously learned concepts from a dynamic data stream, continual learning has recently garnered substantial interest. Since training data can only be used once, online class incremental learning (OCIL) is more practical and difficult. Although replay-based OCIL methods have made great progress, there is still a severe class imbalance problem. Specifically, limited by the small memory size, the number of samples for new classes is much larger than that for old classes, which finally leads to task recency bias and abrupt feature drift. To alleviate this problem, we propose a masking, linkage, and guidance framework (MLG) for OCIL, which consists of three effective modules, i.e. batch-level logit mask (BLM, masking), batch-level feature cross fusion (BFCF, linkage) and accumulative mean feature distillation (AMFD, guidance). The former two focus on class imbalance problem while the last aims to alleviate abrupt feature drift. In BLM, we only activate the logits of classes occurring in a batch, which makes the model learn knowledge within each batch. The BFCF module employs a transformer encoder layer to fuse the sample features within a batch, which rebalances the gradients of classifier’s weights and implicitly learns the sample relationship. Instead of a strict regularization in traditional feature distillation, the proposed AMFD guides previously learned features to move on purpose, which can reduce abrupt feature drift and produce a clearer boundary in feature space. Extensive experiments on four popular datasets for OCIL have shown the effectiveness of proposed MLG framework.}
}
@article{CARRASCO2025111157,
title = {Embedded feature selection for robust probability learning machines},
journal = {Pattern Recognition},
volume = {159},
pages = {111157},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111157},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009087},
author = {Miguel Carrasco and Benjamin Ivorra and Julio López and Angel M. Ramos},
keywords = {Cobb–Douglas, Minimax Probability Machine, Minimum Error Minimax Probability Machine, Second-order cone programming, Support vector machines, Feature selection},
abstract = {Methods:
Feature selection is essential for building effective machine learning models in binary classification. Eliminating unnecessary features can reduce the risk of overfitting and improve classification performance. Moreover, the data we handle typically contains a stochastic component, making it important to develop robust models that are insensitive to data perturbations. Although there are numerous methods and tools for feature selection, relatively few studies address embedded feature selection within robust classification models using penalization techniques.
Objective:
In this work, we introduce robust classifiers with integrated feature selection capabilities, utilizing probability machines based on different penalization techniques, such as the ℓ1-norm or the elastic-net, combined with a novel Direct Feature Elimination process to improve model resilience and efficiency.
Findings:
Numerical experiments on standard datasets demonstrate the effectiveness and robustness of the proposed models in classification tasks even when using a reduced number of features. These experiments were evaluated using original performance indicators, highlighting the models’ ability to maintain high performance with fewer features.
Novelty:
The study discusses the trade-offs involved in combining different penalties to select the most relevant features while minimizing empirical risk. In particular, the integration of elastic-net and ℓ1-norm penalties within a unified framework, combined with the original Direct Feature Elimination approach, presents a novel method for improving both model accuracy and robustness.}
}
@article{HUANG2025111104,
title = {Spatio-temporal interactive reasoning model for multi-group activity recognition},
journal = {Pattern Recognition},
volume = {159},
pages = {111104},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111104},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008550},
author = {Jianglan Huang and Lindong Li and Linbo Qing and Wang Tang and Pingyu Wang and Li Guo and Yonghong Peng},
keywords = {Action recognition, Multi-group activity recognition, Group clustering, Interaction reasoning, Spatio-temporal trajectory},
abstract = {Multi-group activity recognition aims to recognize sub-group activities in multi-person scenes. Existing works explore group-level features by simply using graph neural networks for reasoning about the individual interactions and directly aggregating individual features, which cannot fully mine the interactions between people and between sub-groups, resulting in the loss of useful information for group activity recognition. To address this problem, this paper proposes a Spatio-Temporal Interactive Reasoning Model (STIRM) to better exploit potential spatio-temporal interactions for multi-group activity recognition. In particular, we present an interactive feature extraction strategy to explore correlation features between individuals by analyzing the features of their nearest neighbor. We design a new clustering module that combines the action similarity feature and spatio-temporal trajectory feature to divide people into small groups. In addition, to obtain rich and accurate group-level features, a group interaction reasoning module is constructed to explore the interactions between different small groups and among people in the same group and exclude people who have less impact on group activities according to their importance. Extensive experiments on the Social-CAD, PLPS and JRDB-PAR datasets indicate the superiority of the proposed method over state-of-the-art methods.}
}
@article{TAO2025111128,
title = {FocTrack: Focus attention for visual tracking},
journal = {Pattern Recognition},
volume = {160},
pages = {111128},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111128},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008793},
author = {Jian Tao and Sixian Chan and Zhenchao Shi and Cong Bai and Shengyong Chen},
keywords = {Visual object tracking, Focus attention, Clustering function, Update strategy},
abstract = {Transformer trackers have achieved widespread success based on their attention mechanism. The vanilla attention mechanism focuses on modeling the long-range dependencies between tokens to gain a global perspective. However, in human tracking behavior, the line of sight first skims apparent regions and then focuses on the differences between similar regions. To explore this issue, we build a powerful online tacker with focus attention, named FocTrack. Firstly, we design a focus attention module, which adopts the iterative binary clustering function (IBCF) before self-attention to simulate human behavior. Specifically, for a given cluster, other clusters are treated as apparent tokens that are skimmed during the clustering process, while the subsequent self-attention performs focused discriminative learning on the target cluster. Moreover, we propose a local template update strategy (LTUS) to probe into the effective temporal information for visual object tracking. In the testing, LTUS only replaces outdated local templates to ensure overall reliability and holds a low computational burden. Finally, extensive experiments show that our proposed FocTrack achieves state-of-the-art performance in several benchmarks.In particular, FocTrack achieves 71.5% AUC on the LaSOT, 84.7% AUC on the TrackingNet, and a running speed of around 36 FPS, outperforming the popular approaches.}
}
@article{MOK2025111149,
title = {Corruption-based anomaly detection and interpretation in tabular data},
journal = {Pattern Recognition},
volume = {159},
pages = {111149},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111149},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009002},
author = {Chunghyup Mok and Seoung Bum Kim},
keywords = {Anomaly detection, Tabular data, Explainable artificial intelligence, Variable corruption, Self-supervised learning},
abstract = {Recent advances in self-supervised learning (SSL) have proven crucial in effectively learning representations of unstructured data, encompassing text, images, and audio. Although the applications of these advances in anomaly detection have been explored extensively, applying SSL to tabular data presents challenges because of the absence of prior information on data structure. In response, we propose a framework for anomaly detection in tabular datasets using variable corruption. Through selective variable corruption and assignment of new labels based on the degree of corruption, our framework can effectively distinguish between normal and abnormal data. Furthermore, analyzing the impact of corruption on anomaly scores aids in the identification of important variables. Experimental results obtained from various tabular datasets validate the precision and applicability of the proposed method. The source code can be accessed at https://github.com/mokch/CAIT.}
}
@article{DING2025111091,
title = {SeaTrack: Rethinking Observation-Centric SORT for Robust Nearshore Multiple Object Tracking},
journal = {Pattern Recognition},
volume = {159},
pages = {111091},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111091},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008422},
author = {Jiangang Ding and Wei Li and Ming Yang and Yuanlin Zhao and Lili Pei and Aojia Tian},
keywords = {Nearshore multiple object tracking, Guiding modulation, Multi-scale corner momentum, Long and short-term fusion},
abstract = {Nearshore Multiple Object Tracking (NMOT) aims to locate and associate nearshore objects. Current approaches utilize Automatic Identification Systems (AIS) and radar to accomplish this task. However, video signals can describe the visual appearance of nearshore objects without prior information such as identity, location, or motion. In addition, sea clutter will not affect the capture of living objects by visual sensors. Recognizing this, we analyzed three key long-term challenges of the vision-based NMOT and proposed a tracking pipeline that relies solely on motion information. Maritime objects are highly susceptible to being obscured or submerged by waves, resulting in fragmented tracklets. We first introduced guiding modulation to address the long-term occlusion and interaction of maritime objects. Subsequently, we modeled confidence, altitude, and angular momentum to mitigate the effects of motion blur, ringing, and overshoot artifacts to observations in unstable imaging environments. Additionally, we designed a motion fusion mechanism that combines long-term macro tracklets with short-term fine-grained tracklets. This correction mechanism helps reduce the estimation variance of the Kalman Filter (KF) to alleviate the substantial nonlinear motion of maritime objects. We call this pipeline SeaTrack, which remains simple, online, and real-time, demonstrating excellent performance and scalability in benchmark evaluations.}
}
@article{CHENG2025111115,
title = {GBMOD: A granular-ball mean-shift outlier detector},
journal = {Pattern Recognition},
volume = {159},
pages = {111115},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111115},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008665},
author = {Shitong Cheng and Xinyu Su and Baiyang Chen and Hongmei Chen and Dezhong Peng and Zhong Yuan},
keywords = {Outlier detection, Anomaly detection, Granular-ball computing, Mean-shift, k-nearest neighbor},
abstract = {Outlier detection is a crucial data mining task involving identifying abnormal objects, errors, or emerging trends. Mean-shift-based outlier detection techniques evaluate the abnormality of an object by calculating the mean distance between the object and its k-nearest neighbors. However, in datasets with significant noise, the presence of noise in the k-nearest neighbors of some objects makes the model ineffective in detecting outliers. Additionally, the mean-shift outlier detection technique depends on finding the k-nearest neighbors of an object, which can be time-consuming. To address these issues, we propose a granular-ball computing-based mean-shift outlier detection method (GBMOD). Specifically, we first generate high-quality granular-balls to cover the data. By using the centers of granular-balls as anchors, the subsequent mean-shift process can effectively avoid the influence of noise points in the neighborhood. Then, outliers are detected based on the distance from the object to the displaced center of the granular-ball to which it belongs. Finally, the distance between the object and the shifted center of the granular-ball to which the object belongs is calculated, resulting in the outlier scores of objects. Subsequent experiments demonstrate the effectiveness, efficiency, and robustness of the method proposed in this paper.}
}
@article{ZHANG2025111160,
title = {Topology reorganized graph contrastive learning with mitigating semantic drift},
journal = {Pattern Recognition},
volume = {159},
pages = {111160},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111160},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009117},
author = {Jiaqiang Zhang and Songcan Chen},
keywords = {Graph neural network, Self-supervised learning, Contrastive learning, Node representation learning},
abstract = {Graph contrastive learning (GCL) is an effective paradigm for node representation learning in graphs. The key components hidden behind GCL are data augmentation and positive–negative pair selection. Typical data augmentations in GCL, such as uniform deletion of edges, are generally blind and resort to local perturbation, which is prone to producing under-diversity views. Additionally, there is a risk of making the augmented data traverse to other classes. Moreover, most methods always treat all other samples as negatives. Such a negative pairing naturally results in sampling bias and likewise may make the learned representation suffer from semantic drift. Therefore, to increase the diversity of the contrastive view, we propose two simple and effective global topological augmentations to compensate current GCL. One is to mine the semantic correlation between nodes in the feature space. The other is to utilize the algebraic properties of the adjacency matrix to characterize the topology by eigen-decomposition. With the help of both, we can retain important edges to build a better view. To reduce the risk of semantic drift, a prototype-based negative pair selection is further designed which can filter false negative samples. Extensive experiments on various tasks demonstrate the advantages of the model compared to the state-of-the-art methods.}
}
@article{HUANG2025111126,
title = {Image shadow removal via multi-scale deep Retinex decomposition},
journal = {Pattern Recognition},
volume = {159},
pages = {111126},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111126},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400877X},
author = {Yan Huang and Xinchang Lu and Yuhui Quan and Yong Xu and Hui Ji},
keywords = {Shadow removal, Retinex decomposition, Deep learning},
abstract = {In recent years, deep learning has emerged as an important tool for image shadow removal. However, existing methods often prioritize shadow detection and, in doing so, they oversimplify the lighting conditions of shadow regions. Furthermore, these methods neglect cues from the overall image lighting when re-lighting shadow areas, thereby failing to ensure global lighting consistency. To address these challenges in images captured under complex lighting conditions, this paper introduces a multi-scale network built on a Retinex decomposition model. The proposed approach effectively senses shadows with uneven lighting and re-light them, achieving greater consistency along shadow boundaries. Furthermore, for the design of network, we introduce several techniques for boosting shadow removal performance, including a shadow-aware channel attention module, local discriminative and Retinex decomposition loss functions, and a multi-scale mechanism for guiding Retinex decomposition by concurrently capturing both fine-grained details and large-scale contextual information. Experimental results demonstrate the superiority of our proposed method over existing solutions, particularly for images taken under complex lighting conditions.}
}
@article{CHEN2025111131,
title = {Consistency-driven feature scoring and regularization network for visible–infrared person re-identification},
journal = {Pattern Recognition},
volume = {159},
pages = {111131},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111131},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008823},
author = {Xueting Chen and Yan Yan and Jing-Hao Xue and Nannan Wang and Hanzi Wang},
keywords = {Visible–infrared person re-identification, Local feature learning, Feature weighting, Feature regularization},
abstract = {Recently, visible–infrared person re-identification (VI-ReID) has received considerable attention due to its practical importance. A number of methods extract multiple local features to enrich the diversity of feature representations. However, some local features often involve modality-relevant information, leading to deteriorated performance. Moreover, existing methods optimize the models by only considering the samples at each batch while ignoring the learned features at previous iterations. As a result, the features of the same person images drastically change at different training epochs, hindering the training stability. To alleviate the above issues, we propose a novel consistency-driven feature scoring and regularization network (CFSR-Net), which consists of a backbone network, a local feature learning block, a feature scoring block, and a global–local feature fusion block, for VI-ReID. On the one hand, we design a cross-modality consistency loss to highlight modality-irrelevant local features and suppress modality-relevant local features for each modality, facilitating the generation of a reliable compact local feature. On the other hand, we develop a feature consistency regularization strategy (including a momentum class contrastive loss and a momentum distillation loss) to impose consistency regularization on the learning of different levels of features by considering the learned features at historical epochs. This effectively enables smooth feature changes and thus improves the training stability. Extensive experiments on public VI-ReID datasets clearly show the effectiveness of our method against several state-of-the-art VI-ReID methods. Code will be released at https://github.com/cxtjl/CFSR-Net.}
}
@article{QIAN2025111103,
title = {Apply prior feature integration to sparse object detectors},
journal = {Pattern Recognition},
volume = {159},
pages = {111103},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111103},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008549},
author = {Yu Qian and Qijin Wang and Changxin Wu and Chao Wang and Long Cheng and Yating Hu and Hongqiang Wang},
keywords = {Object detection, Deep learning, Feature extraction, Sparse object candidates},
abstract = {Noisy boxes as queries for sparse object detection has become a hot topic of research in recent years. Sparse R-CNN achieves one-to-one prediction from noisy boxes to object boxes, while DiffusionDet transforms the prediction process of Sparse R-CNN into multiple diffusion processes. Especially, algorithms such as Sparse R-CNN and its improved versions all rely on FPN to extract features for ROI Aligning. But the target only matching one feature map in FPN, which is inefficient and resource-consuming. otherwise, these methods like sparse object detection crop regions from noisy boxes for prediction, resulting in boxes failing to capture global features. In this work, we rethink the detection paradigm of sparse object detection and propose two improvements and produce a new object detector, called Prior Sparse R-CNN. Firstly, we replace the original FPN neck with a neck that only outputs one feature map to improve efficiency. Then, we design aggregated encoder after neck to solve the object scale problem through dilated residual blocks and feature aggregation. Another improvement is that we introduce prior knowledge for noisy boxes to enhance their understanding of global representations. Region Generation network (RGN) is designed by us to generate global object information and fuse it with the features of noisy boxes as prior knowledge. Prior Sparse R-CNN reaches the state-of-the-art 47.0 AP on COCO 2017 validation set, surpassing DiffusionDet by 1.5 AP with ResNet-50 backbone. Additionally, our training epoch requires only 3/5 of the time.}
}
@article{YUAN2025111119,
title = {A newton interpolation network for smoke semantic segmentation},
journal = {Pattern Recognition},
volume = {159},
pages = {111119},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111119},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008707},
author = {Feiniu Yuan and Guiqian Wang and Qinghua Huang and Xuelong Li},
keywords = {Smoke segmentation, Newton interpolation, Newton interpolation module, Structured information, Deep neural network},
abstract = {Smoke has large variances of visual appearances that are very adverse to visual segmentation. Furthermore, its semi-transparency often produces highly complicated mixtures of smoke and backgrounds. These factors lead to great difficulties in labelling and segmenting smoke regions. To improve accuracy of smoke segmentation, we propose a Newton Interpolation Network (NINet) for visual smoke semantic segmentation. Unlike simply concatenating or point-wisely adding multi-scale encoded feature maps for information fusion or re-usage, we design a Newton Interpolation Module (NIM) to extract structured information by analyzing the feature values in the same position but from encoded feature maps with different scales. Interpolated features by our NIM contain long-range dependency and semantic structures across different levels, but traditional fusion of multi-scale feature maps cannot model intrinsic structures embedded in these maps. To obtain multi-scale structured information, we repeatedly use the proposed NIM at different levels of the decoding stages. In addition, we use more encoded feature maps to construct a higher order Newton interpolation polynomial for extracting higher order information. Extensive experiments validate that our method significantly outperforms existing state-of-the-art algorithms on virtual and real smoke datasets, and ablation experiments also validate the effectiveness of our NIMs.}
}
@article{ZHANG2025111087,
title = {One point is all you need for weakly supervised object detection},
journal = {Pattern Recognition},
volume = {159},
pages = {111087},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111087},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008380},
author = {Shiwei Zhang and Zhengzheng Wang and Wei Ke},
keywords = {Object detection, Weak annotation, Similarity-based learning},
abstract = {Object detection with weak annotations has attracted much attention recently. Weakly supervised object detection(WSOD) methods which only use image-level labels to train a detector encounter some severe problems that it cannot cover the whole object and the region proposal methods waste a large amount of time. Meanwhile, point supervised object detection(PSOD) leverages point annotations that remarkably improves the performance. However, point annotation is still complex and increase the costs of annotation. To overcome these issues, we propose a novel method which only requires one point per category for a training image. Compared to point annotation, our method significantly reduces the annotation cost as the number of point annotations is largely reduced. We design a framework to train a detector with one point per category annotation. Firstly, a pseudo box generation module is introduced to generate the corresponding pseudo boxes of the annotated points. Then, inspired by the observation that the features of objects with the same class in an image are very similar, a dense instances mining module is proposed to make use of the similarity between the features of objects with the same class to discover unlabeled instances and generate pseudo category heatmaps. Finally, the pseudo boxes and pseudo category heatmaps are leveraged to train a detector. Experiments conducted on popular open-source datasets verify the effectiveness of our annotation method and framework. Our proposed method outperforms previous WSOD methods and achieves comparable performance with some PSOD methods in a more efficient way.}
}
@article{WU2025111156,
title = {Gradient-based sparse voxel attacks on point cloud object detection},
journal = {Pattern Recognition},
volume = {160},
pages = {111156},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111156},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009075},
author = {Junqi Wu and Wen Yao and Shuai Jia and Tingsong Jiang and Weien Zhou and Chao Ma and Xiaoqian Chen},
keywords = {Point cloud detection, Adversarial examples, Deep learning},
abstract = {Point cloud object detection is crucial for a variety of applications, including autonomous driving and robotics. Voxel-based representation for 3D point clouds has drawn significant attention due to their efficiency and effectiveness. Recent studies have revealed the vulnerability of deep learning models to adversarial attacks, while considerably less attention is paid to the robustness of voxel-based point cloud object detectors. Existing adversarial attacks on the point cloud data involve generating fake obstacles, removing objects or producing fake predictions. Despite the demonstrated success, these approaches have three limitations. First, manipulating point data, which was originally designed for point-based representation, is inapplicable to voxel-based representation. Second, existing works that modified points in the hold scene yield redundant perturbations. Third, the evaluation primarily performed on small-scale datasets, such as KITTI, does not scale well. To address these limitations, we propose a gradient-based sparse voxel attack (GSVA) algorithm for voxel-based 3D point cloud object detectors. Two novel frameworks, i.e., re-voxelization-based voxel attack framework and light voxel attack framework, successfully modify voxel-based representation instead of raw points. In addition to KITTI, extensive experiments on large-scale datasets including nuScenes and Waymo Open Dataset demonstrate the favorable attack performance (with mAP decrease by 86.2%∼99.5%) and the slight perturbation costs (with lowest modification rate of 3.5%) of our voxel attack method over the state-of-the-art approaches.}
}
@article{CHEN2025111127,
title = {HDR reconstruction from a single exposure LDR using texture and structure dual-stream generation},
journal = {Pattern Recognition},
volume = {159},
pages = {111127},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111127},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324008781},
author = {Yu-Hsiang Chen and Shanq-Jang Ruan},
keywords = {Single-image HDR reconstruction, High dynamic range imaging, Inverse tone mapping, Deep learning, Single exposure image},
abstract = {Reconstructing high dynamic range (HDR) imagery from a single low dynamic range (LDR) photograph presents substantial challenges. The challenges are primarily due to the loss of details and information in regions of underexposure or overexposure due to quantization and saturation inherent to camera sensors. Traditional learning-based approaches often struggle with distinguishing overexposed regions within an object from the background, leading to compromised detail retention in these critical areas. Our methodology focuses on meticulously reconstructing structural and textural details to preserve the integrity of the structural information. We propose a new two-stage model architecture for HDR image reconstruction, including a dual-stream network and a feature fusion stage. The dual-stream network is designed to reconstruct structural and textural details, while the feature fusion stage aims to minimize artifacts by utilizing the reconstructed information. We have demonstrated that our proposed method performs better than other state-of-the-art single-image HDR reconstruction algorithms in various quality metrics.}
}
@article{KOU2025111172,
title = {Progressive label enhancement},
journal = {Pattern Recognition},
volume = {160},
pages = {111172},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111172},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009233},
author = {Zhiqiang Kou and Jing Wang and Yuheng Jia and Xin Geng},
keywords = {Label enhancement, Label distribution learning, Multi-label learning, Label ambiguity, Dimension reduction, Manifold},
abstract = {Label Distribution Learning (LDL) leverages label distribution (LD) to represent instances, which helps solve label ambiguity. However, obtaining LD can be extremely challenging in many real-world scenarios. Label Enhancement (LE) has emerged as a solution to enhance logical labels to LD since logical labels are highly available. In this paper, we explore the application of dimension reduction techniques to enhance LE. We present a learning framework known as Progressive Label Enhancement (PLE). PLE progressively conducts dependency-maximization-oriented dimension reduction and LE. First, PLE generates LD by leveraging the manifold structure within the feature space induced by dependency-maximization-driven dimension reduction. Second, PLE optimizes the projection matrix for dependency maximization based on the obtained LD. Finally, extensive experiments conducted on 15 real-world datasets consistently demonstrate that PLE outperforms the other six comparative approaches.}
}