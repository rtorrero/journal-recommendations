@article{JI2019102673,
title = {Saliency detection using Multi-layer graph ranking and combined neural networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102673},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102673},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302949},
author = {Chao Ji and Xinbo Huang and Wen Cao and Yongcan Zhu and Ye Zhang},
keywords = {Machine vision, Saliency detection, Fast R-CNN, Region Net, Local-Global Net},
abstract = {In this paper, a new algorithm based on a combined neural network is proposed to improve salient object detection in the complex images. It consists of two main steps. The first step, an objective function which is optimized on a multi-layer graph structure is constructed to diffuse saliency from borders to salient objects, aiming to roughly estimate the location and extent salient objects of an image, meanwhile, color attribute is adopted to rapidly find a set of object-related regions in the image. The second step, establish a combined neural network with Region Net and Local-Global Net. Region Net is adopted to efficiently generate the salient map with the sharp object boundary. Then Local-Global Net based on multi-scale spatial context is proposed to provide strongly reliable multi-scale contextual information, and thus achieves an optimized performance. Experimental results and comparison analysis demonstrate that the proposed algorithm is more effective and superior than most low-level oriented prior methods in terms of precision recall curves, F-measure and mean absolute errors.}
}
@article{GUO2020102730,
title = {Credit risk assessment of P2P lending platform towards big data based on BP neural network},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102730},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102730},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303517},
author = {Yiping Guo},
keywords = {Peer to peer, Credit risk assessment, Logistic regression, BP neural network, Big data},
abstract = {Peer-to-peer (P2P) lending platform plays a significant role in modern financial systems. However, due to improper supervision, credit risk is inevitable. In this paper, we analyze the traditional financial risk and information technology risk of P2P lending platform. In order to evaluate the performance of assessment algorithms, we present a BP neural network-based algorithm for lending risk assessment. To achieve our task, we crawled large-scale lending data for 2015–2019. Logistic regression is used to compare with BP neural network method. Experimental results show that BP neural network-based algorithm outperforms traditional Logistic regression algorithm and the proposed method can effectively reduce investor risk.}
}
@article{SONG2019102633,
title = {Ultrasonic image processing based on fusion super-resolution reconstruction of familiar models},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102633},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102633},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302548},
author = {Wuli Song and Linna Li and Zihui Ren},
keywords = {Ultrasonic image, Super-resolution, Coefficient representation, Multi feature fusion},
abstract = {Ultrasound image technology is to measure the energy and time of arrival of the reflected echo after the pulse acoustic signal is sent out by the ultrasonic wave. Usually, the distance between the ultrasonic source and the reflector is measured. Super-resolution image reconstruction aims to recover high-resolution images from one or more low-resolution images. Super-resolution image reconstruction is a software approach to solve the problem of low-resolution images by overcoming the limitations of hardware. In this paper, an image super-resolution reconstruction method based on sparse representation model and multi-feature fusion is proposed. Sparse dictionary is used to learn and reconstruct the luminance details of ultrasonic images, and edge interpolation is used to improve the edge clarity. Experimental results show that the proposed method is superior to Bicubic interpolation, SCSR and JOR in PSNR, and is 0.35 dB higher than RAISR. On the FSIM index, this method is also slightly better than other comparison methods, which can get better reconstruction results. Visual effects and numerical evaluation results of reconstructed images are better than several comparison methods.}
}
@article{JIN2019102661,
title = {An image denoising approach based on adaptive nonlocal total variation},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102661},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102661},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302822},
author = {Yan Jin and Xiaoben Jiang and Wenyu Jiang},
keywords = {Adaptive regularization parameter, Image denoising, NLTV model, Split-Bregman method},
abstract = {In the nonlocal total variation (NLTV) model the constant regularization parameter λ cannot adaptively control the balance between the regularization term and the fidelity term, which may results in over-smoothing and the more losing image details in non-flat areas when λ is small, or insufficient noise removal in flat areas when λ is large. It is better that λ has different values according to the characteristics of image areas. In this paper, we introduce an adaptive regularization parameter λ(x) which can recognize flat areas and non-flat areas of an image and propose an improved NLTV model by replacing regularization parameter λ in NLTV model with the function λ(x). In addition, we calculate the similarity weight function of our model from the pre-filtered image to reduce the influence of noise on it. Experimental results demonstrate our approach outperforms some existing methods in terms of objective criteria and subjective visual perception.}
}
@article{ZHOU2019102649,
title = {Analyzing multiple types of behaviors from traffic videos via nonparametric topic model},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102649},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102649},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302706},
author = {Houkui Zhou and Huimin Yu and Roland Hu and Guangqun Zhang and Junguo Hu and Tao He},
keywords = {Nonparametric topic model, Motion pattern, Traffic pattern, Abnormality detection, Beta negative binomial process, Possion factor analysis},
abstract = {Intelligent video surveillance systems have garnered substantial research attention in recent years within the transportation surveillance field. The systems can assist in identifying activities, interactions, and abnormal behaviors of individuals in traffic. We propose a novel unsupervised learning framework based on a two-layer BNBP-PFA topic model to simultaneously model multiple types of behaviors in crowded and complicated traffic videos. We provide the model’s structure, its inference algorithm, and design a corresponding likelihood function based on an abnormality detection algorithm. Compared to similar existing algorithms, ours readily reveals both the local topic-motion pattern and the global topic-traffic pattern. Comparative experiments on two public traffic video datasets show that our model outperforms the state-of-art algorithms in regards to effective topic discovery and abnormality detection.}
}
@article{TANG2020102727,
title = {Salient object detection via two-stage absorbing Markov chain based on background and foreground},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102727},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102727},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303487},
author = {Wei Tang and Zhijian Wang and Jiyou Zhai and Zhangjing Yang},
keywords = {Saliency object detection, Markov chain, Background absorbing, Foreground absorbing},
abstract = {This paper proposes a saliency detection method via two-stage absorbing Markov chain based on background and foreground for detecting salient objects in images. Firstly, image preprocessing is performed, followed by convex hull construction and superpixel segmentation, to prepare for subsequent processing. Secondly, according to the boundary connectivity, the superpixels with lower background probability value in the candidate boundary background set B0 are deleted, and the boundary background set B1 is obtained. With the saliency values of the nodes in the boundary-prior saliency map Sbg1, the background seeds are added appropriately in the region outside the candidate boundary background set B0 and the convex hull H, and the background seed set B is obtained after update. Then, the background-absorbing Markov chain is constructed to generate background-absorbing saliency map Sbg2. By fusing the saliency maps Sbg1 and Sbg2, the first-stage background-based saliency map Sbg is obtained. Thirdly, in the range of the convex hull H, the foreground seed set F is determined according to the saliency map Sbg. Then, the foreground-absorbing Markov chain is constructed, to obtain the second-stage foreground-absorbing saliency map Sfg. Finally, the saliency maps Sbg and Sfg of the two stages are combined to obtain a fused saliency map S, and the final saliency map S∗ is obtained after optimization through smoothing mechanism. Compared with the traditional methods, the performance of the proposed method is significantly improved. The proposed method is tested on three public image datasets, and it shows great accuracy in detecting salient objects.}
}
@article{ZHANG2020102719,
title = {High-quality face image generation based on generative adversarial networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102719},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102719},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303402},
author = {Zhixin Zhang and Xuhua Pan and Shuhao Jiang and Peijun Zhao},
keywords = {Face image generation, GAN, High-quality images},
abstract = {Conventional face image generation using generative adversarial networks (GAN) is limited by the quality of generated images since generator and discriminator use the same backpropagation network. In this paper, we discuss algorithms that can improve the quality of generated images, that is, high-quality face image generation. In order to achieve stability of network, we replace MLP with convolutional neural network (CNN) and remove pooling layers. We conduct comprehensive experiments on LFW, CelebA datasets and experimental results show the effectiveness of our proposed method.}
}
@article{MA2019102646,
title = {Study on short-term network forecasting based on SVM-MFA algorithm},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102646},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102646},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302676},
author = {Wen Ma and Xinyang Zhang and Yong Xin and Shenzhang Li},
keywords = {Support vector machine, Improved firefly algorithm, Load forecasting, Nonlinear regression, Evaluation criteria},
abstract = {Accurate prediction of power supply load is vital in power industry, which provides economic operation decision for the power operation department. For the unpredictability and periodicity of power load, nonlinear intelligent forecasting method is adopted. A modified firefly algorithm (MFA) combined with support vector machine (SVM) is proposed to predict the load of power supply data in this paper. The nonlinear mapping function is used to deal with the nonlinear regression problem in SVM, in which the parameters affect the accuracy of load prediction, so the MFA method is adopted to optimized the parameters of SVM. In order to verify the accuracy of SVM-MFA, mean absolute percentage error (eMAPE) was used as the fitness function for simulation and comparison experiment. The results show that the SVM-MFA proposed in this paper has stronger global search ability and faster convergence rate than the traditional artificial neural network, and it is verified that the method proposed in this paper has higher accuracy and higher stability of network load prediction.}
}
@article{PENG2019102674,
title = {The affective facial recognition task: The influence of cognitive styles and exposure times},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102674},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102674},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302950},
author = {Shuna Peng and Yang Dong and Weisha Wang and Jieyi Hu and Weiyang Dong},
keywords = {Face recognition, Pattern recognition, Cognitive style, Face emotion},
abstract = {The main task of emotional facial recognition is to understand human emotion expression through the recognition of facial expressions, so as to achieve more effective communication and interpersonal communication. Therefore, facial recognition plays an important role in people's daily lives. In addition, the research of facial recognition is also helpful to understand the human perception processing mode, and promote the development of pattern recognition, cognitive science, neural network and other fields. With the development of cognitive science, facial recognition technology has been continuously improved, and emotional facial recognition tasks have received attention in the fields of pattern recognition and artificial intelligence, and have become a research hotspot. Among them, pattern recognition is a cognitive system applied to many fields. For the first time, we confirmed the effects of facial memory time, personal cognitive style, and emotions associated with the target face on facial recognition patterns. This study measured the impact of time, cognitive style, and emotional type of 62 qualified college students. The research results show that cognitive style and facial emotional content are of great significance for face pattern recognition. Specifically, students classified as “dependent” have achieved good results in face pattern recognition, and positive and negative strong emotional faces have left behind those who show neutral emotions. A deeper impression. Finally, an unusual phenomenon was discovered, which indicates that the shorter the time spent on the face of the memory, the higher the recognition score.}
}
@article{CORCORAN2019102617,
title = {A multi-scale topological shape model for single and multiple component shapes},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102617},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102617},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930238X},
author = {Padraig Corcoran and Joviša Žunić and Paul L. Rosin},
keywords = {Multiple component shapes, Topology, Multi-scale, Persistent homology},
abstract = {A novel shape model of multi-scale topological features is proposed which considers those features relating to connected components and holes. This is achieved by considering the persistent homology of a pair of sublevel set functions corresponding to a pair of distance functions defined on the ambient space. The model is applicable to both single and multiple component shapes and, to the authors knowledge, is the first shape model to consider multi-scale topological features of multiple component shapes. It is demonstrated, both qualitatively and quantitatively, that the proposed model models useful multi-scale topological features and outperforms a commonly used benchmark models with respect to the task of multiple component shape retrieval.}
}
@article{LIU2019102687,
title = {Large-scale and adaptive service composition based on deep reinforcement learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102687},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102687},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303086},
author = {Jiang-Wen Liu and Li-Qiang Hu and Zhao-Quan Cai and Li-Ning Xing and Xu Tan},
keywords = {Service composition, Deep reinforcement learning, QoS, Behavior strategy},
abstract = {Service composition is a research hotspot with practical value. With the development of Web service, many Web services with the same functional attributes emerge. However, service composition optimization is still a big challenge since the complex and unstable composition environment. To solve this problem, we propose an adaptive service composition based on deep reinforcement learning, where recurrent neural network (RNN) is utilized for predicting the objective function, improving its expression and generalization ability, and effectively solving the shortcomings of traditional reinforcement learning in the face of large-scale or continuous state space problems. We leverage heuristic behavior selection strategy to divide the state set into hidden state and fully visible state. Effective simulation of hidden state space and fully visible state of the evaluation function can further improve the accuracy and efficiency of the combined results. We conduct comprehensive experiment and experimental results have shown the effectiveness of our method.}
}
@article{MENG2019102656,
title = {Intelligent attack defense scheme based on DQL algorithm in mobile fog computing},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102656},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102656},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302779},
author = {Yuan Meng and Shanshan Tu and Jinliang Yu and Fengming Huang},
keywords = {Moving fog computing, Intelligent attack, Physical layer security, Prospect theory, Reinforcement learning},
abstract = {Fog computing is a technology that can expands the network computing mode of cloud computing and extends network computing from the network center to the network edge. It adds fog layer between cloud data center layer and Internet of Things (IoT) device layer, and provides data storage, processing, forwarding and other functions for devices using the network edge. In mobile fog computing (MFC) networks, fog nodes communicate with end users through wireless networks. Malicious users can choose different attack modes to attack legitimate users. There is a lack of research on the subjective choice of attack modes for malicious users in current work. To solve this problem, an intelligent attack defense scheme based on Double Q-learning (DQL) algorithm in MFC is proposed. Firstly, the security model involving malicious users in MFC is described. Based on Prospect Theory (PT), a static method of subjective zero-sum game between malicious users and legitimate users is constructed. Secondly, a dynamic subjective game scheme based on DQL algorithm is proposed to resist intelligent attacks. The simulation results show that compared with the Q-learning-based method for resisting intelligent attacks, the proposed method can enhance the security of MFC network and enhance the protection performance.}
}
@article{ZHANG2019102632,
title = {Quality assessment towards cell diffraction image based on multi-channel feature fusion},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102632},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102632},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302536},
author = {Xikun Zhang and Jie Hou},
keywords = {Image quality assessment, Cell diffraction image, Deep neural network},
abstract = {Image quality assessment towards cell diffraction image is significant for both the academic and medical domain. It plays an important role in medical detection and recognition, such as cell morphology and heterogeneity classification. However, cell diffraction image quality assessment is still a challenging task due to the high heterogeneity of cells and various appearance. To solve this problem, we propose a cell diffraction image quality assessment. More specifically, we first collect cell diffraction images including Jurkat and Ramos. To remove cell impurity and debris images, we leverage the K-means clustering algorithm and support vector machine (SVM) to eliminate these images. Subsequently, we calculate the Gray Level Co-occurrence Matrix (GLCM) of each image and extract deep representation by using DNN. Afterward, we fuse luminance, contrast, GLCM, and deep representation to calculate the feature similarity between the reference image and the test image. Extensive experiments conducted on Jurkat cells and Ramos cells datasets have shown the effectiveness of our proposed method.}
}
@article{SHI2020102740,
title = {Human-computer interaction based on face feature localization},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102740},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102740},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930361X},
author = {Yan Shi and Zijun Zhang and Kaining Huang and Wudi Ma and Shanshan Tu},
keywords = {Human interaction, Face point detection, Expression recognition, Sight tracking},
abstract = {Human-computer interaction is the way in which humans and machines communicate information. With the rapid development of deep learning technology, the technology of human-computer interaction has also made a corresponding breakthrough. In the past, the way human-computer interaction was mostly relied on hardware devices. Through the coordinated work of multiple sensors, people and machines can realize information interaction. However, as theoretical technology continues to mature, algorithms for human-computer interaction are also being enriched. The popularity of convolutional neural networks has made image processing problems easier to solve. Therefore, real-time human-computer interaction can be performed by using image processing, and intelligent of human-computer interaction can be realized. The main idea of this paper is to use the real-time capture of face images and video information to image the face image information. We perform feature point positioning based on the feature points of the face image. We perform expression recognition based on the feature points that are located. At the same time, we perform ray tracing for the identified human eye area. The feature points of the face and the corresponding expressions and implementation movements represent the user's use appeal. Therefore, we can analyze the user's use appeal by locating the face feature area. We define the corresponding action information for specific user face features. We extract the user's corresponding information according to the user's face features, and perform human-computer interaction according to the user's information.}
}
@article{MUNIR2019102660,
title = {An extensive review on spectral imaging in biometric systems: Challenges & advancements},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102660},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102660},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302810},
author = {Rumaisah Munir and Rizwan Ahmed Khan},
keywords = {Spectral imaging, Biometrics, Face recognition, Spoof attacks, Deep learning},
abstract = {Spectral imaging has recently gained traction for face recognition in biometric systems. We investigate the merits of spectral imaging for face recognition and the current challenges that hamper the widespread deployment of spectral sensors for face recognition. The reliability of conventional face recognition systems operating in the visible range is compromised by illumination changes, pose variations and spoof attacks. Recent works have reaped the benefits of spectral imaging to counter these limitations in surveillance activities (defence, airport security checks, etc.). However, the implementation of this technology for biometrics, is still in its infancy due to multiple reasons. We present an overview of the existing work in the domain of spectral imaging for face recognition, different types of modalities and their assessment, availability of public databases for sake of reproducible research as well as evaluation of algorithms, and recent advancements in the field, such as, the use of deep learning-based methods for recognizing faces from spectral images.}
}
@article{QAYYUM2019102672,
title = {Generation of personalized video summaries by detecting viewer’s emotion using electroencephalography},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102672},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102672},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302937},
author = {Huma Qayyum and Muhammad Majid and Ehatisham ul Haq and Syed Muhammad Anwar},
keywords = {Video summarization, Emotion recognition, Electroencephalography, Feature extraction, Classification},
abstract = {Video summaries produced by low level features are unaware of the viewer’s requirements and result in a semantic gap. Video content evokes certain emotions in a viewer, which can be measured and act as a strong source of information to generate summaries meeting viewer’s expectation. In this paper, we propose a personalized video summarization framework that classifies viewer’s emotion based on electroencephalography (EEG) signals, while watching a video to extract keyframes. Features are extracted from recorded EEG signals in time, frequency and wavelet domain to classify viewer’s emotions. Those frames are selected as keyframes from the video, where different emotions of viewer are evoked. Experiments are performed on 50 viewers and 50 video sequences to validate the effectiveness and efficiency of the proposed framework. It is evident from the results that the proposed method generates summaries with high precision, recall, F-measure, accuracy, and low error, hence reducing the semantic gap.}
}
@article{WANG2020102729,
title = {Analysis of financial business model towards big data and its applications},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102729},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102729},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303505},
author = {Yupeng Wang},
keywords = {Financial business, Big data, Neural network},
abstract = {Finance service based on big data faces many issues, such as fraud, credit. In this paper, we study the development of financial business model under the big data. We first analyze the impact mechanism of big data finance on customer information protection of commercial banks. Customer information has the characteristics of large amount of information, high value of data and strong destructive data leakage. Then, we propose two solutions towards issues of finance service including face anti-spoofing algorithm and financial risk evaluation. Experiments show the effectiveness of our proposed method in improving the reliability and security of modern big data finance.}
}
@article{LI2020102737,
title = {Learning multiple instance deep representation for objects tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102737},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102737},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930358X},
author = {Chunyu Li and Gang Li},
keywords = {Object tracking, Convolutional networks, Multiple Instance Learning},
abstract = {Object tracking has been widely used in various intelligent systems, such as pedestrian tracking, autonomous vehicles. To solve the problem that appearance changes and occlusion may lead to poor tracking performance, we propose a multiple instance learning (MIL) based method for object tracking. To achieve this task, we first manually label the first several frames of video stream in image level, which can indicate that whether a target object in the video stream. Then, we leverage a pre-trained convolutional neural network that has rich prior information to extract deep representation of target object. Since the location of the same object in adjacent frames is similar, we introduce a particle filter to predict the location of target object within a specific region. Comprehensive experiments have shown the effectiveness of our proposed method.}
}
@article{CAI2019102657,
title = {A topic sentiment based method for friend recommendation in online social networks via matrix factorization},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102657},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102657},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302780},
author = {Chongchao Cai and Huahu Xu},
keywords = {Sentiment analysis, Matrix factorization, Social network, Recommendation system},
abstract = {Data sparsity and prediction quality have been recognized as the crucial challenges in recommender system. With the expansion of social network data, social network analysis is becoming more and more important. Traditional Recommendation System assumes that users are independent and distributed equally, which ignores the social interaction or connection among users. In order to solve the prediction quality of friend recommendation in social networks, a user recommendation algorithm for social networks based on sentiment analysis and matrix factorization is proposed in this paper. This method is based on the traditional matrix factorization model. By integrating Sentiment (S), Important (I) and Objective (O) of user topic content in the social network, this paper proposes the approach base on sentiment analysis and matrix factorization to solve the poor prediction accuracy by employing social network. SIO model solves the problem that users in social networks can′t score the content of topics. User-topic matrix is constructed by SIO model. Combining the SIO model with matrix factorization, algorithm called SIO-TMF algorithm is proposed. Applying this method on social network, comparing with some traditional recommendation algorithms from four aspects: accuracy, diversity, novelty and coverage, the experimental results show that the proposed method improves the prediction quality of recommender system.}
}
@article{TAHIR2020102742,
title = {Low complexity high efficiency coding of light fields using ensemble classifiers},
journal = {Journal of Visual Communication and Image Representation},
volume = {66},
pages = {102742},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102742},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303633},
author = {Muhammad Tahir and Imtiaz A. Taj and Pedro A. Assuncao and Muhammad Asif},
keywords = {Light fields, HEVC, Fast encoding, Machine learning, Random forests},
abstract = {Light field images can be efficiently compressed using standard video codecs, such as the High Efficiency Video Coding (HEVC). However, the huge amount of data combined with the high computational complexity of HEVC, poses limitations on high-speed light field capturing and storage. This paper presents a contribution for low complexity encoding of light fields, in different formats using HEVC, based on a Random Forests ensemble classifier. Optimal features for training the classifier are found through a score fusion based approach. Using the HEVC still image profile, the proposed method gives speed-up of 56.23% for sub-aperture images. For pseudo video format, the proposed method outperforms others available in the literature, yielding an average speed-up of 62.18%, 56.54% and 44.73% for Random Access, Low-delay Main and All-Intra profiles respectively, with negligible decrease in RD performance. These are novel results in fast coding of light fields, which are useful for further research and benchmarking.}
}
@article{BARAJASSOLANO2020102690,
title = {Convolutional sparse coding framework for compressive spectral imaging},
journal = {Journal of Visual Communication and Image Representation},
volume = {66},
pages = {102690},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102690},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303116},
author = {Crisostomo Barajas-Solano and Juan-Marcos Ramirez and Henry Arguello},
keywords = {Compressive spectral imaging, Convolutional sparse coding, Sparse representation, Spectral images},
abstract = {Spectral images (SI) can be represented as 3D-arrays of spatial information across multiple wavelengths. Compressive Spectral Imaging (CSI) reduces sensing costs by sensing compressed versions of the scene, recovering a suitable version of the original SI solving a sparsity-inducing inverse problem. On the other hand, Convolutional Sparse Coding (CSC) has been successfully proved for representing gray-scale images, however it misses any correlation between images. This work considers the spatial-spectral correlation within SIs introducing an extension of the CSC signal model describing the SI as the sum of convolutions of 3D sparse coefficient maps with their respective 3D dictionary filters. Furthermore, we use the proposed CSC framework for recovering SIs from CSI measurements. The simulations results, using two different CSI acquisition architectures, show that the proposed CSC framework yields better representations of the SIs than those obtained under the traditional sparse signal representation approach, improving the quality of the recovered SIs.}
}
@article{ZHAI2019102645,
title = {Inversion of organic matter content in wetland soil based on Landsat 8 remote sensing image},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102645},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102645},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302664},
author = {Maotong Zhai},
keywords = {Landsat 8, Soil organic matter, Quantitative inversion, Remote sensing image},
abstract = {Using GF-1 and Landsat8 remote sensing images as data sources, combined with the experimental data of wetland soil sampling in Anyi County and Gao'an Research Area, the ability and difference of two remote sensing images in inversion of soil organic matter content were compared. The results show that the reflectivity of the two remote sensing images in the visible and near-infrared bands is significantly correlated with the soil organic matter content, and the correlation is the most in the near-infrared band. The index model established by the near-infrared band of GF-1 is more than the near-infrared band using Landsat8. The power model estimates slightly better. The multi-regression model established by introducing the blue band (dark blue band) and red band has higher inversion precision than the single-band model, especially the improvement effect of Landsat8 remote sensing image. Compared with Landsat8, GF-1 remote sensing image has higher spatial resolution and shorter revisit period, and has similar predictive ability in detecting soil organic matter content, which can replace Landsat8 remote sensing image.}
}
@article{LI2020102702,
title = {Human motion quality assessment toward sophisticated sports scenes based on deeply-learned 3D CNN model},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102702},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102702},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303232},
author = {Yedong Li and Hongmei He and Zhixin Zhang},
keywords = {Quality assessment, Deep learning, Human activities, Big data, Neural networks},
abstract = {Video may be subject to various distortions during acquisition, processing, compression, storage, transmission, and reproduction, and it results in reduced visual quality. In complex sports scenes under big data environment, the human body's movements are even more so. The quality of human motion can intuitively affect the human visual experience. Therefore, it is necessary to determine an intelligent quality assessment model to evaluate human motion in complex motion scenarios under big data environment. It can be used to dynamically monitor and adjust video quality, and it can be used for algorithms and parameter settings in motion image processing systems. With the popularity of deep learning, convolutional neural networks have become a very important method in the field of computer vision research. Based on the 2D-CNN algorithm, we propose a 3D convolutional neural network model for human motion quality assessment in complex motion scenarios. The model captures the pose characteristics, motion trajectory, video brightness and contrast in time and space. The model feeds back the reference and distorted video pairs into the network, with each output layer acting as a feature map. The local similarity between the feature maps obtained from the reference video and the distorted video is then calculated and combined to obtain a global image quality score. Experiments show that the model can achieve competitive performance in big data environment for video quality assessment.}
}
@article{ALOTUM2020102726,
title = {Secure and robust host-adapted color image watermarking using inter-layered wavelet-packets},
journal = {Journal of Visual Communication and Image Representation},
volume = {66},
pages = {102726},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102726},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303475},
author = {Hazem Munawer Al-Otum},
keywords = {Color image watermarking, Wavelet-packets, Multispectral analysis},
abstract = {This work presents a secure and robust color image watermarking for copyright protection applications, that is based on exploiting the multi-spectral properties of the primary color components of the RGB image. The proposed scheme employs the interconnection between the subbands of the primary color components in the wavelet-packet domain. The scheme is constructed to be adaptive, in the sense that the watermark bits are embedded in safe locations, depending on the inter-layer energy of coefficients in the wavelet-packets. The scheme immunity to attacks is improved by applying a two-level security procedure. To validate the high performance of the proposed scheme, several experimental tests were conducted and a comparative analysis was provided. The obtained results have shown improved watermarking robustness against a wide range of attacks while preserving a high watermarking imperceptibility.}
}
@article{ZHU2020102738,
title = {Massive-scale image retrieval based on deep visual feature representation},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102738},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102738},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303591},
author = {Hongpeng Zhu},
keywords = {Image retrieval, Visual feature, DNN},
abstract = {This paper proposes an image retrieval algorithm towards massive-scale multimedia data. In order to be consistent with human visual system, we first design a color attention function to describe the important of different image patches. Subsequently, we combine color and texture to construct candidate regions, which will be fed into a deep neural network (DNN) for deep representation extraction. Then, we design a similarity function to calculate the distance among different images, where top-ranking images are considered as the required images. Experimental results show the effectiveness and robustness of our proposed method.}
}
@article{ALAMEER2020102698,
title = {Objects and scenes classification with selective use of central and peripheral image content},
journal = {Journal of Visual Communication and Image Representation},
volume = {66},
pages = {102698},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102698},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303190},
author = {Ali Alameer and Patrick Degenaar and Kianoush Nazarpour},
keywords = {Visual recognition, Image understanding, Visual-data reduction, Biological visual-systems, Visual perception, Scene analysis},
abstract = {The human visual recognition system is more efficient than any current robotic vision setting. One reason for this superiority is that humans utilize different fields of vision, depending on the recognition task. For instance, experiments on human subjects show that the peripheral vision is more useful than the central vision in recognizing scenes. We tested our recently-developed model, that is, the elastic net-regularized hierarchical MAX (En-HMAX), in recognizing objects and scenes. In various experimental conditions, images were occluded with windows and scotomas of varying sizes. With this model, classification accuracies of up to 90% for objects and scenes were possible. Modelling human experiments, window and scotoma analysis with the En-HMAX model revealed that object and scene recognition are sensitive to the availability of data in the centre and the periphery of the images, respectively. Similarly, results of deep learning models have shown that the classification accuracy diminishes dramatically in the absence of the peripheral vision. These differences led us to further analyse the performance of the En-HMAX model with the parafoveal versus peripheral areas of vision, in a second study. Results of the second study show that approximately 50% of the visual field would be sufficient to achieve 96% accuracy in the classification of unseen images. The En-HMAX model adopts a relative order of importance, similar to the human visual system, depending on the image category. We showed that utilizing the relevant regions of vision can significantly reduce the image processing time and size.}
}
@article{FAN2019102659,
title = {Fusing dynamic deep learned features and handcrafted features for facial expression recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102659},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102659},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302809},
author = {Xijian Fan and Tardi Tjahjadi},
keywords = {Convolutional neural network, Facial expression recognition, Feature extraction},
abstract = {The automated recognition of facial expressions has been actively researched due to its wide-ranging applications. The recent advances in deep learning have improved the performance facial expression recognition (FER) methods. In this paper, we propose a framework that combines discriminative features learned using convolutional neural networks and handcrafted features that include shape- and appearance-based features to further improve the robustness and accuracy of FER. In addition, texture information is extracted from facial patches to enhance the discriminative power of the extracted textures. By encoding shape, appearance, and deep dynamic information, the proposed framework provides high performance and outperforms state-of-the-art FER methods on the CK+ dataset.}
}
@article{SASITHRADEVI2020102754,
title = {A new pyramidal opponent color-shape model based video shot boundary detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {67},
pages = {102754},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102754},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300043},
author = {A. Sasithradevi and S. {Mohamed Mansoor Roomi}},
keywords = {Shot boundary detection, Abrupt transition, Gradual transition, Opponent color space, Ensemble algorithm},
abstract = {Video shot boundary detection (VSBD) is one of the most essential criteria for many intelligent video analysis-related applications, such as video retrieval, indexing, browsing, categorization and summarization. VSBD aims to segment big video data into meaningful fragments known as shots. This paper put forwards a new pyramidal opponent colour-shape (POCS) model which can detect abrupt transition (AT) and gradual transition (GT) simultaneously, even in the presence of illumination changes, huge object movement between frames, and fast camera motion. First, the content of frames in the video subjected to VSBD is represented by the proposed POCS model. Consequently, the temporal nature of the POCS model is subjected to a suitable segment (SS) selection procedure in order to minimize the complexity of VSBD method. The SS from the video frames is examined for transitions within it using a bagged-trees classifier (BTC) learned on a balanced training set via parallel processing. To prove the superiority of the proposed VSBD algorithm, it is evaluated on the TRECVID 2001, TRECVID2007 and VIDEOSEG2004 data sets for classifying the basic units of video according to no transition (NT), AT and GT. The experimental evaluation results in an F1-score of 95.13%, 98.13% and 97.11% on the TRECVID 2001, TRECVID2007 and VIDEOSEG2004 data sets, respectively.}
}
@article{ZHOU2019102655,
title = {Scene categorization towards urban tunnel traffic by image quality assessment},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102655},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102655},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302767},
author = {Huachun Zhou and Sheng Zhou},
keywords = {Scene categorization, Tunnel traffic analysis, Information entropy, Rough clustering, Image quality model},
abstract = {Scene categorization is an indispensable technique in intelligent systems, such as scene parsing, video surveillance or autonomous driving. Considering traffic analysis under big data, in this paper, we propose scene categorization towards urban tunnel traffic based on image quality assessment. Specifically, the dataset is obtained through analyzing urban tunnel traffic incidents from 2016 to 2018. And we classify the traffic accidents in the big data environment. Then, the vehicles in the surveillance videos are extracted using conventional detector. The spatial information of vehicles in the image reflects the traffic situation. In order to encode such important information, we leverage the information clustering algorithm based on information entropy for image classification. Afterward, we establish a quality evaluation model based on each clustered images. The trained image quality assessment model will guide tunnel traffic classification and event analysis. The experimental results show the correct rate is more than 90%, and the overall detection effect is better than the k-modes algorithm and the Ng’k-modes algorithm.}
}
@article{YANG2019102631,
title = {Finite element model of concrete material based on CT image processing technology},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102631},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102631},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302524},
author = {Wenwei Yang},
keywords = {CT image, Numerical model, Concrete, Failure process},
abstract = {With the development of concrete performance research, more and more attention has been paid to the study of concrete performance, resulting in a variety of micro-structure finite element models, such as lattice model, beam-particle model, random aggregate model, with the emergence of CT technology, the realization of the non-destructive state of concrete internal micro-structure with digital The rendering method is presented. If the real or near-real finite element model of meso-structure can be established by using the information of CT plane images, it will play a certain role in the numerical simulation of concrete. Concrete includes aggregate, cement mortar and pore three parts, from the image characteristics, aggregate close to white, pore tend to black, cement mortar is between the two. Because of the relative obvious density difference between aggregate, mortar and pore, after CT scanning and converting to image, each component of concrete has better contrast, and it is easier to observe and extract aggregate contour. In this paper, CT image processing technology is used to preprocess the section image of concrete cylinder specimens in order to obtain accurate aggregate geometry and position information. On this basis, the reconstructed micro-finite element model is simulated and simulated in MATLAB, and the results are compared with those of other finite element models. The results show that the finite element concrete micro-model can make up for the shortcomings of the traditional random aggregate concrete model, and better reflect the mechanical characteristics of concrete materials, which opens up a new way for the ultimate in-depth study of the micro-damage mechanism of concrete materials.}
}
@article{LIU2020102723,
title = {Driver fatigue detection based on deeply-learned facial expression representation},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102723},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102723},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930344X},
author = {Zhongmin Liu and Yuxi Peng and Wenjin Hu},
keywords = {Fatigue detection, MB-LBP, PERCLOS, Fuzzy reasoning},
abstract = {Driver fatigue detection is a significant application in smart cars. In order to improve the accuracy and timeliness of driver fatigue detection, a fatigue detection algorithm based on deeply-learned facial expression analysis is proposed. Specifically, the face key point detection model is first trained by multi block local binary patterns (MB-LBP) and Adaboost classifier. Subsequently, the eyes and mouth state are detected by using the trained model to detect the 24 facial features. Afterwards, we calculate the number of two parameters that can describe the driver's fatigue state and the proportion of the closed eye time within the unit time (PERCLOS) and yawning frequency. Finally, the fuzzy inference system is utilized to deduce the driver's fatigue state (normal, slight fatigue, severe fatigue). Experimental results show that the proposed algorithm can detect driver fatigue degree quickly and accurately.}
}
@article{HUANG2020102700,
title = {Hierarchical Learning-Guided human motion quality assessment in big data environment},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102700},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102700},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303219},
author = {Zhiwei Huang and Yan Li and Shiguang Luo},
keywords = {Quality assessment, Reinforcement learning, Human activities, Big data, Hierarchical networks},
abstract = {Image information may be distorted during acquisition, processing, compression, and transmission. It is necessary to propose an intelligent image quality assessment model toward big data environment to quantify the degree of distortion of the image. This paper proposes a quality assessment model for human motion images. In complex scenes, the human body's action posture can be taken as an important feature point. Usually, in different scenes, the parts that affect the quality of the human body's posture are different. In other words, the weights of feature points that affect quality are different in different scenarios. However, due to the categorization of human movements, we can learn the quality assessment methods of different types of movements through sample training. Inspired by feature learning in the field of machine learning, we propose a hierarchical quality learning approach. We cast quality assessment as quality feature learning and layer by layer. The hierarchical quality learning method is based on deep reinforcement learning. The key part is that the method focuses on the region that containing more information on the features of the quality and enlarges the region layer by layer. Finally, we can determine the part of the body that affects the quality assessment. We compare this method with the subjective quality assessment results of the human observers and find that the proposed method achieves effective performance in big data environment to evaluate human motion quality.}
}
@article{GUO2020102741,
title = {Data encryption based blockchain and privacy preserving mechanisms towards big data},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102741},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102741},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303621},
author = {Leiyong Guo and Hui Xie and Yu Li},
keywords = {Blockchain, Data encryption, Privacy preserving, Big data},
abstract = {Blockchain is a key technique which can support Bitcoin. Blockchain is a decentralized infrastructure that uses chained data structure to verify and store data, and uses distributed node consensus mechanism to generate and update data. Blockchain has become a hot research topic since its attributes of decentralization, verifiability and anti-tampering. To stimulate the development of Blockchain, we conduct a comprehensive research on Blockchain. Specifically, we discuss various mainstream consensus mechanisms used in blockchain technology, and thoroughly analyze anonymity and privacy protection in digital currency. Aiming at data encryption mechanism, we discuss existing anonymity and privacy protection schemes. Our discussion can further promote the development of Blockchain.}
}
@article{LI2019102689,
title = {Application research of digital image technology in graphic design},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102689},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102689},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303104},
author = {Zhenyu Li},
keywords = {Digital image technology, Digital image, Graphic design, Image adjustment},
abstract = {With the development of the information age and the popularity of Internet computer technology, the application field of digital image technology has been further expanded. Digital image technology can not only buy the interchange of the current picture scene, the image adjustment, but also change the color texture of the image and the shape of the main body of the picture. The application of digital technology and imaging in graphic design has become an inevitable trend, and they play an invaluable role in graphic design. Graphic design is designed to meet the growing cultural needs of people, and it is clear that high standards of digital technology and imaging are not lacking. The development of graphic design is inseparable from the promotion of digital image technology, and there is a close relationship between the two. In the context of the rapid development of the current society, people's needs are increasing, and how to meet the visual needs of the social population has become a top priority. This paper takes the relationship between digital image technology and graphic design as the starting point, discusses the processing criteria of digital image technology in graphic design, and provides reference for relevant researchers.}
}
@article{LI2019102628,
title = {Knowledge driven temporal activity localization},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102628},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102628},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302494},
author = {Changlin Li and Zhihui Li and Zongyuan Ge and Mingjie Li},
keywords = {Temporal activity detection, Knowledge constraints, Reasoning module},
abstract = {In this paper, we focus on the problem of temporal activity detection, which aims to directly predict the temporal bounds of actions. Most existing temporal activity detection algorithms treat the classification of each action proposal separately and neglect vital semantic correlations between actions in one video. This will deteriorate the classification performance in the scenario of long-tail problems, where only a handful of examples are available for uncommon actions. To solve this problem, we propose to incorporate knowledge to reason over large scale action classes and maintain semantic coherency within one video. Specifically, we employ an implicit knowledge reasoning module and an explicit knowledge reasoning module to incorporate the knowledge constraints to facilitate temporal activity localization. To demonstrate the superiority of the proposed model, we test the proposed method on large-scale action detection datasets, namely ActivityNet and THUMOS’14 datasets. The experimental results have demonstrated the superiority of the proposed model. Codes and models will be released once this paper is accepted.}
}
@article{ZHAO2020102701,
title = {Perceptual visual quality assessment using deeply-learned gaze shifting kernel},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102701},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102701},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303220},
author = {Feng Zhao and Shiwang Huang and Renyan Long and Tiantian Zhang and Sang-Gyun Na},
keywords = {Image quality assessment, Human visual system, SSIM},
abstract = {Image quality assessment (IQA) is a useful technique in computer vision and machine intelligence. It is widely applied in image retrieval, image clustering and image recognition. IQA algorithms generally rely on human visual system (HVS), which can reflect how human perceive salient regions in the image. In this paper, we leverage both low-level features and high-level semantic features to select salient regions, which will be concatenated to form GSPs by the designed saliency-constraint algorithm to mimic human visual system. We design an enhanced IQA index based on the GSPs to calculate the simialrity between reference image and test image to achieve image quality assessment. Experiments demonstrate that our IQA method can achieve satisfactory performance.}
}
@article{ZOU2019102616,
title = {Scene flow estimation by depth map upsampling and layer assignment for camera-LiDAR system},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102616},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102616},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302378},
author = {Cheng Zou and Bingwei He and Mingzhu Zhu and Liwei Zhang and Jianwei Zhang},
keywords = {3D scene flow, Sensor fusion, Depth map upsampling},
abstract = {This paper presents a scene flow estimation method which functions by depth map upsampling and layer assignment for the camera-LiDAR (Light Detection And Ranging) system. The 3D geometry and motion of the observed scene are estimated simultaneously based on two consecutive frames from a camera and a LiDAR. The proposed technique begins with dense depth map upsamling guided by a corresponding RGB image. The scene is then classified to various moving layers by a hybrid method. Finally, the motion of each layer is constrained by the RGB image and depth image which provide a coarse 3D rigid motion. Experimental results on both public datasets and a real-word platform demonstrate the effectiveness of this technique.}
}
@article{SINGH2020102725,
title = {Identifying biometrics in the wild – A time, erosion and neural inspired framework for gait identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {66},
pages = {102725},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102725},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303463},
author = {Jaiteg Singh and Gaurav Goyal},
keywords = {Convolutional neural network, OU-ISIR, CASIA, Erosion, Silhouette},
abstract = {This paper evaluates the performance of contemporary gait identification systems. A time, erosion and neural inspired framework (TEN-FE) for gait identification was proposed to augment the performance of gait identification systems. Performance of TEN-FE framework was evaluated using CASIA and OU-ISIR large population dataset. Proposed framework relies on CNN and Reinforcement Learning to restrict the impact of confounding factors like baggage and bulky clothing on the accuracy of gait identification systems. Difference in gait signature due to time was also considered and normalized. The results observed a clear increase in system’s performance with minimal complexity and least hardware requirements.}
}
@article{WANG2019102644,
title = {Hierarchically engineering quality-related perceptual features for understanding breast cancer},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102644},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102644},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302652},
author = {Xusheng Wang and Xing Chen and Congjun Cao},
keywords = {Breast cancer, Deep learning, Quality-related, Weakly-supervised, Ranking algorithm},
abstract = {Breast cancer is generally acknowledged as the second leading cause of cancer death among women. Therefore, accurately understanding breast cancer from X-ray images is an indispensable technique in medical sciences and image analysis. In the work, we propose a novel perceptual deep architecture that hierarchically learns deep features from large-scale X-ray images, wherein human visual perception is naturally encoded. More specifically, given a rich number of breast cancer images, we first employ the well-known BING objectness measure to identify all possible visually/semantically salient patches. Due to the relatively huge number of BING object patches, a weakly-supervised ranking algorithm is designed to select high quality object patches according to human visual perception. Subsequently, an aggregation scheme is utilized to derive the deep features of high quality object patches within each brain cancer image. Based on the aggregated deep feature, a multi-class SVM is trained to classify each breast cancer into multiple levels. Extensive comparative studies and visualization results have demonstrated the effectiveness and efficiency of our proposed deep architecture.}
}
@article{CHEN2019102685,
title = {Multisource surveillance video coding with synthetic reference frame},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102685},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102685},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303062},
author = {Yu Chen and Ruimin Hu and Jing Xiao and Zhongyuan Wang},
keywords = {Surveillance video coding, Global knowledge, Local information, Reference frame},
abstract = {Due to the increasing growth of surveillance data, high-efficiency surveillance video coding schemes are demanded. However, the existing conventional coding framework has difficulties in handling rotation and zooming whilst the recently proposed multisource surveillance video coding method is inflexible and prone to be affected by pose errors. To this end, we propose to combine conventional surveillance video coding and multisource surveillance video coding into one unified framework. First, global knowledge in the form of 3D model and initial textures is employed to construct a knowledge based reference frame. Meanwhile, a temporal reference frame is also generated from the reconstructed frames in decoded picture buffer. Then, they are fused to obtain the synthetic reference frame to exploit global and local information. Finally, we add the synthetic reference frame into reference picture list and rearrange the list to optimize the overall efficiency. Experimental results demonstrate the effectiveness of the proposed method over state-of-the-art anchors.}
}
@article{CHEN2019102678,
title = {Quality-guided key frames selection from video stream based on object detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102678},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102678},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302998},
author = {Mingju Chen and Xiaofeng Han and Hua Zhang and Guojun Lin and M.M. Kamruzzaman},
keywords = {Convolutional neural network, Key frame, Object detection, SIFT characteristics, Quality assessment model},
abstract = {Object detection technique is widely applied in modern intelligent systems, such as pedestrian tracking, video surveillance. Key frames selection aims to select more informative frames and reduce amount of redundant information frames. Traditional methods leveraged SIFT feature, which have high key frame selection error rate. In this paper, we propose a novel key frames selection method based on object detection and image quality. Specifically, we first leverage object detector to detect object, such as pedestrian, vehicles. Then, each training frame will be assigned with a quality score, where frames contain objects have high quality score. Afterwards, we leverage CNN based AlexNet architecture for deep feature representation extraction. Our algorithm combines mutual information entropy and SURF image local features to extract key frames. Comprehensive experiments verify the feasibility of practicing the key frame extractor based on convolutional neural network by training the model, and conduct a quality assessment model study.}
}
@article{HE2019102654,
title = {Image quality recognition technology based on deep learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102654},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102654},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302755},
author = {Tao He and Xiaofeng Li},
keywords = {Low quality image, Deep learning, Image recognition, Support vector machines(SVM)},
abstract = {Image plays an important role in today's society and is an important information carrier. However, due to the problems in shooting or processing, image quality is often difficult to be guaranteed, and low-quality images are often difficult to identify, which results in the waste of information. How to effectively identify low-quality images has become a hot research topic in today's society. Deep learning has a good application in image recognition. In this paper, it is applied to low-quality image recognition. An image quality recognition technology based on deep learning is studied to effectively realize low-quality image recognition. Firstly, in the stage of image preprocessing, a low-quality image enhancement method is proposed, which uses non-linear transformation to enhance image contrast image, restore image details and enhance image quality. Secondly, the convolutional neural network is used to extract image features, and the L2 regularization method is introduced to optimize the over-fitting problem. Finally, SVM is used to recognize the output of convolutional neural network to realize low quality image recognition. Through simulation analysis, it is found that the image enhancement method proposed in the pre-processing stage can effectively enhance the image quality, and deep learning can effectively realize the recognition of the enhanced image and improve the recognition accuracy.}
}
@article{CHI2020102752,
title = {Blind tone mapped image quality assessment with image segmentation and visual perception},
journal = {Journal of Visual Communication and Image Representation},
volume = {67},
pages = {102752},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102752},
url = {https://www.sciencedirect.com/science/article/pii/S104732032030002X},
author = {Biwei Chi and Mei Yu and Gangyi Jiang and Zhouyan He and Zongju Peng and Fen Chen},
keywords = {High dynamic range image, Tone mapped image, Image quality assessment, Image segmentation, Visual perception, Feature clustering},
abstract = {With tone mapping, high dynamic range (HDR) image contents can be displayed on low dynamic range (LDR) display devices, in which some important visual information may be distorted. Thus, the tone mapped image (TMI) quality assessment is one of important issues in HDR image/video processing fields. Considering the difference of visual distortion degrees between the flat and complex regions in TMI, and considering that high-quality TMI should preserve as much information as possible of its original HDR image especially in the high/low luminance regions, this paper proposes a new blind TMI quality assessment method with image segmentation and visual perception. First, we design different features to describe the distortion of TMI’s different regions with two kinds of TMI segmentation. Then, considering that there lacks an efficient algorithm to quantify the importance of features, a feature clustering scheme is designed to eliminate the poor effect feature components in the extracted features to improve the effectiveness of the selected features. Finally, considering the diversity of tone mapping operator (TMO), which may cause global and local distortion of TMI, some other global features are also combined. At last, a final feature vector is formed to synthetically describe the distortion in TMI and used to blindly predict the TMI’s quality. Experimental results in the public ESPL-LIVE HDR database show that the Pearson linear correlation coefficient and Spearman rank order correlation coefficient of the proposed method reach 0.8302 and 0.7887, respectively, which is superior to the state-of-the-art blind TMI quality assessment methods, and it means that the proposed method is highly consistent with human visual perception.}
}
@article{WU2020102739,
title = {Massive-scale visual information retrieval towards city residential environment surveillance},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102739},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102739},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303608},
author = {Yuzhe Wu and Zhiyi Xu},
keywords = {Satellite image, Land policy, Rough set, Fuzzy C-means clustering, Quantum ant colony algorithm},
abstract = {Urban residential environment surveillance plays an important role in modern intelligent city. Satellite images have been applied in various fields, and the analysis and processing of satellite images has become an important means to obtain the information perceived by satellites. This paper focuses on city residential environment surveillance based on massive-scale visual information retrieval. Since the shortcomings of low contrast, blurred boundary, large amount of information and susceptibility to noise, the performance of satellite image segmentation is not satisfactory, which will affect residential environment surveillance. We design an improved rough set fuzzy C-means clustering algorithm combined with ant colony algorithm. More specifically, satellite images are classified based on the gradient of pixels according to the indistinguishable relation of the image combined with rough set theory. Then, the traditional fuzzy set-based fuzzy C-means clustering algorithm is applied to the satellite image segmentation technology. Subsequently, the improved algorithm-quantum ant colony algorithm and rough set fuzzy clustering C-means algorithm are combined to achieve accurate segmentation of satellite images. Afterwards, we propose a satellite image retrieval algorithm, which can assist city residential environment surveillance. Comprehensive experiment show that our proposed method is effective and robust in residential environment surveillance.}
}
@article{XU2019102630,
title = {Image quality study of CT imaging examination in children with childhood tumors under ultrasound-guided puncture},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102630},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102630},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302512},
author = {Huiying Xu and Rui Xin and Yufei Zhao and Xianmei Jin},
keywords = {Ultrasound, Puncture, Childhood tumor, CT imaging, Image quality},
abstract = {According to data released by the World Health Organization, malignant tumors have become the second leading cause of death among Chinese children. For malignant tumors, effective diagnostic information directly determines the subsequent treatment outcome. Because of the small age of children with childhood tumors and the specificity of their constitution, many diagnostic methods are not very effective in the diagnosis of children's tumors. Ultrasound-guided puncture is a widely used method. However, puncture and biopsy may lead to a series of serious complications such as bleeding, infection, organ damage and even tumor rupture. Regarding the complications caused by puncture and biopsy, most of the current studies use B-ultrasound imaging or even clinical observation to observe. Ultrasound is not applicable to bones and organs with more gas (such as intestinal tract). In response to these problems, this paper selects hospitalized children with solid tumors in the hospital oncology department, performs CT imaging examination on children who have undergone ultrasound-guided puncture, and conducts quality studies on CT imaging images to observe under ultrasound guidance. The complications after puncture were evaluated, and the feasibility of puncture biopsy in the diagnosis of childhood tumors was evaluated. The results show that the CT image after image quality control can meet the requirements of analyzing the condition and can effectively observe the complications after puncture. The results show that ultrasound-guided puncture can be used for the diagnosis of childhood tumors.}
}
@article{MANIMARAN2020102699,
title = {Visualization, Discriminability and Applications of Interpretable Saak Features},
journal = {Journal of Visual Communication and Image Representation},
volume = {66},
pages = {102699},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102699},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303207},
author = {Abinaya Manimaran and Thiyagarajan Ramanathan and Suya You and C.-C. Jay Kuo},
keywords = {Saak transform, Interpretable machine learning, Image classification, Adversarial attacks},
abstract = {In this work, we study the power of Saak features as an effort towards interpretable deep learning. Being inspired by the operations of convolutional layers of convolutional neural networks, multi-stage Saak transform was proposed. Based on this foundation, we provide an in-depth examination on Saak features, which are coefficients of the Saak transform, by analyzing their properties through visualization and demonstrating their applications in image classification. Being similar to CNN features, Saak features at later stages have larger receptive fields, yet they are obtained in a one-pass feedforward manner without backpropagation. The whole feature extraction process is transparent and is of extremely low complexity. The discriminant power of Saak features is demonstrated, and their classification performance in three well-known datasets (namely, MNIST, CIFAR-10 and STL-10) is shown by experimental results.}
}
@article{ZHAO2020102736,
title = {Artificial intelligence based ensemble approach for intrusion detection systems},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102736},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102736},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303578},
author = {Hongwei Zhao and Mingzhao Li and Haoyu Zhao},
keywords = {Artificial Intelligence, Ensembles, Pattern recognition, Internet attacks, Neural networks},
abstract = {Internet attacks pose a severe threat to most of the online resources and are a prime concern of security administrators these days. In spite of many efforts, the security techniques are unable to detect the intrusions accurately. Most of the methods suffer from the limitations of a high false positive rate, low detection rate and provide one solution which lacks the classification trade-offs. In this work, an effective two-stage method is proposed to produce a pool of non-dominating solutions or Pareto optimal solutions as base models and their ensembles for detecting the intrusions accurately. It generates Pareto optimal solutions to a chromosome structure in stage 1 formulating Pareto front. Whereas, another approximation to the Pareto front of optimal solutions is made to obtain non-dominating ensembles in the second stage. The final prediction ensemble solutions are computed from individual predictions using majority voting approach. Applicability of the suggested method is validated using benchmark dataset NSL-KDD dataset. The experimental results show that the recommended method provides better results than conventional ensemble techniques. The recommended method is also adequate to generate Pareto optimal solutions that address the issue of improving detection accuracy for minority as well as majority attack classes along with handling classification tradeoff problem. The proposed method resulted detection accuracy of 97% with FPR of 2% for KDD dataset respectively. The most attractive feature of the proposed method is that both generation of base classifier and their ensemble thereof are multi-objective in nature addressing the issue of low detection accuracy and classification tradeoffs.}
}
@article{LIU2020102724,
title = {Quality-related English text classification based on recurrent neural network},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102724},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102724},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303451},
author = {Cheng Liu and Xiaofang Wang},
keywords = {Recurrent neural network, Text categorization, English categorization, Feature Word Categorization},
abstract = {With the rapid development of artificial intelligence technology, text categorization technology is becoming more and more mature. However, text categorization in real situations still faces various unconstrained conditions. English text is an important part of text information, it is also an important way for people to get information from abroad. How can everyone get the desired content from the massive data quickly and accurately, it has become a hot issue in current research. This paper improves the current text categorization algorithm based on English quality-related text categorization. The design and implementation of text categorization system are illustrated with an example of English quality-related text categorization system, complete the research work of text categorization algorithm. The core work of this paper is to mine, classify and analyze large amounts of data in English text by using the method of combining cyclic neural network with quality. Finally, the essential features of high quality English texts are obtained. Traditional English text categorization algorithm if the amount of training data is large, it is easy to show some defects such as unclear feature items. In view of these problems, in order to improve the accuracy and flexibility of English text categorization, this paper proposes a quality-related English text categorization method based on cyclic neural network. A mechanism combining attention is proposed to improve the problem of label disorder and make the structure of the model more flexible. The model proposed in this paper is compared and optimized. Experiments show that the accuracy of neural text classification based on quality classification can reach about 96%.}
}
@article{CONDORI2020102748,
title = {An extension of the differential image foresting transform and its application to superpixel generation},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102748},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102748},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303694},
author = {Marcos A.T. Condori and Fábio A.M. Cappabianco and Alexandre X. Falcão and Paulo A.V. Miranda},
keywords = {Image foresting transform, Superpixels, Differential image foresting transform},
abstract = {The Image Foresting Transform (IFT) is a graph-based framework to develop image operators based on optimum connectivity between a root set and the remaining nodes, according to a given path-cost function. Its applications involve a variety of tasks, such as segmentation, boundary tracking, skeletonization, filtering, among others. The Differential Image Foresting Transform (DIFT) allows multiple IFT executions for different root sets and a same monotonically incremental path-cost function, making the processing time proportional to the number of modified nodes. In this paper, we extend the DIFT algorithm for non-monotonically incremental functions with root-based increases. This proposed extension, called Generalized DIFT (GDIFT), has been successfully used as the core part of some modern superpixels methods with state-of-the-art results. Experimental results show considerable efficiency gains over the sequential flow of IFTs for the generation of superpixels, also avoiding inconsistencies in image segmentation, which could occur with the regular DIFT algorithm.}
}
@article{ZHANG2020102643,
title = {Quality-guided video aesthetics assessment with social media context},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102643},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102643},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302640},
author = {Chao Zhang and Sitong Liu and Huizi Li},
keywords = {Video aesthetic assessment, Structure correlation, SVM},
abstract = {Media aesthetic assessment is a key technique in computer vision, which is widely applied in computer game rendering, video/image classification. Low-level and high-level features fusion-based video aesthetic assessment algorithms have achieved impressive performance, which outperform photo- and motion-based algorithms, however, these methods only focus on aesthetic features of single-frame while ignore the inherent relationship between adjacent frames. Therefore, we propose a novel video aesthetic assessment framework, where structural cues among frames are well encoded. Our method consists of two components: aesthetic features extraction and structure correlation construction. More specifically, we incorporate both low-level and high-level visual features to construct aesthetic features, where salient regions are extracted for content understanding. Subsequently, we develop a structure correlation-based algorithm to evaluate the relationship among adjacent frames, where frames with similar structure property should have a strong correlation coefficient. Afterwards, a kernel multi-SVM is trained for video classification and high aesthetic video selection. Comprehensive experiments demonstrate the effectiveness of our method.}
}
@article{SONG2019102684,
title = {Design of high-resolution quantization scheme with exp-Golomb code applied to compression of special images},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102684},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102684},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303050},
author = {Xiaoying Song and Bing Liu and Qijun Huang and Ruihan Hu},
keywords = {High-resolution quantization scheme, Adaptive quantizer, Exp-Golomb code, Medical image, Aerial image, Near-lossless compression},
abstract = {For the compression of special image, such as medical image and aerial image, the reconstructed image quality is of utmost importance in the performance analysis. In this paper, a high-resolution quantization scheme based on the exp-Golomb code is proposed, aiming at improving the reconstructed image quality and realizing high-resolution near-lossless compression. Rather than quantizing the whole image uniformly, which adopted by most popular quantization schemes, an adaptive quantizer with smaller distortion is designed. Both the quantization step size and the quantization proportion are determined adaptively. Compression algorithms based on our adaptive quantizer can provide better reconstructed image quality, and the high frequency information of the image, such as texture and edge, can be better preserved.}
}
@article{XIONG2020102708,
title = {Water leakage image recognition of shield tunnel via learning deep feature representation},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102708},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102708},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303293},
author = {Leijin Xiong and Dingli Zhang and Yu Zhang},
keywords = {Shield tunnel, Water leakage, Deep learning, Image recognition},
abstract = {With the development of urban metro, the research on structural diseases of shield tunnels has been becoming a hot research topic, especially the leakage water diseases. Deep learning-based algorithms have shown impressive performance in image processing domain, such as image classification, image recognition or image retrieval. In this paper, we propose a novel image recognition algorithm for water leakage diseases of shield tunnels based on deep learning algorithm. Water leakage images are classified into six categories, each of which are extracted deep representation for image recognition. We compare our method with Otsu algorithm (OA), Region Growing Algorithm (RGA), and Watershed Algorithm (WA) to show the effectiveness of our proposed method.}
}
@article{HUANG2020102709,
title = {Research on the parallelization of image quality analysis algorithm based on deep learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102709},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102709},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930330X},
author = {Jui-Chan Huang and Hao-Chen Huang and Hsin-Hung Liu},
keywords = {Deep learning, Image distortion, Image quality analysis, Parallelization},
abstract = {Image quality assessment is an indispensable in computer vision applications, such as image classification, image parsing. With the development of Internet, image data acquisition becomes more conveniently. However, image distortion is inevitable due to imperfect image acquisition system, image transmission medium and image recording equipment. Traditional image quality assessment algorithms only focus on low-level visual features such as color or texture, which could not encode high-level features effectively. CNN-based methods have shown satisfactory results in image quality assessment. However, existing methods have problems such as incomplete feature extraction, partial image block distortion, and inability to determine scores. So in this paper, we propose a novel framework for image quality assessment based on deep learning. We incorporate both low-level visual features and high-level semantic features to better describe images. And image quality is analyzed in a parallel processing mode. Experiments are conducted on LIVE and TID2008 datasets demonstrate the proposed model can predict the quality of the distorted image well, and both SROCC and PLCC can reach 0.92 or higher.}
}
@article{LI2020102750,
title = {Matrix-variate variational auto-encoder with applications to image process},
journal = {Journal of Visual Communication and Image Representation},
volume = {67},
pages = {102750},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102750},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303712},
author = {Jinghua Li and Huixia Yan and Junbin Gao and Dehui Kong and Lichun Wang and Shaofan Wang and Baocai Yin},
keywords = {Variational autoencoder, Matrix Gaussian distribution, Variational inference, Face completion, Image denoising},
abstract = {Variational Auto-Encoder (VAE) is an important probabilistic technology to model 1D vectorial data. However, when applying VAE model to 2D image, vectorization is necessary. Vectorization process may lead to dimension curse and lose valuable spatial information. To avoid these problems, we propose a novel VAE model based on matrix variables named as Matrix-variate Variational Auto-Encoder (MVVAE). In this model, input, hidden and latent variables are all in matrix form, therefore inherent spatial structure of 2D images can be maintained and utilized better. Especially, the latent variable is assumed to follow matrix Gaussian distribution which is more suitable for describing 2D images. To solve the weights and the posterior of latent variable, the variational inference process is given. The experiments are designed for three real-world application: reconstruction, denoising and completion. The experimental results demonstrate that MVVAE shows better performance than VAE and other probabilistic methods for modeling and processing 2D data.}
}
@article{XIAO2020102722,
title = {Human action recognition based on convolutional neural network and spatial pyramid representation},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102722},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102722},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303438},
author = {Jihai Xiao and Xiaohong Cui and Feng Li},
keywords = {Human action recognition, Spatial pyramid, Convolution neural networks, Cosine distance measure},
abstract = {Detecting and recognizing human action in natural scenarios, such as indoor and outdoor, is a significant technique in computer vision and intelligent systems, which is widely applied in video surveillance, pedestrian tracking and human-computer interaction. Conventional approaches have been proposed based on various features and achieved impressive performance. However, these methods failed to cope with partial occlusion and changes of posture. In order to address these limitations, we propose a novel human action recognition method. More specifically, in order to capture image spatial composition, we leverage a three-level spatial pyramid feature extraction scheme, where each pyramid is encoded by local features. Thereafter, regions generated by a proposal algorithm are fed into a dual-aggregation net for deep representation extraction. Afterwards, both local features and deep features are fused to describe each image. To describe human action category, we design a metric CXQDA based on Cosine measure and Cross-view Quadratic Discriminant Analysis (XQDA) to calculate the similarity among different action categories. Experimental results demonstrate that our proposed method can effectively cope with object scale variations, partial occlusion and achieve competitive performance.}
}
@article{ZHOU2019102641,
title = {Adaptive illumination-invariant face recognition via local nonlinear multi-layer contrast feature},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102641},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102641},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302627},
author = {Lifang Zhou and Weisheng Li and Yuewei Du and Bangjun Lei and Shan Liang},
keywords = {Illumination invariant face recognition, Local binary patterns, Local nonlinear multi-layer contrast patterns, Fuzzy fusion framework.},
abstract = {Traditional face recognition method usually faces the challenge of varying lighting condition. In this paper, we propose an illumination-invariant local binary descriptor learning method for face recognition. Unlike local binary descriptor (LBP) and its variants, which usually utilize the rigid sign function for binarization despite of data distributions. We first determine a dynamic thresholds strategy including the information of illumination variation to extract nonlinear multi-layer contrast features. Specially, Exponential Discriminant Analysis (EDA) is designed to act as preprocessing which can contribute to improve the discriminative ability of the face image by enlarging the margin between different classes relative to the same class. To further improve the recognition performance, we combined our preliminary work, the adaptive fuzzy fusion framework, to integrate the recognition results for multi-scale features spaces. Extensive experiments conducted on four face databases validate the effectiveness of the proposed method for illumination face recognition.}
}
@article{ZHAO2020102706,
title = {IR saliency detection via a GCF-SB visual attention framework},
journal = {Journal of Visual Communication and Image Representation},
volume = {66},
pages = {102706},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102706},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930327X},
author = {Yufei Zhao and Yong Song and Xu Li and Muhammad Sulaman and Zhengkun Guo and Xin Yang and Fengning Wang and Qun Hao},
keywords = {Saliency detection, IR images, Bayes formula, Visual attention},
abstract = {Infrared (IR) saliency detection with high detection accuracy is a challenging task due to the complex background and low contrast of IR images. In this paper, an IR saliency detection method via a new visual attention framework is proposed, which comprises two phases. In the first phase, a Gray & Contrast Features (GCF) model is established, in which the IR image is processed in two feature channels, a gray feature channel and a contrast feature channel. And then a primary feature map can be obtained by fusing the gray and contrast features from these two channels, which is the basis of the second phase. In the second phase, a Similarity-based Bayes (SB) model is established, in which two prior probabilities and two likelihood functions are calculated according to the previously obtained primary feature map. Finally, the saliency map is calculated with the obtained prior probabilities and likelihood functions by Bayes formula. Experimental results indicate that the proposed method can effectively reduce noise and enhance contrast of IR images with complex background and low contrast, and obtain a higher detection accuracy and robustness than seven state-of-the-art methods.}
}
@article{WANG2020102735,
title = {E-commerce personalized recommendation analysis by deeply-learned clustering},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102735},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102735},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303566},
author = {Kai Wang and Tiantian Zhang and Tianqiao Xue and Yu Lu and Sang-Gyun Na},
keywords = {Clustering algorithm, Deep learning, Recommendation system},
abstract = {With the development of Internet, personalized recommendation has played an important role in human modern lives. Since the number of users’ data is always large-scale, traditional algorithms cannot effectively cope with e-commerce personalized recommendation tasks. This paper proposes an e-commerce product personalized recommendation system based on learning clustering representation. Traditional kNN method has limitation in selecting adjacent object set. Thus, we introduce neighbor factor and time function and leverage dynamic selection model to select the adjacent object set. We combine RNN as well as attention mechanism to design the e-commerce product recommendation system. Comprehensive experimental results have shown the effectiveness of our proposed method.}
}
@article{KIM2019102683,
title = {Multiple object tracking in soccer videos using topographic surface analysis},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102683},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102683},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303049},
author = {Wonjun Kim},
keywords = {Multiple object tracking, Topographic surface, Color similarity, Spatial proximity},
abstract = {Multiple object tracking is still a challenging problem in computer vision even though there have been several attempts lately to resolve the tracking problem in the framework of deep neural networks. In this paper, a novel method for multiple object tracking in soccer videos, which often contain complicated interactions between players with severe occlusions, is introduced. To do this, we propose to interpret the extracted foreground regions in a given frame as the topographic surface. This gives a great help to reliably chase target players by accurately providing the boundary lines of each object even with occlusions. Color similarity and spatial proximity are subsequently employed to refine the estimated position of target players for continuous tracking over whole video sequences. Experimental results on various soccer videos, which are taken of the actual games with the wide-angle camera, demonstrate that the proposed method is effective for tracking multiple players in the dynamic scene of the soccer video.}
}
@article{YAN2020102747,
title = {Detecting spatiotemporal irregularities in videos via a 3D convolutional autoencoder},
journal = {Journal of Visual Communication and Image Representation},
volume = {67},
pages = {102747},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102747},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303682},
author = {Mengjia Yan and Jingjing Meng and Chunluan Zhou and Zhigang Tu and Yap-Peng Tan and Junsong Yuan},
keywords = {Spatiotemporal irregularity detection, Autoencoder, 3D convolution, Anomaly detection, Unsupervised learning, Real-time},
abstract = {Spatiotemporal irregularities (i.e., the uncommon appearance and motion patterns) in videos are difficult to detect, as they are usually not well defined and appear rarely in videos. We tackle this problem by learning normal patterns from regular videos, while treating irregularities as deviations from normal patterns. To this end, we introduce a 3D fully convolutional autoencoder (3D-FCAE) that is trainable in an end-to-end manner to detect both temporal and spatiotemporal irregularities in videos using limited training data. Subsequently, temporal irregularities can be detected as frames with high reconstruction errors, and irregular spatiotemporal patterns can be detected as blurry regions that are not well reconstructed. Our approach can accurately locate temporal and spatiotemporal irregularities thanks to the 3D fully convolutional autoencoder and the explored effective architecture. We evaluate the proposed autoencoder for detecting irregular patterns on benchmark video datasets with weak supervision. Comparisons with state-of-the-art approaches demonstrate the effectiveness of our approach. Moreover, the learned autoencoder shows good generalizability across multiple datasets.}
}
@article{CHENG2019102638,
title = {Road surface condition classification using deep learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102638},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102638},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302597},
author = {Lushan Cheng and Xu Zhang and Jie Shen},
keywords = {Deep learning, Road condition, Activation function, Image recognition, Intelligent driving},
abstract = {Traditional image recognition technology currently cannot achieve the fast real-time high-accuracy performance necessary for road recognition in intelligent driving. Deep learning models have been recently emerging as promising tools to achieve this performance. The recognition performance of such models can be boosted using appropriate selection of the activation functions. This paper proposes a deep learning approach for the classification of road surface conditions, and constructs a new activation function based on the rectified linear unit Rectified Linear Units (ReLu) activation function. The experimental results show a classification accuracy of 94.89% on the road state database. Experiments on public datasets demonstrate that the proposed convolutional neural network model with the improved activation function has better generalization and excellent classification performance.}
}
@article{HELBERT2019102614,
title = {Patch graph-based wavelet inpainting for color images},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102614},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102614},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302354},
author = {David Helbert and Mohamed Malek and Pascal Bourdon and Philippe Carré},
keywords = {Image processing, Inpainting, Color image},
abstract = {We propose a novel inpainting process for color images. Our algorithm is based on the graph-based wavelet regularization and the non-local mean approach. At each step damaged structures are estimated by computing a graph of patches and applying a regularization model using a wavelet transform on graphs. Our approach uses color information of the image to reconstruct missing data according to local geometry. We show that the graph can be used to model geometry information in the frame of inpainting and to merge candidate pixels from a graph-based wavelet regularization. We provide details on numerical approaches and the results highlight an improvement of the geometrical information reconstruction of color images.}
}
@article{ZHAO2020102707,
title = {Application of multimedia technology in water conservancy and hydropower engineering},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102707},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102707},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303281},
author = {Jingfeng Zhao and Jing Zhang},
keywords = {Multimedia, Water conservancy and cydro-power, Demonstration system, Information technology},
abstract = {Multimedia covers a wide range. In general, digital audio production, animation video production, website production, and even game development can all be attributed to multimedia. The definition of multimedia narrowly defined, that is, the project with interactive program development as the main object of this paper, such as interactive CD production, touch screen presentation production, etc. Of course, there will still be a lot of content related to graphic design, animation, video processing, audio production and so on. The application of multimedia technology in water conservancy and hydro-power engineering is characterized by a variety of media means to represent the design, construction process and post-construction scene of water conservancy and hydro-power projects and to simulate the phenomenon in the project, such as the performance of water conservancy and hydro-power projects. Hub layout, structure of main buildings, dam flood discharge, rubber dam dam overflow, sluice dispatching process, ship lock crossing process, etc. In the water conservancy and hydro-power project, computer multimedia technology has been widely used from the general design proposal to the entire pivot project demonstration system. This paper mainly introduces the design and development of the multimedia demonstration system for water conservancy and hydro-power projects.}
}
@article{PANG2019102676,
title = {Salient object detection based on novel graph model},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102676},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102676},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302974},
author = {Yu Pang and Xiaosheng Yu and Ying Wang and Chengdong Wu},
keywords = {Salient object detection, Background prior, Novel graph model, Saliency propagation, Optimization method},
abstract = {In this paper, we present a salient object detection method based on novel graph structure. Given image is segmented into small image regions as basic units, we firstly construct an effective background-based map, each image region’s saliency value is determined by its feature contrast with the image boundary. Then, saliency propagation mechanism is used to update all regions’ saliency values by introducing a novel graph structure to better exploit the relationship between adjacent image regions. Finally, we propose an optimization method to further highlight salient objects and suppress background noises. Experimental results demonstrate adequately the superiority of proposed approach.}
}
@article{CHEN2020102749,
title = {PixelHop: A successive subspace learning (SSL) method for object recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102749},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102749},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303700},
author = {Yueru Chen and C.-C. Jay Kuo},
keywords = {Machine learning, Subspace learning, Computer vision, Pattern recognition},
abstract = {A new machine learning methodology, called successive subspace learning (SSL), is introduced in this work. SSL contains four key ingredients: (1) successive near-to-far neighborhood expansion; (2) unsupervised dimension reduction via subspace approximation; (3) supervised dimension reduction via label-assisted regression (LAG); and (4) feature concatenation and decision making. An image-based object classification method, called PixelHop, is proposed to illustrate the SSL design. It is shown by experimental results that the PixelHop method outperforms the classic CNN model of similar model complexity in three benchmarking datasets (MNIST, Fashion MNIST and CIFAR-10). Although SSL and deep learning (DL) have some high-level concept in common, they are fundamentally different in model formulation, the training process and training complexity. Extensive discussion on the comparison of SSL and DL is made to provide further insights into the potential of SSL.}
}
@article{LIU2019102651,
title = {3DSportNet: 3D sport reconstruction by quality-aware deep multi-video summation},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102651},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102651},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930272X},
author = {Zhenkun Liu},
keywords = {3D reconstruction, Quality model, Weakly-supervised learning},
abstract = {Automatically reconstructing 3D sceneries from video sequences is an indispensable technique in computer 3D games, urban planning, and intelligent navigation. Many previous work relies on complicated and expensive equipment to fulfill 3D reconstruction under constrained environments. Nevertheless, such schemes are not readily to be applied for reconstructing 3D sport sceneries, such as basketball and mountain climbing. In this work, we propose a novel deep architecture: 3DSportNet, which reconstructs 3D sport sceneries by making use of multiple handheld videos captured by smart phones. In particular, given a rich of mobile videos captured by users, we extract multiple deep/shallow visual features from each sport video frame by leveraging the weakly-supervised semantic encoding. Afterward, a geometry-aware quality model is designed to summarize the multiple videos into multiple key frames from each single video, wherein the objective is that the selected key frames can maximally reconstruct the multiple sport videos. Based on this, we employ the key frames to reconstruct sport videos by utilizing the PMVS2 software. Comprehensive experimental comparisons and visualization results have shown that our method can produce very real 3D sport sceneries and athletes. Besides, the 3D reconstruction time consumption is reduced by 95% compared to conventional methods.}
}
@article{LIU2020102721,
title = {Hyperspectral image quality based on convolutional network of multi-scale depth},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102721},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102721},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303426},
author = {Lei Liu and Min Sun and Xiang Ren and Xiuxian Li and Qiaoru Zhang and Li Ma and Yongning Li and Mo Song},
keywords = {Hyperspectral image, Multi-scale deep convolutional network, Quality research, Super-resolution processing},
abstract = {Hyperspectral imagery has been widely used in military and civilian research fields such as crop yield estimation, mineral exploration, and military target detection. However, for the limited imaging equipment and the complex imaging environment of hyperspectral images, the spatial resolution of hyperspectral images is still relatively low, which limits the application of hyperspectral images. So, studying the data characteristics of hyperspectral images deeply and improving the spatial resolution of hyperspectral images is an important prerequisite for accurate interpretation and wide application of hyperspectral images. The purpose of this paper is to deal with super-resolution of the hyperspectral image quickly and accurately, and maintain the spectral characteristics of the hyperspectral image, makes the spectral separability of the substrate in the original image remains unchanged after super-resolution processing. This paper first learns the mapping relationship between the spectral difference of low-resolution hyperspectral image and the spectral difference of the corresponding high-resolution hyperspectral image based on multiple scale convolutional neural network, Thus, apply this mapping relationship to the input low-resolution hyperspectral image generally, getting the corresponding high resolution spectral difference. Constrained space by using the image of reconstructed spectral difference, this requires the low-resolution hyperspectral image generated by the reconstructed image is to be close to the input low-resolution hyperspectral image in space, so that the whole process becomes a closed circulation system where the low-resolution hyperspectral image generation of high-resolution hyperspectral images, then back to low-resolution hyperspectral images. This innovative design further enhances the super-resolution performance of the algorithm. The experimental results show that the hyperspectral image super-resolution method based on convolutional neural network improves the input image spatial information, and the super-resolution performance of the model is above 90%, which can maintain the spectral information well.}
}
@article{JIANG2020102745,
title = {Screen content image quality assessment based on convolutional neural networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {67},
pages = {102745},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102745},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303669},
author = {Xuhao Jiang and Liquan Shen and Qing Ding and Linru Zheng and Ping An},
keywords = {Image quality assessment, Screen content image, No-reference, Convolutional neural network},
abstract = {Screen content image (SCI) is a composite image including textual and pictorial regions resulting in many difficulties in image quality assessment (IQA). Large SCIs are divided into image patches to increase training samples for CNN training of IQA model, and this brings two problems: (1) local quality of each image patch is not equal to subjective differential mean opinion score (DMOS) of an entire image; (2) importance of different image patches is not same for quality assessment. In this paper, we propose a novel no-reference (NR) IQA model based on the convolutional neural network (CNN) for assessing the perceptual quality of SCIs. Our model conducts two designs solving problems which benefits from two strategies. For the first strategy, to imitate full-reference (FR) CNN-based model behavior, a CNN-based model is designed for both FR and NR IQA, and performance of NR-IQA part improves when the image patch scores predicted by FR-IQA part are adopted as the ground-truth to train NR-IQA part. For the second strategy, image patch qualities of one entire SCI are fused to obtain the SCI quality with an adaptive weighting method taking account the effect of the different image patch contents. Experimental results verify that our model outperforms all test NR IQA methods and most FR IQA methods on the screen content image quality assessment database (SIQAD). On the cross-database evaluation, the proposed method outperforms the existing NR IQA method in terms of at least 2.4 percent in PLCC and 2.8 percent in SRCC, which shows high generalization ability and high effectiveness of our model.}
}
@article{MILOTTA2019102664,
title = {Egocentric visitors localization in natural sites},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102664},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102664},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302858},
author = {Filippo L.M. Milotta and Antonino Furnari and Sebastiano Battiato and Giovanni Signorello and Giovanni M. Farinella},
keywords = {Egocentric (First Person) vision, Localization, GPS, Multimodal data fusion},
abstract = {Localizing visitors in natural environments is challenging due to the unavailability of pre-installed cameras or other infrastructure such as WiFi networks. We propose to perform localization using egocentric images collected from the visitor’s point of view with a wearable camera. Localization can be useful to provide services to both the visitors (e.g., showing where they are or what to see next) and to the site manager (e.g., to understand what the visitors pay more attention to and what they miss during their visits). We collected and publicly released a dataset of egocentric videos asking 12 subjects to freely visit a natural site. Along with video, we collected GPS locations by means of a smartphone. Experiments comparing localization methods based on GPS and images highlight that image-based localization is much more reliable in the considered domain and small improvements can be achieved by combining GPS- and image-based predictions using late fusion.}
}
@article{LI2019102640,
title = {Learning attentive dynamic maps (ADMs) for Understanding Human Actions},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102640},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102640},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302615},
author = {Chuankun Li and Yonghong Hou and Wanqing Li and Pichao Wang},
keywords = {Human-robot/machine interaction, Deep learning, Human action recognition},
abstract = {This paper presents a novel end-to-end trainable deep architecture to learn an attentive dynamic map (ADM) for understanding human motion from skeleton data. An ADM intends not only to capture the dynamic information over the period of human motion, referred to as an action, as the conventional dynamic image/map does, but also to embed in it the spatio-temporal attention for the classification of the action. Specifically, skeleton sequences are encoded into sequences of Skeleton Joint Maps (STMs), each STM encodes both joint location (i.e. spatial) and relative temporal order (i.e. temporal) of the skeleton in the sequence. The STM sequences are fed into a customized 3DConvLSTM to explore the local and global spatio-temporal information from which a dynamic map is learned. This dynamic map is subsequently used to learn the spatio-temporal attention at each time-stamp. ADMs are then generated from the learned attention weights and all hidden states of the 3DConvLSTM and used for action classification. The proposed method achieved competitive performance compared with the state-of-the-art results on the Large Scale Combined dataset, MSRC-12 dataset and NTU RGB+D dataset.}
}
@article{CHEN2020102734,
title = {Video compressed sensing reconstruction based on structural group sparsity and successive approximation estimation model},
journal = {Journal of Visual Communication and Image Representation},
volume = {66},
pages = {102734},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102734},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303554},
author = {Jian Chen and Zhifeng Chen and Kaixiong Su and Zheng Peng and Nam Ling},
keywords = {Compressed sensing, Group sparsity, Interframe estimation, Reconstruction algorithms},
abstract = {The existing video compressed sensing (CS) algorithms for inconsistent sampling ignore the joint correlations of video signals in space and time, and their reconstruction quality and speed need further improvement. To balance reconstruction quality with computational complexity, we introduce a structural group sparsity model for use in the initial reconstruction phase and propose a weight-based group sparse optimization algorithm acting in joint domains. Then, a coarse-to-fine optical flow estimation model with successive approximation is introduced for use in the interframe prediction stage to recover non-key frames through alternating optical flow estimation and residual sparse reconstruction. Experimental results show that, compared with the existing algorithms, the proposed algorithm achieves a peak signal-to-noise ratio gain of 1–3dB and a multi-scale structural similarity gain of 0.01–0.03 at a low time complexity, and the reconstructed frames not only have good edge contours but also retain textural details.}
}
@article{SHARMA2020102682,
title = {Anti-forensics of median filtering and contrast enhancement},
journal = {Journal of Visual Communication and Image Representation},
volume = {66},
pages = {102682},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102682},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303037},
author = {Shishir Sharma and Hareesh Ravi and A.V. Subramanyam and Sabu Emmanuel},
keywords = {Anti-forensics, Median filtering, Contrast enhancement, Huber Markov random field},
abstract = {Digital images can be convincingly edited using image editing tools. In order to identify such image processing operations, various forensic techniques have been proposed. In response, anti-forensic operations designed as counter-measures have been devised. In this paper, we propose an anti-forensic technique to counter spatial domain forensic detectors and demonstrate its accuracy on popular image manipulation operations such as median filtering and contrast enhancement. The integrated anti-forensic attack is formulated as an optimization problem. The proposed optimization modifies the image so as to incorporate the median filtering or contrast enhancement operation while ensuring that its spatial characteristics do not change significantly. Through a series of experiments, we prove that the proposed algorithm can severely degrade the performance of median filtering and contrast enhancement detectors. The proposed algorithm also outperforms popular anti-forensic algorithms.}
}
@article{PENG2020102746,
title = {Face presentation attack detection based on chromatic co-occurrence of local binary pattern and ensemble learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {66},
pages = {102746},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102746},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303670},
author = {Fei Peng and Le Qin and Min Long},
keywords = {Presentation attack detection, Face recognition, Color distortion, Chromatic co-occurrence of local binary pattern, Ensemble learning},
abstract = {To counter face presentation attacks in face recognition (FR), color texture has been successfully used for face presentation attack detection (PAD) in recent years. However, the existing research does not fully consider the correlation between different color channels as well as the optimization of classification for face PAD. To resolve these limitations, a face PAD scheme based on chromatic co-occurrence of local binary pattern (CCoLBP) and ensemble learning (EL) is proposed in this paper. A color distortion-based face PAD model is first built, and then the chromatic discrepancies between bona fide faces and artefacts are analyzed. After that, CCoLBP is extracted as the feature to characterize these discrepancies. Meanwhile, an EL based classifier is put forward to reduce the effect of class imbalance and to improve the generalization ability. Experimental results and analysis indicate that the proposed scheme can achieve an overall good performance. Moreover, it can achieve significant improvement in the cross-database test, and its computational complexity can meet the requirement of real time applications.}
}
@article{WANG2019102648,
title = {Generative image deblurring based on multi-scaled residual adversary network driven by composed prior-posterior loss},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102648},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102648},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930269X},
author = {Meng Wang and Shengyu Hou and Huafeng Li and Fan Li},
keywords = {Image deblurring, Generative adversarial network, Residual learning, Prior distribution, Histogram of gradients},
abstract = {Conditional Generative Adversarial Networks (CGANs) have been introduced to generate realistic images from extremely degraded inputs. However, these generative models without prior knowledge of spatial distributions has limited performance to deal with various complex scenes. In this paper, we proposed a image deblurring network based on CGANs to generate ideal images without any blurring assumption. To overcome adversarial insufficiency, an extended classifier with different attribute domains is formulated to replace the original discriminator of CGANs. Inspired by residual learning, a set of skip-connections are cohered to transfer multi-scaled spatial features to the following high-level operations. Furthermore, this adversary architecture is driven by a composite loss that integrates histogram of gradients (HoG) and geodesic distance. In experiments, an uniformed adversarial iteration is circularly applied to improve image degenerations. Extensive results show that the proposed deblurring approach significantly outperforms state-of-the-art methods on both qualitative and quantitative evaluations.}
}
@article{LU2020102718,
title = {Weakly-supervised large-scale image modeling for sport scenes and its applications},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102718},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102718},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303396},
author = {Congsheng Lu and Feng Zhai},
keywords = {Sport scene modeling, Weakly-supervised learning, MDA},
abstract = {Image modeling towards sport scenes plays an important role in sport image classification and analysis. Traditional algorithms for sport image modeling required carefully hand-crafted features, which cannot be popularized in practical application, especially with the emergence of massive-scale data. Weakly-supervised learning algorithms have shown effectiveness in modeling data with image-level labels. Thus, in this paper, we propose a weakly-supervised learning based method for sport image modeling without utilizing bounding box annotations, which can be used for various sport image applications. More specifically, we first collect large-scale sport images from existing datasets and Internet, and we annotate them at image-level labels. Subsequently, we leverage region proposal generation algorithm to select discriminative regions that can effectively represent the category of images. Each region is fed into a pre-trained CNN architecture to extract deep representation. Afterwards, we design an improved multiple discriminant analysis (MDA) algorithm to project these datapoints to a subspace that can more easily to distinguish different sport categories. Comprehensive experiments have shown the effectiveness and robustness of our proposed method.}
}
@article{HOU2019102637,
title = {A fuzzy interaction scheme of mid-air gesture elicitation},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102637},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102637},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302585},
author = {Wenjun Hou and Guangyu Feng and Yiting Cheng},
keywords = {Natural interaction, Gesture elicitation, Fuzzy match, Guessibility},
abstract = {In a virtual assembly scenario, a semantic model of mid-air gesture interaction is established through a user-defined elicitation experiment. Considering the spatial-temporal continuity of the gesture movement, a fuzzy interaction scheme of gesture segments is proposed based on Trie Tree and Levenshtein distance. The experiment result proves that this design can effectively alleviate the effect of missing input sequences and recognition errors. Moreover, it helps relieve the memory load for users by establishing a close correlation of input actions.}
}
@article{ZHANG2019102639,
title = {Cityscape protection using VR and eye tracking technology},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102639},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102639},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302603},
author = {Le-Min Zhang and Ruo-Xi Zhang and Tay-Sheng Jeng and Zi-Yuan Zeng},
keywords = {Urban renewal, Three-dimensional eye tracking, Cityscape feature identification, Human-oriented, Smart city},
abstract = {The traditional method for reconstructing cityscape relies greatly on the subjective judgment of designers, which makes the cityscape simple and homogenized. This paper aims to propose a new integrated approach to protect and design cityscape based on virtual reality (VR) and eye tracking technology. Through the integration and quantification of the eye tracking data and the protocol analysis data in the VR environment, this research has revealed the mechanism of identifying the cityscape features, and discovered the differences in the perception of the cityscape features by different people, thus proposing the multi-cultural integrated strategy for protecting cityscape. This research is of great significance for building a human-oriented scientific planning and protection method and promoting the application of cutting-edge digital technology in the field of smart city governance.}
}
@article{ZHANG2019102634,
title = {Image quality optimization towards lidar registration based on iterative termination},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102634},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102634},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930255X},
author = {Wanyi Zhang and Xiuhua Fu and Chunyang Wang},
keywords = {Laser radar, Registration image, Quality optimization, Image processing},
abstract = {Image quality optimization is a key technique in image processing, whose goal is to improve image quality by image enhancement or image format transform. This paper aims at optimizing image acquisition using Lidar registration, which can cope with disadvantages of conventional algorithms such as low-resolution. Specifically, we propose an iterative termination optimization strategy based on image quality perception features and local mean estimation. First, fuzzy images with different types and degrees of distortion are incorporated to form a representative natural image set, and feature maps of fuzzy images are extracted by the natural scene statistical method in the spatial domain. Noticeably, the proposed algorithm which performs iterative deblurring operation records the optimal iteration point based on recording the quality value FSIM of the restored image, and calibrates the corresponding feature vector in the sample library with the optimal iteration point (step number). Afterwards, we leverage LME method to implement an estimate of the number of iteration steps. Based on these two steps, the estimation of the initial iterative monitoring point is completed, so that the subsequent adaptive iterative termination work is more purposeful to monitor the defuzzification metric. The optimization operation can be completed faster effectively.}
}
@article{WANG2019102647,
title = {Image classification towards transmission line fault detection via learning deep quality-aware fine-grained categorization},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102647},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102647},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302688},
author = {Yanhai Wang and Qingquan Li and Bo Chen},
keywords = {Fine-grained categorization, Fault recognition, Quality model, Fast R-CNN, SVM},
abstract = {Object detection and image classification are basic tasks in computer vision. In this paper, we introduce fault detection towards transmission line. Traditional fault detection methods in the transmission line are prone to be affected by the noise and transient magnitude. To overcome these limitations, we propose a novel fault zone detection method, where quality-aware fine-grained categorization model is well encoded for category cues discovery. The goal of our approach is to recognize the most discriminative image patches for classification. The key techniques of our method include quality-based discriminative feature extraction and wavelet-support vector machine. We extract the features of the line currents by leveraging Fast R-CNN based image samples decomposition, where quality module is utilized to choose the most discriminative regions. Afterwards, the extracted features are fed into a SVM to recognize the fault. We conduct comprehensive experiment on transmission line fault identification to verify the availability and superiority of our proposed method.}
}
@article{CHEN2020102720,
title = {Radar remote sensing image retrieval algorithm based on improved Sobel operator},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102720},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102720},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303414},
author = {Guobin Chen and Zhiyong Jiang and M.M. Kamruzzaman},
keywords = {Radar image retrieval, Blocking histogram, Sobel operator, Gray level co-occurrence matrix (GLCCM)},
abstract = {Aiming at the time-consuming problem caused by large computational load of radar image retrieval, based on blocking histogram, Sobel edge detection operator and gray level co-occurrence matrix (GLCCM), new radar remote sensing image retrieval algorithm based on improved Sobel operator is proposed. Firstly, the Sobel edge detection algorithm is used to process the image, the edge image is acquired, the radar remote sensing image is analyzed from different angles, and then the different radar remote sensing images are transformed. Then, based on the above processing, Radar Remote Sensing Image Retrieval Algorithm is acquired; finally, the plurality of statistic of the matrix is recorded as a feature vector describing the radar image, and the image is retrieved according to the feature vector of the radar image. Through a large number of experiments, Radar Remote Sensing Image Retrieval algorithm can greatly reduce the retrieval time, and it also has a good retrieval effect for images with rich texture.}
}
@article{GUAN2020102744,
title = {An efficient high-capacity reversible data hiding scheme for encrypted images},
journal = {Journal of Visual Communication and Image Representation},
volume = {66},
pages = {102744},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102744},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303657},
author = {Bo Guan and Dawen Xu},
keywords = {Reversible data hiding, Image encryption, Image security, MSB prediction, High capacity},
abstract = {Reversible data hiding in encrypted images is an effective technique to embed information in encrypted domain, without knowing the original content of the image or the encryption key. In this paper, a high-capacity reversible data hiding scheme for encrypted images based on MSB (most significant bit) prediction is proposed. Since the prediction is not always accurate, it is necessary to identify the prediction error and store this information in the location map. The stream cipher is then used to encrypt the original image directly. During the data hiding phase, up to three MSBs of each available pixel in the encrypted image are substituted by the bits of the secret message. At the receiving end, the embedded data can be extracted without any errors and the original image can be perfectly reconstructed by utilizing MSB prediction. Experimental results show that the scheme can achieve higher embedding capacity than most related methods.}
}
@article{WANG2019102663,
title = {Benchmarking deep learning techniques for face recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102663},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102663},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302846},
author = {Qiangchang Wang and Guodong Guo},
keywords = {Deep learning, Convolutional neural networks, Face recognition, GPU, PyTorch, TensorFlow, Caffe, AlexNet, ArcFace, Center-loss, CosFace, DenseNet, GoogLeNet, Inception-v3, LightCNN, ResNet, SphereFace, VGG},
abstract = {Recent progresses in Convolutional Neural Networks (CNNs) and GPUs have greatly advanced the state-of-the-art performance for face recognition. However, training CNNs for face recognition is complex and time-consuming. Multiple factors need to be considered: deep learning frameworks, GPU platforms, deep network models, training datasets and test datasets. The deep models under different frameworks may perform differently. Based on this concern, we compare three deep learning frameworks and benchmark the performance of different CNN models on five GPU platforms. The scalability issue is also explored. Our findings can help researchers select appropriate face recognition models, deep learning frameworks, GPU platforms, and training datasets for their face recognition tasks.}
}
@article{RASOULIDANESH2020102733,
title = {A novel change-detection scheduler for a network of depth sensors},
journal = {Journal of Visual Communication and Image Representation},
volume = {66},
pages = {102733},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102733},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303542},
author = {Maryam S. Rasoulidanesh and Shahram Payandeh},
keywords = {Change detection, Depth sensor, Network sensor, Sensor network scheduler, Background subtraction, RGBD tracking},
abstract = {In many monitoring applications such as smart home and surveillance, deployment of multiple depth sensors increases monitoring area and offers better occlusion handling which is not sensitive to illumination condition in comparison with RGB sensors. However, multiple sensors also increase the volume of data associated with signal processing alongside the associated computational complexity and power consumption. In order to address these drawbacks, this paper proposes a novel change detection algorithm that can be used as a part of a sensor scheduler in a centralized (e.g. star) network configuration. Initially, each sensor in the network performs a unique single scan of the common environment in order to detect any incremental changes in the sensed depth signal. This initial change detection is then used as a basis for several follow-up tasks such as foreground segmentation, background detection, target detection, and tracking for monitoring tasks. Here, instead of processing a complete depth frame, we proposed to utilize a collection of 1D scans of the depth frames. A confidence function is defined that can be used to estimate the reliability of the detected changes in each sensor and to reduce any false positive events which can be triggered by the noise and outliers. Analysis of the proposed confidence function is carried out through performance analysis in the presence of sensor noise and other parameters which can affect the reliability of the sensed data of each sensor. Finally, a score function is defined based on the confidence of the detected parameters and sensor resolution in order to rank and match sensors with the associated objects to be tracked. It results in tracking target(s) by a sensor (or sensors) that offer a high tracking score. This approach offers many advantages such as decreasing the overall system power consumption by placing the sensors with a low confidence value on standby mode and reducing the overall computational overheads.}
}
@article{LIU2019102636,
title = {Progressive complex illumination image appearance transfer based on CNN},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102636},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102636},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302573},
author = {Shiguang Liu and Zhichao Song and Xiaoli Zhang and Ting Zhu},
keywords = {Appearance transfer, Complex illumination, Histogram reshaping, Color transfer},
abstract = {Image appearance transfer is the process of transferring the appearance features from the user-supplied reference image to the target image. Since traditional appearance transfer methods are based on low-level feature, it is difficult to obtain natural effect in the case of complex illumination between the target image and the reference image. This is mainly because that the appearance mapping relation with complex illumination is a highly nonlinear problem. Although traditional methods are good at dealing with linear transformations, they are less suitable for solving the highly nonlinear problems caused by such complex illumination. Moreover, convolutional neural network (CNN) can extract the hierarchical abstraction features at different levels of the network layer, so it can be conveniently used to realize progressive transfer. In this paper, we propose a novel method for progressive appearance transfer for images with complex illumination. Firstly, we convert the input images from the RGB color space to the HSV color space. The illumination transfer is carried out only in the illumination channel of the image, and for the other two channels, the color distribution of the reference image is transferred to the target image. Secondly, CNN is specially designed to extract the hierarchical feature maps. To achieve progressive transfer, the histogram reshaping method is carried out by using the hierarchical feature maps extracted from the CNN. The appearance transfer results are obtained after the illumination transfer and color transfer. To optimize the transfer results, we adopt the joint bilateral filter to smooth the noises. The experimental results show that our method can effectively solve the problem of progressive appearance transfer for images with complex illumination.}
}
@article{PENG2020102705,
title = {Research on image feature extraction and retrieval algorithms based on convolutional neural network},
journal = {Journal of Visual Communication and Image Representation},
volume = {69},
pages = {102705},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102705},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303268},
author = {Xushan Peng and Xiaoming Zhang and Yongping Li and Bangquan Liu},
keywords = {Image retrieval, In-depth learning, Feature extraction, Convolutional neural network},
abstract = {With the rapid development of mobile Internet and digital technology, people are more and more keen to share pictures on social networks, and online pictures have exploded. How to retrieve similar images from large-scale images has always been a hot issue in the field of image retrieval, and the selection of image features largely affects the performance of image retrieval. The Convolutional Neural Networks (CNN), which contains more hidden layers, has more complex network structure and stronger ability of feature learning and expression compared with traditional feature extraction methods. By analyzing the disadvantage that global CNN features cannot effectively describe local details when they act on image retrieval tasks, a strategy of aggregating low-level CNN feature maps to generate local features is proposed. The high-level features of CNN model pay more attention to semantic information, but the low-level features pay more attention to local details. Using the increasingly abstract characteristics of CNN model from low to high. This paper presents a probabilistic semantic retrieval algorithm, proposes a probabilistic semantic hash retrieval method based on CNN, and designs a new end-to-end supervised learning framework, which can simultaneously learn semantic features and hash features to achieve fast image retrieval. Using convolution network, the error rate is reduced to 14.41% in this test set. In three open image libraries, namely Oxford, Holidays and ImageNet, the performance of traditional SIFT-based retrieval algorithms and other CNN-based image retrieval algorithms in tasks are compared and analyzed. The experimental results show that the proposed algorithm is superior to other contrast algorithms in terms of comprehensive retrieval effect and retrieval time.}
}
@article{YANG2020102692,
title = {Unsupervised fisheye image correction through bidirectional loss with geometric prior},
journal = {Journal of Visual Communication and Image Representation},
volume = {66},
pages = {102692},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102692},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930313X},
author = {Shangrong Yang and Chunyu Lin and Kang Liao and Yao Zhao and Meiqin Liu},
keywords = {Unsupervised, Fisheye image correction, Bidirectional loss, Geometric prior, Inpainting, Deep learning},
abstract = {Neural network based methods for fisheye distortion correction are effective and increasingly popular, although training network require a high amount of labeled data. In this paper, we propose an unsupervised fisheye correction network to address the aforementioned issue. During the training process, the predicted parameters are employed to correct strong distortion that exists in the fisheye image and synthesize the corresponding distortion using the original distortion-free image. Thus, the network is constrained with bidirectional loss to obtain more accurate distortion parameters. We calculate the two losses at the image level as opposed to directly minimizing the difference between the predicted and ground truth of distortion parameters. Additionally, we leverage the geometric prior that the distortion distribution depends on the geometric regions of fisheye images and the straight line should be straight in the corrected images. The network focuses more on the geometric prior regions as opposed to equally perceiving the whole image without any attention mechanisms. To generate more appealing corrected results in visual appearance, we introduce a coarse-to-fine inpainting network to fill the hole regions caused by the irreversible mapping function using distortion parameters. Each module of the proposed network is differentiable, and thus the entire framework is completely end-to-end. When compared with the previous supervised methods, our method is more flexible and shows better practical applications for distortion rectification. The experiment results demonstrate that our proposed method outperforms state-of-the-art methods on the correction performance without any labeled distortion parameters.}
}
@article{WU2020102717,
title = {Polygonal approximation based on coarse-grained parallel genetic algorithm},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102717},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102717},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303384},
author = {Zhaobin Wu and Chunxia Zhao and Bin Liu},
keywords = {Polygonal approximation, Coarse-grained parallel genetic algorithms, Ensemble learning},
abstract = {This paper proposes to apply coarse-grained parallel genetic algorithm (CGPGA) to solve polygonal approximation problem. Chromosomes are used to represent digital curves and genes correspond to points of curves. This method divides the whole population into several subpopulations, each of which performs evolutionary process independently. After every migration interval number of generations, these subpopulations exchange their information with each other. Inspired by the designing theory of ensemble learning in machine learning, this paper further improves the basic CGPGA through adopting different but effective genetic algorithms, respectively, in different subpopulations. Both the diversity among different subpopulations and the accuracy in each individual subpopulation are ensured. Experimental results, based on four benchmark curves and four real image curves extracted from the lake maps, show that the basic CGPGA outperforms the used genetic algorithm, and further the improved CGPGA (ICGPGA) is more effective than the basic CGPGA, in terms of the quality of best solutions, the average solutions, and the variance of best solutions. Especially for those larger approximation problems, the ICGPGA is more remarkably superior to some representative genetic algorithms.}
}
@article{HU2019102652,
title = {Research on quality improvement method of deformation monitoring data based on InSAR},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102652},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102652},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302731},
author = {Zhengquan Hu and Bin Li and Yu Liu and Xiaowei Niu},
keywords = {INSAR, Surface deformation, Monitoring, GPU acceleration},
abstract = {In recent years, geological disasters caused by surface deformation frequently occur, which seriously threatens the safety of people's lives and property. Therefore, it is of great significance to strengthen the monitoring of surface deformation. With the continuous advancement of science and technology, traditional monitoring technology is difficult to meet the development requirements of modern society. As a new type of space-to-earth observation technology, INSAR technology has the advantages of high precision and real-time dynamic monitoring, and has been obtained in surface deformation monitoring widely used. This paper briefly analyzes the basic working principle of INSAR technology and its specific application in surface deformation monitoring. The algorithm parallelism of the ground-based SAR deformation monitoring process is analyzed, and the CPU + GPU heterogeneous platform is used to accelerate the implementation to improve the timeliness of deformation monitoring. BP imaging algorithm, interferogram generation, interferogram filtering and phase unwrapping algorithm are designed in parallel, and appropriate parallel granularity planning for multiple loops, adaptive division of optimal thread block size and use of shared memory to reduce duplicate data are adopted. Optimization strategies such as read time enable GPU acceleration processing. Compared with the implementation of CPU platform and CPU + GPU heterogeneous platform, the acceleration effect from tens to hundreds of times is accelerated, and the feasibility of GPU to improve the timeliness of deformation monitoring is verified.}
}
@article{DESOUSA2020102758,
title = {Automated standardization of images of Drosophila embryos},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102758},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102758},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300080},
author = {Daniela Justiniano {de Sousa} and Maira Arruda Cardoso and Paulo Mascarello Bisch and Francisco José Pereira Lopes and Bruno Augusto Nassif Travençolo},
keywords = {Embryo standardization, Anterior-posterior orientation, Dorsal-ventral orientation, Automatic embryo positioning, },
abstract = {Modeling expression patterns of Drosophila, in space and time, plays a critical role to understand the development of multicellular organisms. In confocal microscopy, to produce precise quantitative data it is frequently necessary to process and analyze large amounts of digital images. Automatic preprocessing is a crucial step in this scenario, essential to standardize significant features such as orientation, size, position, direction, lighting condition and texture of embryo images. Even though a lot of efforts have been made, a robust embryo standardization strategy is still needed. In this paper, we propose the method Embrystandar. It is designed to remove background artifacts and standardize the direction and orientation of a Drosophila embryo through a sequence of automatic operations. To test its potential for large-scale image processing, Embrystandar was applied in different databases. It showed to be robust and precise, reaching more than 90% success rate.}
}
@article{AN2019102650,
title = {Deep spectral feature pyramid in the frequency domain for long-term action recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102650},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102650},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302718},
author = {Gaoyun An and Zhenxing Zheng and Dapeng Wu and Wen Zhou},
keywords = {Action recognition, Deep learning, Spectral feature, Video classification},
abstract = {In this paper, we propose a novel Deep Spectral Feature Pyramid in the Frequency domain (DSFP) to share the merits of deep features and spectral approaches for long-term action recognition. More specifically, in the spatial domain, deep features of sparse sampled frames are extracted by Convolutional Neural Networks (CNNs) to cover long-term temporal structure. In the frequency domain, appearance features of sampled frames are partitioned recursively along the time dimension and spectral transform is applied to each partitioned feature respectively. All coefficients of partitioned features are then concatenated into a video-level feature to better model the spatio-temporal structure of actions in the form of a pyramid. So DSFP could model actions from both microcosmic and macroscopic aspects. Extensive experiments conducted on two challenging action benchmarks UCF101 and HMDB51 show that our proposed DSFP is effective for spatio-temporal representation of actions and achieves comparable performance with the state-of-the-arts.}
}
@article{MAHDI2019102662,
title = {An extensive evaluation of deep featuresof convolutional neural networks for saliency prediction of human visual attention},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102662},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102662},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302834},
author = {Ali Mahdi and Jun Qin},
keywords = {Convolutional neural networks, Feature maps, Human fixation prediction, Saliency map, Transfer learning},
abstract = {Based on transfer learning, feature maps of deep convolutional neural networks (DCNNs) have been used to predict human visual attention. In this paper, we conduct extensive comparisons to investigate effects of feature maps on the predictions of the human visual attention using a deep features based saliency model framework. The feature maps of seven pretrained DCNNs are investigated using classical and class activation maps approaches. The performances of various saliency implementations are evaluated over four datasets using three metrics. The results demonstrate that deep feature maps of the pretrained DCNNs can be used to create saliency maps for the prediction of human visual attention. The incorporation of multiple levels of blurred and multi-scale feature maps improves the extraction of salient regions. Moreover, DCNNs pretrained using the Places dataset provide more localized objects that can be beneficial to the top-down saliency maps.}
}
@article{VLASIC2020102731,
title = {Spline-like Chebyshev polynomial model for compressive imaging},
journal = {Journal of Visual Communication and Image Representation},
volume = {66},
pages = {102731},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102731},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303529},
author = {Tin Vlašić and Ivan Ralašić and Azra Tafro and Damir Seršić},
keywords = {Polynomial representation of image, Chebyshev moments, Runge phenomenon, Sparse modeling, Compressive sensing, 2D-imaging},
abstract = {This paper introduces a novel spline-like parametric model for an image representation obtained directly from compressive imaging (CI) measurements. As a representation basis we use Chebyshev polynomials. To avoid common problem of blocking artifacts in block-based reconstruction algorithms, a desired number of derivatives are equated on the block boundaries in a spline-like fashion. This introduces a new set of constraints that fits into CI setup. Unlike splines, the proposed system of equations is underdetermined to provide a necessary degree of freedom for achieving sparsity by solving an ℓ1 optimization problem. Recovered coefficients of the parametric model can be further used for image processing where operations can be elegantly defined and calculated. This offers a new framework for acquisition and processing of analog signals without converting them into samples. Experiments on real measurements show that our model achieves sparse representation without visible blocking artifacts from a reduced set of CI measurements.}
}
@article{ZHAO2020102743,
title = {Superpixels extracted via region fusion with boundary constraint},
journal = {Journal of Visual Communication and Image Representation},
volume = {66},
pages = {102743},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102743},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303645},
author = {Li Zhao and Zhihui Li and Chaoguang Men and Yongmei Liu},
keywords = {Superpixel, Initial segmentation, Edge closing, Gaussian belief propagation, Region fusion},
abstract = {In this paper, we present an accurate superpixel algorithm by region fusion with boundary constraint (RFBC). Superpixels with regular shape and high boundary adherence can be generated in weak boundary and complex texture regions through our algorithm. RFBC includes two steps which are initial segmentation and region fusion respectively. In initial segmentation, broken Canny edges are connected through edge closing algorithm. Subsequently, the closed Canny edges and SLIC superpixel edges are combined together to form the incipient superpixels. In region fusion, gray Gaussian distribution and adjacent relation are used as priori to compute the degree of similarity across incipient superpixels in GBP algorithm. For concreteness, the information of similarity is propagated between regions and the most similar regions are fused, which are accomplished alternatingly to preserve accurate boundaries. Extensive experiments on the Berkeley segmentation benchmark show that the proposed algorithm outperforms the most state-of-the-art algorithms.}
}
@article{MOHAMMADZADE2020102691,
title = {Sparseness embedding in bending of space and time; a case study on unsupervised 3D action recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {66},
pages = {102691},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102691},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303128},
author = {Hoda Mohammadzade and Mohsen Tabejamaat},
keywords = {Unsupervised action recognition, Time series analysis, Sparseness embedding, Human computer interaction},
abstract = {Human action recognition from skeletal data is one of the most popular topics in computer vision which has been widely studied in the literature, occasionally with some very promising results. However, being supervised, most of the existing methods suffer from two major drawbacks; (1) too much reliance on massive labeled data and (2) high sensitivity to outliers, which in turn hinder their applications in such real-world scenarios as recognizing long-term and complex movements. In this paper, we propose a novel unsupervised 3D action recognition method called Sparseness Embedding in which the spatiotemporal representation of action sequences is nonlinearly projected into an unwarped feature representation medium, where unlike the original curved space, one can easily apply the Euclidean metrics. Our strategy can simultaneously integrate the characteristics of nonlinearity, sparsity, and space curvature of sequences into a single objective function, leading to a more robust and highly compact representation of discriminative attributes without any need to label information. Moreover, we propose a joint learning strategy for dealing with the heterogeneity of the temporal and spatial characteristics of action sequences. A set of extensive experiments on six publicly available databases, including UTKinect, TST fall, UTD-MHAD, CMU, Berkeley MHAD, and NTU RGB+D demonstrates the superiority of our method compared with the state-of-the-art algorithms.}
}
@article{SUN2020102704,
title = {Application of fuzzy image restoration in criminal investigation},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102704},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102704},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303256},
author = {Shuo Sun},
keywords = {Fuzzy image restoration, Image degradation, Criminal investigation},
abstract = {The advancement of science and technology has a positive effect on the development of law disciplines. The development of algorithms and artificial intelligence also has a certain impact on judicial practice. Image restoration is a significant technique in image processing. It aims to objectively restore the content or quality of the original image from the degraded image. Image degradation is always generated in image transmission, such as distortion, blur. In modern video surveillance system, image restoration is significant for criminal investigation. However, image restoration based on conventional filter algorithms cannot achieve satisfactory performance. Thus, we first introduce the image restoration algorithms based on different degradation model. Then, we propose some applications of fuzzy image restoration in criminal investigation. We conduct experiments on both degraded images and videos and experimental results have shown the effectiveness of fuzzy image restoration applying to the criminal investigation.}
}
@article{YANG2020102728,
title = {Vision-based optimization of the generalized predictive active disturbance rejection controller},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102728},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102728},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303499},
author = {Yikun Yang and Shengjie Jiao and Jiabo Li},
keywords = {Mixing and spreading equipment for MOH material, Batching system, Active disturbance rejection control, Generalized prediction, Adaptive genetic algorithm},
abstract = {The batching system of the integrated mixing and spreading equipment for MOH material is a nonlinear system with large uncertainty. It is difficult for conventional control strategies to meet the requirements for system performance. This research combines generalized predictive control and active disturbance rejection technique to propose a new generalized predictive active disturbance rejection controller (GPADRC) used in the batching system of MOH material. For the nonlinearity and uncertainty of the batching system, the extended state observer in the active disturbance rejection technique is used for estimation and compensation. The batching system model is converted into an integrator form, based on which the use of generalized predictive control can greatly reduce the impact of nonlinear models and uncertainties on the controller. Aiming at the problem that the parameters of the proposed new controller are numerous and difficult to tune, the adaptive genetic algorithm is used to realize the automatic tuning of the parameters. The simulation experiment shows that the designed GPADRC can well adapt to the working conditions of the batching system and can meet the requirements for various control indicators. At the same time, the adaptive genetic algorithm can realize the rapid tuning of the controller parameters, which reduce the difficulty and time consumption of the tuning process, and improve the applicability and achievability of the designed controller.}
}
@article{CAO2019102635,
title = {Visual tracking via dynamic weighting with pyramid-redetection based Siamese networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102635},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102635},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302561},
author = {Yi Cao and Hongbing Ji and Wenbo Zhang and Fei Xue},
keywords = {Visual tracking, Siamese networks, Dynamic weighting, Residual structure, Convolutional neural networks, Pyramid-redetection},
abstract = {Siamese network based similarity-learning algorithm is currently a significant branch of visual tracking. However, most of existing deep Siamese networks depend much on the offline-trained knowledge and always assume the same importance for different prediction views. In this paper, we first introduce a dynamic weighting module in Siamese framework, which could make the offline-trained network adapt to the current circumstance well and weight predictive response maps discriminatively. The thought stems from the basis that different maps have different predictive preference, which should not be treated equally. Secondly, in order to focus more on the accurate preference, we then introduce the residual structure to form the residual dynamic weighting module. Thirdly, we construct a simple online pyramid-redetection module to avoid local search and also consider the global viewpoint. Extensive experiments on both short-term and long-term tracking demonstrate that the proposed tracker possesses the competitive tracking performance over many mainstream state-of-the-art trackers.}
}
@article{LIU2019102675,
title = {Learning full-reference quality-guided discriminative gradient cues for lane detection based on neural networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102675},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102675},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302962},
author = {Jingyi Liu},
keywords = {Lane detection, Full-reference IQA, CNN, RNN},
abstract = {Learning an intelligent lane detection system is significant to autonomous vehicles, which is a crucial module to smart cars. Although conventional approaches have achieved impressive performance, they suffer from the following limitations: (1) lane perception are confronted with different weather conditions and varied illumination. Existing methods lack a unified framework for characterizing different sceneries and (2) the inefficiency of utilizing images due to the potential label noise. To solve these limitations, we propose a lane detection framework towards autonomous vehicles by learning a full-reference quality-aware discriminative gradient deep model, where two types of deep networks are proposed. More specifically, we first design a gradient-guided deep convolutional network to detect the presence of lane, since the gradient value of lane edge is larger than that of other regions. We leverage full-reference image quality assessment (FR-IQA) method to discover more discriminative gradient cues, and geometric attributes are exploited simultaneously. Subsequently, a recurrent neural layer is designed to represent the spatial distribution of detected lanes whose visual cues are difficult to explicitly define. Noticeably, we only utilize a small proportion of the labeled images, whereas the noisy features are abandoned using sparsity penalty. Extensive experiments have demonstrated the effectiveness of our proposed method.}
}
@article{HOU2020102732,
title = {A novel dark channel prior guided variational framework for underwater image restoration},
journal = {Journal of Visual Communication and Image Representation},
volume = {66},
pages = {102732},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102732},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303530},
author = {Guojia Hou and Jingming Li and Guodong Wang and Huan Yang and Baoxiang Huang and Zhenkuan Pan},
keywords = {Underwater image restoration, Dehazing and denoising, UTV, ADMM, UDCP},
abstract = {Image captured underwater often suffers from low contrast, color distortion and noise problems, which is caused by absorbing and scattering before the light reaches the camera when traveling through water. Underwater image enhancement and restoration from a single image is known to be an ill-posed problem. To overcome these limitations, we establish an underwater total variation (UTV) model relying on underwater dark channel prior (UDCP), in which UDCP is used to estimate the transmission map. We design the data item and smooth item of the unified variational model based on the underwater image formation model. We further employ the alternating direction method of multipliers (ADMM) to accelerate the solving procedure. Numerical experiential results demonstrate that our underwater variational method obtains a good outcome on dehazing and denoising. Furthermore, compared with several other state-of-the-art algorithms, the proposed approach achieves better visual quality, which is illustrated by examples and statistics.}
}
@article{DU2020102716,
title = {A novel approach for space debris recognition based on the full information vectors of star points},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102716},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102716},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303372},
author = {Yun Du and Desheng Wen and Guizhong Liu and Shi Qiu and Dalei Yao and Hongwei Yi and Meiying Liu},
keywords = {Space debris recognition, Star image, Binary classifier, Equal probability density curve, Full information vector},
abstract = {The recognition and detection of space debris has become one of significant research fields recently. Compared with natural images, effective information are very few contained in star images. In the past years, the gray values of star points and the continuity of sequential star images are utilized by numerous algorithms to carry out the recognition and detection through fusion of consecutive star images, which have been achieved good performance. However, with the rapid increase of star image data, those algorithms seem to be inadequate in recognition ability. In this paper, we propose one novel approach based on the full information vectors of star points to recognize moving targets with the machine learning method which is never utilized in space debris recognition field. Besides gray values, we further deeply excavate the characteristics of each star point in a single frame by the equal probability density curve of Gaussian distribution. The elliptical pattern characteristic vectors of star points can be input into the machine learning method for classification of static stars and moving targets in a single frame. Finally, trajectories of moving targets can be determined within 3 frames by the full information vectors. Therefore, traditional processing methods are abandoned and the proposed brand new approach redefines the recognition technical route of space debris. The experimental results demonstrate that moving targets can be successfully recognized in a single frame and the coverage rate of moving targets can reach 100%. Compared with other traditional methods, the proposed approach has better performance and more robustness.}
}