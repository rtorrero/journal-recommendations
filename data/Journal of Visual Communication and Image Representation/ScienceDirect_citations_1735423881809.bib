@article{KE2023103922,
title = {Efficient online real-time video stabilization with a novel least squares formulation and parallel AC-RANSAC},
journal = {Journal of Visual Communication and Image Representation},
volume = {96},
pages = {103922},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103922},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001724},
author = {Jianwei Ke and Alex J Watras and Jae-Jun Kim and Hewei Liu and Hongrui Jiang and Yu Hen Hu},
keywords = {Online real-time video stabilization, Least squares smoothing, Parallel AC-RANSAC},
abstract = {A novel online real-time video stabilization algorithm (LSstab) that suppresses unwanted motion jitters based on cinematography principles is presented. LSstab features a parallel realization of the a-contrario RANSAC (AC-RANSAC) algorithm to estimate the inter-frame camera motion parameters. A novel least squares based smoothing cost function is then proposed to mitigate undesirable camera jitters according to cinematography principles. A recursive least square solver is derived to minimize the smoothing cost function with a linear computation complexity. LSstab is evaluated using a suite of publicly available videos against state-of-the-art video stabilization methods. Results show that LSstab achieves comparable or better performance, which attains real-time processing speed when a GPU is used.}
}
@article{ZHAO2023103946,
title = {MFS enhanced SAM: Achieving superior performance in bimodal few-shot segmentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103946},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103946},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001967},
author = {Ying Zhao and Kechen Song and Wenqi Cui and Hang Ren and Yunhui Yan},
keywords = {Segment anything, Few-shot segmentation, RGB-T SAM, Gated prediction selection},
abstract = {Recently, Segment Anything Model (SAM) has become popular in computer vision field because of its powerful image segmentation ability and high interactivity of various prompts, which opens a new era of large vision foundation models. But is SAM really omnipotent? In this letter, we establish a comprehensive bimodal few-shot segmentation indoor dataset VT-840-5i, and compare SAM with eight state-of-the-art few-shot segmentation (FSS) methods on two benchmark datasets. Qualitative and quantitative experiment results show that although SAM is very effective in general object segmentation, it still has room for improvement in some challenging scenarios. Therefore, we introduce thermal infrared auxiliary information into the segmentation task and provide multiple fusion strategies (MFS) for readers to choose the most suitable approach for the specific task. Finally, we discuss several potential research trends about SAM in the future. Our test results are available at: https://github.com/VDT-2048/Bi-SAM.}
}
@article{HUANG2023103910,
title = {Multi-stage affine motion estimation fast algorithm for versatile video coding using decision tree},
journal = {Journal of Visual Communication and Image Representation},
volume = {96},
pages = {103910},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103910},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001608},
author = {Xiaofeng Huang and Fangtao Zhou and Weihong Niu and Tianci Li and Yu Lu and Yang Zhou and Haibing Yin and Chenggang Yan},
keywords = {Affine motion estimation, Versatile video coding, Inter-mode decision, Decision tree, Fast algorithm},
abstract = {Affine motion estimation (AME) which is newly introduced in versatile video coding (VVC) plays a significant role in the bit-rate reduction for rotation and zooming scenes. However, it brings the complexity extremely increased. In this paper, a fast AME algorithm is designed to cope with the high-complexity problem. Three crucial stages are involved in the fast algorithm, which is the optimal inter-mode decision based on the coding unit (CU) partition, the fast algorithm in affine motion search (AMS), and the fast inter-mode decision based on a decision tree. Specifically, the optimal inter-mode will be determined straightforwardly when the parent CU is checked whether it is translational motion estimation (TME). Then, three fast algorithms are made in AMS, which are early termination based on control point motion vector (CPMV), early termination of the iteration process, and fast fine granularity CPMV search. Finally, the optimal inter-mode is predicted using a decision tree based on eight essential features. Experimental results show that the proposed algorithm achieves 10.20% and 10.33% encoding time-saving on average under Low Delay B (LDB) and Random Access (RA) configuration, while the BD-Rate loss is only 0.12% and 0.14%, respectively.}
}
@article{FAN2023103964,
title = {Multi-branch Segmentation-guided Attention Network for crowd counting},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103964},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103964},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002146},
author = {Zheyi Fan and Zihao Song and Di Wu and Yixuan Zhu},
keywords = {Crowd counting, Multi-task learning, Attention mechanism},
abstract = {Crowd counting has gained increasing attention recently owing to its importance in public safety. However, it remains a challenging task due to background complexities and high annotation costs. To address these challenges, we propose the Multi-branch Segmentation-guided Attention Network (MSGANet). To deal with the complex background, we incorporate segmentation-guided attention branches into both shallow and deep layers of the baseline model, allowing simultaneous consideration of spatial and semantic information. Multi-level attention maps enable the network to minimize the influence of complex backgrounds while focusing on the regions containing crowds. To tackle the issue of costly annotations, MSGANet utilizes only point annotations to generate ground truth density and segmentation maps, eliminating additional expenses. Our results demonstrate that our approach achieves highly competitive performance compared to state-of-the-art crowd counting methods.}
}
@article{RATHOD2023103988,
title = {A recent survey on perceived group sentiment analysis},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103988},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103988},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002389},
author = {Bhoomika Rathod and Rakeshkumar Vanzara and Devang Pandya},
keywords = {Group emotion recognition, Machine learning, Deep learning, Fusion methods, Sentiment analysis},
abstract = {Group sentiment analysis in today’s era is influencing many applications. Sentiment recognition is applicable for individuals, and group as well as crowd. This paper aims to highlight the ongoing present research scenario on perceived group sentiment analysis. The paper provides a brief overview of the strengths and weaknesses of well-known group sentiment analysis approaches. These approaches are broadly split into two basic categories: top-down and bottom-up approaches. A combination of top-down and bottom-up approaches is found superior in performance. This comprehensive review also discusses the available dataset with taxonomic representation and the importance of fusion techniques in group emotion recognition. This paper also narrates the novel attempt to demarcate the group emotion recognition approaches into the categories evolved by the influence of machine learning and deep learning at the feature extraction level and at the classification level. The multi-modality based group sentiment analysis has a great scope to overcome the challenges and improve the performance on real time applications.}
}
@article{LUO2023103935,
title = {Reversible adversarial steganography for security enhancement},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103935},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103935},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001852},
author = {Jie Luo and Peisong He and Jiayong Liu and Hongxia Wang and Chunwang Wu and Shenglie Zhou},
keywords = {Steganography, Reversible adversarial attack, Universal stego post-processing, Deep learning-based stganalysis},
abstract = {In adaptive steganography, existing universal stego post-processing methods can enhance security, but suffer from incorrect extraction of messages and undesired visual defects, especially in flat regions. To address this issue, a reversible adversarial steganography method is proposed by modifying the LSB or 2ndLSB of stego-images, which has promising visual quality and security. To that end, the content-adaptive adversarial perturbations are first generated, which consider image texture with noise residual features to counter deep learning-based steganalyzers. Then, a data compression strategy of adversarial perturbations is designed by applying lossless run-length encoding based on the sparse nature of non-zero elements in the perturbations to reduce the perturbation’s quantity. Finally, reversible data hiding based on ternary coding is applied to embed and extract stego images with compressed adversarial perturbations. Extensive experimental results demonstrate that the proposed method can effectively enhance security and visual quality compared with state-of-the-art methods.}
}
@article{BUADES2023103883,
title = {HDR video synthesis by a nonlocal regularization variational model},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103883},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103883},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001335},
author = {Antoni Buades and Onofre Martorell and Ivan Pereira-Sánchez},
keywords = {HDR video synthesis, Variational methods, Nonlocal regularization},
abstract = {High dynamic range (HDR) video synthesis is a very challenging task. Consecutive frames are acquired with alternate expositions, generally two or three different exposure times. Classical methods aim at registering neighboring frames and fuse them using image HDR techniques. However, the registration often fails to obtain accurate results and the fusion produces ghosting artifacts. Deep learning techniques have recently appeared imitating the structure of existing classical methods. The neural network is intended to estimate the registration function and choose the fusion weights. In this paper, we propose a new method for HDR video synthesis using a variational model. The proposed model uses a nonlocal regularization term to combine pixel information from neighboring frames. The obtained results are competitive with state-of-the-art. Moreover, the proposed method gives a more reliable and understandable solution than deep-learning based ones.}
}
@article{TANG2023103980,
title = {ClothSeg: semantic segmentation network with feature projection for clothing parsing},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103980},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103980},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002304},
author = {Guangyu Tang and Feng Yu and Huiyin Li and Yankang Shi and Li Liu and Tao Peng and Xinrong Hu and Minghua Jiang},
keywords = {Clothing semantic segmentation, Transformer, Feature projection fusion, Pixel distance loss},
abstract = {Semantic segmentation of clothing presents a formidable challenge owing to the non-rigid geometric deformation properties inherent in garments. In this paper, we use the Transformer as the encoder to better learn global information for clothing semantic segmentation. In addition, we propose a Feature Projection Fusion (FPF) module to better utilize local information. This module facilitates the integration of deep feature maps with shallow local details, thereby enabling the network to capture both high-level abstractions and fine-grained details of features. We also design a pixel distance loss in training to emphasize the impact of edge features. This loss calculates the mean of the shortest distances between all predicted clothing edges and the true clothing edges during the training process. We perform extensive experiments and our method achieves 56.30% and 74.97% mIoU on the public dataset CFPD and our self-made dataset LIC, respectively, demonstrating a competitive performance when compared to the state-of-the-art.}
}
@article{CR2023103907,
title = {Multi exposure image fusion based on exposure correction and input refinement using limited low dynamic range images},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103907},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103907},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001578},
author = {Jishnu C.R. and Vishnukumar S.},
keywords = {Exposure value, Multi-exposure image fusion, Intermediate Exposure Stack, Input refinement, Recursive filter, Fast-guided filter},
abstract = {Multi-Exposure Image Fusion (MEF) is an image processing technique that combines several images taken at various exposure levels into one high-quality image. The generation of artifacts and diminished perceptual quality of the fused image under few input images is an inevitable problem in MEF approaches. Although numerous MEF techniques have been proposed in the literature, little effort has been devoted to generate high quality fused images with minimal input images. The proposed work unveils a method for producing a fused image that exhibits superior visual quality with a minimum of two input images. The proposed work begins with refining the input images using an exposure correction strategy and a recursive filter. The refined images along with a selected exposure corrected image form an intermediate exposure stack. A weight map is constructed from the intermediate exposure stack using a set of weighting terms and a fast-guided filter is used to prevent the weight maps from being distorted by external anomalies. Finally, the fusion is performed using the pyramidal decomposition of weight maps and intermediate images. The resultant fused images obtained have been tested and proven to hold superior performance over the state-of-the-art methods in perceptual and empirical analysis. Benefiting from this approach is a framework that seeks fewer input images, an intermediate exposure stack, and a detail-enriched fused image.}
}
@article{ZOUARI2023103894,
title = {Dictionary-based histogram packing technique for lossless image compression},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103894},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103894},
url = {https://www.sciencedirect.com/science/article/pii/S104732032300144X},
author = {Sonia Zouari and Atef Masmoudi},
keywords = {Lossless image compression, Sparse histogram, Histogram packing, histogram sparseness, JPEG-2000, JPEG-LS, JPEG-XL},
abstract = {This paper proposes a dictionary-based histogram packing technique for lossless image compression. It is used to improve the performance of the state-of-the-art lossless image compression standards and methods when compressing sparse and locally sparse histogram images. The proposed method leverages inter-block correlations and similarities not only within the neighborhood but also across the entire image, thereby effectively reducing the block boundary artifacts commonly observed in block-based histogram packing techniques. To achieve this, a dictionary is employed to represent highly correlated blocks using a key that captures the union of their active symbol sets. Experimental results have demonstrated that the proposed method, when applied to sparse and locally sparse histogram images, enhances the performance of various state-of-the-art lossless image compression techniques. Notably, improvements were observed in standards and methods such as JPEG-2000, JPEG-LS, JPEG-XL, PNG, and CALIC.}
}
@article{LIU2023103879,
title = {Accumulated micro-motion representations for lightweight online action detection in real-time},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103879},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103879},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001293},
author = {Yu Liu and Fan Yang and Dominique Ginhac},
keywords = {Motion representation, Spatiotemporal action localization, Online action detection, Real-time computing, Embedded system},
abstract = {In the last decade, the explosive growth of vision sensors and video content has driven numerous application demands for automating human action detection in space and time. Aside from reliable precision, vast real-world scenarios also mandate continuous and instantaneous processing of actions under limited computational budgets. However, existing studies often rely on heavy operations such as 3D convolution and fine-grained optical flow, therefore are hindered in practical deployment. Aiming strictly at a better mixture of detection accuracy, speed, and complexity for online detection, we customize a cost-effective 2D-CNN-based tubelet detection framework coined Accumulated Micro-Motion Action detector (AMMA). It sparsely extracts and fuses visual-dynamic cues of actions spanning a longer temporal window. To lift reliance on expensive optical flow estimation, AMMA efficiently encodes actions’ short-term dynamics as accumulated micro-motion from RGB frames on-the-fly. On top of AMMA’s motion-aware 2D backbone, we adopt an anchor-free detector to cooperatively model action instances as moving points in the time span. The proposed action detector achieves highly competitive accuracy as state-of-the-arts while substantially reducing model size, computational cost, and processing time (6 million parameters, 1 GMACs, and 100 FPS respectively), making it much more appealing under stringent speed and computational constraints. Codes are available on https://github.com/alphadadajuju/AMMA.}
}
@article{BABU2023103976,
title = {Development and performance evaluation of enhanced image dehazing method using deep learning networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103976},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103976},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002262},
author = {G. Harish Babu and Venkata Krishna Odugu and N. Venkatram and B. Satish and K. Revathi and B. Janardhana Rao},
keywords = {Image dehazing, Neural networks, CNN, Deep learning, PSNR, Dehazing network and discriminator network},
abstract = {In this work, a deep learning-based high-performance image dehazing technique is proposed for image processing applications. The end-to-end network model is constructed and implemented using a dehazing network, discriminator network, and fine-tuning network models. These three methods are well-trained individually using appropriate datasets. The individual network models are integrated as an end-to-end network model to enhance the dehazing process. The applied input hazy image is processed by the dehazing network model using the estimation of transmission map and atmospheric light along with parallel convolution layers. A discriminated dehazing image was extracted from the discrimination network. Finally, fine-tuning is carried out based on the results of the discriminator network model. Various hazy images from different datasets are collected and applied to the proposed model, and performance metrics such as PSNR, SSIM, and MSE are evaluated. Qualitative, and quantitative comparison and analysis are carried out between the proposed learning-based image dehazing and existing dehazing methods. The average PSNR value of the proposed dehazing model is obtained by a maximum of 40.7 %, and a minimum of 1.34 %, when compared to the existing works. The average SSIM of the proposed work is increased by a maximum of 22.12 %, and a minimum of 3.38 % with respect to the existing works. The maximum average value of MSE for the proposed model is decreased by 72.6 % and the minimum decrease of MSE is 4.08 % when compared to the state-of-art-works.}
}
@article{SONG2023103895,
title = {Hierarchical deep semantic alignment for cross-domain 3D model retrieval},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103895},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103895},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001451},
author = {Dan Song and Yuting Ling and Tianbao Li and Teng Wang and Xuanya Li},
keywords = {3D model retrieval, Unsupervised domain adaptation, Representation learning},
abstract = {With the development of deep learning and the widespread application of 3D modeling technology, image-based cross-domain 3D model retrieval has attracted more and more researchers’ attention. Existing methods have achieved success by aligning the feature distributions from different domains. However, previous methods just statistically align the domain-level or class-level feature distributions, leaving sample discriminability a margin to be improved for retrieval. To address this issue, this paper proposes a Hierarchical Deep Semantic Alignment Network (HDSAN) for cross-domain 3D model retrieval, which combines the proposed sample-level semantic enhancement with global domain alignment and class semantic alignment. Concretely, we adopt adversarial domain adaptation at the domain level and dynamically align the class centers of two domains at the class level. To further improve sample discriminability, we design intra-domain and cross-domain triplet center alignment to enhance the semantic representation ability at the sample level. Experiments on two commonly-used cross-domain 3D model retrieval datasets MI3DOR-1 and MI3DOR-2 demonstrate the effectiveness of the proposed method.}
}
@article{CHEN2023103923,
title = {Improving semantic segmentation with knowledge reasoning network},
journal = {Journal of Visual Communication and Image Representation},
volume = {96},
pages = {103923},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103923},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001736},
author = {Shengjia Chen and Xiwei Yang and Zhixin Li},
keywords = {Semantic segmentation, Knowledge reasoning, Graph convolutional network, External knowledge, Context information},
abstract = {Most methods cannot segment the semantic regions accurately due to the lack of global-level supervision or guidance of external knowledge. To overcome this limitation, we propose a Knowledge Reasoning Network (KRNet) that consists of two crucial modules: (1) a prior knowledge mapping module that incorporates external knowledge by graph convolutional network to guide learning semantic representations and (2) a knowledge reasoning module that correlates these representations with a graph built on the external knowledge and explores their interactions via the knowledge reasoning. In the prior knowledge mapping module, we adopt an algorithm to mine knowledge from an external large-scale relational modeling dataset. In the knowledge reasoning module, we adopt an iterative mechanism to perform knowledge reasoning and explore the interaction between features. Reasoning makes the spatial distribution of categories more significant. We establish state-of-the-art results on Cityscapes and ADE20K datasets, which demonstrates the effectiveness of our methods on semantic segmentation.}
}
@article{CHEN2023103871,
title = {3D pedestrian localization fusing via monocular camera},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103871},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103871},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001219},
author = {Hao Chen and Jiande Sun and Shanxin Zhang and Hui Yuan and Huaxiang Zhang and Jia Zhang},
keywords = {Monocular camera, Pedestrian localization, Pseudo-LiDAR},
abstract = {In the field of intelligent transportation, autonomous driving technologies, especially visual sensing solutions have attracted increasing attention in recent years. There are still some challenges in pedestrian location based on the monocular camera, as the pedestrian is a non-rigid object and its depth information cannot be obtained from the monocular camera easily and accurately. In this paper, a pedestrian location framework based on monocular cameras is proposed. The framework consists of three parts: coarse positioning, auxiliary information generation and information fusion. In the part of coarse positioning, the human skeleton information is obtained from the monocular images and a light-weight feed-forward neural network is used to predict the pedestrian position based on the skeleton information. In the part of auxiliary information generation, pseudo-LiDAR points with pedestrian depth information are generated from the monocular images through an auxiliary network. Finally, the outputs of the above two parts are fused to achieve the pedestrian location. The experimental results on KITTI dataset show that our method has achieved better performance than other methods.}
}
@article{PENG2023103947,
title = {DDFusion: An efficient multi-exposure fusion network with dense pyramidal convolution and de-correlation fusion},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103947},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103947},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001979},
author = {Pai Peng and Yang Liu and Zhongliang Jing and Han Pan and Hao Zhang},
keywords = {Multi-exposure fusion, Dense-pyramidal convolution, De-correlation convolution},
abstract = {In this work, we propose DDFusion, a novel multi-exposure image fusion network. DDFusion addresses the limitations of existing methods by effectively recovering details near extremely bright regions and learning associations between non-contiguous regions. To achieve this, our network incorporates a dense pyramidal (DensePy) convolution block in the encoder for multi-scale feature extraction, and a de-correlation fusion (DF) block for enabling structurally coherent and edge-preserving multi-scale feature fusion. It facilitates a smoother transition from highlighted areas to adjacent regions in the fused image. Experimental results demonstrate the superiority of DDFusion over state-of-the-art deep methods in terms of both visual quality and quantitative evaluation. Moreover, DDFusion achieves stronger multi-scale feature extraction capability with smaller computational complexity.}
}
@article{REN2023103993,
title = {Constructing comprehensive and discriminative representations with diverse attention for occluded person re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103993},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103993},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002432},
author = {Tengfei Ren and Qiusheng Lian and Dan Zhang},
keywords = {Person re-identification, Representation learning, Diverse attention, Transformer},
abstract = {Occluded person re-identification (Re-ID) is a challenging task that aims to match occluded person images to holistic ones across different camera views. Feature diversity is crucial for achieving high-performance Re-ID. Previous methods rely on additional annotations or hand-crafted rules to achieve feature diversity, which are inefficient or infeasible for occluded Re-ID. To address this, we propose the Diverse Attention Net (DANet) which utilizes attention mechanism to achieve diverse feature mining. Specifically, DANet incorporates a pair of complementary Diverse Parallel Attention Modules (DPAM), which, under the attention decorrelation constraint (ADC), help the model automatically capture diverse discriminative features in a global scope. Additionally, we propose an Efficient Transformer layer that can seamlessly integrate with the proposed DPAM and synergistically enhance the capability to handle occlusions. The resulting DANet construct a set of comprehensive representations that encode diverse discriminative features. Extensive experiments demonstrate DANet achieves state-of-the-art performance on both occluded and holistic ReID benchmarks.}
}
@article{SHERLYALPHONSE2023103948,
title = {Alibaba and forty thieves algorithm and novel Prioritized Prewitt Pattern(PPP)-based convolutional neural network (CNN) using hyperspherically compressed weights for facial emotion recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103948},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103948},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001980},
author = {A. {Sherly Alphonse} and S. Abinaya and S. Abirami},
keywords = {Optimization, CNN, Weights, Emotion, Hyperparameters},
abstract = {The visual representations created using the self-distillation paradigm of Bootstrap Your Emotion Latent (BYEL) are empirically found to be less evenly distributed than those created using proposed technique. This proposed work promotes the compression of weights on a hypersphere by minimizing the hyperspherical energy of network weights using a novel method of optimizing manifolds through Riemannian metrics and the Conjugate gradient technique. The proposed work demonstrates how regularising the networks of the BYEL architecture reduces the hyperspherical energy of neurons by directly optimising a measure of uniformity alongside the standard loss. This leads to more uniformly distributed representation and better performance for downstream tasks. The Alibaba and Forty Thieves Algorithm-based Optimization (AFTAO) methodology is used to select the most precise collection of hyperparameters for a novel Prioritized Prewitt Pattern (PPP)-based Convolutional Neural Network (CNN) that results in a higher accuracy for all the six datasets used for facial emotion recognition.}
}
@article{YANG2023103896,
title = {DB-TASNet for disease diagnosis and lesion segmentation in medical images},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103896},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103896},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001463},
author = {Yuqing Yang and Ping He and Shengrui Wang and Yu Tian and Wei Zhang},
keywords = {Disease diagnosis, Lesion segmentation, Transformer, U-Net, DB-TASNet},
abstract = {Deep learning algorithms have been successfully used in the field of medical image analysis and have greatly improved application of intelligent algorithms to medical diagnosis. However, existing deep-learning-based diagnostic methods still suffer from several drawbacks: (1) In most medical image multi-tasking methods, focus segmentation and disease classification are often performed linearly, resulting in excessive reliance on the final results of focus segmentation. (2) The computational cost of the traditional attention mechanism for performing the segmentation task is very high and the convolutional architecture cannot be used to model long-distance dependencies, which in turn affects the segmentation accuracy. To address these issues, we propose a disease diagnosis and lesion segmentation model, Dual-Branch with Transformer Axial-attention Segmentation Net (DB-TASNet). DB-TASNet is built by the DenseNet-121 classification network and U-Net segmentation network improved using an axial-attention transformer model. Moreover, DB-TASNet also includes a lesion integration module to integrate segmentation results with the classification network in order to increase its attention to lesions and improve the diagnosis results. Experimental results on the Pneumothorax dataset provided by the Society for Imaging Informatics in Medicine (SIIM) show that the average AUC of the DB-TASNet classification task reaches 0.939, and the DICE coefficient of the segmentation task reaches 0.886. Such performance suggests that the proposed model may provide an efficient and effective diagnosis tool for medical personnel.}
}
@article{SHI2023103965,
title = {CPA-YOLOv7: Contextual and pyramid attention-based improvement of YOLOv7 for drones scene target detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103965},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103965},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002158},
author = {Houwang Shi and Wenzhong Yang and Danni Chen and Min Wang},
keywords = {Deep learning, Small target detection, Multi-scale feature fusion, Attention mechanism, Unmanned aerial vehicle view small object, Loss function},
abstract = {Target detection in unmanned aerial vehicle application scenarios has other problems, such as dense targets. The existing unmanned aerial vehicle target detection model with high computational complexity makes it difficult to meet real-time unmanned aerial vehicle target detection, and the detection accuracy of small targets is low. To address these problems, we propose an improved YOLOv7 small target detection model based on context and pyramidal attention that can cope with dense unmanned aerial vehicle scenarios - CPA-YOLOv7. This model embeds our proposed lightweight multi-scale attentional feature spatial pyramid pooling module, which can better distinguish between small and large target features, reducing the computational effort while improving the detection accuracy of the model. Secondly, we design a contextual dynamic fusion attention module in the network to fuse global and local contextual information and dynamically assign features to multiple groups of channels; in the multi-scale fusion process, it effectively increases the characterization ability of small target features and enables the network to better focus on small target information. Finally, we improve Wise-Intersection-over-Union loss as the regression loss function, add a moderation factor to retain some of the high and low-quality sample weights to improve the regression accuracy of high-quality anchor frames, and use the dynamic non-monotonic focusing mechanism to increase the model's focus on ordinary quality anchor frames to improve the model's localization performance and robustness to low-quality samples. Numerous experimental results show that on the unmanned aerial vehicle datasets VisDrone2021-DET and AI-TOD, the mAP values of our model are 2.3% and 1.1% higher than those of the YOLOv7 model with fewer parameters introduced, and the computational speed reaches 146 frames per second (FPS), which can meet the real-time requirements of unmanned aerial vehicle detection.}
}
@article{ZHANG2023103952,
title = {V-shaped neural network structure based on multi-scale features for image denoising},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103952},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103952},
url = {https://www.sciencedirect.com/science/article/pii/S104732032300202X},
author = {Jing Zhang and Liu Sang and Zhicheng Zhang and Minhao Shao and Yunsong Li},
keywords = {Image denoising, Fixed-scale, Multi-scale, Sampling, V-shaped subnetwork},
abstract = {Due to the good performance of deep learning, more and more image denoising methods incorporating deep learning are implemented, including fixed-scale-based methods and multi-scale-based methods. It is easy for fixed-scale-based neural networks to achieve a better denoising performance when their depth is increased, whereas feature extraction of multiple scales from images can be obtained using multi-scale-based neural networks. In this work, a multi-scale image denoising method has been proposed which is mainly based on fixed-scale but combines the method of obtaining feature information from shallow image sampling. We propose a diamond-shaped module and a V-shaped subnetwork for extracting the features of images obtained from shallow sampling and improve the up-sampling and down-sampling units to obtain a better sampling effect. Densely connecting images of same size in MSDN can solve the problem of gradient vanishing since increasing the depth of the network and shallow image sampling prevent the loss of image details due to excessive sampling. Experiments show that our method can efficiently retain the image details during image denoising, and this method is especially helpful for preserving minute image details as compared to the other state-of-the-art denoising algorithms.}
}
@article{DING2023103912,
title = {Screen content video quality assessment based on spatiotemporal sparse feature},
journal = {Journal of Visual Communication and Image Representation},
volume = {96},
pages = {103912},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103912},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001621},
author = {Rui Ding and Huanqiang Zeng and Hao Wen and Hailiang Huang and Shan Cheng and Junhui Hou},
keywords = {Screen content video, Spatiotemporal sparse feature, Three dimensional Difference of Gaussian, Dictionary learning},
abstract = {In recent years, the explosive growth of application scenarios have occurred in screen content videos (SCVs), in which the SCVs are unavoidably suffered from the quality degradation and the video quality assessment (VQA) of the SCVs becomes very essential. In view of this, a full-reference VQA algorithm, called the spatiotemporal sparse feature-based model (SSFM) is proposed in this paper, aiming to give a precise and efficient quality evaluation about the distorted SCVs. Note that the SCVs are full of edge information which the human eyes are highly sensitive to, and the sparse coding can provide accurate quantitative predictions which are consistent with the perception resulted from the cerebral cortex in various receptive field models of the visual cortex. With these considerations, three dimensional Difference of Gaussian (3D-DOG) filter and 3D Sparse Dictionary are developed to extract multi-scale spatiotemporal features and obtain spatiotemporal sparse features respectively, from the reference and distorted SCVs. Based on these features, the spatiotemporal sparse feature similarity can be measured and followed by generating the quality scores of the distorted SCVs under evaluation. Compared to other classic and state-of-the-art image/video quality evaluation metrics, the experimental results of the proposed SSFM on the screen content video database (SCVD) are more consistent with the perceived evaluation of SCVs according to the human visual system (HVS).}
}
@article{CAO2023103936,
title = {UAV small target detection algorithm based on an improved YOLOv5s model},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103936},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103936},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001864},
author = {Shihai Cao and Ting Wang and Tao Li and Zehui Mao},
keywords = {UAV, Small target detection, Deep learning, Attention mechanism, YOLOv5},
abstract = {The targets of UAV target detection are usually small targets, and the backgrounds are complex. In this work, aiming at the problem that small targets are easy to be missed or misdetected during the UAV detection, an improved YOLOv5s_MSES target detection algorithm based on YOLOv5s is proposed. First of all, to solve the problem of UAV’s difficulty in detecting small targets, the detection layer is ameliorated into the small target detection layer STD, which makes the model more easily detect the small targets. Then, the multi-scale feature fusion module is added to improve the detection accuracy of the small targets. Furthermore, by combining multi-scale module and attention module, a new connection method is proposed to retain the large scale of feature information. Finally, in contrast with some existent methods, the experimental results of VisDrone2019 UAV target detection dataset show that our proposed YOLOv5s_MSES can achieve the better detection effect, and more effectively complete the small target detection task for UAV aerial photography images.}
}
@article{LIU2023103884,
title = {Age estimation by extracting hierarchical age-related features},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103884},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103884},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001347},
author = {Na Liu and Fan Zhang and Fuqing Duan},
keywords = {Age estimation, Convolutional neural network (CNN), Deep learning, Global and local features, Multi-task learning},
abstract = {Image-based facial age estimation is considered an intractable problem because aging characteristics are hard to obtain. Most previous works have focused on extracting age-related features, but rarely explored which local region plays an important role. Several works combine local face regions with global face to estimate age in a heuristic way, where the local regions are uniformly cropped for each individual. In this paper, we design an individual adaptive segmentation of local regions of interest to perform personalized local features extraction and build hierarchical age features by erasing the local regions of interest iteratively for each individual. A joint multi-input and multi-output (MIMO) network for multi-task learning of age classification and regression tasks is designed by combining global features and personalized local features as inputs. In addition, we conduct extensive experiments to validate the effectiveness of the proposed method for age estimation, which beats most state-of-the-art methods in three public datasets and also works well for gender and race estimation.}
}
@article{LIU2023103989,
title = {MAPoseNet: Animal pose estimation network via multi-scale convolutional attention},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103989},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103989},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002390},
author = {Sicong Liu and Qingcheng Fan and Shuqin Li and Chunjiang Zhao},
keywords = {Animal pose estimation, Attention mechanism, Asymmetric convolution, Feature pyramid},
abstract = {Animal pose estimation serves as an upstream task for recognizing and understanding animal behavior. Over the last year, the accuracy of the deep learning-based method has steadily improved, but at the expense of the model’s inference speed. This paper uses an efficient and powerful model to improve inference speed and accuracy. The classic encoder–decoder architecture is chosen. For estimating animal pose, our model based on a feature pyramid and a multi-scale asymmetric convolution attention mechanism is developed and named MAPoseNet (Animal Pose Estimation Network Via Multi-scale Convolutional Attention). MAPoseNet consists of an encoder and a decoder. Rather than typical self-attention, the encoder’s attention mechanism comprises multi-scale, asymmetric convolutions that are lightweight and instrumental in improving inference speed. A feature pyramid and a feature balance module make up the decoder. The public dataset AP-10K is used to train and test MAPoseNet. A series of experimental results demonstrate that the MAPoseNet model provides cutting-edge performance. MAPoseNet outperforms HRFormer by 1.3 AP and 0.8 AR, with 33.7% fewer FLOPs and 66% faster inference speed. And our model surpasses HRNet and HRFormer on the Animal Pose dataset as well. Our model has achieved a win-win situation regarding inference speed and accuracy.}
}
@article{QI2023103953,
title = {Comprehensive receptive field adaptive graph convolutional networks for action recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103953},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103953},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002031},
author = {Hantao Qi and Xin Guo and Hualei Xin and Songyang Li and Enqing Chen},
keywords = {Graph convolutional network, Receptive field, Temporal covariance pooling, Attention},
abstract = {Action recognition has shown a broad prospect for application in human–computer interaction and autonomous vehicles. According to skeleton data being lightweight and robust to environmental disturbance, skeleton-based methods are increasingly used in action recognition. Existing Graph Convolutional Networks (GCNs) have limitations such as Global Average Pooling (GAP) causing information oversmoothing, attention mechanism increasing computational complexity, and fixed spatial configuration partition strategy limiting flexibility. Our model overcomes these limitations by incorporating Temporal Covariance Pooling (TCP) for better feature utilization, Sample-Shared attention for weight sharing, and a Comprehensive Receptive Field strategy for optimal joint connections. Experiments show that the recognition accuracy of our model outperforms baseline 2s-AGCN by 1.1 % and 0.7 % under X-Sub and X-View for NTU RGB + D dataset, 1.8 % and 1.9 % in X-Sub and X-Set for NTU RGB + D 120 dataset, and 1.6 % on Kinetics-Skeleton dataset.}
}
@article{CHEN2023103977,
title = {Toward high imperceptibility deep JPEG steganography based on sparse adversarial attack},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103977},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103977},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002274},
author = {Beijing Chen and Yuxin Nie and Jianhua Yang},
keywords = {JPEG, Steganography, Adversarial attack, Deep neural network, Steganalysis},
abstract = {JPEG steganography is an important branch of information hiding. However, with the development of deep learning-based steganalysis models, steganography is facing great challenges. To better resist these steganalysis models, this paper proposes a deep JPEG steganography framework based on sparse adversarial attacks. According to the vulnerability of deep steganalysis models to adversarial attacks, sparse adversarial attacks are introduced into the deep JPEG steganographic structure to improve the security of stego images. In addition, to resist steganalysis from JPEG and spatial domains, both JPEG and spatial domain steganalysis models are involved in adversarial training. Finally, to further enhance the imperceptibility of adversarial stego images, the visual perception loss is designed from the perspective of human eyes. Experimental results indicate that compared with the existing methods, the proposed method has higher imperceptibility and security and can resist modern deep steganalysis models in both JPEG and spatial domains to a certain extent. The source code is available at https://github.com/imagecbj/SAE-JS.}
}
@article{LI2023103958,
title = {Towards object tracking for quadruped robots},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103958},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103958},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002080},
author = {Yang Li and Kao Zhang and Zhao Chen and Wanping Ouyang and Mingpeng Cui and Chenxi Jiang and Daiqin Yang and Zhenzhong Chen},
keywords = {Object tracking, Siamese network, Quadruped robot},
abstract = {With the development of quadruped robot technology, object tracking for quadruped robots has become an important research topic where violent camera shaking caused by the robot’s movement makes this task very challenging. In this letter, a quadruped robot object tracking dataset (QROD-111), including 111 video sequences, is first established, which was collected through our quadruped robot platform. A tracking algorithm based on Siamese network is then proposed where an alignment module is introduced to alleviate tracking difficulties caused by the quadruped robot movement. Moreover, a scale adaptation subnetwork is designed to alleviate the impact of the object scale variation during the whole tracking process. Experimental results demonstrate that our algorithm can achieve advanced performance for quadruped robot object tracking.}
}
@article{HAN2023103932,
title = {Low-light images enhancement and denoising network based on unsupervised learning multi-stream feature modeling},
journal = {Journal of Visual Communication and Image Representation},
volume = {96},
pages = {103932},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103932},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001827},
author = {Guang Han and Yingfan Wang and Jixin Liu and Fanyu Zeng},
keywords = {Low-light enhancement, Generative adversarial network, Multi-stream modeling, Multi-scale feature fusion},
abstract = {Inspired by the generative adversarial networks EnlightenGAN, this paper proposes a novel low-light images Enhancement and Denoising model based on unsupervised learning Multi-Stream feature modeling (MSED). The model has two stages: generator network and discriminator network. Generator network includes global and local feature modeling network. First, Swin Transformer Block is innovatively introduced in the global feature modeling of generator network stage. It makes the interaction between the image and the convolutional kernel related to the image content. Its shift window mechanism can model the global feature dependency of the input image with less memory consumption, and extract the color, texture, and shape features of the image, thereby effectively suppressing noise and artifacts. Second, in the local feature modeling, a multi-scale image and feature fusion branch is added. It not only extracts reduced features from large-scale low-light images, but also extracts features from multiple downsampled low-light images, and then combines the two through attention and DSFF. By utilizing the complementary information of the reduced features and downsampled images, various underexposure/ overexposure phenomena caused by low-light images can be effectively avoided. In the discriminator network stage, the deep/shallow feature aggregation module is added to enhance the discrimination ability, and the inconsistencies are suppressed by learning the contradictory information of spatial filter, so that the shallow representation of information and the deep semantic information can guide each other. Thanks to the synergy of the above three innovative work, compared with many existing advanced low-light images enhancement models, MSED can achieve SOTA level performance on several public datasets. However, when dealing with low-light images with blurry content caused by rapid motion, MSED still cannot effectively restore their detailed information.}
}
@article{WANG2023103986,
title = {Blind image watermark decoder in NSST-FPCET domain using Weibull Mixtures-HMT},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103986},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103986},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002365},
author = {Xiangyang Wang and Yixuan Shen and Tingting Wang and Panpan Niu},
keywords = {Image watermarking, NSST-FPCET magnitudes, Weibull Mixtures-HMT, Otsu-Canny edge detection, Maximum likelihood decision},
abstract = {Balancing imperceptibility, robustness, and data payload is a key topic in digital watermarking technology. Many contributions point to statistical modeling as an effective solution to this problem. On the basis of this, in this paper, we devise a hybrid domain image watermark decoder based on Weibull mixtures based hidden Markov tree (Weibull Mixtures-HMT) model. In the embedding phase, Otsu-Canny edge detection method is used to map the positions of high entropy blocks to the target subband obtained by non-subsampled shearlet transform (NSST), and fast polar complex exponential transform (FPCET) is computed in the target blocks to obtain the NSST-FPCET domain. The watermark signals are embedded into NSST-FPCET magnitudes by a linear method. In the extraction phase, NSST-FPCET magnitude coefficients are modeled by Weibull Mixtures-HMT model, which describes the distribution characteristics as well as the dependencies of NSST-FPCET magnitudes. The efficient variance reduced stochastic expectation maximization method is employed for estimating the parameters of Weibull Mixtures-HMT model. A statistical image watermark decoder in NSST-FPCET domain is finally achieved by the maximum likelihood decision. Massive experiments are performed so as to confirm the superiority of the designed scheme in trade-off among the data payload, imperceptibility, and robustness.}
}
@article{KIM2023103881,
title = {Part-attentive kinematic chain-based regressor for 3D human modeling},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103881},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103881},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001311},
author = {Jeonghwan Kim and Gi-Mun Um and Jeongil Seo and Wonjun Kim},
keywords = {3D human modeling, Deep Neural Networks, Body part attentions, Kinematic chain-based decoder},
abstract = {As the demand for realistic representation and its applications increases rapidly, 3D human modeling via a single RGB image has become the essential technique. Owing to the great success of deep neural networks, various learning-based approaches have been introduced for this task. However, partial occlusions still give the difficulty to accurately estimate the 3D human model. In this letter, we propose the part-attentive kinematic regressor for 3D human modeling. The key idea of the proposed method is to predict body part attentions based on each body center position and estimate parameters of the 3D human model via corresponding attentive features through the kinematic chain-based decoder in a one-stage fashion. One important advantage is that the proposed method has a good ability to yield natural shapes and poses even with severe occlusions. Experimental results on benchmark datasets show that the proposed method is effective for 3D human modeling under complicated real-world environments. The code and model are publicly available at: https://github.com/DCVL-3D/PKCN_release}
}
@article{NAGARAJAN2023103905,
title = {Deep ensemble-based hard sample mining for food recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103905},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103905},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001554},
author = {Bhalaji Nagarajan and Marc Bolaños and Eduardo Aguilar and Petia Radeva},
keywords = {Knowledge representation, Hard-sample mining, Food recognition, Deep ensembles, Data augmentation},
abstract = {Deep neural networks represent a compelling technique to tackle complex real-world problems, but are over-parameterized and often suffer from over- or under-confident estimates. Deep ensembles have shown better parameter estimations and often provide reliable uncertainty estimates that contribute to the robustness of the results. In this work, we propose a new metric to identify samples that are hard to classify. Our metric is defined as coincidence score for deep ensembles which measures the agreement of its individual models. The main hypothesis we rely on is that deep learning algorithms learn the low-loss samples better compared to large-loss samples. In order to compensate for this, we use controlled over-sampling on the identified ”hard” samples using proper data augmentation schemes to enable the models to learn those samples better. We validate the proposed metric using two public food datasets on different backbone architectures and show the improvements compared to the conventional deep neural network training using different performance metrics.}
}
@article{WANG2023103950,
title = {End-to-end wavelet block feature purification network for efficient and effective UAV object tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103950},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103950},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002006},
author = {Haijun Wang and Lihua Qi and Haoyu Qu and Wenlai Ma and Wei Yuan and Wei Hao},
keywords = {Wavelet, Transformer, Unmanned aerial vehicle, Self-attention learning, Downsampling-upsampling strategy},
abstract = {Recently, unmanned aerial vehicle (UAV) object tracking tasks have significantly improved with the emergence of deep learning. However, owing to the object feature pollution caused by motion blur, illumination variation, and occlusion, most of the existing trackers often fail to precisely localize the target in the complex real-world circumstances. To overcome this challenge, we present a novel wavelet block feature purification network (WFPN) for efficient and effective UAV tracking. WFPN is mainly composed of downsampling network through wavelet transforms and upsampling network through inverse wavelet transforms. To be specific, the downsampling network performs discrete wavelet transform (DWT) to reduce interference information and preserve original feature details, while the upsampling network applies inverse DWT (IDWT) to reconstruct decontaminated feature information. Additionally, a novel sequential encoder is introduced to achieve a better purification effect. Finally, a pooling distance loss is devised to improve the purification effect of DWT downsampling network. Extensive experiments show that our WFPN achieves promising tracking performance on three well-known UAV benchmarks, especially on sequences with feature pollution. Moreover, our method runs at 33.2 frames per second on the edge platform of Nvidia Jetson AGX Orin, which is suitable for UAVs with limited onboard payload and computing capability.}
}
@article{WANG2023103929,
title = {Measuring dense false positive regions from segmentation result for whole slide tissue histology image},
journal = {Journal of Visual Communication and Image Representation},
volume = {96},
pages = {103929},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103929},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001797},
author = {Zhao Wang and Qianyu Feng and Germán Corredor and Can Koyuncu and Cheng Lu},
keywords = {Image segmentation, Evaluation metric, Histology image},
abstract = {Evaluating a segmentation model is critical for constructing computer-aided diagnosis (CAD) systems, in which the segmented regions will be used for downstream analysis. However, existing segmentation evaluation metrics may not reflect every aspect of a trained segmentation model performance in the context of the whole slide tissue histology images (WSIs). Specifically, existing segmentation metrics generally ignore the impact of densely packed false positive pixels (DFP) in the WSI segmentation result. This study presents a new and efficient metric, named MAFaR, accounting for DFP regions in digital WSI segmentation. The proposed metric consists of two modules: 1) Estimation of DFP regions; 2) Calculation of MAFaR score. In module 1, a Gaussian Kernel Density Estimation method was used to estimate the density of the false positive (FP) regions in segmentation result (SR). Then a two-step Mean-shift clustering algorithm was applied to the high-density FP regions to estimate the DFP regions. In module 2, the ratio of DFP regions area to the positive regions area were used for MAFaR calculation. We conducted two experiments to evaluate the effectiveness of the MAFaR score. In the first experiment, we compared MAFaR with existing metrics related to FP regions, the proposed MAFaR score can reflect the impact of DFP regions. In the second experiment, MAFaR scores were compared with the manual evaluation scores given by three experienced engineers and found high correlation (Spearman’s rank correlation coefficients greater than 0.7) and high agreement (Kendall coefficient = 0.839). Therefore, the MAFaR can be used with other segmentation metrics for evaluating WSI segmentation.}
}
@article{HOU2023103974,
title = {Object drift determination network based on dual-template joint decision-making in long-term visual tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103974},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103974},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002249},
author = {Zhiqiang Hou and Jiaxin Zhao and Zhuo Wang and Sugang Ma and Wangsheng Yu and Jiulun Fan},
keywords = {Long-term object visual tracking, Object drift determination network, Template update, Convolutional neural network, Computer vision, Deep learning},
abstract = {Object drift determination is a crucial issue in long-term tracking. Most existing object drift determination criteria require manually selecting different thresholds on different datasets to determine whether the object is lost. In this case, choosing the appropriate threshold is a complex problem. An object drift determination network based on dual-template joint decision-making is proposed to address this issue. The proposed object drift determination network not only does not require selection of thresholds on different datasets, and can be used as a plug-and-play module of short-term visual tracking algorithm to achieve long-term visual tracking tasks with good generalization ability. The proposed object drift determination network is applied to four short-term baseline trackers and constructs four long-term visual tracking algorithms. Experimental results verify that all four improved algorithms significantly improve long-term visual tracking performance compared to the original algorithms. In addition, the determined speed of the object drift determination network proposed in this paper reaches 111 FPS, which has little effect on the long-term visual tracking speed.}
}
@article{YU2023103962,
title = {Night vision self-supervised Reflectance-Aware Depth Estimation based on reflectance},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103962},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103962},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002122},
author = {Yao Yu and Fangling Pu and Hongjia Chen and Rui Tang and Jinwen Li and Xin Xu},
keywords = {Monocular depth estimation, Reflectance extraction, Night vision},
abstract = {The depth estimation of nighttime images is a challenging problem due to the lack of accurate ground-truth depth labels. Although various self-supervised methods leveraging texture information have been proposed to solve the problem, the performance is still not satisfactory due to the imaging limitations of visible cameras. To this end, we propose a self-supervised Reflectance-Aware Depth Estimation approach based on reflectance for nighttime images. Two major factors strengthen the proposed approach: a Reflectance Extraction Network and a feature consistency loss. We introduce the Reflectance Extraction Network to extract texture information based on the finding that the texture is beneficial for depth estimation. Then, we utilize the feature consistency loss to help the baseline network to learn the intrinsic feature rather than the images’ light. Experiment results on the challenging Oxford RobotCar dataset confirm the robustness and effectiveness of our approach.}
}
@article{XIE2023103889,
title = {Multi-scale convolutional attention network for lightweight image super-resolution},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103889},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103889},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001396},
author = {Feng Xie and Pei Lu and Xiaoyong Liu},
keywords = {Image super-resolution, Convolutional neural network, Lightweight, Attention mechanism},
abstract = {Convolutional neural network (CNN) based methods have recently achieved extraordinary performance in single image super-resolution (SISR) tasks. However, most existing CNN-based approaches increase the model’s depth by stacking massive kernel convolutions, bringing expensive computational costs and limiting their application in mobile devices with limited resources. Furthermore, large kernel convolutions are rarely used in lightweight super-resolution designs. To alleviate the above problems, we propose a multi-scale convolutional attention network (MCAN), a lightweight and efficient network for SISR. Specifically, a multi-scale convolutional attention (MCA) is designed to aggregate the spatial information of different large receptive fields. Since the contextual information of the image has a strong local correlation, we design a local feature enhancement unit (LFEU) to further enhance the local feature extraction. Extensive experimental results illustrate that our proposed MCAN can achieve better performance with lower model complexity compared with other state-of-the-art lightweight methods.}
}
@article{PAN2023103959,
title = {Action recognition method based on lightweight network and rough-fine keyframe extraction},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103959},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103959},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002092},
author = {Hao Pan and Qiuhong Tian and Saiwei Li and Weilun Miao},
keywords = {Keyframe extract, Action recognition, 3DCNN, Attention mechanism},
abstract = {To address the issues of large number of parameters and low recognition accuracy of action recognition networks, we propose an effective action recognition method based on lightweight network and rough-fine keyframe extraction. The method consists of three modules. The first module proposes a keyframe extraction network based on grayscale and feature descriptors, and employs a rough-fine idea to extract video keyframe. It reduces the redundancy of keyframe and enhances their ability to express action semantics. The second module introduces an attention-based feature extraction network, which combines decoupling ideas with attention mechanisms to enhance the accuracy of the action recognition network, while significantly reducing the network parameters. The third module is an improved attention module which optimizes the representation of local information. Finally, addition of the residual module fuses feature information between different convolutional layers. Experiments on two different datasets show that the number of parameters in the proposed method is only 6.4M. On publicly available datasets of HMDB51 and UCF101, the method achieves recognition accuracy of 75.69% and 93.18% without pre-training, respectively. The proposed method is valid and feasible on multiple public datasets.}
}
@article{HERMES2023103893,
title = {Point cloud-based scene flow estimation on realistically deformable objects: A benchmark of deep learning-based methods},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103893},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103893},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001438},
author = {Niklas Hermes and Alexander Bigalke and Mattias P. Heinrich},
keywords = {Scene flow estimation, 3D, Point clouds, Computer vision, Deep learning, Convolutional neural networks},
abstract = {Flow estimation on 3D point clouds is a challenging problem in the field of computer vision, which has great significance in many areas, such as autonomous driving and human interaction applications. Within the last years, the field of motion analysis has made great progress. The evaluation of the existing approaches mostly focuses on scenarios where objects are affected by rigid transformations. However, in many application areas such as gesture recognition or pose tracking, the detection of shape changes is essential and breaking them down to local rigid transformations is accompanied by loss of information. One component of our contributions is that we specifically prepared existing datasets for scene flow estimation on deformable objects. Additionally, we benchmark existing methods and analyze their behavior on various subtasks. The results show that already close to 80% of correct correspondences can be found on synthetic hand data, while only around 50% are found on real hand data. Our experimental validation and analysis help to build an understanding of new possibilities in broader areas. Furthermore, they should help to inspire possible further research directions.}
}
@article{LI2023103880,
title = {Depth cue enhancement and guidance network for RGB-D salient object detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103880},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103880},
url = {https://www.sciencedirect.com/science/article/pii/S104732032300130X},
author = {Xiang Li and Qing Zhang and Weiqi Yan and Meng Dai},
keywords = {RGB-D salient object detection, Depth cue enhancement, Multi-modal feature fusion, Depth guidance},
abstract = {Depth maps have been proven profitable to provide supplements for salient object detection in recent years. However, most RGB-D salient object detection approaches ignore that there are usually low-quality depth maps, which will inevitably result in unsatisfactory results. In this paper, we propose a depth cue enhancement and guidance network (DEGNet) for RGB-D salient object detection by exploring the depth quality enhancement and utilizing the depth cue guidance to generate predictions with highlighted objects and suppressed backgrounds. Specifically, a depth cue enhancement module is designed to generate high-quality depth maps by enhancing the contrast between the foreground and the background. Then considering the different characteristics of unimodal RGB and depth features, we use different feature enhancement strategies to strengthen the representation capability of side-output unimodal features. Moreover, we propose a depth-guided feature fusion module to excavate depth cues provided by the depth stream to guide the fusion of multi-modal features by fully making use of different modal properties, thus generating discriminative cross-modal features. Besides, we aggregate cross-modal features at different levels to obtain the final prediction by adopting a pyramid feature shrinking structure. Experimental results on six benchmark datasets demonstrate that the proposed network DEGNet outperforms 17 state-of-the-art methods.}
}
@article{SAMARANAYAKE2023103933,
title = {Detecting Water in Visual Image Streams from UAV with Flight Constraints},
journal = {Journal of Visual Communication and Image Representation},
volume = {96},
pages = {103933},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103933},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001839},
author = {Harin Samaranayake and Oshan Mudannayake and Dushani Perera and Prabhash Kumarasinghe and Chathura Suduwella and Kasun {De Zoysa} and Prasad Wimalaratne},
keywords = {Water Surface Identification, Unmanned Ariel Vehicles, Drones, UNet, Dense Optical Flow},
abstract = {Unmanned Ariel Vehicles (UAVs) require identifying water surfaces during flight maneuvers, mainly for safety in execution and its applications. We introduce two novel techniques to identify water surfaces from front-facing and downward-facing cameras mounted on a UAV. The first method — UNet-RAU, a unique architecture based on UNet and Reflection Attention Units, segments water pixels from front-facing camera views, utilizing the reflection property of water surfaces. On the On-Road and Off-Road datasets of Puddle-1000, UNet-RAU improved its performance by 2% over the state-of-the-art FCN-RAU. Additionally, the UNet-RAU generated an F1-score of 80.97% on our Drone-Water-Front dataset. The second method — Dense Optical Flow based Water Detection (DOF-WD), detects water surfaces in videos of downward-facing cameras. This method utilizes downwash-generated ripples and natural texture features on a water surface to identify water in low and high altitudes, respectively. We empirically validated the performance of the DOF-WD method using our Drone-Water-Down dataset.}
}
@article{YAO2023103963,
title = {Undirected graph representing strategy for general room layout estimation},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103963},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103963},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002134},
author = {Hui Yao and Jun Miao and Yilin Zheng and Guoxiang Zhang and Jun Chu},
keywords = {Indoor layout estimation, CNN, Undirected graph, Cuboid room, Non-cuboid room},
abstract = {Room layout estimation aims to predict the spatial structure of a room from a single image. Most existing methods relying on 2D cues are suitable for cuboid rooms, but non-cuboid rooms. 3D layout estimation methods can reconstruct 3D models of general rooms, but they are trained with depth information on high collection costs, consume large computation resources, and run slowly. This paper considers an undirected graph representation method of the general room layouts, which includes cuboid and non-cuboid rooms consisting of a single ceiling, a single floor, and multiple walls. To this end, we first predict the positions of the layout vertices and then use the network automatically learn the connection relationship between the vertices. The final layout is obtained through a simple layout inference post-processing algorithm. The experimental results both on cuboid and non-cuboid datasets validate the effectiveness and efficiency of our method. The code is available at https://github.com/Hui-Yao/2D-graph-layout-estimation.}
}
@article{WANG2023103877,
title = {Learning-based JNCD prediction for quality-wise perceptual quantization in HEVC},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103877},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103877},
url = {https://www.sciencedirect.com/science/article/pii/S104732032300127X},
author = {Hongkui Wang and Lianmin Zhang and Li Yu and Hailang Yang and Haibing Yin and Sitong Ding and Haifeng Xu and He Wang},
keywords = {Just noticeable coding distortion (JNCD) prediction, Perceptual quantization, Video coding},
abstract = {In visual perception, human only perceive discrete-scale quality levels over a wide range of coding bitrate. More clearly, the videos compressed with a series of quantization parameters (QPs) only have limited perceived quality levels. In this paper, perceptual quantization is transformed into the problem of how to determine the just perceived QP for each quality level, and a just noticeable coding distortion (JNCD) based perceptual quantization scheme is proposed. Specifically, multiple visual masking effects are analyzed and a linear regression (LR) based JNCD model is proposed to predict JNCD thresholds for all quality levels at first. According to the JNCD prediction model, the frame-level perceptual QPs for all quality levels are then derived on the premise of that coding distortions are infinitely close to the predicted JNCD thresholds. Based on the predicted frame-level perceptual QPs, the perceived QPs of all quality levels for each coding unit (CU) are finally determined according to a perceptual modulation function. Experimental results show that the proposed quality-wise perceptual quantization scheme is superior to the existing perceptual video coding algorithms significantly, i.e., the proposed perceptual quantization could save more bitrate with better quality.}
}
@article{JIANG2023103934,
title = {RDD-net: Robust duplicated-diffusion watermarking based on deep network},
journal = {Journal of Visual Communication and Image Representation},
volume = {96},
pages = {103934},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103934},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001840},
author = {Guowei Jiang and Zhouyan He and Jiangtao Huang and Ting Luo and Haiyong Xu and Chongchong Jin},
keywords = {Watermarking, Deep network, Robustness, RGB channels, Channel connection},
abstract = {This paper proposes a novel robust duplicated-diffusion watermarking model based on deep network (RDD-net). RDD-net employs the encoder-noise-decoder structure for end-to-end training to obtain high image watermarking invisibility and robustness. Firstly, a duplicated-diffusion strategy is employed in the encoder to replicate the original watermark iteratively until all copies are diffused to the whole image so that the generalized ability in resisting various noises is obtained. Then, a channel connection technique is designed to extract inherent image features for fusing watermark for robustness by mining correlations of RGB three channels of the color image. Meanwhile, each channel is fused with watermark to increase robustness. Another attempt to improve the watermarking performance is the optimizer, which optimizes the watermark distribution by evaluating the similarities between the encoded image and the original image, as well as between the encoded image and the noised image. Our extensive experimental results demonstrate that the proposed RDD-net not only resists different noises, but also obtains better image quality and higher robustness than the existing watermarking models.}
}
@article{FENG2023103882,
title = {Exploring the potential of Siamese network for RGBT object tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103882},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103882},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001323},
author = {Liangliang Feng and Kechen Song and Junyi Wang and Yunhui Yan},
keywords = {RGBT tracking, Siamese network, Multi-modal fusion, Attention mechanisms},
abstract = {Siamese tracking is one of the most promising object tracking methods today due to its balance of performance and speed. However, it still performs poorly when faced with some challenges such as low light or extreme weather. This is caused by the inherent limitations of visible images, and a common way to cope with it is to introduce infrared data as an aid to improve the robustness of tracking. However, most of the existing RGBT trackers are variants of MDNet (Hyeonseob Nam and Bohyung Han, Learning multi-domain convolutional neural networks for visual tracking, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 4293–4302.), which have significant limitations in terms of operational efficiency. On the contrary, the potential of Siamese tracking in the field of RGBT tracking has not been effectively exploited due to the reliance on large-scale training data. To solve this dilemma, in this paper, we propose an end-to-end Siamese RGBT tracking framework that is based on cross-modal feature enhancement and self-attention (SiamFEA). We draw on the idea of migration learning and employ local fine-tuning to reduce the dependence on large-scale RGBT data and verify the feasibility of this approach, and then we propose a reliable fusion approach to efficiently fuse the features of different modalities. Specifically, we first propose a cross-modal feature enhancement module to exploit the complementary properties of dual-modality, followed by capturing non-local attention in channel and spatial dimensions for adaptive weighted fusion, respectively. Our network was trained end-to-end on the LasHeR (Chenglong Li, Wanlin Xue, Yaqing Jia, Zhichen Qu, Bin Luo, Jin Tang, LasHeR: A Large-scale High-diversity Benchmark for RGBT Tracking, CoRR abs/2104.13202, 2021) training set and reached new SOTAs on GTOT (C. Li, H. Cheng, S. Hu, X. Liu, J. Tang, L. Lin, Learning collaborative sparse representation for grayscale-thermal tracking, IEEE Trans. Image Process, 25 (12) (2016) 5743–5756.), RGBT234 (C. Li, X. Liang, Y. Lu, N. Zhao, and J. Tang, “Rgb-t object tracking: Benchmark and baseline,” Pattern Recognition, vol. 96, p. 106977, 2019.), and LasHeR (Chenglong Li, Wanlin Xue, Yaqing Jia, Zhichen Qu, Bin Luo, Jin Tang, LasHeR: A Large-scale High-diversity Benchmark for RGBT Tracking, CoRR abs/2104.13202, 2021) while running in real-time.}
}
@article{LI2023103906,
title = {Multi-view convolutional vision transformer for 3D object recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103906},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103906},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001566},
author = {Jie Li and Zhao Liu and Li Li and Junqin Lin and Jian Yao and Jingmin Tu},
keywords = {Multi-view, 3D object recognition, Feature fusion, Convolutional neural networks},
abstract = {With the rapid development of three-dimensional (3D) vision technology and the increasing application of 3D objects, there is an urgent need for 3D object recognition in the fields of computer vision, virtual reality, and artificial intelligence robots. The view-based method projects 3D objects into two-dimensional (2D) images from different viewpoints and applies convolutional neural networks (CNN) to model the projected views. Although these methods have achieved excellent recognition performance, there is not sufficient information interaction between the features of different views in these methods. Inspired by the recent success achieved by vision transformer (ViT) in image recognition, we propose a hybrid network by taking advantage of CNN to extract multi-scale local information of each view, and of transformer to capture the relevance of multi-scale information between different views. To verify the effectiveness of our multi-view convolutional vision transformer (MVCVT), we conduct experiments on two public benchmarks, ModelNet40 and ModelNet10, and compare with those of some state-of-the-art methods. The final results show that MVCVT has competitive performance in 3D object recognition.}
}
@article{ZHOU2023103951,
title = {AMCFNet: Asymmetric multiscale and crossmodal fusion network for RGB-D semantic segmentation in indoor service robots},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103951},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103951},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002018},
author = {Wujie Zhou and Yuchun Yue and Meixin Fang and Shanshan Mao and Rongwang Yang and Lu Yu},
keywords = {Multiscale feature, Crossmodal fusion, Differential feature integration, RGB-D information, Semantic segmentation},
abstract = {Red-green-blue and depth (RGB-D) semantic segmentation is essential for indoor service robots to achieve accurate artificial intelligence. Various RGB-D indoor semantic segmentation methods have been proposed since the widespread adoption of depth maps. These methods have focused mainly on integrating the multiscale and crossmodal features extracted from RGB images and depth maps in the encoder and used unified strategies to recover the local details at the decoder progressively. However, these methods emphasized crossmodal fusion at the encoder, neglecting the distinguishability between RGB and depth features during decoding, thereby undermining the segmentation performance. To adequately exploit the features, we propose an efficient encoder-decoder architecture called asymmetric multiscale and crossmodal fusion network (AMCFNet) endowed with a differential feature integration strategy. Unlike existing methods, we use simple crossmodal fusion at the encoder and design an elaborate decoder to improve the semantic segmentation performance. Specifically, considering high- and low-level features, we propose a semantic aggregation module (SAM) to process the multiscale and crossmodal features in the last three network layers to aggregate high-level semantic information through a cascaded pyramid structure. Moreover, we design a spatial detail supplement module using low-level spatial details from depth maps to adaptively fuse these details and the information obtained from the SAM. Extensive experiments are conducted to demonstrate that the proposed AMCFNet outperforms state-of-the-art approaches.}
}
@article{SASIPRIYAA2023103878,
title = {SFGDO: Smart flower gradient descent optimization enabled generative adversarial network for recognition of Tamil handwritten character},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103878},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103878},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001281},
author = {N. Sasipriyaa and Natesan P. and Gothai E.},
keywords = {Generative adversarial network, Character recognition, Bilateral filter, Binarization, Gradient based thresholding},
abstract = {The Tamil language is complicated to identify, and thus more efforts are devised in the literary works. The objective is to develop a model called the Smart Flower Gradient Descent optimization-based Generative Adversarial Network (SFGDO-based GAN) for recognising Tamil handwriting. The dataset is first used to acquire the input image. The undesired noises are then pre-processed using a bilateral filter, and the binarization procedure is carried out using gradient-based thresholding. The necessary characteristics are then extracted for further categorization of Tamil characters, including character length, character width, statistical features, Local Binary Pattern (LBP), Convolutional Neural Network (CNN), density features, and Histogram of Oriented Gradients (HOG) features. Finally, utilising the proposed SFGDO enabled GAN, Tamil handwritten characters are recognised. The proposed Smart Flower Gradient Descent optimization (SFGDO) algorithm is developed by integrating Smart Flower Optimization Algorithm SFOA) and Gradient Descent Optimization (GDO).}
}
@article{YUAN2023103975,
title = {JPEG image encryption with grouping coefficients based on entropy coding},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103975},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103975},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002250},
author = {Yuan Yuan and Hongjie He and Yaoling Yang and Ningxiong Mao and Fan Chen and Muqadar Ali},
keywords = {JPEG image encryption, Permutation encryption, Classification permutation, Modulo encryption, Undivided RSV, Coefficients group},
abstract = {In the existing JPEG image encryption schemes, the block feature values that can be used to reduce key search space are either difficult to be changed or need to be changed through overflow processing which leads to low generality and high encryption runtime. To change block feature values without overflow processing, a novel JPEG image encryption scheme is proposed. For AC encryption, the complete and end AC groups based on undivided RSV (run/size, value) (ACG-URSV) are permuted separately to change different features. Complete ACG-URSV containing different number of RSVs is used to change the non-zero coefficients count (NCC) and energy of AC coefficients (EAC). End ACG-URSV containing the zero coefficients after position of last non-zero AC coefficient (PLZ) is mainly used to change PLZ. Besides, the intra-block RSV permutation and block permutation are used to further destroy correlation. For DC encryption, the positive DC prediction error (PDC-PE) groups modulo encryption is proposed to avoid overflow processing. The experimental results show that this paper reduces encryption runtime by more than half, improves generality by at least 20%. When quality factor of 90, the average change rates of NCC, EAC and PLZ values are increased by 96.28%, 11.68% and 29.15%, respectively.}
}
@article{YANG2023103925,
title = {Design of supervision-scalable learning systems: Methodology and performance benchmarking},
journal = {Journal of Visual Communication and Image Representation},
volume = {96},
pages = {103925},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103925},
url = {https://www.sciencedirect.com/science/article/pii/S104732032300175X},
author = {Yijing Yang and Hongyu Fu and C.-C. Jay Kuo},
keywords = {Subspace learning, Weak supervision, Scalable learning systems, Image classification},
abstract = {The design of robust learning systems that offer stable performance under a wide range of supervision degrees is investigated in this work. We choose the image classification problem as an illustrative example and focus on the design of modularized systems that consist of three learning modules: representation learning, feature learning, and decision learning. We discuss ways to adjust each module so that the design is robust with respect to different training sample numbers. Based on these ideas, we propose two families of learning systems. One adopts the classical histogram of oriented gradients (HOG) features, while the other uses successive-subspace-learning (SSL) features. We test their performance against LeNet-5 and ResNet-18, two end-to-end optimized neural networks, for MNIST, Fashion-MNIST and CIFAR-10 datasets. The number of training samples per image class goes from the extremely weak supervision condition (i.e., one labeled sample per class) to the strong supervision condition (i.e., 4096 labeled samples per class) with a gradual transition in between (i.e., 2n, n=0,1,…,12). Experimental results show that the two families of modularized learning systems have more robust performance than LeNet-5 and ResNet-18. They both outperform the two deep learning networks by a large margin for small n and have performance comparable for large n.}
}
@article{HWANG2023103930,
title = {Making depthwise convolution SR-friendly via kernel attention injection},
journal = {Journal of Visual Communication and Image Representation},
volume = {96},
pages = {103930},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103930},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001803},
author = {Seongmin Hwang and Daeyoung Han and Moongu Jeon},
keywords = {Single image super-resolution (SISR), Linear depthwise convolution, Kernel attention, Intra-kernel correlation, Determinant pooling},
abstract = {Despite the remarkable results achieved by deep convolutional neural networks (CNNs) in single image super-resolution (SISR), their computational cost increase exponentially as CNN models get deeper and wider. Consequently, depthwise separable convolutions (DSConvs) have emerged as the fundamental building basis for various contemporary lightweight network architectures. However, depthwise convolution is sub-optimal for restoring missing high-frequency details. In this paper, we commence with the vanilla depthwise convolution and progressively develop it to alleviate the above problem. Specifically, we introduce an effective method, kernel attention injection, to integrate intra-kernel correlation into the depthwise convolution layer by incorporating the learned kernel attention into the depthwise kernel. We find that the determinant is capable of capturing the correlation between diagonal elements of the depthwise kernel by summing the products of the diagonals. Therefore, we utilize the determinant as the kernel pooling method. Furthermore, we propose to remove the non-linear activation function behind the depthwise convolution (i.e., linear depthwise convolution) to preserve informative features for reconstructing faithful high-resolution (HR) images. Built on this recipe, we introduce kernel-attentive linear depthwise separable convolutions (KDSConvs), which exhibits promising super-resolution performance. Our experimental results underline the potential of depthwise convolution in super-resolution tasks by demonstrating that the proposed KDSConvs significantly outperforms DSConvs in terms of both quantitative measurements and visual quality. The code will be released at https://github.com/AwesomeHwang/KALDN.}
}
@article{XIANG2023103902,
title = {MTMVC: Semi-supervised 3D hand pose estimation using multi-task and multi-view consistency},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103902},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103902},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001529},
author = {Donghai Xiang and Wei Xu and Yuting Zhang and Bei Peng and Guotai Wang and Kang Li},
keywords = {Hand pose estimation, Semi-supervised learning, Deep learning, Consistency constraint},
abstract = {The high performance of state-of-the-art deep learning methods for 3D hand pose estimation heavily depends on a large annotated training set. However, it is difficult and time-consuming to obtain the annotations for 3D hand poses. To leverage unannotated images to reduce the annotation cost, we propose a semi-supervised method based on Multi-Task and Multi-View Consistency (MTMVC) for hand pose estimation. First, we obtain the joints based on heatmap prediction and coordinate regression parallelly and encourage their consistency. Second, we introduce multi-view consistency to encourage the predicted poses to be rotation-invariant. Thirdly, to make the network pay more attention to the hand region, we propose a spatially weighted consistency. Experiments on four public datasets showed that our proposed MTMVC outperformed existing semi-supervised hand pose estimation methods, and by only using half of the annotations, the accuracy of our method was comparable to those of several state-of-the-art fully supervised methods.}
}
@article{YE2023103903,
title = {CCA-FPN: Channel and content adaptive object detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103903},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103903},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001530},
author = {Zhiyang Ye and Chengdong Lan and Min Zou and Xu Qiu and Jian Chen},
keywords = {Feature pyramid network, Object detection, Channel and content adaptive, Feature enhancement module},
abstract = {Feature pyramid network (FPN) is a typical detector commonly for solving the issue of object detection at different scales. However, the lateral connections in FPN lead to the loss of feature information due to the reduction of feature channels. Moreover, the top-down feature fusion will weaken the feature representation in the process of feature delivery because of features with different semantic information. In this paper, we propose a feature pyramid network with channel and content adaptive feature enhancement module (CCA-FPN), which uses a channel adaptive guided mechanism module (CAGM) and multi-scale content adaptive feature enhancement module (MCAFEM) to alleviate these problems. We conduct comprehensive experiments on the MS COCO dataset. By replacing FPN with CCA-FPN in ATSS, our models achieve 1.3 percentage points higher Average Precision (AP) when using ResNet50 as backbone. Furthermore, our CCA-FPN achieves 0.3 percentage points higher than the AugFPN which is the state-of-the-art FPN-based detector.}
}
@article{MENG2023103927,
title = {Towards real-world haze removal with uncorrelated graph model},
journal = {Journal of Visual Communication and Image Representation},
volume = {96},
pages = {103927},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103927},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001773},
author = {Xiaozhe Meng and Yuxin Feng and Fan Zhou and Yun Liang and Zhuo Su},
keywords = {Image dehazing, Quality interference, Feature decorrelation, Light control},
abstract = {The scene restoration effect of the dehazing algorithms usually suffers from double quality interference. First, the dehazing methods based on the atmospheric scattering model (ASM) lead to light loss. Second, the synthetic data used for supervised learning may produce apparent data deviation from the real hazy scene, which seriously weakens the generalization ability of the model. To address the above problems, we propose an uncorrelated graph dehazing model for real-world scenes. The process firstly eliminates the correlation in the representation space by establishing a directed acyclic graph with isolated points, thus enhancing the generalization performance of the model. Secondly, to suppress over-exposure that may occur on real-world data, the proposed model training is performed with a combination of light control. This work improves the ASM and constructs a new dataset applicable to natural haze scenes. Finally, the effectiveness of the proposed method can be verified through multiple experimental comparisons.}
}
@article{WANG2023103875,
title = {Wavelet-FCWAN: Fast and Covert Watermarking Attack Network in Wavelet Domain},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103875},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103875},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001256},
author = {Chunpeng Wang and Fanran Sun and Zhiqiu Xia and Qi Li and Jian Li and Bing Han and Bin Ma},
keywords = {Watermarking attack, Wavelet transform, Deep learning, Digital watermarking, Imperceptibility},
abstract = {The protection of digital image content has become an important topic of scientific research with the continuous development and updates of Internet and multimedia technology. In the past few years, various watermarking algorithms with good robustness and imperceptibility have been proposed, but the development of watermarking attack techniques has stagnated. In this paper, we have attempted to use deep learning to develop a new watermarking attack scheme and present the Fast and Covert Watermarking Attack Network in Wavelet Domain (Wavelet-FCWAN). The watermarking attack scheme employs noise filling as a preprocessing step for the watermarked image, performs wavelet transform operation on the preprocessed watermarked image, and inputs the wavelet transformed sub-image into Wavelet-FCWAN along with the noise level map in parallel. The network can be trained quickly and produces a better watermarking attack effect while ensuring the visual quality and detail retention of the attacked image. The experiments show that Wavelet-FCWAN demonstrates the superiority of its watermarking attack effect by comparing different attack strategies, and it can produce varying degrees of attack effects on various image watermarking algorithms with high levels of universality and imperceptibility.}
}
@article{DENG2023103972,
title = {A bidirectional fusion branch network with penalty term-based trihard loss for person re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103972},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103972},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002225},
author = {Zelin Deng and Shaobao Liu and Pei He and Yun Song and Qiang Tang and WenBo Li},
keywords = {Deep learning, Person re-identification, Feature pyramid, Bidirectional fusion branch network, Penalty term-based trihard loss},
abstract = {Person re-identification (Re-ID) is the recognition of the same person in different camera views. Because of the existence of highly similar persons and great differences of the same person in different scenes, and the fact that the features extracted by current mainstream models lose some fine-grained information, it is likely for the models to misidentify the query person. To tackle these challenges, we introduce a bidirectional fusion branch network with penalty term-based trihard loss (BFB-PTT). The BFB-PTT constructs a bidirectional fusion branch (BFB) network based on feature pyramid, where low-level features are transferred to a high-level feature space through fewer convolutional layers than most of the traditional CNN-based models have, thus retaining more local features to discriminate different pedestrians more accurately and effectively. Meanwhile, we propose using the penalty term-based trihard loss (PTT) to optimize the spatial structure of pedestrian’s samples, so that the similar samples are drawn closer together in order to reduce the variabilities of the same person in different scenes. We have conducted comprehensive experiments and analyses on the proposed method’s effectiveness on three challenging benchmarks, and the results show that our approach achieves competitive performance with the state-of-art models.}
}
@article{SHI2023103899,
title = {Boosting separated softmax with discrimination for class incremental learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103899},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103899},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001499},
author = {Lei Shi and Kai Zhao and Zhenyong Fu},
keywords = {Incremental learning, Discrimination enhancement, Discriminative separated softmax},
abstract = {Deep neural networks (DNNs) suffer from catastrophic forgetting when learning new classes continually and tend to classify old samples to new classes. Existing methods like exemplar-memory and knowledge distillation alleviate forgetting, but face prediction bias due to data imbalance. Separated softmax (Ahn et al., 2021) was proposed to solve this problem. However, they ignore the discrimination between old classes and new classes, which greatly limits the performance of DNNs. To enhance model’s discrimination, we propose discriminative separated softmax. We divide new samples into two parts: one combined with old samples to build a balanced dataset of all seen classes, and the other combined with old samples as an imbalanced dataset. Furthermore, we apply mixup for these two datasets respectively to enhance discrimination. We evaluate our method on CIFAR100 and ImageNet100. Experimental results show that our method can effectively improve the discrimination and achieves superior performance compared with separated softmax, while outperforming state-of-the-art methods.}
}
@article{XU2023103983,
title = {Unsupervised industrial anomaly detection with diffusion models},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103983},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103983},
url = {https://www.sciencedirect.com/science/article/pii/S104732032300233X},
author = {Haohao Xu and Shuchang Xu and Wenzhen Yang},
keywords = {Anomaly detection, Diffusion model, Image reconstruction, Unsupervised learning},
abstract = {Due to the limitations of autoencoders and generative adversarial networks, the performance of reconstruction-based unsupervised image anomaly detection methods are not satisfactory. In this paper, we aim to explore the potential of a more powerful generative model, the diffusion model, in the anomaly detection problem. Specifically, we design a Reconstructed Diffusion Models (RecDMs) based on conditional denoising diffusion implicit models for image reconstruction. To eliminate the stochastic nature of the generation process, our key idea is to use a learnable encoder to extract meaningful semantic representations, which are then used as signal conditions in an iterative denoising process to guide the model in recovering the image, while avoiding falling into an ”identical shortcut” to meaningless image reconstruction. To accurately locate anomaly regions, we introduce a discriminative network to obtain the pixel-level anomaly segmentation map based on the reconstructed image. Our experiments demonstrate the effectiveness of the proposed method, achieving a new state-of-the-art image-level AUC score of 98.1% and a pixel-level AUC score of 94.6% on the MVTec AD dataset, among all reconstruction-based methods. We also show the significant potential and promising future of our method on the challenging real-world dataset, the CHL AD dataset.}
}
@article{TIAN2023103891,
title = {Gesture image recognition method based on DC-Res2Net and a feature fusion attention module},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103891},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103891},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001414},
author = {Qiuhong Tian and Wenxuan Sun and Lizao Zhang and Hao Pan and Qiaohong Chen and Jialu Wu},
keywords = {Hand gesture recognition, Multiscale features, Feature fusion attention module, Selective kernel network},
abstract = {To extract decisive features from gesture images and solve the problem of information redundancy in the existing gesture recognition methods, we propose a new multi-scale feature extraction module named densely connected Res2Net (DC-Res2Net) and design a feature fusion attention module (FFA). Firstly, based on the new dimension residual network (Res2Net), the DC-Res2Net uses channel grouping to extract fine-grained multi-scale features, and dense connection has been adopted to extract stronger features of different scales. Then, we apply a selective kernel network (SK-Net) to enhance the representation of effective features. Afterwards, the FFA has been designed to remove redundant information in features by fusing low-level location features with high-level semantic features. Finally, experiments have been conducted to validate our method on the OUHANDS, ASL, and NUS-II datasets. The results demonstrate the superiority of DC-Res2Net and FFA, which can extract more decisive features and remove redundant information while ensuring high recognition accuracy and low computational complexity.}
}
@article{WANG2023103968,
title = {EERCA-ViT: Enhanced Effective Region and Context-Aware Vision Transformers for image sentiment analysis},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103968},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103968},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002183},
author = {Xiaohua Wang and Jie Yang and Min Hu and Fuji Ren},
keywords = {Visual sentiment analysis, Enhanced Effective Region, Context-aware, Vision transformer, Double branch},
abstract = {Different parts of an image have a strong or weak guiding effect on emotions. The key to emotion recognition of images is to fully exploit the regions associated with emotions. Therefore, this paper proposes a visual sentiment classification model with two branches based on visual transformer, termed as Enhanced Effective Region and Context-Aware Vision Transformers (EERCA-ViT). This model includes a primary branch and an auxiliary branch. The primary branch simulates interdependencies between patches by squeezing and stimulating patches (P-SE), thereby highlighting effective region features in the global feature. The auxiliary branch removes feature patches that have been tagged by the primary branch through the context-aware module (CAM), forcing the network to discover different sentiment discriminative regions. At the same time, a two-part loss function is constructed to improve the robustness of the model. Finally, extensive experiments on six benchmark datasets show that the proposed method outperforms the state-of-the-art image sentiment analysis methods. Furthermore, the effectiveness of the different modules of the framework (P-SE and CAM) has been well demonstrated through extensive comparative experiments.}
}
@article{JIN2023103887,
title = {Event-guided low light image enhancement via a dual branch GAN},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103887},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103887},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001372},
author = {Haiyan Jin and Qiaobin Wang and Haonan Su and Zhaolin Xiao},
keywords = {Low-light enhancement, Feature fusion, Event camera, Gradient reconstruction},
abstract = {In the low light conditions, images are corrupted by low contrast and severe noise, but event cameras capture event streams with clear edge structures. Therefore, we propose an Event-Guided Low Light Image Enhancement method using a dual branch generative adversarial networks and recover clear structure with the guide of events. To overcome the lack of paired training datasets, we first synthesize three datasets containing low-light event streams, low-light images, and the ground truth normal-light images. Then, in the generator network, we develop an end-to-end dual branch network consisting of a image enhancement branch and a gradient reconstruction branch. The image enhancement branch is employed to enhance the low light images, and the gradient reconstruction branch is utilized to learn the gradient from events. Moreover, we develops the attention based event-image feature fusion module which selectively fuses the event and low-light image features, and the fused features are concatenated into the image enhancement branch and gradient reconstruction branch, which respectively generate the enhanced images with clear structure and more accurate gradient images. Extensive experiments on synthetic and real datasets demonstrate that the proposed event guided low light image enhancement method produces visually more appealing enhancement images, and achieves a good performance in structure preservation and denoising over state-of-the-arts.}
}
@article{XIE2023103984,
title = {A novel complex-valued convolutional network for real-world single image dehazing},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103984},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103984},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002341},
author = {Xinxiu Xie and Chaofeng Li and Tuxin Guan and Yuhui Zheng and Xiaojun Wu},
keywords = {Real-world image dehazing, Complex-valued convolutional network, Complex-valued attention, Complex-valued feature learning, Phase information},
abstract = {Previous deep learning-based single image dehazing methods are implemented by using real-valued convolutional neural network (RV-CNN), which ignore the phase information of the image, and tend to cause models to perform well on synthetic datasets but low generalization on real-world datasets. In this paper, we propose a novel Complex-Valued convolutional Dehazing Network (CVD-Net), which considers both amplitude and phase information of image for haze removal in real-world scenes. Specifically, we construct complex-valued transformation module, including complex-valued convolutional layer and residual block embedded into the middle of the network to extract features of image efficiently, and also design a complex-valued selected fusion module (CVSF) and complex-valued attention module (CVAM) to promote the interaction between different scale features for preserving image detailed information. Both qualitative and quantitative experimental results show that the proposed CVD-Net can effectively remove the haze, and has good generalization in real-world hazy images.}
}
@article{BI2023103956,
title = {MIL-ViT: A multiple instance vision transformer for fundus image classification},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103956},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103956},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002067},
author = {Qi Bi and Xu Sun and Shuang Yu and Kai Ma and Cheng Bian and Munan Ning and Nanjun He and Yawen Huang and Yuexiang Li and Hanruo Liu and Yefeng Zheng},
keywords = {Vision transformer, Multiple instance learning, Fundus image, Attention aggregation, Calibrated attention mechanism},
abstract = {Despite the great success of deep learning approaches, retinal disease classification is still challenging as the early-stage pathological regions of retinal diseases may be extremely tiny and subtle, which are difficult for networks to detect. The feature representations learnt by deep learning models focusing more on the local view may lead to indiscriminative semantic-level representation. On the contrary, if they focus more on the global semantic-level, they may ignore the discerning subtle local pathological regions. To address this issue, in this paper, we propose a hybrid framework, combining the strong global semantic representation learning capability of the vision Transformer (ViT) and the excellent capacity of local representation extraction from the conventional multiple instance learning (MIL). Particularly, a multiple instance vision Transformer (MIL-ViT) is implemented, where the vanilla ViT branch and the MIL branch generate semantic probability distributions separately, and a bag consistency loss is proposed to minimize the difference between them. Moreover, a calibrated attention mechanism is developed to embed the instance representation into the bag representation in our MIL-ViT. To further improve the feature representation capability for fundus images, we pre-train the vanilla ViT on a large-scale fundus image database. The experimental results validate that the generalization capability of the model using our pre-trained weights for fundus disease diagnosis is better than the one using ImageNet pre-trained weights. Extensive experiments on four publicly available benchmarks demonstrate that our proposed MIL-ViT outperforms latest fundus image classification methods, including various deep learning models and deep MIL methods. All our source code and pre-trained models are publicly available at https://github.com/greentreeys/MIL-VT.}
}
@article{RAJ2023103960,
title = {An approximate tensor singular value decomposition approach for the fast grouping of whole-body human poses},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103960},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103960},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002109},
author = {M.S. Subodh Raj and Sudhish N. George},
keywords = {Approximate tensor-SVD, Human pose depth map, Human pose grouping, Tensor-QR decomposition},
abstract = {We propose a fast method to group similar human poses from single-view depth maps using an approximate singular value decomposition approach in the tensor domain. To this end, the input tensor is decomposed, and a representation tensor is learned using the tensor-QR decomposition and ℓ2,1 norm minimization. The spatial structure and, thereby, the spatial information in each depth map, vital in performing accurate grouping of human poses, is preserved by adopting the 3D tensor approach in treating the data. For the seamless integration of discriminatory information of each pose, a new dissimilarity measure based on sub-tensors is devised and integrated into the optimization problem. Experimental analysis showcases the ability of the proposed method to achieve 10%–15% improvement in the grouping results compared with the state-of-the-art counterparts. The proposed algorithm also converges in less than ten iterations on average, thereby improving CPU time.}
}
@article{SINGH2023103931,
title = {Indoor dataset for Person Re-Identification: Exploring the impact of backpacks},
journal = {Journal of Visual Communication and Image Representation},
volume = {96},
pages = {103931},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103931},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001815},
author = {Divya Singh and Jimson Mathew and Mayank Agarwal and Mahesh Govind},
keywords = {Person Re-Identification, WB, WoB, Dataset, PRid dataset, Bag, Knapsack},
abstract = {Person Re-Identification (PR-Id) encounters misclassification issues when re-identifying persons with different backpacks. These bags manifest as large and distinct regions in the images surpassing other finer details of a person. As a result, a CNN model swiftly detects and prioritizes these image regions for re-identification. However, the bags are subject to alterations or may be similar among multiple persons, resulting in misclassification. To ensure that a CNN model does not consider bags as unique features of specific persons and prioritize them for re-identification, images of persons with diverse backpacks are crucial in the training dataset. Moreover, these images enhance the model’s focus on other unique regions of a person. Although the current datasets show potential, incorporating such images could enhance their effectiveness. Therefore, in this paper, we propose an indoor PR-Id dataset named “With Bag/Without Bag-ReID” (WB/WoB-ReID). The set “with_bag” in WB/WoB-ReID dataset includes identities with different backpacks for the first time. We also incorporate identities without bags and with varying numbers of image counts in three other sets, namely “without_bag”, “both_small,” and “both_large”. We assess WB/WoB-ReID and three other PR-Id datasets: Market1501, CUHK03, and DukeMTMC-reID on various existing approaches. The highest mAP achieved on the“with_bag” is 74%, “without_bag” is 96.7% and other datasets are 97.78%, 95.20% and 92.4%. The results show that incorporating identities with diverse bags reduces the mAP, highlighting the misclassifications that arise specifically in the presence of bags.}
}
@article{CHEN2023103957,
title = {Multi-view graph convolution network for the recognition of human action with spatial and temporal occlusion problems},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103957},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103957},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002079},
author = {Yang Chen and Ling Wang and Dekun Hu and Hong Cheng},
keywords = {Human action recognition, Spatial–temporal occlusion, Multi-view, Graph network},
abstract = {Human action recognition holds great significance in the field of social security. However, the challenges posed by occlusion and changes in viewpoint create specific obstacles in achieving accurate recognition. In this study, we propose a multi-view graph convolution fusion method to effectively address this issue. Specifically, considering the limited availability of public human action datasets that include occlusion, we introduce an adaptive multi-view spatial–temporal occlusion generation method. It allows us to generate occlusion skeleton data from multiple viewpoints, ensuring a close resemblance to real-life scenarios with minimal modifications to existing public datasets. Additionally, we present a plug-and-play multi-view information fusion module, briefed as MGL, which aims to solve the occlusion problem. The MGL combines the capabilities of Graph Convolutional network (GCN) and Long Short-Term Memory network (LSTM), in which GCN is employed to reconstruct human skeleton information using multi-view spatial occlusion data, and LSTM captures the long-term dependencies within the temporal sequences of the multi-view data. Moreover, an attention mask mechanism is introduced to highlight key joint features. Experiment results illustrate the excellent performance of our method on the NTU RGB+D 60 and NTU RGB+D 120 datasets.}
}
@article{PATEL2023103904,
title = {An efficient optimization of measurement matrix for compressive sensing},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103904},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103904},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001542},
author = {Saumya Patel and Ankita Vaish},
keywords = {Measurement matrix, Optimization, Compressive sensing, Sparsity},
abstract = {Compressive sensing (CS) is a new paradigm for signal acquisition and reconstruction, which can reconstruct the signal at less than the Nyquist sampling rate. The sampling of the signal occurs through a measurement matrix (MM); thus, MM generation is significant in the context of the CS framework. In this paper, an optimization algorithm is introduced for the generation of the MM of CS based on Restricted Isometric Property (RIP) mandates that eigenvalues of the sensing matrix fall within an interval also minimizes the mutual coherence of the sensing matrix (i.e. the product of the MM and sparsifying matrix). A novel gradient-based iterative optimization method is used to reduce the eigenvalues of the sensing matrix by SVD decomposition. Meanwhile, the proposed algorithm can also reduce the operational complexity. Experimental results and analysis prove that the optimized MM reduces the maximum mutual and average mutual coherence between the MM and the sparsifying basis, which shows the effectiveness of the proposed algorithm over some state-of-art works.}
}
@article{TRUJILLOPINO2023103928,
title = {Accurate subvoxel location and characterization of edges in 3D images based on the Partial Volume Effect},
journal = {Journal of Visual Communication and Image Representation},
volume = {96},
pages = {103928},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103928},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001785},
author = {Agustín Trujillo-Pino and Miguel Alemán-Flores and Daniel Santana-Cedrés and Nelson Monzón},
keywords = {3D images, Edge detection, Subvoxel accuracy, Partial volume effect, Feature extraction},
abstract = {An accurate estimation of the position, orientation, principal curvatures, and change in intensity of the edges in a 3D image provides highly useful information for many applications. The use of derivative operators to compute the gradient vector and the Hessian matrix in each voxel usually generates inaccurate results. This paper presents a new edge detector which is derived from the Partial Volume Effect (PVE). Instead of assuming continuity in the image values, edge features are extracted from the distribution of intensities within a neighborhood of each edge voxel. First, the influence of the intensities of the voxels in first- and second-order edges is analyzed to demonstrate that these types of edges can be precisely characterized from the intensity distribution. Afterward, this approach is extended to especially demanding situations by considering how adverse conditions can be tackled. This extension includes filtering noisy images, characterizing edges in blurred regions, and using windows with floating limits for close edges. The proposed technique has been tested on synthetic and real images, including some particularly difficult objects, and achieving a highly accurate subvoxel characterization of the edges. An open source implementation of our method is provided.}
}
@article{LIU2023103876,
title = {Detection of GAN generated image using color gradient representation},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103876},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103876},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001268},
author = {Yun Liu and Zuliang Wan and Xiaohua Yin and Guanghui Yue and Aiping Tan and Zhi Zheng},
keywords = {Image generative model, Generative adversarial networks, Fake image identification},
abstract = {With the development of generative adversarial network (GANs) technology, the technology of GAN generates images has evolved dramatically. Distinguishing these GAN generated images is challenging for the human eye. Moreover, the GAN generated fake images may cause some behaviors that endanger society and bring great security problems to society. Research on GAN generated image detection is still in the exploratory stage and many challenges remain. Motivated by the above problem, we propose a novel GAN image detection method based on color gradient analysis. We consider the difference in color information between real images and GAN generated images in multiple color spaces, and combined the gradient information and the directional texture information of the generated images to extract the gradient texture features for GAN generated images detection. Experimental results on PGGAN and StyleGAN2 datasets demonstrate that the proposed method achieves good performance, and is robust to other various perturbation attacks.}
}
@article{CHANG2023103973,
title = {Cryptanalysis of iterative encryption and image sharing scheme based on the VQ attack},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103973},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103973},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002237},
author = {Chin-Chen Chang and Jui-Chuan Liu and Kai Gao},
keywords = {Reversible data hiding, Cryptanalysis, Vector quantization, CNN, Security},
abstract = {Reversible data hiding in encrypted images (RDHEI) is widely employed to protect privacy in images stored on cloud storage. Recently, Yu et al. [22] proposed an effective image encryption scheme called iterative encryption and image sharing (IEIS), which is highly compatible with RDHEI. In this study, we analyze the characteristics of IEIS and present a cryptanalysis scheme based on vector quantization (VQ) attack. To capture the pixel-changing pattern of an image block, the concept of pixel difference matrix (PDM) is introduced. VQ attack is subsequently utilized to estimate the ciphertext block. Furthermore, we employ a well-trained convolutional neural network (CNN) based discriminator to identify the proper block permutation sequence within the secret key space. Experimental results demonstrate the efficacy of our proposed cryptanalysis scheme. Based on our study, we conclude that the IEIS scheme raises security concerns.}
}
@article{AHMAD2023103892,
title = {Transforming spatio-temporal self-attention using action embedding for skeleton-based action recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103892},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103892},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001426},
author = {Tasweer Ahmad and Syed Tahir Hussain Rizvi and Neel Kanwal},
keywords = {Human action recognition, Graph convolutional network, Link prediction, Self-attention, Computer vision},
abstract = {Over the past few years, skeleton-based action recognition has attracted great success because the skeleton data is immune to illumination variation, view-point variation, background clutter, scaling, and camera motion. However, effective modeling of the latent information of skeleton data is still a challenging problem. Therefore, in this paper, we propose a novel idea of action embedding with a self-attention Transformer network for skeleton-based action recognition. Our proposed technology mainly comprises of two modules as, (i) action embedding and (ii) self-attention Transformer. The action embedding encodes the relationship between corresponding body joints (e.g., joints of both hands move together for performing clapping action) and thus captures the spatial features of joints. Meanwhile, temporal features and dependencies of body joints are modeled using Transformer architecture. Our method works in a single-stream (end-to-end) fashion, where multiple-layer perceptron (MLP) is used for classification. We carry out an ablation study and evaluate the performance of our model on a small-scale SYSU-3D dataset and large-scale NTU-RGB+D and NTU-RGB+D 120 datasets where the results establish that our method performs better than other state-of-the-art architectures.}
}
@article{XUE2023103969,
title = {A hierarchical multi-modal cross-attention model for face anti-spoofing},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103969},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103969},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002195},
author = {Hao Xue and Jing Ma and Xiaoyu Guo},
keywords = {Facial recognition, Face anti-spoofing, Multi-modal, Feature fusion, Hierarchical feature extraction, Cross-attention},
abstract = {Facial recognition has become popular in interactive systems as a means to authenticate identity. However, Facial recognition can be easily attacked illegally through face spoofing. In this paper, we propose a hierarchical multi-modal cross-attention model for face anti-spoofing, which can be flexibly applied in both single-modal and multi-modal scenarios. In order to map features among modalities thoroughly, we also design a novel attention mechanism, namely W-MSA-CA (Window-based Multihead Self-Attention and Cross Attention), which leverages both Multi-modal Multihead Self-Attention (MMSA) and Multi-modal Patch Cross attention (MPCA) to fuse multi-modal features. We test the proposed model on the public datasets and the results show that our model’s capability to detect various types of spoofing is effective.}
}
@article{ZHANG2023103961,
title = {Infer unseen from seen: Relation regularized zero-shot visual dialog},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103961},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103961},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002110},
author = {Zefan Zhang and Shun Li and Yi Ji and Chunping Liu},
keywords = {Visual dialog, Zero-shot learning, Attention},
abstract = {The Visual Dialog task requires retrieving the correct answer based on detected objects, a current question, and history dialogs. However, in real-world scenarios, most existing models face the hard-positive problem and are unable to reason about unseen features, which limits their generalization ability. To address this issue, we propose two Relation Regularized Modules (RRM) in this article. The first is the Visual Relation Regularized Module (VRRM), which seeks known visual features that have semantic relations with unknown visual features and leverages these known features to assist in understanding the unknown features. The second is the Text Relation Regularized Module (TRRM), which enhances the keywords in the answers to strengthen the understanding of unknown text features. To evaluate the effectiveness of these modules, we propose two zero-shot Visual Dialog splits for verification: Visual Zero-shot VisDial with unseen visual features and Text Zero-shot VisDial with unseen answers. Experimental results demonstrate that our proposed modules achieve state-of-the-art performance in zero-shot Visual Dialog with unseen visual features and unseen answers, while also producing comparable results on the benchmark VisDial v1.0 test dataset.}
}
@article{GOU2023103888,
title = {A novel fast intra algorithm for VVC based on histogram of oriented gradient},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103888},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103888},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001384},
author = {Aorui Gou and Heming Sun and Chao Liu and Xiaoyang Zeng and Yibo Fan},
keywords = {Versatile video coding, Histogram of oriented gradient, Fast mode decision, Fast partition decision, Intra prediction},
abstract = {The latest Versatile Video Coding (VVC) standard incorporates a series of effective and complex new intra coding tools, which obtains superior coding efficiency than the High Efficiency Video Coding (HEVC). However, this makes the intra coding more complicated and time-consuming. A fast algorithm for VVC from two aspects of fast mode decision and fast partition decision is proposed in this paper. For the fast mode decision, the relationship between bins with Histogram of Oriented Gradient (HOG) and intra modes is created for the mode selection, decreasing the planar modes for SATD and RDO. Moreover, we analyze the maximum bins to determine the final modes, and we use the modes of left and upper blocks as a reference for the current CU, which can early terminate RDO. Moreover, a two-step fast partition algorithm is proposed based on HOG for fast partition decision, in which two thresholds are investigated to control the uniformity of textures. The proposed fast algorithm is implemented on the VVC test model, and the experimental results show that it can achieve 69.07% time savings with only 2.96% BDBR increases averagely, which outperforms other relatively existing state-of-the-art methods. Moreover, to convince the universality of our algorithm, we further implement our method in Fraunhofer Versatile Video Encoder (VVenc) and Fraunhofer Versatile Video Decoder (VVdec), which have five settings to control the trade-off between encoding quality and efficiency for intra coding. The fast intra mode decision algorithm and fast partition algorithm decrease the complexity of intra coding for both VTM and VVenc, which shows the efficiency and universality of the proposed fast partition and fast mode decision algorithms.}
}
@article{LIN2023103985,
title = {Random hand gesture authentication via efficient Temporal Segment Set Network},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103985},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103985},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002353},
author = {Yihong Lin and Wenwei Song and Wenxiong Kang},
keywords = {Biometrics, Random hand gesture, Hand gesture authentication, Two-stream neural network, Attention mechanism},
abstract = {Biometric authentication technologies are rapidly gaining popularity, and hand gestures are emerging as a promising biometric trait due to their rich physiological and behavioral characteristics. Hand gesture authentication can be categorized as defined hand gesture authentication and random hand gesture authentication. Unlike defined hand gesture authentication, random hand gesture authentication is not constrained to specific hand gesture types, allowing users to perform hand gestures randomly during enrollment and verification, thus more flexible and friendly. However, in random hand gesture authentication, the model needs to extract more generalized physiological and behavioral features from different viewpoints and positions without gesture templates, which is more challenging. In this paper, we present a novel efficient Temporal-Segment-Set-Network (TS2N) that directly extracts both behavioral and physiological features from a single RGB video to further enhance the performance of random hand gesture authentication. Our method adopts a new motion pseudo-modality and leverages a set-based representation to capture behavioral characteristics online. Additionally, we propose a channel-spatial attention mechanism, Contextual Squeeze-and-Excitation Network (CoSEN), to better abstract and understand physiological characteristics by explicitly modeling the channel-spatial interdependence, thereby adaptively recalibrating channel-specific and spatial-specific responses. Extensive experiments on the largest public hand gesture authentication dataset SCUT-DHGA demonstrate TS2N’s superiority against 21 state-of-the-art models in terms of EER (5.707% for full version and 6.664% for lite version) and computational cost (98.9022G for full version and 46.3741G for lite version). The code is available at https://github.com/SCUT-BIP-Lab/TS2N.}
}
@article{YU2023103900,
title = {Adaptive multi-teacher softened relational knowledge distillation framework for payload mismatch in image steganalysis},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103900},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103900},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001505},
author = {Lifang Yu and Yunwei Li and Shaowei Weng and Huawei Tian and Jing Liu},
keywords = {Image steganalysis, PM (payload mismatch), BPDNets, AWA, SRKD},
abstract = {In this paper, we focus on improving the detection accuracy when payload mismatch occurs in steganalysis, by proposing an adaptive multi-teacher softened relational knowledge distillation framework, which combines multiple balanced payload difference networks (BPDNets), adaptive weight allocation (AWA) and softened relational knowledge distillation (SRKD). BPDNets aims to equivalently concern about the payload difference signal, so that the student can still achieve satisfactory detection accuracy even under unseen payloads due to that a stego signal can be considered as the sum of multiple payload difference signals. AWA assigns appropriate weights to each BPDNet based on the confidence evaluated using the cross entropy and batch entropy loss, thus effectively integrating the knowledge of BPDNets. SRKD combining the softmax with temperature τ other than standard logits to guide the student to inherit more knowledge of relationships between images. Extensive experiments verify that the student achieves better detection performance than state-of-the-art networks.}
}
@article{FU2023103924,
title = {Class semantic enhancement network for semantic segmentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {96},
pages = {103924},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103924},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001748},
author = {Siming Fu and Hualiang Wang and Haoji Hu and Xiaoxuan He and Yongwen Long and Jianhong Bai and Yangtao Ou and Yuanjia Huang and Mengqiu Zhou},
keywords = {Semantic segmentation, Attention, Graph module},
abstract = {Existing semantic segmentation methods favor class semantic consistency by extracting long-range contextual features through multi-scale and attention strategies. These methods ignore the relations between feature channels and classes, which are essential to represent consistent class semantics. To this end, we propose the Class Semantic Enhancement Network (CSENet) which boosts the segmentation performance of a backbone network in a coarse-to-fine manner. CSENet consists of two basic modules – (1) the Class Semantic Channel Graph Module (CSCG) module, which captures inter-dependencies among channels and strengthens the channel-class relation, and (2) the Class Prior Fully Convolution (CP-FC) module, which utilizes the channel-class relation as class priors to refine the segmentation results. Extensive experiments have demonstrated that the proposed CSENet is able to learn discriminative feature representations and achieves state-of-the-art performance on three benchmark datasets, including PASCAL Context, ADE20K and COCO Stuff.}
}
@article{MAJUMDER2023103911,
title = {A unique database synthesis technique for coverless data hiding},
journal = {Journal of Visual Communication and Image Representation},
volume = {96},
pages = {103911},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103911},
url = {https://www.sciencedirect.com/science/article/pii/S104732032300161X},
author = {Anandaprova Majumder and Sumana Kundu and Suvamoy Changder},
keywords = {Security and privacy protection, Coverless data hiding, Privacy attacks, Object synthesis, Database synthesis, Steganography},
abstract = {Coverless data hiding is a powerful technique for data security, but synthesizing text or images, often leads to semantically incorrect results. Prior state-of-the-art works have typically used media files as covers or as references for cover synthesis. This scope of improvement has inspired us to propose a novel data hiding technique by synthesizing self-sufficient, independent object without using any cover, even as metadata for data hiding. The approach focuses on synthesis of a database, by generating a dataset for the domain values of the attributes of it. Application of comparison-based sorting technique to the dataset, reveals the hidden message. The skill of the technique lies in unlimited hiding capacity, while preserving the semantic integrity of the database. Multiple security measures are taken into account along with thorough analysis of time complexity, to evaluate the efficacy of our method, that surpasses recently proposed approaches, providing solution for highly resistant covert communication.}
}
@article{ARSHAD2023103970,
title = {IPVNet: Learning implicit point-voxel features for open-surface 3D reconstruction},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103970},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103970},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002201},
author = {Mohammad Samiul Arshad and William J. Beksi},
keywords = {3D reconstruction, Open surfaces, Implicit functions},
abstract = {Reconstruction of 3D open surfaces (e.g., non-watertight meshes) is an underexplored area of computer vision. Recent learning-based implicit techniques have removed previous barriers by enabling reconstruction in arbitrary resolutions. Yet, such approaches often rely on distinguishing between the inside and outside of a surface in order to extract a zero level set when reconstructing the target. In the case of open surfaces, this distinction often leads to artifacts such as the artificial closing of surface gaps. However, real-world data may contain intricate details defined by salient surface gaps. Implicit functions that regress an unsigned distance field have shown promise in reconstructing such open surfaces. Nonetheless, current unsigned implicit methods rely on a discretized representation of the raw data. This not only bounds the learning process to the representation’s resolution, but it also introduces outliers in the reconstruction. To enable accurate reconstruction of open surfaces without introducing outliers, we propose a learning-based implicit point-voxel model (IPVNet). IPVNet predicts the unsigned distance between a surface and a query point in 3D space by leveraging both raw point cloud data and its discretized voxel counterpart. Experiments on synthetic and real-world public datasets demonstrates that IPVNet outperforms the state of the art while producing far fewer outliers in the resulting reconstruction.}
}
@article{PERUMAL2023103949,
title = {DenSplitnet: Classifier-invariant neural network method to detect COVID-19 in chest CT data},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103949},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103949},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001992},
author = {Murukessan Perumal and M Srinivas},
keywords = {Chest-CT-scan images, Novel coronavirus, COVID-19, Deep learning, Self-supervised learning, Computer vision},
abstract = {Objective:
COVID-19 has made an unprecedented impact on humanity. The Healthcare sector, in an effort to curb COVID-19, could leverage Artificial Intelligence (AI) to its aid, especially in diagnosing it through the classification of Chest-CT scans. However, data scarcity plagues the medical domain. Therefore, any AI solution must be capable of learning from limited data. Also the resulting AI could relieve radiologists from exhaustion and aid medical practitioners as a valuable diagnostic tool.
Methods:
Our proposed model DenSplitnet uses Dense blocks to learn image features, Self-Supervised Learning for pre-training to learn the context, and a novel two-way split branch at the final classification layer for classifier-invariant generalization ability.
Results:
DenSplitnet achieves state-of-the-art performance on four benchmark chest-CT scan datasets for COVID-19. The model performs well on the Sars-cov-2 ct-scan dataset. The Test accuracy is 91.92%, F1-Score is 91.30%, Precision is 96.55%, and Recall is 87.04%. The model achieves test accuracy of 86.17%, Precision of 82.94%, Recall of 83.72%, and F1-Score of 83.23% on the Sars-cov-2 ct-scan multiclass dataset. The model obtains a test accuracy of 86.21%, an F1-Score of 83.91%, and an AUC of 0.95 in the Covid-ct-dataset. The model obtains a test accuracy that is 73.11% and an F1-Score of 70.97% in the TransferLearn-ct dataset.
Conclusion:
The findings support the practical usability of the DenSplitnet as a technological AI assistance to radiologists and other medical professionals, alongside Grad-CAM plots for explainable AI and the theoretical examination of the classifier-invariant generalization capacity of the network. The healthcare sector can benefit from this technology in a number of ways.}
}
@article{LUO2023103897,
title = {Deep semantic image compression via cooperative network pruning},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103897},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103897},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001475},
author = {Sihui Luo and Gongfan Fang and Mingli Song},
keywords = {Deep image compression, Network pruning, Semantic perception},
abstract = {Incorporating semantic analysis into image compression can significantly reduce the repetitive computation of fundamental semantic analysis in downstream applications such as semantic image retrieval. In this paper, we tackle the semantic image compression task, which embeds semantics in the compressed bitstream. An intuitive solution to this task is joint multi-task training, which generally results in the trade-off of one task to accommodate the other. We thus provide an alternative pilot solution: given a pair of pre-trained teacher networks that specialize in image compression and semantic inference respectively, we first fuse both models to acquire an ensemble model and then leverage cooperative network pruning and retraining to condense the knowledge. Various experiments on five benchmark datasets validate that the proposed method achieves on par and in many cases better performance than the teachers yet comes in a more compact size, and outperforms its multi-task learning and knowledge distillation counterparts.}
}
@article{LIU2023103994,
title = {IFGLT: Information fusion guided lightweight Transformer for image denoising},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103994},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103994},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002444},
author = {Fengyin Liu and Ziqun Zhou and Changyou Men and Quan Sun and Kejie Huang},
keywords = {Image denoising, Transformer, Lightweight network},
abstract = {Image denoising is a low-level computer vision task that aims to reconstruct high-quality images from noisy ones. However, large networks with a high computational burden have been employed in existing works in pursuit of high-quality images. This paper introduces an Information Fusion Guided Lightweight Transformer (IFGLT) that can lessen the computational burden and achieve superior restoration results. The Feature Enhancement Module (FEM) optimizes the computing cost of the Transformer layer by layer using various techniques such as group mapping, channel generation, fusion convolution, and window rearrangement. The Information Compensation Module (ICM) gradually compensates for missing information by leveraging the original image. The Lightweight Sample Module (LSM) performs up-sampling and down-sampling with the minimal computing cost by altering the order of feature transformation. The experimental results demonstrate that our proposed IFGLT attains higher objective indices and achieves better visual effects with reduced computing cost in comparison to conventional methods.}
}
@article{ISLAM2023103908,
title = {Iterative graph filtering network for 3D human pose estimation},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103908},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103908},
url = {https://www.sciencedirect.com/science/article/pii/S104732032300158X},
author = {Zaedul Islam and A. Ben Hamza},
keywords = {Human pose estimation, Graph regularization, Gauss–Seidel, Modulation, Skip connection},
abstract = {Graph convolutional networks (GCNs) have proven to be an effective approach for 3D human pose estimation. By naturally modeling the skeleton structure of the human body as a graph, GCNs are able to capture the spatial relationships between joints and learn an efficient representation of the underlying pose. However, most GCN-based methods use a shared weight matrix, making it challenging to accurately capture the different and complex relationships between joints. In this paper, we introduce an iterative graph filtering framework for 3D human pose estimation, which aims to predict the 3D joint positions given a set of 2D joint locations in images. Our approach builds upon the idea of iteratively solving graph filtering with Laplacian regularization via the Gauss–Seidel iterative method. Motivated by this iterative solution, we design a Gauss–Seidel network (GS-Net) architecture, which makes use of weight and adjacency modulation, skip connection, and a pure convolutional block with layer normalization. Adjacency modulation facilitates the learning of edges that go beyond the inherent connections of body joints, resulting in an adjusted graph structure that reflects the human skeleton, while skip connections help maintain crucial information from the input layer’s initial features as the network depth increases. We evaluate our proposed model on two standard benchmark datasets, and compare it with a comprehensive set of strong baseline methods for 3D human pose estimation. Our experimental results demonstrate that our approach outperforms the baseline methods on both datasets, achieving state-of-the-art performance. Furthermore, we conduct ablation studies to analyze the contributions of different components of our model architecture and show that the skip connection and adjacency modulation help improve the model performance.}
}
@article{ZHAO2023103913,
title = {TASTNet: An end-to-end deep fingerprinting net with two-dimensional attention mechanism and spatio-temporal weighted fusion for video content authentication},
journal = {Journal of Visual Communication and Image Representation},
volume = {96},
pages = {103913},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103913},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001633},
author = {Gejian Zhao and Fengyong Li and Heng Yao and Chuan Qin},
keywords = {Robust video fingerprinting, Discrimination, Two-dimensional attention, Spatio-temporal fusion},
abstract = {In this paper, a robust fingerprinting scheme for video content authentication with two-dimensional attention mechanism and spatio-temporal weighted fusion called TASTNet is proposed, which can automatically extracts key spatio-temporal features from the input video and maps them to the corresponding fingerprint. Detailedly, the two-dimensional attention mechanism is applied to resist different kinds of digital manipulations for robustness enhancement. To incorporate perceptual characteristics, a spatio-temporal weighted fusion method based on LTSM is presented to integrate frame-level features into video-level features while retaining the temporal order. In the process of fusion, key frames are allocated with larger weights according to inter-frame correlation. With these two steps, we can obtain representative video features that contain principal perception information. In addition, the proposed scheme utilizes deep metric learning for training, and we design multiple constraints to make the generated fingerprint more compact and discriminable. Extensive experiments demonstrate that our scheme can achieve superior performances with respect to robustness and discrimination compared with some state-of-the-art schemes.}
}
@article{PANG2023103937,
title = {Vehicle re-identification based on grouping aggregation attention and cross-part interaction},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103937},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103937},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001876},
author = {Xiyu Pang and Xin Tian and Xiushan Nie and Yilong Yin and Gangwu Jiang},
keywords = {Vehicle re-identification, Grouping aggregation attention, Cross-part interaction},
abstract = {Vehicle re-identification (Re-ID) plays an important role in intelligent transportation systems. To solve the key problem of small inter-class difference, we propose a vehicle Re-ID network based on grouping aggregation attention and cross-part interaction (GACP), which uses a channel context local interaction attention block (CCLI) and a cross-part interaction module (CPIM) to extract global and local discriminative features, respectively. The CCLI constructs the context information of each channel and performs local interaction to realize the communication of local and long-range information, thus effectively infer the weights of channels. In addition, the CCLI further reduces the number of parameters and improves the attention effect through a grouping aggregation module and an attention enhancement constraint. The CPIM enhances the correlation between vehicle parts obtained by rigid segmentation on feature maps to mine robust local information. Extensive experiments on two popular datasets VeRi776 and VehicleID demonstrate the effectiveness of the proposed method.}
}
@article{LIU2023103885,
title = {A no-reference panoramic image quality assessment with hierarchical perception and color features},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103885},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103885},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001359},
author = {Yun Liu and Xiaohua Yin and Chang Tang and Guanghui Yue and Yan Wang},
keywords = {Omnidirectional images, No-reference quality assessment, Hierarchical perception, Color information},
abstract = {The ultimate goal of no-reference omnidirectional image quality assessment (NR-OIQA) is to design a comprehensive perception method that can accurately assess the quality of damaged omnidirectional images without prior knowledge. However, most existing studies cannot attain credible accuracy because of lacking a non-neuroscience-based or non-biology-based model. Inspired by this, an original visual perception-based and neuroscience-based OIQA model by considering the hierarchical perception features of the HVS, which includes specific information, local saliency information, global information, and color information which is often ignored by researchers is proposed in this work. According to the hierarchical process in neuroscience, the high-frequency co-occurrence matrix (HFCM)-based and variance-based specific features are applied to perceive details that are first distorted in the frequency domain. The entropy-based combination of the paranormal saliency map(PSM) and superpixel segmentation with the simple linear iterative clustering (SLIC) algorithm is employed to emphasize the rich quality-aware local saliency information. The global panoramic statistical (GPS) model is utilized to express global semantics distortion as the high-level feature. Visual-aware color texture descriptor with the cross-channel local binary pattern(CCLBP) which effectively reflects the correlation and dependency of pixels between different color channels is employed to map color information. Finally, all above features have been extracted and combined with subjective scores to assess the objective quality scores by support vector regression (SVR). Experiments express that our method has more accuracy and stronger stability on CVIQD2018 and OIQA databases.}
}
@article{S2023103982,
title = {HY-LSTM: A new time series deep learning architecture for estimation of pedestrian time to cross in advanced driver assistance system},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103982},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103982},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002328},
author = {Veluchamy S and Michael Mahesh K and Muthukrishnan R and Karthi S},
keywords = {Advanced driver assistance system, Pedestrian time, LSTM, OCNN, Deep joint segmentation},
abstract = {Advanced driver assistance systems (ADASs), particularly pedestrian protection systems (PPSs), have emerged as a hot research topic with the goal of enhancing traffic safety. The development of reliable on-board pedestrian detection systems is a critical problem for PPSs. It is extremely difficult to provide the required resilience of this type of system due to the fluctuating look of pedestrians (e.g., varied attire, changing size, aspect ratio, and dynamic shape) and the unstructured surroundings. The detection of pedestrians has gained huge focus among researchers because of its huge applications in the domain of automated vehicles. In the previous decades, the majority of examinations are done to obtain better solutions for detecting pedestrians, but fewer of them concentrated on determining the pedestrian. It is one of crucial interest to study regarding transport safety as it implies minimizing the count of traffic collisions and protecting pedestrians who are more susceptible to accidents. Predicting pedestrian conduct is critical for road safety, traffic management systems, ADAS, and autonomous vehicles in general. The fundamental problem in the field of self-driving and smart automobiles is identifying impediments, particularly people, and taking action to avoid collisions with them. Various studies have been conducted in this sector by many researchers, yet there are still many mistakes in the proper identification of pedestrians. Hence, this paper devises an approach to estimate pedestrian time for crossing in an advanced driving assistance system (ADAS). The inputted videos undergo keyframe extraction wherein crucial keyframes are extracted. The deep joint segmentation is further applied for identifying the pedestrian followed by intention classification. Then, the estimation of pedestrian time to cross is predicted using Hybrid-Long short term memory (HY-LSTM), and it is a new time series model obtained by unifying LSTM and Object-based convolution neural network (OCNN), where the layer and hyperparameters of OCNN are optimally derived using Gradient Chef Based Optimization (GCBO). The proposed GCBO-HY-LSTM outperformed showing the least Mean absolute error (MAE) of 0.038, Mean square error (MSE) of 0.029, and Root Mean square error (RMSE) of 0.170.}
}
@article{QIU2023103909,
title = {A dual-task region-boundary aware neural network for accurate pulmonary nodule segmentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {96},
pages = {103909},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103909},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001591},
author = {Junrong Qiu and Bin Li and Riqiang Liao and Hongqiang Mo and Lianfang Tian},
keywords = {Pulmonary nodules, Segmentation, Region-boundary joint learning, Multi-task learning},
abstract = {Recently, the performance of pulmonary nodule segmentation has improved fast because of the development of deep learning-based methods. However, existing CNN-based methods still don’t solve the two issues, leading to poor segmentation: (1) The information of small nodules lost in continuous down-sampling operation; (2) Discriminative features can’t be extracted from non-solid nodules because whose gray values are close to the surrounding environment. This study proposes a novel dual-task region-boundary aware deep convolutional neural network to solve the problems above. A hierarchical feature module is proposed to capture multi-scale information, improving small nodules' segmentation accuracy. Boundary guided module is introduced to extract edge features and predict nodule boundaries directly. In addition, a feature aggregation module is proposed to aggregate features at different levels. Besides, to better optimize the segment results and reduce overfitting, the region-boundary aware loss function is proposed. The method is trained and tested on the LIDC-IDRI dataset and the LUNA16 dataset, and the results demonstrate that our method is efficient. More importantly, our method is suitable for the segmentation of various nodules, especially small nodules, GGO nodules, and part-solid nodules.}
}
@article{ALSHAMI2023103954,
title = {Pose2Trajectory: Using transformers on body pose to predict tennis player’s trajectory},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103954},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103954},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002043},
author = {Ali AlShami and Terrance Boult and Jugal Kalita},
keywords = {Body joints, Object detection, Human pose estimation},
abstract = {Tracking the trajectory of tennis players can help camera operators in production. Predicting future movement enables cameras to automatically track and predict a player’s future trajectory without human intervention. It is also intellectually satisfying to predict future human movement in the context of complex physical tasks. Swift advancements in sports analytics and the wide availability of videos for tennis have inspired us to propose a novel method called Pose2Trajectory, which predicts a tennis player’s future trajectory as a sequence derived from their body joints’ data and ball position. Demonstrating impressive accuracy, our approach capitalizes on body joint information to provide a comprehensive understanding of the human body’s geometry and motion, thereby enhancing the prediction of the player’s trajectory. We use encoder–decoder Transformer architecture trained on the joints and trajectory information of the players with ball positions. The predicted sequence can provide information to help close-up cameras to keep tracking the tennis player, following centroid coordinates. We generate a high-quality dataset from multiple videos to assist tennis player movement prediction using object detection and human pose estimation methods. It contains bounding boxes and joint information for tennis players and ball positions in singles tennis games. Our method shows promising results in predicting the tennis player’s movement trajectory with different sequence prediction lengths using the joints and trajectory information with the ball position.}
}
@article{FAN2023103978,
title = {Multi-scale dynamic fusion for correcting uneven illumination images},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103978},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103978},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002286},
author = {Junyu Fan and Jinjiang Li and Lu Ren and Zheng Chen},
keywords = {Uneven illumination image correction, Multi-scale dynamic fusion, Balance factor},
abstract = {Images taken under non-ideal lighting conditions often suffer from uneven illumination, resulting in image distortion and unclear details. To address these issues, researchers have developed various methods for image enhancement. However, most of these methods are only applicable to specific types of images, especially those with underexposed exposures. Therefore, to achieve this goal, in this article, we propose a multi-scale dynamic fusion method for correcting uneven illumination images. This method can balance the overall illumination of unevenly illuminated images captured under non-ideal lighting conditions, while maintaining the original brightness contrast and enhancing the image details. We constructed a multi-branch multi-scale fully convolutional neural network, which uses attention mechanisms to focus on the bright and dark areas that need to be processed in the main branch, while the side branch is used to maintain the image’s color and naturalness. Extracting features at different scales is beneficial for detail recovery, while a larger receptive field results in better brightness contrast and a more realistic visual experience. Finally, the attention fusion method is used to fuse the features of different branches to obtain the corrected results.}
}
@article{KSHIRSAGAR2023103901,
title = {YOLOv3-based human detection and heuristically modified-LSTM for abnormal human activities detection in ATM machine},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103901},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103901},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001517},
author = {Aniruddha Prakash Kshirsagar and H. Azath},
keywords = {Human tracking, Abnormal human activities detection, Bank-automated teller machines, You only look once, Version 3, Enhanced long short-term memory, Hybrid spider monkey-chicken swarm optimization},
abstract = {In existing works, accurately analyzing human activities is a complicated problem in public places. Consequently, the detection of human activities becomes challenging the computer vision technology. The major scope of the research is to develop an abnormal Human Activity Recognition (HAR) model using deep structured architectures for detecting the suspicious activities of humans in the ATM using the video surveillance system. The classification phase utilizes the enhanced deep learning approach named improved Long Short-Term Memory (LSTM) by optimizing certain parameters in LSTM by hybrid optimization algorithm for accurately classifying the normal and abnormal activities of humans. This hybrid optimization algorithm is developed and termed Hybrid Spider Monkey-Chicken Swarm Optimization (HSM-CSO) for achieving the effective performance of the deep learning-based classification. Hence, the designed HAR model in ATM is proven that it helps to improve the system performance and also give relief from prohibited activities or crimes and false alarms for humans.}
}
@article{LU2023103926,
title = {Underwater image enhancement method based on denoising diffusion probabilistic model},
journal = {Journal of Visual Communication and Image Representation},
volume = {96},
pages = {103926},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103926},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001761},
author = {Siqi Lu and Fengxu Guan and Hanyu Zhang and Haitao Lai},
keywords = {Denoising diffusion probabilistic model (DDPM), Underwater image enhancement, Deep learning, Underwater image restoration},
abstract = {Underwater images often suffer from severe distortion, which seriously affects the image quality and the application. Current underwater image enhancement methods have poor generalization capability and cannot be adapted to all types of underwater images. In recent years, the diffusion model based on the denoising diffusion probabilistic model (DDPM) has achieved excellent results in various fields of computer vision. Inspired by the DDPM, an underwater image enhancement method based on the DDPM (UW-DDPM) was proposed in this paper. The UW-DDPM trained on paired datasets and utilized two U-Net networks to complete image denoising as well as image distribution transformation, which effectively improved the quality of underwater images. By testing on real underwater image datasets, UW-DDPM achieved better improvement in visual effects and evaluation metrics than the existing model.}
}
@article{ZHU2023103874,
title = {Learning knowledge representation with meta knowledge distillation for single image super-resolution},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103874},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103874},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001244},
author = {Han Zhu and Zhenzhong Chen and Shan Liu},
keywords = {Knowledge distillation, Single image super-resolution, Representation of knowledge, Meta learning, Texture-aware dynamic kernel},
abstract = {Although the deep CNN-based super-resolution methods have achieved outstanding performance, their memory cost and computational complexity severely limit their practical employment. Knowledge distillation (KD), which can efficiently transfer knowledge from a cumbersome network (teacher) to a compact network (student), has demonstrated its advantages in some computer vision applications. The representation of knowledge is vital for knowledge transferring and student learning, which is generally defined in hand-crafted manners or uses the intermediate features directly. In this paper, we propose a model-agnostic meta knowledge distillation method under the teacher–student architecture for the single image super-resolution task. It provides a more flexible and accurate way to help teachers transmit knowledge in accordance with the abilities of students via knowledge representation networks (KRNets) with learnable parameters. Specifically, the texture-aware dynamic kernels are generated from local information to decompose the distillation problem into texture-wise supervision for further promoting the recovery quality of high-frequency details. In addition, the KRNets are optimized in a meta-learning manner to ensure the knowledge transferring and the student learning are beneficial to improving the reconstructed quality of the student. Experiments conducted on various single image super-resolution datasets demonstrate that our proposed method outperforms existing defined knowledge representation-related distillation methods and can help super-resolution algorithms achieve better reconstruction quality without introducing any extra inference complexity.}
}
@article{MA2023103971,
title = {Semantic segmentation with cross convolution and multi-layer feature refinement},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103971},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103971},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002213},
author = {Yingdong Ma and Nan Jing},
keywords = {Semantic segmentation, Cross convolution, Multi-scale context, Feature fusion},
abstract = {Multi-level features and contextual information have been proven effective in semantic segmentation. As a common solution, dilated convolution with multiple dilation rates is widely adopted by various computer vision tasks. However, since a large number of pixels are not involved in convolutional calculation, dilated convolution suffers from the information loss problem. Another important aspect for image segmentation is that most feature fusion methods combine different level features directly, which ignores the semantic gap between shallow layer features and deep layer features. In this work, we propose the multi-scale cross convolution to alleviate the information loss problem. Cross convolution conducts convolutional operations in horizontal and vertical direction with different kernels. By combining cross convolution with dilated convolutions using different convolution kernels, more pixels are engaged in convolutional operation to capture multi-scale features. To address the issue of semantic gap between multi-layer features, a feature fusion scheme is developed in which a dual attention mechanism is applied to conduct feature refinement in both spatial and channel dimensions. Comprehensive experiments are conducted to evaluate the proposed method on Cityscapes and ADE20K datasets. Experimental results demonstrate that the cross convolution and feature fusion method improve segmentation performance significantly and achieve competitive performance over state-of-the-art approaches.}
}
@article{ZHANG2023103898,
title = {Transformer-based global–local feature learning model for occluded person re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103898},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103898},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001487},
author = {Guoqing Zhang and Chao Chen and Yuhao Chen and Hongwei Zhang and Yuhui Zheng},
keywords = {Occluded person re-identification, Vision transformer, Feature fusion},
abstract = {Most recent occluded person re-identification (re-ID) methods usually learn global features directly from pedestrian images, or use additional pose estimation and semantic analysis model to learn local features, while ignoring the relationship between global and local features, thus incorrectly retrieving different pedestrians with similar attributes as the same pedestrian. Moreover, learning local features using auxiliary models brings additional computational cost. In this work, we propose a Transformer-based dual-branch feature learning model for occluded person re-ID. Firstly, we propose a global–local feature interaction module to learn the relationship between global and local features, thus enhancing the richness of information in pedestrian features. Secondly, we randomly erase local areas in the input image to simulate the real occlusion situation, thereby improving the model’s adaptability to the occlusion scene. Finally, a spilt group module is introduced to explore the local distinguishing features of pedestrian. Numerous experiments validate the effectiveness of our proposed method.}
}
@article{XIANG2023103890,
title = {EMHIFormer: An Enhanced Multi-Hypothesis Interaction Transformer for 3D human pose estimation in video},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103890},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103890},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001402},
author = {Xuezhi Xiang and Kaixu Zhang and Yulong Qiao and Abdulmotaleb El Saddik},
keywords = {3D human pose estimation, Transformer, Cross-hypothesis, Enhanced regression head},
abstract = {Monocular 3D human pose estimation is a challenging task because of depth ambiguity and occlusion. Recent methods exploit spatio-temporal information and generate different hypotheses for simulating diverse solutions to alleviate these problems. However, these methods do not fully extract spatial and temporal information and the relationship of each hypothesis. To ease these limitations, we propose EMHIFormer (Enhanced Multi-Hypothesis Interaction Transformer) to model 3D human pose with better performance. In detail, we build connections between different Transformer layers so that our model is able to integrate spatio-temporal information from the previous layer and establish more comprehensive hypotheses. Furthermore, a cross-hypothesis model consisting of a parallel Transformer is proposed to strengthen the relationship between various hypotheses. We also design an enhanced regression head which adaptively adjusts the channel weights to export the final 3D human pose. Extensive experiments are conducted on two challenging datasets: Human3.6M and MPI-INF-3DHP to evaluate our EMHIFormer. The results show that EMHIFormer achieves competitive performance on Human3.6M and state-of-the-art performance on MPI-INF-3DHP. Compared with the closest counterpart, MHFormer, our model outperforms it by 0.6% P-MPJPE and 0.5% MPJPE on Human3.6M dataset and 46.0% MPJPE on MPI-INF-3DHP.}
}
@article{TANG2023103967,
title = {SAE-PPL: Self-guided attention encoder with prior knowledge-guided pseudo labels for weakly supervised video anomaly detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103967},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103967},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002171},
author = {Jun Tang and Zhentao Wang and Guanyu Hao and Ke Wang and Yan Zhang and Nian Wang and Dong Liang},
keywords = {Weakly supervised video anomaly detection, Self-training, Multiple instance learning, Attention mechanism},
abstract = {Recently, many weakly supervised video anomaly detection (WS-VAD) methods focus on generating reasonable pseudo-labels for frames by using video-level annotations. To achieve this goal, some existing label-noise cleaning techniques and pseudo-label generators are used by them, but their performance is limited. The main reason is that the useful prior knowledge, i.e., the graduality of anomaly events, is ignored. Here, we propose a Self-Training Framework, which assumes flexible soft boundaries between abnormal and normal clips. The framework consists of: (1) A prior knowledge guided pseudo-label generator that incorporates prior knowledge of video segment distributions into the MIL framework to generate high-confidence pseudo labels; (2) An improved self-guided attention encoder is developed to capture multiscale long-term spatiotemporal features, where dependencies among anomaly frames are preserved. Moreover, a pseudo-label based self-training scheme is adopted to supervise the encoder. Experimental results verify the superiority of our method over baseline approaches.}
}
@article{ZHU2023103886,
title = {Multiscale residual gradient attention for face anti-spoofing},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103886},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103886},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001360},
author = {Shiwei Zhu and Shijun Xiang},
keywords = {Face anti-spoofing, Residual gradient convolution, Adjacent depth loss, Local depth auxiliary supervision},
abstract = {In the field of face anti-spoofing (FAS), how to extract the representative features to distinguish between real and spoof faces and train the corresponding deep networks are two vital issues. In this paper, we propose a simple but effective end-to-end FAS model based on an innovative texture extractor and a depth auxiliary supervision mechanism. In the feature extraction stage, we first design the residual gradient convolutions based on the redesigned gradient operators, which are used to extract fine-grained texture features. The extraction of texture features is based on multiple scales by dividing the texture differences between living and spoofing faces into three levels reasonably. Then we construct a multiscale residual gradient attention (MRGA) to obtain representative texture features from multiple levels texture features. By combining the proposed feature extractor MRGA and existing vision transformer (ViT), the MRGA-ViT is proposed to generate related semantics and obtain final classification results. In the training stage, we also propose a local depth auxiliary supervision based on a novel adjacent depth loss, which utilizes the correlation information of adjacent pixels adequately compared with traditional depth loss. The proposed MRGA-ViT model achieves competitive performance in generalization and stability ability, e.g., the ACER(%) values of intra testing on OULU-NPU database are 1.8, 2.6, 1.6 ± 1.2 and 1.9 ± 2.7 respectively, the AUC(%) of cross type testing attains 99.45 ± 0.57, the ACER(%) values of cross dataset testing are 28.1 and 36.7 respectively. Experimental results prove that the proposed model is competitive to other state-of-the-art works on generalization and stability performance.}
}
@article{XING2023103955,
title = {Learning full context feature for human motion prediction},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103955},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103955},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002055},
author = {Huiqin Xing and Yicong Zhou and Jianyu Yang and Yang Xiao},
keywords = {Human motion prediction, Context feature, Motion constraint, Dictionary},
abstract = {Human motion prediction aims to predict the target poses given the previous poses. Most existing methods are devoted to extracting richer motion features from only the given previous poses to predict the target poses. However, we consider that the post poses after the target poses are helpful in acquiring the context feature and constraint between neighbor motions, which is also important for motion prediction. In this paper, we explore to make use of the post motion information for a powerful human motion prediction method. Specifically, we propose a human motion prediction model which learns the motion constraint from both the previous and post poses, in order to fully utilize the context features of the target poses. During training, the proposed memory dictionary module is used to learn the mapping from previous features to post features. In testing, the proposed memory dictionary module fully exploits the learned mapping to calculate the future motion feature according to the input previous feature. Thus, the context feature of human motion is enriched in our method. We evaluate the proposed method on two large-scale datasets, Human3.6M and CMU-Mocap. The experimental results demonstrate that our method improves the motion prediction performance, especially for long-term human motion.}
}
@article{ZHANG2023103979,
title = {A no-reference underwater image quality evaluator via quality-aware features},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103979},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103979},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002298},
author = {Siqi Zhang and Yuxuan Li and Lu Tan and Huan Yang and Guojia Hou},
keywords = {Underwater image, No-reference image quality assessment, Quality-aware features, Gaussian process regression},
abstract = {In this paper, we propose a novel no-reference evaluator based on quality-aware features, called QA-UIQE, for underwater image quality assessment. QA-UIQE extracts and fuses a set of quality-aware features including naturalness, color, contrast, sharpness, and structure. Technically, we first present a new color-cast weighted colorfulness measurement as well as color consistency measurement to characterize color, and design a saliency-weighted contrast measurement to improve the distinguishing ability of measuring contrast. Also, the locally mean subtracted and contrast normalized, maximum local variation, and local entropy are incorporated to measure naturalness, sharpness and structure, respectively. Afterward, we integrate the feature vectors extracted from the training set into Gaussian process regression to predict the image quality. Moreover, we collect a real-world underwater image dataset for testing the generalization ability of our method. The experimental results illustrate that our QA-UIQE has a superior prediction accuracy and is highly consistent with human visual perception.}
}