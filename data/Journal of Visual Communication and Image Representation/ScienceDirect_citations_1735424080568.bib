@article{ZHU2021103000,
title = {Circular intra prediction for 360 degree video coding},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {103000},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.103000},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302169},
author = {Linwei Zhu and Yun Zhang and Na Li and Jinyong Pi and Shiqi Wang},
keywords = {Circular intra prediction, Projection deformation, 360 degree video, Versatile video coding},
abstract = {Different from traditional 2D video, the contents of 360 degree video are deformed due to the projection from 3D sphere to 2D plane. As a result, the traditional Angular Intra Prediction (AIP) with a linear pattern may not be always efficient. To further improve the coding performance of 360 degree video, a novel intra prediction method is presented in this paper, i.e., Circular Intra Prediction (CIP), which takes consideration of the spherical characteristics of 360 degree video. In specific, the proposed CIP is performed in a circular pattern, where the center of circle is located around the to-be-predicted block, and different centers of circle are able to produce different CIP modes. The distance between center of this circle and center of the to-be-predicted block is adaptively determined according to the degree of projection deformation, where stronger projection deformation needs shorter distance, and vice versa. As the increase of the distance, the CIP is more and more close to the traditional AIP. In addition, one additional binary flag is utilized to achieve better coding performance from the competition between AIP and CIP with the rate-distortion optimization. The proposed algorithm is implemented on the platform of Versatile video coding Test Model (VTM) 5.0 + 360Lib 9.1. Extensive experiments show that the proposed method can achieve bit rate reduction on this platform for 360 degree video coding.}
}
@article{WU2021103048,
title = {Salient object detection via a boundary-guided graph structure},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103048},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103048},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000213},
author = {Yunhe Wu and Tong Jia and Yu Pang and Jiaduo Sun and Dingyu Xue},
keywords = {Salient object detection, Coarse saliency map, Weighting integration framework, Boundary-guided graph structure, Adaptive strategy, Iterative propagation mechanism},
abstract = {Graph-based salient object detection methods have gained more and more attention recently. However, existing works fail to separate effectively salient object and background in some challenging scenes. Inspired by this observation, we propose an effective salient object detection method based on a novel boundary-guided graph structure. More specifically, the input image is firstly segmented into a series of superpixels. Then we integrate two prior cues to generate the coarse saliency map, a novel weighting mechanism is proposed to balance the proportion of two prior cues according to their performance. Secondly, we propose a novel boundary-guided graph structure to explore deeply the intrinsic relevance between superpixels. Based on the proposed graph structure, an iterative propagation mechanism is constructed to refine the coarse saliency map. Experimental results on four datasets show adequately the superiority of the proposed method than other state-of-the-art methods.}
}
@article{XIAO2021103038,
title = {Real-time video super-resolution using lightweight depthwise separable group convolutions with channel shuffling},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103038},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103038},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100016X},
author = {Zhijiao Xiao and Zhikai Zhang and Kwok-Wai Hung and Simon Lui},
keywords = {Super-resolution, Lightweight alignment module, Channel shuffle, Residual networks},
abstract = {In recent years, convolutional neural networks (CNNs) have accelerated the developments of video super resolution (SR) for achieving higher image quality. However, the computational cost of existing CNN-based video super-resolution is too heavy for real-time applications. In this paper, we propose a new video super-resolution framework using lightweight frame alignment module and well-designed up-sampling module for real-time processing. Specifically, our framework, which is called as Lightweight Shuffle Video Super-Resolution Network (LSVSR), combines channel shuffling, depthwise convolution and pointwise group convolution to significantly reduce the computational burden during frame alignment and high-resolution frame reconstruction. On the public benchmark datasets, our proposed network outperforms the state-of-the-art lightweight video SR networks in terms of objective (PSNR and SSIM) and subjective evaluations, number of network parameters and floating-point operations. Our network can achieve real-time 540P to 2160P 4× super-resolution for more than 60fps using desktop GPUs or mobile phones with neural processing unit.}
}
@article{WENG2021102932,
title = {High capacity reversible data hiding in encrypted images using SIBRW and GCC},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {102932},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102932},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301632},
author = {Shaowei Weng and Caiying Zhang and Tiancong Zhang and Kaimeng Chen},
keywords = {Reversible data hiding, Image encryption, SIBRW, GCC, Image recovery, Embedding performance},
abstract = {In this paper, a reversible data hiding in encrypted images (RDHEI) method combining GCC (group classification encoding) and SIBRW containing sixteen image-based rearrangement ways is proposed to achieve high-capacity data embedding in encrypted images. Each way of SIBRW aims at bringing strongly-correlated bits of each higher bit-plane together by rearranging each higher bit-plane. For each higher bit-plane, the optimal way achieving the most concentrated aggregation performance is selected from SIBRW to rearrange this bit-plane, and then, GCC compresses the rearranged bit-plane in group-by-group manner. By making full use of strong-correlation between adjacent groups, GCC can compress not only consecutive several groups whose bits are valued 1 (or 0) but also a single group so that a large embedding space is provided. The encryption method including the bit-level XOR-encryption and scrambling operations enhances the security. The experimental results show that the proposed scheme can achieve large embedding capacity and high security.}
}
@article{GONG2021102961,
title = {Person Re-identification with Global-Local Background_bias Net},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102961},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102961},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301875},
author = {Yuxiu Gong and Ronggui Wang and Juan Yang and Lixia Xue and Min Hu},
keywords = {Person Re-identification, Body misalignment, Foreground features, Background information},
abstract = {Person Re-identification (Re-ID) is an important technique in intelligent video surveillance. Because of the variations on camera viewpoints and body poses, there are some problems such as body misalignment, the diverse background clutters and partial bodies occlusion, etc. To address these problems, we propose the Global-Local Background_bias Net (GLBN), a novel network architecture that consists of Foreground Partial Segmentation Net (FPSN), Global Aligned Supervision Net (GASN) and Background_bias Constraint Net (BCN) modules. Firstly, to enhance the adaptability of foreground features and reduce the interference of the background, FPSN is applied to perform local segmentation on the foreground image. Secondly, global features generated by GASN are purposed to supervise the learning of local features. Finally, BCN constrains the background information to reduce the impact of background information again. Extensive experiments implemented on the mainstream evaluation datasets including Market1501, DukeMTMC-reID and CUHK03 indicate that our method is efficient and robust.}
}
@article{WANG2021103006,
title = {Reliable fusion of ToF and stereo data based on joint depth filter},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {103006},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.103006},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302200},
author = {Xuanyin Wang and Tianpei Lin and Xuesong Jiang and Ke Xiang and Feng Pan},
keywords = {ToF, Stereo vision, Data fusion, 3D block matching, Seed-growing},
abstract = {To obtain reliable depth images with high resolution, a novel method is proposed in this study that fuses data acquired from time-of-flight (ToF) and stereo cameras, through which the advantages of both active and passive sensing are utilised. Based on the classic error model of the ToF, gradient information is introduced to establish the likelihood distribution for all disparity candidates. The stereo likelihood is estimated in parallel based on a 3D adaptive support-weight approach. The two independent likelihoods are unified using a maximum likelihood estimation, a process also referred to as a joint depth filter herein. Conventional post-processing methods such as a mutual consistency check are also used after applying a joint depth filter. We also propose a novel hole-filling method based on the seed-growing algorithm to retrieve missing disparities. Experiment results show that the proposed fusion method can produce reliable high-resolution depth maps and outperforms other compared methods.}
}
@article{SUN2020102948,
title = {Automated work efficiency analysis for smart manufacturing using human pose tracking and temporal action localization},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102948},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102948},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301759},
author = {Hao Sun and Guanghan Ning and Zhiqun Zhao and Zhongchao Huang and Zhihai He},
keywords = {Smart manufacturing, Deep learning, Human pose estimation, Dynamic time warping, Temporal activity localization, Generative adversarial networks},
abstract = {In this paper, we aim to develop an automatic system to monitor and evaluate worker’s efficiency for smart manufacturing based on human pose tracking and temporal action localization. First, we explore the generative adversarial networks (GANs) to achieve significantly improved estimation of human body joints. Second, we formulate the automated worker efficiency analysis into a temporal action localization problem in which the action video performed by the worker is matched against a reference video performed by a teacher. We extract invariant spatio-temporal features from the human body pose sequences and perform cross-video matching using dynamic time warping. Our proposed human pose estimation method achieves state-of-the-art performance on the benchmark dataset. Our automated work efficiency analysis is able to achieve action localization with an average IoU (intersection over union) score large than 0.9. This represents one of the first systems to provide automated worker efficiency evaluation.}
}
@article{HAITFRAENKEL2021103041,
title = {Revealing stable and unstable modes of denoisers through nonlinear eigenvalue analysis},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103041},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103041},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000171},
author = {Ester Hait-Fraenkel and Guy Gilboa},
keywords = {Eigenfunctions, Nonlinear operators, Denoising, Power iteration, Total-variation, EPLL},
abstract = {In this paper, we propose to analyze stable and unstable modes of black-box image denoisers through nonlinear eigenvalue analysis. We aim to find input images for which the denoiser output is proportional to the input. We treat this as a generalized nonlinear eigenproblem. Potential implications are wide, as most image processing algorithms can be viewed as black-box operators. We introduce a generalized nonlinear power-method to solve eigenproblems for such operators. This allows us to reveal stable modes of the denoiser: optimal inputs, achieving superior PSNR in noise removal. Analogously to the linear case, such stable modes show coarse structures and correspond to large eigenvalues. We also provide a method to generate unstable modes, which the denoiser suppresses strongly, which are textural with small eigenvalues. We validate the method using total-variation (TV) and demonstrate it on the EPLL (Zoran–Weiss) and the Non-local means denoisers. Finally, we suggest an encryption–decryption application.}
}
@article{2023103841,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103841},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(23)00091-3},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000913}
}
@article{NATH2021103061,
title = {Activity recognition in video sequences over qualitative abstracts of a diagram-based representation schema},
journal = {Journal of Visual Communication and Image Representation},
volume = {76},
pages = {103061},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103061},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000298},
author = {Chayanika D. Nath and Shyamanta M. Hazarika},
keywords = {Cognitive vision, Video analysis, Activity recognition, Qualitative spatial and temporal reasoning, Diagrammatic reasoning},
abstract = {Explicit reasoning over a spatial substrate, i.e., space–time information structures underlying a spatial problem, simplifies reasoning. Diagrammatic reasoning makes use of diagrams for exploiting such underlying structures. This paper proposes a novel approach combining diagrammatic reasoning with qualitative spatial and temporal reasoning techniques to visualize and perceive spatio-temporal relations among objects in a video. The hybrid techniques explore information over the spatial substrate for relational extractions. Different relations among objects in transition define short-term activities. Mealy machines are learned over patterns of short-term activities as activity recognizers. The proposed representation and recognition mechanism is validated by conducting experiments for video activity recognition from DARPA Mind’s Eye and J-HMDB dataset.}
}
@article{2023103813,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {92},
pages = {103813},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(23)00063-9},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000639}
}
@article{WANG2020102913,
title = {A heuristic framework for perceptual saliency prediction},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102913},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102913},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301516},
author = {Yongfang Wang and Peng Ye and Yumeng Xia and Ping An},
keywords = {Saliency prediction, Orientation selectivity, Visual acuity, Visual error sensitivity, Free energy principle},
abstract = {Saliency prediction can be regarded as the human spontaneous activity. The most effective saliency model should highly approximate the response of viewers to the perceived information. In the paper, we exploit the perception response for saliency detection and propose a heuristic framework to predict salient region. First, to find the perceptually meaningful salient regions, an orientation selectivity based local feature and a visual Acuity based global feature are proposed to jointly predict candidate salient regions. Subsequently, to further boost the accuracy of saliency map, we introduce a visual error sensitivity based operator to activate the meaningful salient regions from a local and global perspective. In addition, an adaptive fusion method based on free energy principle is designed to combine the sub-saliency maps from each image channel to obtain the final saliency map. Experimental results on five natural and emotional datasets demonstrate the superiority of the proposed method compared to twelve state-of-the-art algorithms.}
}
@article{2023103762,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103762},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(23)00012-3},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000123}
}
@article{CHEN2023103802,
title = {Corrigendum to “FFTI: Image inpainting algorithm via features fusion and two-steps inpainting” [J. Visual Commun. Image Represent. 91 (2023) 103776]},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103802},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103802},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000524},
author = {Yuantao Chen and Runlong Xia and Ke Zou and Kai Yang}
}
@article{WU2021102985,
title = {Non-maximum suppression for object detection based on the chaotic whale optimization algorithm},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102985},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102985},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302042},
author = {Guixian Wu and Yuancheng Li},
keywords = {Post-processing step, Object detection, Non-maximum suppression},
abstract = {Non-maximum suppression (NMS) as a post-processing step for object detection is mainly used to remove redundant bounding boxes in the object and plays a vital role in many detectors. Its positioning accuracy mainly depends on the bounding box with the highest score, and this strategy is difficult to eliminate the false positive. In order to solve the problem, this paper regards the post-processing step as a combinatorial optimization problem and combines the chaotic whale optimization algorithm and non-maximum suppression. The chaotic search method is used to generate an initial combinatorial solution, and the whale optimization algorithm is discretized to create an updated combinatorial strategy. Under the guidance of the fitness function, the optimal combination is searched. In addition, the method of difference set area (DSA) is proposed to optimize the final detection result. The experiment uses the current mainstream framework Faster R-CNN as the detector on PASCAL VOC2012, COCO2017 and the Warships datasets. The experimental results show that the proposed method can significantly improve the average precision (AP) of detectors compared with the most advanced methods.}
}
@article{ZABIHI2020102931,
title = {Exploiting object features in deep gaze prediction models},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102931},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102931},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301620},
author = {Saman Zabihi and Eghbal Mansoori and Mehran Yazdi},
keywords = {Human visual system, Deep gaze prediction model, Image segmentation, Object detection, Prior bias, Convolutional network structure},
abstract = {The human visual system analyzes the complex scenes rapidly. It devotes the limited perceptual resources to the most salient subsets and/or objects of scenes while ignoring their less salient parts. Gaze prediction models try to predict the human eye fixations (human gaze) under free-viewing conditions while imitating the attentive mechanism. Previous studies on saliency benchmark datasets have shown that visual attention is affected by the salient objects of the scenes and their features. These features include the identity, the location, and the visual features of objects in the scenes, beside to the context of the input image. Moreover, the human eye fixations often converge to the specific parts of salient objects in the scenes. In this paper, we propose a deep gaze prediction model using object detection via image segmentation. It uses some deep neural modules to find the identity, location, and visual features of the salient objects in the scenes. In addition, we introduce a deep module to capture the prior bias of human eye fixations. To evaluate our model, several challenging saliency benchmark datasets are used in the experiments. We also conduct an ablation study to show the effectiveness of our proposed modules and its architecture. Despite its fewer parameters, our model has comparable, or even better performance on some datasets, to the state-of-the-art saliency models.}
}
@article{ASAD2021103047,
title = {Anomaly3D: Video anomaly detection based on 3D-normality clusters},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103047},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103047},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000201},
author = {Mujtaba Asad and Jie Yang and Enmei Tu and Liming Chen and Xiangjian He},
keywords = {Spatiotemporal latent features, 3D-CAE, Anomaly detection, Video analysis, Autonomous video surveillance},
abstract = {Abnormal behavior detection in surveillance videos is necessary for public monitoring and safety. In human-based surveillance systems, it requires continuous human attention and observation, which is a difficult task. The autonomous detection of such events is of essential significance. However, due to the scarcity of labeled data and the low occurrence probability of these events, abnormal event detection is a challenging vision problem. In this paper, we introduce a novel two-stage architecture for detecting anomalous behavior in videos. In the first stage, we propose a 3D Convolutional Autoencoder (3D-CAE) architecture to extract spatio-temporal features from normal event training videos. In 3D-CAE, the encoder and decoder architectures are based on 3D convolutions, which can learn both appearance and the motion features effectively in an unsupervised manner. In the second stage, we group the 3D spatio-temporal features into different normality clusters, and then remove the sparse clusters to represent a stronger pattern of normality. From these clusters, one-class SVM classifier is used to distinguish between normal and abnormal events based on the normality scores. Experimental results on four different benchmark datasets show significant performance improvement compared to state-of-the-art approaches while providing results in real-time.}
}
@article{ZAFARI2020102962,
title = {Resolving overlapping convex objects in silhouette images by concavity analysis and Gaussian process},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102962},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102962},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301863},
author = {Sahar Zafari and Mariia Murashkina and Tuomas Eerola and Jouni Sampo and Heikki Kälviäinen and Heikki Haario},
keywords = {Segmentation, Overlapping objects, Convex objects, Image processing, Computer vision, Gaussian process, Kriging, Branch and bound},
abstract = {This paper introduces a novel method for segmentation of clustered partially overlapping convex objects in silhouette images. The proposed method involves three main steps: pre-processing, contour evidence extraction, and contour estimation. Contour evidence extraction starts by recovering contour segments from a binarized image by detecting concave points. After this the contour segments which belong to the same objects are grouped. The grouping is formulated as a combinatorial optimization problem and solved using the branch and bound algorithm. Finally, the full contours of the objects are estimated by a Gaussian process regression method. The experiments on a challenging dataset consisting of nanoparticles demonstrate that the proposed method outperforms three current state-of-art approaches in overlapping convex objects segmentation. The method relies only on edge information and can be applied to any segmentation problems where the objects are partially overlapping and have a convex shape.}
}
@article{MA2021103066,
title = {Steganalytic feature based adversarial embedding for adaptive JPEG steganography},
journal = {Journal of Visual Communication and Image Representation},
volume = {76},
pages = {103066},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103066},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000328},
author = {Sai Ma and Xianfeng Zhao},
keywords = {Steganography, Adversarial embedding, Non-data-driven},
abstract = {In this paper, we present a novel adversarial embedding scheme named Steganalytic Feature based Adversarial Embedding (SFAE), which is elaborately designed in a non-data-driven style. Firstly, a novel DCTR based adversary is designed to generate adversarial stego images which can not only resist feature based steganalysis but also deep learning based steganalysis. Specifically, our adversary consists of an end-to-end neural network structure, while its inner weights are set according to DCTR rather than learned from datasets. Secondly, we use the minimum distance to the cover in steganalytic space as the criterion to select the optimal adversarial stego image, rather than fooling the adversary. Last but not least, we present two SFAE implementations to adapt to different cases. One is Iterative SFAE, which needs to calculate gradients multiple times. Iterative SFAE is more secure but has higher complexity. It fits the case that the steganographer has adequate computing resources. Another implementation is Oneshot SFAE, which can calculate gradients once. Oneshot SFAE trades the security for lower complexity. It fits the steganographer that has stricter requirements for running time. Experiments demonstrate that SFAE is effective to improve the security of conventional steganographic schemes against the state-of-the-art steganalysis including both feature based steganalysis and deep learning based steganalysis.}
}
@article{CHEN2021102999,
title = {Long-term rate control for concurrent multipath real-time video transmission in heterogeneous wireless networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {102999},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102999},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302157},
author = {Feng Chen and Jie Zhang and Mingkui Zheng and Jiyan Wu and Nam Ling},
keywords = {Visual communications, Video coding, Lyapunov optimization, Heterogeneous wireless networks, Multi-homing, Rate control},
abstract = {Concurrent multipath transmission provides an effective solution for streaming high-quality mobile videos in heterogeneous wireless networks. Rate control is commonly adopted in multimedia communication systems to fully utilize the available network bandwidth. This paper proposes a novel rate control for concurrent multipath video transmission. The existing rate control algorithms mainly adapt bit rate in the short-term pattern, i.e., without considering the long-term video transmission quality. We propose a long-term rate control scheme that takes into account the status of both the transmission buffer and video frames. First, a mathematical model is developed to formulate the non-convex problem of long-term quality maximization. Second, we develop a dynamic programming solution for online encoding bit rate control based on buffer status. The performance evaluation is conducted in a real test bed over LTE and Wi-Fi networks. Experimental results demonstrate that the proposed long-term rate control scheme achieves appreciable improvements over the short-term rate control schemes in terms of video quality and delay performance.}
}
@article{ONGUN2020102970,
title = {Recognition of occupational therapy exercises and detection of compensation mistakes for Cerebral Palsy},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102970},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102970},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301905},
author = {Mehmet Faruk Ongun and Uğur Güdükbay and Selim Aksoy},
keywords = {Gesture recognition, Cerebral palsy, Occupational therapy, Compensation mistake, Hidden Markov model, Virtual rehabilitation},
abstract = {Depth camera-based virtual rehabilitation systems are gaining attention in occupational therapy for cerebral palsy patients. When developing such a system, domain-specific exercise recognition is vital. To design such a gesture recognition method, some obstacles need to be overcome: detection of gestures not related to the defined exercise set and recognition of incorrect exercises performed by the patients to compensate for their lack of ability. We propose a framework based on hidden Markov models for the recognition of upper extremity functional exercises. We determine critical compensation mistakes together with restrictions for classifying these mistakes with the help of occupational therapists. We first eliminate undefined gestures by evaluating two models that produce adaptive threshold values. Then we utilize specific negative models based on feature thresholding and train them for each exercise to detect compensation mistakes. We perform various tests using our method in a laboratory environment under the supervision of occupational therapists.}
}
@article{LI2021103058,
title = {A lightweight multi-scale aggregated model for detecting aerial images captured by UAVs},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103058},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103058},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000286},
author = {Zhaokun Li and Xueliang Liu and Ye Zhao and Bo Liu and Zhen Huang and Richang Hong},
keywords = {Aerial images, UAVs, Small-size targets, Mutil-scale aggregation, Attention, Model compression},
abstract = {Detecting the objects of interesting from aerial images captured by UAVs is one of the core modules in the UAV-based applications. However, it is very difficult to detection objects from aerial images. The reason is that the scale of objects in the aerial images captured by UAVs varies greatly and needs to meet certain real-time performance in detection. To deal with these challenges, we proposed a lightweight model named DSYolov3. We made the following improvements to the Yolov3 model: 1) multiple scale-aware decision discrimination network to detect objects in different scales, 2) a multi-scale fusion-based channel attention model to exploit the channel-wise information complementation, 3) a sparsity-based channel pruning to compress the model. Extensive experimental evaluation has demonstrated the effectiveness and efficiency of our approach. By the proposed approach, we could not only achieve better performance than most existing detectors but also ensure the models practicable on the UAVs.}
}
@article{YIN2020102945,
title = {Multi-stage all-zero block detection for HEVC coding using machine learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102945},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102945},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301747},
author = {Haibing Yin and Haoyun Yang and Xiaofeng Huang and Hongkui Wang and Chenggang Yan},
keywords = {Multi-stage AZB detection, Rate-distortion optimization, Soft-decision quantization, Machine learning},
abstract = {Compared with deadzone hard-decision quantization (HDQ), rate-distortion optimized quantization (RDOQ) in HEVC brings non-negligible coding gain, however consumes considerable computations caused by exhaustive search over multiple candidates to determine optimal output level. Benefiting from efficient prediction in HEVC, transform blocks are frequently quantized to all zero, especially in small-size blocks. It is worthwhile to detect all zero block (AZB) for transform blocks to bypass subsequent computation-intensive RDOQ. Traditional thresholding based AZB detection algorithms are well-suited for deadzone quantized blocks, however miss partial optimal results in RDOQ and suffer from more or less accuracy degradation in RDOQ. This paper proposes a novel multi-stage AZB detection algorithm for RDOQ blocks with good tradeoff between complexity and accuracy. At the first stage, genuine all zero blocks (G_AZB) which are quantized to all zero both in HDQ and RDOQ are prejudged by comparison with conservative threshold determined by mathematical derivation for deadzone HDQ. At the second stage, an adaptive threshold model is built using adaptive deadzone offset by simulating the behavior patterns existing in RDOQ, aiming to further detect the pseudo AZB (P_AZB) which are quantized to all zero in RDOQ however not all zero in HDQ. At the final stage, machine learning based detection is proposed to classify the remaining “cunning” all zero blocks using eight distinguished RDO-related features, by which subtle working mechanism in RDOQ is leveraged. The experimental results demonstrate that the proposed algorithm achieves up to 7.471% total coding computation saving with 0.064% BD-RATE increment compared with RDOQ on average. Moreover, the average FNR and FPR detection accuracies are 6.3% and 6.5% respectively.}
}
@article{FU2021102982,
title = {Conditional generative adversarial network for EEG-based emotion fine-grained estimation and visualization},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102982},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102982},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302030},
author = {Boxun Fu and Fu Li and Yi Niu and Hao Wu and Yang Li and Guangming Shi},
keywords = {Affective computing, Electroencephalography, Generative adversarial network, Fine-grained},
abstract = {In the field of affective computing (AC), coarse-grained AC has been developed and widely applied in many fields. Electroencephalogram (EEG) signals contain abundant emotional information. However, it is difficult to develop fine-grained AC due to the lack of fine-grained labeling data and suitable visualization methods for EEG data with fine labels. To achieve a fine mapping of EEG data directly to facial images, we propose a conditional generative adversarial network (cGAN) to establish the relationship between EEG data associated with emotions, a coarse label, and a facial expression image in this study. In addition, a corresponding training strategy is also proposed to realize the fine-grained estimation and visualization of EEG-based emotion. The experiments prove the reasonableness of the proposed method for the generation of fine-grained facial expressions. The image entropy of the generated image indicates that the proposed method can provide a satisfactory visualization of fine-grained facial expressions.}
}
@article{SHAO2021103017,
title = {Strong ghost removal in multi-exposure image fusion using hole-filling with exposure congruency},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103017},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.103017},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302273},
author = {Hua Shao and Mei Yu and Gangyi Jiang and Zhiyong Pan and Zongju Peng and Fen Chen},
keywords = {Multi-exposure image fusion, Weak ghost, Strong ghost, Ghost removal, Exposure congruency, Hole-filling},
abstract = {It is the most crucial problem to remove ghost in the multi-exposure image fusion of dynamic scene. The traditional fusion methods have good effects to remove weak ghosts. However, they cannot effectively remove strong ghosts. This paper proposes a new strong ghost removal method in multi-exposure image fusion using hole-filling with exposure congruency. First, analyzing the characteristics of strong ghosts, a detection scheme for strong ghost regions is designed by combining histogram matching and exposure difference detection. Subsequently, to effectively extract image local features, a multi-scale fusion network for non-strong ghost regions is designed to obtain a pre-fused image. Further, based on the distribution characteristics of strong ghosts, a hole-filling model with exposure congruency is designed to remove the strong ghosts. Experimental results show that compared with the state-of-the-art methods, the proposed method can obtain better performance in both of subjective and objective evaluation, particularly in terms of effectively removing strong ghosts.}
}
@article{2022103700,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103700},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(22)00220-6},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002206}
}
@article{XIE2021103010,
title = {Semantic-aware visual attributes learning for zero-shot recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {103010},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.103010},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302224},
author = {Yurui Xie and Tiecheng Song and Wei Li},
keywords = {Zero-shot learning, Human-designed attributes, Visual attributes, Semantic representation},
abstract = {Zero-shot learning (ZSL) aims to recognize unseen image classes without requiring any training samples of these specific classes. The ZSL problem is typically achieved by building up a semantic embedding space like attributes to bridge the visual features and class labels of images. Currently, most ZSL approaches focus on learning a visual-semantic alignment from seen classes using only the human-designed attributes, and then ZSL problem is solved by transferring semantic knowledge from seen classes to the unseen classes. However, few works indicate if the human-designed attributes are discriminative enough for image class prediction. To address this issue, we propose a semantic-aware dictionary learning (SADL) framework to explore these discriminative visual attributes across seen and unseen classes. Furthermore, the semantic cues are elegantly integrated into the feature representations via learned visual attributes for recognition task. Experiments conducted on two challenging benchmark datasets show that our approach outweighs other state-of-the-art ZSL methods.}
}
@article{ZHAO2021103036,
title = {Various density light field image coding based on distortion minimization interpolation},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103036},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103036},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000158},
author = {Shengyang Zhao and Zhibo Chen},
keywords = {Light field image, View partition, Camera array, Image compression, View synthesis, Linear approximation},
abstract = {In recent years, the light field (LF) as a new imaging modality has attracted wide interest. The large data volume of LF images poses great challenge to LF image coding, and the LF images captured by different devices show significant differences in angular domain. In this paper we propose a view prediction framework to handle LF image coding with various sampling density. All LF images are represented as view arrays. We first partition the views into reference view (RV) set and intermediate view (IV) set. The RVs are rearranged into a pseudo sequence and directly compressed by a video encoder. Other views are then predicted by the RVs. To exploit the four dimensional signal structure, we propose the linear approximation prior (LAP) to reveal the correlation among LF views and efficiently remove the LF data redundancy. Based on the LAP, a distortion minimization interpolation (DMI) method is used to predict IVs. To robustly handle the LF images with different sampling density, we propose an Iteratively Updating depth image based rendering (IU-DIBR) method to extend our DMI. Some auxiliary views are generated to cover the target region and then the DMI calculates reconstruction coefficients for the IVs. Different view partition patterns are also explored. Extensive experiments on different types LF images also valid the efficiency of the proposed method.}
}
@article{LIU2021102971,
title = {A novel (k1,k2,n)-threshold two-in-one secret image sharing scheme for multiple secrets},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102971},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102971},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301930},
author = {Lintao Liu and Yuliang Lu and Xuehu Yan},
keywords = {Ideal secret sharing, RGVCS, Polynomial-based SISS, TiOSISS, Multiple secrets, -threshold},
abstract = {A two-in-one secret image sharing scheme (TiOSISS) is the combination of two different secret image sharing schemes (SISSs), which has advantages of both schemes, such as the simple stacking-to-see property and precise recovery with computing devices available. Since most of current TiOSISSs depend on steganography techniques, it results in several drawbacks, such as large pixel expansion and poor visual quality with shares stacking. Besides, researchers ignore the independence between two different SISSs, that is, both SISSs should deal with irrelevant secret images with different thresholds. By controlling the randomness of the sharing phase according to constraints from both SISSs, we combine polynomial-based SISS (PSISS) and random-grid-based visual cryptography scheme (RGVCS) together, and propose an ideal (k1,k2,n)-threshold TiOSISS for multiple secrets. The proposed TiOSISS not only overcomes drawbacks above, but also has high scalability to improve its performances by utilizing different RGVCSs. Sufficient analyses and experiments are provided to verify its security and effectiveness.}
}
@article{KHARGHANIAN2021103062,
title = {Pain detection using batch normalized discriminant restricted Boltzmann machine layers},
journal = {Journal of Visual Communication and Image Representation},
volume = {76},
pages = {103062},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103062},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000316},
author = {Reza Kharghanian and Ali Peiravi and Farshad Moradi and Alexandros Iosifidis},
keywords = {Pain detection, Convolutional deep belief network, Discriminant Feature Learning, Representation learning, Batch Normalization},
abstract = {A system for automatic pain detection whereby pain-related features are extracted from facial images using a four-layer Convolutional Deep Belief Network (CDBN) is proposed in this study. The CDBN is trained by greedy layer-wise procedure whereby each added layer is trained as a Convolutional Restricted Boltzmann Machine (CRBM) by contrastive divergence. Since conventional CRBM is trained in a purely unsupervised manner, there is no guarantee that learned features are appropriate for the supervised task at hand. A discriminative objective based on between-class and within-class distances is proposed to adapt CRBM to learn task-related features. When discriminative and generative objectives are appropriately combined, a competitive classification performance can be achieved. Moreover, we introduced batch normalization (BN) units in the structure of the CRBM model to smooth optimization landscape and speed up the learning process. BN units come right before sigmoid units. Extracted features are then used to train a linear SVM to classify each frame into pain or no-pain classes. Extensive experiments on UNBC-McMaster Shoulder Pain database demonstrate the effectiveness of the proposed method for automatic pain detection.}
}
@article{RAZASHAHID2020102949,
title = {Contour and region harmonic features for sub-local facial expression recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102949},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102949},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301772},
author = {Ali {Raza Shahid} and Sheheryar Khan and Hong Yan},
keywords = {Contour description, Facial expression recognition, Local facial shape harmonics, Region description},
abstract = {Expression recognition relies on intensity, edges, and geometry that overlooks the actual shape curvatures of facial regions. This paper presents a novel two-stage approach to distinguish seven expressions on the basis of eleven different facial areas. The combination of contour and region harmonics is used to develop the interrelationship of sub-local areas in the human face for expression recognition. We applied a multi-class support vector machine (SVM) with subject dependent k-fold cross-validation to classify the human emotions into expressions. We tested our proposed method on three public facial expression datasets for sub-local regions in human face and achieved 94.90%, 93.43%, and 92.57% recognition rate for the CK+, CFEE, and MUG datasets respectively. Experiments show that the contour and region harmonics have high classification power and can be computed efficiently. Our method provides higher accuracy, less computing time, and less memory space than existing techniques, including deep learning.}
}
@article{WANG2021102953,
title = {Unsupervised video object segmentation with distractor-aware online adaptation},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102953},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102953},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301814},
author = {Ye Wang and Jongmoo Choi and Yueru Chen and Siyang Li and Qin Huang and Kaitai Zhang and Ming-Sui Lee and C.-C. Jay Kuo},
keywords = {Unsupervised video object segmentation, Pseudo ground truth, Motion saliency, Hard negative mining, Online adaptation},
abstract = {Unsupervised video object segmentation is a crucial application in video analysis when there is no prior information about the objects. It becomes tremendously challenging when multiple objects occur and interact in a video clip. In this paper, a novel unsupervised video object segmentation approach via distractor-aware online adaptation (DOA) is proposed. DOA models spatiotemporal consistency in video sequences by capturing background dependencies from adjacent frames. Instance proposals are generated by the instance segmentation network for each frame and they are grouped by motion information as positives or hard negatives. To adopt high-quality hard negatives, the block matching algorithm is then applied to preceding frames to track the associated hard negatives. General negatives are also introduced when there are no hard negatives in the sequence. The experimental results demonstrate these two kinds of negatives are complementary. Finally, we conduct DOA using positive, negative, and hard negative masks to update the foreground and background segmentation. The proposed approach achieves state-of-the-art results on two benchmark datasets, the DAVIS 2016 and the Freiburg-Berkeley motion segmentation (FBMS)-59.}
}
@article{SRAN2021102964,
title = {Integrating saliency with fuzzy thresholding for brain tumor extraction in MR images},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102964},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102964},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301899},
author = {Paramveer Kaur Sran and Savita Gupta and Sukhwinder Singh},
keywords = {Saliency, Fuzzy, Segmentation, ROI, Medical Images},
abstract = {The automatic detection and extraction of tumor area in Magnetic Resonance Imaging (MRI) is an important and challenging task. This paper presents a fully automatic and unsupervised method for fast and accurate extraction of brain tumor area from MR images. The proposed method named as Saliency Based Segmentation (SBS) is based on visual saliency. The saliency model detects the pathologically important area and then fuzzy thresholding is used for extraction of the detected region. The performance of SBS is compared with Adaptively Regularized Kernel-Based Fuzzy C-Means Clustering, Mean Shift and Fuzzy C-Means clustering with Level Set Method. The experimental evaluation validated on BRATS database using Jaccard index (0.84 ± 0.04), Dice Index (0.91 ± 0.02), Execution time (2.99 ± 0.29), Precision (0.82 ± 0.16), Recall (0.97 ± 0.03) and F-measure (0.88 ± 0.10) demonstrates that SBS achieves better segmentation results even in the presence of noise and uneven illumination in images.}
}
@article{GAUTAM2021102993,
title = {A Model-based dehazing scheme for unmanned aerial vehicle system using radiance boundary constraint and graph model},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102993},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102993},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302108},
author = {Sidharth Gautam and Tapan Kumar Gandhi and B.K. Panigrahi},
keywords = {Remote sensing, Satellite imaging, Unmanned aerial vehicle imaging, Image restoration, Image enhancement, Image dehazing},
abstract = {Unmanned aerial vehicle system (UAVs) imaging has become a challenging area of research due to the dynamic atmospheric environment. The images captured by UAVs are often deteriorated by factors such as clouds occlusion, poor atmospheric illumination, and limited capability of the imaging system. To tackle problems, this paper presents a novel visibility restoration scheme for UAVs images by considering the following two assumptions: (1) The actual scene radiance of a UAVs image is bounded. (2) Pixels sharing the same appearance must have the same transmission value in a local neighborhood. Inspired by above assumptions, an image boundary constraint utilizing the median filter has been imposed on the RGB channel for the rough estimation of transmission-map in aerial images. Furthermore, a graph-model based optimization technique has been used for the transmission-map refinement. The experimental results demonstrate the efficiency of the proposed method in terms of metrics correspond to the human-visual-system (HVS).}
}
@article{LYU2021103057,
title = {Copy Move Forgery Detection based on double matching},
journal = {Journal of Visual Communication and Image Representation},
volume = {76},
pages = {103057},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103057},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000274},
author = {Qiyue Lyu and Junwei Luo and Ke Liu and Xiaolin Yin and Jiarui Liu and Wei Lu},
keywords = {Digital forensics, Copy move forgery detection, Delaunay triangle, Double matching},
abstract = {Copy Move is a technique widespreadly used in digital image tampering, meaning Copy Move Forgery Detection (CMFD) is still a significant research. In this paper, a novel CMFD method is proposed, including double matching process and region localizing process. In double matching process, the first matching is conducted on Delaunay triangles consisting of Local Intensity Order Pattern (LIOP) keypoints, to find the approximate location of suspicious regions. In order to find sufficient keypoint pairs, the existing set of matching triangles is expanded by adding their neighbors iteratively, covering the whole tampered regions, and the second matching with a looser threshold is conducted on the vertices. In the region localizing process, considering the case of multiple copies, Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is used to classify the keypoint pairs described in a new model. Experimental results indicate that the proposed method, with good robustness, outperforms some state-of-the-art methods.}
}
@article{CHEN2020102967,
title = {Image splicing localization using residual image and residual-based fully convolutional network},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102967},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102967},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301929},
author = {Beijing Chen and Xiaoming Qi and Yang Zhou and Guanyu Yang and Yuhui Zheng and Bin Xiao},
keywords = {Splicing localization, Fully convolutional network, Residual block, Residual image, Condition random field},
abstract = {Fully convolutional networks (FCNs) have been efficiently applied in splicing localization. However, the existing FCN-based methods still have three drawbacks: (a) their performance in detecting image details is unsatisfactory; (b) deep FCNs are difficult to train; (c) results of multiple FCNs are merged using fixed parameters to weigh their contributions. So, an improved method is proposed. Firstly, both the original spliced image and its corresponding residual image are regarded as the inputs of the network. Secondly, the residual block is introduced into FCN as residual-based FCN (RFCN) to make the network easier to optimize. Thirdly, three different RFCNs are merged to enhance locating maps with two learnable weight parameters. Besides, condition random field is introduced into the whole network to improve the results further. Experimental results on five datasets show that the proposed method performs better than some existing methods in localization ability, generalization ability, and robustness against additional operations.}
}
@article{ZAMIRI2021103003,
title = {Image annotation based on multi-view robust spectral clustering},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {103003},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.103003},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302182},
author = {Mona Zamiri and Hadi {Sadoghi Yazdi}},
keywords = {Image annotation, Geo-tagged photos, Recommender systems, Maximum correntropy criterion, Multi-view spectral clustering, Geographical information},
abstract = {Nowadays, image annotation has been a hot topic in the semantic retrieval field due to the abundant growth of digital images. The purpose of these methods is to realize the content of images and assign appropriate keywords to them. Extensive efforts have been conducted in this field, which effectiveness is limited between low-level image features and high-level semantic concepts. In this paper, we propose a Multi-View Robust Spectral Clustering (MVRSC) method, which tries to model the relationship between semantic and multi-features of training images based on the Maximum Correntropy Criterion. A Half-Quadratic optimization framework is used to solve the objective function. According to the constructed model, a few tags are suggested based on a novel decision-level fusion distance. The stability condition and bound calculation of MVRSC are analyzed, as well. Experimental results on real-world Flickr and 500PX datasets, and Corel5K confirm the superiority of the proposed method over other competing models.}
}
@article{QIN2021103072,
title = {Semantic loop closure detection based on graph matching in multi-objects scenes},
journal = {Journal of Visual Communication and Image Representation},
volume = {76},
pages = {103072},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103072},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000389},
author = {Cao Qin and Yunzhou Zhang and Yingda Liu and Guanghao Lv},
keywords = {Loop closure detection, Object detection, Semantic, Simultaneous localization and mapping (SLAM), Graph matching},
abstract = {Robust loop-closure detection is essential for visual SLAM. Traditional methods often focus on the geometric and visual features in most scenes but ignore the semantic information provided by objects. Based on this consideration, we present a strategy that models the visual scene as semantic sub-graph by only preserving the semantic and geometric information from object detection. To align two sub-graphs efficiently, we use a sparse Kuhn–Munkres algorithm to speed up the search for correspondence among nodes. The shape similarity and the Euclidean distance between objects in the 3-D space are leveraged unitedly to measure the image similarity through graph matching. Furthermore, the proposed approach has been analyzed and compared with the state-of-the-art algorithms at several datasets as well as two indoor real scenes, where the results indicate that our semantic graph-based representation without extracting visual features is feasible for loop-closure detection at potential and competitive precision.}
}
@article{TANG2021103039,
title = {Single image deraining using Context Aggregation Recurrent Network},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103039},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103039},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000146},
author = {Qunfang Tang and Jie Yang and Haibo Liu and Zhiqiang Guo and Wenjing Jia},
keywords = {Image deraining, Context awareness, Dilated convolution, Recurrent network, Perceptual loss},
abstract = {Single image deraining is a challenging problem due to the presence of non-uniform rain densities and the ill-posedness of the problem. Moreover, over-/under-deraining can directly impact the performance of vision systems. To address these issues, we propose an end-to-end Context Aggregation Recurrent Network, called CARNet, to remove rain streaks from single images. In this paper, we assume that a rainy image is the linear combination of a clean background image with rain streaks and propose to take advantage of the context information and feature reuse to learn the rain streaks. In our proposed network, we first use the dilation technique to effectively aggregate context information without sacrificing the spatial resolution, and then leverage a gated subnetwork to fuse the intermediate features from different levels. To better learn and reuse rain streaks, we integrate a LSTM module to connect different recurrences for passing the information learned from the previous stages about the rain streaks to the following stage. Finally, to further refine the coarsely derained image, we introduce a refinement module to better preserve image details. As for the loss function, the L1-norm perceptual loss and SSIM loss are adopted to reduce the gridding artifacts caused by the dilated convolution. Experiments conducted on synthetic and real rainy images show that our CARNet achieves superior deraining performance both qualitatively and quantitatively over the state-of-the-art approaches.}
}
@article{CHEN2020102934,
title = {Improving the representation of image descriptions for semantic image retrieval with RDF},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102934},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102934},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301644},
author = {Hua Chen and AiBin Guo and Wenlong Ni and Yan Cheng},
keywords = {Image representation, RDF, Image retrieval, Cross-lingual retrieval, Semantic image retrieval},
abstract = {The past few years have witnessed a surge of interest in many topics at the intersection of natural language processing and computer vision. In particular, using objects together with their attributes and relations to represent images or interpret languages has been proved useful across a wide variety of applications. The goal of this work is to provide an improved RDF-based model to represent images for enhancing textual based image retrieval. We use natural language processing tools to obtain a set of objects, attributes and relations; and then model them into graphical structures with RDF-based model. We also conduct some preliminary experiments to show how to handle textual based image retrieval for complex queries or multilingual queries. The experimental results show that our approach improves the representation of image descriptions, which is suitable for enhancing image retrieval with high-level semantics.}
}
@article{CHTOUROU2021103093,
title = {Person re-identification based on gait via Part View Transformation Model under variable covariate conditions},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103093},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103093},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000547},
author = {Imen Chtourou and Emna Fendri and Mohamed Hammami},
keywords = {Re-identification, Gait, Part View Transformation Model (PVTM), View angle variation, Covariate factors},
abstract = {Human gait represents an attractive biometric modality to re-identify a person as it requires non contact and it is perceivable at a distance. However, the view angle variation and the presence of covariate factors cause significant difficulties for recognizing gaits. In order to deal with such constraints, this paper presents a Part View Transformation Model (PVTM) for gait based applications. Compared with previous methods, the PVTM is applied on selected relevant parts chosen through a semantic classification step. Conducted on the CASIA-B gait database, experimental results show that the proposed method outperforms well known multi-view methods even under covariate factors (i.e. carrying bag, clothing).}
}
@article{CHEN2021103060,
title = {High-capacity reversible data hiding in encrypted image based on Huffman coding and differences of high nibbles of pixels},
journal = {Journal of Visual Communication and Image Representation},
volume = {76},
pages = {103060},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103060},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000304},
author = {Chih-Cheng Chen and Chin-Chen Chang and Kaimeng Chen},
keywords = {Reversible data hiding, Image encryption, Huffman coding, High nibble},
abstract = {In this paper, we propose a new reversible data hiding method in encrypted images. Due to spatial correlation, there is a large probability that the adjacent pixels of the image have small differences, which is especially obvious on the high four most significant bits (high nibbles) of the pixels. If the high nibble of each pixel is regarded as a 4-bit value, the differences between the high nibbles of the adjacent pixels are mostly concentrated in a small range. Based on this fact, Huffman coding was used to encode all the differences between the high nibbles of the adjacent pixels in order to compress the four most significant bit (MSB) planes efficiently and create a large-capacity room. After creating room, a stream cipher is used to encrypt the image, and the room is reserved in the encrypted image for data hiding without losing information. The experimental results showed that the proposed method can achieve a larger embedding rate and better visual quality of the marked decrypted image than other related methods.}
}
@article{XIONG2020102947,
title = {A large-scale remote sensing database for subjective and objective quality assessment of pansharpened images},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102947},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102947},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301760},
author = {Yiming Xiong and Feng Shao and Xiangchao Meng and Qiuping Jiang and Weiwei Sun and Randi Fu and Yo-Sung Ho},
keywords = {Pansharpened image quality assessment, Pansharpened Remote Sensing Image Quality Database (PRSIQD), Panchromatic, Multispectral, Pansharpening, Remote sensing},
abstract = {Pansharpening is a process to fuse a low spatial resolution multispectral image and a high spatial resolution panchromatic image to produce a high-resolution multispectral image. Quality assessment of pansharpened images is challenging due to without actual reference images. There are two main types of assessment methods: reduced resolution (RR) assessment based on Wald’s protocol, and full resolution (FR) assessment without reference. Currently, it is lack of large-scale benchmark databases for subjective and objective performance evaluation of different image pansharpening methods. In this paper, we construct a large-scale database named Pansharpened Remote Sensing Image Quality Database (PRSIQD) from both qualitative and quantitative perspectives, which contains 13,620 pansharpened images acquired from IKONOS, QuickBird, Gaofen-1, WorldView-2, WorldView-3 and WorldView-4 satellite sensors. In addition, we have comprehensively analyzed the advantages and disadvantages of the existing pansharpening quality assessment methods on different satellite sensors, thematic datasets and bands.}
}
@article{SAHU2021103008,
title = {Single image dehazing using a new color channel},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {103008},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.103008},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302212},
author = {Geet Sahu and Ayan Seal and Ondrej Krejcar and Anis Yazidi},
keywords = {Image dehazing, Atmospheric light, Radiance, Illuminance scaling factor},
abstract = {Images with hazy scene suffer from low-contrast, which reduces the visible quality of the scene, thus making object detection a more challenging task. Low-contrast can result from foggy weather conditions during image acquisition. Dehazing is a process of removal of haze from the photography of a hazy scene. Single-image dehazing based on dark channel priors are well-known techniques in this field. However, the performance of such techniques is limited to priors or constraints. Moreover, this type of method fails when images have sky-region. So, a method is proposed, which can restore the visibility of hazy images. First, a hazy image is divided into blocks of size 32 × 32, then the score of each block is calculated to select a block having the highest score. Atmospheric light is calculated from the selected block. A new color channel is considered to remove atmospheric scattering, obtained channel value and atmospheric light are then used to calculate the transmission map in the second step. Third, radiance is computed using a transmission map and atmospheric light. The illumination scaling factor is adopted to enhance the quality of a dehazed image in the final step. Experiments are performed on six datasets namely, I-HAZE, O-HAZE, BSDS500, FRIDA, RESIDE dataset and natural images from Google. The proposed method is compared against 11 state-of-the-art methods. The performance is analyzed using fourteen quantitative evaluation metrics. All the results demonstrate that the proposed method outperforms 11 state-of-the-art methods in most of the cases.}
}
@article{GENG2020102914,
title = {Exploiting multigranular salient features with hierarchical multi-mode attention network for pedestrian re-IDentification},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102914},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102914},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301528},
author = {Yanbing Geng and Yongjian Lian and Mingliang Zhou and Yixue Kong and Yinong Zhu},
keywords = {Pedestrian re-identification, Hierarchical, Multi-mode attention network, Hierarchical adaptive fusion, Fused attention},
abstract = {In this paper, we propose an end-to-end hierarchical-based multi-mode attention network and adaptive fusion (HMAN-HAF) strategy to learn different-level salient features for re-ID tasks. First, according to each layer’s characteristics, a hierarchical multi-mode attention network (HMAN) is designed to adopt different attention models for different-level salient feature learning. Specifically, refined channel-wise attention (CA) is adopted to capture high-level valuable semantic information, an attentive region model (AR) is used to detect salient regions in the low layer, and fused attention (FA) is designed to capture the salient regions of valuable channels in the middle layer. Second, a hierarchical adaptive fusion (HAF) is constructed to fulfill the complementary strengths of different-level salient features. Experimental results demonstrate that the proposed method outperforms the state-of-the-art methods on the following challenging benchmarks: Market-1501, DukeMTMC-reID and CUHK03.}
}
@article{CAO2021102963,
title = {Densely connected network with improved pyramidal bottleneck residual units for super-resolution},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102963},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102963},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301887},
author = {Feilong Cao and Baijie Chen},
keywords = {Super-resolution, Convolution neural network, Pyramidal network, Residual-feature learning},
abstract = {Recent studies have shown that super-resolution can be significantly improved by using deep convolution neural network. Although applying a larger number of convolution kernels can extract more features, increasing the number of feature mappings will dramatically increase the training parameters and time complexity. In order to balance the workload among all units and maintain appropriate time complexity, this paper proposes a new network structure for super-resolution. For the sake of making full use of context information, in the structure, the operations of division (S) and fusion (C) are added to the pyramidal bottleneck residual units, and the dense connected methods are used. The proposed network include a preliminary feature extraction net, seven residual units with dense connections, seven convolution layers with the size of 1×1 after each residual unit, and a deconvolution layer. The experimental results show that the proposed network has better performance than most existing methods.}
}
@article{SONG2021103055,
title = {Human pose estimation and its application to action recognition: A survey},
journal = {Journal of Visual Communication and Image Representation},
volume = {76},
pages = {103055},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103055},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000262},
author = {Liangchen Song and Gang Yu and Junsong Yuan and Zicheng Liu},
keywords = {Pose estimation, Action recognition},
abstract = {Human pose estimation aims at predicting the poses of human body parts in images or videos. Since pose motions are often driven by some specific human actions, knowing the body pose of a human is critical for action recognition. This survey focuses on recent progress of human pose estimation and its application to action recognition. We attempt to provide a comprehensive review of recent bottom-up and top-down deep human pose estimation models, as well as how pose estimation systems can be used for action recognition. Thanks to the availability of commodity depth sensors like Kinect and its capability for skeletal tracking, there has been a large body of literature on 3D skeleton-based action recognition, and there are already survey papers such as [1] about this topic. In this survey, we focus on 2D skeleton-based action recognition where the human poses are estimated from regular RGB images instead of depth images. We summarize the performance of recent action recognition methods that use pose estimated from color images as input, then show that there is much room for improvements in this direction.}
}
@article{XU2023103809,
title = {Corrigendum to “Generative detect for occlusion object based on occlusion generation and feature completing” [J. Visual Commun. Image Represent. 78 (2021) 103189]},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103809},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103809},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000597},
author = {Can Xu and Wenxi Lang and Rui Xin and Kaichen Mao and Haiyan Jiang}
}
@article{TINNATHI2021102966,
title = {An efficient copy move forgery detection using adaptive watershed segmentation with AGSO and hybrid feature extraction},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102966},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102966},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301917},
author = {Sreenivasu Tinnathi and G. Sudhavani},
keywords = {Copy-move forgery detection, Segments, Adaptive Galactic Swarm Optimization, RANSAC, Adaptive thresholding},
abstract = {Copy-move forgery detection (CMFD) is the process of determining the presence of copied areas in an image. CMFD approaches are mainly classified into two groups: keypoint-based and block-based techniques. In this paper, a new CMFD approach is proposed on the basis of both block and keypoint based approaches. Initially, the forged image is partitioned into non overlapped segments utilizing adaptive watershed segmentation, wherein adaptive H-minima transform is used for extracting the markers. Also, an Adaptive Galactic Swarm Optimization (AGSO) algorithm is used to select optimal gap parameter while selecting the markers for reducing the undesired regional minima, which can increase the segmentation performance. After that, the features from every segment are extracted as segment features (SF) using Hybrid Wavelet Hadamard Transform (HWHT). Then, feature matching is performed using adaptive thresholding. The false matches or outliers can be removed with the help of Random Sample Consensus (RANSAC) algorithm. Finally, the Forgery Region Extraction Algorithm (FREA) is utilized for detecting the copied portion from the host image. Experimental results indicate that the proposed scheme find out image forgery region with Precision = 92.45%; Recall = 93.67% and F1 = 92.75% on MICC-F600 dataset and Precision = 94.52%; Recall = 95.32% and F1 = 93.56% on Bench mark dataset at pixel level. Also, it outperforms the existing approaches when the image undergone certain geometrical transformation and image degradation.}
}
@article{2022103535,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103535},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(22)00076-1},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000761}
}
@article{YAO2021102986,
title = {Motion vector modification distortion analysis-based payload allocation for video steganography},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102986},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102986},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302054},
author = {Yuanzhi Yao and Nenghai Yu},
keywords = {Video steganography, Motion vector, Payload allocation, Motion vector modification distortion, Residue deviation propagation},
abstract = {Video steganography forms a covert communication channel by data embedding in cover elements. To consider inter-frame mutual embedding impacts, this paper proposes a payload allocation strategy in video steganography based on motion vector modification distortion analysis. Firstly, the motion vector modification distortion caused by data embedding is analyzed. Then, a rate–distortion model reflecting the residue deviation propagation in successive inter-coded frames is derived. According to this model, the residue deviation propagation weight of each inter-coded frame can be computed. Finally, an inter-frame payload allocation strategy is designed in order to restrain the residue deviation propagation. Experimental results demonstrate that the proposed payload allocation strategy can enhance existing motion vector-based video steganographic methods in terms of undetectability and video coding performance. Besides, the lower computational complexity can be achieved.}
}
@article{PADMANABHAN2021103002,
title = {Optimal feature selection-based biometric key management for identity management system: Emotion oriented facial biometric system},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {103002},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.103002},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302170},
author = {Suresh Padmanabhan and Radhika K.R.},
keywords = {Identity management system, Facial emotions, Metaheuristic optimization},
abstract = {Identity management systems with biometric key binding make digital transactions secure and reliable. A novel methodology is proposed to develop an intelligent key management system using facial emotions. Key binding with facial emotions makes use of an intrinsic user specific trait facilitating a more natural computer to human interaction. The proposed system utilizes metaheuristic swarm intelligence based optimization techniques to extract optimal features. The work demonstrates key binding by encrypting an image with a secret key bound to optimal features extracted from facial emotions. Efficiency and correctness of proposed key management is validated by successful decryption at receiving end with any one of the enrolled emotions given as input. Deer Hunting Optimization Algorithm and Chicken Swarm Optimization are merged to select optimal features from facial emotions. The derived algorithm is called Fitness Sorted Deer Hunting Optimization Algorithm with Rooster Update. Seven facial emotions — anger, disgust, fear, happiness, sadness, surprise and neutral are used to extract optimal features from Japanese Female Facial Expressions and Yale Facial datasets to train the neural network. Proposed work achieved better performance results over state-of-art optimization algorithms such as whale optimization algorithm, grey wolf optimization, chicken swarm optimization and deer hunting optimization algorithm. Accuracy of proposed model is 2.2% better than deer hunting optimization algorithm and 12.3% better than chicken swarm optimization for a key length 80.}
}
@article{WANG2021102951,
title = {Hypergraph-regularized sparse representation for single color image super resolution},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102951},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102951},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301796},
author = {Minghua Wang and Qiang Wang},
keywords = {Color image super resolution, Alternating Direction Method of Multipliers (ADMM), Joint Color Dictionary Training (JCDT), Hypergraph regularization, Self-channel and cross-channel information},
abstract = {Sparsity-based single image super resolution method generates the High-Resolution (HR) output via a corresponding dictionary from the Low-Resolution (LR) input. However, most of these existing methods ignore the complementary information from color channels, which causes the loss of a valid prior and the limitation of HR image quality improvement. In this paper, hypergraph regularization is first incorporated with Joint Color Dictionary Training (JCDT) model and HR image reconstruction (HRIR) model. A novel Hypergraph-regularized Sparse coding-based Super Resolution (HG-ScSR) is proposed. This regularization can not only focus on the illuminance information, but also exploit the self-channel and cross-channel information of three color RGB channels from high-resolution image patches. Especially, the complex relationship is explored among every color image patch pixel and the consistency of the similar pixels is enforced. Both simulated and real data experiments verify the higher performance of the proposed HG-ScSR.}
}
@article{LU2021103071,
title = {Line-based visual odometry using local gradient fitting},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103071},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103071},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000377},
author = {Junxin Lu and Zhijun Fang and Yongbin Gao and Jieyu Chen},
keywords = {Visual odometry, Line, Gradient, Texture-less, RGB-D},
abstract = {Visual odometry aims to estimate the relative pose between frames, which is a fundamental task for visual SLAM. In this paper, we present a novel line-based visual odometry (VO) algorithm that fully utilizes the characteristic of line to estimate the projected line of adjacent frame by minimizing the local gradient fitness evaluation. In contrast to the current feature-based or line-based visual odometry, we don′t need to explicitly match points or lines of two frames, which is non-trivial and inaccurate in challenging scenarios such as texture-less scenes. In our method, the projected line is calculated simultaneously with the local gradient fitting function of pose estimation based on the constraint that the orientation of the projected line should be perpendicular to the gradient orientation of pixels of its local regions. The proposed method is more robust and reliable than other line-based VO since it fully uses the pixel orientations in the local regions to estimate the projected line and relative pose. We evaluate our method on the real-world RGB-D dataset and synthetic benchmark dataset. Experimental results show that our method achieves the state-of-the-art algorithms in indoors scenes, especially in texture-less scenes.}
}
@article{CHAITANYA2021103014,
title = {Single image dehazing using improved cycleGAN},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {103014},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.103014},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302248},
author = {B.S.N.V. Chaitanya and Snehasis Mukherjee},
keywords = {CycleGAN, Cyclic consistency loss, AOD-NET, Single image dehazing, SSIM loss},
abstract = {Haze is an aggregation of very fine, widely dispersed, solid and/or liquid particles suspended in the atmosphere. In this paper, we propose an end-to-end network for single image dehazing, which enhances the CycleGAN model by introducing a transformer architecture within the generator, which is specific for haze removal. The proposed model is trained in an unpaired fashion with clear and hazy images altogether and does not require pairs of hazy and corresponding ground-truth clear images. Furthermore, the proposed model does not depend on estimating the parameters of the atmospheric scattering model. Rather, it uses a K-estimation module as the generator’s transformer for complete end-to-end modeling. The feature transformer introduced in the proposed generator model transforms the encoded features into desired feature space and then feeds them into the CycleGAN decoder to create a clear image. In the proposed model we further modified the cycle consistency loss to include the SSIM loss along with pixel-wise mean loss to produce a new loss function specific for the reconstruction task, which enhances the performance of the proposed model. The model performs well even on the high-resolution images provided in the NTIRE 2019 challenge dataset for single image dehazing. Further, we perform experiments on NYU-Depth and reside beta datasets. Results of our experiments show the efficacy of the proposed approach compared to the state-of-the-art in removing the haze from the input image.}
}
@article{LI2021103037,
title = {AdvCapsNet: To defense adversarial attacks based on Capsule networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103037},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103037},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000134},
author = {Yueqiao Li and Hang Su and Jun Zhu},
keywords = {Capsule, Adversarial, Defense, Robustness},
abstract = {Convolutional neural networks have achieved the state-of-the-art results across numerous applications, but recent work finds that these models can be easily fooled by adversarial perturbations. This is partially due to gradient calculation instability, which may be amplified throughout network layers (Liao et al., 2018). To address this issue, we propose a novel AdvCapsNet derived from Capsule (Sabour et al., 2017), which utilizes a significantly more complicated non-linearity, to defend against adversarial attacks. In this paper, we focus on the transfer-based black-box adversarial attacks, which are more practical than their white-box counterparts. Specifically, we investigate vanilla Capsule’s robustness and boost its performance by introducing an adversarial loss function as regularization. The weight updating between capsule layers is implemented via dynamic routing regularized by the additional adversarial term. Extensive experiments demonstrate that the proposed AdvCapsNet can significantly boost Capsule’s robustness and that AdvCapsNet is far more resistance to adversarial attacks than alternative baselines, including both CNN- and Capsule-based defense models.}
}
@article{SRIDHAR2021102996,
title = {Two in One Image Secret Sharing Scheme (TiOISSS) for extended progressive visual cryptography using simple modular arithmetic operations},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102996},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102996},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302121},
author = {Srividhya Sridhar and Gnanou Florence Sudha},
keywords = {Visual Cryptography, Progressive, Polynomial, Meaningful shares and Modular Arithmetic},
abstract = {Existing Extended Progressive Visual Cryptography scheme (EPVCS) suffers from the problem of pixel expansion, poor quality of reconstructed image and residual trace of cover images in the reconstructed image. Hence in this paper, Two in One Image secret sharing scheme for EPVCS is proposed which decodes the encrypted image in two stages. The proposed scheme provides k,n threshold construction using meaningful shares without pixel expansion and demonstrates that the reconstructed image has improved quality compared to the existing schemes. To reduce the computational complexity, the proposed scheme uses simple Modular Arithmetic operations instead of Galois field. The proposed scheme has the additional advantages of supporting any value of k and n, no overhead in resizing the secret image and no residual trace of cover image. Simulation results and performance analysis show the effectiveness of proposed scheme with improved contrast, 99% Structural Similarity of the reconstructed image and good progressive reconstruction.}
}
@article{KHALID2021103092,
title = {Gaussian Process-based Feature-Enriched Blind Image Quality Assessment},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103092},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103092},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000535},
author = {Hassan Khalid and Dr. Muhammad Ali and Nisar Ahmed},
keywords = {Image quality assessment (IQA), No-reference (NR), Natural scene statistics, Feature selection, Gaussian process regression, Blind image quality assessment (BIQA)},
abstract = {The objective of blind-image quality assessment (BIQA) research is the prediction of perceptual quality of images, without reference information. The human’s perceptual assessment of quality of an image is the backbone of BIQA research. Therefore, human-provided, mean opinion score (perceptual quality) has been analyzed in detail, and it has been observed to follow the Gaussian distribution and thus can be ideally modeled by the same. In this paper, we have proposed an integrated two-stage Gaussian process-based hybrid-feature selection algorithm for the BIQA problem. Moreover, a new consolidated feature set (obtained from the proposed algorithm), consisting of momentous Natural Scene Statistics (NSS)-based features is used in combination with the Gaussian process regression algorithm for the design of a new blind-image quality evaluator, referred to as GPR-BIQA. The proposed evaluator is tested on eight IQA legacy databases, and it is found that the proposed evaluator proficiently correlate with the human opinion, and outperformed a substantial number of existing approaches.}
}
@article{YANG2021103019,
title = {Image super-resolution based on deep neural network of multiple attention mechanism},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103019},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103019},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000018},
author = {Xin Yang and Xiaochuan Li and Zhiqiang Li and Dake Zhou},
keywords = {Super-resolution, CNN, Attention mechanism, Channel attention, Spatial attention},
abstract = {At present, the main super-resolution (SR) method based on convolutional neural network (CNN) is to increase the layer number of the network by skip connection so as to improve the nonlinear expression ability of the model. However, the network also becomes difficult to be trained and converge. In order to train a smaller but better performance SR model, this paper constructs a novel image SR network of multiple attention mechanism(MAMSR), which includes channel attention mechanism and spatial attention mechanism. By learning the relationship between the channels of the feature map and the relationship between the pixels in each position of the feature map, the network can enhance the ability of feature expression and make the reconstructed image more close to the real image. Experiments on public datasets show that our network surpasses some current state-of-the-art algorithms in PSNR, SSIM, and visual effects.}
}
@article{RODRIGUEZSALAS2021103054,
title = {A minimal model for classification of rotated objects with prediction of the angle of rotation},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103054},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103054},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000250},
author = {Rosemberg {Rodriguez Salas} and Petr Dokládal and Eva Dokladalova},
keywords = {Image Classification, Convolutional neural network, Rotation invariance, Prediction of angle of rotation, Steerable filters},
abstract = {In classification tasks, the robustness against various image transformations remains a crucial property of CNN models. When acquired using the data augmentation it comes at the price of a considerable increase in training time and the risk of overfitting. Consequently, researching other ways to endow CNNs with invariance to various transformations is an intensive field of study. This paper presents a new reduced, rotation-invariant, classification model composed of two parts: a feature representation mapping and a classifier. We provide an insight into the principle and we show that the proposed model is trainable. The model we obtain is smaller and has angular prediction capabilities. We illustrate the results on the MNIST-rot and CIFAR-10 datasets. We achieve the state-of-the-art classification score on MNIST-rot, and improve by 20% the state of the art score on rotated CIFAR-10. In all cases, we can predict the rotation angle.}
}
@article{WANG2021102955,
title = {Text to photo-realistic image synthesis via chained deep recurrent generative adversarial network},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102955},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102955},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301838},
author = {Min Wang and Congyan Lang and Songhe Feng and Tao Wang and Yi Jin and Yidong Li},
keywords = {Text-to-image synthesis, Logic relationships, Computational bottlenecks, Parameters sharing},
abstract = {Despite the promising progress made in recent years, automatically generating high-resolution realistic images from text descriptions remains a challenging task due to semantic gap between human-written descriptions and diversities of visual appearance. Most existing approaches generate the rough images with the given text descriptions, while the relationship between sentence semantics and visual content is not holistically exploited. In this paper, we propose a novel chained deep recurrent generative adversarial network (CDRGAN) for synthesizing images from text descriptions. Our model uses carefully designed chained deep recurrent generators that simultaneously recovers global image structures and local details. Specially, our method not only considers the logic relationships of image pixels, but also removes computational bottlenecks through parameters sharing. We evaluate our method on three public benchmarks: CUB, Oxford-102 and MS COCO datasets. Experimental results show that our method significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.}
}
@article{YUAN2021102995,
title = {Noise reduction for sonar images by statistical analysis and fields of experts},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102995},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102995},
url = {https://www.sciencedirect.com/science/article/pii/S104732032030211X},
author = {Fei Yuan and Fengqi Xiao and Kaihan Zhang and Yifan Huang and En Cheng},
keywords = {Sonar image, Denoising, Gamma distribution, Fields of Experts},
abstract = {Sonar images are usually suffering from speckle noise which results in poor visual quality. In order to improve the sonar imaging quality, removing or reducing these speckle noises is a very important and arduous task. In this paper, the imaging principle and noise characteristics of the side-scan sonar (SSS) are analyzed, and five typical probability distribution functions are used to fit the seabed reverberation. Through experiment comparison, the Gamma distribution is selected to simulate the noise of the SSS image caused by the reverberation. Simultaneously, the fields of experts denoising algorithm based on the Gamma distribution (Gamma FoE) is proposed for SSS image denoising. In order to perceive and measure the denoising effect better, evaluation indexes of Fast Noise Variance Estimation (FNVE, an image noise estimation method) and Blind Referenceless Image Spatial Quality Evaluator (BRISQUE, an image quality evaluation method) are selected for image quality perception. The final results of the SSS image denoise experiment show that the Gamma FoE denoise algorithm has a better effect on SSS image denoise application than other denoise algorithms.}
}
@article{ZHANG2020102942,
title = {SAR-NAS: Skeleton-based action recognition via neural architecture searching},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102942},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102942},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301711},
author = {Haoyuan Zhang and Yonghong Hou and Pichao Wang and Zihui Guo and Wanqing Li},
keywords = {Neural architecture search, Action recognition, Skeleton},
abstract = {This paper presents a study of automatic design of neural network architectures for skeleton-based action recognition. Specifically, we encode a skeleton-based action instance into a tensor and carefully define a set of operations to build two types of network cells: normal cells and reduction cells. The recently developed DARTS (Differentiable Architecture Search) is adopted to search for an effective network architecture that is built upon the two types of cells. All operations are 2D based in order to reduce the overall computation and search space. Experiments on the challenging NTU RGB+D and Kinectics datasets have verified that most of the networks developed to date for skeleton-based action recognition are likely not compact and efficient. The proposed method provides an approach to search for such a compact network that is able to achieve comparative or even better performance than the state-of-the-art methods.}
}
@article{BUM2021102973,
title = {Sentiment-based sub-event segmentation and key photo selection},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102973},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102973},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301954},
author = {Junghyun Bum and Joyce Jiyoung Whang and Hyunseung Choo},
keywords = {Personal photo collection, Event segmentation, Key photo selection, Summarization, Sentiment analysis},
abstract = {The number of people collecting photos has surged owing to social media and cloud services in recent years. A typical approach to summarize a photo collection is dividing it into events and selecting key photos from each event. Despite the fact that a certain event comprises several sub-events, few studies have proposed sub-event segmentation. We propose the sentiment analysis-based photo summarization (SAPS) method, which automatically summarizes personal photo collections by utilizing metadata and visual sentiment features. For this purpose, we first cluster events using metadata of photos and then calculate the novelty scores to determine the sub-event boundaries. Next, we summarize the photo collections using a ranking algorithm that measures sentiment, emotion, and aesthetics. We evaluate the proposed method by applying it to the photo collections of six participants consisting of 5,480 photos in total. We observe that our sub-event segmentation based on sentiment features outperforms the existing baseline methods. Furthermore, the proposed method is also more effective in finding sub-event boundaries and key photos, because it focuses on detailed sentiment features instead of general content features.}
}
@article{ZHANG2021103084,
title = {Discriminative semantic region selection for fine-grained recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103084},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103084},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000493},
author = {Chunjie Zhang and Da-Han Wang and Haisheng Li},
keywords = {Fine-grained recognition, Discriminative region selection, Semantic correlation, Object categorization},
abstract = {Performances of fine-grained recognition have been greatly improved thanks to the fast developments of deep convolutional neural networks (DCNN). DCNN methods often treat each image region equally. Besides, researchers often rely on visual information for classification. To solve these problems, we propose a novel discriminative semantic region selection method for fine-grained recognition (DSRS). We first select a few image regions and then use the pre-trained DCNN models to predict their semantic correlations with corresponding classes. We use both visual and semantic representations to represent image regions. The visual and semantic representations are then linearly combined for joint representation. The combination parameters are determined by considering both semantic distinctiveness and spatial-semantic correlations. We use the joint representations for classifier training. A testing image can be classified by obtaining the visual and semantic representations and encoded for joint representation and classification. Experiments on several publicly available datasets demonstrate the proposed method's superiority.}
}
@article{IRFAN2021103027,
title = {Exploiting color for graph-based 3D point cloud denoising},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103027},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103027},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000092},
author = {Muhammad Abeer Irfan and Enrico Magli},
keywords = {Point cloud denoising, Color denoising, Convex optimization, Tikhonov regularization, Total variation, Graph signal processing},
abstract = {A point cloud is a representation of a 3D scene as a discrete collection of geometry plus other attributes such as color, normal, transparency associated with each point. The traditional acquisition process of a 3D point cloud, e.g. using depth information acquired directly by active sensors or indirectly from multi-viewpoint images, suffers from a significant amount of noise. Hence, the problem of point cloud denoising has recently received a lot of attention. However, most existing techniques attempt to denoise only the geometry of each point, based on the geometry information of the neighboring points; there are very few works at all considering the problem of denoising the color attributes of a point cloud. In this paper, we move beyond the state of the art and we propose a novel technique employing graph-based optimization, taking advantage of the correlation between geometry and color, and using it as a powerful tool for several different tasks, i.e. color denoising, geometry denoising, and combined geometry and color denoising. The proposed method is based on the notion that the correct location of a point also depends on the color attribute and not only the geometry of the neighboring points, and the correct color also depends on the geometry of the neighbors. The proposed method constructs a suitable k-NN graph from geometry and color and applies graph-based convex optimization to obtain the denoised point cloud. Extensive simulation results on both real-world and synthetic point clouds show that the proposed denoising technique outperforms state-of-the-art methods using both subjective and objective quality metrics.}
}
@article{2022103604,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103604},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(22)00128-6},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001286}
}
@article{ZHANG2021103082,
title = {Correlation filter via random-projection based CNNs features combination for visual tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103082},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103082},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100047X},
author = {Mingke Zhang and Long Xu and Jing Xiong and Xuande Zhang},
keywords = {Object tracking, Correlation filter, Deep features, Random-projection},
abstract = {Object tracking based on the Convolutional Neural Networks (CNNs) with multiple feature correlation filter (CF) has become one of the best object tracking frameworks. In this paper, we propose a novel approach of CNNs based CF, which combines deep features from CNNs into low-dimensional features. To achieve the dimensionality reduction, random-projection is used due to its data-independence and superior computational efficiency over other widely used. In our proposed approach, the spectral graph theory is applied to generate a random projection matrix. This method bypasses the time-consuming Gram–Schmidt orthogonalization, where the dimension of the feature is high. The combined features have very low dimensions, less than one tenth of the dimensions of the original deep features from CNNs, offering an improvement of tracking speed and without loss of performance simultaneously. Extensive experiments are conducted on large-scale benchmark datasets. The results demonstrate that the proposed algorithm outperforms the state-of-the-art methods.}
}
@article{2022103650,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103650},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(22)00170-5},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001705}
}
@article{NIE2020102950,
title = {A view-free image stitching network based on global homography},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102950},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102950},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301784},
author = {Lang Nie and Chunyu Lin and Kang Liao and Meiqin Liu and Yao Zhao},
keywords = {41A05, 41A10, 65D05, 65D17},
abstract = {Image stitching is a traditional but challenging computer vision task, aiming to obtain a seamless panoramic image. Recently, researchers begin to study the image stitching task using deep learning. However, the existing learning methods assume a relatively fixed view during the image capturing, thus show a poor generalization ability to flexible view cases. To address the above problem, we present a cascaded view-free image stitching network based on a global homography. This novel image stitching network does not have any restriction on the view of images and it can be implemented in three stages. In particular, we first estimate a global homography between two input images from different views. And then we propose a structure stitching layer to obtain the coarse stitching result using the global homography. In the last stage, we design a content revision network to eliminate ghosting effects and refine the content of the stitching result. To enable efficient learning on various views, we also present a method to generate synthetic datasets for network training. Experimental results demonstrate that our method can achieve almost 100% elimination of artifacts in overlapping areas at the cost of acceptable slight distortions in non-overlapping areas, compared with traditional methods. In addition, the proposed method is view-free and more robust especially in a scene where feature points are difficult to detect.}
}
@article{BANITALEBIDEHKORDI2021103011,
title = {No-reference quality assessment of HEVC video streams based on visual memory modelling},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103011},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.103011},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302236},
author = {Mehdi Banitalebi-Dehkordi and Abbas Ebrahimi-Moghadam and Morteza Khademi and Hadi Hadizadeh},
keywords = {No-Reference Video Quality, Visual Memory, Saliency Detection, Bitstream, HEVC},
abstract = {Providing adequate Quality of Experience (QoE) to end-users is crucial for streaming service providers. In this paper, in order to realize automatic quality assessment, a No-Reference (NR) bitstream Human-Vision-System-(HVS)-based video quality assessment (VQA) model is proposed. Inspired by discoveries from the neuroscience community, which suggest there is a considerable overlap between active areas of the brain when engaging in video quality assessment and saliency detection tasks, saliency maps are used in the proposed method to improve the quality assessment accuracy. To this end, saliency maps are first generated from features extracted from the HEVC bitstream. Then, saliency map statistics are employed to create a model of visual memory. Finally, a support vector regression pipeline learns an estimate of the video quality from the visual memory, saliency, and frame features. Evaluations on SJTU dataset indicate that the proposed bitstream based no-reference video quality assessment algorithm achieves a competitive performance.}
}
@article{NIU2021103068,
title = {Fast and effective Keypoint-based image copy-move forgery detection using complex-valued moment invariants},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103068},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103068},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000365},
author = {P. Niu and C. Wang and W. Chen and H. Yang and X. Wang},
keywords = {Copy-move forgery detection, Complex-valued moment invariants, Magnitude-phase hierarchical matching, Adaptive clustering, Gaussian weighted similarity measure, Two-stage false matches filtering},
abstract = {Copy-move forgery is one of the most common image tampering schemes, with the potential use for misleading the opinion of the general public. Keypoint-based detection methods exhibit remarkable performance in terms of computational cost and robustness. However, these methods are difficult to effectively deal with the cases when 1) forgery only involves small or smooth regions, 2) multiple clones are conducted or 3) duplicated regions undergo geometric transformations or signal corruptions. To overcome such limitations, we propose a fast and accurate copy-move forgery detection algorithm, based on complex-valued invariant features. First, dense and uniform keypoints are extracted from the whole image, even in small and smooth regions. Then, these keypoints are represented by robust and discriminative moment invariants, where a novel fast algorithm is designed especially for the computation of dense keypoint features. Next, an effective magnitude-phase hierarchical matching strategy is proposed for fast matching a massive number of keypoints while maintaining the accuracy. Finally, a reliable post-processing algorithm is developed, which can simultaneously reduce false negative rate and false positive rate. Extensive experimental results demonstrate the superior performance of our proposed scheme compared with existing state-of-the-art algorithms, with average pixel-level F-measure of 94.54% and average CPU-time of 36.25 s on four publicly available datasets.}
}
@article{KUMAR2021103052,
title = {Gait recognition based on vision systems: A systematic survey},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103052},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103052},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000249},
author = {Munish Kumar and Navdeep Singh and Ravinder Kumar and Shubham Goel and Krishan Kumar},
keywords = {Gait recognition, Surveillance, Biometric, Person identification},
abstract = {With the growing popularity of biometrics technology in the pattern recognition field, especiallyidentification of human has gained the attention of researchers from both academia and industry. One such type of biometric technique is Gait recognition, which is used to identify a human being based on their walking style. Generally, two types of approaches are adopted by any algorithm designed for gait recognition, namely model based and model free approaches. The key reason behind the popularity of gait recognition is that it can identify a person from a considerable distance while other biometrics has failed to do so. In this paper, the authors have conducted a survey of extant studies on gait recognition in consideration of gait recognition approaches and phases of a gait cycle. Moreover, some aspects like floor sensors, accelerometer based recognition, the influences of environmental factors, which are ignored by exiting surveys, are also covered in our survey study. The information of gait is usually obtained from different parts of silhouettes. This paper also describes different benchmark datasets for gait recognition. This study will provide firsthand knowledge to the researchers working on the gait recognition domain in any real-world field. It has been observed that work done on the gait recognition with sufficiently high accuracy is limited in comparison to research on various other biometric recognition systems and has enough potential for future research.}
}
@article{AGRAWAL2021103087,
title = {A joint cumulative distribution function and gradient fusion based method for dehazing of long shot hazy images},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103087},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103087},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000523},
author = {Subhash Chand Agrawal and Anand Singh Jalal},
keywords = {Image dehazing, Joint cumulative distribution, Nearby haze, Faraway haze, Image fusion, Gradient domain},
abstract = {Hazy or foggy weather conditions significantly degrade the visual quality of an image in an outdoor environment. It also changes the color and reduces the contrast of an image. This paper introduces a novel single image dehazing technique to restore a hazy image without considering the physical model of haze formation. In order to find haze-free image, the proposed method does not require the transmission map and its costly refinement process. Since haze effect is dependent on the depth, it severely degrades the visibility of the objects located at a far distance. The objects close to the camera are unaffected. In this paper, we propose a fusion-based haze removal method based on the joint cumulative distribution function (JCDF) that treats faraway haze and nearby haze separately. The output images after the JCDF module, fused in the gradient domain to produce a haze-free image. The proposed method not only significantly enhances visibility but also preserves texture details. The proposed method is experimented and evaluated on a large set of challenging hazy images (large scene depth, night time, dense fog, etc.). Both qualitative and quantitative measures show that the performance of the proposed method is better than the state-of-the-art dehazing techniques.}
}
@article{LI2020102920,
title = {Depth segmentation in real-world scenes based on U–V disparity analysis},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102920},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102920},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301541},
author = {Xiaohan Li and Lu Chen and Shuang Li and Xiang Zhou},
keywords = {Depth scene segmentation, U–V disparity map, Projection characteristics analysis, Object detection, RANSAC algorithm},
abstract = {Depth segmentation has the challenge of separating the objects from their supporting surfaces in a noisy environment. To address the issue, a novel segmentation scheme based on disparity analysis is proposed. First, we transform a depth scene into the corresponding U-V disparity map. Then, we conduct a region-based detection method to divide the object region into several targets in the processed U-disparity map. Thirdly, the horizontal plane regions may be mapped as slant lines in the V-disparity map, the Random Sample Consensus (RANSAC) algorithm is improved to fit such multiple lines. Moreover, noise regions are reduced by image processing strategies during the above processes. We respectively evaluate our approach on both real-world scenes and public data sets to verify the flexibility and generalization. Sufficient experimental results indicate that the algorithm can efficiently segment and label a full-view scene into a group of valid regions as well as removing surrounding noise regions.}
}
@article{LI2020102956,
title = {Improved-StoryGAN for sequential images visualization},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102956},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102956},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301826},
author = {Chunye Li and Liya Kong and Zhiping Zhou},
keywords = {Story visualization, Weighted Activation Degree (WAD), Dilated Convolution, Gated Convolution},
abstract = {Story visualization is a novel and challenging topic that intersects computer vision and natural language processing, which needs to generate sequential images based on a story. It is related to text-to-image generation and video generation. Apart from ensuring the quality of the results, the synthesized images of story visualization are supposed to be consistent with each other and reflect the input story. In order to improve the performance of generated sequential images, we have developed the baseline model StoryGAN. Firstly, we use Dilated Convolution in the discriminators to expand the receptive field of the convolution kernel in the feature maps, thus enhancing the quality of the generated sequential images. In addition, Weighted Activation Degree (WAD) is introduced in the discriminators to provide a robust evaluation in view of similarity between the generated images and the target story, which results in enhancement on the consistency between the generated images and the target story. Last but not least, Bi-GRU stores the historical and future information of each sentence to effectively extract the textual features. What’s more, in order to make full use of the features of the long story features, Gated Convolution is used to replace the original MLP in the Initial State Encoder to improve the consistence between the generated sequential images. Experimental results and visual sequential images demonstrate the outperformance of the model we develop, compared with the other models.}
}
@article{LIU2021103033,
title = {Interactive information module for person re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103033},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103033},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000122},
author = {Xudong Liu and Jun Kong and Min Jiang and Sha Li},
keywords = {Person re-identification, Interactive Information Module, Global-map Attention Module, Labeled-class Mutual Learning},
abstract = {In person re-identification (Re-ID) task, multi-branch networks acquire better performance by combining global features and local features. Obviously, local branch can obtain detailed information of person pictures but may work on invalid regions when person pictures have imprecise bounding boxes. On the contrary, global branch can be aware of the position of person but hard to acquire detailed information of person pictures. Meanwhile, lots of multi-branch networks ignore mutual information among different branches. Therefore, it is necessary to enhance interaction of global branch and local branch. For this purpose, we propose Interactive Information Module (IIM). IIM includes two components named Global-map Attention Module (GAM) and Labeled-class Mutual Learning (LML), respectively. GAM leverages heatmaps generated by global branch to guide calculation of local attention and obtains a composite global feature by combining local features. GAM relys more on the performance of global branch which decides the quality of heatmaps. To improve performance of global branch, we propose LML to promote convergent rate of global branch. Extensive experiments implemented on Market-1501, DukeMTMC-ReID, and CUHK03-NP datasets confirm that our method achieves state-of-the-art results.}
}
@article{LIN2021102969,
title = {An anisotropic reference matrix for image steganography},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102969},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102969},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301942},
author = {Juan Lin and Ji-Hwei Horng and Yanjun Liu and Chin-Chen Chang},
keywords = {Anisotropic reference matrix, Data hiding, Steganography, Visual quality, Embedding capacity},
abstract = {A lot of image steganographic techniques (also called data hiding) conceal secret data by utilizing a reference matrix (RM), and some new types of RMs, such as the turtle shell, an octagon-shaped shell, the Sudoku table, and so on, have been proposed in recent years. In this article, we present a novel type of RM called anisotropic RM. By employing a full search strategy, we have found the optimal parameters for constructing the anisotropic RM. To judge the performance of a parameter set quickly, a theoretical peak signal-to-noise ratio (PSNR) evaluation method is proposed. In addition, we extend the RM to three-dimensional (3D) space. Experimental results show that our data hiding scheme, based on the anisotropic RM, has a better quality stego image than previous methods. Moreover, the 3D RM works better than the traditional 2D RM using the same embedding capacity.}
}
@article{LI2021103004,
title = {Learning to predict the quality of distorted-then-compressed images via a deep neural network},
journal = {Journal of Visual Communication and Image Representation},
volume = {76},
pages = {103004},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.103004},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302194},
author = {Bowen Li and Meng Tian and Weixia Zhang and Hongtai Yao and Xianpei Wang},
keywords = {Image quality assessment, Convolutional neural network, Full-reference IQA, No-reference IQA, Distorted-then-compressed},
abstract = {Being captured by amateur photographers, reciprocally propagated through multimedia pipelines, and compressed with different levels, real-world images usually suffer from a wide variety of hybrid distortions. Faced with this scenario, full-reference (FR) image quality assessment (IQA) algorithms can not deliver promising predictions due to the inferior references. Meanwhile, existing no-reference (NR) IQA algorithms remain limited in their efficacy to deal with different distortion types. To address this obstacle, we explore a NR-IQA metric by predicting the perceptual quality of distorted-then-compressed images using a deep neural network (DNN). First, we propose a novel two-stream DNN to handle both authentic distortions and synthetic compressions and adopt effective strategies to pre-train the two branches of the network. Specifically, we transfer the knowledge learned from in-the-wild images to account for authentic distortions by utilizing a pre-trained deep convolutional neural network (CNN) to provide meaningful initializations. Meanwhile, we build a CNN for synthetic compressions and pre-train it on a dataset including synthetic compressed images. Subsequently, we bilinearly pool these two sets of features as the image representation. The overall network is fine-tuned on an elaborately-designed auxiliary dataset, which is annotated by a reliable objective quality metric. Furthermore, we integrate the output of the authentic-distortion-aware branch with that of the overall network following a two-step prediction manner to boost the prediction performance, which can be applied in the distorted-then-compressed scenario when the reference image is available. Extensive experimental results on several databases especially on the LIVE Wild Compressed Picture Quality Database show that the proposed method achieves state-of-the-art performance with good generalizability and moderate computational complexity.}
}
@article{DUTTA2021103089,
title = {Depth-aware blending of smoothed images for Bokeh effect generation},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103089},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103089},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000511},
author = {Saikat Dutta},
keywords = {Bokeh effect, Blur kernel, AIM 2019, Deep learning},
abstract = {Bokeh effect is used in photography to capture images where the closer objects look sharp and everything else stays out-of-focus. Bokeh photos are generally captured using Single Lens Reflex cameras using shallow depth-of-field. Most of the modern smartphones can take bokeh images by leveraging dual rear cameras or a good auto-focus hardware. However, for smartphones with single-rear camera without a good auto-focus hardware, we have to rely on software to generate bokeh images. This kind of system is also useful to generate bokeh effect in already captured images. In this paper, an end-to-end deep learning framework is proposed to generate high-quality bokeh effect from images. The original image and different versions of smoothed images are blended to generate Bokeh effect with the help of a monocular depth estimation network. The model is trained through three phases to generate visually pleasing bokeh effect. The proposed approach is compared against a saliency detection based baseline and a number of approaches proposed in AIM 2019 Challenge on Bokeh Effect Synthesis. Extensive experiments are shown in order to understand different parts of the proposed algorithm. The network is lightweight and can process an HD image in 0.03 s. This approach ranked second in AIM 2019 Bokeh effect challenge-Perceptual Track.}
}
@article{2022103560,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {86},
pages = {103560},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(22)00096-7},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000967}
}
@article{TEK2021103015,
title = {Adaptive convolution kernel for artificial neural networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103015},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.103015},
url = {https://www.sciencedirect.com/science/article/pii/S104732032030225X},
author = {F. Boray Tek and İlker Çam and Deniz Karlı},
keywords = {Adaptive convolution, Multi-scale convolution, Image classification, Residual networks},
abstract = {Many deep neural networks are built by using stacked convolutional layers of fixed and single size (often 3 × 3) kernels. This paper describes a method for learning the size of convolutional kernels to provide varying size kernels in a single layer. The method utilizes a differentiable, and therefore backpropagation-trainable Gaussian envelope which can grow or shrink in a base grid. Our experiments compared the proposed adaptive layers to ordinary convolution layers in a simple two-layer network, a deeper residual network, and a U-Net architecture. The results in the popular image classification datasets such as MNIST, MNIST-CLUTTERED, CIFAR-10, Fashion, and “Faces in the Wild” showed that the adaptive kernels can provide statistically significant improvements on ordinary convolution kernels. A segmentation experiment in the Oxford-Pets dataset demonstrated that replacing ordinary convolution layers in a U-shaped network with 7 × 7 adaptive layers can improve its learning performance and ability to generalize.}
}
@article{WU2020102960,
title = {Analytical derivatives for differentiable renderer: 3D pose estimation by silhouette consistency},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102960},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102960},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301851},
author = {Zaiqiang Wu and Wei Jiang and Hongyan Yu},
keywords = {Inverse graphics, Differentiable renderer, 3D pose estimation},
abstract = {Differentiable renderer is widely used in optimization-based 3D reconstruction which requires gradients for optimization. The existing differentiable renderers obtain gradients via numerical techniques. However, these methods are inaccurate and inefficient. Motivated by this fact, we propose a differentiable renderer with analytical gradients. The main obstacle of traditional renderer being differentiable is the discrete sampling operation of rasterization. To obtain a differentiable rasterization renderer, we define pixel intensity as a double integral over the pixel grid, and then derive the analytical gradients with respect to vertices. 3D pose estimation by multi-viewpoint silhouettes is conducted to reveal the effectiveness and efficiency of the proposed method. Experimental results show that 3D pose estimation without 3D and 2D joint supervision can produce competitive results. The findings also indicate that the proposed method has higher accuracy and efficiency than previous differentiable renderers.}
}
@article{CUI2020102923,
title = {AP-GAN: Predicting skeletal activity to improve early activity recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102923},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102923},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301577},
author = {Ran Cui and Gang Hua and Jingran Wu},
keywords = {Early activity recognition, Activity prediction, Skeleton, GAN},
abstract = {Early activity recognition is a classification task before the completion of activity. The study of early activity recognition is beneficial to avoid serious result. Previous studies have focused on extracting effective activity features and modeling for quick and accurate classification. It is challenging because of lack of available information. In order to get a firm basis for judgment, this paper adds an activity prediction module prior to recognition module. The main task of the module is to predict subsequent motions according to observed motions. To avoid motion blur, the structure of GAN (Generative Adversarial Networks) is used to generate the predicted motions. Compared with the traditional deep learning model, dilated neural network has advantages in large-span spatiotemporal feature modeling. The dilated RNN (Recurrent Neural Networks) and CNN (Convolutional Neural Networks) are introduced to the recognition module. In order to make the activity prediction and recognition modules work together, this paper designs and introduces a hard class mining mechanism to improve the learning ability of hard class samples. The proposed method is validated on four skeletal activity datasets and achieves state-of-the-art accuracy.}
}
@article{LI2021102981,
title = {Low-rank embedded orthogonal subspace learning for zero-shot classification},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102981},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102981},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302029},
author = {Xiao Li and Min Fang and Jichuan Liu},
keywords = {Zero shot classification, Low-rank, Class similarity, Manifold structure},
abstract = {Zero-shot classification methods have attracted considerable attention in recent years. Existing ZSC methods encounter domain shift, hubness and visual–semantic gap problems. To address these problems, we propose a low-rank embedded orthogonal subspace learning method (LEOSL) for ZSC. Many previous works project visual features to the semantic space. However, they often suffer from the visual–semantic gap problem. To handle this problem, we project the visual representations and semantic representations to the common subspace. To address the domain shift problem, we restrict the mapping functions with a low-rank constraint. To handle the hubness problem, we introduce the class similarity term so that samples of the same class are located near each other, while samples of different classes are located far away. Furthermore, we restrict the shared representations in the subspace with an orthogonal constraint to remove the correlation between samples. The results show the superiority of LEOSL compared to many state-of-the-art methods.}
}
@article{WANG2021102998,
title = {Block-based image matching for image retrieval},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102998},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102998},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302145},
author = {Yanhong Wang and Ruizhen Zhao and Liequan Liang and Xinwei Zheng and Yigang Cen and Shichao Kan},
keywords = {Composite descriptors, Block index, Vector of locally aggregated descriptors, Image matching, Feature fusion, Deep feature, Image retrieval},
abstract = {Due to the lighting, translation, scaling and rotation, image matching is a challenge task in computer vision area. In the past decades, local descriptors (e.g. SIFT, SURF and HOG, etc.) and global features (e.g. HSV, CNN, etc.) play a vital role for this task. However, most image matching methods are based on the whole image, i.e., matching the entire image directly base on some image representation methods (e.g. BoW, VLAD and deep learning, etc.). In most situations, this idea is simple and effective, but we recognize that a robust image matching can be realized based on sub-images. Thus, a block-based image matching algorithm is proposed in this paper. First, a new local composite descriptor is proposed, which combines the advantages of local gradient and color features with spatial information. Then, VLAD method is used to encode the proposed composite descriptors in one block, and block-CNN feature is extracted at the same time. Second, a block-based similarity metric is proposed for similarity calculation of two images. Finally, the proposed methods are verified on several benchmark datasets. Compared with other methods, experimental results show that our method achieves better performance.}
}
@article{WANG2021103051,
title = {Quality assessment for color correction-based stitched images via bi-directional matching},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103051},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103051},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000237},
author = {Xuejin Wang and Xiongli Chai and Feng Shao},
keywords = {Image quality assessment, Stitched image, Bi-directional matching, Color correction, Color aberration},
abstract = {With the deepening of social information, the panoramic image has drawn a significant interest of viewers and researchers as it can provide a very wide field of view (FoV). Since panoramic images are usually obtained by capturing images with the overlapping regions and then stitching them together, image stitching plays an important role in generating panoramic images. In order to effectively evaluate the quality of stitched images, a novel quality assessment method based on bi-directional matching is proposed for stitched images. Specifically, dense correspondences between the testing and benchmark stitched images are first established by bi-directional SIFT-flow matching. Then, color-aware, geometric-aware and structure-aware features are respectively extracted and fused via support vector regression (SVR) to obtain the final quality score. Experiments on our newly constructed database and ISIQA database demonstrate that the proposed method can achieve comparable performance compared with the conventional blind quality metrics and the quality metrics specially designed for stitched images.}
}
@article{SHARMA2021103045,
title = {Distance based kernels for video tensors on product of Riemannian matrix manifolds},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103045},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103045},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000195},
author = {Krishan Sharma and Renu Rameshan},
keywords = {Video tensor, Product manifold geometry, Sparse representation, Grassmann manifold, SPD manifold, Riemannian manifold, Kernel methods},
abstract = {In this paper, we explore the inherent geometry of video tensors by modeling them as points in product of Riemannian matrix manifolds. A video tensor is decomposed into three modes (factors) using matrix unfolding operation and each mode is represented as a point in a product space of Grassmannian and symmetric positive definite (SPD) matrix manifold. Hence a video is represented as a point in the Cartesian product of three such product spaces. Being a manifold valued (non-Euclidean) representation, application of several state-of-the-art Euclidean machine learning algorithms lead to inferior results. To overcome this, we propose positive definite kernels which map the points from product manifold space to Hilbert space. The proposed kernel functions implicitly make use of geodesic distance on product manifold to obtain a similarity measure and generate a kernel-gram matrix. In addition, we generate a discriminative feature representation for each manifold valued point using kernel-gram matrix diagonalization. Classification is performed in a sparse framework. The proposed methodology is tested over three publicly available datasets for hand gesture, traffic signal and sign language recognition. Experimentation performed over these datasets show that the proposed methodology is powerful in terms of classification accuracy in comparison with the state-of-the-art methods.}
}
@article{HU2020102957,
title = {Weakly supervised instance segmentation using multi-stage erasing refinement and saliency-guided proposals ordering},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102957},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102957},
url = {https://www.sciencedirect.com/science/article/pii/S104732032030184X},
author = {Zheng Hu and Zhi Liu and Gongyang Li and Linwei Ye and Lei Zhou and Yang Wang},
keywords = {Weakly supervised instance segmentation, Image-level annotations, Multi-stage erasing refinement, Saliency-guided proposals ordering},
abstract = {Weakly supervised instance segmentation is a new research topic in the field of computer vision. Compared with fully supervised instance segmentation, weakly supervised methods use weaker data annotations such as points, scribbles or class labels which are easy to obtain. Among these annotations, image-level instance segmentation using only class labels as supervision is the most challenging task. In this paper, we propose a novel weakly supervised instance segmentation framework using a multi-stage erasing refinement method and a saliency-guided proposals ordering method. Firstly, the multi-stage erasing refinement method is exploited to enhance the instance representation by iteratively discovering separate object-related regions, so as to obtain more complete discriminative regions. Then, the saliency-guided proposals ordering method utilizes the saliency map to alleviate the background noise and better select the object proposals for generating the instance segmentation result. Experimental results on the PASCAL VOC 2012 dataset and the COCO dataset demonstrate that our framework achieves superior performance compared with the state-of-the-art weakly supervised instance segmentation models and the ablation study shows the effectiveness of the proposed two methods.}
}
@article{ZHANG2020102930,
title = {An improved noise loss correction algorithm for learning from noisy labels},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102930},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102930},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301619},
author = {Qian Zhang and Feifei Lee and Ya-gang Wang and Ran Miao and Lei Chen and Qiu Chen},
keywords = {Noisy labels, KL entropy, Mix-up loss, DNN},
abstract = {Despite excellent performance in image classification researches, the training of the deep neural networks (DNN) needs a large set of clean data with accurate annotations. The collection of a dataset is easy, but annotating the collected data is difficult on the contrary. There are many image data on the websites, which contain inaccurate annotations, but trainings on these datasets may make networks easier to over-fit noisy data and cause performance degradation. In this work, we propose an improved joint optimization framework for noise correction, which uses the Combination of Mix-up entropy and Kullback-Leibler entropy (CMKL) as the loss function. The new loss function can achieve better fine-tuning results after updating all label annotations. The experimental results on publicly available CIFAR-10 dataset and Clothing1M dataset show superior performance of our approach compared with other state-of-the-art methods.}
}
@article{CHU2021103067,
title = {Multi-label image recognition by using semantics consistency, object correlation, and multiple samples},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103067},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103067},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000353},
author = {Wei-Ta Chu and Si-Heng Huang},
keywords = {Multi-label image recognition, Object correlation, Semantics consistency, Multiple samples},
abstract = {An image can be annotated from the local perspective, based on objects visually present. An image can also be annotated from the global perspective, based on implicit emotion or meanings derived from it. We propose three points relatively little studied before. First, semantics remain the same even if the image is manipulated by some geometric processes. Second, object correlation is important in image labelling. We propose to use a standard recurrent neural network to take object sequences in random orders. Third, we observe that some entity can be represented by multiple image samples, and multiple samples can be jointly considered to improve recognition performance. These three points are implemented in a network that jointly considers global and local information. With comprehensive evaluation studies, we verify that a simple network with these points is effective and is able to achieve competitive performance compared to the state of the arts.}
}
@article{REN2021103031,
title = {Saliency detection via cross-scale deep inference},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103031},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103031},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000110},
author = {Dakai Ren and Xiangming Wen and Tao Jia and Jiazhong Chen and Zongyi Li},
keywords = {Cross-scale deep inference, Multi-layer attention, Image saliency, Deep learning},
abstract = {The small, moderate, and large scale saliency patterns in images are valuable to be extracted in saliency detection. By the observation that the probability of small and large saliency patterns appearing in datasets is lower than that of moderate scale saliency patterns. As results, a deep saliency model trained on such datasets would converge to moderate scale saliency patterns, and it is hard to well infer the small and large scale saliency patterns because they are not encoded efficiently in the model for their low probability. Thus a novel but simple saliency detection method using cross-scale deep inference is presented in this paper. Moreover, a new network architecture, in which the attention mechanism is exploited by multiple layers, is proposed to improve the receptive fields of various scale saliency patterns in different scale images. The presented cross-scale deep inference could improve the representation power of small and large scale saliency patterns encoded in multiple scale images efficiently. The quantitative and qualitative evaluation demonstrates our deep model achieves a promising results across a wide of metrics.}
}
@article{ZHOU2021103018,
title = {Facial expression recognition using frequency multiplication network with uniform rectangular features},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103018},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.103018},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302261},
author = {Jinzhao Zhou and Xingming Zhang and Yubei Lin and Yang Liu},
keywords = {Facial expression recognition, Uniform rectangular features, Frequency multiplication network},
abstract = {Facial expression recognition (FER) is a popular research field in cognitive interaction systems and artificial intelligence. Many deep learning methods achieve outstanding performances at the expense of enormous computation workload. Limiting their application in small devices or offline scenarios. To cope with this drawback, this paper proposes the Frequency Multiplication Network (FMN), a deep learning method operating in the frequency domain that significantly reduces network capacity and computation workload. By taking advantage of the frequency domain conversion, this novel deep learning method utilizes multiplication layers for effective feature extraction. In conjunction with the Uniform Rectangular Features (URF), our method further improves the performance and reduces the training effort. On three publicly available datasets (CK+, Oulu, and MMI), our method achieves substantial improvements in comparison to popular approaches.}
}
@article{TAGORE2021103029,
title = {Person re-identification from appearance cues and deep Siamese features},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103029},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103029},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000109},
author = {Nirbhay Kumar Tagore and Ayushman Singh and Sumanth Manche and Pratik Chattopadhyay},
keywords = {Hierarchical re-identification approach, Color-based clustering, Silhouette part-based analysis, Siamese Convolution Box, IIT(BHU) re-identification data set},
abstract = {Automated person re-identification in a multi-camera surveillance setup is very important for effective tracking and monitoring crowd movement. In this paper, we propose an efficient hierarchical re-identification approach in which color histogram-based comparison is employed to find the closest matches in the gallery set, and next deep feature-based comparison is carried out using the Siamese network. Reduction in search space after the first level of matching helps in improving the accuracy as well as efficiency of prediction by the Siamese network by eliminating dissimilar elements. A silhouette part-based feature extraction scheme is adopted in each level of hierarchy to preserve the relative locations of the different body parts and make the appearance descriptors more discriminating. The proposed approach has been evaluated on five public data sets and also a new data set captured in our laboratory. Results reveal that it outperforms most state-of-the-art approaches in terms of overall accuracy.}
}
@article{ZALIK2021103050,
title = {Lossless chain code compression with an improved Binary Adaptive Sequential Coding of zero-runs},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103050},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103050},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000225},
author = {Borut Žalik and Domen Mongus and Krista Rizman Žalik and David Podgorelec and Niko Lukač},
keywords = {Lossless data compression, Run-Length Encoding, Integer coding, Golomb coding},
abstract = {A new method for encoding a sequence of integers, named Binary Adaptive Sequential Coding with Return to Bias, is proposed in this paper. It extends the compressing pipeline for chain codes’ compression consisting of Burrows Wheeler Transform, Move-To-Front Transform, and Adaptive Arithmetic Coding. We also explain when to include the Zero-Run Transform into the above-mentioned pipeline. The Zero-Run Transform generates a sequence of integers corresponding to the number of zero-runs. This sequence is encoded by Golomb coding, Binary Adaptive Sequential Coding, and the new Binary Adaptive Sequential Coding with Return to Bias. Finally, a comparison is performed with the two state-of-the-art methods. The proposed method achieved similar compression efficiency for the Freeman chain code in eight directions. However, for the chain codes with shorter alphabets (Freeman chain code in four directions, Vertex Chain Code, and Three-OrThogonal chain code), the introduced method outperforms the referenced ones.}
}
@article{CHEN2021103064,
title = {Reversible data hiding based on three shadow images using rhombus magic matrix},
journal = {Journal of Visual Communication and Image Representation},
volume = {76},
pages = {103064},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103064},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100033X},
author = {Sisheng Chen and Chin-Chen Chang},
keywords = {Reversible data hiding, Three shadow images, Rhombus magic matrix, Hierarchical data security},
abstract = {This paper proposes a multi-image-based reversible data hiding method using a rhombus magic matrix. It takes a pixel pair as a position coordinate of a 256×256 modulus function matrix and extracts a 5-order rhombus matrix. It first embeds a 5-ary secret digit by producing three shadow pixel pairs which satisfies the predefined distance condition. Then it embeds a 6-ary secret digit by permuting the three shadow pixel pairs and assigning them to three ordered shadow images. Third, it embeds another 5-ary secret digit by modifying the pixel pair within a 3-order rhombus magic matrix in a shadow image. The receiver can extract the secret data and recover the original cover image when obtaining all shadow images. It also introduces a new application scenario for hierarchical security data transmission. The experimental results and analysis show that the proposed security scheme provides high embedding capacity with good visual quality of shadow images.}
}
@article{ZHANG2021102997,
title = {Attention-based contextual interaction asymmetric network for RGB-D saliency prediction},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102997},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102997},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302133},
author = {Xinyue Zhang and Ting Jin and Wujie Zhou and Jingsheng Lei},
keywords = {RGB-D image, Saliency prediction, Attention mechanism, Contextual interaction},
abstract = {Saliency prediction on RGB-D images is an underexplored and challenging task in computer vision. We propose a channel-wise attention and contextual interaction asymmetric network for RGB-D saliency prediction. In the proposed network, a common feature extractor provides cross-modal complementarity between the RGB image and corresponding depth map. In addition, we introduce a four-stream feature-interaction module that fully leverages multiscale and cross-modal features for extracting contextual information. Moreover, we propose a channel-wise attention module to highlight the feature representation of salient regions. Finally, we refine coarse maps through a corresponding refinement block. Experimental results show that the proposed network achieves a performance comparable with state-of-the-art saliency prediction methods on two representative datasets.}
}
@article{ZHANG2021103044,
title = {Parallel-fusion LSTM with synchronous semantic and visual information for image captioning},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103044},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103044},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000183},
author = {Jing Zhang and Kangkang Li and Zhe Wang},
keywords = {Image captioning, Parallel-fusion LSTM, Attention mechanism, Guiding LSTM},
abstract = {For synchronously combining the dynamic semantic and visual information in the decoder part of image captioning, we propose a novel parallel-fusion LSTM (pLSTM) structure in this paper. Two parallel LSTMs with attributes and visual information of image are fused by the hidden states at every time step, which makes the attributes and visual information complementary or enhanced for generating more accurate captions. According to the different ways of integrating semantic information from attribute LSTM to visual LSTM, we propose two models pLSTM with attention (pLSTM-A) and pLSTM with guiding (pLSTM-G). pLSTM-A can automatically capture the crucial semantic and visual information to generate captions, and pLSTM-G directly adjusts the hidden state of visual LSTM by synchronous semantic information to the critical region. For verifying the effectiveness of our proposed pLSTM, we conduct a series of experiments on MSCOCO and Flickr30K datasets, and the experimental results outperform some state-of-the-art image captioning methods.}
}
@article{2023103789,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103789},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(23)00039-1},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000391}
}
@article{SINGHPARIHAR2021102991,
title = {Multiview video summarization using video partitioning and clustering},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102991},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102991},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320302091},
author = {Anil {Singh Parihar} and Joyeeta Pal and Ishita Sharma},
keywords = {Video summarization, Surveillance videos, Multiview video, Video partition, Clustering},
abstract = {Multiview video summarization plays a crucial role in abstracting essential information form multiple videos of the same location and time. In this paper, we propose a new approach for the multiview summarization. The proposed approach uses the BIRCH clustering algorithm for the first time on the initial set of frames to get rid of the static and redundant. The work presents a new approach for shot boundary detection using frame similarity measures Jaccard and Dice. The algorithm performs effectively synchronized merging of keyframes from all camera-views to obtain the final summary. Extensive experimentation conducted on various datasets suggests that the proposed approach significantly outperforms most of the existing video summarization approaches. To state a few, a 1.5% improvement on video length reduction, 24.28% improvement in compression ratio, and 6.4% improvement in quality assessment ratio is observed on the lobby dataset.}
}