@article{LI2021103294,
title = {PSGU: Parametric self-circulation gating unit for deep neural networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103294},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103294},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001942},
author = {Zhengze Li and Xiaoyuan Yang and Kangqing Shen and Fazhen Jiang and Jin Jiang and Huwei Ren and Yixiao Li},
keywords = {Deep learning, Neural network, Activation function, PSGU, Initialization},
abstract = {Activation functions are of great importance for the performance and training of deep neural networks. High-performance activation function is expected to effectively prevent the gradient from vanishing and help network converge. This paper provides a novel smooth activation function, called Parameterized Self-circulating Gating Unit (PSGU), aiming to train an adaptive activation function to improve the performance of deep networks. Compared with other works, we propose and study the self-circulation gating property of activation function, and analyze its influence on the signal transmission in network by controlling the flow of information. Specifically, we theoretically analyze and propose the initialization based on PSGU, which adequately explores the properties in neighborhood of the origin. Finally, the proposed activation function and initialization are compared with other methods on commonly-used network architectures, the achieved performances of using PSGU alone or combining with our proposed initialization are over par with the state of the art.}
}
@article{CHEN2022103432,
title = {Local perspective based synthesis for vehicle re-identification: A transformation state adversarial method},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103432},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103432},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002911},
author = {Yanbing Chen and Wei Ke and Hong Lin and Chan-Tong Lam and Kai Lv and Hao Sheng and Zhang Xiong},
keywords = {Vehicle re-identification, Data synthesis, Local-region perspective transformation, Transformation state adversarial, Candidate pool, Parameter generator network},
abstract = {Vehicle re-identification (V-ReID) aims at discovering an image of a specific vehicle from a set of images typically captured by different cameras. Vehicles are one of the most important objects in cross-camera target recognition systems, and recognizing them is one of the most difficult tasks due to the subtle differences in the visible characteristics of vehicle rigid objects. Compared to various methods that can improve re-identification accuracy, data augmentation is a more straightforward and effective technique. In this paper, we propose a novel data synthesis method for V-ReID based on local-region perspective transformation, transformation state adversarial learning and a candidate pool. Specifically, we first propose a parameter generator network, which is a lightweight convolutional neural network, to generate the transformation states. Secondly, an adversarial module is designed in our work, it ensures that noise information is added as much as possible while keeping the labeling and structure of the dataset intact. With this adversarial module, we are able to promote the performance of the network and generate more proper and harder training samples. Furthermore, we use a candidate pool to store harder samples for further selection to improve the performance of the model. Our system pays more balanced attention to the features of vehicles. Extensive experiments show that our method significantly boosts the performance of V-ReID on the VeRi-776, VehicleID and VERI-Wild datasets.}
}
@article{DIAS2022103395,
title = {Cross-dataset emotion recognition from facial expressions through convolutional neural networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103395},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103395},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002637},
author = {William Dias and Fernanda Andaló and Rafael Padilha and Gabriel Bertocco and Waldir Almeida and Paula Costa and Anderson Rocha},
keywords = {Emotion recognition, Facial analysis, Cross-dataset evaluation, Deep learning},
abstract = {The face is the window to the soul. This is what the 19th-century French doctor Duchenne de Boulogne thought. Using electric shocks to stimulate muscular contractions and induce bizarre-looking expressions, he wanted to understand how muscles produce facial expressions and reveal the most hidden human emotions. Two centuries later, this research field remains very active. We see automatic systems for recognizing emotion and facial expression being applied in medicine, security and surveillance systems, advertising and marketing, among others. However, there are still fundamental questions that scientists are trying to answer when analyzing a person’s emotional state from their facial expressions. Is it possible to reliably infer someone’s internal state based only on their facial muscles’ movements? Is there a universal facial setting to express basic emotions such as anger, disgust, fear, happiness, sadness, and surprise? In this research, we seek to address some of these questions through convolutional neural networks. Unlike most studies in the prior art, we are particularly interested in examining whether characteristics learned from one group of people can be generalized to predict another’s emotions successfully. In this sense, we adopt a cross-dataset evaluation protocol to assess the performance of the proposed methods. Our baseline is a custom-tailored model initially used in face recognition to categorize emotion. By applying data visualization techniques, we improve our baseline model, deriving two other methods. The first method aims to direct the network’s attention to regions of the face considered important in the literature but ignored by the baseline model, using patches to hide random parts of the facial image so that the network can learn discriminative characteristics in different regions. The second method explores a loss function that generates data representations in high-dimensional spaces so that examples of the same emotion class are close and examples of different classes are distant. Finally, we investigate the complementarity between these two methods, proposing a late-fusion technique that combines their outputs through the multiplication of probabilities. We compare our results to an extensive list of works evaluated in the same adopted datasets. In all of them, when compared to works that followed an intra-dataset protocol, our methods present competitive numbers. Under a cross-dataset protocol, we achieve state-of-the-art results, outperforming even commercial off-the-shelf solutions from well-known tech companies.}
}
@article{LIU2022103380,
title = {3DFP-FCGAN: Face completion generative adversarial network with 3D facial prior},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103380},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103380},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002510},
author = {Jing Liu and Weikang Wang and Jiexiao Yu and Chunping Zhang and Yuting Su},
keywords = {Face completion, 2D geometry priors, 3D depth prior, Generative model},
abstract = {Face completion is a domain-specific image inpainting problem. Most existing face completion methods fail to synthesize fine-grained facial structures due to the undifferentiated treatment of face images and other scene images. To handle this problem, we propose an end-to-end deep generative model based approach which makes full use of the facial prior knowledge, including 2D facial geometry priors from facial parsing maps and landmarks, as well as the 3D depth prior. We adopt a coarse-to-fine inpainting framework where the 2D facial geometry priors based on coarse faces are extracted to guide the refinement network for better planar facial textures and structures. Moreover, a novel 3D regularized reconstruction loss is proposed for the enhancement of the stereo perception of generated faces. Experimental results on two large-scale benchmarks CelebA and CelebA-HQ show that our method significantly outperforms the state-of-the-arts in generating more visually realistic and pleasing faces. Code is available at .}
}
@article{CHAKRABORTY2021103273,
title = {Sensor-based image manipulation localization with Discriminative Random fields and Graph Cut},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103273},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103273},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001784},
author = {Sujoy Chakraborty and Matthias Kirchner},
keywords = {Photo-response non-uniformity, Discriminative random field, Manipulation localization, Correlation predictor},
abstract = {We present PRNU-based image manipulation localization as a probabilistic labeling task in a flexible discriminative random field (DRF) setup. Instead of reaching local decisions independent of each other, discriminative random fields incorporate local inter-label dependencies while keeping the formulation general enough to make label assignments depend on both local and non-local image characteristics. With an improved form of association potential combining normalized correlation and the deviation of the measured correlation from the expected correlation and an interaction potential defined as the weighted L2 norm squared between intensities of neighboring pixels, we were able to localize even considerably small manipulations on realistic tampered images. We experimented with different combinations of window sizes to capture features to predict the correlation more accurately than already existing algorithms. Experimental results indicate that our algorithm outperforms recent state of the art methods based on multiscale analysis strategies. We also found that for inspecting manipulated images which are JPEG compressed, it helps to train the predictor with JPEG images rather than with uncompressed images and for all quality factors, it is possible to work with two predictors, one trained for images with lower quality factors and another for higher quality factors.}
}
@article{SONG2022103411,
title = {A novel partial-to-partial registration method based on sampling network},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103411},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103411},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002753},
author = {Yanan Song and Weiming Shen and Peng Lu},
keywords = {Point cloud registration, Partial correspondence, Sampling network, Deep learning},
abstract = {Point cloud registration is mainly to estimate a rigid transformation between point clouds. The traditional optimization-based registration method requires a good initial position, and it is easy to fall into a local optimal solution. Some learning-based methods are introduced to reduce the dependence on the initial transformation, but they cannot handle partial-to-partial registration tasks. This paper proposes a learning-based registration method for partial-to-partial scenario. The local geometry is encoded into the feature representation of each point. A transformer network is used to enhance attention features. A designed sampling network down-sample key matching points and their corresponding features. The rigid transformation is calculated according to virtual correspondence by a singular value decomposition layer. The ModelNet40 dataset and Stanford 3D Scanning models are used to test the registration performance. Experimental results show that the proposed method achieves better registration accuracy than traditional methods, and it is robust to any initial transformation and noise.}
}
@article{XING2022103374,
title = {DVL2021: An ultra high definition video dataset for perceptual quality study},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103374},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103374},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002479},
author = {Fengchuang Xing and Yuan-Gen Wang and Hanpin Wang and Jiefeng He and Jinchun Yuan},
keywords = {UHD video dataset, Video quality assessment, Authentic distortion, Synthetic distortion},
abstract = {This paper describes an ultra high definition (UHD) video dataset named DVL2021 for the perceptual study of video quality assessment (VQA). To our knowledge, DVL2021 is the first authentically distorted 4K (3840 × 2160) UHD video quality dataset. The dataset contains 206 versatile 4K UHD video sequences, which are all collected in in-the-wild scenarios. Each sequence is captured at 50 frames per second (fps), stored in raw 10-bit 4:2:0 YUV format, and has a duration of 10 s. Following the subjective evaluation method of TV image quality granted by ITU-R BT.500-13, 32 unique participants take part in the manual annotation process, whose ages are from teenage to sixties (32.7 years old on average). DVL2021 has the following merits: (1) enormous variety of video contents, (2) captured by different types of cameras, (3) complex types and multiple levels of authentic distortion, (4) broadly distributed temporal/spatial information, and (5) a wide spectrum of mean opinion scores (MOS) distribution. Furthermore, we conduct a benchmark experiment by evaluating several mainstream VQA methods on DVL2021. The baseline results are higher than 0.75 in Spearman’s rank order correlation coefficient (SROCC) metric. Our study provides a basis for the UHD VQA problem. DVL2021 is publicly available at https://github.com/GZHU-DVL/DVL2021.}
}
@article{REZAEI2022103357,
title = {A-contrario framework for detection of alterations in varnished surfaces},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103357},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103357},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002352},
author = {Alireza Rezaei and Sylvie {Le Hégarat-Mascle} and Emanuel Aldea and Piercarlo Dondi and Marco Malagodi},
keywords = {A-contrario framework, Defect detection, Preventive conservation, Historical violins},
abstract = {Preventive conservation is the constant monitoring of the state of conservation of an artwork to reduce the risk of damages and so to minimize the necessity of restorations. Many methods have been proposed during time, generally including a mix of different analytical techniques. In this work, we present a probabilistic approach based on the a-contrario framework for the detection of alterations on varnished surfaces, in particular those of historical musical instruments. Our method is a one step Number of False Alarms (NFA) clustering solution which considers simultaneously gray-level and spatial density information in a single background model. The proposed approach is robust to noise and avoids parameter tuning as well as any assumption about the shape and size of the worn-out areas. Tests have been conducted on UV induced fluorescence (UVIFL) image sequences included in the “Violins UVIFL imagery” dataset. UVIFL photography is a well known diagnostic technique used to see details of a surface not perceivable with visible light. The obtained results prove the capability of the algorithm to properly detect the altered regions. Comparisons with other the state-of-the-art clustering methods show improvement in both precision and recall.}
}
@article{KAPLAN2021103315,
title = {Scale aware remote sensing image enhancement using rolling guidance},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103315},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103315},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002078},
author = {N.H. Kaplan and I. Erer},
keywords = {Image enhancement, Remote sensing images, Rolling guidance filter, Edge preserving filters, Multiscale decomposition},
abstract = {Enhancement of remotely sensed images is a challenging problem, since the enhanced image has to have an improved contrast and edge information while preserving the original radiance values as much as possible. In this paper, a scale aware enhancement method based on rolling guidance is proposed for remotely sensed images. For each scale, a guidance image is defined and the approximation image is provided by an iterative joint filtering of the approximation and guidance images. Then the extracted details are amplified through an adaptive scheme and added to the final level approximation layer to provide the resulting enhanced image. A comparative study between the proposed methods with classical edge preserving filters and traditional methods have been carried out by using several criteria. The proposed methods have an average of 12% improvement for contrast gain (CG) metric and 81% improvement for enhancement measurement (EME) metric compared to the closest comparison method.}
}
@article{GUAN2021103333,
title = {Improving UNIWARD distortion function via isotropic construction and hierarchical merging},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103333},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103333},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002194},
author = {Qingxiao Guan and Hefeng Chen and Weiming Zhang and Nenghai Yu},
keywords = {Steganography, Distortion, UNIWARD},
abstract = {Distortion function is designed for evaluating the cost of modifications in adaptive steganography. UNIWARD is a successful and popular distortion scheme which achieves high performance both for spatial and JPEG images. In this paper, we analyze the UNIWARD scheme with some empirical rules of distortion function designation. Based on that we propose our scheme to improve UNIWARD distortion. In our scheme, we focus on the symmetric characteristic of UNIWARD, and suggest that not only use original wavelet filters but also their flippings to calculate sub-models of UNIWARD distortion to maintain its isotropic properties. Moreover, we design several schemes to merge sub-models, which could maintain its invariance regard to flipping or rotation and improve its security against steganalysis detection. Experimental results show our revised UNIWARD achieves better performance for spatial and JPEG image in comparison with original UNIWARD.}
}
@article{CHEN2022103409,
title = {Visual secret sharing scheme with (n,n) threshold based on WeChat Mini Program codes},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103409},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103409},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100273X},
author = {Jia Chen and Yongjie Wang and Xuehu Yan and Jiayu Wang and Longlong Li},
keywords = {Visual secret sharing, WeChat Mini Program codes, Error correction, Access control},
abstract = {With the widespread use of WeChat Mini Programs, their security has attracted more and more attention. The WeChat Mini Program can be accessed by scanning a WeChat Mini Program code. We should protect the code thus to protect the Mini Program. In this paper, based on studying the function in each pattern of WeChat Mini Program codes, a visual secret sharing (VSS) scheme for WeChat Mini Program codes (MPCVSS) with (n,n)(n≥4) threshold is proposed to control and identify the users of Mini Program. MPCVSS combines the error-correcting characteristic of WeChat Mini Program codes with the theory of VSS. A secret WeChat Mini Program code is shared into n shared codes slightly modified from n cover codes. Each shared code is a valid code that can be scanned and decoded correctly. The secret code can be recovered by XORing n shares. The recovered code can be decoded as the same secret Mini Program. Theoretical analysis and experiments show that the proposed VSS scheme is practical and feasible.}
}
@article{BAISA2021103279,
title = {Occlusion-robust online multi-object visual tracking using a GM-PHD filter with CNN-based re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103279},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103279},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001814},
author = {Nathanael L. Baisa},
keywords = {Online visual tracking, GM-PHD filter, Prediction, CNN features, Augmented likelihood, Re-identification},
abstract = {We propose a novel online multi-object visual tracker using a Gaussian mixture Probability Hypothesis Density (GM-PHD) filter and deep appearance learning. The GM-PHD filter has a linear complexity with the number of objects and observations while estimating the states and cardinality of time-varying number of objects, however, it is susceptible to miss-detections and does not include the identity of objects. We use visual-spatio-temporal information obtained from object bounding boxes and deeply learned appearance representations to perform estimates-to-tracks data association for target labeling as well as formulate an augmented likelihood and then integrate into the update step of the GM-PHD filter. We also employ additional unassigned tracks prediction after the data association step to overcome the susceptibility of the GM-PHD filter towards miss-detections caused by occlusion. Extensive evaluations on MOT16, MOT17 and HiEve benchmark data sets show that our tracker significantly outperforms several state-of-the-art trackers in terms of tracking accuracy and identification.}
}
@article{YE2022103378,
title = {A comprehensive framework of multiple semantics preservation in neural style transfer},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103378},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103378},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002509},
author = {Wujian Ye and Xueke Zhu and Zuoteng Xu and Yijun Liu and Chin-Chen Chang},
keywords = {Neural style transfer, Multiple semantic preservation, Comprehensive framework, Stylized artistic quality},
abstract = {Recently neural style transfer has achieved great development, but there is still a big gap compared with manual creation. Most of the existing methods ignore the comprehensive consideration of preserving various semantic information of original content images, resulting in distortion or loss of original content features of the generated works, which are dull and difficult to convey the original themes and emotions. In this paper, we analyze the ability of the existing methods to maintain single semantic information and propose a fast style transfer framework with multi-semantic preservation. The experiments indicate that our method can effectively retain the original semantic information including salience and depth features, so that the final artwork has better visual effect by highlighting its regional focus and depth information. Compared with existing methods, our method has better ability in semantic preservation and can generate more artworks with distinct regions, controllable semantics, diverse contents and rich emotions.}
}
@article{DHAMIJA2022103393,
title = {A novel active shape model-based DeepNeural network for age invariance face recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103393},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103393},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002625},
author = {Ashutosh Dhamija and R.B. Dubey},
keywords = {Age invariance face recognition, CNN architecture, Improved Active shape model, Handcrafted and deep features, Principal component analysis, The features reduction},
abstract = {Scientific efforts have expanded in age-invariant face recognition (AIFR). Matching faces of large age difference is, therefore, a problem, mostly because of a substantial disparity in the appearance of both young and old age. Owing to age, both the appearance and shape of the face are impaired, making recognition of the face the most challenging task. In recent years, AIFR has become a very common and demanding task. The set of feature extraction and classification algorithm is of prime importance in this field. As the numbers of features obtained from the datasets are large, there is a need to introduce a dimensionality reduction method to map high dimensionality feature space to low variance filter to form the final integrated face age model to be used in the classification process. In this paper, we introduced a novel concept of an improved Active Shape Model (ASM) in conjunction with a specially designed 7-layered Convolutional Neural Network (CNN) in order to accomplish a combination of feature extraction and classification in a single unit. The study approach involves conducting extensive experiments to evaluate the proposed system's performance using three standard datasets: FG-NET, LAG, and CACD. The results reveal that the proposed method outperforms state-of-the-art approaches and achieves excellent accuracy in face recognition across age. The maximum accuracies achieved by demonstrated ASM-CNN methodology for FG-NET, LAG, and CACD databases are 95.02%, 91.76 % and 99.4 % respectively.}
}
@article{CHU2021103319,
title = {Attention guided feature pyramid network for crowd counting},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103319},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103319},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002108},
author = {Huanpeng Chu and Jilin Tang and Haoji Hu},
keywords = {Crowd counting, Feature pyramid network, Attention mechanism, Density map generation},
abstract = {Crowd counting has become a hot topic because of its wide applications in video surveillance and public security. However, one main problem of the deep learning methods for crowd counting is that the location information about the crowd is degraded irreversibly due to the spatial down-sampling of convolutional neural networks, which degrades the quality of generated density maps. To remedy the above problem, we propose an attention guided feature pyramid network (AG-FPN) for crowd counting, which can adaptively generate a high-quality density map with accurate spatial locations by combining the high- and low-level features. An attention block is added to each encoder layer to further emphasize the crowd regions and suppress the background clutters in feature extraction. Experimental results on the ShanghaiTech, UCF_CC_50, WorldExpo’10 and UCF-QNRF datasets demonstrate the superiority of the proposed method over state-of-the-art approaches.}
}
@article{XIAO2021103373,
title = {Structure-prior deep neural network for lane detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103373},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103373},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002467},
author = {Degui Xiao and Lin Zhuo and Jianfang Li and Jiazhi Li},
keywords = {Lane marking detection, Deep neural network, Structure-prior},
abstract = {Lane detection is an important task of road environment perception for autonomous driving. Deep learning methods based on semantic segmentation have been successfully applied to lane detection, but they require considerable computational cost for high complexity. The lane detection is treated as a particular semantic segmentation task due to the prior structural information of lane markings which have long continuous shape. Most traditional CNN are designed for the representation learning of semantic information, while this prior structural information is not fully exploited. In this paper, we propose a recurrent slice convolution module (called RSCM) to exploit the prior structural information of lane markings. The proposed RSCM is a special recurrent network structure with several slice convolution units (called SCU). The RSCM could obtain stronger semantic representation through the propagation of the prior structural information in SCU. Furthermore, we design a distance loss in consideration of the prior structure of lane markings. The lane detection network can be trained more steadily via the overall loss function formed by combining segmentation loss with the distance loss. The experimental results show the effectiveness of our method. We achieve excellent computation efficiency while keeping decent detection quality on lane detection benchmarks and the computational cost of our method is much lower than the state-of-the-art methods.}
}
@article{WANG2021103293,
title = {Video object segmentation via random walks on two-frame graphs comprising superpixels},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103293},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103293},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001930},
author = {Hui Wang and Weibin Liu and Weiwei Xing},
keywords = {Random walks, Video object segmentation, Optical flow gradient, Spatiotemporal consistency},
abstract = {We propose a novel video object segmentation method employing random walkers to travel on graphs constructed on two consecutive frames. First, we estimate the initial foreground and background distributions by minimising an energy function that incorporates the stationary distributions of the random walks. The random walkers frequently travel between similar nodes of the graph constructed on two adjacent frames, which enables the incorporation of the inter-frame information into the energy function effectively and elegantly. Then, we refine the initial results by simulating the movements of multiple random walkers. We process the sequence in a recursive manner, which naturally propagates the previous segmentation labels to the subsequent frames. Additionally, we develop a strategy for adjusting the superpixel number using region similarity and the average Frobenius norm of optical flow gradient. This strategy can improve performance significantly. Furthermore, we discuss the feature selection problem in the method to select a more effective feature representation. Extensive and comparable experiments on Segtrack and Segtrack v2 demonstrate that the proposed algorithm yields higher performance than several recent state-of-the-art approaches.}
}
@article{SHI2021103312,
title = {Residual attention-based tracking-by-detection network with attention-driven data augmentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103312},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103312},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002066},
author = {Zaifeng Shi and Cheng Sun and Qingjie Cao and Zhe Wang and Qiangqiang Fan},
keywords = {Object tracking, Data augmentation, Attention mechanism, Deep learning},
abstract = {Tracking-by-detection (TBD) is a significant framework for visual object tracking. However, current trackers are usually updated online based on random sampling with a probability distribution. The performance of the learning-based TBD trackers is limited by the lack of discriminative features, especially when the background is full of semantic distractors. We propose an attention-driven data augmentation method, in which a residual attention mechanism is integrated into the TBD tracking network as supplementary references to identify discriminative image features. A mask generating network is used to simulate changes in target appearances to obtain positive samples, where attention information and image features are combined to identify discriminative features. In addition, we propose a method for mining hard negative samples, which searches for semantic distractors with the response of the attention module. The experiments on the OTB2015, UAV123, and LaSOT benchmarks show that this method achieves competitive performance in terms of accuracy and robustness.}
}
@article{LONG2021103272,
title = {Depth completion towards different sensor configurations via relative depth map estimation and scale recovery},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103272},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103272},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001772},
author = {Yangqi Long and Huimin Yu and Biyang Liu},
keywords = {Depth estimation, Depth completion, Relative depth, Scale recovery, Geometry structure},
abstract = {Depth completion, which combines additional sparse depth information from the range sensors, substantially improves the accuracy of monocular depth estimation, especially using the deep-learning-based methods. However, these methods can hardly produce satisfactory depth results when the sensor configuration changes at test time, which is important for real-world applications. In this paper, the problem is tackled by our proposed novel two-stage mechanism, which decomposes depth completion into two subtasks, namely relative depth map estimation and scale recovery. The relative depth map is first estimated from a single color image with our designed scale-invariant loss function. Then the scale map is recovered with the additional sparse depth. Experiments on different densities and patterns of the sparse depth input show that our model always produces satisfactory depth results. Besides, our approach achieves state-of-the-art performance on the indoor NYUv2 dataset and performs competitively on the outdoor KITTI dataset, demonstrating the effectiveness of our method.}
}
@article{WANG2021103355,
title = {Discrete hashing with triple supervision learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103355},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103355},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002340},
author = {Shaohua Wang and Xiao Kang and Fasheng Liu and Xiushan Nie and Xingbo Liu},
keywords = {ANN search, Discrete hashing, Triple supervision},
abstract = {In recent years, discrete supervised hashing methods have attracted increasing attention because of their high retrieval efficiency and precision. However, in these methods, some effective semantic information is typically neglected, which means that all the information is not sufficiently utilized. Moreover, these methods often only decompose the first-order features of the original data, ignoring the more fine-grained higher-order features. To address these problems, we propose a supervised hashing learning method called discrete hashing with triple supervision learning (DHTSL). Specifically, we integrate three aspects of semantic information into this method: (1) the bidirectional mapping of semantic labels; (2) pairwise similarity relations; (3) second-order features from the original data. We also design a discrete optimization method to solve the proposed objective function. Moreover, an out-of-sample extension strategy that can better maintain the independence and balance of hash codes is employed to improve retrieval performance. Extensive experiments on three widely used datasets demonstrate its superior performance.}
}
@article{NGUYEN2021103330,
title = {Dynamic texture representation based on oriented magnitudes of Gaussian gradients},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103330},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103330},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002182},
author = {Thanh Tuan Nguyen and Thanh Phuong Nguyen and Frédéric Bouchara},
keywords = {Dynamic textures, Gaussian-filtered derivatives, Oriented magnitudes, LBP, CLBP, Video representation},
abstract = {Efficiently capturing shape and turbulent motions of dynamic textures (DTs) for video description is a challenge in real applications due to the negative influences of the well-known problems: environmental elements, illumination, scale, and noise. In this paper, we propose an efficient and simple framework for DT representation based on the oriented features of high-order Gaussian gradients. Firstly, 2D/3D Gaussian-based filtering kernels in high-order partial derivatives are taken into account the video analysis as a preprocessing step to obtain corresponding gradient-filtered images/volumes. After that, the oriented features, which are robust against the above issues, are extracted by decomposing the Gaussian derivative magnitudes into oriented components. Finally, a shallow local encoding is utilized for structuring spatio-temporal features from these oriented magnitudes. This allows constructing discriminative descriptors with promising performances compared to those based on the non-oriented ones. Experimental results for DT classification task on benchmark datasets have verified the interest of our proposal.}
}
@article{FERNANDES2021103291,
title = {Using curved angular intra-frame prediction to improve video coding efficiency},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103291},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103291},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001929},
author = {Ramon Fernandes and Gustavo Sanchez and Rodrigo Cataldo and Luciano Agostini and César Marcon},
keywords = {Intra-frame prediction, High Efficiency Video Coding (HEVC), Video coding, Predictive coding},
abstract = {This article presents a new curved-based intra-frame prediction method for current and upcoming video coding standards. Our proposal extends conventional straight-line angular modes found on intra-prediction tools to model curved texture characteristics, enhancing the intra-frame prediction process. Our work targets the High Efficiency Video Coding (HEVC) standard for evaluation, although our curved-based method can be used by any other video coding standard. We model curved intra-frame prediction using an offset-based displacement calculation to each predicted sample. The proposal incurs a small bitstream overhead for transmitting the displacement information, which is offset by encoding efficiency gains. Experimental results demonstrate reduced residual energy; consequently, improving BD-Rate for the tested sequences. Evaluations applying eight curve displacement values show an average BD-Rate reduction of 2.69%, 2.49%, and 0.86% for All-Intra-8, All-Intra 10, and Random-Access configurations, respectively. The proposal allows further BD-Rate improvements, albeit at higher encoding complexity.}
}
@article{HUANG2022103437,
title = {A self-embedding secure fragile watermarking scheme with high quality recovery},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103437},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103437},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000025},
author = {Li Huang and Da Kuang and Cheng-long Li and Yu-jian Zhuang and Shao-hua Duan and Xiao-yi Zhou},
keywords = {Fragile watermarking, Tamper detection and localization, Image recover, Cloud storage},
abstract = {In recent years, with the development of cloud storage, more and more people upload images to the cloud for storage. However, confidentiality and integrity issues may arise during transmission and storage to the cloud. Aiming at these security problems, a fragile watermarking scheme based on the encrypted domain is proposed. A watermark is divided into two types, one is for detection, the other is for recovery. After embedding the two types of watermarks into the host image, the watermarked image will be transferred to the cloud for storage. A three-level tamper detection mechanism is used in the detection process, and the first-level tamper detection can be processed in the cloud. While in recovery process, a mechanism of “block-level detection, pixel-level recovery” is proposed to recover the tampered area. The experimental results show that the watermarked image has greatly changed the original image and guarantees the confidentiality. The three-level tamper detection mechanism can accurately detect the tampered area, the image can be effectively restored in different situations, when the tampering rate is as high as 80%, the average PSNR reaches 34.62 dB, and the average SSIM is higher than 0.93.}
}
@article{SAADSHAKEEL2022103423,
title = {Deep low-rank feature learning and encoding for cross-age face recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103423},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103423},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002856},
author = {M. {Saad Shakeel} and Kin-Man Lam},
keywords = {Cross-age face recognition, Feature encoding, Kernel canonical correlation analysis, Low-rank features, Manifold learning},
abstract = {Cross-age face recognition (CAFR) is a challenging task, due to significant intra-personal variations. Furthermore, the training and testing data may contain random noise components. To address these issues, this paper proposes a deep low-rank feature learning and encoding method. Firstly, our method employs manifold learning in the low-rank optimization, which preserves the global and local structure of the data samples, while learning the clean low-rank features. Secondly, we encode the low-rank features using our locality-constrained feature encoding method, which learns an age-insensitive codebook from training data, and enables the intra-class samples to share the same local bases in a codebook. In the testing stage, the gallery and probe features are encoded by the learned codebook, which represents the images of the same identity by similar codewords for recognition. Furthermore, the periocular region of human faces is investigated for CAFR. Extensive experiments on five datasets demonstrate the effectiveness of our method.}
}
@article{LIANG2021103356,
title = {Fixation prediction for advertising images: Dataset and benchmark},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103356},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103356},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002339},
author = {Song Liang and Ruihang Liu and Jiansheng Qian},
keywords = {Saliency prediction, Advertising, OCR, Lightweight architecture},
abstract = {Existing saliency prediction methods focus on exploring a universal saliency model for natural images, relatively few on advertising images which typically consists of both textual regions and pictorial regions. To fill this gap, we first build an advertising image database, named ADD1000, recording 57 subjects’ eye movement data of 1000 ad images. Compared to natural images, advertising images contain more artificial scenarios and show stronger persuasiveness and deliberateness, while the impact of this scene heterogeneity on visual attention is rarely studied. Moreover, text elements and picture elements express closely related semantic information to highlight product or brand in ad images, while their respective contribution to visual attention is also less known. Motivated by these, we further propose a saliency prediction model for advertising images based on text enhanced learning (TEL-SP), which comprehensively considers the interplay between textual region and pictorial region. Extensive experiments on the ADD1000 database show that the proposed model outperforms existing state-of-the-art methods.}
}
@article{YU2021103276,
title = {Deep feature enhancing and selecting network for weakly supervised temporal action localization},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103276},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103276},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001802},
author = {Jiaruo Yu and Yongxin Ge and Xiaolei Qin and Ziqiang Li and Sheng Huang and Feiyu Chen},
keywords = {Weakly supervised, Temporal action localization, Deep learning},
abstract = {Weakly supervised temporal action localization is a challenging computer vision problem that uses only video-level labels and lacks the supervision of temporal annotations. In this task, the majority of existing methods usually identify the most discriminative snippets and ignore other relevant snippets. To address this problem, we propose a deep feature enhancing and selecting network. It generates multiple masks for both capturing more complete temporal interval of actions and keeping its high classification accuracy. After that, we further propose a novel selection strategy to balance the influence of multiple masks and improve the model performance. In the experiments, we evaluate the proposed method on the THUMOS’14 and ActivityNet datasets, and the results show the effectiveness of our approach for weakly supervised temporal action localization.}
}
@article{BARKOKY2022103371,
title = {Complex Network-based features extraction in RGB-D human action recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103371},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103371},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002455},
author = {Alaa Barkoky and Nasrollah Moghaddam Charkari},
keywords = {Human action recognition, Complex network, Meta-path, 3D skeleton joints},
abstract = {Analysis of human behavior through visual information has been one of the active research areas in computer vision community during the last decade. Vision-based human action recognition (HAR) is a crucial part of human behavior analysis, which is also of great demand in a wide range of applications. HAR was initially performed via images from a conventional camera; however, depth sensors have recently embedded as an additional informative resource to cameras. In this paper, we have proposed a novel approach to largely improve the performance of human action recognition using Complex Network-based feature extraction from RGB-D information. Accordingly, the constructed complex network is employed for single-person action recognition from skeletal data consisting of 3D positions of body joints. The indirect features help the model cope with the majority of challenges in action recognition. In this paper, the meta-path concept in the complex network has been presented to lessen the unusual actions structure challenges. Further, it boosts recognition performance. The extensive experimental results on two widely adopted benchmark datasets, the MSR-Action Pairs, and MSR Daily Activity3D indicate the efficiency and validity of the method.}
}
@article{HUANG2022103402,
title = {A new head pose tracking method based on stereo visual SLAM},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103402},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103402},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002698},
author = {Suibin Huang and Kun Yang and Hua Xiao and Peng Han and Jian Qiu and Li Peng and Dongmei Liu and Kaiqing Luo},
keywords = {Head pose tracking, Stereo visual SLAM, Bundle adjustment},
abstract = {Real-time and reliable head pose tracking is the basis of human–computer interaction and face analysis applications. Aiming at the problems of accuracy and real time performance in current tracking method, a new head pose tracking method based on stereo visual SLAM is proposed in this paper. The sparse head map is constructed based on ORB feature points extraction and stereo matching, then the 3D-2D matching relations between 3D mappoints and 2D feature points are obtained by projection matching. Finally, the camera pose solved by the Bundle Adjustment is converted to head pose, which realizes the tracking of head pose. The experimental results show that this method can obtain high precise head pose. The mean errors of three Euler angles are all less than 1°. Therefore, the proposed head pose tracking method can track and estimate precise head pose in real time under smooth background.}
}
@article{ZHANG2021103348,
title = {YOLSO: You Only Look Small Object},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103348},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103348},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002297},
author = {Jinpu Zhang and Lei Zhang and Tianyu Liu and Yuehuan Wang},
keywords = {Small object detection, Background-aware, Granular feature aggregation, Accurate location, High speed},
abstract = {Small object detection is challenging and far from satisfactory. Most general object detectors suffer from two critical issues with small objects: (1) Feature extractor based on classification network cannot express the characteristics of small objects reasonably due to insufficient appearance information of targets and a large amount of background interference around them. (2) The detector requires a much higher location accuracy for small objects than for general objects. This paper proposes an effective and efficient small object detector YOLSO to address the above problems. For feature representation, we analyze the drawbacks in previous backbones and present a Half-Space Shortcut(HSSC) module to build a background-aware backbone. Furthermore, a coarse-to-fine Feature Pyramid Enhancement(FPE) module is introduced for layer-wise aggregation at a granular level to enhance the semantic discriminability. For loss function, we propose an exponential L1 loss to promote the convergence of regression, and a focal IOU loss to focus on prime samples with high classification confidence and high IOU. Both of them significantly improves the location accuracy of small objects. The proposed YOLSO sets state-of-the-art results on two typical small object datasets, MOCOD and VeDAI, at a speed of over 200 FPS. In the meantime, it also outperforms the baseline YOLOv3 by a wide margin on the common COCO dataset.}
}
@article{WEN2021103311,
title = {Towards better semantic consistency of 2D medical image segmentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103311},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103311},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002054},
author = {Yang Wen and Leiting Chen and Yu Deng and Jin Ning and Chuan Zhou},
keywords = {Image segmentation, Convolutional neural network, Semantics, Deep learning},
abstract = {The latest deep neural networks for medical segmentation typically utilize transposed convolutional filters and atrous convolutional filters for spatial restoration and larger receptive fields, leading to dilution and inconsistency of visual semantics. To address such issues, we propose a novel attentional up-concatenation structure to build an auxiliary path for direct access to multi-level features. In addition, we employ a new structural loss to bring better morphological awareness and reduce the segmentation flaws caused by the semantic inconsistencies. Thorough experiments on the challenging optic cup/disc segmentation, cellular segmentation and lung segmentation tasks were performed to evaluate the proposed methods. Further ablation analysis demonstrated the effectiveness of the different components of the model and illustrated its efficiency. The proposed methods achieved the best performance and speed compared to the state-of-the-art models in three tasks on seven public datasets, including DRISHTI-GS, RIM-r3, REFUGE, MESSIDOR, TNBC, GlaS and LUNA.}
}
@article{LIU2021103332,
title = {3-D Epanechnikov Mixture Regression in integral imaging compression},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103332},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103332},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002170},
author = {Boning Liu and Yan Zhao and Xiaomeng Jiang and Shigang Wang and Jian Wei},
keywords = {3-D Epanechnikov Kernel, 3-D Epanechnikov Mixture Regression, 3D holoscopic image compression, Integral imaging compression, Image modeling},
abstract = {Integral imaging is a kind of 3D display with no glasses, which represents the future developments. Elementary image array (EIA) is an essential component of integral imaging. Our coding framework includes pre-processing, modeling, and reconstruction. We acquire the sub-EIA from the original EIA and get the offsets between adjacent elementary images (EIs) through pre-processing. As for modeling, we get the optimal combination of 3-D Epanechnikov Mixture Regression (3-D EMR) or 3-D Gaussian Mixture Regression (3-D GMR) by Elementary Image Adaptive Model Selection (EI-AMLS) algorithm to achieve the best modeling of sub-EIA. Finally, the linear-based reconstruction is completed according to the correlation between adjacent EIs. Our decoded images realize a clearer outline reconstruction and more superior coding efficiency than HEVC and JPEG2000 below about 0.05bpp. Furthermore, the proposed method can achieve the same visual effect as HEVC with only 15% to 80% time consumed.}
}
@article{WU2021103289,
title = {Sequential alignment attention model for scene text recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103289},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103289},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001917},
author = {Yan Wu and Jiaxin Fan and Renshuai Tao and Jiakai Wang and Haotong Qin and Aishan Liu and Xianglong Liu},
keywords = {Scene text recognition, Attention-gated recurrent unit, Attention mechanism, Connectionist temporal classification},
abstract = {Scene text recognition has been a hot research topic in computer vision due to its various applications. The state-of-the-art solutions usually depend on the attention-based encoder-decoder framework that learns the mapping between input images and output sequences in a purely data-driven way. Unfortunately, there often exists severe misalignment between feature areas and text labels in real-world scenarios. To address this problem, this paper proposes a sequential alignment attention model to enhance the alignment between input images and output character sequences. In this model, an attention gated recurrent unit (AGRU) is first devised to distinguish the text and background regions, and further extract the localized features focusing on sequential text regions. Furthermore, CTC guided decoding strategy is integrated into the popular attention-based decoder, which not only helps to boost the convergence of the training but also enhances the well-aligned sequence recognition. Extensive experiments on various benchmarks, including the IIIT5k, SVT, and ICDAR datasets, show that our method substantially outperforms the state-of-the-art methods.}
}
@article{HUSSAIN2021103269,
title = {A novel deep learning framework for double JPEG compression detection of small size blocks},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103269},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103269},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001759},
author = {Israr Hussain and Shunquan Tan and Bin Li and Xinghong Qin and Dostdar Hussain and Jiwu Huang},
keywords = {Multimedia forensics, Discrete cosine transform, Deep learning, Double JPEG compression, Convolutional neural network},
abstract = {Double JPEG compression detection plays a vital role in multimedia forensics, to find out whether a JPEG image is authentic or manipulated. However, it still remains to be a challenging task in the case when the quality factor of the first compression is much higher than that of the second compression, as well as in the case when the targeted image blocks are quite small. In this work, we present a novel end-to-end deep learning framework taking raw DCT coefficients as input to distinguish between single and double compressed images, which performs superior in the above two cases. Our proposed framework can be divided into two stages. In the first stage, we adopt an auxiliary DCT layer with sixty-four 8 × 8 DCT kernels. Using a specific layer to extract DCT coefficients instead of extracting them directly from JPEG bitstream allows our proposed framework to work even if the double compressed images are stored in spatial domain, e.g. in PGM, TIFF or other bitmap formats. The second stage is a deep neural network with multiple convolutional blocks to extract more effective features. We have conducted extensive experiments on three different image datasets. The experimental results demonstrate the superiority of our framework when compared with other state-of-the-art double JPEG compression detection methods either hand-crafted or learned using deep networks in the literature, especially in the two cases mentioned above. Furthermore, our proposed framework can detect triple and even multiple JPEG compressed images, which is scarce in the literature as far as we know.}
}
@article{BEDDIAR2022103407,
title = {Fall detection using body geometry and human pose estimation in video sequences},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103407},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103407},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002728},
author = {Djamila Romaissa Beddiar and Mourad Oussalah and Brahim Nini},
keywords = {Body geometry, Elderly assistance, Fall detection, Pose estimation, Video sequence},
abstract = {According to the World Health Organization, falling is a significant health problem that causes thousands of deaths every year. Fall detection and fall prediction tasks enable accurate medical assistance to vulnerable populations whenever required, allowing local authorities to predict daily health care resources and to reduce fall damages accordingly. We present in this paper, a fall detection approach that explores human body geometry available at different frames of the video sequence. Especially, pose estimation, the angle and the distance between the vector formed by the head-centroid of the identified facial image and the center hip of the body, and the vector aligned with the horizontal axis of the center hip, are employed to construct new distinctive image features. A two-class Support Vector Machine (SVM) classifier and a Temporal Convolution Network (TCN) are trained on the newly constructed feature images. At the same time, a Long-Short-Term Memory (LSTM) network is trained on the calculated angle and distance sequences to classify fall and non-fall activities. We perform experiments on the Le2i FD dataset and the UR FD dataset, where we also propose a cross-dataset evaluation. The results demonstrate the effectiveness and efficiency of the developed approach.}
}
@article{2023103917,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103917},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(23)00167-0},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001670}
}
@article{LIAO2022103422,
title = {Rotation-aware correlation filters for robust visual tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103422},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103422},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002844},
author = {Jiawen Liao and Chun Qi and Jianzhong Cao and Xiaofang Wang and Long Ren and Chaoning Zhang},
keywords = {Correlation filter, Visual tracking, Rotation, Phase correlation, Kalman filter},
abstract = {Recent years have witnessed several modified discriminative correlation filter (DCF) models exhibiting excellent performance in visual tracking. A fundamental drawback to these methods is that rotation of the target is not well addressed which leads to model deterioration. In this paper, we propose a novel rotation-aware correlation filter to address the issue. Specifically, samples used for training of the modified DCF model are rectified when rotation occurs, rotation angle is effectively calculated using phase correlation after transforming the search patch from Cartesian coordinates to the Log-polar coordinates, and an adaptive selection mechanism is further adopted to choose between a rectified target patch and a rectangular patch. Moreover, we extend the proposed approach for robust tracking by introducing a simple yet effective Kalman filter prediction strategy. Extensive experiments on five standard benchmarks show that the proposed method achieves superior performance against state-of-the-art methods while running in real-time on single CPU.}
}
@article{LIU2021103354,
title = {No-reference stereoscopic image quality evaluator based on human visual characteristics and relative gradient orientation},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103354},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103354},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002327},
author = {Yun Liu and Baoqing Huang and Hongwei Yu and Zhi Zheng},
keywords = {Stereoscopic image quality, Binocularity, Monocular feature, Binocular feature, Features extraction and regression},
abstract = {Stereoscopic image quality assessment (SIQA) is of great significance to the development of modern three-dimensional (3D) display technology. In this work, by further mining the relationship between visual features and stereoscopic image quality perception, we build a new no-reference SIQA model, which combines the monocular and binocular features. Statistical quality-aware structural features from relative gradient orientation (RGO) map and texture features from the histogram of the weighted local binary pattern (LBP) in the texture image (TLBP) are not only extracted from both monocular view, but also extracted from binocular views to predict binocular quality perception. Meanwhile, the color statistical features ignored by most models and the binocularity feature is extracted to complement the monocular features and the above binocular features, respectively. Finally, all the extracted features and subjective scores are used to predict the objective quality score through the support vector regression (SVR) model. Experiments on four popular stereoscopic image databases show that the proposed model achieves high consistency with subjective assessment, and the performance of the model is very competitive with the latest models.}
}
@article{TAN2021103299,
title = {Learning complementary Siamese networks for real-time high-performance visual tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103299},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103299},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100198X},
author = {Ke Tan and Ting-Bing Xu and Zhenzhong Wei},
keywords = {Visual tracking, Matching visualization, Complementary learning, Siamese networks},
abstract = {Recently, Siamese based methods have made a breakthrough in the visual tracking field. However, the existing trackers still cannot take full advantage of the deep features. In this work, we improve the performances of Siamese trackers by complementary learning with different types of matching features. Specifically, a Matching Activation Network (MAN) is firstly designed to highlight the matching regions of the search image given a template. Since only sparse parts of feature maps contribute to the matching result, an important design choice is to emphasize the weak-matching features by erasing the strong-matching ones and learn complementary classifiers from both types of features. Then we propose a novel complementary region proposal network (CoRPN) to take complementary features as inputs and their outputs complement to each other, which are fused to improve the performance. Experiments show that our proposed tracker achieves leading performances on five tracking datasets while retaining real-time speed.}
}
@article{RAVEENDRA2022103401,
title = {Tamper video detection and localization using an adaptive segmentation and deep network technique},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103401},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103401},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002686},
author = {Malle Raveendra and K. Nagireddy},
keywords = {Video forgery, Double compression, Modified Brain storm optimization (MBSO), Adaptive galactic Swarm optimization (AGSO), Deep Neural Network (DNN)},
abstract = {In this work we have explored the hybrid deep learning architecture for recognizing the tampering from the videos. This hybrid architecture explores the features from the authentic videos to categorize the tampered portions from the forged videos. Initially, the process begins by compressing the input video using the Discrete cosine transform (DCT) based double compression approach. Then, the filtering process is carried out to improve the quality of compressed frame using the bilateral filtering. Then, the modified segmentation approach is applied to segment the frames into different regions. The features from these segmented portions are extracted and fed into hybrid DNN-AGSO (deep neural network- Adaptivf RELATED WORKSe Galactic Swarm Optimization) using Gabor wavelet transform (GWT) technique. Three different datasets are used to evaluate the overall performance they are, VTD, MFC-18, and VIRAT by MATLAB platform. The recognition rate achieved by VTD, MFC-18, and VIRAT datasets are 96%, 95.2%, and 93.47% respectively.}
}
@article{MA2021103328,
title = {Multi-focus image fusion based on multi-scale sparse representation},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103328},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103328},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002169},
author = {Xiaole Ma and Zhihai Wang and Shaohai Hu},
keywords = {Multi-scale decomposition, Sparse representation, Adaptive fusion rule, Sum modified Laplacian, Multi-focus image fusion},
abstract = {Although colorful information in natural scenes can be collected, due to the limitation of camera depth of field, it is hard to capture an image with all-in-focus. Sparse representation (SR)-based methods have shown their powerful potentiality and ability in multi-focus image fusion. However, because of sparse coding and information compress, the existing fusion methods based on SR are imperfect to seize the rich details and significant texture information in source images. As a result, a fusion method based on multi-scale sparse representation for registered multi-focus images (MIF-MsSR) is proposed in this paper, where an adaptive fusion rule for sparse coefficients is presented. At first, source images are processed by multi-scale decomposition and sub-images with different scales can be obtained. According to image features with different richness in these sub-images, dictionaries with different sizes and redundancy are thereby trained. By comprehensively considering the relationships of focused areas, out-of-focused areas and boundary areas between the source images, an adaptive fusion rule based on l0−max and Sum Modified Laplacian (SML) is proposed. Finally, a fused image with all-in-focus can be obtained by sparse reconstruction and inverse multi-scale decomposition. Excessive experiments on multi-focus images have demonstrated that the proposed MIF-MsSR not only reserves the integrity of the information in source images, but also has better fusion performance on subjective and objective indicators than other state-of-the-art methods.}
}
@article{GAO2021103366,
title = {A discriminant kernel entropy-based framework for feature representation learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103366},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103366},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100239X},
author = {Lei Gao and Lin Qi and Ling Guan},
keywords = {Kernel entropy component analysis, Discriminant analysis, Feature representation learning, Iris visualization, Face recognition, Emotion recognition, Object recognition},
abstract = {The intelligent multimedia processing community has developed increasing interest in kernel entropy component analysis (KECA) due to its abilities in effective data transformation and dimensionality reduction. However, since only the unsupervised structural information of Renyi entropy from the given data set is utilized, KECA alone is incapable of generating high quality discriminant features. Aiming to develop a new and generic approach for feature representation learning, this paper proposes a discriminant kernel entropy-based framework, which integrates KECA and a complete discriminant strategy (consisting of regular and irregular discriminant information), to explore discriminant feature representations from the given data set. The framework is realized and further optimized to generate a more powerful discriminant descriptor for feature representation learning, leading to improved performance. Since the joint utilization of kernel entropy estimation and the complete discriminant strategy is able to reveal the distribution and semantic information of the given data, the proposed framework opens up a new front for discriminant feature representation learning via information theoretic learning (ITL). To demonstrate the generic nature and effectiveness of the proposed framework, experiments are conducted on two different data sources; the visual data source (e.g., University of California Irvine (UCI) database, Olivetti Research Lab (ORL) database, Caltech 256 database) and the audio data source (Ryerson Multimedia Lab (RML) audio emotion database). The results show this framework yields superior performance over other methods on the data sets evaluated for feature representation learning.}
}
@article{YANG2021103335,
title = {Weighted truncated nuclear norm regularization for low-rank quaternion matrix completion},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103335},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103335},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002200},
author = {Liqiao Yang and Kit Ian Kou and Jifei Miao},
keywords = {Quaternion matrix completion, Low-rank, Quaternion truncated nuclear norm, Weights},
abstract = {In recent years, quaternion matrix completion (QMC) based on low-rank regularization has been gradually used in image processing. Unlike low-rank matrix completion (LRMC) which handles RGB images by recovering each color channel separately, QMC models retain the connection of three channels and process them as a whole. Most of the existing quaternion-based methods formulate low-rank QMC (LRQMC) as a quaternion nuclear norm (a convex relaxation of the rank) minimization problem. The main limitation of these approaches is that they minimize the singular values simultaneously such that cannot approximate low-rank attributes efficiently. To achieve a more accurate low-rank approximation, we introduce a quaternion truncated nuclear norm (QTNN) for LRQMC and utilize the alternating direction method of multipliers (ADMM) to get the optimization in this paper. Further, we propose weights to the residual error quaternion matrix during the update process for accelerating the convergence of the QTNN method with admissible performance. The weighted method utilizes a concise gradient descent strategy which has a theoretical guarantee in optimization. The effectiveness of our method is illustrated by experiments on real visual data sets.}
}
@article{SHAO2021103302,
title = {Internal and external spatial–temporal constraints for person reidentification},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103302},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103302},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001991},
author = {Zhenfeng Shao and Jiaming Wang and Tao Lu and Ruiqian Zhang and Xiao Huang and Xianwei Lv},
keywords = {Person reidentification, Convolution neural network, Attention mechanism, Spatial–temporal constraint},
abstract = {Spatial–temporal information is easy to achieve in a practical surveillance scene, but it is often neglected in most current person reidentification (ReID) methods. Employing spatial–temporal information as a constrain has been verified as beneficial for ReID. However, there is no effective modeling according to the pedestrian movement law. In this paper, we present a ReID framework with internal and external spatial–temporal constraints, termed as IESC-ReID. A novel residual spatial attention module is proposed to build a spatial–temporal constraint and increase the robustness to partial occlusions or camera viewpoint changes. A Laplace-based spatial–temporal constraint is also introduced to eliminate irrelevant gallery images, which are gathered by the internal learning network. IESC-ReID constrains the attention within the functioning range of the channel space, and utilizes additional spatial–temporal constrains to further constrain results. Intensive experiments show that these constraints consistently improve the performance. Extensive experimental results on numerous publicly available datasets show that the proposed method outperforms several state-of-the-art ReID algorithms. Our code is publicly available at https://github.com/jiaming-wang/IESC.}
}
@article{TAO2021103370,
title = {An adaptive two phase blind image deconvolution algorithm for an iterative regularization model},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103370},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103370},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002443},
author = {Shuyin Tao and Wende Dong and Jian Xu and Jianfeng Lu and Guili Xu and Yueting Chen},
keywords = {Blind image deconvolution, -norm gradient regularization, TV regularization},
abstract = {This paper proposes a blind image deconvolution method which consists of two sequential phases, i.e., blur kernel estimation and image restoration. In the first phase, we adopt the L0-norm of image gradients and total variation (TV) to regularize the latent image and blur kernel, respectively. Then we design an alternating optimization algorithm which jointly incorporates the estimation of intermediately restored image, blur kernel and regularization parameters into account. In the second phase, we propose to take the mixture of L0-norm of image gradients and TV to regularize the latent image, and design an efficient non-blind deconvolution algorithm to achieve the restored image. Experimental results on both a benchmark image dataset and real-world blurred images show that the proposed method can effectively restore image details while suppress noise and ringing artifacts, the result is of high quality which is competitive with some state of the art methods.}
}
@article{LIU2021103310,
title = {Unsupervised person re-identification by Intra–Inter Camera Affinity Domain Adaptation},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103310},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103310},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002042},
author = {Guiqing Liu and Jinzhao Wu},
keywords = {Intra–Inter Camera Affinity Domain Adaptation, Person re-identification, Generative adversarial learning, Affinity transfer learning},
abstract = {Person re-identification (re-ID) based on unsupervised domain adaptation intends to distill knowledge from annotated source dataset to identify target persons in another dataset. Although the advanced UDA re-ID models are dominated by pseudo-label methods, they almost transform images from various camera views into the same feature space, without considering the camera distribution gaps, which may lead to generate considerably noisy pseudo-labels. In this study, we develop an Intra–Inter Camera Affinity Domain Adaptation (I2CADA) to tackle these problems for UDA person re-ID. Precisely, I2CADA framework is composed of two modules. The first one is generative adversarial learning module, aiming to train a feature extractor that can map target data to source feature space by supervised learning and adversarial learning, which can relieve the distribution gap between different datasets (domains). The second one is affinity transfer learning module, which simultaneously considers intra-camera clustering and inter-camera separation among persons with similar appearances in the target domain, thus mitigating the distribution inconsistency among person images collected from multiple target camera views. Besides, comprehensive experiments exhibit that I2CADA outperforms the existing UDA person re-identification approaches.}
}
@article{YANG2021103345,
title = {Shape transformer nets: Generating viewpoint-invariant 3D shapes from a single image},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103345},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103345},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002285},
author = {Jinglun Yang and Youhua Li and Lu Yang},
keywords = {3D shape generation, Invariant viewpoint, Disentanglement, B-spline surfaces},
abstract = {Single-view 3D shapes generation has achieved great success in recent years. However, current methods always blind the learning of shapes and viewpoints. The generated shape only fit the observed viewpoints and would not be optimal from unknown viewpoints. In this paper, we propose a novel encoder–decoder based network which contains a disentangled transformer to generate the viewpoint-invariant 3D shapes. The differentiable and parametric Non-uniform B-spline (NURBS) surface generation and 3D-to-3D viewpoint transformation are incorporated to learn the viewpoint-invariant shape and the camera viewpoint, respectively. Our new framework allows us to learn the latent geometric parameters of shapes and viewpoints without knowing the ground truth viewpoint. That can simultaneously generate camera-viewpoint and viewpoint-invariant 3D shapes of the object. We analyze the effects of disentanglement and show both quantitative and qualitative results of shapes generated at various unknown viewpoints.}
}
@article{ZHAO2022103408,
title = {A contrast improved OR and XOR based (k,n) visual cryptography scheme without pixel expansion},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103408},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103408},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002716},
author = {Yongkang Zhao and Fang-Wei Fu},
keywords = {Visual cryptography, Multiple decryptions, Contrast, Perfect recovery, Threshold},
abstract = {Visual cryptography scheme (VCS) is a branch of secret sharing, in which a secret image is encrypted into n noise-like shares. In the recovering phase, the secret image can be reconstructed by stacking sufficient shares. It’s obvious that VCS with multiple decryptions can further widen the range of application. However, the investigations on the existing scheme with multiple decryptions are not sufficient. In this paper, we develop a novel contrast improved OR and XOR based (k,n)-VCS without pixel expansion. Significantly, we give a general simplified calculation formula to compute the theoretical contrast of the proposed scheme. In addition, if there are no computing devices, then we can reconstruct the secret image by stacking the shares directly. Meanwhile, we recover the secret image perfectly by performing XOR operation when computing devices are available. Since the proposed scheme is based on the parity basis matrices, our scheme has no pixel expansion. Finally, Theoretical analysis and experimental results are conducted to evaluate the efficiency and security of the proposed scheme.}
}
@article{SHEN2022103386,
title = {Human skeleton representation for 3D action recognition based on complex network coding and LSTM},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103386},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103386},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002558},
author = {Xiangpei Shen and Yanrui Ding},
keywords = {Skeleton-based action recognition, Complex network coding, LSTM, Feature extraction},
abstract = {3D skeleton sequences contain more effective and discriminative information than RGB video and are more suitable for human action recognition. Accurate extraction of human skeleton information is the key to the high accuracy of action recognition. Considering the correlation between joint points, in this work, we first propose a skeleton feature extraction method based on complex network. The relationship between human skeleton points in each frame is coded as a network. The changes of action over time are described by a time series network composed of skeleton points. Network topology attributes are used as feature vectors, complex network coding and LSTM are combined to recognize human actions. The method was verified on the NTU RGB + D60, MSR Action3D and UTKinect-Action3D dataset, and have achieved good performance, respectively. It shows that the method of extracting skeleton features based on complex network can properlyidentify different actions. This method that considers the temporal information and the relationship between skeletons at the same time plays an important role in the accurate recognition of human actions.}
}
@article{BAFFOUR2021103368,
title = {Spatial self-attention network with self-attention distillation for fine-grained image recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103368},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103368},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100242X},
author = {Adu Asare Baffour and Zhen Qin and Yong Wang and Zhiguang Qin and Kim-Kwang Raymond Choo},
keywords = {Fine-grained recognition, Spatial self-attention, Knowledge distillation, Convolutional neural network},
abstract = {The underlining task for fine-grained image recognition captures both the inter-class and intra-class discriminate features. Existing methods generally use auxiliary data to guide the network or a complex network comprising multiple sub-networks. They have two significant drawbacks: (1) Using auxiliary data like bounding boxes requires expert knowledge and expensive data annotation. (2) Using multiple sub-networks make network architecture complex and requires complicated training or multiple training steps. We propose an end-to-end Spatial Self-Attention Network (SSANet) comprising a spatial self-attention module (SSA) and a self-attention distillation (Self-AD) technique. The SSA encodes contextual information into local features, improving intra-class representation. Then, the Self-AD distills knowledge from the SSA to a primary feature map, obtaining inter-class representation. By accumulating classification losses from these two modules enables the network to learn both inter-class and intra-class features in one training step. The experiment findings demonstrate that SSANet is effective and achieves competitive performance.}
}
@article{LIU2021103267,
title = {Multispectral background subtraction with deep learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103267},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103267},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001747},
author = {Rongrong Liu and Yassine Ruichek and Mohammed {El Bagdouri}},
keywords = {Background subtraction, Multispectral images, Deep learning, Convolutional neural networks},
abstract = {In this paper, we follow the trend of deep learning and make an attempt to investigate the potential benefit of using multispectral images via convolutional neural networks for background subtraction task. The major contributions of this work lie in two aspects, based on the impressive algorithm FgSegNet_v2. Firstly, we extract three channels out of the seven of the FluxData FD-1665 multispectral dataset to match the number of input channels of the VGG16 deep model. Some combinations of three-channel based multispectral images perform better than RGB images. Secondly, a new convolutional encoder is designed to use all the multispectral channels available to further explore the information of multispectral images. The results outperform the RGB images and also other approaches using the same multispectral dataset.}
}
@article{WANG2022103421,
title = {Reversal of pixel rotation: A reversible data hiding system towards cybersecurity in encrypted images},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103421},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103421},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002832},
author = {Xu Wang and Ching-Chun Chang and Chia-Chen Lin and Chin-Chen Chang},
keywords = {Reversible data hiding, Encrypted images, Pixel rotation, Lossless scheme},
abstract = {Due to privacy and security concerns, the researches of reversible data hiding in encrypted images (RDHEI) have become increasingly important. Conventional schemes vacate the spare room after image encryption (VRAE) suffer from the low embedding rate, high error rate of data extraction, and imperfect image recovery. To address these issues, we propose a separable reversible data hiding scheme for encrypted images that utilizes a novel pixel rotation technique to embed data into fully encrypted images. The block complexities of four decrypted rotation states are considered when recovering image. To realize perfect image recovery, we further devise a lossless version (LPR-RDHEI). Experimental results demonstrate that the proposed PR-RDHEI scheme achieves an embedding rate of 0.4994 bpp on average and ensures lossless data extraction. Meanwhile, the proposed LPR-RDHEI scheme still has a 0.4494 bpp embedding rate on average. The embedding rates of our two schemes are significantly improved compared with state-of-the-arts.}
}
@article{FAN2021103349,
title = {A comparative study between PVO-based framework and multi-predictor mechanism in reversible data hiding},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103349},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103349},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002315},
author = {Guojun Fan and Zhibin Pan and Quan Zhou and Xinyi Gao and Xiaoran Zhang},
keywords = {Reversible data hiding, General multi-predictor framework, Pixel-value-ordering (PVO), Additional predictors, Adaptive complexity},
abstract = {Sorting-based reversible data hiding (RDH) methods like pixel-value-ordering (PVO) can predict pixel values accurately and achieve an extremely low distortion on the embedded image. However, the excellent performance of these methods was not well explained in previous works, and there are unexploited common points among them. In this paper, we propose a general multi-predictor (GMP) framework to summarize PVO-based RDH methods and explain their high prediction accuracy. Moreover, by utilizing the proposed GMP framework, a more efficient sorting-based RDH method is given as an example to show the generality and applicability of our framework. Comparing with other PVO-based methods, the proposed example method can achieve significant improvement in embedding performance. It is hopeful that more efficient sorting-based RDH algorithms can be designed according to our proposed framework by designing better predictors and their combination methods.}
}
@article{REN2021103369,
title = {Gaze estimation via bilinear pooling-based attention networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103369},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103369},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002431},
author = {Dakai Ren and Jiazhong Chen and Jian Zhong and Zhaoming Lu and Tao Jia and Zongyi Li},
keywords = {Gaze tracking, Deep learning, Bilinear pooling, Attention},
abstract = {Attention mechanism has been found effective for human gaze estimation, and the attention and diversity of learned features are two important aspects of attention mechanism. However, the traditional attention mechanism used in existing gaze model is more prone to utilize first-order information that is attentive but not diverse. Though the existing bilinear pooling-based attention could overcome the shortcoming of traditional attention, it is limited to extract high-order contextual information. Thus we introduce a novel bilinear pooling-based attention mechanism, which could extract the second-order contextual information by the interaction between local deep learned features. To make the gaze-related features robust for spatial misalignment, we further propose an attention-in-attention method, which consists of a global average pooling and an inner attention on the second-order features. For the purpose of gaze estimation, a new bilinear pooling-based attention networks with attention-in-attention is further proposed. Extensive evaluation shows that our method surpasses the state-of-the-art by a big margin.}
}
@article{YOU2022103399,
title = {Attention integrated hierarchical networks for no-reference image quality assessment},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103399},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103399},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002674},
author = {Junyong You and Jari Korhonen},
keywords = {Attention, Hierarchical networks, Image quality assessment (IQA), Perceptual mechanisms, Quality perception},
abstract = {Quality assessment of natural images is influenced by perceptual mechanisms, e.g., attention and contrast sensitivity, and quality perception can be generated in a hierarchical process. This paper proposes an architecture of Attention Integrated Hierarchical Image Quality networks (AIHIQnet) for no-reference quality assessment. AIHIQnet consists of three components: general backbone network, perceptually guided neck network, and head network. Multi-scale features extracted from the backbone network are fused to simulate image quality perception in a hierarchical manner. The attention and contrast sensitivity mechanisms modelled by an attention module capture essential information for quality perception. Considering that image rescaling potentially affects perceived quality, appropriate pooling methods in the non-convolution layers in AIHIQnet are employed to accept images with arbitrary resolutions. Comprehensive experiments on publicly available databases demonstrate outstanding performance of AIHIQnet compared to state-of-the-art models. Ablation experiments were performed to investigate the variants of the proposed architecture and reveal importance of individual components.}
}
@article{TIAN2021103324,
title = {Stitched image quality assessment based on local measurement errors and global statistical properties},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103324},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103324},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002157},
author = {Chongzhen Tian and Xiongli Chai and Feng Shao},
keywords = {Image stitching, Stitched image quality assessment, Structural distortion, Geometric error, Quality aggregation},
abstract = {Image stitching is developed to generate wide-field images or panoramic images for virtual reality applications. However, the quality assessment of stitched images with respect to various stitching algorithms has been less studied. Effective stitched image quality assessment (SIQA) is advantageous to evaluate the performance of various stitching methods and optimize the design of stitching methods. In this paper, we propose a novel SIQA method by exploiting local measurement errors and global statistical properties for feature extraction. Comprehensive image attributes including ghosting, misalignment, structural distortion, geometric error, chromatic aberrations and blur are considered either locally or globally. The extracted local and global features are aggregated into an overall quality via regression. Experimental results on two benchmark databases demonstrate the superiority of the proposed metric over both the state-of-the-art quality models designed for natural images and stitched images.}
}
@article{BAO2021103306,
title = {Visible and thermal images fusion architecture for few-shot semantic segmentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103306},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103306},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002030},
author = {Yanqi Bao and Kechen Song and Jie Wang and Liming Huang and Hongwen Dong and Yunhui Yan},
keywords = {V-T semantic segmentation, Thermal images, Few-shot semantic segmentation},
abstract = {Few-shot semantic segmentation (FSS) has drawn great attention in the community of computer vision, due to its remarkable potential for segmenting novel objects with few pixel-annotated samples. However, some interference factors, such as insufficient illumination and complex background, can impose more challenge to the segmentation performance than fully-supervised when the number of samples is insufficient. Therefore, we propose the visible and thermal (V-T) few-shot semantic segmentation task, which utilize the complementary and similar information of visible and thermal images to boost few-shot segmentation performance. As the first step, we build a novel outdoor city dataset Tokyo Multi-Spectral-4i for the V-T few-shot semantic segmentation task. In addition, a fusion architecture is proposed, which consists of an Edge Similarity fusion module (ES) and a Texture Edge Prototype module (TEP). The ES module fuses the bi-modal information by exploiting the edge similarity in the visible and thermal images. The TEP module extracts the prototype from two models by collaborating the representativeness and complementarity of the visible and thermal feature. Finally, extensive experiments conducted on the proposed datasets demonstrate that our architecture can achieve state-of-the-arts results.}
}
@article{URRAHMAN2022103415,
title = {QoE optimization for HTTP adaptive streaming: Performance evaluation of MEC-assisted and client-based methods},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103415},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103415},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002790},
author = {Waqas {ur Rahman} and Muhammad Bilal Amin and Md Delowar Hossain and Choong {Seon Hong} and Eui-Nam Huh},
keywords = {Quality of experience, DASH, Fairness, HTTP adaptive streaming, Video},
abstract = {Seamless streaming of high quality video under unstable network condition is a big challenge. HTTP adaptive streaming (HAS) provides a solution that adapts the video quality according to the network conditions. Traditionally, HAS algorithm runs at the client side while the clients are unaware of bottlenecks in the radio channel and competing clients. The traditional adaptation strategies do not explicitly coordinate between the clients, servers, and cellular networks. The lack of coordination has been shown to lead to suboptimal user experience. As a response, multi-access edge computing (MEC)-assisted adaptation techniques emerged to take advantage of computing and content storage capabilities in mobile networks. In this study, we investigate the performance of both MEC-assisted and client-side adaptation methods in a multi-client cellular environment. Evaluation and comparison are performed in terms of not only the video rate and dynamics of the playback buffer but also the fairness and bandwidth utilization. We conduct extensive experiments to evaluate the algorithms under varying client, server, dataset, and network settings. Results demonstrate that the MEC-assisted algorithms improve fairness and bandwidth utilization compared to the client-based algorithms for most settings. They also reveal that the buffer-based algorithms achieve significant quality of experience; however, these algorithms perform poorly compared with throughput-based algorithms in protecting the playback buffer under rapidly varying bandwidth fluctuations. In addition, we observe that the preparation of the representation sets affects the performance of the algorithms, as does the playback buffer size and segment duration. Finally, we provide suggestions based on the behaviors of the algorithms in a multi-client environment.}
}
@article{LENG2021103344,
title = {Augmented two stream network for robust action recognition adaptive to various action videos},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103344},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103344},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002273},
author = {Chuanjiang Leng and Qichuan Ding and Chengdong Wu and Ange Chen},
keywords = {Two-stream network, Action recognition, Data skew},
abstract = {In video-based action recognition, using videos with different frame numbers to train a two-stream network can result in data skew problems. Moreover, extracting the key frames from a video is crucial for improving the training and recognition efficiency of action recognition systems. However, previous works suffer from problems of information loss and optical-flow interference when handling videos with different frame numbers. In this paper, an augmented two-stream network (ATSNet) is proposed to achieve robust action recognition. A frame-number-unified strategy is first incorporated into the temporal stream network to unify the frame numbers of videos. Subsequently, the grayscale statistics of the optical-flow images are extracted to filter out any invalid optical-flow images and produce the dynamic fusion weights for the two branch networks to adapt to different action videos. Experiments conducted on the UCF101 dataset demonstrate that ATSNet outperforms previously defined methods, improving the recognition accuracy by 1.13%.}
}
@article{LAO2021103282,
title = {Three Degree Binary Graph and Shortest Edge Clustering for re-ranking in multi-feature image retrieval},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103282},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103282},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100184X},
author = {Guihong Lao and Shenglan Liu and Chenwei Tan and Yang Wang and Guangzhe Li and Li Xu and Lin Feng and Feilong Wang},
keywords = {Unsupervised re-ranking, Image retrieval, Multi-feature fusion, Graph learning},
abstract = {Graph methods have been widely employed in re-ranking for image retrieval. Although we can effectively find visually similar images through these methods, the ranking lists given by those approaches may contain some candidates which appear to be irrelevant to a query. Most of these candidates fall into two categories: (1) the irrelevant outliers located near to the query images in a graph; and (2) the images from another cluster which close to the query. Therefore, eliminating these two types of images from the ordered retrieval sets is expected to further boost the retrieval precision. In this paper, we build a Three Degree Binary Graph (TDBG) to eliminate the outliers and utilize a set-based greedy algorithm to reduce the influence of adjacent manifolds. Moreover, a multi-feature fusion method is proposed to enhance the retrieval performance further. Experimental results obtained on three public datasets demonstrate the superiority of the proposed approach.}
}
@article{WANG2022103414,
title = {Cross-layer progressive attention bilinear fusion method for fine-grained visual classification},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103414},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103414},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002789},
author = {Chaoqing Wang and Yurong Qian and Weijun Gong and Junjong Cheng and Yongqiang Wang and Yuefei Wang},
keywords = {Fine-grained visual classification, Feature fusion, Attention, Progressive},
abstract = {Fine-grained visual classification (FGVC) is a critical task in the field of computer vision. However, FGVC is full of challenges due to the large intra-class variation and small inter-class variation of the classes to be classified on an image. The key in dealing with the problem is to capture subtle visual differences from the image and effectively represent the discriminative features. Existing methods are often limited by insufficient localization accuracy and insufficient feature representation capabilities. In this paper, we propose a cross-layer progressive attention bilinear fusion (CPABF in short) method, which can efficiently express the characteristics of discriminative regions. The CPABF method involves three components: 1) Cross-Layer Attention (CLA) locates and reinforces the discriminative region with low computational costs; 2) The Cross-Layer Bilinear Fusion Module (CBFM) effectively integrates the semantic information from the low-level to the high-level 3) Progressive Training optimizes the parameters in the network to the best state in a delicate way. The CPABF shows excellent performance on the four FGVC datasets and outperforms some state-of-the-art methods.}
}
@article{PAN2022103405,
title = {Visual cryptography scheme for secret color images with color QR codes},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103405},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103405},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002704},
author = {Jeng-Shyang Pan and Tao Liu and Hong-Mei Yang and Bin Yan and Shu-Chuan Chu and Tongtong Zhu},
keywords = {Visual cryptography scheme, Color QR code, Color XOR},
abstract = {Visual cryptography scheme can divide the secret image into several shares. It can be used to enhance the secure transmission of the secret image on the Internet. Most schemes cannot fully restore the secret color image and generate meaningful shares. This paper proposes two new schemes by using color XOR to solve these problems. The first proposed scheme generates meaningless shares. The second proposed scheme can generate n−1 meaningful shares and a meaningless share. Based on the first proposed scheme, n−1 shares are modified to color QR codes in the second proposed scheme. These color QR codes can be decoded by the general decoder instead of the standard decoder. All shares are performed the color XOR to restore the secret color image completely. This paper uses some experiments to test two proposed schemes. Experimental results show that the two proposed schemes are feasible.}
}
@article{WANG2021103300,
title = {Multi-scale attention network for image super-resolution},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103300},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103300},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001978},
author = {Li Wang and Jie Shen and E. Tang and Shengnan Zheng and Lizhong Xu},
keywords = {Super-resolution, Multi-scale, Attention mechanism, Lightweight},
abstract = {The power of convolutional neural networks (CNN) has demonstrated irreplaceable advantages in super-resolution. However, many CNN-based methods need large model sizes to achieve superior performance, making them difficult to apply in the practical world with limited memory footprints. To efficiently balance model complexity and performance, we propose a multi-scale attention network (MSAN) by cascading multiple multi-scale attention blocks (MSAB), each of which integrates a multi-scale cross block (MSCB) and a multi-path wide-activated attention block (MWAB). Specifically, MSCB initially connects three parallel convolutions with different dilation rates hierarchically to aggregate the knowledge of features at different levels and scales. Then, MWAB split the channel features from MSCB into three portions to further improve performance. Rather than being treated equally and independently, each portion is responsible for a specific function, enabling internal communication among channels. Experimental results show that our MSAN outperforms most state-of-the-art methods with relatively few parameters and Mult-Adds.}
}
@article{HU2022103420,
title = {Deep network based stereoscopic image quality assessment via binocular summing and differencing},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103420},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103420},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002820},
author = {Jinbin Hu and Xuejin Wang and Xiongli Chai and Feng Shao and Qiuping Jiang},
keywords = {Stereoscopic image quality assessment, Deep regression network, Binocular summing, Binocular differencing},
abstract = {With the development of deep networks in dealing with various visual tasks, the deep network based on binocular vision is expected to tackle the issue of stereoscopic image quality assessment. Here, we present a stereoscopic image quality assessment method using the deep network with four channels together, which takes the left view, right view, binocular summing view, and binocular differencing view as the inputs of the network. The visual features are enhanced through the concatenation in a weighted way, so that the binocular vision can be adequately included in the binocular addition and subtraction information. Compared with the state-of-the-art metrics, the proposed method exhibits relatively high performances on four benchmark databases.}
}
@article{ZHANG2021103350,
title = {Global and local information aggregation network for edge-aware salient object detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103350},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103350},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002303},
author = {Qing Zhang and Liqian Zhang and Dong Wang and Yanjiao Shi and Jiajun Lin},
keywords = {Salient object detection, Saliency, Multi-level feature aggregation, Attention, Salient edge},
abstract = {Aggregation of local and global contextual information by exploiting multi-level features in a fully convolutional network is a challenge for the pixel-wise salient object detection task. Most existing methods still suffer from inaccurate salient regions and blurry boundaries. In this paper, we propose a novel edge-aware global and local information aggregation network (GLNet) to fully exploit the integration of side-output local features and global contextual information and utilization of contour information of salient objects. The global guidance module (GGM) is proposed to learn discriminative multi-level information with the direct guidance of global semantic knowledge for more accurate saliency prediction. Specifically, the GGM consists of two key components, where the global feature discrimination module exploits the inter-channel relationship of global semantic features to boost representation power, and the local feature discrimination module enables different side-output local features to selectively learn informative locations by fusing with global attentive features. Besides, we propose an edge-aware aggregation module (EAM) to employ the correlation between salient edge information and salient object information for generating estimated saliency maps with explicit boundaries. We evaluate our proposed GLNet on six widely-used saliency detection benchmark datasets by comparing with 17 state-of-the-art methods. Experimental results show the effectiveness and superiority of our proposed method on all the six benchmark datasets.}
}
@article{KARACA2021103385,
title = {MultiTempGAN: Multitemporal multispectral image compression framework using generative adversarial networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103385},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103385},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002546},
author = {Ali Can Karaca and Ozan Kara and Mehmet Kemal Güllü},
keywords = {Multispectral image compression, Generative adversarial networks, Big data, Remote sensing, Multitemporal images},
abstract = {Multispectral satellites that measure the reflected energy from the different regions on the Earth generate the multispectral (MS) images continuously. The following MS image for the same region can be acquired with respect to the satellite revisit period. The images captured at different times over the same region are called multitemporal images. Traditional compression methods generally benefit from spectral and spatial correlation within the MS image. However, there is also a temporal correlation between multitemporal images. To this end, we propose a novel generative adversarial network (GAN) based prediction method called MultiTempGAN for compression of multitemporal MS images. The proposed method defines a lightweight GAN-based model that learns to transform the reference image to the target image. Here, the generator parameters of MultiTempGAN are saved for the reconstruction purpose in the receiver system. Due to MultiTempGAN has a low number of parameters, it provides efficiency in multitemporal MS image compression. Experiments were carried out on three Sentinel-2 MS image pairs belonging to different geographical regions. We compared the proposed method with JPEG2000-based conventional compression methods and three deep learning methods in terms of signal-to-noise ratio, mean spectral angle, mean spectral correlation, and laplacian mean square error metrics. Additionally, we have also evaluated the change detection performances and visual maps of the methods. Experimental results demonstrate that MultiTempGAN not only achieves the best metric values among the other methods at high compression ratios but also presents convincing performances in change detection applications.}
}
@article{WANG2021103307,
title = {Indoor scene understanding based on manhattan and non-manhattan projection of spatial right-angles},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103307},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103307},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002029},
author = {Luping Wang and Hui Wei},
keywords = {Non-manhattan, Ceiling, Indoor scene, Spatial right-angle, Monocular vision},
abstract = {Understanding of indoor scenes has considerable value in computer vision. Most previous methods infer indoor scenes via manhattan assumption. However, attic ceilings do not satisfy manhattan assumption and understanding them remains a big challenge. Non-manhattan ceilings can be seen as compositions of spatial right-angles projections. In this paper, we presented a method to understand indoor scenes including both manhattan structures and non-manhattan attic ceilings from a single image. First, angle projections are detected and assigned to different clusters. Then vanishing points of attic ceilings can be estimated. Third, it is possible to determine the attic ceilings of non-manhattan surfaces. The proposed approach requires no prior training. We compared the estimated attic layout against the ground truth and measured the percentage of pixels that were incorrectly classified. Experimental results showed that the method can understand indoor scenes including both manhattan and non-manhattan attic ceilings, meeting the requirements of robot navigation.}
}
@article{MORADI2021103259,
title = {A salient object detection framework using linear quadratic regulator controller},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103259},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103259},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001693},
author = {Morteza Moradi and Farhad Bayat and Mostafa Charmi},
keywords = {Salient object detection, Control system, Linear quadratic regulator, Optimal control, Angular embedding},
abstract = {In this paper, a novel salient object detection framework based on Linear Quadratic Regulator (LQR) controller is proposed. The major goal of this research is to take advantage of optimal control theory for improving the performance of detecting salient objects in images. In this regard, for the sake of detection of salient and non-salient regions, two LQR-based control systems are employed. In the proposed framework, for the initialization of the control systems, background and foreground estimations have been done with two different strategies. Doing so, we would ultimately have more effective distinction between those regions. After the initialization step, the control systems refine both estimations in parallel until reaching a steady state for each of them. Within the mentioned process, by using optimal control concept, specifically LQR controller (for the first time in the field), control signals which are in charge of determining saliency values, would be constantly optimized. At the end, the raw saliency map will be generated by combination of background and foreground optimized initial maps. Finally, the integrated saliency map will be refined by using angular embedding method. The experimental evaluations on three benchmark datasets shows that the proposed framework performs well and introduces comparable results with some deep learning based methods.}
}
@article{B2022103400,
title = {SpyGAN sketch: Heterogeneous Face Matching in video for crime investigation},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103400},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103400},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002662},
author = {Yogameena B. and Geeta Jakkamsetti and Aishwarya S.},
keywords = {SpyGAN, Heterogeneous face matching, Generated sketch, Illumination, OWN short face-video linked dataset},
abstract = {Automatic retrieval of faces from videos based on query images effectively helps during the investigation. When the suspect’s image is unavailable, a face sketch, drawn based on eyewitness’s memory recollection, is used to search against photos. Present research works primarily focus on Heterogeneous Face Matching (HFM) sketches to mugshot images in databases. This paper proposes a sketch-face matching in a video that includes profile faces, different illumination, and poses, using a new Generative Adversarial Network called SpyGAN. Faces in the video detected using YOLOv3 are converted into realistic sketches by the proposed SpyGAN focusing on key facial regions. The generated sketches are represented using PCA-SIFT descriptors and are matched based on the cosine distance metric. Experimental results show that the proposed methodology has achieved an accuracy of 88.9% on the Chokepoint dataset and 78% on the OWN Short face-video linked dataset and has demonstrated effectiveness over the state-of-the-art methods.}
}
@article{YANG2021103329,
title = {Multi-frame co-saliency spatio-temporal regularization correlation filters for object tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103329},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103329},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002145},
author = {Xi Yang and Shaoyi Li and Jun Ma and Hao Liu and Jie Yan},
keywords = {Object tracking, Correlation filter, Co-saliency, Spatio-temporal regularization, Heterogeneous fusion},
abstract = {The spatial regularization weight of the correlation filter is not related to the object content and the model degradation in the tracking process. To solve this problem, a new multi-frame co-saliency spatio-temporal regularization correlation filters (MCSRCF) is proposed for visual object tracking. To the best our knowledge, this is the first application of co-saliency regularization to CF-based tracking. In MCSRCF, grayscale features, directional gradient histogram (HOG) features and CNN features are extracted to improve the tracking precision of the tracker. Secondly, the three-dimensional spatial saliency and semantic saliency are introduced to obtain the initial weight of the spatial regularization with object content information. Then, the heterogeneous saliency fusion method is exploited to add a co-saliency spatial regularization term to the objective function to make the spatial penalty weight learn the change of the object region. In additional, the temporal saliency regularization is introduced to learn the information between adjacent frames, which reduces the overfitting effect caused by inaccurate samples. A variety of evaluations are conducted on public benchmarks, and the experimental results show that the proposed tracker achieves good robustness against many state-of-the-art trackers in various complex scenarios.}
}
@article{LI2021103360,
title = {Learning to capture dependencies between global features of different convolution layers},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103360},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103360},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002388},
author = {Zhangwei Li and Anshun Hu and Xiaofei Wang and Jun Hu and Guijun Zhang},
keywords = {Deep learning, Object detection, Non-local neural network, Global features dependencies},
abstract = {NLNet has been considered as one milestone in the study of capturing long-range dependencies. Many recent studies modify the internal structure of NLNet directly and apply them to video object detection and semantic segmentation tasks. The dependencies between local and global features have been well developed, but the dependencies between global features of different convolution layers are rarely considered. Convolution is a local operation, so the global features of different convolution layers cannot be directly related, resulting in the loss of dependencies between global features. Given the vulnerability, this study designs a network that can efficiently capture the dependencies between the global features of different convolution layers, potentially further improving the accuracy. Furthermore, for the calculation of the dependency matrix, based on the Dot-product used in NLNet, we propose RELU-Dot-product, which can achieve higher accuracy. We evaluatethe proposed method on image classification and object detection tasks. The data sets involved are CIFAR10, CIFAR100, Tiny-imagenet, VOC2007, VOC2012 and MS COCO. Experiments show that our method can significantly improve network performance by introducing a few parameters.}
}
@article{CHAI2022103419,
title = {M2OVQA: Multi-space signal characterization and multi-channel information aggregation for quality assessment of compressed omnidirectional videos},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103419},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103419},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002819},
author = {Xiongli Chai and Feng Shao},
keywords = {Omnidirectional video quality assessment, Image quality assessment, Compression distortion, Signal characterization, Information aggregation},
abstract = {Considering the high requirements for omnidirectional video compression, we propose an objective quality evaluation method to assess quality loss in encoding omnidirectional videos. According to characteristics of 360° videos, we consider multi-space signal characterization (MSSC) to fully characterize the distortions of video signals from spatial/image domains to frequency domains and from image content to motion information, and further consider multi-channel information aggregation (MCIA) to fuse scores from multiple projection planes and temporal divided groups. The main innovation of our method is to establish a universal framework in bridging the connection between typical quality assessment and 360° quality assessment to measure 360° video quality effectively and efficiently. Experimental results show that our method outperforms state-of-the-art 2D quality metrics and quality metrics for omnidirectional images.}
}
@article{YANG2021103263,
title = {Learning discriminative motion feature for enhancing multi-modal action recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103263},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103263},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001723},
author = {Jianyu Yang and Yao Huang and Zhanpeng Shao and Chunping Liu},
keywords = {Motion feature, Bag of features, Dynamic image, Action recognition},
abstract = {Video action recognition is an important topic in computer vision tasks. Most of the existing methods use CNN-based models, and multiple modalities of image features are captured from the videos, such as static frames, dynamic images, and optical flow features. However, these mainstream features contain much static information including object and background information, where the motion information of the action itself is not distinguished and strengthened. In this work, a new kind of motion feature is proposed without static information for video action recognition. We propose a quantization of motion network based on the bag-of-feature method to learn significant and discriminative motion features. In the learned feature map, the object and background information is filtered out, even if the background is moving in the video. Therefore, the motion feature is complementary to the static image feature and the static information in the dynamic image and optical flow. A multi-stream classifier is built with the proposed motion feature and other features, and the performance of action recognition is enhanced comparing to other state-of-the-art methods.}
}
@article{HAN2021103296,
title = {Blind image quality assessment with channel attention based deep residual network and extended LargeVis dimensionality reduction},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103296},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103296},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001966},
author = {Han Han and Li Zhuo and Jiafeng Li and Jing Zhang and Meng Wang},
keywords = {Blind image quality assessment, ResNet-50, Channel attention mechanism, LargeVis dimensionality reduction},
abstract = {Image Quality Assessment (IQA) is one of the fundamental problems in the fields of image processing, image/video coding and transmission, and so on. In this paper, a Blind Image Quality Assessment (BIQA) approach with channel attention based deep Residual Network (ResNet)and extended LargeVis dimensionality reduction is proposed. Firstly, ResNet50 with channel attention mechanism is used as the backbone network to extract the deep features from the image. In order to reduce the dimensionality of the deep features, LargeVis, which is originally designed for the visualization of large scale high-dimensional data, is extended by using Support Vector Regression (SVR) to perform on a single feature vector data. The extended LargeVis can remove the redundant information of the deep features so as to obtain a low-dimensional and discriminative feature representation. Finally, the quality prediction model is established by using SVR as the fitting method. The low-dimensional feature representation and quality score of the image form the pair-wise data samples to train the fitting model. Experimental results on authentic distortions datasets and synthetic distortions datasets show that our proposed method can achieve superior performance compared with the state-of-the-art methods.}
}
@article{LI2021103367,
title = {Gaze prediction for first-person videos based on inverse non-negative sparse coding with determinant sparse measure},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103367},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103367},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002418},
author = {Yujie Li and Benying Tan and Shotaro Akaho and Hideki Asoh and Shuxue Ding},
keywords = {Gaze prediction, First-person video, Sparse coding, Determinant measure, Inverse},
abstract = {Gaze prediction is a significant approach for processing a large amount of incoming visual information of videos. Recent gaze prediction algorithms often employ sparse models with the assumption that every superpixel in the video frames can be represented as linear combinations of a few salient superpixels. However, they are not actuated enough because of the insufficient knowledge that video signals contain a non-negative request. Hence, we develop a novel gaze prediction based on an inverse sparse coding framework with a determinant sparse measure. By introducing this sparse measure, the solutions are non-negative and sparser than conventional sparse constraints. However, the proposed optimization problem becomes nonconvex, which is difficult to solve. To efficiently address the corresponding nonconvex optimization problem, we propose a novel algorithm based on the difference in convex function programming, which can yield the global solutions. Experimental results indicate the improved accuracy of the proposed approach compared with state-of-the-art algorithms.}
}
@article{CHEN2022103384,
title = {Optimization and regularization of complex task decomposition for blind removal of multi-factor degradation},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103384},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103384},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002534},
author = {Gongping Chen and Zhisheng Gao and Bin Zhou and Chenglin Zuo},
keywords = {Complex task, Decomposition regularization, Convolutional neural network, Atmospheric turbulence, Blind restoration},
abstract = {Most existing image restoration methods based on deep neural networks are developed for images which only degraded by a single degradation mode and imaging under an ideal condition. They cannot be directly used to restore the images degraded by multi-factor coupling. A complex task decomposition regularization optimization strategy (TDROS) is proposed to solve the problem. The restoration of images degraded by multi-factor coupling is a complex task that can be solved by separating these multiple factors, that is, breaking the complex task into numbers of simpler tasks to make the entire complex problem be overcome more easily. Motivated by this idea, the TDROS decomposes the complex task of image restoration into two sub-task: the potential task constrained by regularization and the main task for reconstructing high-definition images. In TDROS, the front of the neural network is focused on the restoration of images degraded by additive noise, while the other part of the network is focused mainly on the restoration of images degraded by blur. We applied the TDROS to an 11-layer convolutional neural network (CNN) and compared it with initial CNNs from the aspects of restoration accuracy and generalization ability. Based on these results, we used TDROS to design a novel network model for the restoration of atmospheric turbulence-degraded images. The experimental results demonstrate that the proposed TDROS can improve the generalization ability of the existing network more effectively than current popular methods, offering a better solution for the problem of severely degraded image restoration. Moreover, the TDROS concept provides a flexible framework for low-level visual complex tasks and can be easily incorporated into existing CNNs.}
}
@article{LI2021103304,
title = {What and how well you exercised? An efficient analysis framework for fitness actions},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103304},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103304},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002017},
author = {Jianwei Li and Qingrui Hu and Tianxiao Guo and Siqi Wang and Yanfei Shen},
keywords = {Computer vision, Action assessment, Image processing, Action recognition, Intelligent sports, Performance analysis},
abstract = {Human action analysis has been an active research area in computer vision, and has many useful applications such as human computer interaction. Most of the state-of-the-art approaches of human action analysis are data-driven and focus on general action recognition. In this paper, we aim to analyze fitness actions with skeleton sequences and propose an efficient and robust fitness action analysis framework. Firstly, fitness actions from 15 subjects are captured and built to a fitness action dataset (Fitness-28). Secondly, skeleton information is extracted and made alignment with a simplified human skeleton model. Thirdly, the aligned skeleton information is transformed to an uniform human center coordinate system with the proposed spatial–temporal skeleton encoding method. Finally, the action classifier and local–global geometrical registration strategy are constructed to analyze the fitness actions. Experimental results demonstrate that our method can effectively assess fitness action, and have a good performance on artificial intelligence fitness system.}
}
@article{MAZUMDAR2022103417,
title = {Two-stream encoder–decoder network for localizing image forgeries},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103417},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103417},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002777},
author = {Aniruddha Mazumdar and Prabin Kumar Bora},
keywords = {Image forensics, Forgery localization, CNN, Encoder–decoder network},
abstract = {This paper proposes a novel two-stream encoder–decoder network that utilizes both the high-level and the low-level image features for precisely localizing forged regions in a manipulated image. This is motivated by the fact that the forgery creation process generally introduces both the high-level artefacts (e.g., unnatural contrast) and the low-level artefacts (e.g., noise inconsistency) to the forged images. In the proposed two-stream network, one stream learns the low-level manipulation-related features in the encoder side by extracting noise residuals through a set of high-pass filters in the first layer. In the second stream, the encoder learns the high-level image manipulation features from the input image RGB values. The coarse feature maps each encoder are upsampled by the corresponding decoder network to produce the dense feature maps. The dense feature maps of the two streams are concatenated and fed to a final convolutional layer with sigmoidal activation to produce the pixel-wise prediction. We have carried out experimental analyses on multiple standard forensics datasets to evaluate the performance of the proposed method. The experimental results show the efficacy of the proposed method with respect to the state-of-the-art.}
}
@article{FANG2021103314,
title = {Label projection online hashing for balanced similarity},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103314},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103314},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100208X},
author = {Yuzhi Fang and Huaxiang Zhang and Li Liu},
keywords = {Online hashing, Label semantic, Image retrieval, Balanced similarity},
abstract = {Since online hashing has the advantages of low storage and fast calculation ,it attracts the attention of many scholars. However, the learning of new data streams separates the similarity between new data and existing data in many online hashing methods, which leads to poor retrieval performance. In addition, the similarity measure ignores the expression of different similarity. In this paper, we propose a novel supervised method, namely Label Projection Online Hashing for Balanced Similarity (LPOH). Compared with existing online hashing methods, LPOH aims to solve the problem of the effective establishment of the projection between the label vector and the binary code, and the successful realization of description of different similarity between the same labeled data. Specifically, LPOH overcomes the problem of similarity deviation caused by data imbalance via establishing a mapping matrix to derive a relationship between the data label vector and the binary code. Furthermore, the error between the binary code and the hash function concerning data streams is described. Extensive experiments on widely-used three benchmark datasets demonstrate that LPOH outperforms the state-of-the-art online hashing methods.}
}
@article{WU2022103397,
title = {Single image shadow detection via uncertainty analysis and GCN-based refinement strategy},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103397},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103397},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002650},
author = {Wen Wu and Kai Zhou and Xiao-Diao Chen},
keywords = {Deep learning, Image processing, Shadow detection, Uncertainty quantification, Graph convolution networks},
abstract = {Learning-based shadow detection methods have achieved an impressive performance, while these works still struggle on complex scenes, especially ambiguous soft shadows. To tackle this issue, this work proposes an efficient shadow detection network (ESDNet) and then applies uncertainty analysis and graph convolutional networks for detection refinement. Specifically, we first aggregate global information from high-level features and harvest shadow details in low-level features for obtaining an initial prediction. Secondly, we analyze the uncertainty of our ESDNet for an input shadow image and then take its intensity, expectation, and entropy into account to formulate a semi-supervised graph learning problem. Finally, we solve this problem by training a graph convolution network to obtain the refined detection result for every training image. To evaluate our method, we conduct extensive experiments on several benchmark datasets, i.e., SBU, UCF, ISTD, and even on soft shadow scenes. Experimental results demonstrate that our strategy can improve shadow detection performance by suppressing the uncertainties of false positive and false negative regions, achieving state-of-the-art results.}
}
@article{QIN2021103325,
title = {Adversarial steganography based on sparse cover enhancement},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103325},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103325},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002133},
author = {Chuan Qin and Weiming Zhang and Xiaoyi Dong and Hongyue Zha and Nenghai Yu},
keywords = {Steganography, Adversarial example, Deep neural network},
abstract = {CNN (Convolutional Neural Network) steganalyzers achieve enormous improvements in detecting stego images. However, they are easily deceived by adversarial steganography, which combines adversarial attack and steganography. Currently, there are two kinds of adversarial steganography, function separation and cover enhancement. ADV-EMB (ADVersarial EMBedding) is a typical function separation method. It forces the steganographic modifications along side the gradient directions of the target CNN steganalyzer on partial image elements. It results in relatively low deceiving success rate against the target model. ADS (ADversarial Steganography) is the first adversarial steganography, which is based on cover enhancement. It introduces much distortions, so it can be easily detected by non-target steganalyzers. To overcome such defects of the previous works, in this paper, we propose a novel cover enhancement method, denoted as SPS-ENH (SParSe ENHancement). Through sparse ±1 adversarial perturbations, we effectively compress the distortions caused in cover enhancement. In addition, a re-trying scheme is introduced to further reduce the distortion scale. Extensive experiments show that the proposed method outperforms the previous works in the average classification error rates under non-target steganalyzers and deceiving success rates against target CNN models. When combining with the min–max strategy, the proposed method converges in less iterations and provides higher security level than ADV-EMB.}
}
@article{XU2022103430,
title = {Exemplar-based image inpainting using adaptive two-stage structure-tensor based priority function and nonlocal filtering},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103430},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103430},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002893},
author = {Ting Xu and Ting-Zhu Huang and Liang-Jian Deng and Xi-Le Zhao and Jin-Fan Hu},
keywords = {Exemplar-based image inpainting, Non-local texture matching, Structure-tensor, Texture and structure synthesis, Object removal, Remote sensing image inpainting},
abstract = {For the exemplar-based image inpainting problem, the filling order and local intensity smoothness are two crucial factors that should be considered carefully. This work gives a new exemplar-based image inpainting method, preventing geometric structures from being destroyed and reconstructing textures well to obtain elegant-looking outputs. For a better filling order, we define a new adaptive two-stage structure-tensor based priority function. To promote the local intensity smoothness, we adopt a non-local way, and at the same time, propose a weighted filter based on a Gaussian-like function to generate the ideal filling patch by combining non-local patches. We compare the proposed method with some recent state-of-the-art image inpainting approaches on different tasks, such as texture and structure synthesis, object removal, and remote sensing images inpainting. Experimental results demonstrate the superiority of the proposed method, both visually and quantitatively.}
}
@article{MANSOURI2021103359,
title = {Reversible data hiding with automatic contrast enhancement using two-sided histogram expansion},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103359},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103359},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002376},
author = {Saeideh Mansouri and Hossein {Khaleghi Bizaki} and Mohammad Fakhredanesh},
keywords = {Reversible data hiding, Reversible contrast enhancement, Two-sided histogram shifting, Embedding capacity},
abstract = {Recently, reversible data hiding (RDH) has emerged into a new class of data hiding methods that enables exact retrieving of both embedded data and cover medium. In the present study, a novel automatic RDH method with contrast enhancement is proposed, in which the data is embedded through two-sided histogram expansion. Two-sided histogram shifting doubles the number of bits embedded at each iteration. Moreover, it preserves the mean brightness of the cover image and prevents it from over enhancement with less calculation. Experimental results on two sets of images show that the proposed method enhances the image contrast at an appropriate level without using a mean brightness controller during data embedding and provides higher information security compared to the existing RDH approaches.}
}
@article{KUMAR2021103376,
title = {A multi-exposure fusion framework for contrast enhancement of hazy images employing dynamic stochastic resonance},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103376},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103376},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002492},
author = {Avishek Kumar and Rajib Kumar Jha and Naveen K. Nishchal},
keywords = {Dynamic stochastic resonance, Image dehazing, Restoration, Weight maps, Multi-scale fusion, Modified atmospheric scattering equation},
abstract = {Current imaging devices coupled with advanced hardware and software are smart enough to enhance low light images taken in clear weather. But in hazy or foggy environments, the captured images are of degraded quality. To address this issue, image processing algorithms are employed to enhance the degraded images to make useful for extracting meaningful features. In this study, we propose a haze removal algorithm to improve the color and contrast of images captured in hazy environments. The first step involves generation of images with various exposures using the theory of dynamic stochastic resonance. The images are then fused in a multi-scale fusion framework crafting weight maps viz. haze density, chromaticity, and luminance gradient. The fusion process focuses on uniformly enhancing the dark and bright regions of the image. However, it may overemphasize haze affected regions. Therefore, in the second step, the atmospheric scattering equation is referred and its modified version is applied that accomplishes the haze removal task. Quantitative and qualitative analyses demonstrate the effectiveness of the proposed method.}
}
@article{ZHOU2021103317,
title = {Multi-ColorGAN: Few-shot vehicle recoloring via memory-augmented networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103317},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103317},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002091},
author = {Wei Zhou and Si-Bao Chen and Li-Xiang Xu and Bin Luo},
keywords = {Memory-augmented network, Generative adversarial network, Image recoloring, Vehicle recoloring, Few-shot learning},
abstract = {Despite the notable successes of Generative adversarial networks (GANs) achieved to date, applying them to real-world problems still poses significant challenges. In real traffic surveillance scenarios, for the task of generating images of multiple color of truck heads and cars without changing textures and license plates, conditional image generation hardly manipulate the generated images by the color attribute. Image style transfer methods inevitably produce color smearing. Even state-of-the-art methods of disentangled representation learning (e.g. MixNMatch) cannot disentangle colors individually, ensuring that irrelevant factors, such as texture remain the same. To solve this problem, we present an approach called Multi-ColorGAN based on memory-augmented networks for multi-color real vehicle coloring/generation with limited data. In particular, our model could filter out unwanted color changes in specific areas with a simple but effective method called Fusion Module, and generate more natural color images. Experiments on three vehicle image benchmarks and a new truck image dataset are conducted to evaluate the proposed Multi-ColorGAN compared to state-of-the-art.}
}
@article{ZHANG2021103280,
title = {Sign language recognition based on global-local attention},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103280},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103280},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001838},
author = {Shujun Zhang and Qun Zhang},
keywords = {Sign language recognition, 3D convolution network, Global-local attention, Time series modeling},
abstract = {Video-level sign language recognition is still a challenging task due to the influence of sign language-independent factors and timing requirements. This paper constructs a sign language recognition framework based on global-local feature description, and proposes a three-dimensional residual global network model with attention layer and a local network model based on target detection. The global feature description is based on the whole video behavior for time series modeling. The improved timing conversion layer is used to explore the timing information of different periods and learn the video representations of different timings. In the local module the hand is located through the target detection network to highlight its key role in the whole sign language behavior, which strengthens the category differences, and compensates the global network. Experiments on two well-known Chinese sign language datasets (SLR_Dataset and DEVSIGN_D) show that the proposed method can obtain higher recognition accuracy (respectively 89.2%, 91%) and better generalization performance.}
}
@article{M2022103416,
title = {Human identification system using 3D skeleton-based gait features and LSTM model},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103416},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103416},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002807},
author = {Rashmi M. and Ram Mohana Reddy Guddeti},
keywords = {Biometric, Deep learning, Gait recognition, Human identification, Long Short Term Memory (LSTM), Smart surveillance},
abstract = {Vision-based gait emerged as the preferred biometric in smart surveillance systems due to its unobtrusive nature. Recent advancements in low-cost depth sensors resulted in numerous 3D skeleton-based gait analysis techniques. For spatial–temporal analysis, existing state-of-the-art algorithms use frame-level information as the timestamp. This paper proposes gait event-level spatial–temporal features and LSTM-based deep learning model that treats each gait event as a timestamp to identify individuals from walking patterns observed in single and multi-view scenarios. On four publicly available datasets, the proposed system stands superior to state-of-the-art approaches utilizing a variety of conventional benchmark protocols. The proposed system achieved a recognition rate of greater than 99% in low-level ranks during the CMC test, making it suitable for practical applications. The statistical study of gait event-level features demonstrated retrieved features’ discriminating capacity in classification. Additionally, the ANOVA test performed on findings from K folds demonstrated the proposed system’s significance in human identification.}
}
@article{QIAO2021103320,
title = {Geographical position spoofing detection based on camera sensor fingerprint},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103320},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103320},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100211X},
author = {Tong Qiao and Qianru Zhao and Ning Zheng and Ming Xu and Li Zhang},
keywords = {Geographical position spoofing detection, Source identification, Camera sensor fingerprint},
abstract = {Currently, position check-in on mobile devices has become a fashionable social activity. Meanwhile, criminals probably tamper the geographical position (geo-position) information to provide an alibi. Therefore, it is of importance to identify the authenticity of geo-position. To our knowledge, many current methods for geo-position spoofing detection mainly rely on geo-position information in the database. However, these methods possibly fail in the case of missing prior information or lacking rich training samples. To address that challenge, this paper proposes an alternative manner for detecting the geo-position spoofing via camera sensor fingerprint. In particular, the camera sensor fingerprint is first extracted through the images posted by an inquiry user based on the well-designed denoising filter. Second, the authenticity of the geo-position is verified by comparing the consistency of the residual noise from newly-posted images with position check-in and the unique camera sensor fingerprint from an inquiry user. Finally, the extensive experiments are conducted on the image database, that empirically indicates the relevance of our proposed simple but effective method.}
}
@article{WU2021103261,
title = {Multiple attention encoded cascade R-CNN for scene text detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103261},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103261},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001711},
author = {Yirui Wu and Wenxiang Liu and Shaohua Wan},
keywords = {Cascade R-CNN, Deep representation learning, Applications to robust image recognition, Multiple attention encoding, Scene text detection, Multi-oriented text},
abstract = {Inspired by instance segmentation algorithms, researchers have proposed quantity of segmentation-based methods for text detection, achieving remarkable results on scene text with arbitrary orientation and large aspect ratios. Following their success, we believe cascade architecture and extracting contextual information in multiple aspects are powerful to boost performance on the basis of segmentation-based methods, especially in decreasing false positive texts in complex natural scene. Based on such consideration, we propose a multiple-context-aware and cascade CNN structure, which appropriately encodes multiple categories of context information into a cascade R-CNN framework. Specifically, the proposed method consists of two stages, i.e., feature generation and cascade detection. During the first stage, we define ISTK (Isolated Selective Text Kernel) module to refine feature map, which sequentially encodes channel-wise and kernel-size attention information by designing multiple branches and different kernel sizes in isolate form. Afterwards, we build long-range spatial dependencies in feature map via non-local operations. Built on contextual feature map, Cascade Mask R-CNN structure progressively refines accurate boundaries of text instances with multi-stage framework. We conduct comparative experiments on ICDAR2015 and 2017-MLT datasets, where the proposed method outperform comparative methods in terms of effectiveness and efficiency measurements.}
}
@article{YU2021103295,
title = {Distinguishing between natural and recolored images via lateral chromatic aberration},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103295},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103295},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001954},
author = {Yangxin Yu and Ning Zheng and Tong Qiao and Ming Xu and Jiasheng Wu},
keywords = {Image forensics, Recolored images detection, Lateral chromatic aberration},
abstract = {With the development of image colorization technique, the recolored images (RIs) become more and more authentic, making it very difficult to visually distinguish from natural images (NIs). Recently, researchers have proposed the detection methods towards recolored images. However, the current detection still has limitations such as poor generalization, large-scale training samples, high-dimensional features for training, and high computation cost. To address those issues, this paper proposes a novel method based on the lateral chromatic aberration (LCA) inconsistency and its statistical differences. Generally, RIs have fewer numbers of LCA characteristics than that of NIs, that inspire us to design the classifier for distinguishing two types of images. In particular, we propose to adopt very low 5-dimensional features to feed a classical SVM mechanism. The baseline ImageNet and Oxford datasets are used to verify the effectiveness of the proposed method, in which the performance of our proposed method rivals the prior arts.}
}
@article{MO2022103431,
title = {DCA-CycleGAN: Unsupervised single image dehazing using Dark Channel Attention optimized CycleGAN},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103431},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103431},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002923},
author = {Yaozong Mo and Chaofeng Li and Yuhui Zheng and Xiaojun Wu},
keywords = {Image dehazing, Adversarial network, Dark channel attention, Local discriminator},
abstract = {Single image dehazing has great significance in computer vision. In this paper, we propose a novel unsupervised Dark Channel Attention optimized CycleGAN (DCA-CycleGAN) to deal with the challenging scene with uneven and dense haze concentration. Firstly, the DCA-CycleGAN adopts the dark channel as input and then generate attention through a DCA subnetwork to handle the nonhomogeneous haze. Secondly, in addition to the conventional global discriminator, we also leverage two local discriminators to enhance the dehazing performance on the local dense haze, and a new local adversarial loss calculated strategy is been proposed. Specifically, the dehazing generator consists of two subnetworks: an auto-encoder and a dark channel attention subnetwork. The auto-encoder consists of an encoder, a feature transformation module, and a decoder. The dark channel attention subnetwork has the same structure as the encoder and the feature transformation module to ensure the same receptive field, which utilizes the dark channel to generate attention map and fine-tune the auto-encoder. Experimental results against several state-of-the-art methods demonstrate that our method can generate better visual effects, and is effective.}
}
@article{KIM2021103364,
title = {Low-light image enhancement by diffusion pyramid with residuals},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103364},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103364},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002406},
author = {Wonjun Kim},
keywords = {Low-light image enhancement, Scene illumination, Diffusion pyramid with residuals},
abstract = {With the advancement of the camera-related technology in mobile devices, the vast amount of photos have been taken and shared in our daily life. However, many users still have unsatisfactory experiences with low-visible photos, which are frequently acquired under complicated real-world environments. In this paper, a novel yet simple method for low-light image enhancement has been proposed without any learning procedure. The key idea of the proposed method is to estimate properties of the scene illumination both in global and local manner by exploiting the diffusion pyramid with residuals. Specifically, the residual of each scale level in the diffusion pyramid is combined with the corresponding input. This restored result efficiently highlights local details across different scale spaces, thus it is helpful for preserving the boundary of illuminations. By conducting max-pooling with restored results from different levels of the diffusion pyramid, which are resized to the original resolution, the illumination component is accurately inferred from a given image. Compared to recent learning-based approaches, one important advantage of the proposed method is to effectively avoid the overfitting problem to the specific training dataset. Experimental results on various benchmark datasets demonstrate the efficiency and robustness of the proposed method for low-light image enhancement in real-world scenarios.}
}
@article{PRADHAN2022103396,
title = {A CBIR system based on saliency driven local image features and multi orientation texture features},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103396},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103396},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002649},
author = {Jitesh Pradhan and Arup Kumar Pal and Haider Banka},
keywords = {Content-based Image Retrieval (CBIR), Gabor filter, Object detection, Saliency map, Singular value decomposition (SVD)},
abstract = {In Content-based Image Retrieval (CBIR), the user provides the query image in which only a selective portion of the image carries the foremost vital information known as the object region of the image. However, the human visual system also focuses on a particular salient region of an image to instinctively understand its semantic meaning. Therefore, the human visual attention technique can be well imposed in the CBIR scheme. Inspired by these facts, we initially utilized the signature saliency map-based approach to decompose the image into its respective main object region (ObR) and non-object region (NObR). ObR possesses most of the vital image information, so block-level normalized singular value decomposition (SVD) has been used to extract salient features of the ObR. In most natural images, NObR plays a significant role in understanding the actual semantic meaning of the image. Accordingly, multi-directional texture features have been extracted from NObR using Gabor filter on different wavelengths. Since the importance of ObR and NObR features are not equal, a new homogeneity-based similarity matching approach has been devised to enhance retrieval accuracy. Finally, we have demonstrated retrieval performances using both the combined and distinct ObR and NObR features on seven standard coral, texture, object, and heterogeneous datasets. The experimental outcomes show that the proposed CBIR system has a promising retrieval efficiency and outperforms various existing systems substantially.}
}
@article{KISHANBABU2022103382,
title = {CDGAN: Cyclic Discriminative Generative Adversarial Networks for image-to-image transformation},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103382},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103382},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002522},
author = {Kancharagunta {Kishan Babu} and Shiv Ram Dubey},
keywords = {Deep networks, Computer vision, Generative Adversarial Nets, Image-to-image transformation, Cyclic-Discriminative Adversarial loss},
abstract = {Generative Adversarial Networks (GANs) have facilitated a new direction to tackle the image-to-image transformation problem. Different GANs use generator and discriminator networks with different losses in the objective function. Still there is a gap to fill in terms of both the quality of the generated images and close to the ground truth images. In this work, we introduce a new Image-to-Image Transformation network named Cyclic Discriminative Generative Adversarial Networks (CDGAN) that fills the above mentioned gaps. The proposed CDGAN generates high quality and more realistic images by incorporating the additional discriminator networks for cycled images in addition to the original architecture of the CycleGAN. The proposed CDGAN is tested over three image-to-image transformation datasets. The quantitative and qualitative results are analyzed and compared with the state-of-the-art methods. The proposed CDGAN method outperforms the state-of-the-art methods when compared over the three baseline Image-to-Image transformation datasets. The code is available at https://github.com/KishanKancharagunta/CDGAN.}
}
@article{ZHU2021103303,
title = {Camera style transformation with preserved self-similarity and domain-dissimilarity in unsupervised person re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103303},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103303},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002005},
author = {Zhiqin Zhu and Yaqin Luo and Sixin Chen and Guanqiu Qi and Neal Mazur and Chengyan Zhong and Qiwang Li},
keywords = {Unsupervised domain adaptation, Person re-identification, Cross-domain, Circle loss, Maximum mean discrepancy},
abstract = {The inconsistency caused by different factors, such as different camera imaging methods, complex imaging environments, and changes in light, present a huge challenge to person re-identification (re-ID). Unsupervised domain adaptation (UDA) can solve the inconsistency issue to a certain extent, but different datasets may not have any overlapping of people’s identities. Therefore, it is necessary to pay attention to people’s identities in solving domain-dissimilarity. A camera imaging style transformation with preserved self-similarity and domain-dissimilarity (CSPSD) is proposed to solve the cross-domain issue in person re-ID. First, CycleGAN is applied to determine the style conversion between source and target domains. Intra-domain identity constraints are used to maintain identity consistency between source and target domains during the image style transformation process. Maximum mean difference (MMD) is used to reduce the difference in feature distribution between source and target domains. Then, a one-to-n mapping method is proposed to achieve the mapping between positive pairs and distinguish negative pairs. Any sample image from the source domain and its transformed image or a transformed image with the same identity information compose a positive pair. The transformed image and any image from the target domain compose a negative pair. Next, a circle loss function is used to improve the learning speed of positive and negative pairs. Finally, the proposed CSPSD that can effectively reduce the difference between domains and an existing feature learning network work together to learn a person re-ID model. The proposed method is applied to three public datasets, Market-1501, DukeMTMC-reID, and MSMT17. The comparative experimental results confirm the proposed method can achieve highly competitive recognition accuracy in person re-ID.}
}
@article{GURUPRASAD2021103274,
title = {Effective compressed sensing MRI reconstruction via hybrid GSGWO algorithm},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103274},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103274},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001796},
author = {Shrividya Guruprasad and S.H. Bharathi and D. {Anto Ramesh Delvi}},
keywords = {Compressed sensing, Cross guided bilateral filter, Quasi random sampling, Hybridized Galactic Swarm Optimization and Grey Wolf Optimization, Hybridized Walsh Hadamard Transform and Discrete Wavelet Transform},
abstract = {In a bio-imaging context, the main issues which obstruct the CS (Compressed sensing) application are image reconstruction time and computational cost. This paper presents an effective compressed sensing-based MRI reconstruction through a hybrid optimization algorithm. Initially, the preprocessing stage is performed using Cross guided bilateral filter. Then the K-space is generated by the Fourier transform. The hybrid Walsh Hadamard Transform and Discrete Wavelet Transform (HWHDWT) is utilized for the compressive sensing of the images. Finally, the Hybrid Galactic Swarm Optimization and Grey Wolf Optimization (HGSGWO) algorithm are developed for MRI reconstruction. The dataset collected from a hospital which contains MRI images both in JPEG and DICOM format. The performance of SSIM (Structural Similarity Index), PSNR (Peak Signal to Noise Ratio), MSE (mean square error) and reconstruction time are evaluated for images and it is compared with the existing methods.}
}
@article{WEI2022103418,
title = {Person re-identification based on deep learning — An overview},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103418},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103418},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002765},
author = {Wenyu Wei and Wenzhong Yang and Enguang Zuo and Yunyun Qian and Lihua Wang},
keywords = {Person re-identification, Deep learning, Convolutional neural networks, Attention mechanism},
abstract = {Person re-identification(ReID) is an intelligent video surveillance technology that retrieves the same person from different cameras. This task is extremely challenging due to changes in person poses, different camera views, and occlusion. In recent years, person ReID based on deep learning technology has received widespread attention due to the rapid development and excellent performance of deep learning. In this paper, we first divide person ReID based on deep learning approaches into seven types, i.e., fused hand-crafted features deep model, representation learning model, metric learning model, part-based deep model, video-based model, GAN-based model, unsupervised model. Furthermore, we launched a brief overview of the seven types. Then, we introduce some examples of commonly used datasets, compare the performance of some algorithms on image and video datasets in recent years, and analyze the advantages and disadvantages of various methods. Finally, we summarize the possible future research directions of person ReID technology.}
}
@article{LIU2021103321,
title = {Video summary generation by visual shielding compressed sensing coding and double-layer affinity propagation},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103321},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103321},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002121},
author = {Jixin Liu and Dan Yu and Zheng Tang},
keywords = {Visual shielding, Video summary technology, Compressed sensing, Double-layer affinity propagation, Feature fusion},
abstract = {Video summary technology based on keyframe extraction is an effective means to rapidly access video content. Traditional video summary generation technology requires high video resolution, which poses a problem as most existing studies have no targeted solutions for videos that are subject to privacy protection. We propose a novel keyframe extraction algorithm for video data in the visual shielding domain, named visual shielding compressed sensing coding and double-layer affinity propagation (VSCS-DAP). VSCS-DAP involves three main steps. First, the video is compressed by compressed sensing technology to provide a visual shielding effect (protecting the privacy of monitored figures), while the data volume is significantly reduced. Then, pyramid histogram of oriented gradients (PHOG) features are extracted from the compressed video to be clustered by the first step affinity propagation (AP) to gain the summaries of the first stage. Finally, the PHOG and Hist fusion features are extracted from the keyframes of the first stage, and they cluster the fused PHOG-Hist features by the second step AP algorithm to obtain the final output summaries. Experimental results obtained on two common video datasets show that our method exhibits advantages including low redundancy and few missing frames, low computational complexity, strong real-time performance, and robustness to vision-shielded video.}
}
@article{CAI2022103358,
title = {Proximal-Gen for fast compressed sensing recovery},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103358},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103358},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002364},
author = {Lei Cai and Yuli Fu and Tao Zhu and Youjun Xiang and Huanqiang Zeng},
keywords = {Compressed sensing, Generative models, Generator range, Reconstruction efficiency},
abstract = {Compressed sensing (CS) can recover an image from a few random measurements by exploiting the sparsity assumption on the structure of images. Some recent generative model-based CS recovery methods have removed the sparsity constraint, but their recovery process is slow and the recovered signal is constrained to be in the generator range. Here, we propose a new framework, called Proximal-Gen, for CS recovery. Specifically, we first formulate a general domain of the recovered signals, this allows the subsequent recovery algorithms to recover the signals that deviate from the generator range. Then based on the general domain, we develop a fast recovery algorithm, which mainly consists of two sub-algorithms, namely network-based projected gradient descent (NPGD) and denoiser-based proximal gradient descent (DPGD). The NPGD is used to obtain an intermediate signal lying in the generator range, while the DPGD is proposed to recover a deviation signal. Compared with multiple recent generative model-based recovery methods, our method can achieve better reconstruction performance and higher efficiency under most measurements.}
}
@article{HADJADJI2022103375,
title = {Multi-oriented run length based static and dynamic features fused with Choquet fuzzy integral for human fall detection in videos},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103375},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103375},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002480},
author = {Bilal Hadjadji and Matthieu Saumard and Michael Aron},
keywords = {Multi-oriented run length, Static and dynamic features, Choquet fuzzy integral, Tree architecture, Human fall detection},
abstract = {Since a huge part of elderly people are living alone, assisted-living tools have become an essential in-home telemonitoring device. Hence, this paper proposes an automatic human fall detection in videos. In order to improve the system reliability, a new shape descriptor called multi-oriented run length (MORL) is proposed. This descriptor is exploited in a proposed scheme to generate static and dynamic features to represent human falls with complementary information. The generated static and dynamic features are fused through the Choquet fuzzy integral. Experimental results conducted on three well-known datasets containing almost 1300 video segments show an interesting adaptation of the proposed approaches. More precisely, the proposed MORL descriptor shows its superiority against known descriptors such as LBP and HOG. Moreover, Choquet fuzzy integral significantly improves the results versus standard combiners. In general, the obtained results highlight the reliability of the proposed system versus recent studies for human fall detection.}
}
@article{HE2021103278,
title = {Detection of moving objects using adaptive multi-feature histograms},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103278},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103278},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001826},
author = {Wei He and Wujing Li and Guoyun Zhang and Bing Tu and Yong {Kwan Kim} and Jianhui Wu and Qi Qi},
keywords = {Moving object detection, Background subtraction, Local compact binary descriptor, Multi-feature histogram},
abstract = {This paper presents a moving object detection scheme that incorporates three innovations. First, considering the inter-frame consistency of pixels, we extend the compact binary face descriptor (CBFD) to the temporal domain and propose a novel local binary descriptor named temporally-consistent local compact binary descriptor (TC-LCBD), which exploits the useful correlation of the intensities of inter-frame pixels to guarantee good performance in complex scenes. We do this mainly because the background scene between frames has a significant coherence. Second, both color and TC-LCBD features are modeled as a group of adaptive histograms for characterizing each pixel, which can enhance the robustness to dynamic backgrounds and illumination changes. Third, by comparing changes in histogram proximity between two adjacent frames, we can dynamically adjust the model sensitivity and adaptation rate without user intervention. Experimental results on well-known, challenging data sets demonstrate that the proposed method significantly outperforms many state-of-the-art methods.}
}