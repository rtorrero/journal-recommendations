@article{LUO2022103630,
title = {Confidence based class weight and embedding discrepancy constraint network for partial domain adaptation},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103630},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103630},
url = {https://www.sciencedirect.com/science/article/pii/S104732032200150X},
author = {Mingkai Luo and Zhao Yang and Weiwei Ai and Jiehao Liu},
keywords = {Partial domain adaptation, Deep transfer learning, Adversarial alignment, Classification learning},
abstract = {Partial domain adaptation (PDA) is a special domain adaptation task where the label space of the target domain is a subset of the source domain. In this work, we present a novel adversarial PDA method named Confidence Based Class Weight and Embedding Discrepancy Constraint Network (CEN). Specifically, we design a robust weighting scheme that takes sample confidence and class information into account. It can automatically distinguish outlier samples in the source domain and reduce their importance. Besides, we consider the relationship between feature norm and domain shift. We limit the expectation of the feature norms of both domains to an adaptive value. By this means, we can align the feature distributions and help the deep model learn domain-invariant representations. Comprehensive experiments on three domain adaptation datasets Office-31, Office-home, and Visda2017 show that our approach surpasses state-of-the-art methods on various PDA tasks.}
}
@article{ZHANG2022103671,
title = {SiamMBFAN: Siamese tracker with multi-branch feature aggregation network},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103671},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103671},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001912},
author = {Hao Zhang and Yan Piao and Bailiang Huang and Baolin Tan},
keywords = {Visual tracking, Siamese network, Multi-branch network, Feature aggregation},
abstract = {Siamese trackers have attracted considerable attention in the field of object tracking because of their high precision and speed. However, one of the main disadvantages of Siamese trackers is that their feature extraction network is relatively single. They often use AlexNet or ResNet50 as the backbone network. AlexNet is shallow and thus cannot easily extract abundant semantic information, whereas ResNet50 has many convolutional layers, reducing the real-time performance of Siamese trackers. We propose a multi-branch feature aggregation network with different designs in the shallow and deep convolutional layers. We use the residual module to build the shallow convolutional layers to extract textural and edge features. The deep convolution layers, designed with two independent branches, are built with residual and parallel modules to extract different semantic features. The proposed network has a depth of only nine modules, and thus it is a simple and effective network. We then apply the network to a Siamese tracker to form SiamMBFAN. We design multi-layer classification and regression subnetworks in the Siamese tracker by aggregating the last three modules of the two branches, improving the localization ability of the tracker. Our tracker achieves a better balance between performance and speed. Finally, SiamMBFAN is tested on four challenging benchmarks, including OTB100, VOT2016, VOT2018, and UAV123. Compared with other trackers, our tracker improves by 7% (OTB100).}
}
@article{CHANG2022103643,
title = {Stereo image quality assessment considering the difference of statistical feature in early visual pathway},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103643},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103643},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001638},
author = {Yongli Chang and Sumei Li and Jie Jin and Anqi Liu and Wei Xiang},
keywords = {Stereo image quality assessment, Retinal ganglion cells, ON and OFF receptive fields, Monocular and binocular features},
abstract = {Human visual theory is closely related to stereo image quality assessment (SIQA), which determines whether the evaluation results of SIQA method can keep good consistency with subjective perception. Many SIQA methods are not fully based on human visual theory, so there is still room for improvement. The research on the visual system tends to the dorsal and ventral pathways, which ignores the information differences in the early visual pathways. It is worth noting that the ON and OFF receptive fields in retinal ganglion cells (RGCs) respond asymmetrically to the statistical features of images. Inspired by this, in this paper, we propose an SIQA method based on monocular and binocular visual features, which takes into account the difference of ON and OFF response features in early visual pathways. Moreover, the different information interaction mechanisms of visual cortex are used to fuse the response maps information of left and right images. Final, monocular and binocular features are extracted and sent to support vector regression (SVR) for quality prediction. Experimental results show that the proposed method is superior to several mainstream SIQA metrics on four publicly available stereo image databases.}
}
@article{SHAKEEL2022103628,
title = {Multi-scale attention guided network for end-to-end face alignment and recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103628},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103628},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001481},
author = {M. Saad Shakeel and Yuxuan Zhang and Xin Wang and Wenxiong Kang and Arif Mahmood},
keywords = {Attention network, Feature alignment, Multi-scale features, Adaptive feature fusion},
abstract = {Attention modules embedded in deep networks mediate the selection of informative regions for object recognition. In addition, the combination of features learned from different branches of a network can enhance the discriminative power of these features. However, fusing features with inconsistent scales is a less-studied problem. In this paper, we first propose a multi-scale channel attention network with an adaptive feature fusion strategy (MSCAN-AFF) for face recognition (FR), which fuses the relevant feature channels and improves the network’s representational power. In FR, face alignment is performed independently prior to recognition, which requires the efficient localization of facial landmarks, which might be unavailable in uncontrolled scenarios such as low-resolution and occlusion. Therefore, we propose utilizing our MSCAN-AFF to guide the Spatial Transformer Network (MSCAN-STN) to align feature maps learned from an unaligned training set in an end-to-end manner. Experiments on benchmark datasets demonstrate the effectiveness of our proposed MSCAN-AFF and MSCAN-STN.}
}
@article{PEHLIVAN2023103709,
title = {Improved action proposals using fine-grained proposal features with recurrent attention models},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103709},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103709},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002292},
author = {Selen Pehlivan and Jorma Laaksonen},
keywords = {Temporal action proposal generation, Untrimmed video understanding, Temporal convolution, Recurrent models, Attention},
abstract = {Recent models for the temporal action proposal task show that local properties can be an alternative to the region proposal network (RPN) for generating good proposal candidates on untrimmed videos. In this study, we devise an RPN model with a new two-stage pipeline and a new joint scoring function for temporal proposals. The evaluation of local properties is integrated into our RPN model to search for the best proposal candidates that can be distinguished mainly in fine details of proposal regions. Our network models proposals in multiple scales using two recurrent neural network layers with attention mechanisms. We observe that joint training of the RPN with local clues and multi-scale modeling of proposals with recurrent attention mechanisms improve the performance of the proposal generation task. Our model yields state-of-the-art results on the THUMOS-14 and comparable results on the ActivityNet-1.3 datasets.}
}
@article{ZHOU2023103725,
title = {Illumination-aware window transformer for RGBT modality fusion},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103725},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103725},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002450},
author = {Lin Zhou and Zhenzhong Chen},
keywords = {Multispectral image, Multi-modal learning, Transformers},
abstract = {Combination of RGB and thermal sensors has been proven to be useful for many vision applications. However, how to effectively fuse the information of two modalities remains a challenging problem. In this paper, we propose an Illumination-Aware Window Transformer (IAWT) fusion module to handle the RGB and thermal multi-modality fusion. Specifically, the IAWT fusion module adopts a window-based multi-modality attention combined with additional estimated illumination information. The window-based multi-modality attention infers dependency cross modalities within a local window, thus implicitly alleviate the problem caused by weakly spatial misalignment of the RGB and thermal image pairs within specific dataset. The introduction of estimated illumination feature enables the fusion module to adaptively merge the two modalities according to illumination conditions so as to make full use of the complementary characteristics of RGB and thermal images under different environments. Besides, our proposed fusion module is task-agnostic and data-specific, which means it can be used for different tasks with RGBT inputs. To evaluate the advances of the proposed fusion method, we embed the IAWT fusion module into different networks and conduct the experiments on various RGBT tasks, including pedestrian detection, semantic segmentation and crowd counting. Extensive results demonstrate the superior performance of our method.}
}
@article{KHELLATKIHEL2022103627,
title = {Gender and ethnicity recognition based on visual attention-driven deep architectures},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103627},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103627},
url = {https://www.sciencedirect.com/science/article/pii/S104732032200147X},
author = {Souad Khellat-Kihel and Jawad Muhammad and Zhenan Sun and Massimo Tistarelli},
keywords = {Visual-attention, Periocular regions, Gender prediction, Deep learning, Local description},
abstract = {Most of the time, when people observe, interact or speak to each other, they focus the attention on the ocular parts of the face. This daily life experience has a strong impact on the analysis of periocular facial regions. These facial regions may be exploited in order to identify individuals for several applications, including access control and services such as telebanking and electronic transactions. In this paper we suggest studying the efficiency of the periocular regions on gender and race prediction. Most researchers propose a local texture description based on LBP (Local Binary Pattern) and HoG (Histogram of Oriented Gradients) for the purpose of predicting gender. On the other hand, Deep learning techniques were proposed to predict the gender. However, this requires a huge labeled periocular data for gender which is not available. Also, the expressivity of gender and race can be decreased on the final representation of the Deep architectures comparing to the earlier stages. To overcome these points and for the aim of predicting gender and race, considering also the high impact of DCNNs (Deep Convolutional Neural Networks) techniques to solve several aspects in biometrics, we suggest a Deep architecture based on visual attention on the periocular part. The visual saliency extraction is based on primary layers’ activation by analyzing the feature-maps. We study how the visual attention-based features coupled to Deep Neural Networks can be used to discriminate between gender and race, hence extract a significant feature from periocular regions. Different pretrained architectures such as Alexnet and ResNet-50 were considered to extract visual saliency points or interest points. Several experiments were performed on periocular regions and a comparative study was conducted. The present results not only demonstrate the feasibility but also the robustness of the extracted interest points.}
}
@article{ZHAO2022103668,
title = {JFLN: Joint Feature Learning Network for 2D sketch based 3D shape retrieval},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103668},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103668},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001882},
author = {Yue Zhao and Qi Liang and Ruixin Ma and Weizhi Nie and Yuting Su},
keywords = {Sketch-based 3D shape retrieval, Attention mechanism, Cross-modal retrieval},
abstract = {Cross-modal retrieval attracts much research attention due to its wide applications in numerous search systems. Sketch based 3D shape retrieval is a typical challenging cross-modal retrieval task for the huge divergence between sketch modality and 3D shape view modality. Existing approaches project the sketches and shapes into a common space for feature update and data alignment. However, these methods contain several disadvantages: Firstly, the majority approaches ignore the modality-shared information for divergence compensation in descriptor generation process. Secondly, traditional fusion method of multi-view features introduces much redundancy, which decreases the discrimination of shape descriptors. Finally, most approaches only focus on the cross-modal alignment, which omits the modality-specific data relevance. To address these limitations, we propose a Joint Feature Learning Network (JFLN). Firstly, we design a novel modality-shared feature extraction network to exploit both modality-specific characteristics and modality-shared information for descriptor generation. Subsequently, we introduce a hierarchical view attention module to gradually focus on the effective information for multiview feature updating and aggregation. Finally, we propose a novel cross-modal feature learning network, which can simultaneously contribute to modality-specific data distribution and cross-modal data alignment. We conduct exhaustive experiments on three public databases. The experimental results validate the superiority of the proposed method. Full Codes are available at https://github.com/dlmuyy/JFLN.}
}
@article{DENIPITIYAGE2022103612,
title = {PointCaps: Raw point cloud processing using capsule networks with Euclidean distance routing},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103612},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103612},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001365},
author = {Dishanika Denipitiyage and Vinoj Jayasundara and Ranga Rodrigo and Chamira U.S. Edussooriya},
keywords = {Point cloud reconstruction, Classification, Capsule networks, Error routing},
abstract = {Raw point cloud processing using capsule networks is widely adopted in classification, reconstruction, and segmentation due to its ability to preserve spatial agreement of the input data. However, most of the existing capsule based network approaches are computationally heavy and fail at representing the entire point cloud as a single capsule. We address these limitations in existing capsule network based approaches by proposing PointCaps, a novel convolutional capsule architecture with parameter sharing. Along with PointCaps, we propose a novel Euclidean distance routing algorithm and a class-independent latent representation. The latent representation captures physically interpretable geometric parameters of the point cloud, with dynamic Euclidean routing, PointCaps well-represents the spatial (point-to-part) relationships of points. PointCaps has a significantly lower number of parameters and requires a significantly lower number of FLOPs while achieving better reconstruction with comparable classification and segmentation accuracy for raw point clouds compared to state-of-the-art capsule networks.}
}
@article{YANG2023103713,
title = {DRBR-HDR: Dual-Branch recursive band reconstruction network for HDR with large motions},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103713},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103713},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002334},
author = {Yingjie Yang and Yongfang Wang and Han Zhang},
keywords = {Inverse tone mapping, Band reconstruction, Global features, High dynamic range images},
abstract = {Ghosting artifacts due to misaligned imaging and missing content of the moving regions are major challenges of synthesizing high dynamic range (HDR) images from multiple low-dynamic range (LDR) with different exposures in dynamic scenes. Therefore, it hopes the HDR reconstruction model can align the LDRs’ features and restore the missing content without artifacts. In the paper, a new dual-branch recursive band reconstruction network for high dynamic range (DRBR-HDR) is proposed to generate credible result in missing content regions, which not only uses global features as supplementary information to help local features from different receptive fields for efficient feature alignment but also designs a series of coarse-to-fine band representation to better repair missing areas in the process of recursion. In addition, we introduce an interactive attention mechanism for local branches to alleviate ghosting artifacts. The experimental results demonstrate that DRBR-HDR achieves state-of-the-art performance compared with that of the prevailing HDR reconstruction methods in various challenging scenes. Index Terms—inverse tone mapping, band reconstruction, global features, high dynamic range images.}
}
@article{ZHU2022103632,
title = {Dual attention interactive fine-grained classification network based on data augmentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103632},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103632},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001523},
author = {Qiangxi Zhu and Wenlan Kuang and Zhixin Li},
keywords = {Data augmentation, Hierarchical training, Denoising autoencoder, Dual attention mechanism, Interactive attention},
abstract = {The key to fine-grained image classification is to find discriminative regions. Most existing methods only use simple baseline networks or low-recognition attention modules to discover object differences, which will limit the model to finding discriminative regions hidden in images. This article proposes an effective method to solve this problem. The first is a novel layered training method, which uses a new training method to enhance the feature extraction ability of the baseline model. The second step focuses on key regions of the image based on improved long short-term memory (LSTM) and multi-head attention. In the third step, based on the feature map obtained by the dual attention network, spatial mapping is performed by a multi-layer perceptron (MLP). Then the element-by-element mutual multiplication calculation of the channel is performed to obtain a feature map with finer granularity. Finally, the CUB-200-2011, FGVC Aircraft, Stanford Cars, and MedMNIST v2 datasets achieved good performance.}
}
@article{CHEN2022103656,
title = {Single underwater image haze removal with a learning-based approach to blurriness estimation},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103656},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103656},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001766},
author = {Jian Chen and Hao-Tian Wu and Lu Lu and Xiangyang Luo and Jiankun Hu},
keywords = {Underwater image, Image dehazing, Image restoration, Image enhancement},
abstract = {Underwater images are usually degraded due to light scattering and absorption. To recover the scene radiance of degraded underwater images, a new haze removal method is presented by incorporating a learning-based approach to blurriness estimation with the image formation model. Firstly, the image blurriness is estimated with a linear model trained on a set of selected grayscale images, the average Gaussian images and blurriness images. With the estimated image blurriness, three intermediate background lights (BLs) are computed to obtain the synthesized BL. Then the scene depth is calculated by using the estimated image blurriness and BL to construct a transmission map and restore the scene radiance. Compared with other haze removal methods, haze in degraded underwater images can be removed more accurately with our proposed method. Moreover, visual inspection, quantitative evaluation and application test demonstrate that our method is superior to the compared methods and beneficial to high-level vision tasks.}
}
@article{LIU2022103642,
title = {Adaptive weight multi-channel center similar deep hashing},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103642},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103642},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001626},
author = {Xinghua Liu and Guitao Cao and Qiubin Lin and Wenming Cao},
keywords = {Multi-channel, Center similar, Multimodal retrieval, Deep cross-modal hashing},
abstract = {To increase the richness of the extracted text modality feature information and deeply explore the semantic similarity between the modalities. In this paper, we propose a novel method, named adaptive weight multi-channel center similar deep hashing (AMCDH). The algorithm first utilizes three channels with different configurations to extract feature information from the text modality; and then adds them according to the learned weight ratio to increase the richness of the information. We also introduce the Jaccard coefficient to measure the semantic similarity level between modalities from 0 to 1, and utilize it as the penalty coefficient of the cross-entropy loss function to increase its role in backpropagation. Besides, we propose a method of constructing center similarity, which makes the hash codes of similar data pairs close to the same center point, and dissimilar data pairs are scattered at different center points to generate high-quality hash codes. Extensive experimental evaluations on four benchmark datasets show that the performance of our proposed model AMCDH is significantly better than other competing baselines. The code can be obtained from https://github.com/DaveLiu6/AMCDH.git.}
}
@article{PRESOTTO2022103666,
title = {Weakly supervised learning based on hypergraph manifold ranking},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103666},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103666},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001869},
author = {João Gabriel Camacho Presotto and Samuel Felipe {dos Santos} and Lucas Pascotti Valem and Fabio Augusto Faria and João Paulo Papa and Jurandy Almeida and Daniel Carlos Guimarães Pedronette},
keywords = {Weakly supervised learning, Manifold learning, Ranking, Hypergraph},
abstract = {Significant challenges still remain despite the impressive recent advances in machine learning techniques, particularly in multimedia data understanding. One of the main challenges in real-world scenarios is the nature and relation between training and test datasets. Very often, only small sets of coarse-grained labeled data are available to train models, which are expected to be applied on large datasets and fine-grained tasks. Weakly supervised learning approaches handle such constraints by maximizing useful training information in labeled and unlabeled data. In this research direction, we propose a weakly supervised approach that analyzes the dataset manifold to expand the available labeled set. A hypergraph manifold ranking algorithm is exploited to represent the contextual similarity information encoded in the unlabeled data and identify strong similarity relations, which are taken as a path to label expansion. The expanded labeled set is subsequently exploited for a more comprehensive and accurate training process. The proposed model was evaluated jointly with supervised and semi-supervised classifiers, including Graph Convolutional Networks. The experimental results on image and video datasets demonstrate significant gains and accurate results for different classifiers in diverse scenarios.}
}
@article{SANG2023103708,
title = {Image quality assessment based on self-supervised learning and knowledge distillation},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103708},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103708},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002280},
author = {Qingbing Sang and Ziru Shu and Lixiong Liu and Cong Hu and Qin Wu},
keywords = {Knowledge distillation, Self-supervised learning, Image quality evaluation},
abstract = {Deep neural networks have achieved great success in a wide range of machine learning tasks due to their excellent ability to learn rich semantic features from high-dimensional data. Deeper networks have been successful in the field of image quality assessment to improve the performance of image quality assessment models. The success of deep neural networks majorly comes along with both big models with hundreds of millions of parameters and the availability of numerous annotated datasets. However, the lack of large-scale labeled data leads to the problems of over-fitting and poor generalization of deep learning models. Besides, these models are huge in size, demanding heavy computation power and failing to be deployed on edge devices. To deal with the challenge, we propose an image quality assessment based on self-supervised learning and knowledge distillation. First, the self-supervised learning of soft target prediction given by the teacher network is carried out, and then the student network is jointly trained to use soft target and label on knowledge distillation. Experiments on five benchmark databases show that the proposed method is superior to the teacher network and even outperform the state-of-the-art strategies. Furthermore, the scale of our model is much smaller than the teacher model and can be deployed in edge devices for smooth inference.}
}
@article{LIANG2022103667,
title = {Efficient graph attentional network for 3D object detection from Frustum-based LiDAR point clouds},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103667},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103667},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001870},
author = {Zhenming Liang and Yingping Huang and Zhenwei Liu},
keywords = {3D object detection, Multi-sensors fusion, Graph convolutional networks, Attention mechanism, Autonomous driving},
abstract = {LiDAR-based 3D object detection is important for autonomous driving scene perception, but point clouds produced by LiDAR are irregular and unstructured in nature, and cannot be adopted by the conventional Convolutional Neural Networks (CNN). Recently, Graph Convolutional Networks (GCN) has been proved as an ideal way to handle non-Euclidean structure data, as well as for point cloud processing. However, GCN involves massive computation for searching adjacent nodes, and the heavy computational cost limits its applications in processing large-scale LiDAR point cloud in autonomous driving. In this work, we adopt a frustum-based point cloud-image fusion scheme to reduce the amount of LiDAR point clouds, thus making the GCN-based large-scale LiDAR point clouds feature learning feasible. On this basis, we propose an efficient graph attentional network to accomplish the goal of 3D object detection in autonomous driving, which can learn features from raw LiDAR point cloud directly without any conversions. We evaluate the model on the public KITTI benchmark dataset, the 3D detection mAP is 63.72% on KITTI Cars, Pedestrian and Cyclists, and the inference speed achieves 7.9 fps on a single GPU, which is faster than other methods of the same type.}
}
@article{ZHANG2023103696,
title = {Perceptual quality assessment for fine-grained compressed images},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103696},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103696},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002164},
author = {Zicheng Zhang and Wei sun and Wei Wu and Ying Cheng and Xiongkuo Min and Guangtao Zhai},
keywords = {Image compression, Full-reference, Image quality assessment, Fine-grained},
abstract = {Recent years have witnessed the rapid development of image storage and transmission systems, in which image compression plays an important role. Generally speaking, image compression algorithms are developed to ensure good visual quality at limited bit rates. However, due to the different compression optimization methods, the compressed images may have different levels of quality, which needs to be evaluated quantificationally. Nowadays, the mainstream full-reference (FR) metrics are effective to predict the quality of compressed images at coarse-grained levels (the bit rates differences of compressed images are obvious), however, they may perform poorly for fine-grained compressed images whose bit rates differences are quite subtle. Therefore, to better improve the Quality of Experience (QoE) and provide useful guidance for compression algorithms, we propose a full-reference image quality assessment (FR-IQA) method for compressed images of fine-grained levels. Specifically, the reference images and compressed images are first converted to YCbCr color space. The gradient features are extracted from regions that are sensitive to compression artifacts. Then we employ the Log-Gabor transformation to further analyze the texture difference. Finally, the obtained features are fused into a quality score. The proposed method is validated on the fine-grained compression image quality assessment (FGIQA) database, which is especially constructed for assessing the quality of compressed images with close bit rates. The experimental results show that our metric outperforms mainstream FR-IQA metrics on the FGIQA database. We also test our method on other commonly used compression IQA databases and the results show that our method obtains competitive performance on the coarse-grained compression IQA databases as well.}
}
@article{WU2022103631,
title = {Block-based progressive visual cryptography scheme with uniform progressive recovery and consistent background},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103631},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103631},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001511},
author = {Xiaotian Wu and Zhonglin Luo},
keywords = {Secret sharing, Visual cryptography, Progressive recovery, Consistent background, Image block},
abstract = {Block-based progressive visual cryptography scheme (BPVCS) divides a secret image into non-overlapping blocks and encodes each block as sub-shadows. The final shadows for BPVCS are created by combining the associated sub-shadows. When enough shadows are superimposed, some of the secret blocks will be exposed. More information will be revealed as more shadows are used. This is referred to as progressive recovery. Hou et al. introduced a (2,n)-BPVCS. Yang et al. further extended the (2,n) scheme to a general (k,n) scheme. However, Yang et al. (k,n)-BPVCS suffers from the non-uniform progressive recovery and inconsistent background of recovered secret blocks. In this paper, we introduce a (k,n)-BPVCS to address the mentioned two defects. Theoretical analysis and experimental results are provided to illustrate the benefits of the proposed approach.}
}
@article{HAI2023103712,
title = {R2RNet: Low-light image enhancement via Real-low to Real-normal Network},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103712},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103712},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002322},
author = {Jiang Hai and Zhu Xuan and Ren Yang and Yutong Hao and Fengzhu Zou and Fang Lin and Songchen Han},
keywords = {Retinex theory, Low-light image enhancement, Image processing, Real-world low/normal-light image pairs},
abstract = {Images captured in weak illumination conditions could seriously degrade the image quality. Solving a series of degradation of low-light images can effectively improve the visual quality of images and the performance of high-level visual tasks. In this study, a novel Retinex-based Real-low to Real-normal Network (R2RNet) is proposed for low-light image enhancement, which includes three subnets: a Decom-Net, a Denoise-Net, and a Relight-Net. These three subnets are used for decomposing, denoising, contrast enhancement and detail preservation, respectively. Our R2RNet not only uses the spatial information of the image to improve the contrast but also uses the frequency information to preserve the details. Therefore, our model achieved more robust results for all degraded images. Unlike most previous methods that were trained on synthetic images, we collected the first Large-Scale Real-World paired low/normal-light images dataset (LSRW dataset) to satisfy the training requirements and make our model have better generalization performance in real-world scenes. Extensive experiments on publicly available datasets demonstrated that our method outperforms the existing state-of-the-art methods both quantitatively and visually. In addition, our results showed that the performance of the high-level visual task (i.e., face detection) can be effectively improved by using the enhanced results obtained by our method in low-light conditions. Our codes and the LSRW dataset are available at: https://github.com/JianghaiSCU/R2RNet.}
}
@article{YANG2022103639,
title = {AS-Net: An attention-aware downsampling network for point clouds oriented to classification tasks},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103639},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103639},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001596},
author = {Yakun Yang and Anhong Wang and Donghan Bu and Zewen Feng and Jie Liang},
keywords = {Point cloud, Downsampling, Attention, Constraint match, Classification task},
abstract = {3D point cloud has tremendous potential in many application tasks. However, the huge amount of data limits this potential. To simplify point clouds and improve their downstream application efficiency, this paper proposes AS-Net, an attention-aware downsampling network oriented to classification tasks. AS-Net realizes downsampling through an Attention-aware Sampling Module, which including an Input Embedding Module and an Attention Module. The former is designed to extract the global and local features of the point cloud, the latter is to generate the Sampling-Map to simulate the differentiable downsampling. Thanks to the attention mechanism, AS-Net may select the critical points of the original point cloud for classification tasks. In addition, AS-Net designs a Constraint Matching Module to match the sampled points to be a subset of the original point cloud at the inference phase. For end-to-end training, AS-Net construct a joint loss function that includes a task loss, a sampling loss, and a constraint loss. Extensive experiments on the ModelNet10/40 and ShapeNet datasets demonstrate that AS-Net achieves a good performance on the point cloud classification task. Especially when the downsampling size is small, the result is better than the referenced methods.}
}
@article{LIN2022103684,
title = {Infrared dim and small target detection based on U-Transformer},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103684},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103684},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002048},
author = {Jian Lin and Kai Zhang and Xi Yang and Xiangzheng Cheng and Chenhui Li},
keywords = {Infrared small and dim target detection, Swin transformer, Anchor free, Object detection, Heatmap},
abstract = {Infrared dim and small target detection is a key technology for space-based infrared search and tracking systems. Traditional detection methods have a high false alarm rate and fail to handle complex background and high-noise scenarios. Also, the methods cannot effectively detect targets on a small scale. In this paper, a U-Transformer method is proposed, and a transformer is introduced into the infrared dim and small target detection. First, a U-shaped network is constructed. In the encoder part, the self-attention mechanism is used for infrared dim and small target feature extraction, which helps to solve the problems of losing dim and small target features of deep networks. Meanwhile, by using the encoding and decoding structure, infrared dim and small target features are filtered from the complex background while the shallow features and semantic information of the target are retained. Experiments show that anchor-free and transformer have great potential for infrared dim and small target detection. On the datasets with a complex background, our method outperforms the state-of-the-art detectors and meets the real-time requirement. The code is publicly available at https://github.com/Linaom1214/U-Transformer.}
}
@article{MANSRI2023103724,
title = {Reference picture selection with decreased temporal dependency for HEVC error resilience},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103724},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103724},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002449},
author = {Islem Mansri and Nasreddine Kouadria and Noureddine Doghmane and Saliha Harize and Amara Bekhouch},
keywords = {HEVC/H.265, Error-resilience, Video transmission, Low-delay, Error-propagation, Error concealment},
abstract = {The high level of compression efficiency achieved by the High-Efficiency Video Coding standard (HEVC) decreases the robustness of the encoded bitstreams. This increased susceptibility to network errors leads to end video quality degradation. Moreover, due to the high computational complexity of HEVC, high-resolution video transmission with time constraints over hostile channels such as wireless networks becomes more challenging. This paper proposes a reference picture selection-based error-resilient method to reduce the temporal error propagation due to high-trip delay and frame-copy concealment error. First, the encoder selects the reference pictures based on the error status received from the feedback channel, taking into consideration the Rate-Distortion-Optimization (RDO). Second, the temporal information mismatch prediction resulting from the error concealment is reduced by decreasing the temporal dependency between adjacent frames based on new motion-estimation tools. Results show a PSNR gain of about 6.13 dB, 5.20 dB and 4.72 dB for 1080p, 720p and 480p resolutions respectively.}
}
@article{RASMUSSEN2022103655,
title = {Supporting workspace awareness in remote assistance through a flexible multi-camera system and Augmented Reality awareness cues},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103655},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103655},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001754},
author = {Troels Rasmussen and Tiare Feuchtner and Weidong Huang and Kaj Grønbæk},
keywords = {Remote assistance, Augmented reality, Workspace awareness, Awareness cues, Calibration},
abstract = {Workspace awareness is critical for remote assistance with physical tasks, yet it remains difficult to facilitate. For example, if the remote helper is limited to the single viewpoint provided by the worker’s hand-held or head-mounted camera, she lacks the ability to gain an overview of the workspace. This may be addressed by granting the helper view-independence, e.g., through a multi-camera system. However, it can be cumbersome to set up and calibrate multiple cameras, and it can be challenging for the local worker to identify the current viewpoint of the remote helper. We present CueCam, a multi-camera remote assistance system that supports mutual workspace awareness through a flexible ad-hoc camera calibration and various Augmented Reality cues that communicate the helper’s viewpoint and focus. In particular, we propose visual cues presented through a head-mounted Augmented Reality display (Virtual Hand, Color Cue), and sound cues emitted from the cameras’ physical locations (Spatial Sound). Findings from a lab study indicate that all proposed cues effectively support the worker’s awareness of helper’s location and focus, while the Color Cue demonstrated superiority in task performance and preference ratings during a search task.}
}
@article{WANG2022103679,
title = {Context-dependent emotion recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103679},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103679},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001997},
author = {Zili Wang and Lingjie Lao and Xiaoya Zhang and Yong Li and Tong Zhang and Zhen Cui},
keywords = {Context-based Emotion Recognition, Tubal Transformer, Hierarchical Fusion, Affective Computing},
abstract = {Most previous methods for emotion recognition focus on facial emotion and ignore the rich context information that implies important emotion states. To make full use of the contextual information to make up for the facial information, we propose the Context-Dependent Net (CD-Net) for robust context-aware human emotion recognition. Inspired by the long-range dependency of the transformer, we introduce the tubal transformer which forms the shared feature representation space to facilitate the interactions among the face, body, and context features. Besides, we introduce the hierarchical feature fusion to recombine the enhanced multi-scale face, body, and context features for emotion classification. Experimentally, we verify the effectiveness of the proposed CD-Net on the two large emotion datasets, CAER-S and EMOTIC. On the one hand, the quantitative evaluation results demonstrate the superiority of the proposed CD-Net over other state-of-the-art methods. On the other hand, the visualization results show CD-Net can capture the dependencies among the face, body, and context components and focus on the important features related to the emotion.}
}
@article{WANG2022103665,
title = {Bounding box regression with balance for harmonious object detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103665},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103665},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001857},
author = {Chenzhong Wang and Xun Gong},
keywords = {Object detection, Reinforcement learning, Bounding box regression},
abstract = {Localization is an essential part of object detection, which is usually accomplished by bounding box regression guided by ℓn-norm-based or IoU-based loss functions, where IoU is known for its scale-invariant characteristics. However, introducing the scale-invariance into regression loss in traditional IoU-based methods may result in a bias in favor of smaller boxes and cause redundancy and unstable oscillations. To make up for these shortages of IoU-based losses, we propose a Scale-Balanced Factor (SF) that stabilizes the regression process via a simple adaptive factor. Furthermore, to compensate for the imbalance of different types of losses caused by SF and other IoU-based loss functions, regression losses are always multiplied by a hyperparameter, which is purely empirical and is hard to find an optimum. To address this issue, a Multi-Task Reinforced Equilibrium (MRE) is proposed to dynamically tweak the learning rate of each task based on reinforcement learning. The MRE can guarantee more balanced parameters and maximize the benefit of SF or other improvement methods for IoU. By incorporating the proposed SF and MRE into the classic detectors (RetinaNet, YOLO, and Faster R-CNN, etc.), we have achieved significant performance gains on MS COCO (0.8 AP∼1.9 AP) and PASCAL VOC (0.6 AP∼2.2 AP).}
}
@article{YANG2023103710,
title = {Dynamic representation-based tracker for long-term pedestrian tracking with occlusion},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103710},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103710},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002309},
author = {Zhen Yang and Zhiyi Huang and Dunyun He and Tao Zhang and Fan Yang},
keywords = {Pedestrian tracking with occlusion, Dynamic representation-based tracker (DRT), Adaptive representation network (ARN), Pose supervised module (PSM)},
abstract = {This paper presents a dynamic representation-based tracker (DRT) to handle occlusions in the long-term pedestrian tracking of a single target. In our DRT, an adaptive representation network (ARN) is first constructed to extract multiple features, including classical features such as appearance and pose as well as some vector-format deep features. These features are then stacked to form a dynamic representation so as to convert the target tracking into a matching problem between the target features and candidate features, where the Euclidean distance (ED) and locality-constrained linear coding (LLC) are used as measurements in the decision-making. Next, the target state is determined through a voting procedure according to the feature matching error. Finally, a pose supervised module (PSM) and an IOU filtering module (IFM) are applied, respectively, to refine the target state and to filter out some invalid candidate targets that have been detected. Experimental results on public benchmark datasets show that our DRT is quite robust to complex environments with long-term pedestrian occlusions, and outperforms several existing state-of-the-arts trackers as it produces the best performance on both the pedestrian tracking dataset with occlusion (PTDO) and the pedestrian tracking dataset with occlusion plus (PTDO Plus).}
}
@article{LI2023103695,
title = {Single image deraining using multi-scales context information and attention network},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103695},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103695},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002152},
author = {Pengcheng Li and Shan Gai},
keywords = {Image deraining, Convolutional neural network, Attention mechanism, Depthwise separable convolution, Feature fusion},
abstract = {The existing deraining methods based on convolutional neural networks (CNNs) have made great success, but some remaining rain streaks can degrade images drastically. In this work, we proposed an end-to-end multi-scale context information and attention network, called MSCIANet. The proposed network consists of multi-scale feature extraction (MSFE) and multi-receptive fields feature extraction (MRFFE). Firstly, the MSFE can pick up features of rain streaks in different scales and propagate deep features of the two layers across stages by skip connections. Secondly, the MRFFE can refine details of the background by attention mechanism and the depthwise separable convolution of different receptive fields with different scales. Finally, the fusion of these outputs of two subnetworks can reconstruct the clean background image. Extensive experimental results have shown that the proposed network achieves a good effect on the deraining task on synthetic and real-world datasets. The demo can be available at https://github.com/CoderLi365/MSCIANet.}
}
@article{DENG2022103610,
title = {An enhanced image quality assessment by synergizing superpixels and visual saliency},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103610},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103610},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001341},
author = {Jiehang Deng and Haomin Chen and Zhongming Yuan and Guosheng Gu and Shihe Xu and Shaowei Weng and Hao Wang},
keywords = {Full reference, Image quality assessment, Visual saliency, Superpixel segmentation, Limitations, Complementary},
abstract = {Superpixel and saliency-based evaluation methods play important roles in full reference image quality assessment (FR IQA). However, we find that these methods have one complementary principle and three limitations: (1) the weighted maps of superpixel-based methods conflict with the perception of the human visual system; (2) saliency-based methods are inefficient in terms of the block distortion; (3) the general two-direction gradient extraction factor must be extended to be multidirectional. To address these limitations, we propose an enhanced image quality assessment by synergizing superpixels and visual saliency. Specifically, the calculation of a newly proposed framework involves three similarities and two strategies: the saliency, superpixel and multidirectional gradient similarities of the neighborhoods, and the saliency pooling strategy, the fusion strategy of these similarities. Theoretical analysis and experimental results show that the proposed method can effectively address the limitations noted above and outperform the existing methods.}
}
@article{ZHANG2023103711,
title = {Design of anchor boxes and data augmentation for transformer-based vehicle localization},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103711},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103711},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002310},
author = {Rui Zhang and Yan Tian and Zhaocheng Xu and Dongsheng Liu},
keywords = {Object localization, Intelligent transportation systems, Data augmentation, Machine learning, Computer vision},
abstract = {Vehicle localization is an important task in the signal processing field. In recent years, context exploration has been widely studied, especially the nonlocal dependencies in an image, using, for example, attention and transformer mechanisms. However, these approaches encounter difficulties in achieving accurate localization owing to ineffective design and use of queries. Motivated by the fact that spatial information is determined by decoder embeddings and details of reference boxes, we propose a method of explicitly and dynamically modeling anchor boxes in the query generation module. Moreover, we design a geometry-aware data augmentation approach to increase the diversity of the data by employing multiple augmentation methods on an image. Experiments conducted on public datasets show that our approach can improve the average precision by approximately 1.1%.}
}
@article{SI2022103638,
title = {A comprehensive benchmark analysis for sand dust image reconstruction},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103638},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103638},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001584},
author = {Yazhong Si and Fan Yang and Ya Guo and Wei Zhang and Yipu Yang},
keywords = {Sand dust image, Benchmark dataset, Image reconstruction, Comprehensive evaluation, Convolutional neural networks},
abstract = {Recently, numerous sand dust removal algorithms have been proposed. To our best knowledge, however, most methods evaluated their performance in a no-reference way using few selected real-world images from the internet. It is unclear how to quantitatively analyze the performance of the algorithms in a supervised way. Moreover, due to the absence of large-scale datasets, there are no well-known sand dust reconstruction report algorithms up till now. To bridge the gaps, we presented a comprehensive perceptual study and analysis of real-world sandstorm images, then constructed a Sand-dust Image Reconstruction Benchmark(SIRB) for training Convolutional Neural Networks(CNNs) and evaluating the algorithm’s performance. We adopted the existing image transformation neural network trained on SIRB as the baseline to illustrate the generalization of SIRB for training CNNs. Finally, we conducted a comprehensive evaluation to demonstrate the performance and limitations of the sandstorm enhancement algorithms, which shed light on future research in sandstorm image reconstruction.}
}
@article{ZHONG2023103719,
title = {SCPNet: Self-constrained parallelism network for keypoint-based lightweight object detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103719},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103719},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002395},
author = {Xian Zhong and Mengdie Wang and Wenxuan Liu and Jingling Yuan and Wenxin Huang},
keywords = {Keypoint-based lightweight object detection, Parallel multi-scale fusion, Parallel shuffle block, Self-constrained detection},
abstract = {Keypoint-based object detection achieves better performance without positioning calculations and extensive prediction. However, they have heavy backbone, and high-resolution is restored using upsampling that obtain unreliable features. We propose a self-constrained parallelism keypoint-based lightweight object detection network (SCPNet), which speeds inference, drops parameters, widens receptive fields, and makes prediction accurate. Specifically, the parallel multi-scale fusion module (PMFM) with parallel shuffle blocks (PSB) adopts parallel structure to obtain reliable features and reduce depth, adopts repeated multi-scale fusion to avoid too many parallel branches. The self-constrained detection module (SCDM) has a two-branch structure, with one branch predicting corners, and employing entad offset to match high-quality corner pairs, and the other branch predicting center keypoints. The distances between the paired corners’ geometric centers and the center keypoints are used for self-constrained detection. On MS-COCO 2017 and PASCAL VOC, SCPNet’s results are competitive with the state-of-the-art lightweight object detection. https://github.com/mengdie-wang/SCPNet.git.}
}
@article{KIM2022103654,
title = {Fine-grained neural architecture search for image super-resolution},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103654},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103654},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001742},
author = {Heewon Kim and Seokil Hong and Bohyung Han and Heesoo Myeong and Kyoung Mu Lee},
keywords = {Image super-resolution, Neural architecture search, Convolutional neural network},
abstract = {Designing efficient deep neural networks has achieved great interest in image super-resolution (SR). However, exploring diverse network structures is computationally expensive. More importantly, each layer in a network has a distinct role that leads to the design of a specialized structure. In this work, we present a novel neural architecture search (NAS) algorithm that efficiently explores layer-wise structures. Specifically, we construct a supernet allowing flexibility in choosing the number of channels and per-channel activation functions according to the role of each layer. The search process runs efficiently via channel pruning since gradient descent jointly optimizes the Mult-Adds and the accuracy of the searched models. We facilitate estimating the model Mult-Adds in a differentiable manner using relaxations in the backward pass. The searched model, named FGNAS, outperforms the state-of-the-art NAS-based SR methods by a large margin.}
}
@article{SHANG2023103683,
title = {Low complexity inter coding scheme for Versatile Video Coding (VVC)},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103683},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103683},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002036},
author = {Xiwu Shang and Guoping Li and Xiaoli Zhao and Yifan Zuo},
keywords = {Versatile Video Coding (VVC), Low complexity, Inter coding, CU size decision, Mode decision},
abstract = {The latest coding standard Versatile Video Coding (VVC) developed by the Joint Video Experts Team (JVET) and Video Coding Experts Group (VCEG) was finalized in 2020. By introducing several new coding techniques, VVC improves the compression efficiency by 50% compared with H.265/HEVC. However, the coding complexity increases dramatically, which obstructs it from real-time application. To tackle this issue, a fast inter coding algorithm utilizing coding information is proposed to speed up the coding process. First, by analyzing the coding areas of the neighboring CUs, we predict the coding area of the current CU to terminate unnecessary splitting modes. Then, the temporally optimal coding mode generated during the prediction process is further utilized to shrink the candidate modes to speed up the coding process. Finally, the distribution of neighboring prediction modes are exploited to measure the motion complexity of the current CU, based on which the unnecessary prediction modes can be skipped earlier. Experimental results demonstrate that the proposed method can reduce the coding complexity by 40.08% on average with 0.07 dB BDPSNR decrease and 1.56% BDBR increase, which outperforms the state-of-the-art approach.}
}
@article{CHEN2023103707,
title = {Multiscale spatial temporal attention graph convolution network for skeleton-based anomaly behavior detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103707},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103707},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002279},
author = {Xiaoyu Chen and Shichao Kan and Fanghui Zhang and Yigang Cen and Linna Zhang and Damin Zhang},
keywords = {Multiscale spatial temporal graph, Spatial attention graph convolution, Skeleton-based anomaly behavior detection},
abstract = {Anomaly behavior detection plays a significant role in emergencies such as robbery. Although a lot of works have been proposed to deal with this problem, the performance in real applications is still relatively low. Here, to detect abnormal human behavior in videos, we propose a multiscale spatial temporal attention graph convolution network (MSTA-GCN) to capture and cluster the features of the human skeleton. First, based on the human skeleton graph, a multiscale spatial temporal attention graph convolution block (MSTA-GCB) is built which contains multiscale graphs in temporal and spatial dimensions. MSTA-GCB can simulate the motion relations of human body components at different scales where each scale corresponds to different granularity of annotation levels on the human skeleton. Then, static, globally-learned and attention-based adjacency matrices in the graph convolution module are proposed to capture hierarchical representation. Finally, extensive experiments are carried out on the ShanghaiTech Campus and CUHK Avenue datasets, the final results of the frame-level AUC/EER are 0.759/0.311 and 0.876/0.192, respectively. Moreover, the frame-level AUC is 0.768 for the human-related ShanghaiTech subset. These results show that our MSTA-GCN outperforms most of methods in video anomaly detection and we have obtained a new state-of-the-art performance in skeleton-based anomaly behavior detection.}
}
@article{NARWAL2022103670,
title = {A comprehensive survey and mathematical insights towards video summarization},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103670},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103670},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001900},
author = {Pulkit Narwal and Neelam Duhan and Komal {Kumar Bhatia}},
keywords = {Video Summarization, Video Abstraction, Video Segmentation, Prior Knowledge, Dynamic Video Summary, Personalized Video Summary},
abstract = {Video Summarization is a technique to reduce the original raw video into a short video summary. Video summarization automates the task of acquiring key frames/segments from the video and combining them to generate a video summary. This paper provides a framework for summarization based on different criteria and also compares different literature work related to video summarization. The framework deals with formulating model for video summarization based on different criteria. Based on target audience/ viewership, number of videos, type of output intended, type of video summary and summarization factor; a model generating video summarization framework is proposed. The paper examines significant research works in the area of video summarization to present a comprehensive review against the framework. Different techniques, perspectives and modalities are considered to preserve the diversity of survey. This paper examines important mathematical formulations to provide meaningful insights for video summarization model creation.}
}
@article{CHEN2022103678,
title = {Meta-transfer-adjustment learning for few-shot learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103678},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103678},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001985},
author = {Yadang Chen and Hui Yan and Zhi-Xin Yang and Enhua Wu},
keywords = {Few-shot learning, Deep neural networks, Feature adjustment, Task adjustment},
abstract = {Deep neural network models with strong feature extraction capacity are prone to overfitting and fail to adapt quickly to new tasks with few samples. Gradient-based meta-learning approaches can minimize overfitting and adapt to new tasks fast, but they frequently use shallow neural networks with limited feature extraction capacity. We present a simple and effective approach called Meta-Transfer-Adjustment learning (MTA) in this paper, which enables deep neural networks with powerful feature extraction capabilities to be applied to few-shot scenarios while avoiding overfitting and gaining the capacity for quickly adapting to new tasks via training on numerous tasks. Our presented approach is classified into two major parts, the Feature Adjustment (FA) module, and the Task Adjustment (TA) module. The feature adjustment module (FA) helps the model to make better use of the deep network to improve feature extraction, while the task adjustment module (TA) is utilized for further improve the model’s fast response and generalization capabilities. The proposed model delivers good classification results on the benchmark small sample datasets MiniImageNet and Fewshot-CIFAR100, as proved experimentally.}
}
@article{XU2022103626,
title = {An unsupervised fusion network for boosting denoising performance},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103626},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103626},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001468},
author = {Shaoping Xu and Xiaojun Chen and Jie Luo and Xiaohui Cheng and Nan Xiao},
keywords = {Image denoising, Boosting denoising performance, Deep fusion network, Unsupervised training strategy},
abstract = {While many efforts have been devoted to addressing image denoising and achieve continuously improving results during the past few decades, it is fair to say that no a stand-alone method is consistently better than others. Nonetheless, many existing denoising methods, each having a different denoising capability, can yield various but complementary denoised images with respect to specific local areas. To effectively exploit the complementarity and diversity among the denoised images obtained with different denoisers, in this work we fuse them to produce an overall better result, which is fundamental to achieve robust and competitive denoising performance especially for complex scenes. A framework called deep fusion network (DFNet) is proposed to generate a consistent estimation about the final denoised image, taking advantage of the complementarity of denoisers and suppressing the bias. Specifically, given a noisy image, we first exploit a set of representative image denoisers to denoise it respectively, and obtain the corresponding initial denoised images. Then these initial denoised images are concatenated and fed into the proposed DFNet, and the proposed DFNet seeks to adjust its network parameters to produce the fused image (as the final denoised image) with an unsupervised training strategy through minimizing the carefully designed loss function. The experimental results show that our approach outperforms the stand-alone methods as well as the ones using combination strategy by large margin both in objective and subjective evaluations. Compared to the those methods that are relatively close to our strategy, the proposed DFNet is extensible and parameter free, which means it can cope with a variable number of different denoisers and avoid the manual intervention during the fusion process. The proposed DFNet has greater flexibility and better practicality.}
}
@article{XU2023103723,
title = {Image compressive sensing via hybrid regularization combining centralized group sparse representation and deep denoiser prior},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103723},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103723},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002437},
author = {Jin Xu and Zhizhong Fu},
keywords = {Image compressive sensing, Hybrid regularization, Centralized group sparse representation, Deep denoiser prior},
abstract = {To effectively solve the ill-posed image compressive sensing (CS) reconstruction problem, it is essential to properly exploit image prior knowledge. In this paper, we propose an efficient hybrid regularization approach for image CS reconstruction, which can simultaneously exploit both internal and external image priors in a unified framework. Specifically, a novel centralized group sparse representation (CGSR) model is designed to more effectively exploit internal image sparsity prior by suppressing the group sparse coding noise (GSCN), i.e., the difference between the group sparse coding coefficients of the observed image and those of the original image. Meanwhile, by taking advantage of the plug-and-play (PnP) image restoration framework, a state-of-the-art deep image denoiser is plugged into the optimization model of image CS reconstruction to implicitly exploit external deep denoiser prior. To make our hybrid internal and external image priors regularized image CS method (named as CGSR-D-CS) tractable and robust, an efficient algorithm based on the split Bregman iteration is developed to solve the optimization problem of CGSR-D-CS. Experimental results demonstrate that our CGSR-D-CS method outperforms some state-of-the-art image CS reconstruction methods (either model-based or deep learning-based methods) in terms of both objective quality and visual perception.}
}
@article{ZHU2022103675,
title = {Personality modeling from image aesthetic attribute-aware graph representation learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103675},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103675},
url = {https://www.sciencedirect.com/science/article/pii/S104732032200195X},
author = {Hancheng Zhu and Yong Zhou and Qiaoyue Li and Zhiwen Shao},
keywords = {Social media, Personality modeling, Image aesthetics, Attribute-aware graph, Convolutional neural network},
abstract = {Recently, inferring users’ personality traits on social media has attracted extensive attention. Existing studies have shown that users’ personality traits can be inferred from their preferences for images. However, since users’ preferences on images are often affected by multiple factors, some liked images cannot effectively reflect their personality traits. To handle this issue, this paper proposes a personality modeling approach based on image aesthetic attribute-aware graph representation learning, which can leverage aesthetic attributes to refine the liked images that are consistent with users’ personality traits. Specifically, we first utilize a Convolutional Neural Network (CNN) to train an aesthetic attribute prediction module. Then, attribute-aware graph representation learning is introduced to refine the images with similar aesthetic attributes from users’ liked images. Finally, the aesthetic attributes of all refined images are combined to predict personality traits through a Multi-Layer Perceptron (MLP). Experimental results and visual analysis have shown that the proposed method is superior to state-of-the-art personality modeling methods.}
}
@article{HUANG2023103694,
title = {Efficient image dehazing algorithm using multiple priors constraints},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103694},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103694},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002140},
author = {Zilong Huang and Hongyuan Jing and Aidong Chen and Chen Hong and Xinna Shang},
keywords = {Image dehazing, Atmospheric scattering model, Atmospheric light estimation, Multiple prior constraints},
abstract = {In this study, a robust and efficient image dehazing technique based on the atmospheric scattering model is proposed, which effectively overcomes the limitations of a single prior condition. It is composed of a transmission estimation module and an atmospheric light estimation module. The transmission estimation module integrates multiple dehazing prior strategies and effectively optimises transmission estimation and application range. The atmospheric light estimation module uses the fuzzy C-means clustering algorithm (FCM) to estimate the atmospheric light of different scenes in an image. Unlike in the previous work, the atmospheric light in this module is a nonglobal value, and a pixel-level atmospheric light value matrix is obtained. Numerous experiments show that the proposed dehazing algorithm is superior to state-of-the-art methods.}
}
@article{WANG2023103718,
title = {Multi-Scale and spatial position-based channel attention network for crowd counting},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103718},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103718},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002383},
author = {Lin Wang and Jie Li and Siqi Zhang and Chun Qi and Pan Wang and Fengping Wang},
keywords = {Crowd counting, Spatial position-based channel attention model, Multi-scale structure, Adaptive loss},
abstract = {Crowd counting algorithms have recently incorporated attention mechanisms into convolutional neural networks (CNNs) to achieve significant progress. The channel attention model (CAM), as a popular attention mechanism, calculates a set of probability weights to select important channel-wise feature responses. However, most CAMs roughly assign a weight to the entire channel-wise map, which makes useful and useless information being treat indiscriminately, thereby limiting the representational capacity of networks. In this paper, we propose a multi-scale and spatial position-based channel attention network (MS-SPCANet), which integrates spatial position-based channel attention models (SPCAMs) with multiple scales into a CNN. SPCAM assigns different channel attention weights to different positions of channel-wise maps to capture more informative features. Furthermore, an adaptive loss, which uses adaptive coefficients to combine density map loss and headcount loss, is constructed to improve network performance in sparse crowd scenes. Experimental results on four public datasets verify the superiority of the scheme.}
}
@article{ZHANG2022103637,
title = {Adaptive multi-histogram reversible data hiding with contrast enhancement},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103637},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103637},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001572},
author = {Tiancong Zhang and Caijie Yang and Shaowei Weng and Tanshuai Hou},
keywords = {Contrast enhancement, FCM clustering, Multiple histograms, Reversible data hiding, F Genetic algorithm},
abstract = {Unlike existing reversible data hiding with contrast enhancement (RDHCE) methods, which excessively improve the image contrast for achieving the required capacity, the proposed method improves the image contrast appropriately while providing satisfactory embedding capacity. To this end, an adaptive multi-histogram RDHCE method is proposed in this study to improve the local and global contrast by considering the local properties of the histograms. On the one hand, fuzzy C-means clustering combining multiple features that are deliberately designed for contrast enhancement is employed to generate seven sharply-distributed prediction error histograms (PEHs). Subsequently, the genetic algorithm is utilized to adaptively select the optimal pairs achieving the best embedding performance for each PEH according to the local characteristics of PEH distribution, resulting in improving the local contrast adaptively and embedding significant amount of data. Additionally, two-sided histogram shifting (HS) is utilized to improve the global contrast appropriately while embedding reasonable amount of data. The experimental results demonstrate that the proposed method achieves better local and global contrast while providing a high embedding capacity compared with other existing RDHCE methods.}
}
@article{AZADEGAN2023103734,
title = {Improving video quality by predicting inter-frame residuals based on an additive 3D-CNN model},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103734},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103734},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002541},
author = {Hamid Azadegan and Ali-Asghar Beheshti Shirazi},
keywords = {Quality improvement, Compression error, Video compression, Deep learning, Inter prediction},
abstract = {Video compression is essential for uploading videos to online platforms which usually have bandwidth limitations. However, the compression reduces the visual quality. To overcome this problem, the visual quality of the low bitrate compressed videos for various standards, including H.264 and HEVC in decoders, needs to be improved. Accordingly, this paper proposes a novel method for improving video quality based on 3D convolutional neural networks (CNNs). This method is totally compatible with the encoders of video compression standards, i.e., H.264, VVC, and HEVC, and can be implemented easily. In particular, the proposed neural network model receives five frames of the low bitrate compressed video as input and subsequently predicts the compression error of frames using the first and fifth frames. Finally, it reconstructs an improved version of the frame with high quality. The CNN is an Additive (3D) model that can predict the eliminated inter-frame redundancies resulting from compression. Our goal is to increase the peak signal to noise ratio (PSNR) and structural index similarity (SSIM) of the luminance (Y) and chrominance (U, V) frames in the video. Additive 3D-CNN achieves an average of 12.4%, 9.9% and 5% BD-rate increases for LP, LB and RA for the Y component. The results indicate that the new proposed algorithm outperforms the previous methods in terms of PSNR, SSIM, and BD-rate.}
}
@article{XUE2022103682,
title = {Learn decision trees with deep visual primitives},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103682},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103682},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002024},
author = {Mengqi Xue and Haofei Zhang and Qihan Huang and Jie Song and Mingli Song},
keywords = {Interpretability, Deep neural network, Discrete representation learning},
abstract = {In this paper, we strive to propose a self-interpretable framework, termed PrimitiveTree, that incorporates deep visual primitives condensed from deep features with a conventional decision tree, bridging the gap between deep features extracted from deep neural networks (DNNs) and trees’ transparent decision-making processes. Specifically, we utilize a codebook, which embeds the continuous deep features into a finite discrete space (deep visual primitives) to distill the most common semantic information. The decision tree adopts the spatial location information and the mapped primitives to present the decision-making process of the deep features in a tree hierarchy. Moreover, the trained interpretable PrimitiveTree can inversely explain the constituents of the deep features, highlighting the most critical and semantic-rich image patches attributing to the final predictions of the given DNN. Extensive experiments and visualization results validate the effectiveness and interpretability of our method.}
}
@article{ZHANG2022103677,
title = {Isomorphic model-based initialization for convolutional neural networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103677},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103677},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001973},
author = {Hong Zhang and Yang Li and Hanqing Yang and Bin He and Yu Zhang},
keywords = {Convolutional neural networks, Weight initialization, Isomorphic model, Structural weight transformation},
abstract = {Modern deep convolutional neural networks(CNNs) are often designed to be scalable, leading to the model family concept. A model family is a large (possibly infinite) collection of related neural network architectures. The isomorphism of a model family refers to the fact that the models within it share the same high-level structure. Meanwhile, the models within the model family are called isomorphic models for each other. Existing weight initialization methods for CNNs use random initialization or data-driven initialization. Even though these methods can perform satisfactory initialization, the isomorphism of model families is rarely explored. This work proposes an isomorphic model-based initialization method (IM Init) for CNNs. It can initialize any network with another well-trained isomorphic model in the same model family. We first formulate the widely used general network structure of CNNs. Then a structural weight transformation is presented to transform the weight between two isomorphic models. Finally, we apply our IM Init to the model down-sampling and up-sampling scenarios and confirm its effectiveness in improving accuracy and convergence speed through experiments on various image classification datasets. In the model down-sampling scenario, IM Init initializes the smaller target model with a larger well-trained source model. It improves the accuracy of RegNet200MF by 1.59% on the CIFAR-100 dataset and 1.9% on the CUB200 dataset. Inversely, IM Init initializes the larger target model with a smaller well-trained source model in the model up-sampling scenario. It significantly speeds up the convergence of RegNet600MF and improves the accuracy by 30.10% under short training schedules. Code will be available.}
}
@article{LI2022103625,
title = {Skeleton-based deep pose feature learning for action quality assessment on figure skating videos},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103625},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103625},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001456},
author = {Huiying Li and Qing Lei and Hongbo Zhang and Jixiang Du and Shangce Gao},
keywords = {Action quality assessment, Figure skating sport videos, Spatial–temporal pose feature extraction, Action relation learning},
abstract = {Most of the existing Action Quality Assessment (AQA) methods for scoring sports videos have deeply researched how to evaluate the single action or several sequential-defined actions that performed in short-term sport videos, such as diving, vault, etc. They attempted to extract features directly from RGB videos through 3D ConvNets, which makes the features mixed with ambiguous scene information. To investigate the effectiveness of deep pose feature learning on automatically evaluating the complicated activities in long-duration sports videos, such as figure skating and artistic gymnastic, we propose a skeleton-based deep pose feature learning method to address this problem. For pose feature extraction, a spatial–temporal pose extraction module (STPE) is built to capture the subtle changes of human body movements and obtain the detail representations for skeletal data in space and time dimensions. For temporal information representation, an inter-action temporal relation extraction module (ATRE) is implemented by recurrent neural network to model the dynamic temporal structure of skeletal subsequences. We evaluate the proposed method on figure skating activity of MIT-skate and FIS-V datasets. The experimental results show that the proposed method is more effective than RGB video-based deep feature learning methods, including SENet and C3D. Significant performance progress has been achieved for the Spearman Rank Correlation (SRC) on MIT-Skate dataset. On FIS-V dataset, for the Total Element Score (TES) and the Program Component Score (PCS), better SRC and MSE have been achieved between the predicted scores against the judge’s ones when compared with SENet and C3D feature methods.}
}
@article{SU2023103706,
title = {Attention-adaptive multi-scale feature aggregation dehazing network},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103706},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103706},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002267},
author = {Zhuo Su and Ruizhi Liu and Yuxin Feng and Fan Zhou},
keywords = {Single image dehazing, Feature attention, Residual aggregation},
abstract = {In this paper, we propose an end-to-end Attention-adaptive Multi-scale Feature Aggregation Dehazing Network (AMA-Net). The AMA-Net is based on U-Net and designs with three attention-driven modules, Joint Attention Residual Block (JAB), Joint Attention Feature Aggregation Group (JAAG), and Layer Adaptive Attention Feature Aggregation Module (LAA). To be more specific, considering the unevenly distributed haze in images, we introduce the JAB, which adaptively assigns weights to make networks pay attention to important features; to fully utilize the residual features, we propose the residual aggregation (via three JABs) in JAAG; since most feature aggregation methods for dehazing networks do not filter and refine features at different layers, we add LAA to the decoder to weight the features at different layers for aggregation. Through the ablation studies, we verify the effectiveness of the JAB, JAAG, and LAA. Experimental results on synthetic and real-world datasets show that the proposed AMA-Net outperforms relevant state-of-the-art methods.}
}
@article{LI2022103641,
title = {VirtualActionNet: A strong two-stream point cloud sequence network for human action recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103641},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103641},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001614},
author = {Xing Li and Qian Huang and Zhijian Wang and Tianjin Yang},
keywords = {Two-stream network, 3D action recognition, Point cloud sequence},
abstract = {In this paper, we propose a strong two-stream point cloud sequence network VirtualActionNet for 3D human action recognition. In the data preprocessing stage, we transform the depth sequence into a point cloud sequence as the input of our VirtualActionNet. In order to encode intra-frame appearance structures, static point cloud technologies are first employed as a virtual action generation sequence module to abstract the point cloud sequence into a virtual action sequence. Then, a two-stream network framework is presented to model the virtual action sequence. Specifically, we design an appearance stream module for aggregating all the appearance information preserved in each virtual action frame. Moreover, a motion stream module is introduced to capture dynamic changes along the time dimension. Finally, a joint loss strategy is adopted during data training to improve the action prediction accuracy of the two-stream network. Extensive experiments on three publicly available datasets demonstrate the effectiveness of the proposed VirtualActionNet.}
}
@article{PARIHAR2023103722,
title = {Densely connected convolutional transformer for single image dehazing},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103722},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103722},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002425},
author = {Anil Singh Parihar and Abhinav Java},
keywords = {Image Dehazing, Transformers, Attention},
abstract = {Image Dehazing is an important low-level vision task that aims to remove the haze from an image. In this paper, we proposed Densely Connected Convolutional Transformer (DCCT) for single image dehazing. DCCT is an efficient architecture that combines the multi-head Performer with the local dependencies. To prevent loss of information between features at different levels, we propose a learnable connection layer that is used to fuse features at different levels across the entire architecture. We guide the training of DCCT through a joint loss considering a supervised metric learning approach that allows us to consider both negative and positive features for a multi-image perceptual loss. We validate the design choices and the effectiveness of the proposed DCCT through ablation studies. Through comparison with the representative techniques, we establish that the proposed DCCT is highly competitive with the state of the art.}
}
@article{WENG2023103732,
title = {Adaptive smoothness evaluation and multiple asymmetric histogram modification for reversible data hiding},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103732},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103732},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002528},
author = {Shaowei Weng and Tanshuai Hou and Tiancong Zhang and Jeng-Shyang Pan},
keywords = {Reversible data hiding (RDH), FCM, Multiple features, Asymmetric predictor, IDPSO},
abstract = {In this paper, an adaptive reversible data hiding (RDH) algorithm based on multiple asymmetric histograms is proposed by making full use of the image content. Different from existing multiple prediction error histogram (PEHs) modification methods that directly cluster all the pixels of a cover image into multiple categories, we firstly utilize a smoothness threshold to exclude as many pixels in complex regions as possible for reducing unnecessary pixel shifting, and then exploit fuzzy C-means with multiple deliberately-designed features to construct multiple sharply-distributed categories, which helps in increasing the subsequent embedding performance. Two asymmetric PEHs for each class are generated using a pair of asymmetric predictors, and the short part of each asymmetric PEH is modified to reduce the number of invalid modifications. The improved discrete particle swarm optimization is used to adaptively select the best bin while reducing computational complexity. The experimental results show that the proposed method outperforms several state-of-the-art RDH methods.}
}
@article{GUO2023103717,
title = {Multi-stage feature-fusion dense network for motion deblurring},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103717},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103717},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002371},
author = {Cai Guo and Qian Wang and Hong-Ning Dai and Ping Li},
keywords = {Motion deblurring, Multi-stage network, Feature-fusion dense connections, Channel-based multi-layer perceptrons},
abstract = {Although convolutional neural networks (CNNs) have recently shown considerable progress in motion deblurring, most existing methods that adopt multi-scale input schemes are still challenging in accurately restoring the heavily-blurred regions in blurry images. Several recent methods aim to further improve the deblurring effect using larger and more complex models, but these methods inevitably result in huge computing costs. To address the performance-complexity trade-off, we propose a multi-stage feature-fusion dense network (MFFDNet) for motion deblurring. Each sub-network of our MFFDNet has the similar structure and the same scale of input. Meanwhile, we propose a feature-fusion dense connection structure to reuse the extracted features, thereby improving the deblurring effect. Moreover, instead of using the multi-scale loss function, we only calculate the loss function at the output of the last stage since the input scale of our sub-network is invariant. Experimental results show that MFFDNet maintains a relatively small computing cost while outperforming state-of-the-art motion-deblurring methods. The source code is publicly available at: https://github.com/CaiGuoHS/MFFDNet_release.}
}
@article{LI2022103689,
title = {HCFN: Hierarchical cross-modal shared feature network for visible-infrared person re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103689},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103689},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002097},
author = {Yueying Li and Huaxiang Zhang and Li Liu},
keywords = {Visible-infrared person re-identification, Feature extraction, Deep learning, Hierarchical attention mechanism},
abstract = {Compared with traditional visible–visible person re-identification, the modality discrepancy between visible and infrared images makes person re-identification more challenging. Existing methods rely on learning efficient transformation mechanisms in paired images to reduce the modality gap, which inevitably introduces noise. To get rid of these limitations, we propose a Hierarchical Cross-modal shared Feature Network (HCFN) to mine modality-shared and modality-specific information. Since infrared images lack color and other information, we construct an Intra-modal Feature Extraction Module (IFEM) to learn the content information and reduce the difference between visible and infrared images. In order to reduce the heterogeneous division, we apply a Cross-modal Graph Interaction Module (CGIM) to align and narrow the set-level distance of the inter-modal images. By jointly learning two modules, our method can achieve 66.44% Rank-1 on SYSU-MM01 dataset and 74.81% Rank-1 on RegDB datasets, respectively, which is superior compared with the state-of-the-art methods. In addition, ablation experiments demonstrate that HCFN is at least 4.9% better than the baseline network.}
}
@article{ZHU2022103681,
title = {Multi-scale gradient attention guidance and adaptive style fusion for image inpainting},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103681},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103681},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002012},
author = {Ye Zhu and Chao Wang and Shuze Geng and Yang Yu and Xiaoke Hao},
keywords = {Image inpainting, Style transfer, Gradient attention, Multi-scale gradient loss},
abstract = {Image inpainting aims to fill in the missing regions of damaged images with plausible content. Existing inpainting methods tend to produce ambiguous artifacts and implausible structures. To address the above issues, our method aims to fully utilize the information of known regions to provide style and structural guidance for missing regions. Specifically, the Adaptive Style Fusion (ASF) module reduces artifacts by transferring visual style features from known regions to missing regions. The Gradient Attention Guidance (GAG) module generates accurate structures by aggregating semantic information along gradient boundary regions. In addition, the Multi-scale Attentional Feature Extraction (MAFE) module extracts global contextual information and enhances the representation of image features. The sufficient experimental results on the three datasets demonstrate that our proposed method has superior performance in terms of visual plausibility and structural consistency compared to state-of-the-art inpainting methods.}
}
@article{ZHANG2023103705,
title = {Object semantic-guided graph attention feature fusion network for Siamese visual tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103705},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103705},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002255},
author = {Jianwei Zhang and Mengen Miao and Huanlong Zhang and Jingchao Wang and Yanchun Zhao and Zhiwu Chen and Jianwei Qiao},
keywords = {Visual tracking, Siamese network, Semantic-guided, Graph attention},
abstract = {The similarity matching between the template and the search area plays a key role in Siamese-based trackers. Most Siamese-based trackers adopt correlation operation to perform feature fusion on the template branch and search branch for similarity matching. However, the correlation operation directly uses the template feature to slide the window on the search area feature without distinguishing the discriminant part of the target and the background noise, which blurs the spatial information of the response feature. To address this issue, this work proposes a novel object semantic-guided graph attention feature fusion network that both removes background information and focuses on the discriminative part of the object. The proposed network effectively removes background noise by utilizing an adaptive template instead of the fixed-size template used by the correlation operation. The network also models the contextual semantic relations of the target and uses the resulting semantic relations to guide the feature fusion process in a part-based manner, thereby accurately highlighting the discriminative parts of the target. Therefore, the problem of blurring response feature caused by correlation operation is effectively resolved. Furthermore, we propose an object-aware prediction network to learn object-aware features for classification and regression task, which effectively improves the discriminative ability of the prediction network. Experiments on many challenging benchmarks like OTB-100, LaSOT, TColor-128, GOT-10k and VOT2019, show that our methods achieves excellent performance.}
}
@article{SHAMSUDDIN2022103636,
title = {From synthetic to natural — single natural image dehazing deep networks using synthetic dataset domain randomization},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103636},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103636},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001560},
author = {Abdul Fathaah Shamsuddin and Krupasankari Ragunathan and Abhijith P. and Deepak {Raja Sekar P.M.} and Praveen Sankaran},
keywords = {Dehazing, Deep learning, Domain randomization, Dataset},
abstract = {Image dehazing methods aim to solve the problem of poor visibility in images due to haze. Techniques proposed for image dehazing in literature focus on image priors, haze lines or data driven statistical models. Variations of the classical methods relying on prior model or haze line model use no-reference image quality metrics to prove their dehazing performance. Recently developed deep learning models rely on huge amounts of hazy, haze-free pairs for training, and uses PSNR and SSIM like image reconstruction metrics to show their performance. These methods perform poorly on no-reference image quality assessments and also dehazes poorly at the depths of the image. These methods though can be optimized for memory usage and are faster. This work presents a deep learning model (Feature Fusion Attention Network) trained on a domain randomized synthetic dataset generated in simulation. The proposed model achieves the highest scores on blind image assessments through the gradient rationing technique for a deep learning-based approach by a significant margin. The images were evaluated on full-reference metrics as well and obtained favorable results. This approach also yields one of the highest edge sharpness obtained after dehazing. The training procedure adopted to obtain significant gains on real-world dehazing, without using any real-world data is also detailed in this paper.}
}
@article{LI2023103716,
title = {Spatial and temporal information fusion for human action recognition via Center Boundary Balancing Multimodal Classifier},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103716},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103716},
url = {https://www.sciencedirect.com/science/article/pii/S104732032200236X},
author = {Xing Li and Qian Huang and Zhijian Wang},
keywords = {Human action recognition, Gaussian pyramid depth motion images, Depth temporal maps, Center Boundary Balancing Multimodal Classifier},
abstract = {This paper proposes a novel multimodal data classifier named Center Boundary Balancing Multimodal Classifier (CBBMC) to fuse and classify the spatial and temporal descriptors for recognizing human actions from depth video sequences. CBBMC is a composite algorithm integrating feature fusion and feature classification, in which Center Boundary Balancing Projection (CBBP) is used to balance the center and boundary information of feature class spaces. In order to solve the problem of multimodal information redundancy and isolation, two feature selection and fusion schemes of CBBMC based on embedded feature selection are presented. Moreover, two new action descriptors called Gaussian Pyramid Depth Motion Images (GP-DMI) and Depth Temporal Maps (DTM) are introduced to capture the multi-scale spatial and fine-grained temporal information of human activities. Finally, we present an effective spatial and temporal information fusion framework based on CBBMC for human action recognition. In order to evaluate the performance of the proposed approach, extensive experiments are conducted. The proposed method achieved impressive results on four benchmark datasets, namely MSR Action3D (96.33%), UTD-MHAD (94.41%), DHA (95.65%), and NTU RGB+D (83.31% cross-subject and 87.66% cross-view), even though only the depth modality was used. The experimental results demonstrate the effectiveness of our method.}
}
@article{VISHWAKARMA2022103676,
title = {No-Reference Video Quality Assessment using novel hybrid features and two-stage hybrid regression for score level fusion},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103676},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103676},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001961},
author = {Anish Kumar Vishwakarma and Kishor M. Bhurchandi},
keywords = {Human visual system, No-reference video quality assessment, 3D steerable DWT, Perceptual features, User generated content, Support vector regression},
abstract = {This paper presents a novel No-Reference Video Quality Assessment (NR-VQA) model that utilizes proposed 3D steerable wavelet transform-based Natural Video Statistics (NVS) features as well as human perceptual features. Additionally, we proposed a novel two-stage regression scheme that significantly improves the overall performance of quality estimation. In the first stage, transform-based NVS and human perceptual features are separately passed through the proposed hybrid regression scheme: Support Vector Regression (SVR) followed by Polynomial curve fitting. The two visual quality scores predicted from the first stage are then used as features for the similar second stage. This predicts the final quality scores of distorted videos by achieving score level fusion. Extensive experiments were conducted using five authentic and four synthetic distortion databases. Experimental results demonstrate that the proposed method outperforms other published state-of-the-art benchmark methods on synthetic distortion databases and is among the top performers on authentic distortion databases. The source code is available at https://github.com/anishVNIT/two-stage-vqa.}
}
@article{ARORA2022103624,
title = {Anti-phishing technique based on dynamic image captcha using multi secret sharing scheme},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103624},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103624},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001444},
author = {Akanksha Arora and Hitendra Garg and Shivendra Shivani},
keywords = {Phishing, Dynamic image CPTCHA, Visual cryptography, Multi secret sharing},
abstract = {With the cutting-edge improvement of web, online abuses have been increasing rapidly. Phishing is the most widely recognized abuses performed by digital crooks nowadays. It is an activity to steal private data (for example, client names, passwords and Visa data) in an electronic correspondence. It is a sort of fraud with the end goal of monetary benefit and other fake exercises. It utilizes phony websites that resemble genuine ones. Phishing messages might contain links to sites that are contaminated with malware. In this paper, “an anti-phishing approach using multi secret sharing scheme” is implemented as an answer to this problem. Here, Dynamic Image CAPTCHA based verification using multi secret sharing is performed. Image CAPTCHA is divided into two pieces called shares. Multiple secret pictures are revealed by overlapping the same set of shares at different angles. In the proposed approach, shares are of different modes i.e., user’s share is imprinted on a physical transparency while server’s share is in digital mode. By using the proposed approach, websites and end clients can cross confirm their identity.}
}
@article{LV2023103721,
title = {SRI-Net: Similarity retrieval-based inference network for light field salient object detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103721},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103721},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002413},
author = {Chengtao Lv and Xiaofei Zhou and Bin Zhu and Deyang Liu and Bolun Zheng and Jiyong Zhang and Chenggang Yan},
keywords = {Light field, Focal slice retrieval, Similarity retrieval, Salient object detection},
abstract = {The cutting-edge RGB saliency models are prone to fail for some complex scenes, while RGB-D saliency models are often affected by inaccurate depth maps. Fortunately, light field images can provide a sufficient spatial layout depiction of 3D scenes. Therefore, this paper focuses on salient object detection of light field images, where a Similarity Retrieval-based Inference Network (SRI-Net) is proposed. Due to various focus points, not all focal slices extracted from light field images are beneficial for salient object detection, thus, the key point of our model lies in that we attempt to select the most valuable focal slice, which can contribute more complementary information for the RGB image. Specifically, firstly, we design a focal slice retrieval module (FSRM) to choose an appropriate focal slice by measuring the foreground similarity between the focal slice and RGB image. Secondly, in order to combine the original RGB image and the selected focal slice, we design a U-shaped saliency inference module (SIM), where the two-stream encoder is used to extract multi-level features, and the decoder is employed to aggregate multi-level deep features. Extensive experiments are conducted on two widely used light field datasets, and the results firmly demonstrate the superiority and effectiveness of the proposed SRI-Net.}
}
@article{LI2023103729,
title = {Denoising image by matrix factorization in U-shaped convolutional neural network},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103729},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103729},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002498},
author = {Qifan li},
keywords = {Image denoising, Matrix factorization, Convolution network, Image restoration},
abstract = {Image denoising requires both spatial details and global contextualized information to recover a clean version from the deteriorative one. Previous deep convolution networks usually focus on modeling the local feature and stacked convolution blocks to expand the receptive field, which can catch the long-distance dependencies. However, contrary to the expectation, the extracted local feature incapacity recovers the global details by traditional convolution while the stacked blocks hinder the information flow. To tackle these issues, we introduce the Matrix Factorization Denoising Module (MD) to model the interrelationship between the global context aggregating process and the reconstructed process to attain the context details. Besides, we redesign a new basic block to ease the information flow and maintain the network performance. In addition, we conceive the Feature Fusion Module (FFU) to fuse the information from the different sources. Inspired by the multi-stage progressive restoration architecture, we adopt two-stage convolution branches progressively reconstructing the denoised image. In this paper, we propose an original and efficient neural convolution network dubbed MFU. Experimental results on various image denoising datasets: SIDD, DND, and synthetic Gaussian noise datasets show that our MFU can produce comparable visual quality and accuracy results with state-of-the-art methods.}
}
@article{LI2022103640,
title = {Language-guided graph parsing attention network for human-object interaction recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103640},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103640},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001602},
author = {Qiyue Li and Xuemei Xie and Jin Zhang and Guangming Shi},
keywords = {Human-object interaction, Language-guided, Graph parsing attention network, Word embedding},
abstract = {This paper focuses on the task of human-object interaction (HOI) recognition, which aims to classify the interaction between human and objects. It is a challenging task partially due to the extremely imbalanced data among classes. To solve this problem, we propose a language-guided graph parsing attention network (LG-GPAN) that makes use of the word distribution in language to guide the classification in vision. We first associate each HOI class name with a word embedding vector in language and then all the vectors can construct a language space specified for HOI recognition. Simultaneously, the visual feature is extracted from the inputs via the proposed graph parsing attention network (GPAN) for better visual representation. The visual feature is then transformed into the linguistic one in language space. Finally, the output score is obtained via measuring the distance between the linguistic feature and the word embedding of classes in language space. Experimental results on the popular CAD-120 and V-COCO datasets validate our design choice and demonstrate its superior performance in comparison to the state-of-the-art.}
}
@article{JIANG2022103664,
title = {The encoding method of position embeddings in vision transformer},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103664},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103664},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001845},
author = {Kai Jiang and Peng Peng and Youzao Lian and Weisheng Xu},
keywords = {Vision transformer, Position embeddings, Gabor filters},
abstract = {In contrast to Convolutional Neural Networks (CNNs), Vision Transformers (ViT) cannot capture sequence ordering of input tokens and require position embeddings. As a learnable fixed-dimension vector, the position embedding improves accuracy while limiting the migration of the model between different input sizes. Hence, this paper conducts an empirical study on position embeddings of pre-trained models, which mainly focuses on two questions: (1) What do the position embeddings learn from training? (2) How do the position embeddings affect the self-attention modules? This paper analyzes the pattern of position embedding in pre-trained models and finds that the linear combination of Gabor filters and edge markers can fit the learned position embeddings well. The Gabor filters and edge markers can occupy some channels to append the position information, and the edge markers have flowed to values in self-attention modules. The experimental results can guide future work to choose suitable position embeddings.}
}
@article{CAO2023103693,
title = {Hyperspectral image classification based on three-dimensional adaptive sampling and improved iterative shrinkage-threshold algorithm},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103693},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103693},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002139},
author = {Chunhong Cao and Hongxuan Duan and Xieping Gao},
keywords = {Feature learning, Discriminative feature, Three-dimensional adaptive sampling, Hyperspectral images classification},
abstract = {Abundant spectral information of hyperspectral images (HSI) provides rich information for HSI classification, which often brings high dimensional data resulting in the dilemma between the demand for fine data and the limited resources such as computation, storage as well as transmission band-width. To address this issue, we propose a deep hierarchical feature representation model based on three-dimensional adaptive sampling and improved iterative shrinkage-threshold algorithm (ISTA) for HSI classification. Due to the adaptive sampling, we improve ISTA with deep learning network for spectral–spatial feature representation since the ISTA is no longer applicable for the sampled data reconstruction. Through end-to-end joint learning, the proposed method can not only effectively reduce the required data, but also learn discriminative features for HSI classification, which will be meaningful for the HSI’s transmission from the space satellites and fast classification. Experimental results demonstrate the effectiveness and superiority of the proposed method on three public HSI datasets.}
}
@article{TANG2022103674,
title = {Multi-level mutual supervision for cross-domain Person Re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103674},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103674},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001948},
author = {Chunren Tang and Dingyu Xue and Dongyue Chen},
keywords = {Unsupervised domain adaptation, Mutual supervision, Person Re-identification, Cross domain},
abstract = {The challenges of cross-domain person re-identification mainly derive from two aspects: (1) The missing of target data labels. (2) The bias between source domain and target domain. Most of existing works focus on only one problem in the above two or deal with them separately. In this paper, we propose a new approach referred as to multi-level mutual supervision to achieve full utilization of labeled source data and unlabeled target data. Along this approach, we construct a dual-branch framework of which the upper branch is trained with original source data and target data while the lower branch is trained with augmented source data and target data. By applying common-pseudo-label and Maximum Mean Discrepancy (MMD) loss in our framework, the mutual supervision in multi levels is achieved. The results show that our model achieves SOTA performance on multiple popular benchmark datasets.}
}
@article{AYDIN2022103661,
title = {A new Copy-Move forgery detection method using LIOP},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103661},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103661},
url = {https://www.sciencedirect.com/science/article/pii/S104732032200181X},
author = {Yıldız Aydın},
keywords = {Copy-move forgery, LIOP, YCbCr, Keypoint, Image processing},
abstract = {The most prevalent type of digital image falsification occurs when a portion of a image is copied and pasted onto another section of the same image. Falsification of the image made in this way is called copy-move forgery (CMF). This study presents a new and effective approach for copy-move forgery detection (CMFD) using the Local Intensity Order Pattern (LIOP) to overcome the restrictions of existing CMFD techniques. The input image is first converted to a YCbCr color space and then split into Y, Cb, and Cr color channels. The LIOP features are then extracted from each color channel and all the features are combined. The feature vectors are ordered lexicographically and related features are detected by comparing the LIOP features. Although the LIOP feature has rarely been used in CMFD prior to this study, the success rate of the proposed method is high. In addition, since the channels are not correlated to each other in the YCbCr color space, each color channel is considered as a gray image, and the success rate is increased by combining the features extracted from each of the color channels. The proposed approach was assessed using the CoMoFoD and GRIP datasets. Experimental findings demonstrated that the suggested method was successful and displayed robustness in post-processing attacks.}
}
@article{ZHOU2023103688,
title = {An improved all-optical diffractive deep neural network with less parameters for gesture recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103688},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103688},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002085},
author = {Yuanguo Zhou and Shan Shui and Yijun Cai and Chengying Chen and Yingshi Chen and Reza Abdi-Ghaleh},
keywords = {Deep learning, Diffractive deep neural network, Gesture recognition},
abstract = {As a framework of optical machine learning, all-optical diffractive neural network (D2NN) has delivered an ideal outcome of feature detection and target classification, currently raising high interest in the optics and photonics community. In this paper, we applied an improved D2NN architecture to the field of gesture recognition, which features more complicated contour than the common MNIST handwriting recognition in the previous literature. The proposed network structure incorporates the wavelet-like phase modulation pattern technique and the highway network on the basis of all-optical neural network. Through modulating the phase of incident light, the wavelet-like pattern can substantially reduce the parameters in the network layer. In addition, a highway network is employed to address the vanishing gradient phenomenon in the training process. In the experiment, we numerically achieved blind testing accuracy of 95.6% for identifying ten different gestures, and the number of parameters is only 3% of the regular D2NN. Reliability test and analysis show that the proposed method is a high-efficiency solution with low-parameters expecting for implementation of various machine learning tasks.}
}
@article{SHAO2022103615,
title = {PTR-CNN for in-loop filtering in video coding},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103615},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103615},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001390},
author = {Tong Shao and Tianqi Liu and Dapeng Wu and Chia-Yang Tsai and Zhijun Lei and Ioannis Katsavounidis},
keywords = {Video coding, Blocking artifact, In-loop filter, Convolutional neural network, Image quality assessment},
abstract = {A deep learning method called PTR-CNN (Predicted frame with Transform unit partition and prediction Residual aided CNN) is proposed for in-loop filtering in video compression. To reduce the computational complexity of an end-to-end CNN in-loop filter, a non-learning method of reference frame selection is designed to select the highest quality frame based on the frame’s blurriness and smoothiness scores. The transform unit (TU) partition and the prediction residual (PR) of the current frame are used as extra inputs to the neural network as the filtering guidance. The selected similar and high quality reference frame (RF) and the current unfiltered frame (CUF) are input to a CNN based motion compensation module to generate a predicted frame (PF). Finally input the PF, the CUF, the CUF’s TU partition and the CUF’s PR into the main CNN to reconstruct the filtered frame. The model is implemented in Tensorflow and tested in HEVC and AV1. Experimental results show that the complexity of proposed PTR-CNN is less than SOTA CNN-based reference aided in-loop filtering methods and slightly outperforms their RD performance. The scheme introduces a complexity overhead of 7% on the encoder. In particular, for random access, the proposed model achieves 11.78% coding gain over HEVC with DBF/SAO off, while has a gain of 4.76% over HEVC with DBF/SAO on. Ablation study demonstrates that the RF contributes about 10% of the total gain, and the TU and PR contribute over 4% of the total one, proving the effectiveness of each module. Moreover, it is observed that the proposed method can restore detailed structures and textures and hence improve the subjective quality.}
}
@article{LATORRECARMONA2023103704,
title = {Proposal of a new fidelity measure between computed image quality and observers quality scores accounting for scores variability},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103704},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103704},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002243},
author = {Pedro Latorre-Carmona and Rafael Huertas and Marius Pedersen and Samuel Morillas},
keywords = {STRESS, Psycophysics, Image quality metric, Evaluation},
abstract = {Assessment of the visual quality of colour images is usually a difficult process, validated through hard-to-carry-out psychophysical experiments, used to record observer quality scores. Visual image quality metrics aim to maximise the agreement between computed indexes and observer scores, or opinions. Therefore, in this area, it is of critical importance to have appropriate measures of this agreement (i.e. performance) between the computed image quality metric values and observer’s quality scores, both for the development, as well as for the use of image quality metrics. Among the measures of agreement, the most used one nowadays is the well-known Pearson correlation coefficient, while Spearman rank correlation coefficient is also commonly used. The aim of this paper is two-fold. First, to introduce the Standardised Residual Sum of Squares (STRESS) as an alternative metric for the agreement between computed image quality and observers quality scores and analyse its properties and advantages in front of Pearson, Spearman and Kendall correlation coefficients; Second, to introduce a new version of STRESS (called USTRESS) that takes observers’ scores variability into account. The results on synthetic and real datasets support that STRESS has a series of benefits in front of the classical approaches and that the inclusion of uncertainty in STRESS has an important effect on the results, quantified by statistical significance tests. A free to download MATLAB code version of USTRESS is available at https://viplab.webs.upv.es/resources/}
}
@article{SUN2023103728,
title = {MADPL-net: Multi-layer attention dictionary pair learning network for image classification},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103728},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103728},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002486},
author = {Yulin Sun and Guangming Shi and Weisheng Dong and Xuemei Xie},
keywords = {Multi-layer dictionary learning, Attention dictionary pair learning, Deep encoder, Image classification},
abstract = {With the great success of deep neural networks, combining deep learning with traditional dictionary learning has become a hot issue. However, the performance of these methods is still limited for several reasons. First, some existing methods update dictionary learning and classifier as two independent modules, which limits the classification performance. Second, the non-attention dictionary is learned to represent all images, reducing the model representation flexibility. In this paper, we design a novel end-to-end model named Multi-layer Attention Dictionary Pair Learning Network (MADPL-net), which integrates the learning schemes of the convolutional neural network, deep encoder learning, and attention dictionary pair learning (ADicL) into a unified framework. The encoder layer contains the ADicL block, which selects more image-attentive atoms in the dictionary pair block via the softmax function to ensure MADPL-net classification capability. In addition, ADicL schema can yield discriminative dictionary atoms and feature maps with high inter-class separation and high intra-class compactness. To improve the sparse representation learning performance, MADPL-net adds l1−norm constraint of the analysis dictionary to the cross-entropy loss function. Extensive experiments show that MADPL-net can achieve excellent performance over other state-of-the-arts.}
}
@article{XIA2022103680,
title = {GCENet: Global contextual exploration network for RGB-D salient object detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103680},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103680},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002000},
author = {Chenxing Xia and Songsong Duan and Xiuju Gao and Yanguang Sun and Rongmei Huang and Bin Ge},
keywords = {Salient object detection, Convolution neural network, Multi-scale, Global contextual},
abstract = {Representing contextual features at multiple scales is important for RGB-D SOD. Recently, due to advances in backbone convolutional neural networks (CNNs) revealing stronger multi-scale representation ability, many methods achieved comprising performance. However, most of them represent multi-scale features in a layer-wise manner, which ignores the fine-grained global contextual cues in a single layer. In this paper, we propose a novel global contextual exploration network (GCENet) to explore the performance gain of multi-scale contextual features in a fine-grained manner. Concretely, a cross-modal contextual feature module (CCFM) is proposed to represent the multi-scale contextual features at a single fine-grained level, which can enlarge the range of receptive fields for each network layer. Furthermore, we design a multi-scale feature decoder (MFD) that integrates fused features from CCFM in a top-down way. Extensive experiments on five benchmark datasets demonstrate that the proposed GCENet outperforms the other state-of-the-art (SOTA) RGB-D SOD methods.}
}
@article{KONG2022103659,
title = {Contour enhanced image super-resolution},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103659},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103659},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001791},
author = {Linhua Kong and Yiming Wang and Dongxia Chang and Yao Zhao},
keywords = {Contour, Attention mechanism, Deep convolution neural network},
abstract = {Recently, very deep convolution neural network (CNN) has shown strong ability in single image super-resolution (SISR) and has obtained remarkable performance. However, most of the existing CNN-based SISR methods rarely explicitly use the high-frequency information of the image to assist the image reconstruction, thus making the reconstructed image looks blurred. To address this problem, a novel contour enhanced Image Super-Resolution by High and Low Frequency Fusion Network (HLFN) is proposed in this paper. Specifically, a contour learning subnetwork is designed to learn the high-frequency information, which can better learn the texture of the image. In order to reduce the redundancy of the contour information learned by the contour learning subnetwork during fusion, the spatial channel attention block (SCAB) is introduced, which can select the required high-frequency information adaptively. Moreover, a contour loss is designed and it is used with the ℓ1 loss to optimize the network jointly. Comprehensive experiments demonstrate the superiority of our HLFN over state-of-the-art SISR methods.}
}
@article{KAPLAN2023103720,
title = {Real-world image dehazing with improved joint enhancement and exposure fusion},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103720},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103720},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002401},
author = {Nur Huseyin Kaplan},
keywords = {Image dehazing, Image enhancement, Image sharpening, Image filtering},
abstract = {In this work, a single image dehazing method that improves the haze removal capacity of the Joint Contrast Enhancement and Exposure Fusion (CEEF) method with Smoothing-Sharpening Image Filter (SSIF) is presented. In this method, the hazy image is first sharpened with SSIF to obtain a sharper image. In this way, the difference between haze and objects is amplified. Then, the AHE procedure in CEEF is replaced by CLAHE to obtain an enhanced CEEF. The enhanced CEEF is applied to the filtering result to obtain the final dehazed image. Observations demonstrate that the proposed method obtains enhanced results while reducing the amount of haze. The visual and quantitative comparisons between the proposed method and state-of-the-art dehazing methods show that the proposed method has better dehazing performance and has a 50% improvement in terms of the FADE metric compared to the closest result.}
}
@article{LI2023103692,
title = {Recaptured screen image identification based on vision transformer},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103692},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103692},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002127},
author = {Guihao Li and Heng Yao and Yanfen Le and Chuan Qin},
keywords = {Recaptured screen images, Image forensics, Demoiréing operation, Vision transformer, Recapture identification},
abstract = {Due to the copyright issues often involved in the recapture of LCD screen content, recaptured screen image identification has received lots of concerns in image source forensics. This paper analyzes the characteristics of convolutional neural network (CNN) and vision transformer (ViT) in extracting features and proposes a cascaded network structure that combines local-feature and global-feature extraction modules to detect the recaptured screen image from original images with or without demoiréing operation. We first extract the local features of the input images with five convolutional layers and feed the local features into the ViT to enhance the local perception capability of the ViT module, and further extract the global features of the input images. Through thorough experiments, our method achieves a detection accuracy rate of 0.9691 in our generated dataset and 0.9940 in the existing mixture dataset, both showing the best performance among the compared methods.}
}
@article{DUBEY2022103620,
title = {Improving small objects detection using transformer},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103620},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103620},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001432},
author = {Shikha Dubey and Farrukh Olimov and Muhammad Aasim Rafique and Moongu Jeon},
keywords = {Normalized inductive bias, Features fusion, Small-objects detection, Transformer, Self-attention},
abstract = {General artificial intelligence counteracts the inductive bias of an algorithm and tunes the algorithm for out-of-distribution generalization. A conspicuous impact of the inductive bias is an unceasing trend in improving deep learning performance. Although a quintessential attention-based object detection technique, DETR, shows better accuracy than its predecessors, its accuracy deteriorates for detecting small-sized (in-perspective) objects. This study examines the inductive bias of DETR and proposes a normalized inductive bias for object detection using data fusion, SOF-DETR. A technique of lazy-fusion of features is introduced in SOF-DETR, which sustains deep contextual information of objects present in an image. The features from multiple subsequent deep layers are fused for object queries that learn long and short-distance spatial association in an image using the attention mechanism. Experimental results on the MS COCO and Udacity Self Driving Car datasets assert the effectiveness of the added normalized inductive bias and feature fusion techniques, showing increased COCO mAP scores on small-sized objects.}
}
@article{LI2022103663,
title = {Joint strong edge and multi-stream adaptive fusion network for non-uniform image deblurring},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103663},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103663},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001833},
author = {Zihan Li and Guangmang Cui and Jufeng Zhao and Qinlei Xiang and Bintao He},
keywords = {Non-uniform motion deblurring, Attention mechanisms, Edge extraction algorithm, Generative adversarial network},
abstract = {Non-uniform motion deblurring has been a challenging problem in the field of computer vision. Currently, deep learning-based deblurring methods have made promising achievements. In this paper, we propose a new joint strong edge and multi-stream adaptive fusion network to achieve non-uniform motion deblurring. The edge map and the blurred map are jointly used as network inputs and Edge Extraction Network (EEN) guides the Deblurring Network (DN) for image recovery and to complement the important edge information. The Multi-stream Adaptive Fusion Module (MAFM) adaptively fuses the edge information and features from the encoder and decoder to reduce feature redundancy to avoid image artifacts. Furthermore, the Dense Attention Feature Extraction Module (DAFEM) is designed to focus on the severely blurred regions of blurry images to obtain important recovery information. In addition, an edge loss function is added to measure the difference of edge features between the generated and clear images to further recover the edges of the deblurred images. Experiments show that our method outperforms currently public methods in terms of PSNR, SSIM and VIF, and generates images with less blur and sharper edges.}
}
@article{BATTIATO2022103635,
title = {CNN-based first quantization estimation of double compressed JPEG images},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103635},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103635},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001559},
author = {Sebastiano Battiato and Oliver Giudice and Francesco Guarnera and Giovanni Puglisi},
keywords = {First quantization estimation, Multimedia forensics, JPEG, Image tampering},
abstract = {Multiple JPEG compressions leave artifacts in digital images: residual traces that could be exploited in forensics investigations to recover information about the device employed for acquisition or image editing software. In this paper, a novel First Quantization Estimation (FQE) algorithm based on convolutional neural networks (CNNs) is proposed. In particular, a solution based on an ensemble of CNNs was developed in conjunction with specific regularization strategies exploiting assumptions about neighboring element values of the quantization matrix to be inferred. Mostly designed to work in the aligned case, the solution was tested in challenging scenarios involving different input patch sizes, quantization matrices (both standard and custom) and datasets (i.e., RAISE and UCID collections). Comparisons with state-of-the-art solutions confirmed the effectiveness of the presented solution demonstrating for the first time to cover the widest combinations of parameters of double JPEG compressions.}
}
@article{WANG2022103658,
title = {Image copy-move forgery detection based on dynamic threshold with dense points},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103658},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103658},
url = {https://www.sciencedirect.com/science/article/pii/S104732032200178X},
author = {Xiangyang Wang and Wencong Chen and Panpan Niu and Hongying Yang},
keywords = {Copy-move tampering, FJFMs, SLIC, Dynamic threshold, WLD},
abstract = {Copy-move tampering is one of the most popular tampering techniques at present. The tampered region of the image has good fusion with the original image, which increases the difficulty of detection. After years of research, the current detection method based on key points still has the following problems: 1) Failure to achieve forgery detection of small areas/self-similar areas/smooth areas, 2) Lack of reasonable feature point extraction methods, 3) The various stages of copy-move forgery detection (CMFD) work are relatively independent and lack close connections, 4) A fixed threshold is used as the region of interest similarity metric in the matching and localization stages. The failure to consider tampered images and the diversity of tampered regions leads to the limited detection capability of the algorithm. Considering the actual situation of tampering with the picture, to solve the above problems, we propose a copy-move forgery detection method based on the dynamic threshold. First, we determine the point extraction strategy in each super pixel block according to the size of the simple line interface calculation (SLIC) super pixel block and the Weber local descriptor (WLD) descriptor to ensure the reasonable allocation of feature points and reduce unnecessary points. These key points are then characterized by the scaling, flip and rotation invariants of the fractional general Jacobi-Fourier moments (FJFMs). Then, the matching and mismatch filtering thresholds of each feature point are determined through the WLD and SLIC features, and the SLIC feature is used to replace the distance threshold to improve the detection accuracy of small manufacturing areas. Finally, based on the matching results and SLIC features, an effective positioning method is proposed to improve the speed and accuracy of positioning. Experimental results show that the proposed algorithm is superior to the classic methods in recent years in terms of time and accuracy.}
}
@article{MENASRI2022103673,
title = {Hardware implementation of HEVC CABAC binarization/de-binarization},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103673},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103673},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001936},
author = {Wahiba Menasri and Manel Djabri and Sarah Chennoufi and Abdellah Skoudarli and Mounir Bouhedda and Omar Benzineb},
keywords = {Binarization, CABAC, De-binarization, FPGA, HEVC, Matlab, VHDL},
abstract = {High efficiency video coding (HEVC) video codec applies different techniques in order to achieve high compression ratios and video quality that supports real-time applications. One of the critical techniques in HEVC is the Context adaptive Binary Arithmetic Coding (CABAC) which is type of entropy coding. CABAC comes at the cost of increased computational complexity, especially for parallelization and pipeline of these blocks: binarization, context modeling and binary arithmetic encoding. The Binarization (BZ) and de-Binarization (DBZ) methods are considered as important techniques in HEVC CABAC encoder and decoder respectively. Indeed, an important goal is to get high throughput in hardware architectures of CABAC BZ and DBZ in order to achieve high resolution applications. This work is the only one found on recent literature which focuses on design and implementation of full BZ and full DBZ compatible with H.265 and H.264. Consequently, a hardware architectures of BZ and DBZ are designed and implemented by using VHDL language, targeted an FPGA virtex4 xc4vsx25-12ff668 board and emulated with ModelSim. As a result, the implementation of BZ and DBZ can process 2 bins/cycle for each syntax element when operated at 697.83 MHz and 789.26 MHz, respectively. The proposed designs exhibits an improved high-throughput of 1395.66 Mbins/s for BZ and 1578.52 Mbins/s for the DBZ. The obtained Area Efficiencies in our proposed BZ and DBZ are about 0.544 Mbins/s/slices and 0.606 Mbins/s/slices, respectively, and it is better than many recent works.}
}
@article{KUMAR2022103644,
title = {VI-NET: A hybrid deep convolutional neural network using VGG and inception V3 model for copy-move forgery classification},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103644},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103644},
url = {https://www.sciencedirect.com/science/article/pii/S104732032200164X},
author = {Sanjeev Kumar and Suneet K. Gupta and Manjit Kaur and Umesh Gupta},
keywords = {Copy-move forgery, COMOFOD dataset, Convolution neural network, VGG16, Inception V3},
abstract = {Nowadays, various image editing tools are available that can be utilized for manipulating the original images; here copy-move forgery is most common forgery. In copy-move forgery, some part of the original image is copied and pasted into the same image at some other location. However, Artificial Intelligence (AI) based approaches can extract manipulated features easily. In this study, a deep learning-based method is proposed to classify the copy-move forged images. For classifying the forged images, a deep learning (DL) based hybrid model is presented named as VI-NET using fusion of two DL architectures, i.e., VGG16 and Inception V3. Further, output of two models is concatenated and connected with two additional convolutional layers. Cross-validation protocols, K10 (90 % training, 10 % testing), K5 (80 % training, 20 % testing), and K2 (50 % training, 50 % testing) are applied on the COMOFOD dataset. Moreover, the performance of VI-NET is compared with transfer learning and machine learning models using evaluation metrics such as accuracy, precision, recall, F1 score, etc. Proposed hybrid model performed better than other approaches with classification accuracy of 99 ± 0.2 % in comparison to accuracy of 95 ± 4 % (Inception V3), 93 ± 5 % (MobileNet), 59 ± 8 % (VGG16), 60 ± 1 % (Decision tree), 87 ± 1 % (KNN), 54 ± 1 % (Naïve Bayes) and 65 ± 1 % (random forest) under K10 protocol. Similarly, results are evaluated based on K2 and K5 validation protocols. It is experimentally observed that the proposed model performance is better than existing standard and customized deep learning architectures.}
}
@article{LI2022103687,
title = {Siamese visual tracking with multilayer feature fusion and corner distance IoU loss},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103687},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103687},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002073},
author = {Weisheng Li and Junye Zhu},
keywords = {Visual Tracking Siamese network, Multilayer feature fusion, Intersection over union (IoU) loss},
abstract = {The tracker based on the Siamese network regards tracking tasks as solving a similarity problem between the target template and search area. Using shallow networks and offline training, these trackers perform well in simple scenarios. However, due to the lack of semantic information, they have difficulty meeting the accuracy requirements of the task when faced with complex backgrounds and other challenging scenarios. In response to this problem, we propose a new model, which uses the improved ResNet-22 network to extract deep features with more semantic information. Multilayer feature fusion is used to obtain a high-quality score map to reduce the influence of interference factors in the complex background on the tracker. In addition, we propose a more powerful Corner Distance IoU (intersection over union) loss function so that the algorithm can better regression to the bounding box. In the experiments, the tracker was extensively evaluated on the object tracking benchmark data sets, OTB2013 and OTB2015, and the visual object tracking data sets, VOT2016 and VOT2017, and achieved competitive performance, proving the effectiveness of this method.}
}
@article{XU2023103727,
title = {CCFNet: Cross-Complementary fusion network for RGB-D scene parsing of clothing images},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103727},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103727},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002474},
author = {Gao Xu and Wujie Zhou and Xiaohong Qian and Lv Ye and Jingsheng Lei and Lu Yu},
keywords = {RGB-D, Cross-complementary fusion, Cross-feature enhancement module, Scene parsing, Cross-modal fusion module},
abstract = {Schemes to complement context relationships by cross-scale feature fusion have appeared in many RGB-D scene parsing algorithms; however, most of these works conduct multi-scale information interaction after multi-modal feature fusion, which ignores the information loss of the two modes in the original coding. Therefore, a cross-complementary fusion network (CCFNet) is designed in this paper to calibrate the multi-modal information before feature fusion, so as to improve the feature quality of each mode and the information complementarity ability of RGB and the depth map. First, we divided the features into low, middle, and high levels, among which the low-level features contain the global details of the image and the main learning features include texture, edge, and other features. The middle layer features contain not only some global detail features but also some local semantic features. Additionally, the high-level features contain rich local semantic features. Then, the feature information lost in the coding process of low and middle level features is supplemented and extracted through the designed cross feature enhancement module, and the high-level features are extracted through the feature enhancement module. In addition, the cross-modal fusion module is designed to integrate multi-modal features of different levels. The experimental results verify that the proposed CCFNet achieves excellent performance on the RGB-D scene parsing dataset containing clothing images, and the generalization ability of the model is verified by the dataset NYU Depth V2.}
}
@article{LIU2023103691,
title = {Visual video evaluation association modeling based on chaotic pseudo-random multi-layer compressed sensing for visual privacy-protected keyframe extraction},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103691},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103691},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002115},
author = {Jixin Liu and Yicong Li and Guang Han and Ning Sun},
keywords = {Multi-layer VPP coding, Visual evaluation algorithm, Keyframe extraction, Keyframe extraction performance evaluation index, Association model},
abstract = {In current society, artificial intelligence processing technology offers convenient video monitoring, but also raises the risk of privacy leakage. Theoretically, the data used in intelligent video processing methods may directly convey visual information containing private content. For the above problem, this paper uses a multi-layer visual privacy-protected (VPP) coding method to blur private content in the video at the visual level, while avoiding the loss of important visual features contained in the video as much as possible. And this provides a guarantee of the quality of the subsequent keyframe extraction step. Then a visual evaluation algorithm is proposed for assessing the quality of VPP-encoded video privacy protection. And the experiment shows that the results are consistent with those of subjective evaluation. In addition, for VPP-encoded video, we propose an unsupervised two-layer clustering keyframe extraction method with corresponding performance evaluation index. Finally, an association model is established to balance the privacy protection quality and the keyframe extraction performance.}
}
@article{WEI2023103715,
title = {HD-YOLO: Using radius-aware loss function for head detection in top-view fisheye images},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103715},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103715},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002358},
author = {Xuan Wei and Yun Wei and Xiaobo Lu},
keywords = {Fisheye images, Head detection, Radius-aware Loss Function, Deep learning},
abstract = {People detection is commonly used in computer vision systems, particularly for video surveillance and passenger flow statistics. Unlike standard cameras, fisheye cameras offer a large field of view and reduce occlusions when mounted overhead. However, due to the orientation variation of people in fisheye images, head detection models suffer from severe distortion when applied to fisheye images captured by top-view fisheye cameras. This work develops an end-to-end head detection method named HD-YOLO against complex situations in top-view fisheye images. The radius-aware loss function is designed to make HD-YOLO adapt to the impact of fisheye distortion, and the channel attention module is added to the model. We have also created new fisheye-image datasets for evaluation. Experiments showed that HD-YOLO outperforms other baseline methods on public and self-built datasets.}
}
@article{CHEN2022103646,
title = {HLTD-CSA: Cover selection algorithm based on hybrid local texture descriptor for color image steganography},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103646},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103646},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001663},
author = {Menghua Chen and Peisong He and Jiayong Liu},
keywords = {Color image steganography, Cover selection, Hybrid local texture, Channel correlations},
abstract = {Cover selection is one of the important techniques to improve the security of image steganography. However, existing methods mostly focus on cover selection of grayscale image. In this paper, we propose a novel Cover Selection Algorithm of color images based on Hybrid Local Texture Descriptor, named HLTD-CSA. A green-channel related Local Binary Pattern (LBP) is designed which utilizes local statistics of intra-channel and cross-channel correlations efficiently. Besides, Local Phase Quantization is introduced to serve as a complementary component of our improved LBP. To further enhance the performance of cover selection for color image, a hybrid local texture descriptor (HLTD) is obtained by combining these two types of local texture descriptors with a proper combination strategy. Finally, the proposed algorithm selects images which have larger values of HLTD to construct the cover image set. Extensive experiments are conducted to verify the effectiveness of our method.}
}
@article{JI2022103634,
title = {ETS-3D: An Efficient Two-Stage Framework for Stereo 3D Object Detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103634},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103634},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001547},
author = {Chaofeng Ji and Guizhong Liu and Dan Zhao},
keywords = {3D Object Detection, Deep Learning, Stereo Matching, Autonomous Driving},
abstract = {We propose an efficient two-stage framework for stereo 3D object detection, called ETS-3D. Contrary to many recent approaches that rely on depth maps predicted using time-consuming stereo matching models, our approach utilizes the well-designed features to generate high-quality 3D proposals in stage-1, without explicitly exploiting predicted depth map. Specifically, we leverage pixel-wise correlation to produce normalized cost volumes to weight the left image features, and fuse multi-scale weighted features to obtain the weighted and fused features for 3D proposal generation. To maintain fast computation, only the filtered positive 3D proposals are fed into the stage-2 sub-network for further proposal refinement and quality prediction. Furthermore, we reconstruct the 3D proposal features in stage-2 to make use of different feature representations, achieving more accurate detection results. The experimental results on the KITTI 3D object detection benchmark demonstrate that our method achieves state-of-the-art performance, and can run at more than 10 fps.}
}
@article{LIANG2022103662,
title = {Multi-scale and multi-patch transformer for sandstorm image enhancement},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103662},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103662},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001821},
author = {Pengwei Liang and Wenyu Ding and Lu Fan and Haoyu Wang and Zihong Li and Fan Yang and Bo Wang and Chongyi Li},
keywords = {Sandstorm image enhancement, Transformer, Two-stage network, Dataset},
abstract = {Sandstorm is a meteorological phenomenon common in arid and semi-arid regions. A sandstorm can carry large volumes of sand unexpectedly, which leads to severe color deviations and significantly degraded visibility when an image is taken in such a scenario. However, existing image enhancement methods cannot enhance sandstorm images well due to the challenging degradations and the scarcity of sandstorm training data. In this paper, we propose a Transformer with rotary position embedding to perform sandstorm image enhancement via building multi-scale and multi-patch dependencies. Our key insights in this work are 1) a multi-scale Transformer can globally eliminate the color deviations of sandstorm images via aggregating global information, 2) a multi-patch Transformer can recover local details well via learning the spatial variant degradations, and 3) a U-shape Transformer with rotary position embedding as the core unit of multi-scale and multi-patch Transformer can effectively build the long-range dependencies. We also contribute a real-world Sandstorm Image Enhancement (SIE) dataset including 1,400 sandstorm images with different degrees of degradations and various scenes. Experiments performed on synthetic images and real-world sandstorm images demonstrate that our proposed method not only obtains visually pleasing results but also outperforms state-of-the-art methods qualitatively and quantitatively.}
}
@article{AULILLINAS2022103672,
title = {Accelerating BPC-PaCo through Visually Lossless Techniques},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103672},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103672},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001924},
author = {Francesc Aulí-Llinàs and Carlos {de Cea-Dominguez} and Miguel Hernández-Cabronero},
keywords = {High-throughput image coding, Visually lossless coding, JPEG2000},
abstract = {Fast image codecs are a current need in applications that deal with large amounts of images. Graphics Processing Units (GPUs) are suitable processors to speed up most kinds of algorithms, especially when they allow fine-grain parallelism. Bitplane Coding with Parallel Coefficient processing (BPC-PaCo) is a recently proposed algorithm for the core stage of wavelet-based image codecs tailored for the highly parallel architectures of GPUs. This algorithm provides complexity scalability to allow faster execution at the expense of coding efficiency. Its main drawback is that the speedup and loss in image quality is controlled only roughly, resulting in visible distortion at low and medium rates. This paper addresses this issue by integrating techniques of visually lossless coding into BPC-PaCo. The resulting method minimizes the visual distortion introduced in the compressed file, obtaining higher-quality images to a human observer. Experimental results also indicate 12% speedups with respect to BPC-PaCo.}
}
@article{DELIBASOGLU2022103616,
title = {Motion detection in moving camera videos using background modeling and FlowNet},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103616},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103616},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001407},
author = {Ibrahim Delibasoglu and Irfan Kosesoy and Muhammed Kotan and Feyza Selamet},
keywords = {Motion detection, Moving object detection, Dense optical flow, Moving camera},
abstract = {Real-time moving object detection is challenging for moving cameras due to the moving background. Many studies use homography matrix to compensate for global motion by warping the background model to the current frame. Then, the pixel difference between the current frame and the background model is used for background subtraction. Moving pixels are extracted by applying adaptive threshold and some post-processing techniques. On the other hand, deep learning-based dense optical flow can be efficient enough to extract the moving pixels, but it increases computational cost. This study proposes a method to enhance a classical background modeling method with deep learning-based dense optical flow. The main contribution of this paper is to propose a fusing algorithm for dense optical flow and background modeling approach. The background modeling methods are error-prone, especially with continuous camera movement, while the optical flow method alone may not always be efficient. Our hybrid method fuses both techniques to improve the detection accuracy. We propose a software architecture to run background modeling and dense optical flow methods in parallel processes. The proposed implementation approach significantly increases the method’s working speed, while the proposed fusion and combining strategy improve detection results. The experimental results show that the proposed method can run at high speed and has satisfying performance against the methods in the literature.}
}
@article{ZHENG2022103660,
title = {Bi-projection for 360°image object detection bridged by RoI Searcher},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103660},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103660},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001808},
author = {Zishuo Zheng and Chunyu Lin and Lang Nie and Kang Liao and Yao Zhao},
keywords = {360°image, Object detection, Equirectangular, Cubemap},
abstract = {Object detection on 360°images is a vital component of 3D environment perception. The existing methods either treat panoramic images (usually represented in equirectangular projection—ERP) as normal FoV images and endure the distortions or project them into the less-distortion format and narrow the FoV, leading to unsatisfactory performance in practical applications. To solve this problem, we propose a dual-projection 360°object detection network named Bip R-CNN, consisting of three modules: a bi-projection feature extractor, a cross-projection region-of-interest (RoI) searcher, and a classification and regression predictor. Specifically, we extract the equirectangular and corresponding dual-cubemap features simultaneously from the input images. Besides, Projection-Inter Feature Fusion and Projection-Intra Feature Fusion are designed to allow the mutual interaction between the bi-projective features and promote the integration of features at different scales, respectively. In the proposed cross-projection RoI Searcher, we search for the bounding box (BBox) locations on cubemap from the corresponding ERP spherical proposals, bridging the RoIs of two different projection formats at feature level. Finally, the cube proposals are used to detect objects in the last predictor module. Considering the scarceness of the existing panoramic dataset (only indoor scenes), we propose an efficient approach to convert conventional datasets into annotated panoramic datasets without manual intervention, increasing the diversity of panoramic datasets. Extensive experiments are conducted on the synthetic and real-world datasets with spherical criteria, demonstrating our superiority to other state-of-the-art solutions.}
}
@article{HU2022103686,
title = {Decomposing style, content, and motion for videos},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103686},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103686},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002061},
author = {Yaosi Hu and Dacheng Yin and Yuwang Wang and Zhenzhong Chen and Chong Luo},
keywords = {Video decomposition, Video synthesis, Self-supervised learning},
abstract = {In this paper, we present the first video decomposition framework, named SyCoMo, that factorizes a video into style, content, and motion. Such a fine-grained decomposition enables flexible video editing, and for the first time allows for tripartite video synthesis. SyCoMo is a unified and domain-agnostic learning framework which can process videos of various object categories without domain-specific design or supervision. Different from other motion decomposition work, SyCoMo derives motion from style-free content by isolating style from content in the first place. Content is organized into subchannels, each of which corresponds to an atomic motion. This design naturally forms an information bottleneck which facilitates a clean decomposition. Experiments show that SyCoMo decomposes videos of various categories into interpretable content subchannels and meaningful motion patterns. Ablation studies also show that deriving motion from style-free content makes the keypoints or landmarks of the object more accurate. We demonstrate the photorealistic quality of the novel tripartite video synthesis in addition to three bipartite synthesis tasks named as style, content, and motion transfer.}
}
@article{WEI2022103629,
title = {Semantic guided knowledge graph for large-scale zero-shot learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103629},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103629},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001493},
author = {Jiwei Wei and Haotian Sun and Yang Yang and Xing Xu and Jingjing Li and Heng Tao Shen},
keywords = {Large-scale zero-shot learning, Semantic guided knowledge graph, Multi-granularity fusion network},
abstract = {Zero-shot learning has received growing attention, which aims to improve generalization to unseen concepts. The key challenge in zero-shot tasks is to precisely model the relationship between seen and unseen classes. Most existing zero-shot learning methods capture inter-class relationships via a shared embedding space, leading to inadequate use of relationships and poor performance. Recently, knowledge graph-based methods have emerged as a new trend of zero-shot learning. These methods use a knowledge graph to accurately model the inter-class relationships. However, the currently dominant method for zero-shot learning directly extracts the fixed connection from off-the-shelf WordNet, which will inherit the inherent noise in WordNet. In this paper, we propose a novel method that adopts class-level semantic information as a guidance to construct a new semantic guided knowledge graph (SG-KG), which can correct the errors in the existing knowledge graph and accurately model the inter-class relationships. Specifically, our method includes two main steps: noise suppression and semantic enhancement. Noise suppression is used to eliminate noise edges in the knowledge graph, and semantic enhancement is used to connect two classes with strong relations. To promote high efficient information propagation among classes, we develop a novel multi-granularity fusion network (MGFN) that integrates discriminative information from multiple GCN branches. Extensive experiments on the large-scale ImageNet-21K dataset and AWA2 dataset demonstrate that our method consistently surpasses existing methods and achieves a new state-of-the-art result.}
}
@article{LI2023103726,
title = {Cartoon-Texture decomposition with patch-wise decorrelation},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103726},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103726},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002462},
author = {Xiaofang Li and Weiwei Wang and Xiangchu Feng and Tingting Qi},
keywords = {Cartoon-Texture decomposition, Patch-wise cosine similarity, Total variation},
abstract = {Cartoon-Texture decomposition (CTD) is a fundamental task and has wide applications in image processing and computer vision. To enhance separation of the cartoon and texture, existing models explicitly introduce correlation terms to decorrelate the two components. However, existing correlations usually ignore the local geometric structure information, thus insufficient to decorrelate cartoon and texture. In this work, we propose the patch-wise cosine similarity to decorrelate the cartoon and texture. The proposed decorrelation term takes the local geometric information into account and is more effective in separating cartoon and texture. Combining our decorrelation term with the regularities for cartoon (Relative Total Variation (RTV)) and texture (div(L1)-norm), we propose a new CTD model. Extended experiments show that the proposed model outperforms existing methods in CTD, especially in preserving edges of the cartoon.}
}
@article{ZHU2023103714,
title = {Multiscale Global-Aware Channel Attention for Person Re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103714},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103714},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002346},
author = {Yingjie Zhu and Wenzhong Yang and Liejun Wang and Danny Chen and Min Wang and Fuyuan Wei and HaiLaTi KeZiErBieKe and Yuanyuan Liao},
keywords = {Person re-identification, Multi-scale, Global-aware, Channel attention},
abstract = {Most person re-identification methods are researched under various assumptions. However, viewpoint variations or occlusions are often encountered in practical scenarios. These are prone to intra-class variance. In this paper, we propose a multiscale global-aware channel attention (MGCA) model to solve this problem. It imitates the process of human visual perception, which tends to observe things from coarse to fine. The core of our approach is a multiscale structure containing two key elements: the global-aware channel attention (GCA) module for capturing the global structural information and the adaptive selection feature fusion (ASFF) module for highlighting discriminative features. Moreover, we introduce a bidirectional guided pairwise metric triplet (BPM) loss to reduce the effect of outliers. Extensive experiments on Market-1501, DukeMTMC-reID, and MSMT17, and achieve the state-of-the-art results on mAP. Especially, our approach exceeds the current best method by 2.0% on the most challenging MSMT17 dataset.}
}
@article{KUO2023103685,
title = {Green learning: Introduction, examples and outlook},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103685},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103685},
url = {https://www.sciencedirect.com/science/article/pii/S104732032200205X},
author = {C.-C. Jay Kuo and Azad M. Madni},
keywords = {Machine learning, Green learning, Trust learning, Deep learning},
abstract = {Rapid advances in artificial intelligence (AI) in the last decade have been largely built upon the wide applications of deep learning (DL). However, the high carbon footprint yielded by larger and larger DL networks has become a concern for sustainability. Furthermore, DL decision mechanism is somewhat obscure in that it can only be verified by test data. Green learning (GL) is being proposed as an alternative paradigm to address these concerns. GL is characterized by low carbon footprints, lightweight model, low computational complexity, and logical transparency. It offers energy-efficient solutions in cloud centers as well as mobile/edge devices. GL also provides a more transparent, logical decision-making process which is essential to gaining people’s trust. Several statistical tools such as unsupervised representation learning, supervised feature learning, and supervised decision learning, have been developed to achieve this goal in recent years. We have seen a few successful GL examples with performance comparable with state-of-the-art DL solutions. This paper introduces the key characteristics of GL, its demonstrated applications, and future outlook.}
}
@article{LIU2022103645,
title = {Blind deblurring with fractional-order calculus and local minimal pixel prior},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103645},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103645},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001651},
author = {Jing Liu and Jieqing Tan and Xianyu Ge and Dandan Hu and Lei He},
keywords = {Motion deblurring, Deconvolution, Kernel estimation, Fractional-Order calculus Theory, Local Minimal Pixel Prior},
abstract = {Fractional-order calculus is an extension of integer order calculus.In signal processing, fractional-order calculus can non-linearly enhance the low-frequency signal and suppress the high-frequency signal. In this paper, a new fractional-order local minimum pixel prior (FOLMP) is proposed by combining fractional-order calculus with the local minimum pixel prior. The FOLMP of the sharp images includes fewer non-zero pixels than the blur images. A new blur kernel estimation algorithm is proposed by combining L0 regularized FOLMP with the maximum posterior probability. Furthermore, thekernel similarity is employed to adjust the iteration times to accelerate the computational efficiency. Comparative experiments show that the proposed algorithm can perform better on different types of datasets than the most advanced algorithms. In addition, non-overlapping image patches are adopted to compute the FOLMP, and the kernel similarity is used to suppress excessive iterations.Therefore, the proposed algorithm is several times or even tens of times more efficient than the classical prior-based methods.}
}
@article{JAISWAL2022103690,
title = {DFD-SS: Document Forgery Detection using Spectral – Spatial Features for Hyperspectral Images},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103690},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103690},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002103},
author = {Garima Jaiswal and Arun Sharma and Sumit {Kumar Yadav}},
keywords = {Document forgery, Spectral, Spatial, Spectral-spatial, Autoencoders, Unsupervised Deep Learning},
abstract = {In the present era of machines and edge-cutting technologies, still document frauds persist. They are done intuitively by using almost identical inks, that it becomes challenging to detect them—this demands an approach that efficiently investigates the document and leaves it intact. Hyperspectral imaging is one such a type of approach that captures the images from hundreds to thousands of spectral bands and analyzes the images through their spectral and spatial features, which is not possible by conventional imaging. Deep learning is an edge-cutting technology known for solving critical problems in various domains. Utilizing supervised learning imposes constraints on its usage in real scenarios, as the inks used in forgery are not known prior. Therefore, it is beneficial to use unsupervised learning. An unsupervised feature extraction through a Convolutional Autoencoder (CAE) followed by Logistic Regression (LR) for classification is proposed (CAE-LR). Feature extraction is evolved around spectral bands, spatial patches, and spectral-spatial patches. We inspected the impact of spectral, spatial, and spectral-spatial features by mixing inks in equal and unequal proportion using CAE-LR on the UWA writing ink hyperspectral images dataset for blue and black inks. Hyperspectral images are captured at multiple correlated spectral bands, resulting in information redundancy handled by restoring certain principal components. The proposed approach is compared with eight state-of-art approaches used by the researchers. The results depicted that by using the combination of spectral and spatial patches, the classification accuracy enhanced by 4.85% for black inks and 0.13% for blue inks compared to state-of-art results. In the present scenario, the primary area concern is to identify and detect the almost similar inks used in document forgery, are efficiently managed by the proposed approach.}
}
@article{MAO2022103669,
title = {Multimodal object tracking by exploiting appearance and class information},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103669},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103669},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001894},
author = {Zhongjie Mao and Xi Chen and Jia Yan and Tao Qu},
keywords = {Object tracking, Multimodal, Object class},
abstract = {In this work, we study the method exploiting natural language network to improve tracking performance. We propose a novel architecture which can combine class and visual information presented in tracking. To this end, we introduce a multimodal feature association network, allowing us to correlate the target class with its appearance during training and aid the localization of the target during inference. Specifically, we first utilize an appearance model to extract the target visual features, from which we obtain appearance cues, for instance shape and color. In order to employ target class information, we design a learned lightweight embedding network to embed the target class into a feature representation. The association network of our architecture contains a multimodal fusion module and a predictor module. The fusion module is used to combine features from class and appearance, yielding multimodal features with more expressive representations for the subsequent module. The predictor module is used to determine the target location in the current frame, from which we associate the class to the appearance. The class embedding module thus can learn appearance cues by exploiting the back-propagation functionality. To verify the abilities of our method, we select the official training and test splits of the LaSOT with annotated images and classes to perform experiments. In particular, we analyze the imbalance in the samples and employ a class validator discriminator to alleviate this problem. Extensive experimental results on LaSOT, UAV20L and UAV123@10fps demonstrate our method achieves competitive results while maintaining a considerable real-time speed.}
}
@article{LIU2023103730,
title = {A deep recursive multi-scale feature fusion network for image super-resolution},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103730},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103730},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002504},
author = {Feiqiang Liu and Xiaomin Yang and Bernard {De Baets}},
keywords = {Single Image Super-Resolution (SISR), Recursive networks, Multi-scale features, Progressive feature fusion},
abstract = {Recently, Convolutional Neural Networks (CNNs) have achieved great success in Single Image Super-Resolution (SISR). In particular, the recursive networks are now widely used. However, existing recursion-based SISR networks can only make use of multi-scale features in a layer-wise manner. In this paper, a Deep Recursive Multi-Scale Feature Fusion Network (DRMSFFN) is proposed to address this issue. Specifically, we propose a Recursive Multi-Scale Feature Fusion Block (RMSFFB) to make full use of multi-scale features. Besides, a Progressive Feature Fusion (PFF) technique is proposed to take advantage of the hierarchical features from the RMSFFB in a global manner. At the reconstruction stage, we use a deconvolutional layer to upscale the feature maps to the desired size. Extensive experimental results on benchmark datasets demonstrate the superiority of the proposed DRMSFFN in comparison with the state-of-the-art methods in both quantitative and qualitative evaluations.}
}
@article{NIU2022103657,
title = {Unbiased feature generating for generalized zero-shot learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {89},
pages = {103657},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103657},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001778},
author = {Chang Niu and Junyuan Shang and Junchu Huang and Junmei Yang and Yuting Song and Zhiheng Zhou and Guoxu Zhou},
keywords = {Generalized zero-shot learning, Generative adversarial network, Image classification},
abstract = {Generalized zero-shot learning (GZSL) aims at training a model on seen data to recognize objects from both seen and unseen classes. Existing generated-based methods show encouraging performance by directly generating unseen samples. However, due to insufficient exploration of unseen label space and limited class-wise semantic descriptions, existing methods still face the bias problem. In this paper, we divide the bias problem into seen-biased and neighbor-biased problems and propose a GZSL method named Unbiased Feature Generating. For the seen-biased problem, we train a classifier in complete label space by introducing the discriminative information contained in fake unseen samples. For the neighbor-biased problem, we generate untypical samples and refine the classification boundaries among neighbor classes. The classifier in complete label space and generator are trained in an iterative process to complement each other. The experimental results on four widely used datasets verify our method achieves encouraging performance compared with the state-of-the-art methods.}
}
@article{FU2022103633,
title = {CCNet: CNN model with channel attention and convolutional pooling mechanism for spatial image steganalysis},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103633},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103633},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001535},
author = {Tong Fu and Liquan Chen and Zhangjie Fu and Kunliang Yu and Yu Wang},
keywords = {Steganalysis, Convolutional neural network, Channel attention, Convolutional pooling},
abstract = {Image steganalysis based on convolutional neural networks(CNN) has attracted great attention. However, existing networks lack attention to regional features with complex texture, which makes the ability of discrimination learning miss in network. In this paper, we described a new CNN designed to focus on useful features and improve detection accuracy for spatial-domain steganalysis. The proposed model consists of three modules: noise extraction module, noise analysis module and classification module. A channel attention mechanism is used in the noise extraction module and analysis module, which is realized by embedding the SE(Squeeze-and-Excitation) module into the residual block. Then, we use convolutional pooling instead of average pooling to aggregate features. The experimental results show that detection accuracy of the proposed model is significantly better than those of the existing models such as SRNet, Zhu-Net and GBRAS-Net. Compared with these models, our model has better generalization ability, which is critical for practical application.}
}