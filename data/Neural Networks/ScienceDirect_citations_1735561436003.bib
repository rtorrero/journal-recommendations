@article{TRETIAK2023550,
title = {Neural network model for imprecise regression with interval dependent variables},
journal = {Neural Networks},
volume = {161},
pages = {550-564},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000680},
author = {Krasymyr Tretiak and Georg Schollmeyer and Scott Ferson},
keywords = {Imprecise regression, Interval data, Neural network, Uncertainty},
abstract = {This paper presents a computationally feasible method to compute rigorous bounds on the interval-generalization of regression analysis to account for epistemic uncertainty in the output variables. The new iterative method uses machine learning algorithms to fit an imprecise regression model to data that consist of intervals rather than point values. The method is based on a single-layer interval neural network which can be trained to produce an interval prediction. It seeks parameters for the optimal model that minimizes the mean squared error between the actual and predicted interval values of the dependent variable using a first-order gradient-based optimization and interval analysis computations to model the measurement imprecision of the data. An additional extension to a multi-layer neural network is also presented. We consider the explanatory variables to be precise point values, but the measured dependent values are characterized by interval bounds without any probabilistic information. The proposed iterative method estimates the lower and upper bounds of the expectation region, which is an envelope of all possible precise regression lines obtained by ordinary regression analysis based on any configuration of real-valued points from the respective y-intervals and their x-values.}
}
@article{SHENG2023309,
title = {Global synchronization of complex-valued neural networks with unbounded time-varying delays},
journal = {Neural Networks},
volume = {162},
pages = {309-317},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.041},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001132},
author = {Yin Sheng and Haoyu Gong and Zhigang Zeng},
keywords = {Global synchronization, Complex-valued neural networks, Unbounded time-varying delays},
abstract = {This paper investigates global synchronization of complex-valued neural networks (CVNNs) with unbounded time-varying delays. By applying analytical method and inequality techniques, an algebraic criterion is established to ensure global synchronization of the CVNNs via a devised feedback controller, which generalizes some existing outcomes. Finally, two numerical simulations and one application in image encryption are provided to verify the effectiveness of the theoretical results.}
}
@article{ADOLFI2023199,
title = {Successes and critical failures of neural networks in capturing human-like speech recognition},
journal = {Neural Networks},
volume = {162},
pages = {199-211},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.032},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001016},
author = {Federico Adolfi and Jeffrey S. Bowers and David Poeppel},
keywords = {Audition, Speech, Neural networks, Robustness, Human-like AI},
abstract = {Natural and artificial audition can in principle acquire different solutions to a given problem. The constraints of the task, however, can nudge the cognitive science and engineering of audition to qualitatively converge, suggesting that a closer mutual examination would potentially enrich artificial hearing systems and process models of the mind and brain. Speech recognition — an area ripe for such exploration — is inherently robust in humans to a number transformations at various spectrotemporal granularities. To what extent are these robustness profiles accounted for by high-performing neural network systems? We bring together experiments in speech recognition under a single synthesis framework to evaluate state-of-the-art neural networks as stimulus-computable, optimized observers. In a series of experiments, we (1) clarify how influential speech manipulations in the literature relate to each other and to natural speech, (2) show the granularities at which machines exhibit out-of-distribution robustness, reproducing classical perceptual phenomena in humans, (3) identify the specific conditions where model predictions of human performance differ, and (4) demonstrate a crucial failure of all artificial systems to perceptually recover where humans do, suggesting alternative directions for theory and model building. These findings encourage a tighter synergy between the cognitive science and engineering of audition.}
}
@article{WU2023286,
title = {Robust fall detection in video surveillance based on weakly supervised learning},
journal = {Neural Networks},
volume = {163},
pages = {286-297},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.042},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001776},
author = {Lian Wu and Chao Huang and Shuping Zhao and Jinkai Li and Jianchuan Zhao and Zhongwei Cui and Zhen Yu and Yong Xu and Min Zhang},
keywords = {Fall detection, Multiple instance learning, Dual-modal fusion, Weakly supervised learning},
abstract = {Fall event detection has been a research hotspot in recent years in the fields of medicine and health. Currently, vision-based fall detection methods have been considered the most promising methods due to their advantages of a non-contact characteristic and easy deployment. However, the existing vision-based fall detection methods mainly use supervised learning in model training and require much time and energy for data annotations. To address these limitations, this work proposes a detection method that uses a weakly supervised learning-based dual-modal network. The proposed method adopts a deep multiple instance learning framework to learn the fall events using weak labels. As a result, the proposed method does not require time-consuming fine-grained annotations. The final detection result of each video is obtained by integrating the information obtained from two streams of the dual-modal network using the proposed dual-modal fusion strategy. Experimental results on two public benchmark datasets and a proposed dataset demonstrate the superiority of the proposed method over the current state-of-the-art methods.}
}
@article{LI2023177,
title = {Learning defense transformations for counterattacking adversarial examples},
journal = {Neural Networks},
volume = {164},
pages = {177-185},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001259},
author = {Jincheng Li and Shuhai Zhang and Jiezhang Cao and Mingkui Tan},
keywords = {Adversarial examples, Classification boundary, Defense transformations, Affine transformations},
abstract = {Deep neural networks (DNNs) are vulnerable to adversarial examples with small perturbations. Adversarial defense thus has been an important means which improves the robustness of DNNs by defending against adversarial examples. Existing defense methods focus on some specific types of adversarial examples and may fail to defend well in real-world applications. In practice, we may face many types of attacks where the exact type of adversarial examples in real-world applications can be even unknown. In this paper, motivated by that adversarial examples are more likely to appear near the classification boundary and are vulnerable to some transformations, we study adversarial examples from a new perspective that whether we can defend against adversarial examples by pulling them back to the original clean distribution. We empirically verify the existence of defense affine transformations that restore adversarial examples. Relying on this, we learn defense transformations to counterattack the adversarial examples by parameterizing the affine transformations and exploiting the boundary information of DNNs. Extensive experiments on both toy and real-world data sets demonstrate the effectiveness and generalization of our defense method. The code is avaliable at https://github.com/SCUTjinchengli/DefenseTransformer.}
}
@article{LEE2023682,
title = {Feature Alignment by Uncertainty and Self-Training for Source-Free Unsupervised Domain Adaptation},
journal = {Neural Networks},
volume = {161},
pages = {682-692},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000722},
author = {JoonHo Lee and Gyemin Lee},
keywords = {Unsupervised domain adaptation, Source-free domain adaptation, Uncertainty, Self-training, Image classification},
abstract = {Most unsupervised domain adaptation (UDA) methods assume that labeled source images are available during model adaptation. However, this assumption is often infeasible owing to confidentiality issues or memory constraints on mobile devices. Some recently developed approaches do not require source images during adaptation, but they show limited performance on perturbed images. To address these problems, we propose a novel source-free UDA method that uses only a pre-trained source model and unlabeled target images. Our method captures the aleatoric uncertainty by incorporating data augmentation and trains the feature generator with two consistency objectives. The feature generator is encouraged to learn consistent visual features away from the decision boundaries of the head classifier. Thus, the adapted model becomes more robust to image perturbations. Inspired by self-supervised learning, our method promotes inter-space alignment between the prediction space and the feature space while incorporating intra-space consistency within the feature space to reduce the domain gap between the source and target domains. We also consider epistemic uncertainty to boost the model adaptation performance. Extensive experiments on popular UDA benchmark datasets demonstrate that the proposed source-free method is comparable or even superior to vanilla UDA methods. Moreover, the adapted models show more robust results when input images are perturbed.}
}
@article{ZHAO2023367,
title = {Remix: Towards the transferability of adversarial examples},
journal = {Neural Networks},
volume = {163},
pages = {367-378},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001892},
author = {Hongzhi Zhao and Lingguang Hao and Kuangrong Hao and Bing Wei and Xin Cai},
keywords = {Deep neural networks, Black-box attack, Adversarial transferability},
abstract = {Deep neural networks (DNNs) are susceptible to adversarial examples, which are crafted by deliberately adding some human-imperceptible perturbations on original images. To explore the vulnerability of models of DNNs, transfer-based black-box attacks are attracting increasing attention of researchers credited to their high practicality. The transfer-based approaches can launch attacks against models easily in the black-box setting by resultant adversarial examples, whereas the success rates are not satisfactory. To boost the adversarial transferability, we propose a Remix method with multiple input transformations, which could achieve multiple data augmentation by utilizing gradients from previous iterations and images from other categories in the same iteration. Extensive experiments on the NeurIPS 2017 adversarial dataset and the ILSVRC 2012 validation dataset demonstrate that the proposed approach could drastically enhance the adversarial transferability and maintain similar success rates of white-box attacks on both undefended models and defended models. Furthermore, extended experiments based on LPIPS show that our method could maintain a similar perceived distance compared to other baselines.}
}
@article{GAO20231,
title = {Generalized image outpainting with U-transformer},
journal = {Neural Networks},
volume = {162},
pages = {1-10},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.021},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000849},
author = {Penglei Gao and Xi Yang and Rui Zhang and John Y. Goulermas and Yujie Geng and Yuyao Yan and Kaizhu Huang},
keywords = {Image outpainting, Transformer, U-shaped structure, Temporal spatial predictor},
abstract = {In this paper, we develop a novel transformer-based generative adversarial neural network called U-Transformer for generalized image outpainting problems. Different from most present image outpainting methods conducting horizontal extrapolation, our generalized image outpainting could extrapolate visual context all-side around a given image with plausible structure and details even for complicated scenery, building, and art images. Specifically, we design a generator as an encoder-to-decoder structure embedded with the popular Swin Transformer blocks. As such, our novel neural network can better cope with image long-range dependencies which are crucially important for generalized image outpainting. We propose additionally a U-shaped structure and multi-view Temporal Spatial Predictor (TSP) module to reinforce image self-reconstruction as well as unknown-part prediction smoothly and realistically. By adjusting the predicting step in the TSP module in the testing stage, we can generate arbitrary outpainting size given the input sub-image. We experimentally demonstrate that our proposed method could produce visually appealing results for generalized image outpainting against the state-of-the-art image outpainting approaches.}
}
@article{TANG2023272,
title = {Functional connectivity learning via Siamese-based SPD matrix representation of brain imaging data},
journal = {Neural Networks},
volume = {163},
pages = {272-285},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001818},
author = {Yunbo Tang and Dan Chen and Jia Wu and Weiping Tu and Jessica J.M. Monaghan and Paul Sowman and David Mcalpine},
keywords = {Brain functional connectivity, Symmetric positive definite matrix, Siamese network, Graph convolution},
abstract = {Measurement of brain functional connectivity has become a dominant approach to explore the interaction dynamics between brain regions of subjects under examination. Conventional functional connectivity measures largely originate from deterministic models on empirical analysis, usually demanding application-specific settings (e.g., Pearson’s Correlation and Mutual Information). To bridge the technical gap, this study proposes a Siamese-based Symmetric Positive Definite (SPD) Matrix Representation framework (SiameseSPD-MR) to derive the functional connectivity of brain imaging data (BID) such as Electroencephalography (EEG), thus the alternative application-independent measure (in the form of SPD matrix) can be automatically learnt: (1) SiameseSPD-MR first exploits graph convolution to extract the representative features of BID with the adjacency matrix computed considering the anatomical structure; (2) Adaptive Gaussian kernel function then applies to obtain the functional connectivity representations from the deep features followed by SPD matrix transformation to address the intrinsic functional characteristics; and (3) Two-branch (Siamese) networks are combined via an element-wise product followed by a dense layer to derive the similarity between the pairwise inputs. Experimental results on two EEG datasets (autism spectrum disorder, emotion) indicate that (1) SiameseSPD-MR can capture more significant differences in functional connectivity between neural states than the state-of-the-art counterparts do, and these findings properly highlight the typical EEG characteristics of ASD subjects, and (2) the obtained functional connectivity representations conforming to the proposed measure can act as meaningful markers for brain network analysis and ASD discrimination.}
}
@article{AGUILERA2023565,
title = {Regularizing transformers with deep probabilistic layers},
journal = {Neural Networks},
volume = {161},
pages = {565-574},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.01.032},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000448},
author = {Aurora Cobo Aguilera and Pablo M. Olmos and Antonio Artés-Rodríguez and Fernando Pérez-Cruz},
keywords = {Natural language processing, Regularization, Deep learning, Transformers, Variational auto-encoder, Missing data},
abstract = {Language models (LM) have grown non-stop in the last decade, from sequence-to-sequence architectures to attention-based Transformers. However, regularization is not deeply studied in those structures. In this work, we use a Gaussian Mixture Variational Autoencoder (GMVAE) as a regularizer layer. We study its advantages regarding the depth where it is placed and prove its effectiveness in several scenarios. Experimental result demonstrates that the inclusion of deep generative models within Transformer-based architectures such as BERT, RoBERTa, or XLM-R can bring more versatile models, able to generalize better and achieve improved imputation score in tasks such as SST-2 and TREC or even impute missing/noisy words with richer text.}
}
@article{SOLAK2023186,
title = {A general framework for robust stability analysis of neural networks with discrete time delays},
journal = {Neural Networks},
volume = {162},
pages = {186-198},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.040},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001120},
author = {Melike Solak and Ozlem Faydasicok and Sabri Arik},
keywords = {Dynamical neural networks, Interval matrices, Discrete delays, Robust stability, Lyapunov functionals},
abstract = {Robust stability of different types of dynamical neural network models including time delay parameters have been extensively studied, and many different sets of sufficient conditions ensuring robust stability of these types of dynamical neural network models have been presented in past decades. In conducting stability analysis of dynamical neural systems, some basic properties of the employed activation functions and the forms of delay terms included in the mathematical representations of dynamical neural networks are of crucial importance in obtaining global stability criteria for dynamical neural systems. Therefore, this research article will examine a class of neural networks expressed by a mathematical model that involves the discrete time delay terms, the Lipschitz activation functions and possesses the intervalized parameter uncertainties. This paper will first present a new and alternative upper bound value of the second norm of the class of interval matrices, which will have an important impact on obtaining the desired results for establishing robust stability of these neural network models. Then, by exploiting wellknown Homeomorphism mapping theory and basic Lyapunov stability theory, we will state a new general framework for determining some novel robust stability conditions for dynamical neural networks possessing discrete time delay terms. This paper will also make a comprehensive review of some previously published robust stability results and show that the existing robust stability results can be easily derived from the results given in this paper.}
}
@article{SUN2023256,
title = {Differential evolution based dual adversarial camouflage: Fooling human eyes and object detectors},
journal = {Neural Networks},
volume = {163},
pages = {256-271},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.041},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001764},
author = {Jialiang Sun and Wen Yao and Tingsong Jiang and Donghua Wang and Xiaoqian Chen},
keywords = {Object detection, Adversarial attack, Camouflage, Differential evolution},
abstract = {Deep neural network-based object detectors are vulnerable to adversarial examples. Among existing works to fool object detectors, the camouflage-based method is more often adopted due to its adaptation to multi-view scenarios and non-planar objects. However, most of them can still be easily observed by human eyes, which limits their application in the real world. To fool human eyes and object detectors simultaneously, we propose a differential evolution based dual adversarial camouflage method. Specifically, we try to obtain the camouflage texture by the two-stage training, which can be wrapped over the surface of the object. In the first stage, we optimize the global texture to minimize the discrepancy between the rendered object and the scene background, making human eyes difficult to distinguish. In the second stage, we design three loss functions to optimize the local texture, which is selected from the global texture, making object detectors ineffective. In addition, we introduce the differential evolution algorithm to search for the near-optimal areas of the object to attack, improving the adversarial performance under certain attack area limitations. Experimental results show that our proposed method can obtain a good trade-off between fooling human eyes and object detectors under multiple specific scenes and objects.}
}
@article{PAOLINI2023531,
title = {CHARLES: A C++ fixed-point library for Photonic-Aware Neural Networks},
journal = {Neural Networks},
volume = {162},
pages = {531-540},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001247},
author = {Emilio Paolini and Lorenzo {De Marinis} and Luca Maggiani and Marco Cococcioni and Nicola Andriolli},
keywords = {Photonic neuromorphic computing, Photonic Aware Neural Networks, Hardware accelerators, Fixed-point training/inference, C++ library},
abstract = {In this paper we present CHARLES (C++ pHotonic Aware neuRaL nEtworkS), a C++ library aimed at providing a flexible tool to simulate the behavior of Photonic-Aware Neural Network (PANN). PANNs are neural network architectures aware of the constraints due to the underlying photonic hardware, mostly in terms of low equivalent precision of the computations. For this reason, CHARLES exploits fixed-point computations for inference, while it supports both floating-point and fixed-point numerical formats for training. In this way, we can compare the effects due to the quantization in the inference phase when the training phase is performed on a classical floating-point model and on a model exploiting high-precision fixed-point numbers. To validate CHARLES and identify the most suited numerical format for PANN training, we report the simulation results obtained considering three datasets: Iris, MNIST, and Fashion-MNIST. Fixed-training is shown to outperform floating-training when executing inference on bitwidths suitable for photonic implementation. Indeed, performing the training phase in the floating-point domain and then quantizing to lower bitwidths results in a very high accuracy loss. Instead, when fixed-point numbers are exploited in the training phase, the accuracy loss due to quantization to lower bitwidths is significantly reduced. In particular, we show that for Iris dataset, fixed-training achieves a performance similar to floating-training. Fixed-training allows to obtain an accuracy of 90.4% and 68.1% with the MNIST and Fashion-MNIST datasets using only 6 bits, while the floating-training reaches an accuracy of just 25.4% and 50.0% when exploiting the same bitwidths.}
}
@article{LIN2023162,
title = {Lifelong Text-Audio Sentiment Analysis learning},
journal = {Neural Networks},
volume = {162},
pages = {162-174},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000710},
author = {Yuting Lin and Peng Ji and Xiuyi Chen and Zhongshi He},
keywords = {Text-audio sentiment analysis, Lifelong machine learning, Cross-modality learning, Multi-task learning},
abstract = {Sentiment analysis refers to the mining of textual context, which is conducted with the aim of identifying and extracting subjective opinions in textual materials. However, most existing methods neglect other important modalities, e.g., the audio modality, which can provide intrinsic complementary knowledge for sentiment analysis. Furthermore, much work on sentiment analysis cannot continuously learn new sentiment analysis tasks or discover potential correlations among distinct modalities. To address these concerns, we propose a novel Lifelong Text-Audio Sentiment Analysis (LTASA) model to continuously learn text-audio sentiment analysis tasks, which effectively explores intrinsic semantic relationships from both intra-modality and inter-modality perspectives. More specifically, a modality-specific knowledge dictionary is developed for each modality to obtain shared intra-modality representations among various text-audio sentiment analysis tasks. Additionally, based on information dependence between text and audio knowledge dictionaries, a complementarity-aware subspace is developed to capture the latent nonlinear inter-modality complementary knowledge. To sequentially learn text-audio sentiment analysis tasks, a new online multi-task optimization pipeline is designed. Finally, we verify our model on three common datasets to show its superiority. Compared with some baseline representative methods, the capability of the LTASA model is significantly boosted in terms of five measurement indicators.}
}
@article{ROKAI2023212,
title = {Edge computing on TPU for brain implant signal analysis},
journal = {Neural Networks},
volume = {162},
pages = {212-224},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.036},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001089},
author = {János Rokai and István Ulbert and Gergely Márton},
keywords = {Spike sorting, Deep learning, Brain–computer interface, Feature extraction, Edge device, Electrophysiology},
abstract = {The ever-increasing number of recording sites of silicon-based probes imposes a great challenge for detecting and evaluating single-unit activities in an accurate and efficient manner. Currently separate solutions are available for high precision offline evaluation and separate solutions for embedded systems where computational resources are more limited. We propose a deep learning-based spike sorting system, that utilizes both unsupervised and supervised paradigms to learn a general feature embedding space and detect neural activity in raw data as well as predict the feature vectors for sorting. The unsupervised component uses contrastive learning to extract features from individual waveforms, while the supervised component is based on the MobileNetV2 architecture. One of the key advantages of our system is that it can be trained on multiple, diverse datasets simultaneously, resulting in greater generalizability than previous deep learning-based models. We demonstrate that the proposed model does not only reaches the accuracy of current state-of-art offline spike sorting methods but has the unique potential to run on edge Tensor Processing Units (TPUs), specialized chips designed for artificial intelligence and edge computing. We compare our model performance with state of art solutions on paired datasets as well as on hybrid recordings as well. The herein demonstrated system paves the way to the integration of deep learning-based spike sorting algorithms into wearable electronic devices, which will be a crucial element of high-end brain–computer interfaces.}
}
@article{SAKTHIVEL2023225,
title = {Disturbance rejection for multi-weighted complex dynamical networks with actuator saturation and deception attacks via hybrid-triggered mechanism},
journal = {Neural Networks},
volume = {162},
pages = {225-239},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.031},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001004},
author = {R. Sakthivel and O.M. Kwon and M.J. Park and S.M. Lee and R. Sakthivel},
keywords = {Complex dynamical networks, Equivalent-input-disturbance, Multi-weights, Actuator saturation, Deception attacks},
abstract = {In this work, we address hybrid-driven-based robust synchronization problem for multi-weighted complex dynamical networks with actuator saturation and deception attacks. The hybrid-triggered mechanism, which combines a switch between the event-triggered scheme and the time-triggered scheme, is often used to reduce the data transmission and the alleviate network burden. Further, the equivalent-input-disturbance technique is applied to eliminate the unknown disturbance effect of the addressed system. Moreover, a memory controller is designed under actuator saturation to ensure that the resultant augmented system is asymptotically synchronized even in the presence of deception attacks. Finally, three numerical examples are given to show the validity of the obtained theoretical results.}
}
@article{PAN2023638,
title = {Nonconvex low-rank tensor approximation with graph and consistent regularizations for multi-view subspace learning},
journal = {Neural Networks},
volume = {161},
pages = {638-658},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000795},
author = {Baicheng Pan and Chuandong Li and Hangjun Che},
keywords = {Multi-view clustering, Subspace clustering, Spectral clustering, Nonconvex low-rank tensor approximation},
abstract = {Multi-view clustering is widely used to improve clustering performance. Recently, the subspace clustering tensor learning method based on Markov chain is a crucial branch of multi-view clustering. Tensor learning is commonly used to apply tensor low-rank approximation to represent the relationships between data samples. However, most of the current tensor learning methods have the following shortcomings: the information of the local graph is not taken into account, the relationships between different views are not shown, and the existing tensor low-rank representation takes a biased tensor rank function for estimation. Therefore, a nonconvex low-rank tensor approximation with graph and consistent regularizations (NLRTGC) model is proposed for multi-view subspace learning. NLRTGC retains the local manifold information through graph regularization, and adopts a consistent regularization between multi-views to keep the diagonal block structure of representation matrices. Furthermore, a nonnegative nonconvex low-rank tensor kernel function is used to replace the existing classical tensor nuclear norm via tensor-singular value decomposition (t-SVD), so as to reduce the deviation from rank. Then, an alternating direction method of multipliers (ADMM) which makes the objective function monotonically non-increasing is proposed to solve NLRTGC. Finally, the effectiveness and superiority of the NLRTGC are shown through abundant comparative experiments with various state-of-the-art algorithms on noisy datasets and real world datasets.}
}
@article{QU2023494,
title = {Emphasizing unseen words: New vocabulary acquisition for end-to-end speech recognition},
journal = {Neural Networks},
volume = {161},
pages = {494-504},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.01.027},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000278},
author = {Leyuan Qu and Cornelius Weber and Stefan Wermter},
keywords = {Automatic speech recognition, Continual learning, Out-of-vocabulary word recognition, End-to-end learning, Loss rescaling},
abstract = {Due to the dynamic nature of human language, automatic speech recognition (ASR) systems need to continuously acquire new vocabulary. Out-Of-Vocabulary (OOV) words, such as trending words and new named entities, pose problems to modern ASR systems that require long training times to adapt their large numbers of parameters. Different from most previous research focusing on language model post-processing, we tackle this problem on an earlier processing level and eliminate the bias in acoustic modeling to recognize OOV words acoustically. We propose to generate OOV words using text-to-speech systems and to rescale losses to encourage neural networks to pay more attention to OOV words. Specifically, we enlarge the classification loss used for training neural networks’ parameters of utterances containing OOV words (sentence-level), or rescale the gradient used for back-propagation for OOV words (word-level), when fine-tuning a previously trained model on synthetic audio. To overcome catastrophic forgetting, we also explore the combination of loss rescaling and model regularization, i.e. L2 regularization and elastic weight consolidation (EWC). Compared with previous methods that just fine-tune synthetic audio with EWC, the experimental results on the LibriSpeech benchmark reveal that our proposed loss rescaling approach can achieve significant improvement on the recall rate with only a slight decrease on word error rate. Moreover, word-level rescaling is more stable than utterance-level rescaling and leads to higher recall rates and precision rates on OOV word recognition. Furthermore, our proposed combined loss rescaling and weight consolidation methods can support continual learning of an ASR system.}
}
@article{SUFRIYANA202399,
title = {Human-guided deep learning with ante-hoc explainability by convolutional network from non-image data for pregnancy prognostication},
journal = {Neural Networks},
volume = {162},
pages = {99-116},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.020},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000837},
author = {Herdiantri Sufriyana and Yu-Wei Wu and Emily Chia-Yu Su},
keywords = {Explainable artificial intelligence, Causal diagram, Deep learning, Electronic health records, Prelabor rupture of membranes},
abstract = {Background and Objective:
Deep learning is applied in medicine mostly due to its state-of-the-art performance for diagnostic imaging. Supervisory authorities also require the model to be explainable, but most explain the model after development (post hoc) instead of incorporating explanation into the design (ante hoc). This study aimed to demonstrate a human-guided deep learning with ante-hoc explainability by convolutional network from non-image data to develop, validate, and deploy a prognostic prediction model for PROM and an estimator of time of delivery using a nationwide health insurance database.
Methods:
To guide modeling, we constructed and verified association diagrams respectively from literatures and electronic health records. Non-image data were transformed into meaningful images utilizing predictor-to-predictor similarities, harnessing the power of convolutional neural network mostly used for diagnostic imaging. The network architecture was also inferred from the similarities.
Results:
This resulted the best model for prelabor rupture of membranes (n=883, 376) with the area under curves 0.73 (95% CI 0.72 to 0.75) and 0.70 (95% CI 0.69 to 0.71) respectively by internal and external validations, and outperformed previous models found by systematic review. It was explainable by knowledge-based diagrams and model representation.
Conclusions:
This allows prognostication with actionable insights for preventive medicine.}
}
@article{BAO2023312,
title = {Resilient fixed-time stabilization of switched neural networks subjected to impulsive deception attacks},
journal = {Neural Networks},
volume = {163},
pages = {312-326},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001806},
author = {Yuangui Bao and Yijun Zhang and Baoyong Zhang},
keywords = {Fixed-time stabilization, Switched neural networks, Impulsive systems, Impulsive deception attacks},
abstract = {This article focuses on the resilient fixed-time stabilization of switched neural networks (SNNs) under impulsive deception attacks. A novel theorem for the fixed-time stability of impulsive systems is established by virtue of the comparison principle. Existing fixed-time stability theorems for impulsive systems assume that the impulsive strength is not greater than 1, while the proposed theorem removes this assumption. SNNs subjected to impulsive deception attacks are modeled as impulsive systems. Some sufficient criteria are derived to ensure the stabilization of SNNs in fixed time. The estimation of the upper bound for the settling time is also given. The influence of impulsive attacks on the convergence time is discussed. A numerical example and an application to Chua’s circuit system are given to demonstrate the effectiveness of the theoretical results.}
}
@article{HAN2023670,
title = {TL-ADA: Transferable Loss-based Active Domain Adaptation},
journal = {Neural Networks},
volume = {161},
pages = {670-681},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000679},
author = {Kyeongtak Han and Youngeun Kim and Dongyoon Han and Hojun Lee and Sungeun Hong},
keywords = {Active Domain Adaptation, Loss prediction, Pseudo labels, Transferable query selection, Ranking loss},
abstract = {The field of Active Domain Adaptation (ADA) has been investigating ways to close the performance gap between supervised and unsupervised learning settings. Previous ADA research has primarily focused on query selection, but there has been little examination of how to effectively train newly labeled target samples using both labeled source samples and unlabeled target samples. In this study, we present a novel Transferable Loss-based ADA (TL-ADA) framework. Our approach is inspired by loss-based query selection, which has shown promising results in active learning. However, directly applying loss-based query selection to the ADA scenario leads to a buildup of high-loss samples that do not contribute to the model due to transferability issues and low diversity. To address these challenges, we propose a transferable doubly nested loss, which incorporates target pseudo labels and a domain adversarial loss. Our TL-ADA framework trains the model sequentially, considering both the domain type (source/target) and the availability of labels (labeled/unlabeled). Additionally, we encourage the pseudo labels to have low self-entropy and diverse class distributions to improve their reliability. Experiments on several benchmark datasets demonstrate that our TL-ADA model outperforms previous ADA methods, and in-depth analysis supports the effectiveness of our proposed approach.}
}
@article{EPIFANO2023581,
title = {Revisiting the fragility of influence functions},
journal = {Neural Networks},
volume = {162},
pages = {581-588},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.029},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001648},
author = {Jacob R. Epifano and Ravi P. Ramachandran and Aaron J. Masino and Ghulam Rasool},
keywords = {Machine learning, Supervised learning, Deep learning, Explainable AI, Influence functions, Bayesian neural networks},
abstract = {In the last few years, many works have tried to explain the predictions of deep learning models. Few methods, however, have been proposed to verify the accuracy or faithfulness of these explanations. Recently, influence functions, which is a method that approximates the effect that leave-one-out training has on the loss function, has been shown to be fragile. The proposed reason for their fragility remains unclear. Although previous work suggests the use of regularization to increase robustness, this does not hold in all cases. In this work, we seek to investigate the experiments performed in the prior work in an effort to understand the underlying mechanisms of influence function fragility. First, we verify influence functions using procedures from the literature under conditions where the convexity assumptions of influence functions are met. Then, we relax these assumptions and study the effects of non-convexity by using deeper models and more complex datasets. Here, we analyze the key metrics and procedures that are used to validate influence functions. Our results indicate that the validation procedures may cause the observed fragility.}
}
@article{WANG2023359,
title = {Hierarchical Attention Master–Slave for heterogeneous multi-agent reinforcement learning},
journal = {Neural Networks},
volume = {162},
pages = {359-368},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.037},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001090},
author = {Jiao Wang and Mingrui Yuan and Yun Li and Zihui Zhao},
keywords = {Multi-agent reinforcement learning, Communication, Heterogeneous agents, Self-attention, Cooperative games},
abstract = {Most multi-agent reinforcement learning (MARL) approaches optimize strategy by improving itself, while ignoring the limitations of homogeneous agents that may have single function. However, in reality, the complex tasks tend to coordinate various types of agents and leverage advantages from one another. Therefore, it is a vital research issue how to establish appropriate communication among them and optimize decision. To this end, we propose a Hierarchical Attention Master–Slave (HAMS) MARL, where the Hierarchical Attention balances the weight allocation within and among clusters, and the Master–Slave architecture endows agents independent reasoning and individual guidance. By the offered design, information fusion, especially among clusters, is implemented effectively, and excessive communication is avoided, moreover, selective composed action optimizes decision. We evaluate the HAMS on both small and large scale heterogeneous StarCraft II micromanagement tasks. The proposed algorithm achieves the exceptional performance with more than 80% win rates in all evaluation scenarios, which obtains an impressive win rate of over 90% in the largest map. The experiments demonstrate a maximum improvement in win rate of 47% over the best known algorithm. The results show that our proposal outperforms recent state-of-the-art approaches, which provides a novel idea for heterogeneous multi-agent policy optimization.}
}
@article{WANG2023318,
title = {LCM-Captioner: A lightweight text-based image captioning method with collaborative mechanism between vision and text},
journal = {Neural Networks},
volume = {162},
pages = {318-329},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001272},
author = {Qi Wang and Hongyu Deng and Xue Wu and Zhenguo Yang and Yun Liu and Yazhou Wang and Gefei Hao},
keywords = {Text-based image captioning, Multimodal information, Lightweight network, Collaborative attention mechanism, Feature transformation},
abstract = {Text-based image captioning (TextCap) aims to remedy the shortcomings of existing image captioning tasks that ignore text content when describing images. Instead, it requires models to recognize and describe images from both visual and textual content to achieve a deeper level of comprehension of the images. However, existing methods tend to use numerous complex network architectures to improve performance, which still fails to adequately model the relationship between vision and text on the one side, while on the other side this leads to long running times, high memory consumption, and other unfavorable deployment problems. To solve the above issues, we have developed a lightweight captioning method with a collaborative mechanism, LCM-Captioner, which balances high efficiency with high performance. First, we propose a feature-lightening transformation for the TextCap task, named TextLighT, which is able to learn rich multimodal representations while mapping features to lower dimensions, thereby reducing memory costs. Next, we present a collaborative attention module for visual and text information, VTCAM, to facilitate the semantic alignment of multimodal information to uncover important visual objects and textual content. Finally, the conducted extensive experiments on the TextCaps dataset demonstrate the effectiveness of our method. Code is available at https://github.com/DengHY258/LCM-Captioner.}
}
@article{ZHOU202334,
title = {Episodic task agnostic contrastive training for multi-task learning},
journal = {Neural Networks},
volume = {162},
pages = {34-45},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.023},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000862},
author = {Fan Zhou and Yuyi Chen and Jun Wen and Qiuhao Zeng and Changjian Shui and Charles X. Ling and Shichun Yang and Boyu Wang},
keywords = {Multi-task learning, Meta learning, Contrastive learning},
abstract = {Learning knowledge from different tasks to improve the general learning performance is crucial for designing an efficient algorithm. In this work, we tackle the Multi-task Learning (MTL) problem, where the learner extracts the knowledge from different tasks simultaneously with limited data. Previous works have been designing the MTL models by taking advantage of the transfer learning techniques, requiring the knowledge of the task index, which is not realistic in many practical scenarios. In contrast, we consider the scenario that the task index is not explicitly known, under which the features extracted by the neural networks are task agnostic. To learn the task agnostic invariant features, we implement model agnostic meta-learning by leveraging the episodic training scheme to capture the common features across tasks. Apart from the episodic training scheme, we further implemented a contrastive learning objective to improve the feature compactness for a better prediction boundary in the embedding space. We conduct extensive experiments on several benchmarks compared with several recent strong baselines to demonstrate the effectiveness of the proposed method. The results showed that our method provides a practical solution for real-world scenarios, where the task index is agnostic to the learner and can outperform several strong baselines, achieving state-of-the-art performances.}
}
@article{KAWAI2023298,
title = {Learning long-term motor timing/patterns on an orthogonal basis in random neural networks},
journal = {Neural Networks},
volume = {163},
pages = {298-311},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001831},
author = {Yuji Kawai and Jihoon Park and Ichiro Tsuda and Minoru Asada},
keywords = {Reservoir computing, Motor timing, Random neural network, Orthogonal basis, Modular neural network},
abstract = {The ability of the brain to generate complex spatiotemporal patterns with specific timings is essential for motor learning and temporal processing. An approach that can model this function, using the spontaneous activity of a random neural network (RNN), is associated with orbital instability. We propose a simple system that learns an arbitrary time series as the linear sum of stable trajectories produced by several small network modules. New finding in computer experiments is that the trajectories of the module outputs are orthogonal to each other. They created a dynamic orthogonal basis acquiring a high representational capacity, which enabled the system to learn the timing of extremely long intervals, such as tens of seconds for a millisecond computation unit, and also the complex time series of Lorenz attractors. This self-sustained system satisfies the stability and orthogonality requirements and thus provides a new neurocomputing framework and perspective for the neural mechanisms of motor learning.}
}
@article{ROY2023472,
title = {Deep learning-accelerated computational framework based on Physics Informed Neural Network for the solution of linear elasticity},
journal = {Neural Networks},
volume = {162},
pages = {472-489},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001314},
author = {Arunabha M. Roy and Rikhi Bose and Veera Sundararaghavan and Raymundo Arróyave},
keywords = {Physics Informed Neural Networks (PINNs), Artificial neural networks (ANNs), Linear elasticity, Bi-harmonic equations, Deep learning},
abstract = {The paper presents an efficient and robust data-driven deep learning (DL) computational framework developed for linear continuum elasticity problems. The methodology is based on the fundamentals of the Physics Informed Neural Networks (PINNs). For an accurate representation of the field variables, a multi-objective loss function is proposed. It consists of terms corresponding to the residual of the governing partial differential equations (PDE), constitutive relations derived from the governing physics, various boundary conditions, and data-driven physical knowledge fitting terms across randomly selected collocation points in the problem domain. To this end, multiple densely connected independent artificial neural networks (ANNs), each approximating a field variable, are trained to obtain accurate solutions. Several benchmark problems including the Airy solution to elasticity and the Kirchhoff–Love plate problem are solved. Performance in terms of accuracy and robustness illustrates the superiority of the current framework showing excellent agreement with analytical solutions. The present work combines the benefits of the classical methods depending on the physical information available in analytical relations with the superior capabilities of the DL techniques in the data-driven construction of lightweight, yet accurate and robust neural networks. The models developed herein can significantly boost computational speed using minimal network parameters with easy adaptability in different computational platforms.}
}
@article{YIN2023379,
title = {Adams-based hierarchical features fusion network for image dehazing},
journal = {Neural Networks},
volume = {163},
pages = {379-394},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001557},
author = {Shibai Yin and Shuhao Hu and Yibin Wang and Weixing Wang and Yee-Hong Yang},
keywords = {Image dehazing, Ordinary differential equation, Spatial attention, CNN},
abstract = {Recent developments in Convolutional Neural Networks (CNNs) have made them one of the most powerful image dehazing methods. In particular, the Residual Networks (ResNets), which can avoid the vanishing gradient problem effectively, are widely deployed. To understand the success of ResNets, recent mathematical analysis of ResNets reveals that a ResNet has a similar formulation as the Euler method in solving the Ordinary Differential Equations (ODE’s). Hence, image dehazing which can be formulated as an optimal control problem in dynamical systems can be solved by a single-step optimal control method, such as the Euler method. This optimal control viewpoint provides a new perspective to address the problem of image restoration. Motivated by the advantages of multi-step optimal control solvers in ODE’s, which include better stability and efficiency than single-step solvers, e.g. Euler, we propose the Adams-based Hierarchical Feature Fusion Network (AHFFN) for image dehazing with modules inspired by a multi-step optimal control method named the Adams–Bashforth method. Firstly, we extend a multi-step Adams–Bashforth method to the corresponding Adams block, which achieves a higher accuracy than that of single-step solvers because of its more effective use of intermediate results. Then, we stack multiple Adams blocks to mimic the discrete approximation process of an optimal control in a dynamical system. To improve the results, the hierarchical features from stacked Adams blocks are fully used by combining Hierarchical Feature Fusion (HFF) and Lightweight Spatial Attention (LSA) with Adams blocks to form a new Adams module. Finally, we not only use HFF and LSA to fuse features, but also highlight important spatial information in each Adams module for estimating the clear image. The experimental results using synthetic and real images demonstrate that the proposed AHFFN obtains better accuracy and visual results than that of state-of-the-art methods.}
}
@article{CAO202328,
title = {Adaptive fixed-time output synchronization for complex dynamical networks with multi-weights},
journal = {Neural Networks},
volume = {163},
pages = {28-39},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.032},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001673},
author = {Yuting Cao and Linhao Zhao and Qishui Zhong and Shiping Wen and Kaibo Shi and Jianying Xiao and Tingwen Huang},
keywords = {Adaptive control, Complex dynamical networks, Fixed-time output synchronization, Multiple weights},
abstract = {This paper addresses fixed-time output synchronization problems for two types of complex dynamical networks with multi-weights (CDNMWs) by using two types of adaptive control methods. Firstly, complex dynamical networks with multiple state and output couplings are respectively presented. Secondly, several fixed-time output synchronization criteria for these two networks are formulated based on Lyapunov functional and inequality techniques. Thirdly, by employing two types of adaptive control methods, fixed-time output synchronization issues of these two networks are dealt with. At last, the analytical results are verified by two numerical simulations.}
}
@article{COSCRATO2023117,
title = {NLS: An accurate and yet easy-to-interpret prediction method},
journal = {Neural Networks},
volume = {162},
pages = {117-130},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.043},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001156},
author = {Victor Coscrato and Marco H.A. Inácio and Tiago Botari and Rafael Izbicki},
keywords = {Machine-learning, Neural networks, Interpretation, Explainable ML},
abstract = {Over the last years, the predictive power of supervised machine learning (ML) has undergone impressive advances, achieving the status of state of the art and super-human level in some applications. However, the employment rate of ML models in real-life applications is much slower than one would expect. One of the downsides of using ML solution-based technologies is the lack of user trust in the produced model, which is related to the black-box nature of these models. To leverage the application of ML models, the generated predictions should be easy to interpret while maintaining a high accuracy. In this context, we develop the Neural Local Smoother (NLS), a neural network architecture that yields accurate predictions with easy-to-obtain explanations. The key idea of NLS is to add a smooth local linear layer to a standard network. We show experiments that indicate that NLS leads to a predictive power that is comparable to state-of-the-art machine learning models, but that at the same time is easier to interpret.}
}
@article{YANG2023165,
title = {Lifelong learning with Shared and Private Latent Representations learned through synaptic intelligence},
journal = {Neural Networks},
volume = {163},
pages = {165-177},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S089360802300182X},
author = {Yang Yang and Jie Huang and Dexiu Hu},
keywords = {Shared and Private Latent Representations, Synaptic Intelligence, Lifelong learning, Entire learning trajectory, Task-invariant, Task-specific},
abstract = {This paper explores a novel lifelong learning method with Shared and Private Latent Representations (SPLR), which are learned through synaptic intelligence. To solve a sequence of tasks, by considering the entire parameter learning trajectory, SPLR can learn task-invariant representation which changes little, and task-specific features that change greatly along the entire parameter updating trajectory. Therefore, in the lifelong learning scenarios, our model can obtain a task-invariant structure shared by all tasks and also contain some private properties that are task-specific to each task. To reduce the parameter quantity, a ℓ1 regularization to promote sparsity is employed in the weights. We use multiple datasets under lifelong learning scenes to verify our SPLR, on these datasets it can get comparable performance compared with existing lifelong learning approaches, and learn a sparse network which means fewer parameters while requiring less model training time.}
}
@article{LIANG202321,
title = {Multi-UAV autonomous collision avoidance based on PPO-GIC algorithm with CNN–LSTM fusion network},
journal = {Neural Networks},
volume = {162},
pages = {21-33},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.027},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000904},
author = {Chengqing Liang and Lei Liu and Chen Liu},
keywords = {Autonomous collision avoidance strategy, Multi-UAV, Long short-term memory network, Deep reinforcement learning, Generalized integral compensator},
abstract = {This paper is concerned with the autonomous effective collision avoidance strategy for multiple unmanned aerial vehicles (multi-UAV) in limited airspace under the framework of proximal policy optimization (PPO) algorithm. An end-to-end deep reinforcement learning (DRL) control strategy and a potential-based reward function are designed. Next, the CNN-LSTM (CL) fusion network is constructed by fusing the convolutional neural network (CNN) and the long short-term memory network (LSTM), which realizes the feature interaction among the information of multi-UAV. Then, a generalized integral compensator (GIC) is introduced into the actor-critic structure, and the CLPPO-GIC algorithm is proposed by combining CL and GIC. Finally, we validate the learned policy in various simulation environments by performance evaluation. The simulation results show that the introduction of the LSTM network and GIC can further improve the efficiency of collision avoidance, and the robustness and accuracy of the algorithm are verified in different environments.}
}
@article{LIU2023693,
title = {A neurodynamic approach for nonsmooth optimal power consumption of intelligent and connected vehicles},
journal = {Neural Networks},
volume = {161},
pages = {693-707},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000746},
author = {Jingxin Liu and Xiaofeng Liao and Jin-song Dong and Amin Mansoori},
keywords = {Neurodynamic approach, Distributed optimization, Nonsmooth analysis, Intelligent and connected vehicles, Power consumption},
abstract = {This paper investigates a class of power consumption minimization and equalization for intelligent and connected vehicles cooperative system. Accordingly, a distributed optimization problem model related to power consumption and data rate of intelligent and connected vehicles is presented, where the power consumption cost function of each intelligent and connected vehicle may be nonsmooth, and the corresponding control variable is subject to the constraints generated by data acquisition, compression coding, transmission and reception. We propose a distributed subgradient-based neurodynamic approach with projection operator to achieve the optimal power consumption of intelligent and connected vehicles. By differential inclusion and nonsmooth analysis, it is confirmed that the state solution of neurodynamic system converges to the optimal solution of the distributed optimization problem. With the help of the algorithm, all intelligent and connected vehicles asymptotically reach a consensus on an optimal power consumption. Simulation results show that the proposed neurodynamic approach is capable of effectively solving the problem of power consumption optimal control for intelligent and connected vehicles cooperative system.}
}
@article{WANG202375,
title = {Fixed-time synchronization of delayed memristive neural networks with impulsive effects via novel fixed-time stability theorem},
journal = {Neural Networks},
volume = {163},
pages = {75-85},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.036},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001715},
author = {Dongshu Wang and Luke Li},
keywords = {Memristive neural networks, Fixed-time stability, Fixed-time synchronization, Time-varying impulse strength, Hybrid impulsive effects},
abstract = {In this study, the fixed-time synchronization (FXTS) of delayed memristive neural networks (MNNs) with hybrid impulsive effects is explored. To investigate the FXTS mechanism, we first propose a novel theorem about the fixed-time stability (FTS) of impulsive dynamical systems, where the coefficients are extended to functions and the derivatives of Lyapunov function (LF) are allowed to be indefinite. After that, we obtain some new sufficient conditions for achieving FXTS of the system within a settling-time using three different controllers. At last, to verify the correctness and effectiveness of our results, a numerical simulation was conducted. Significantly, the impulse strength studied in this paper can take different values at different points, so it can be regarded as a time-varying function, unlike those in previous studies (the impulse strength takes the same value at different points). Hence, the mechanisms in this article are of more practical applicability.}
}
@article{ZHU202383,
title = {An effective knowledge graph entity alignment model based on multiple information},
journal = {Neural Networks},
volume = {162},
pages = {83-98},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.029},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000989},
author = {Beibei Zhu and Tie Bao and Ridong Han and Hai Cui and Jiayu Han and Lu Liu and Tao Peng},
keywords = {Entity alignment, Knowledge graph, Structure, Semantic, String},
abstract = {Entity alignment refers to matching entities with the same realistic meaning in different knowledge graphs. The structure of a knowledge graph provides the global signal for entity alignment. But in the real world, a knowledge graph provides insufficient structural information in general. Moreover, the problem of knowledge graph heterogeneity is common. The semantic and string information can alleviate the problems caused by the sparse and heterogeneous nature of knowledge graphs, yet both of them have not been fully utilized by most existing work. Therefore, we propose an entity alignment model based on multiple information (EAMI), which employs structural, semantic and string information. EAMI learns the structural representation of a knowledge graph by using multi-layer graph convolutional networks. To acquire more accurate entity vector representation, we incorporate the attribute semantic representation into the structural representation. In addition, to further improve entity alignment, we study the entity name string information. There is no training required to calculate the similarity of entity names. Our model is tested on publicly available cross-lingual datasets and cross-resource datasets, and the experimental results demonstrate the effectiveness of our model.}
}
@article{WANG2023614,
title = {Data augmentation with norm-AE and selective pseudo-labelling for unsupervised domain adaptation},
journal = {Neural Networks},
volume = {161},
pages = {614-625},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000692},
author = {Qian Wang and Fanlin Meng and Toby P. Breckon},
keywords = {Unsupervised Domain Adaptation, Data augmentation, Variational autoencoder, Selective Pseudo-Labelling},
abstract = {We address the Unsupervised Domain Adaptation (UDA) problem in image classification from a new perspective. In contrast to most existing works which either align the data distributions or learn domain-invariant features, we directly learn a unified classifier for both the source and target domains in the high-dimensional homogeneous feature space without explicit domain alignment. To this end, we employ the effective Selective Pseudo-Labelling (SPL) technique to take advantage of the unlabelled samples in the target domain. Surprisingly, data distribution discrepancy across the source and target domains can be well handled by a computationally simple classifier (e.g., a shallow Multi-Layer Perceptron) trained in the original feature space. Besides, we propose a novel generative model norm-AE to generate synthetic features for the target domain as a data augmentation strategy to enhance the classifier training. Experimental results on several benchmark datasets demonstrate the pseudo-labelling strategy itself can lead to comparable performance to many state-of-the-art methods whilst the use of norm-AE for feature augmentation can further improve the performance in most cases. As a result, our proposed methods (i.e. naive-SPL and norm-AE-SPL) can achieve comparable performance with state-of-the-art methods with the average accuracy of 93.4% and 90.4% on Office-Caltech and ImageCLEF-DA datasets, and achieve competitive performance on Digits, Office31 and Office-Home datasets with the average accuracy of 97.2%, 87.6% and 68.6% respectively.}
}
@article{CHEN2023340,
title = {Traffic forecasting with graph spatial–temporal position recurrent network},
journal = {Neural Networks},
volume = {162},
pages = {340-349},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001260},
author = {Yibi Chen and Kenli Li and Chai Kiat Yeo and Keqin Li},
keywords = {Adaptive graph learning, Approximate personalized propagation, Spatial–temporal, Traffic forecasting, Position graph convolution},
abstract = {With the development of social economy and smart technology, the explosive growth of vehicles has caused traffic forecasting to become a daunting challenge, especially for smart cities. Recent methods exploit graph spatial–temporal characteristics, including constructing the shared patterns of traffic data, and modeling the topological space of traffic data. However, existing methods fail to consider the spatial position information and only utilize little spatial neighborhood information. To tackle above limitation, we design a Graph Spatial–Temporal Position Recurrent Network (GSTPRN) architecture for traffic forecasting. We first construct a position graph convolution module based on self-attention and calculate the dependence strengths among the nodes to capture the spatial dependence relationship. Next, we develop approximate personalized propagation that extends the propagation range of spatial dimension information to obtain more spatial neighborhood information. Finally, we systematically integrate the position graph convolution, approximate personalized propagation and adaptive graph learning into a recurrent network (i.e. Gated Recurrent Units). Experimental evaluation on two benchmark traffic datasets demonstrates that GSTPRN is superior to the state-of-art methods.}
}
@article{HUANG2023233,
title = {Incomplete multi-view clustering network via nonlinear manifold embedding and probability-induced loss},
journal = {Neural Networks},
volume = {163},
pages = {233-243},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001302},
author = {Cheng Huang and Jinrong Cui and Yulu Fu and Dong Huang and Min Zhao and Lusi Li},
keywords = {Incomplete multi-view clustering, Deep clustering, Consistent learning, Manifold learning, Gaussian mixture models},
abstract = {Incomplete multi-view clustering, which included missing data in different views, is more challenging than multi-view clustering. For the purpose of eliminating the negative influence of incomplete data, researchers have proposed a series of solutions. However, the present incomplete multi-view clustering methods still confront three major issues: (1) The interference of redundant features hinders these methods to learn the most discriminative features. (2) The importance role of local structure is not considered during clustering. (3) These methods fail to utilize data distribution information to guide models update to decrease the effects of outliers and noise. To address above issues, a novel deep clustering network which exerted on incomplete multi-view data was proposed in this paper. We combine multi-view autoencoders with nonlinear manifold embedding method UMAP to extract latent consistent features of incomplete multi-view data. In the clustering method, we introduce Gaussian Mixture Model (GMM) to fit the complex distribution of data and deal with the interference of outliers. In addition, we reasonably utilize the probability distribution information generated by GMM, using probability-induced loss function to integrate feature learning and clustering as a joint framework. In experiments conducted on multiple benchmark datasets, our method captures incomplete multi-view data features effectively and perform excellent.}
}
@article{NAKAMURA2023516,
title = {Decoding self-motion from visual image sequence predicts distinctive features of reflexive motor responses to visual motion},
journal = {Neural Networks},
volume = {162},
pages = {516-530},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.020},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001545},
author = {Daiki Nakamura and Hiroaki Gomi},
keywords = {Visual motion coding, Manual following response, Ocular following response, Visuomotor response, Spatiotemporal frequency tuning, Convolutional neural network},
abstract = {Visual motion analysis is crucial for humans to detect external moving objects and self-motion which are informative for planning and executing actions for various interactions with environments. Here we show that the image motion analysis trained to decode the self-motion during human natural movements by a convolutional neural network exhibits similar specificities with the reflexive ocular and manual responses induced by a large-field visual motion, in terms of stimulus spatiotemporal frequency tuning. The spatiotemporal frequency tuning of the decoder peaked at high-temporal and low-spatial frequencies, as observed in the reflexive ocular and manual responses, but differed significantly from the frequency power of the visual image itself and the density distribution of self-motion. Further, artificial manipulations of the learning data sets predicted great changes in the specificity of the spatiotemporal tuning. Interestingly, despite similar spatiotemporal frequency tunings in the vertical-axis rotational direction and in the transversal direction to full-field visual stimuli, the tunings for center-masked stimuli were different between those directions, and the specificity difference is qualitatively similar to the discrepancy between ocular and manual responses, respectively. In addition, the representational analysis demonstrated that head-axis rotation was decoded by relatively simple spatial accumulation over the visual field, while the transversal motion was decoded by more complex spatial interaction of visual information. These synthetic model examinations support the idea that visual motion analyses eliciting the reflexive motor responses, which are critical in interacting with the external world, are acquired for decoding self-motion.}
}
@article{BAI202311,
title = {ProMask: Probability mask representation for skeleton detection},
journal = {Neural Networks},
volume = {162},
pages = {11-20},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.033},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001028},
author = {Xiuxiu Bai and Lele Ye and Zhe Liu and Bin Liu},
keywords = {Skeleton detection, Convolutional neural network, Robustness, Probability representation},
abstract = {Detecting object skeletons in natural images presents challenges due to varied object scales and complex backgrounds. The skeleton is a highly compressing shape representation, which can bring some essential advantages but cause difficulties in detection. This skeleton line occupies a small part of the image and is overly sensitive to spatial position. Inspired by these issues, we propose the ProMask, which is a novel skeleton detection model. The ProMask includes the probability mask representation and vector router. This skeleton probability mask describes the gradual formation process of skeleton points, which can achieve high detection performance and robustness. Moreover, the vector router module possesses two sets of orthogonal basis vectors in a two-dimensional space, which can dynamically adjust the predicted skeleton position. Experiments show that our approach realizes better performance, efficiency, and robustness than state-of-the-art methods. We consider that our proposed skeleton probability representation will serve as a standard configuration for future skeleton detection, since it is reasonable, simple, and very effective.}
}
@article{LIANG2023156,
title = {Graph contrastive learning with implicit augmentations},
journal = {Neural Networks},
volume = {163},
pages = {156-164},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001788},
author = {Huidong Liang and Xingjian Du and Bilei Zhu and Zejun Ma and Ke Chen and Junbin Gao},
keywords = {Graph neural networks, Contrastive learning, Latent augmentations, Graph auto-encoders},
abstract = {Existing graph contrastive learning methods rely on augmentation techniques based on random perturbations (e.g., randomly adding or dropping edges and nodes). Nevertheless, altering certain edges or nodes can unexpectedly change the graph characteristics, and choosing the optimal perturbing ratio for each dataset requires onerous manual tuning. In this paper, we introduce Implicit Graph Contrastive Learning (iGCL), which utilizes augmentations in the latent space learned from a Variational Graph Auto-Encoder by reconstructing graph topological structure. Importantly, instead of explicitly sampling augmentations from latent distributions, we further propose an upper bound for the expected contrastive loss to improve the efficiency of our learning algorithm. Thus, graph semantics can be preserved within the augmentations in an intelligent way without arbitrary manual design or prior human knowledge. Experimental results on both graph-level and node-level show that the proposed method achieves state-of-the-art accuracy on downstream classification tasks compared to other graph contrastive baselines, where ablation studies in the end demonstrate the effectiveness of modules in iGCL.}
}
@article{YANG202353,
title = {Fixed/prescribed-time synchronization of BAM memristive neural networks with time-varying delays via convex analysis},
journal = {Neural Networks},
volume = {163},
pages = {53-63},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.031},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001661},
author = {Jinrong Yang and Guici Chen and Song Zhu and Shiping Wen and Junhao Hu},
keywords = {Fixed-time synchronization, Prescribed-time synchronization, Memristive neural networks, BAM neural networks},
abstract = {The synchronization problem of bidirectional associative memory memristive neural networks (BAMMNNs) with time-varying delays plays an essential role in the implementation and application of neural networks. Firstly, under the framework of the Filippov’s solution, the discontinuous parameters of the state-dependent switching are transformed by convex analysis method, which is different from most previous approaches. Secondly, based on Lyapunov function and some inequality techniques, several conditions for the fixed-time synchronization (FXTS) of the drive-response systems are obtained by designing special control strategies. Moreover, the settling time (ST) is estimated by the improved fixed-time stability lemma. Thirdly, the driven-response BAMMNNs are investigated to be synchronized within a prescribed time by designing new controllers based on the FXTS results, where ST is irrelevant to the initial values of BAMMNNs and the parameters of controllers. Finally, a numerical simulation is exhibited to verify the correctness of the conclusions.}
}
@article{KO2023330,
title = {SuperstarGAN: Generative adversarial networks for image-to-image translation in large-scale domains},
journal = {Neural Networks},
volume = {162},
pages = {330-339},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.042},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001144},
author = {Kanghyeok Ko and Taesun Yeom and Minhyeok Lee},
keywords = {Generative adversarial networks, Image-to-image translation, Domain translation, Face image translation, Image generation},
abstract = {Image-to-image translation with generative adversarial networks (GANs) has been extensively studied in recent years. Among the models, StarGAN has achieved image-to-image translation for multiple domains with a single generator, whereas conventional models require multiple generators. However, StarGAN has several limitations, including the lack of capacity to learn mappings among large-scale domains; furthermore, StarGAN can barely express small feature changes. To address the limitations, we propose an improved StarGAN, namely SuperstarGAN. We adopted the idea, first proposed in controllable GAN (ControlGAN), of training an independent classifier with the data augmentation techniques to handle the overfitting problem in the classification of StarGAN structures. Since the generator with a well-trained classifier can express small features belonging to the target domain, SuperstarGAN achieves image-to-image translation in large-scale domains. Evaluated with a face image dataset, SuperstarGAN demonstrated improved performance in terms of Fréchet Inception distance (FID) and learned perceptual image patch similarity (LPIPS). Specifically, compared to StarGAN, SuperstarGAN exhibited decreased FID and LPIPS by 18.1% and 42.5%, respectively. Furthermore, we conducted an additional experiment with interpolated and extrapolated label values, indicating the ability of SuperstarGAN to control the degree of expression of the target domain features in generated images. Additionally, SuperstarGAN was successfully adapted to an animal face dataset and a painting dataset, where it can translate styles of animal faces (i.e., a cat to a tiger) and styles of painters (i.e., Hassam to Picasso), respectively, which explains the generality of SuperstarGAN regardless of datasets.}
}
@article{BALA2023757,
title = {MonkeyNet: A robust deep convolutional neural network for monkeypox disease detection and classification},
journal = {Neural Networks},
volume = {161},
pages = {757-775},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000850},
author = {Diponkor Bala and Md. Shamim Hossain and Mohammad Alamgir Hossain and Md. Ibrahim Abdullah and Md. Mizanur Rahman and Balachandran Manavalan and Naijie Gu and Mohammad S. Islam and Zhangjin Huang},
keywords = {Monkeypox disease, Dataset, Machine learning, Deep learning, Convolutional neural network, Classification},
abstract = {The monkeypox virus poses a new pandemic threat while we are still recovering from COVID-19. Despite the fact that monkeypox is not as lethal and contagious as COVID-19, new patient cases are recorded every day. If preparations are not made, a global pandemic is likely. Deep learning (DL) techniques are now showing promise in medical imaging for figuring out what diseases a person has. The monkeypox virus-infected human skin and the region of the skin can be used to diagnose the monkeypox early because an image has been used to learn more about the disease. But there is still no reliable Monkeypox database that is available to the public that can be used to train and test DL models. As a result, it is essential to collect images of monkeypox patients. The “MSID” dataset, short form of “Monkeypox Skin Images Dataset”, which was developed for this research, is free to use and can be downloaded from the Mendeley Data database by anyone who wants to use it. DL models can be built and used with more confidence using the images in this dataset. These images come from a variety of open-source and online sources and can be used for research purposes without any restrictions. Furthermore, we proposed and evaluated a modified DenseNet-201 deep learning-based CNN model named MonkeyNet. Using the original and augmented datasets, this study suggested a deep convolutional neural network that was able to correctly identify monkeypox disease with an accuracy of 93.19% and 98.91% respectively. This implementation also shows the Grad-CAM which indicates the level of the model’s effectiveness and identifies the infected regions in each class image, which will help the clinicians. The proposed model will also help doctors make accurate early diagnoses of monkeypox disease and protect against the spread of the disease.}
}
@article{ZANG2023746,
title = {Learning to Generate Tips from Song Reviews},
journal = {Neural Networks},
volume = {161},
pages = {746-756},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.01.049},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000618},
author = {Jingya Zang and Cuiyun Gao and Yupan Chen and Ruifeng Xu and Lanjun Zhou and Xuan Wang},
keywords = {Tip generation, Review ranking, Pre-trained model, Song reviews, Data annotation},
abstract = {Reviews of songs play an important role in online music service platforms. Prior research shows that users can make quicker and more informed decisions when presented with meaningful song reviews. However, reviews of songs are generally long in length and most of them are non-informative for users. It is difficult for users to efficiently grasp meaningful messages for making decisions. To solve this problem, one practical strategy is to provide tips, i.e., short, concise, empathetic, and self-contained descriptions about songs. Tips are produced from song reviews and should express non-trivial insights about the songs. To the best of our knowledge, no prior studies have explored the tip generation task in music domain. In this paper, we create a dataset named MTips for the task and propose a learning-to-generate framework named GenTMS for automatically generating tips from song reviews. The dataset involves 8,003 Chinese tips/non-tips from 128 songs which are distributed in five different song genres. Experimental results show that GenTMS achieves top-10 precision at 85.56%, outperforming the baseline models by at least 3.34%. Besides, to simulate the practical usage of our proposed framework, we also experiment with previously-unseen songs, during which GenTMS also achieves the best performance with top-10 precision at 78.89% on average. The results demonstrate the effectiveness of the proposed framework in tip generation of the music domain.}
}
@article{COPPOLINO202397,
title = {An explainable artificial intelligence approach to spatial navigation based on hippocampal circuitry},
journal = {Neural Networks},
volume = {163},
pages = {97-107},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.030},
url = {https://www.sciencedirect.com/science/article/pii/S089360802300165X},
author = {Simone Coppolino and Michele Migliore},
keywords = {Robot spatial navigation, Spike-time-dependent plasticity, Hippocampal circuitry, Spiking neurons network},
abstract = {Learning to navigate a complex environment is not a difficult task for a mammal. For example, finding the correct way to exit a maze following a sequence of cues, does not need a long training session. Just a single or a few runs through a new environment is, in most cases, sufficient to learn an exit path starting from anywhere in the maze. This ability is in striking contrast with the well-known difficulty that any deep learning algorithm has in learning a trajectory through a sequence of objects. Being able to learn an arbitrarily long sequence of objects to reach a specific place could take, in general, prohibitively long training sessions. This is a clear indication that current artificial intelligence methods are essentially unable to capture the way in which a real brain implements a cognitive function. In previous work, we have proposed a proof-of-principle model demonstrating how, using hippocampal circuitry, it is possible to learn an arbitrary sequence of known objects in a single trial. We called this model SLT (Single Learning Trial). In the current work, we extend this model, which we will call e-STL, to introduce the capability of navigating a classic four-arms maze to learn, in a single trial, the correct path to reach an exit ignoring dead ends. We show the conditions under which the e-SLT network, including cells coding for places, head-direction, and objects, can robustly and efficiently implement a fundamental cognitive function. The results shed light on the possible circuit organization and operation of the hippocampus and may represent the building block of a new generation of artificial intelligence algorithms for spatial navigation.}
}
@article{RUBIO2023437,
title = {Bat algorithm based control to decrease the control energy consumption and modified bat algorithm based control to increase the trajectory tracking accuracy in robots},
journal = {Neural Networks},
volume = {161},
pages = {437-448},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000734},
author = {José de Jesús Rubio},
keywords = {Modified bat algorithm based control, Bat algorithm based control, Simplex algorithm based control, Control energy consumption, Trajectory tracking accuracy, Robots},
abstract = {From the control theory, the best control gain produces a balance between the trajectory tracking accuracy and control energy consumption. The random search of the bat algorithm is one alternative to find the best control gain. In this paper, (1) a bat algorithm based control is proposed to decrease the control energy consumption in robots, where a bat algorithm is used to find the best control gain; and (2) a modified bat algorithm based control is proposed to increase the trajectory tracking accuracy in robots, where a modified bat algorithm is used to find the best control gain. The comparison between the two proposed controls and the simplex based control is illustrated for the trajectory tracking accuracy and control energy consumption in two robots.}
}
@article{SAGAWA2023731,
title = {Cost-effective framework for gradual domain adaptation with multifidelity},
journal = {Neural Networks},
volume = {164},
pages = {731-741},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.035},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001703},
author = {Shogo Sagawa and Hideitsu Hino},
keywords = {Gradual domain adaptation, Active learning, Multifidelity learning},
abstract = {In domain adaptation, when there is a large distance between the source and target domains, the prediction performance will degrade. Gradual domain adaptation is one of the solutions to such an issue, assuming that we have access to intermediate domains, which shift gradually from the source to the target domain. In previous works, it was assumed that the number of samples in the intermediate domains was sufficiently large; hence, self-training was possible without the need for labeled data. If the number of accessible intermediate domains is restricted, the distances between domains become large, and self-training will fail. Practically, the cost of samples in intermediate domains will vary, and it is natural to consider that the closer an intermediate domain is to the target domain, the higher the cost of obtaining samples from the intermediate domain is. To solve the trade-off between cost and accuracy, we propose a framework that combines multifidelity and active domain adaptation. The effectiveness of the proposed method is evaluated by experiments with real-world datasets.}
}
@article{ZHOU2023244,
title = {On the value of label and semantic information in domain generalization},
journal = {Neural Networks},
volume = {163},
pages = {244-255},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.023},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001570},
author = {Fan Zhou and Yuyi Chen and Shichun Yang and Boyu Wang and Brahim Chaib-draa},
keywords = {Domain generalization, Conditional matching, Label and semantic information},
abstract = {In this work, we tackle the domain generalization (DG) problem aiming to learn a universal predictor on several source domains and deploy it on an unseen target domain. Many existing DG approaches were mainly motivated by domain adaptation techniques to align the marginal feature distribution but ignored conditional relations and labeling information in the source domains, which are critical to ensure successful knowledge transfer. Although some recent advances started to take advantage of conditional semantic distributions, theoretical justifications were still missing. To this end, we investigate the theoretical guarantee for a successful generalization process by focusing on how to control the target domain error. Our results reveal that to control the target risk, one should jointly control the source errors that are weighted according to label information and align the semantic conditional distributions between different source domains. The theoretical analysis then leads to an efficient algorithm to control the label distributions as well as match the semantic conditional distributions. To verify the effectiveness of our method, we evaluate it against recent baseline algorithms on several benchmarks. We also conducted experiments to verify the performance under label distribution shift to demonstrate the necessity of leveraging the labeling and semantic information. Empirical results show that the proposed method outperforms most of the baseline methods and shows state-of-the-art performances.}
}
@article{ZHOU2023502,
title = {Miper-MVS: Multi-scale iterative probability estimation with refinement for efficient multi-view stereo},
journal = {Neural Networks},
volume = {162},
pages = {502-515},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001296},
author = {Huizhou Zhou and Haoliang Zhao and Qi Wang and Gefei Hao and Liang Lei},
keywords = {Multi-view stereo, 3D reconstruction, Depth estimation, Stereo vision},
abstract = {Multi-view stereo reconstruction aims to construct 3D scenes from multiple 2D images. In recent years, learning-based multi-view stereo methods have achieved significant results in depth estimation for multi-view stereo reconstruction. However, the current popular multi-stage processing method cannot solve the low-efficiency problem satisfactorily owing to the use of 3D convolution and still involves significant amounts of calculation. Therefore, to further balance the efficiency and generalization performance, this study proposed a multi-scale iterative probability estimation with refinement, which is a highly efficient method for multi-view stereo reconstruction. It comprises three main modules: 1) a high-precision probability estimator, dilated-LSTM that encodes the pixel probability distribution of depth in the hidden state, 2) an efficient interactive multi-scale update module that fully integrates multi-scale information and improves parallelism by interacting information between adjacent scales, and 3) a Pi-error Refinement module that converts the depth error between views into a grayscale error map and refines the edges of objects in the depth map. Simultaneously, we introduced a large amount of high-frequency information to ensure the accuracy of the refined edges. Among the most efficient methods (e.g., runtime and memory), the proposed method achieved the best generalization on the Tanks & Temples benchmarks. Additionally, the performance of the Miper-MVS was highly competitive in DTU benchmark. Our code is available at https://github.com/zhz120/Miper-MVS.}
}
@article{VERWIMP2023659,
title = {CLAD: A realistic Continual Learning benchmark for Autonomous Driving},
journal = {Neural Networks},
volume = {161},
pages = {659-669},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000643},
author = {Eli Verwimp and Kuo Yang and Sarah Parisot and Lanqing Hong and Steven McDonagh and Eduardo Pérez-Pellitero and Matthias {De Lange} and Tinne Tuytelaars},
keywords = {Continual learning, Classification, Object detection, Challenge report, Benchmark},
abstract = {In this paper we describe the design and the ideas motivating a new Continual Learning benchmark for Autonomous Driving (CLAD), that focuses on the problems of object classification and object detection. The benchmark utilises SODA10M, a recently released large-scale dataset that concerns autonomous driving related problems. First, we review and discuss existing continual learning benchmarks, how they are related, and show that most are extreme cases of continual learning. To this end, we survey the benchmarks used in continual learning papers at three highly ranked computer vision conferences. Next, we introduce CLAD-C, an online classification benchmark realised through a chronological data stream that poses both class and domain incremental challenges; and CLAD-D, a domain incremental continual object detection benchmark. We examine the inherent difficulties and challenges posed by the benchmark, through a survey of the techniques and methods used by the top-3 participants in a CLAD-challenge workshop at ICCV 2021. We conclude with possible pathways to improve the current continual learning state of the art, and which directions we deem promising for future research.}
}
@article{TIAN202369,
title = {Deep learning-based open set multi-source domain adaptation with complementary transferability metric for mechanical fault diagnosis},
journal = {Neural Networks},
volume = {162},
pages = {69-82},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.025},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000886},
author = {Jinghui Tian and Dongying Han and Hamid Reza Karimi and Yu Zhang and Peiming Shi},
keywords = {Rotating machinery, Fault diagnosis, Multi-source information, Distribution difference, Open set domain adaptation},
abstract = {Intelligent fault diagnosis aims to build robust mechanical condition recognition models with limited dataset. At this stage, fault diagnosis faces two practical challenges: (1) the variability of mechanical working conditions makes the collected data distribution inconsistent, which brings about the domain shift; (2) some unpredictable unknown fault modes that do not observe in the training dataset may occur in the testing scenario, leading to a category gap. In order to cope with these two entangled challenges, an open set multi-source domain adaptation approach is developed in this study. Specifically, a complementary transferability metric defined on multiple classifiers is introduced to quantify the similarity of each target sample to known classes to weight the adversarial mechanism. By applying an unknown mode detector, unknown faults can be automatically identified. Moreover, a multi-source mutual-supervised strategy is further adopted to mine relevant information between different sources to enhance the model performance. Extensive experiments are conducted on three rotating machinery datasets, and the results show that the proposed method is superior to traditional domain adaptation approaches in the mechanical diagnosis issues that new fault modes occur.}
}
@article{ZHANG2023490,
title = {Improved disturbance observer-based fixed-time adaptive neural network consensus tracking for nonlinear multi-agent systems},
journal = {Neural Networks},
volume = {162},
pages = {490-501},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001338},
author = {Na Zhang and Jianwei Xia and Ju H. Park and Jing Zhang and Hao Shen},
keywords = {Multi-agent systems, Consensus tracking, Adaptive fixed-time control, Disturbance observer, Neural network},
abstract = {This paper is concerned with the problem of fixed-time consensus tracking for a class of nonlinear multi-agent systems subject to unknown disturbances. Firstly, a modified fixed-time disturbance observer is devised to estimate the unknown mismatched disturbance. Secondly, a distributed fixed-time neural network control protocol is designed, in which neural network is employed to approximate the uncertain nonlinear function. Simultaneously, the technique of command filter is applied to fixed-time control, which circumvents the “explosion of complexity” problem. Under the proposed control strategy, all agents are enable to track the desired trajectory in fixed-time, and the consensus tracking error and disturbance estimation error converge to an arbitrarily small neighborhood of the origin, meanwhile, all signals in the closed-loop system remain bounded. Finally, a simulation example is provided to validate the effectiveness of the presented design method.}
}
@article{ISLAM2023271,
title = {HARDC : A novel ECG-based heartbeat classification method to detect arrhythmia using hierarchical attention based dual structured RNN with dilated CNN},
journal = {Neural Networks},
volume = {162},
pages = {271-287},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001211},
author = {Md Shofiqul Islam and Khondokar Fida Hasan and Sunjida Sultana and Shahadat Uddin and Pietro Lio’ and Julian M.W. Quinn and Mohammad Ali Moni},
keywords = {Dilated CNN, Preprocessing, Hierarchical attention, BiGRU–BiLSTM, ECG, Arrhythmia},
abstract = {Deep learning-based models have achieved significant success in detecting cardiac arrhythmia by analyzing ECG signals to categorize patient heartbeats. To improve the performance of such models, we have developed a novel hybrid hierarchical attention-based bidirectional recurrent neural network with dilated CNN (HARDC) method for arrhythmia classification. This solves problems that arise when traditional dilated convolutional neural network (CNN) models disregard the correlation between contexts and gradient dispersion. The proposed HARDC fully exploits the dilated CNN and bidirectional recurrent neural network unit (BiGRU–BiLSTM) architecture to generate fusion features. As a result of incorporating both local and global feature information and an attention mechanism, the model’s performance for prediction is improved. By combining the fusion features with a dilated CNN and a hierarchical attention mechanism, the trained HARDC model showed significantly improved classification results and interpretability of feature extraction on the PhysioNet 2017 challenge dataset. Sequential Z-Score normalization, filtering, denoising, and segmentation are used to prepare the raw data for analysis. CGAN (Conditional Generative Adversarial Network) is then used to generate synthetic signals from the processed data. The experimental results demonstrate that the proposed HARDC model significantly outperforms other existing models, achieving an accuracy of 99.60%, F1 score of 98.21%, a precision of 97.66%, and recall of 99.60% using MIT-BIH generated ECG. In addition, this approach significantly reduces run time when using dilated CNN compared to normal convolution. Overall, this hybrid model demonstrates an innovative and cost-effective strategy for ECG signal compression and high-performance ECG recognition. Our results indicate that an automated and highly computed method to classify multiple types of arrhythmia signals holds considerable promise.}
}
@article{CHEN2023571,
title = {RAFNet: Restricted attention fusion network for sleep apnea detection},
journal = {Neural Networks},
volume = {162},
pages = {571-580},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001454},
author = {Ying Chen and Huijun Yue and Ruifeng Zou and Wenbin Lei and Wenjun Ma and Xiaomao Fan},
keywords = {Sleep apnea detection, Restricted attention, ECG signals, Feature fusion},
abstract = {Sleep apnea (SA) is a common sleep-related breathing disorder, which would lead to damage of multiple systemic organs or even sudden death. In clinical practice, portable device is an important tool to monitor sleep conditions and detect SA events by using physiological signals. However, SA detection performance is still limited due to physiological signals with time-variability and complexity. In this paper, we focus on SA detection with single lead ECG signals, which can be easily collected by a portable device. Under this context, we propose a restricted attention fusion network called RAFNet for sleep apnea detection. Specifically, RR intervals (RRI) and R-peak amplitudes (Rpeak) are generated from ECG signals and divided into one-minute-long segments. To alleviate the problem of insufficient feature information of the target segment, we combine the target segment with two pre- and post-adjacent segments in sequence, (i.e. a five-minute-long segment), as the input. Meanwhile, by leveraging the target segment as the query vector, we propose a new restricted attention mechanism with cascaded morphological and temporal attentions, which can effectively learn the feature information and depress redundant feature information from the adjacent segments with adaptive assigning weight importance. To further improve the SA detection performance, the target and adjacent segment features are fused together with the channel-wise stacking scheme. Experiment results on the public Apnea-ECG dataset and the real clinical FAH-ECG dataset with sleep apnea annotations show that the RAFNet greatly improves SA detection performance and achieves competitive results, which are superior to those achieved by the state-of-the-art baselines.}
}
@article{WANG202340,
title = {Generalized zero-shot domain adaptation via coupled conditional variational autoencoders},
journal = {Neural Networks},
volume = {163},
pages = {40-52},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.033},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001697},
author = {Qian Wang and Toby P. Breckon},
keywords = {Generalized zero-shot learning, Domain adaptation, Generalized zero-shot domain adaptation, Conditional variational autoencoder},
abstract = {Domain adaptation aims to exploit useful information from the source domain where annotated training data are easier to obtain to address a learning problem in the target domain where only limited or even no annotated data are available. In classification problems, domain adaptation has been studied under the assumption all classes are available in the target domain regardless of the annotations. However, a common situation where only a subset of classes in the target domain are available has not attracted much attention. In this paper, we formulate this particular domain adaptation problem within a generalized zero-shot learning framework by treating the labelled source-domain samples as semantic representations for zero-shot learning. For this novel problem, neither conventional domain adaptation approaches nor zero-shot learning algorithms directly apply. To solve this problem, we present a novel Coupled Conditional Variational Autoencoder (CCVAE) which can generate synthetic target-domain image features for unseen classes from real images in the source domain. Extensive experiments have been conducted on three domain adaptation datasets including a bespoke X-ray security checkpoint dataset to simulate a real-world application in aviation security. The results demonstrate the effectiveness of our proposed approach both against established benchmarks and in terms of real-world applicability.}
}
@article{BAI2023327,
title = {Learning task-agnostic and interpretable subsequence-based representation of time series and its applications in fMRI analysis},
journal = {Neural Networks},
volume = {163},
pages = {327-340},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.038},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001739},
author = {Wenjun Bai and Okito Yamashita and Junichiro Yoshimoto},
keywords = {Subsequence-based time-series analysis, Multi-task learning paradigm, fMRI analysis},
abstract = {The recent success of sequential learning models, such as deep recurrent neural networks, is largely due to their superior representation-learning capability for learning the informative representation of a targeted time series. The learning of these representations is generally goal-directed, resulting in their task-specific nature, giving rise to excellent performance in completing a single downstream task but hindering between-task generalisation. Meanwhile, with increasingly intricate sequential learning models, learned representation becomes abstract to human knowledge and comprehension. Hence, we propose a unified local predictive model based on the multi-task learning paradigm to learn the task-agnostic and interpretable subsequence-based time series representation, allowing versatile use of learned representations in temporal prediction, smoothing, and classification tasks. The targeted interpretable representation could convey the spectral information of the modelled time series to the level of human comprehension. Through a proof-of-concept evaluation study, we demonstrate the empirical superiority of learned task-agnostic and interpretable representation over task-specific and conventional subsequence-based representation, such as symbolic and recurrent learning-based representation, in solving temporal prediction, smoothing, and classification tasks. These learned task-agnostic representations can also reveal the ground-truth periodicity of the modelled time series. We further propose two applications of our unified local predictive model in functional magnetic resonance imaging (fMRI) analysis to reveal the spectral characterisation of cortical areas at rest and reconstruct more smoothed temporal dynamics of cortical activations in both resting-state and task-evoked fMRI data, giving rise to robust decoding.}
}
@article{QIAN2023443,
title = {COM: Contrastive Masked-attention model for incomplete multimodal learning},
journal = {Neural Networks},
volume = {162},
pages = {443-455},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S089360802300120X},
author = {Shuwei Qian and Chongjun Wang},
keywords = {Contrastive learning, Attention mechanism, Missing modality, Multimodal learning},
abstract = {Most multimodal learning methods assume that all modalities are always available in data. However, in real-world applications, the assumption is often violated due to privacy protection, sensor failure etc. Previous works for incomplete multimodal learning often suffer from one of the following drawbacks: introducing noise, lacking flexibility to missing patterns and failing to capture interactions between modalities. To overcome these challenges, we propose a COntrastive Masked-attention model (COM). The framework performs cross-modal contrastive learning with GAN-based augmentation to reduce modality gap, and employs a masked-attention model to capture interactions between modalities. The augmentation adapts cross-modal contrastive learning to suit incomplete case by a two-player game, improving the effectiveness of multimodal representations. Interactions between modalities are modeled by stacking self-attention blocks, and attention masks limit them on the observed modalities to avoid extra noise. All kinds of modality combinations share a unified architecture, so the model is flexible to different missing patterns. Extensive experiments on six datasets demonstrate the effectiveness and robustness of the proposed method for incomplete multimodal learning.}
}
@article{PENG2023525,
title = {MixGradient: A gradient-based re-weighting scheme with mixup for imbalanced data streams},
journal = {Neural Networks},
volume = {161},
pages = {525-534},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000801},
author = {Xinyu Peng and Fei-Yue Wang and Li Li},
keywords = {Deep learning, Imbalanced data streams, Sample gradient, Typical samples, Mixup},
abstract = {A challenge for contemporary deep neural networks in real-world problems is learning from an imbalanced data stream, where data tends to be received chunk by chunk over time, and the prior class distribution is severely imbalanced. Although many sophisticated algorithms have been derived, most of them overlook the importance of gradient information. From this perspective, the difficulty of learning from imbalanced data streams lies in the fact that the gradient estimated on an uneven class distribution is not informative enough to reflect the critical pattern of each class. To this end, we propose to assign higher weights on the training samples whose gradients are close to the gradient of corresponding typical samples, thus highlighting the important samples in minority classes and suppressing the noisy samples in majority classes. Such an idea can be combined with Mixup, which exploits the interpolation information of data to further compensate for the information of sample space that the typical samples do not provide and expand the role of the proposed re-weighting scheme. Experiments on artificially induced long-tailed CIFAR data streams and long-tailed MiniPlaces data stream show that the resulting method, termed MixGradient, boosts the generalization performance of DNNs under different imbalance ratios and achieves up to 10% accuracy improvement.}
}
@article{ZEMAN2023418,
title = {SuperFormer: Continual learning superposition method for text classification},
journal = {Neural Networks},
volume = {161},
pages = {418-436},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.01.040},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000527},
author = {Marko Zeman and Jana Faganeli Pucer and Igor Kononenko and Zoran Bosnić},
keywords = {Deep learning, Continual learning, Superposition, Transformers},
abstract = {One of the biggest challenges in continual learning domains is the tendency of machine learning models to forget previously learned information over time. While overcoming this issue, the existing approaches often exploit large amounts of additional memory and apply model forgetting mitigation mechanisms which substantially prolong the training process. Therefore, we propose a novel SuperFormer method that alleviates model forgetting, while spending negligible additional memory and time. We tackle the continual learning challenges in a learning scenario, where we learn different tasks in a sequential order. We compare our method against several prominent continual learning methods, i.e., EWC, SI, MAS, GEM, PSP, etc. on a set of text classification tasks. We achieve the best average performance in terms of AUROC and AUPRC (0.7% and 0.9% gain on average, respectively) and the lowest training time among all the methods of comparison. On average, our method reduces the total training time by a factor of 5.4-8.5 in comparison to similarly performing methods. In terms of the additional memory, our method is on par with the most memory-efficient approaches.}
}
@article{HE2023384,
title = {An interpretive constrained linear model for ResNet and MgNet},
journal = {Neural Networks},
volume = {162},
pages = {384-392},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001284},
author = {Juncai He and Jinchao Xu and Lian Zhang and Jianqing Zhu},
keywords = {Convolutional neural networks, Data-feature mapping, Multigrid iterative methods, ResNet, MgNet},
abstract = {We propose a constrained linear data-feature-mapping model as an interpretable mathematical model for image classification using a convolutional neural network (CNN). From this viewpoint, we establish detailed connections between the traditional iterative schemes for linear systems and the architectures of the basic blocks of ResNet- and MgNet-type models. Using these connections, we present some modified ResNet models that, compared with the original models, have fewer parameters but can produce more accurate results, thereby demonstrating the validity of this constrained learning data-feature-mapping assumption. Based on this assumption, we further propose a general data-feature iterative scheme to demonstrate the rationality of MgNet. We also provide a systematic numerical study on MgNet to show its success and advantages in image classification problems, particularly in comparison with established networks.}
}
@article{HOSAKA2023131,
title = {Effects of parity, frustration, and stochastic fluctuations on integrated conceptual information for networks with two small-sized loops},
journal = {Neural Networks},
volume = {162},
pages = {131-146},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.034},
url = {https://www.sciencedirect.com/science/article/pii/S089360802300103X},
author = {Tadaaki Hosaka},
keywords = {Integrated information theory 3.0, Ising model, Major complex, Bridge, PyPhi},
abstract = {This paper presents an evaluation of the system-level integrated conceptual information of a major complex for a small-scale network containing two loops in accordance with the integrated information theory 3.0 framework. We focus on the following parameters characterizing the system model: (1) number of nodes in the loop, (2) frustration of the loop, and (3) temperature controlling the stochastic fluctuation of the state transition. Effects of these parameters on the integrated conceptual information and conditions for major complexes formed by a single loop, rather than the entire network, are investigated. Our first finding is that parity of the number of nodes forming a loop has a strong effect on the integrated conceptual information. For loops with an even number of nodes, the number of concepts tends to decrease, and the integrated conceptual information becomes smaller. Our second finding is that a major complex is more likely to be formed by a small number of nodes under small stochastic fluctuations. On the other hand, the entire network can easily become a major complex under larger stochastic fluctuations, and this tendency can be reinforced by frustration. It is also shown that, although counterintuitive, the integrated conceptual information can be maximized in the presence of stochastic fluctuations. These results suggest that even when several small subnetworks are connected by only a few connections, such as a bridge, the entire network may become a major complex by introducing some stochastic fluctuations and by frustrating loops with an even number of nodes.}
}
@article{CHEN2023132,
title = {Few-shot remote sensing image scene classification based on multiscale covariance metric network (MCMNet)},
journal = {Neural Networks},
volume = {163},
pages = {132-145},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S089360802300179X},
author = {Xiliang Chen and Guobin Zhu and Mingqing Liu and Zhaotong Chen},
keywords = {FSL, Covariance network, Image scene recognition, Prototype},
abstract = {Few-shot learning (FSL) is a paradigm that simulates the fast learning ability of human beings, which can learn the feature differences between two groups of small-scale samples with common label space, and the label space of the training set and the test set is not repeated. By this way, it can quickly identify the categories of the unseen image in the test set. This method is widely used in image scene recognition, and it is expected to overcome difficulties of scarce annotated samples in remote sensing (RS). However, among most existing FSL methods, images were embed into Euclidean space, and the similarity between features at the last layer of deep network were measured by Euclidean distance. It is difficult to measure the inter-class similarity and intra-class difference of RS images. In this paper, we propose a multi-scale covariance network (MCMNet) for the application of remote sensing scene classification (RSSC). Taking Conv64F as the backbone, we mapped the features of the 1, 2, and 4 layers of the network to the manifold space by constructing a regional covariance matrix to form a covariance network with different scales. For each layer of features, we introduce the center in manifold space as a prototype for different categories of features. We simultaneously measure the similarity of three prototypes on the manifold space with different scales to form three loss functions and optimize the whole network by episodic training strategy. We conducted comparative experiments on three public datasets. The results show that the classification accuracy (CA) of our proposed method is from 1.35 % to 2.36% higher than that of the most excellent method, which demonstrates that the performance of MCMNet outperforms other methods.}
}
@article{DANESHFAR2023108,
title = {An octonion-based nonlinear echo state network for speech emotion recognition in Metaverse},
journal = {Neural Networks},
volume = {163},
pages = {108-121},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.026},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001600},
author = {Fatemeh Daneshfar and Mohammad (Behdad) Jamshidi},
keywords = {Speech emotion recognition, Digital twins, Metaverse, Octonion algebra, Echo state network, Machine learning},
abstract = {While the Metaverse is becoming a popular trend and drawing much attention from academia, society, and businesses, processing cores used in its infrastructures need to be improved, particularly in terms of signal processing and pattern recognition. Accordingly, the speech emotion recognition (SER) method plays a crucial role in creating the Metaverse platforms more usable​ and enjoyable for its users. However, existing SER methods continue to be plagued by two significant problems in the online environment. The shortage of adequate engagement and customization between avatars and users is recognized as the first issue and the second problem is related to the complexity of SER problems in the Metaverse as we face people and their digital twins or avatars. This is why developing efficient machine learning (ML) techniques specified for hypercomplex signal processing is essential to enhance the impressiveness and tangibility of the Metaverse platforms. As a solution, echo state networks (ESNs), which are an ML powerful tool for SER, can be an appropriate technique to enhance the Metaverse’s foundations in this area. Nevertheless, ESNs have some technical issues restricting them from a precise and reliable analysis, especially in the aspect of high-dimensional data. The most significant limitation of these networks is the high memory consumption caused by their reservoir structure in face of high-dimensional signals. To solve all problems associated with ESNs and their application in the Metaverse, we have come up with a novel structure for ESNs empowered by octonion algebra called NO2GESNet. Octonion numbers have eight dimensions, compactly display high-dimensional data, and improve the network precision and performance in comparison to conventional ESNs. The proposed network also solves the weaknesses of the ESNs in the presentation of the higher-order statistics to the output layer by equipping it with a multidimensional bilinear filter. Three comprehensive scenarios to use the proposed network in the Metaverse have been designed and analyzed, not only do they show the accuracy and performance of the proposed approach, but also the ways how SER can be employed in the Metaverse platforms.}
}
@article{YE2023147,
title = {WDMNet: Modeling diverse variations of regional wind speed for multi-step predictions},
journal = {Neural Networks},
volume = {162},
pages = {147-161},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.024},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000874},
author = {Rui Ye and Shanshan Feng and Xutao Li and Yunming Ye and Baoquan Zhang and Yan Zhu and Yao Sun and Yaowei Wang},
keywords = {Deep learning, Regional wind speed prediction, Diverse variations, PDEs construction},
abstract = {Regional wind speed prediction plays an important role in the development of wind power, which is usually recorded in the form of two orthogonal components, namely U-wind and V-wind. The regional wind speed has the characteristics of diverse variations, which are reflected in three aspects: (1) The spatially diverse variations of regional wind speed indicate that wind speed has different dynamic patterns at different positions; (2) The distinct variations between U-wind and V-wind denote that U-wind and V-wind at the same position exhibit different dynamic patterns; (3) The non-stationary variations of wind speed represent that the intermittent and chaotic nature of wind speed. In this paper, we propose a novel framework named Wind Dynamics Modeling Network (WDMNet) to model the diverse variations of regional wind speed and make accurate multi-step predictions. To jointly capture the spatially diverse variations and the distinct variations between U-wind and V-wind, WDMNet leverages a new neural block called Involution Gated Recurrent Unit Partial Differential Equation (Inv-GRU-PDE) as its key component. The block adopts involution to model spatially diverse variations and separately constructs hidden driven PDEs of U-wind and V-wind. The construction of PDEs in this block is achieved by a new Involution PDE (InvPDE) layers. Besides, a deep data-driven model is also introduced in Inv-GRU-PDE block as the complement to the constructed hidden PDEs for sufficiently modeling regional wind dynamics. Finally, to effectively capture the non-stationary variations of wind speed, WDMNet follows a time-variant structure for multi-step predictions. Comprehensive experiments have been conducted on two real-world datasets. Experimental results demonstrate the effectiveness and superiority of the proposed method over state-of-the-art techniques.}
}
@article{JING2023354,
title = {Exploring personalization via federated representation Learning on non-IID data},
journal = {Neural Networks},
volume = {163},
pages = {354-366},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001843},
author = {Changxing Jing and Yan Huang and Yihong Zhuang and Liyan Sun and Zhenlong Xiao and Yue Huang and Xinghao Ding},
keywords = {Federated Learning, Statistical heterogeneity, Non-IID data, Representation learning},
abstract = {Federated Learning (FL) can learn a global model across decentralized data over different clients. However, it is susceptible to statistical heterogeneity of client-specific data. Clients focus on optimizing for their individual target distributions, which would yield divergence of the global model due to inconsistent data distributions. Moreover, federated learning approaches adhere to the scheme of collaboratively learning representations and classifiers, further exacerbating such inconsistency and resulting in imbalanced features and biased classifiers. Hence, in this paper, we propose an independent two-stage personalized FL framework, i.e., Fed-RepPer, to separate representation learning from classification in federated learning. First, the client-side feature representation models are learned using supervised contrastive loss, which enables local objectives consistently, i.e., learning robust representations on distinct data distributions. Local representation models are aggregated into the common global representation model. Then, in the second stage, personalization is studied by learning different classifiers for each client based on the global representation model. The proposed two-stage learning scheme is examined in lightweight edge computing that involves devices with constrained computation resources. Experiments on various datasets (CIFAR-10/100, CINIC-10) and heterogeneous data setups show that Fed-RepPer outperforms alternatives by utilizing flexibility and personalization on non-IID data.}
}
@article{ZHAO2023412,
title = {A learnable sampling method for scalable graph neural networks},
journal = {Neural Networks},
volume = {162},
pages = {412-424},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001326},
author = {Weichen Zhao and Tiande Guo and Xiaoxi Yu and Congying Han},
keywords = {Graph neural networks, Large-scale data, Learnable sampling method},
abstract = {With the development of graph neural networks, how to handle large-scale graph data has become an increasingly important topic. Currently, most graph neural network models which can be extended to large-scale graphs are based on random sampling methods. However, the sampling process in these models is detached from the forward propagation of neural networks. Moreover, quite a few works design sampling based on statistical estimation methods for graph convolutional networks and the weights of message passing in GCNs nodes are fixed, making these sampling methods not scalable to message passing networks with variable weights, such as graph attention networks. Noting the end-to-end learning capability of neural networks, we propose a learnable sampling method. It solves the problem that random sampling operations cannot calculate gradients and samples nodes with an unfixed probability. In this way, the sampling process is dynamically combined with the forward propagation process of the features, allowing for better training of the networks. And it can be generalized to all message passing models. In addition, we apply the learnable sampling method to GNNs and propose two models. Our method can be flexibly combined with different graph neural network models and achieves excellent accuracy on benchmark datasets with large graphs. Meanwhile, loss function converges to smaller values at a faster rate during training than past methods.}
}
@article{JIANG202365,
title = {DropAGG: Robust Graph Neural Networks via Drop Aggregation},
journal = {Neural Networks},
volume = {163},
pages = {65-74},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001569},
author = {Bo Jiang and Yong Chen and Beibei Wang and Haiyun Xu and Bin Luo},
keywords = {Graph neural networks, Drop aggregation, Robust data learning, Graph random aggregation network},
abstract = {Robust learning on graph data is an active research problem in data mining field. Graph Neural Networks (GNNs) have gained great attention in graph data representation and learning tasks. The core of GNNs is the message propagation mechanism across node’s neighbors in GNNs’ layer-wise propagation. Existing GNNs generally adopt the deterministic message propagation mechanism which may (1) perform non-robustly w.r.t structural noises and adversarial attacks and (2) lead to over-smoothing issue. To alleviate these issues, this work rethinks dropout techniques in GNNs and proposes a novel random message propagation mechanism, named Drop Aggregation (DropAGG), for GNNs learning. The core of DropAGG is to randomly select a certain rate of nodes to participate in information aggregation. The proposed DropAGG is a general scheme which can incorporate any specific GNN model to enhance its robustness and mitigate the over-smoothing issue. Using DropAGG, we then design a novel Graph Random Aggregation Network (GRANet) for graph data robust learning. Extensive experiments on several benchmark datasets demonstrate the robustness of GRANet and effectiveness of DropAGG to mitigate the issue of over-smoothing.}
}
@article{WANG2023175,
title = {Mittag-Leffler stability of fractional-order quaternion-valued memristive neural networks with generalized piecewise constant argument},
journal = {Neural Networks},
volume = {162},
pages = {175-185},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.030},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000990},
author = {Jingjing Wang and Song Zhu and Xiaoyang Liu and Shiping Wen},
keywords = {Quaternion-valued, Fractional-order, Memristive neural networks, Mittag-Leffler stability, Generalized piecewise constant argument},
abstract = {This paper studies the global Mittag-Leffler (M-L) stability problem for fractional-order quaternion-valued memristive neural networks (FQVMNNs) with generalized piecewise constant argument (GPCA). First, a novel lemma is established, which is used to investigate the dynamic behaviors of quaternion-valued memristive neural networks (QVMNNs). Second, by using the theories of differential inclusion, set-valued mapping, and Banach fixed point, several sufficient criteria are derived to ensure the existence and uniqueness (EU) of the solution and equilibrium point for the associated systems. Then, by constructing Lyapunov functions and employing some inequality techniques, a set of criteria are proposed to ensure the global M-L stability of the considered systems. The obtained results in this paper not only extends previous works, but also provides new algebraic criteria with a larger feasible range. Finally, two numerical examples are introduced to illustrate the effectiveness of the obtained results.}
}
@article{MA2023557,
title = {Restoration and enhancement on low exposure raw images by joint demosaicing and denoising},
journal = {Neural Networks},
volume = {162},
pages = {557-570},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001442},
author = {Jiaqi Ma and Guoli Wang and Lefei Zhang and Qian Zhang},
keywords = {Raw images, Joint demosaicing and denoising, Image enhancement},
abstract = {Restoring high quality images from raw data in low light is challenging due to various noises caused by limited photon count and complicated Image Signal Process (ISP). Although several restoration and enhancement approaches are proposed, they may fail in extreme conditions, such as imaging short exposure raw data. The first path-breaking attempt is to utilize the connection between a pair of short and long exposure raw data and outputs RGB images as the final results. However, the whole pipeline still suffers from some blurs and color distortion. To overcome those difficulties, we propose an end-to-end network that contains two effective subnets to joint demosaic and denoise low exposure raw images. While traditional ISP are difficult to image them in acceptable conditions, the short exposure raw images can be better restored and enhanced by our model. For denoising, the proposed Short2Long raw restoration subnet outputs pseudo long exposure raw data with little noisy points. Then for demosaicing, the proposed Color consistent RGB enhancement subnet generates corresponding RGB images with the desired attributes: sharpness, color vividness, good contrast and little noise. By training the network in an end-to-end manner, our method avoids additional tuning by experts. We conduct experiments to reveal good results on three raw data datasets. We also illustrate the effectiveness of each module and the well generalization ability of this model.}
}
@article{LI20231,
title = {Few-shot human–object interaction video recognition with transformers},
journal = {Neural Networks},
volume = {163},
pages = {1-9},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000199},
author = {Qiyue Li and Xuemei Xie and Jin Zhang and Guangming Shi},
keywords = {Few-shot learning, Meta-learning, Human–object interaction recognition, Transformers},
abstract = {We propose a novel few-shot learning framework that can recognize human–object interaction (HOI) classes with a few labeled samples. We achieve this by leveraging a meta-learning paradigm where human–object interactions are embedded into compact features for similarity calculation. More specifically, spatial and temporal relationships of HOI in videos are constructed with transformers which boost the performance over the baseline significantly. First, we present a spatial encoder that extracts the spatial context and infers frame-level features of a human and objects in each frame. And then the video-level feature is obtained by encoding a series of frame-level feature vectors with a temporal encoder. Experiments on two datasets, CAD-120 and Something-Else, validate that our approach achieves 7.8% and 15.2% accuracy improvement on 1-shot task, 4.7% and 15.7% on 5-shot task, which outperforms the state-of-the-art methods.}
}
@article{JU2023122,
title = {Few-shot Molecular Property Prediction via Hierarchically Structured Learning on Relation Graphs},
journal = {Neural Networks},
volume = {163},
pages = {122-131},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.034},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001685},
author = {Wei Ju and Zequn Liu and Yifang Qin and Bin Feng and Chen Wang and Zhihui Guo and Xiao Luo and Ming Zhang},
keywords = {Molecular property prediction, Few-shot learning, Graph neural networks, Meta learning},
abstract = {This paper studies few-shot molecular property prediction, which is a fundamental problem in cheminformatics and drug discovery. More recently, graph neural network based model has gradually become the theme of molecular property prediction. However, there is a natural deficiency for existing methods, that is, the scarcity of molecules with desired properties, which makes it hard to build an effective predictive model. In this paper, we propose a novel framework called Hierarchically Structured Learning on Relation Graphs (HSL-RG) for molecular property prediction, which explores the structural semantics of a molecule from both global-level and local-level granularities. Technically, we first leverage graph kernels to construct relation graphs to globally communicate molecular structural knowledge from neighboring molecules and then design self-supervised learning signals of structure optimization to locally learn transformation-invariant representations from molecules themselves. Moreover, we propose a task-adaptive meta-learning algorithm to provide meta knowledge customization for different tasks in few-shot scenarios. Experiments on multiple real-life benchmark datasets show that HSL-RG is superior to existing state-of-the-art approaches.}
}
@article{RAMICIC2023456,
title = {Uncertainty maximization in partially observable domains: A cognitive perspective},
journal = {Neural Networks},
volume = {162},
pages = {456-471},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.044},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001168},
author = {Mirza Ramicic and Andrea Bonarini},
keywords = {Partially observable Markov decision process, Cognitive modeling, Entropy, Reinforcement learning, Attention mechanisms and development, Neural networks for development},
abstract = {Faced with an ever-increasing complexity of their domains of application, artificial learning agents are now able to scale up in their ability to process an overwhelming amount of data. However, this comes at the cost of encoding and processing an increasing amount of redundant information. This work exploits the possibility of learning systems, applied in partially observable domains, to selectively focus on the specific type of information that is more likely related to the causal interaction among transitioning states. A temporal difference displacement criterion is defined to implement adaptive masking of the observations. It can enable a significant improvement of convergence of temporal difference algorithms applied to partially observable Markov processes, as shown by experiments performed under a variety of machine learning problems, ranging from highly complex visuals as Atari games to simple textbook control problems such as CartPole. The proposed framework can be added to most RL algorithms since it only affects the observation process, selecting the parts more promising to explain the dynamics of the environment and reducing the dimension of the observation space.}
}
@article{ZHAO2023306,
title = {Feature relocation network for fine-grained image classification},
journal = {Neural Networks},
volume = {161},
pages = {306-317},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.01.050},
url = {https://www.sciencedirect.com/science/article/pii/S089360802300062X},
author = {Peng Zhao and Yi Li and Baowei Tang and Huiting Liu and Sheng Yao},
keywords = {Fine-grained image classification, Convolutional neural networks, Multi-branch architecture, Attention mechanism},
abstract = {In fine-grained image classification, there are only very subtle differences between classes. It is challenging to learn local discriminative features and remove distractive features in fine-grained image classification. Existing fine-grained image classification methods learn discriminative feature mainly via manual part annotation or attention mechanisms. However, due to the large intraclass variance and interclass similarity, the discriminative information and distractive information still are not distinguished effectively. To address this problem, we propose a feature relocation network (FRe-Net) which takes advantage of the different natures of features learned from different stages of the network. Our network consists of a distractive feature learning module and a relocated high-level feature learning module. In the distractive feature learning module, we propose to exploit the difference between low-level features and high-level features to design a distractive loss Ldistractive, which guides the attention to locate distractive regions more accurately. In the relocated high-level feature learning module, we enhance the representing capacity of the middle-level feature via the attention module and subtract the distractive feature learned from the distractive feature learning module in order to learn more local discriminative features. In end-to-end model training, the distractive feature learning module and the relocated high-level feature learning module are beneficial to each other via joint optimization. We conducted comprehensive experiments on three benchmark datasets widely used in fine-grained image classification. The experimental results show that FRe-Net achieves state-of-the-art performance, which validates the effectiveness of FRe-Net.}
}
@article{YANG2023735,
title = {Energy scheduling for DoS attack over multi-hop networks: Deep reinforcement learning approach},
journal = {Neural Networks},
volume = {161},
pages = {735-745},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.028},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000916},
author = {Lixin Yang and Jie Tao and Yong-Hua Liu and Yong Xu and Chun-Yi Su},
keywords = {Multi-hop networks, DoS attack, Kalman filtering, Markov decision process, Dueling double Q-network},
abstract = {This paper studies the energy scheduling for Denial-of-Service (DoS) attack against remote state estimation over multi-hop networks. A smart sensor observes a dynamic system, and transmits its local state estimate to a remote estimator. Due to the limited communication range of the sensor, some relay nodes are employed to deliver data packets from the sensor to the remote estimator, which constitutes a multi-hop network. To maximize the estimation error covariance with energy constraint, a DoS attacker needs to determine the energy level implemented on each channel. This problem is formulated as an associated Markov decision process (MDP), and the existence of an optimal deterministic and stationary policy (DSP) is proved for the attacker. Besides, a simple threshold structure of the optimal policy is obtained, which significantly reduces the computational complexity. Furthermore, an up-to-date deep reinforcement learning (DRL) algorithm, dueling double Q-network (D3QN), is introduced to approximate the optimal policy. Finally, a simulation example illustrates the developed results and verifies the effectiveness of D3QN for optimal DoS attack energy scheduling.}
}
@article{QIN2023466,
title = {Hybrid distributed finite-time neurodynamic optimization of electric vehicle charging schemes management in microgrid considering time-varying factors},
journal = {Neural Networks},
volume = {161},
pages = {466-475},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000758},
author = {Haohao Qin and Gui Zhao and Yue Li and Hui Wang},
keywords = {Electric vehicle charging scheme, Time-varying factors, Finite-time, Neurodynamic algorithms},
abstract = {In this paper, two distributed finite-time neurodynamic algorithms are proposed to collaboratively manage the charging scheme of electric vehicles (EVs) in the microgrid scenario. First, the upper level model is constructed to optimize the disorderly charging problem of EV users under private charging posts , and explore the optimal charging scheme under charging constraints and time-varying conditions to ensure the benefits of users. The lower layer model explores the optimal public charging scheme under the system operation constraint and the supply–demand balance constraint with the objective of minimizing the overall microgrid operation cost. The optimal solution of the upper model, i.e., the load of EV users under the private charging post, is considered as a parameter of the lower model. In this context, two finite-time neurodynamics with fast convergence rate, executed in a distributed manner, are proposed to track the optimal solution of the problem in real time. Furthermore, the stability and convergence in finite time of the two proposed algorithms are proved using Lyapunov theorem and finite time theorem. Numerical case studies of small-scale and large-scale power systems demonstrate the effectiveness, robustness, and real-time performance of the two proposed algorithms.}
}
@article{SHANG2023219,
title = {Robust data hiding for JPEG images with invertible neural network},
journal = {Neural Networks},
volume = {163},
pages = {219-232},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.037},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001727},
author = {Fei Shang and Yuhang Lan and Jianhua Yang and Enping Li and Xiangui Kang},
keywords = {Robust data hiding, JPEG image, End-to-end, Invertible neural network},
abstract = {JPEG compression will cause severe distortion to the shared compressed image, which brings great challenges to extracting messages correctly from the stego image. To address such challenges, we propose a novel end-to-end robust data hiding scheme for JPEG images. The embedding and extracting secret messages on the quantized discrete cosine transform (DCT) coefficients are implemented by the bi-directional process of the invertible neural network (INN), which can provide intrinsic robustness against lossy JPEG compression. We design a JPEG compression attack module to simulate the JPEG compression process, which helps the network automatically learn how to recover the secret message from JPEG compressed image. Experimental results have demonstrated that our method achieves strong robustness against lossy JPEG compression, and also significantly improves the security compared with the existing data hiding methods on the premise of ensuring image quality and high capacity. For example, the detection error of our method against XuNet has been increased by 3.45% over the existing data hiding methods.}
}
@article{XIAO2023146,
title = {Online continual learning with declarative memory},
journal = {Neural Networks},
volume = {163},
pages = {146-155},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.025},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001594},
author = {Zhe Xiao and Zhekai Du and Ruijin Wang and Ruimeng Gan and Jingjing Li},
keywords = {Continual learning, Deep neural networks, Catastrophic forgetting, Declarative memory, Long-term memory},
abstract = {Deep neural networks are enjoying unprecedented attention and success in recent years. However, catastrophic forgetting undermines the performance of deep models when the training data are arrived sequentially in an online multi-task learning fashion. To address this issue, we propose a novel method named continual learning with declarative memory (CLDM) in this paper. Specifically, our idea is inspired by the structure of human memory. Declarative memory is a major component of long-term memory which helps human beings memorize past experiences and facts. In this paper, we propose to formulate declarative memory as task memory and instance memory in neural networks to overcome catastrophic forgetting. Intuitively, the instance memory recalls the input–output relations (fact) in previous tasks, which is implemented by jointly rehearsing previous samples and learning current tasks as replaying-based methods act. In addition, the task memory aims to capture long-term task correlation information across task sequences to regularize the learning of the current task, thus preserving task-specific weight realizations (experience) in high task-specific layers. In this work, we implement a concrete instantiation of the proposed task memory by leveraging a recurrent unit. Extensive experiments on seven continual learning benchmarks verify that our proposed method is able to outperform previous approaches with tremendous improvements by retaining the information of both samples and tasks.}
}
@article{ALSUBAI2023240,
title = {Genetic hyperparameter optimization with Modified Scalable-Neighbourhood Component Analysis for breast cancer prognostication},
journal = {Neural Networks},
volume = {162},
pages = {240-257},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.035},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001077},
author = {Shtwai Alsubai and Abdullah Alqahtani and Mohemmed Sha},
keywords = {Breast cancer, Data Mining, Modified Scalable-Neighbourhood Component Analysis, Genetic-Hyper-Parameter Optimization},
abstract = {Breast cancer is common among women resulting in mortality when left untreated. Early detection is vital so that suitable treatment could assist cancer from spreading further and save people’s life. The traditional way of detection is a time-consuming process. With the evolvement of DM (Data Mining), the healthcare industry could be benefitted in predicting the disease as it permits the physicians to determine the significant attributes for diagnosis. Though, conventional techniques have used DM-based methods to identify breast cancer, they lacked in terms of prediction rate. Moreover, parametric-Softmax classifiers have been a general option by conventional works with fixed classes, particularly when huge labelled data are present during training. Nevertheless, this turns into an issue for open set cases where new classes are encountered along with few instances to learn a generalized parametric classifier. Thus, the present study aims to implement a non-parametric strategy by optimizing the embedding of a feature rather than parametric classifiers. This research utilizes Deep CNN (Deep Convolutional Neural Network) and Inception V3 for learning visual features which preserve neighbourhood outline in semantic space relying on NCA (Neighbourhood Component Analysis) criteria. Delimited by its bottleneck, the study proposes MS-NCA (Modified Scalable-Neighbourhood Component Analysis) that relies on a non-linear objective function to perform feature fusion by optimizing the distance-learning objective due to which it gains the capability of computing inner feature products without performing mapping which increases the scalability of MS-NCA. Finally, G-HPO (Genetic-Hyper-parameter Optimization) is proposed. In this case, the new stage in the algorithm simply denotes the enhancement in the length of chromosome bringing several hyperparameters into subsequent XGBoost, NB and RF models having numerous layers for identifying the normal and affected cases of breast cancer for which optimized hyper-parameter values of RF (Random Forest), NB (Naïve Bayes), and XGBoost (eXtreme Gradient Boosting) are determined. This process helps in improvising the classification rate which is confirmed through analytical results.}
}
@article{GUO2023288,
title = {Bounded synchronization for uncertain master–slave neural networks: An adaptive impulsive control approach},
journal = {Neural Networks},
volume = {162},
pages = {288-296},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001193},
author = {Yuru Guo and Chang Liu and Yonghua Liu and Yong Xu and Renquan Lu and Tingwen Huang},
keywords = {Bounded synchronization, Master–slave neural networks, Impulsive control, Adaptive control},
abstract = {This paper investigates the bounded synchronization of the discrete-time master–slave neural networks (MSNNs) with uncertainty. To deal with the unknown parameter in the MSNNs, a parameter adaptive law combined with the impulsive mechanism is proposed to improve the estimation efficiency. Meanwhile, the impulsive method also is applied to the controller design for saving the energy. In addition, a novel time-varying Lyapunov functional candidate is employed to depict the impulsive dynamical characteristic of the MSNNs, wherein a convex function related to the impulsive interval is used to obtain a sufficient condition for bounded synchronization of the MSNNs. Based on the above condition, the controller gain is calculated utilizing an unitary matrix. An algorithm is proposed to reduce the boundary of the synchronization error by optimizing its parameters. Finally, a numerical example is provided to illustrate the correctness and the superiority of the developed results.}
}
@article{SUN202310,
title = {Multi-level Feature Interaction and Efficient Non-Local Information Enhanced Channel Attention for image dehazing},
journal = {Neural Networks},
volume = {163},
pages = {10-27},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S089360802300134X},
author = {Hang Sun and Bohui Li and Zhiping Dan and Wei Hu and Bo Du and Wen Yang and Jun Wan},
keywords = {Image dehazing, Multi-level feature interaction, Non-local information, Channel attention},
abstract = {Image dehazing is a challenging task in computer vision. Currently, most dehazing methods adopt the U-Net architecture that directly fuses the decoding layer with the corresponding scale encoding layer. These methods ignore the effective utilization of different encoding layer information and existing feature information dilute problems, resulting in suboptimal edge details and overall scene aspects of dehazed image restoration. In addition, Squeeze and Excitation (SE) channel attention is widely used in dehazing network. However, the two fully-connected layers of dimensionality reduction operation in SE will negatively affect the weight prediction of feature channels, thus reducing the performance of the dehazing network. To solve the above problems, we propose a Multi-level Feature Interaction and Non-local Information Enhanced Channel Attention (MFINEA) dehazing model. Specifically, a multi-level feature interaction module is proposed to enable the decoding layer to fuse shallow and deep feature information extracted from different encoding layers for better recovery of edge details and the overall scene. Furthermore, an efficient non-local information enhanced channel attention module is proposed to mine more effective feature channel information for the weight assignment of the feature maps. The experimental results on several challenging benchmark datasets show that our MFINEA outperforms the state-of-the-art dehazing methods.}
}
@article{NOURI2023575,
title = {Eigen value based loss function for training attractors in iterated autoencoders},
journal = {Neural Networks},
volume = {161},
pages = {575-588},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000667},
author = {Ali Nouri and Seyyed Ali Seyyedsalehi},
keywords = {Associative memory, Iterated autoencoder, Eigen values, Attractor neural networks},
abstract = {The way that the human brain handles the input variations has been one of the most interesting areas of research for neuroscientists. There are some evidences that the human brain acts like an attractor when trying to memorize or retrieve some information. Based on this fact, in this research, a new method is presented for creating attractors during training of an iterated autoencoder. In this method a new loss function is presented which decreases the absolute real of Eigen values while preserving the reconstruction error during training. A fully connected structure is chosen for constructing the iterated autoencoder in this research which mostly faces with local minima especially when they are deep. For getting through this issue, a layer-by-layer pre-training approach is taken to train the network. Using the evaluation on MNIST dataset, it is shown that the proposed model can retrieve 59.98% of test samples which shows a considerable improvement over Dense Associative Memory (DAM) when trained on 100 similar MNIST test samples. The performance of the proposed model is compared to overparameterized autoencoder (OAE) model which was recently presented and showed promising results in constructing associative memories. The results show that the proposed model outperforms OAE in terms of the number of attractors learned by the network in a similar number of network parameters. Finally, the performance of the proposed model is evaluated with corrupted version of training samples, revealing significant robustness when compared to the baseline autoencoder.}
}
@article{MI2023535,
title = {Hierarchical neural network with efficient selection inference},
journal = {Neural Networks},
volume = {161},
pages = {535-549},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000783},
author = {Jian-Xun Mi and Nuo Li and Ke-Yang Huang and Weisheng Li and Lifang Zhou},
keywords = {Convolutional neural network (CNN), Image classification, Category hierarchy, Dynamic computation, Path decision search},
abstract = {The image classification precision is vastly enhanced with the growing complexity of convolutional neural network (CNN) structures. However, the uneven visual separability between categories leads to various difficulties in classification. The hierarchical structure of categories can be leveraged to deal with it, but a few CNNs pay attention to the character of data. Besides, a network model with a hierarchical structure is promising to extract more specific features from the data than current CNNs, since, for the latter, all categories have the same fixed number of layers for feed-forward computation. In this paper, we propose to use category hierarchies to integrate ResNet-style modules to form a hierarchical network model in a top-down manner. To extract abundant discriminative features and improve the computation efficiency, we adopt residual block selection based on coarse categories to allocate different computation paths. Each residual block works as a switch to determine the JUMP or JOIN mode for an individual coarse category. Interestingly, since some categories need less feed-forward computation than others by jumping layers, the average inference time cost is reduced. Extensive experiments show that our hierarchical network achieves higher prediction accuracy with similar FLOPs on CIFAR-10 and CIFAR-100, SVHM, and Tiny-ImageNet datasets compared to original residual networks and other existing selection inference methods.}
}
@article{HASHEMI2023178,
title = {Amortized Bayesian inference on generative dynamical network models of epilepsy using deep neural density estimators},
journal = {Neural Networks},
volume = {163},
pages = {178-194},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.040},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001752},
author = {Meysam Hashemi and Anirudh N. Vattikonda and Jayant Jha and Viktor Sip and Marmaduke M. Woodman and Fabrice Bartolomei and Viktor K. Jirsa},
keywords = {Bayesian inference, Artificial neural networks, Simulation-based inference, Dynamical systems, Whole-brain network modeling, Epilepsy},
abstract = {Whole-brain modeling of epilepsy combines personalized anatomical data with dynamical models of abnormal activities to generate spatio-temporal seizure patterns as observed in brain imaging data. Such a parametric simulator is equipped with a stochastic generative process, which itself provides the basis for inference and prediction of the local and global brain dynamics affected by disorders. However, the calculation of likelihood function at whole-brain scale is often intractable. Thus, likelihood-free algorithms are required to efficiently estimate the parameters pertaining to the hypothetical areas, ideally including the uncertainty. In this study, we introduce the simulation-based inference for the virtual epileptic patient model (SBI-VEP), enabling us to amortize the approximate posterior of the generative process from a low-dimensional representation of whole-brain epileptic patterns. The state-of-the-art deep learning algorithms for conditional density estimation are used to readily retrieve the statistical relationships between parameters and observations through a sequence of invertible transformations. We show that the SBI-VEP is able to efficiently estimate the posterior distribution of parameters linked to the extent of the epileptogenic and propagation zones from sparse intracranial electroencephalography recordings. The presented Bayesian methodology can deal with non-linear latent dynamics and parameter degeneracy, paving the way for fast and reliable inference on brain disorders from neuroimaging modalities.}
}
@article{GRAFFIETI2023369,
title = {Generative negative replay for continual learning},
journal = {Neural Networks},
volume = {162},
pages = {369-383},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001235},
author = {Gabriele Graffieti and Davide Maltoni and Lorenzo Pellegrini and Vincenzo Lomonaco},
keywords = {Continual learning, Generative replay, Continual object recognition, Pseudo-rehearsal, Generative model, Negative replay},
abstract = {Learning continually is a key aspect of intelligence and a necessary ability to solve many real-life problems. One of the most effective strategies to control catastrophic forgetting, the Achilles’ heel of continual learning, is storing part of the old data and replaying them interleaved with new experiences (also known as the replay approach). Generative replay, which is using generative models to provide replay patterns on demand, is particularly intriguing, however, it was shown to be effective mainly under simplified assumptions, such as simple scenarios and low-dimensional data. In this paper, we show that, while the generated data are usually not able to improve the classification accuracy for the old classes, they can be effective as negative examples (or antagonists) to better learn the new classes, especially when the learning experiences are small and contain examples of just one or few classes. The proposed approach is validated on complex class-incremental and data-incremental continual learning scenarios (CORe50 and ImageNet-1000) composed of high-dimensional data and a large number of training experiences: a setup where existing generative replay approaches usually fail.}
}
@article{HAN2023369,
title = {B-mode ultrasound based CAD for liver cancers via multi-view privileged information learning},
journal = {Neural Networks},
volume = {164},
pages = {369-381},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.028},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001636},
author = {Xiangmin Han and Bangming Gong and Lehang Guo and Jun Wang and Shihui Ying and Shuo Li and Jun Shi},
keywords = {Multi-view privileged information learning, B-mode ultrasound, Contrast-enhanced ultrasound, Feature-level knowledge transfer},
abstract = {B-mode ultrasound-based computer-aided diagnosis model can help sonologists improve the diagnostic performance for liver cancers, but it generally suffers from the bottleneck due to the limited structure and internal echogenicity information in B-mode ultrasound images. Contrast-enhanced ultrasound images provide additional diagnostic information on dynamic blood perfusion of liver lesions for B-mode ultrasound images with improved diagnostic accuracy. Since transfer learning has indicated its effectiveness in promoting the performance of target computer-aided diagnosis model by transferring knowledge from related imaging modalities, a multi-view privileged information learning framework is proposed to improve the diagnostic accuracy of the single-modal B-mode ultrasound-based diagnosis for liver cancers. This framework can make full use of the shared label information between the paired B-mode ultrasound images and contrast-enhanced ultrasound images to guide knowledge transfer It consists of a novel supervised dual-view deep Boltzmann machine and a new deep multi-view SVM algorithm. The former is developed to implement knowledge transfer from the multi-phase contrast-enhanced ultrasound images to the B-mode ultrasound-based diagnosis model via a feature-level learning using privileged information paradigm, which is totally different from the existing learning using privileged information paradigm that performs knowledge transfer in the classifier. The latter further fuses and enhances feature representation learned from three pre-trained supervised dual-view deep Boltzmann machine networks for the classification task. An experiment is conducted on a bimodal ultrasound liver cancer dataset. The experimental results show that the proposed framework outperforms all the compared algorithms with the best classification accuracy of 88.91 ± 1.52%, sensitivity of 88.31 ± 2.02%, and specificity of 89.50 ± 3.12%. It suggests the effectiveness of our proposed MPIL framework for the BUS-based CAD of liver cancers.}
}
@article{YAO2023350,
title = {Vertex points are not enough: Monocular 3D object detection via intra- and inter-plane constraints},
journal = {Neural Networks},
volume = {162},
pages = {350-358},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.038},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001119},
author = {Hongdou Yao and Jun Chen and Zheng Wang and Xiao Wang and Xiaoyu Chai and Yansheng Qiu and Pengfei Han},
keywords = {Monocular image, Keypoint detection, Intra-plane, Inter-plane, Location prediction},
abstract = {Existed methods for 3D object detection in monocular images focus mainly on the class of rigid bodies like cars, while more challenging detection like the cyclist is less studied. Therefore, we propose a novel 3D monocular object detection method to improve the accuracy of detection objects with large differences in deformation by introducing the geometric constraints of the object 3D bounding box plane. Considering the map relationship of projection plane and the keypoint, we firstly introduce the geometric constraints of the object 3D bounding box plane, adding the intra-plane constraint while regressing the position and offset of the keypoint itself, so that the position and offset error of the keypoint are always within the error range of the projection plane. For the inter-plane geometry relationship of the 3D bounding box, the prior knowledge is incorporated to optimize the keypoint regression allowing for improved the accuracy of depth location prediction. Experimental results show that the proposed method outperforms some other state-of-the-art methods on cyclist class, and obtains competitive results in the field of real-time monocular detection.}
}
@article{TIAN2023708,
title = {Partial label learning: Taxonomy, analysis and outlook},
journal = {Neural Networks},
volume = {161},
pages = {708-734},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000825},
author = {Yingjie Tian and Xiaotong Yu and Saiji Fu},
keywords = {Partial label learning, Machine learning, Weakly supervised learning, Partial multi-label learning},
abstract = {Partial label learning (PLL) is an emerging framework in weakly supervised machine learning with broad application prospects. It handles the case in which each training example corresponds to a candidate label set and only one label concealed in the set is the ground-truth label. In this paper, we propose a novel taxonomy framework for PLL including four categories: disambiguation strategy, transformation strategy, theory-oriented strategy and extensions. We analyze and evaluate methods in each category and sort out synthetic and real-world PLL datasets which are all hyperlinked to the source data. Future work of PLL is profoundly discussed in this article based on the proposed taxonomy framework.}
}
@article{PAKNEZHAD2023449,
title = {Improving transparency and representational generalizability through parallel continual learning},
journal = {Neural Networks},
volume = {161},
pages = {449-465},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000709},
author = {Mahsa Paknezhad and Hamsawardhini Rengarajan and Chenghao Yuan and Sujanya Suresh and Manas Gupta and Savitha Ramasamy and Hwee Kuan Lee},
keywords = {Incremental learning, Continual learning, Lifelong learning, Generalizable representations, Transparency, Multiple input domains},
abstract = {This paper takes a parallel learning approach in continual learning scenarios. We define parallel continual learning as learning a sequence of tasks where the data for the previous tasks, whose distribution may have shifted over time, are also available while learning new tasks. We propose a parallel continual learning method by assigning subnetworks to each task, and simultaneously training only the assigned subnetworks on their corresponding tasks. In doing so, some parts of the network will be shared across multiple tasks. This is unlike the existing literature in continual learning which aims at learning incoming tasks sequentially, with the assumption that the data for the previous tasks have a fixed distribution. Our proposed method offers promises in: (1) Transparency in the network and in the relationship across tasks by enabling examination of the learned representations by independent and shared subnetworks, (2) Representation generalizability through sharing and training subnetworks on multiple tasks simultaneously. Our analysis shows that compared to many competing approaches such as continual learning, neural architecture search, and multi-task learning, parallel continual learning is capable of learning more generalizable representations. Also, (3) Parallel continual learning overcomes the common issue of catastrophic forgetting in continual learning algorithms. This is the first effort to train a neural network on multiple tasks and input domains simultaneously in a continual learning scenario. Our code is available at https://github.com/yours-anonym/PaRT.}
}
@article{HUANG202386,
title = {Meta attention for Off-Policy Actor-Critic},
journal = {Neural Networks},
volume = {163},
pages = {86-96},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.024},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001582},
author = {Jiateng Huang and Wanrong Huang and Long Lan and Dan Wu},
keywords = {Reinforcement learning, Meta learning, Attention mechanism, Actor-Critic methods},
abstract = {Off-Policy Actor-Critic methods can effectively exploit past experiences and thus they have achieved great success in various reinforcement learning tasks. In many image-based and multi-agent tasks, attention mechanism has been employed in Actor-Critic methods to improve their sampling efficiency. In this paper, we propose a meta attention method for state-based reinforcement learning tasks, which combines attention mechanism and meta-learning based on the Off-Policy Actor-Critic framework. Unlike previous attention-based work, our meta attention method introduces attention in the Actor and the Critic of the typical Actor-Critic framework, rather than in multiple pixels of an image or multiple information sources in specific image-based control tasks or multi-agent systems. In contrast to existing meta-learning methods, the proposed meta-attention approach is able to function in both the gradient-based training phase and the agent’s decision-making process. The experimental results demonstrate the superiority of our meta-attention method in various continuous control tasks, which are based on the Off-Policy Actor-Critic methods including DDPG and TD3.}
}
@article{MAI2023393,
title = {Nested relation extraction via self-contrastive learning guided by structure and semantic similarity},
journal = {Neural Networks},
volume = {162},
pages = {393-411},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001181},
author = {Chengcheng Mai and Kaiwen Luo and Yuxiang Wang and Ziyan Peng and Yu Chen and Chunfeng Yuan and Yihua Huang},
keywords = {Nested relation extraction, Self-contrastive learning, Iterative neural networks, Structure similarity, Semantic similarity},
abstract = {The conventional Relation Extraction (RE) task involves identifying whether relations exist between two entities in a given sentence and determining their relation types. However, the complexity of practical application scenarios and the flexibility of natural language demand the ability to extract nested relations, i.e., the recognized relation triples may be components of the higher-level relations. Previous studies have highlighted several challenges that affect the nested RE task, including the lack of abundant labeled data, inappropriate neural networks, and underutilization of the nested relation structures. To address these issues, we formalize the nested RE task and propose a hierarchical neural network to iteratively identify the nested relations between entities and relation triples in a layer by layer manner. Moreover, a novel self-contrastive learning optimization strategy is presented to adapt our method to low-data settings by fully exploiting the constraints due to the nested structure and semantic similarity between paired input sentences. Our method outperformed the state-of-the-art baseline methods in extensive experiments, and ablation experiments verified the effectiveness of the proposed self-contrastive learning optimization strategy.}
}
@article{ZHU2023205,
title = {Collaborative-guided spectral abundance learning with bilinear mixing model for hyperspectral subpixel target detection},
journal = {Neural Networks},
volume = {163},
pages = {205-218},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000655},
author = {Dehui Zhu and Bo Du and Meiqi Hu and Yanni Dong and Liangpei Zhang},
keywords = {Hyperspectral imagery, Subpixel target detection, Spectral abundance learning, Bilinear mixing model},
abstract = {Detecting subpixel targets is a considerably challenging issue in hyperspectral image processing and interpretation. Most of the existing hyperspectral subpixel target detection methods construct detectors based on the linear mixing model which regards a pixel as a linear combination of different spectral signatures. However, due to the multiple scattering, the linear mixing model cannot​ illustrate the multiple materials interactions that are nonlinear and widespread in real-world hyperspectral images, which could result in unsatisfactory performance in detecting subpixel targets. To alleviate this problem, this work presents a novel collaborative-guided spectral abundance learning model (denoted as CGSAL) for subpixel target detection based on the bilinear mixing model in hyperspectral images. The proposed CGSAL detects subpixel targets by learning a spectral abundance of the target signature in each pixel. In CGSAL, virtual endmembers and their abundance help to achieve good accuracy for modeling nonlinear scattering accounts for multiple materials interactions according to the bilinear mixing model. Besides, we impose a collaborative term to the spectral abundance learning model to emphasize the collaborative relationships between different endmembers, which contributes to accurate spectral abundance learning and further help to detect subpixel targets. Plentiful experiments and analyses are conducted on three real-world and one synthetic hyperspectral datasets to evaluate the effectiveness of the CGSAL in subpixel target detection. The experiment results demonstrate that the CGSAL achieves competitive performance in detecting subpixel targets and outperforms other state-of-the-art hyperspectral subpixel target detectors.}
}
@article{KORAI202346,
title = {A dynamical model of visual motion processing for arbitrary stimuli including type II plaids},
journal = {Neural Networks},
volume = {162},
pages = {46-68},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.039},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001107},
author = {Yusuke Korai and Kenichiro Miura},
keywords = {Computational neuroscience, Dynamical model, Optimization, Motion perception, Eye movement},
abstract = {To explore the operating principle of visual motion processing in the brain underlying perception and eye movements, we model the information processing of velocity estimate of the visual stimulus at the algorithmic level using the dynamical system approach. In this study, we formulate the model as an optimization process of an appropriately defined objective function. The model is applicable to arbitrary visual stimuli. We find that our theoretical predictions qualitatively agree with time evolution of eye movement reported by previous works across various types of stimulus. Our results suggest that the brain implements the present framework as the internal model of motion vision. We anticipate our model to be a promising building block for more profound understanding of visual motion processing as well as for the development of robotics.}
}
@article{HAN2023297,
title = {Dual adaptive learning multi-task multi-view for graph network representation learning},
journal = {Neural Networks},
volume = {162},
pages = {297-308},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.026},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000898},
author = {Beibei Han and Yingmei Wei and Qingyong Wang and Shanshan Wan},
keywords = {Graph network analysis, Multi-view graph network, Multi-task learning, Adaptive graph network represent learning},
abstract = {Graph network analysis, which achieves widely application, is to explore and mine the graph structure data. However, existing graph network analysis methods with graph representation learning technique ignore the correlation between multiple graph network analysis tasks, and they need massive repeated calculation to obtain each graph network analysis results. Or they cannot adaptively balance the relative importance of multiple graph network analysis tasks, that lead to weak model fitting. Besides, most of existing methods ignore multiplex views semantic information and global graph information, which fail to learn robust node embeddings resulting in unsatisfied graph analysis results. To solve these issues, we propose a multi-task multi-view adaptive graph network representation learning model, called M2agl. The highlights of M2agl are as follows: (1) Graph convolutional network with the linear combination of the adjacency matrix and PPMI (positive point-wise mutual information) matrix is utilized as encoder to extract the local and global intra-view graph feature information of the multiplex graph network. Each intra-view graph information of the multiplex graph network can adaptively learn the parameters of graph encoder. (2) We use regularization to capture the interaction information among different graph views, and the importance of different graph views are learned by view attention mechanism for further inter-view graph network fusion. (3) The model is trained oriented by multiple graph network analysis tasks. The relative importance of multiple graph network analysis tasks are adjusted adaptively with the homoscedastic uncertainty. The regularization can be considered as an auxiliary task to further boost the performance. Experiments on real-worlds attributed multiplex graph networks demonstrate the effectiveness of M2agl in comparison with other competing approaches.}
}
@article{CHEN2023258,
title = {A meta-framework for multi-label active learning based on deep reinforcement learning},
journal = {Neural Networks},
volume = {162},
pages = {258-270},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.045},
url = {https://www.sciencedirect.com/science/article/pii/S089360802300117X},
author = {Shuyue Chen and Ran Wang and Jian Lu},
keywords = {Multi-label active learning, Deep reinforcement learning, Meta-learning, Query strategy, Self-attention mechanism},
abstract = {Multi-label Active Learning (MLAL) is an effective method to improve the performance of the classifier on multi-label problems with less annotation effort by allowing the learning system to actively select high-quality examples (example-label pairs) for labeling. Existing MLAL algorithms mainly focus on designing reasonable algorithms to evaluate the potential values (as previously mentioned quality) of the unlabeled data. These manually designed methods may show totally different results on various types of datasets due to the defect of the methods or the particularity of the datasets. In this paper, instead of manually designing an evaluation method, we propose a deep reinforcement learning (DRL) model to explore a general evaluation method on several seen datasets and eventually apply it to unseen datasets based on a meta framework. In addition, a self-attention mechanism along with a reward function is integrated into the DRL structure to address the label correlation and data imbalanced problems in MLAL. Comprehensive experiments show that our proposed DRL-based MLAL method is able to produce comparable results as compared with other methods reported in the literature.}
}
@article{YANG2023589,
title = {Memory-efficient Transformer-based network model for Traveling Salesman Problem},
journal = {Neural Networks},
volume = {161},
pages = {589-597},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000771},
author = {Hua Yang and Minghao Zhao and Lei Yuan and Yang Yu and Zhenhua Li and Ming Gu},
keywords = {Combinatorial optimization, Deep reinforcement learning, TSP, Transformer},
abstract = {Combinatorial optimization problems such as Traveling Salesman Problem (TSP) have a wide range of real-world applications in transportation, logistics, manufacturing. It has always been a difficult problem to solve large-scale TSP problems quickly because of memory usage limitations. Recent research shows that the Transformer model is a promising approach. However, the Transformer has several severe problems that prevent it from quickly solving TSP combinatorial optimization problems, such as quadratic time complexity, especially quadratic space complexity, and the inherent limitations of the encoder and decoder itself. To address these issues, we developed a memory-efficient Transformer-based network model for TSP combinatorial optimization problems, termed Tspformer, with two distinctive characteristics: (1) a sampled scaled dot-product attention mechanism with O(Llog(L)) (L is the length of input sequences) time and space complexity, which is the most different between our work and other works. (2) due to the reduced space complexity, GPU/CPU memory usage is significantly reduced. Extensive experiments demonstrate that Tspformer significantly outperforms existing methods and provides a new solution to the TSP combinatorial optimization problems. Our Pytorch code will be publicly available on GitHub https://github.com/yhnju/tspFormer.}
}
@article{LI2023195,
title = {A novel semi-supervised meta learning method for subject-transfer brain–computer interface},
journal = {Neural Networks},
volume = {163},
pages = {195-204},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.039},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001740},
author = {Jingcong Li and Fei Wang and Haiyun Huang and Feifei Qi and Jiahui Pan},
keywords = {Semi-supervised, Meta learning, Transfer learning, Event-related potential, Emotion recognition, Sleep staging},
abstract = {The brain–computer interface (BCI) provides a direct communication pathway between the human brain and external devices. However, the models trained for existing subjects perform poorly on new subjects, which is termed the subject calibration problem. In this paper, we propose a semi-supervised meta learning (SSML) method for subject-transfer calibration. The proposed SSML learns a model-agnostic meta learner with existing subjects and then fine-tunes the meta learner in a semi-supervised learning manner, i.e. using a few labelled samples and many unlabelled samples of the target subject for calibration. It is significant for BCI applications in which labelled data are scarce or expensive while unlabelled data are readily available. Three different BCI paradigms are tested: event-related potential detection, emotion recognition and sleep staging. The SSML achieved classification accuracies of 0.95, 0.89 and 0.83 in the benchmark datasets of three paradigms. The runtime complexity of SSML grows linearly as the number of samples of target subject increases so that is possible to apply it in real-time systems. This study is the first attempt to apply semi-supervised model-agnostic meta learning methodology for subject calibration. The experimental results demonstrated the effectiveness and potential of the SSML method for subject-transfer BCI applications.}
}
@article{V2023425,
title = {Framework for Segmented threshold ℓ0 gradient approximation based network for sparse signal recovery},
journal = {Neural Networks},
volume = {162},
pages = {425-442},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001223},
author = {Vivekanand V. and Deepak Mishra},
keywords = {Sparse recovery, Thresholding,  norm minimization, Polynomial approximation, Basis function network},
abstract = {Signal reconstruction from compressed sensed data need iterative methods since the sparse measurement matrix is analytically non invertible. The iterative thresholding and ℓ0 function minimization are of special interest as these two operations provide sparse solution. However these methods need an inverse operation corresponding to the measurement matrix for estimating the reconstruction error. The pseudo-inverse of the measurement matrix is used in general for this purpose. Here a sparse signal recovery framework using an approximate inverse matrix Q and iterative segment thresholding of ℓ0 and ℓ1 norm with residue addition is presented. Two recovery algorithms are developed using this framework. The ℓ0 based method is later developed to a basis function dictionary based network for sparse signal recovery. The proposed framework enables the users experiment with different inverse matrix to achieve better efficiency in sparse signal recovery and implement the algorithm in computationally efficient way.}
}
@article{WU2023598,
title = {Feature flow regularization: Improving structured sparsity in deep neural networks},
journal = {Neural Networks},
volume = {161},
pages = {598-613},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S089360802300076X},
author = {Yue Wu and Yuan Lan and Luchan Zhang and Yang Xiang},
keywords = {Deep neural networks, Structured pruning, Image classification, Regularization},
abstract = {Pruning is a model compression method that removes redundant parameters and accelerates the inference speed of deep neural networks (DNNs) while maintaining accuracy. Most available pruning methods impose various conditions on parameters or features directly. In this paper, we propose a simple and effective regularization strategy to improve the structured sparsity and structured pruning in DNNs from a new perspective of evolution of features. In particular, we consider the trajectories connecting features of adjacent hidden layers, namely feature flow. We propose feature flow regularization (FFR) to penalize the length and the total absolute curvature of the trajectories, which implicitly increases the structured sparsity of the parameters. The principle behind FFR is that short and straight trajectories will lead to an efficient network that avoids redundant parameters. Experiments on CIFAR-10 and ImageNet datasets show that FFR improves structured sparsity and achieves pruning results comparable to or even better than those state-of-the-art methods.}
}
@article{ZANG2023626,
title = {UDRN: Unified Dimensional Reduction Neural Network for feature selection and feature projection},
journal = {Neural Networks},
volume = {161},
pages = {626-637},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000813},
author = {Zelin Zang and Yongjie Xu and Linyan Lu and Yulan Geng and Senqiao Yang and Stan Z. Li},
keywords = {Dimensional reduction, High-dimensional data analysis, Feature selection, Feature projection},
abstract = {Dimensional reduction (DR) maps high-dimensional data into a lower dimensions latent space with minimized defined optimization objectives. The two independent branches of DR are feature selection (FS) and feature projection (FP). FS focuses on selecting a critical subset of dimensions but risks destroying the data distribution (structure). On the other hand, FP combines all the input features into lower dimensions space, aiming to maintain the data structure, but lacks interpretability and sparsity. Moreover, FS and FP are traditionally incompatible categories and have not been unified into an amicable framework. Therefore, we consider that the ideal DR approach combines both FS and FP into a unified end-to-end manifold learning framework, simultaneously performing fundamental feature discovery while maintaining the intrinsic relationships between data samples in the latent space. This paper proposes a unified framework named Unified Dimensional Reduction Network (UDRN) to integrate FS and FP in an end-to-end way. Furthermore, a novel network framework is designed to implement FS and FP tasks separately using a stacked feature selection network and feature projection network. In addition, a stronger manifold assumption and a novel loss function are proposed. Furthermore, the loss function can leverage the priors of data augmentation to enhance the generalization ability of the proposed UDRN. Finally, comprehensive experimental results on four image and four biological datasets, including very high-dimensional data, demonstrate the advantages of DRN over existing methods (FS, FP, and FS&FP pipeline), especially in downstream tasks such as classification and visualization.}
}