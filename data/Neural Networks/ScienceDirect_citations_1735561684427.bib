@article{MAITH2021534,
title = {Optimal attention tuning in a neuro-computational model of the visual cortex–basal ganglia–prefrontal cortex loop},
journal = {Neural Networks},
volume = {142},
pages = {534-547},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002719},
author = {Oliver Maith and Alex Schwarz and Fred H. Hamker},
keywords = {Visual attention, Basal ganglia, Optimal attentional tuning, PFC, Feature-based attention},
abstract = {Visual attention is widely considered a vital factor in the perception and analysis of a visual scene. Several studies explored the effects and mechanisms of top-down attention, but the mechanisms that determine the attentional signal are less explored. By developing a neuro-computational model of visual attention including the visual cortex–basal ganglia loop, we demonstrate how attentional alignment can evolve based on dopaminergic reward during a visual search task. Unlike most previous modeling studies of feature-based attention, we do not implement a manually predefined attention template. Dopamine-modulated covariance learning enable the basal ganglia to learn rewarded associations between the visual input and the attentional gain represented in the PFC of the model. Hence, the model shows human-like performance on a visual search task by optimally tuning the attention signal. In particular, similar as in humans, this reward-based tuning in the model leads to an attentional template that is not centered on the target feature, but a relevant feature deviating away from the target due to the presence of highly similar distractors. Further analyses of the model shows, attention is mainly guided by the signal-to-noise ratio between target and distractors.}
}
@article{WU202128,
title = {Learning to recognize while learning to speak: Self-supervision and developing a speaking motor},
journal = {Neural Networks},
volume = {143},
pages = {28-41},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021001982},
author = {Xiang Wu and Juyang Weng},
keywords = {MHTG, Motor development, Developmental networks, CCI PCA, Speech recognition, Speech synthesis},
abstract = {Traditionally, learning speech synthesis and speech recognition were investigated as two separate tasks. This separation hinders incremental development for concurrent synthesis and recognition, where partially-learned synthesis and partially-learned recognition must help each other throughout lifelong learning. This work is a paradigm shift—we treat synthesis and recognition as two intertwined aspects of a lifelong learning agent. Furthermore, in contrast to existing recognition or synthesis systems, babies do not need their mothers to directly supervise their vocal tracts at every moment during the learning. We argue that self-generated non-symbolic states/actions at fine-grained time level help such a learner as necessary temporal contexts. Here, we approach a new and challenging problem—how to enable an autonomous learning system to develop an artificial speaking motor for generating temporally-dense (e.g., frame-wise) actions on the fly without human handcrafting a set of symbolic states. The self-generated states/actions are Muscles-like, High-dimensional, Temporally-dense and Globally-smooth (MHTG), so that these states/actions are directly attended for concurrent synthesis and recognition for each time frame. Human teachers are relieved from supervising learner’s motor ends. The Candid Covariance-free Incremental (CCI) Principal Component Analysis (PCA) is applied to develop such an artificial speaking motor where PCA features drive the motor. Since each life must develop normally, each Developmental Network-2 (DN-2) reaches the same network (maximum likelihood, ML) regardless of randomly initialized weights, where ML is not just for a function approximator but rather an emergent Turing Machine. The machine-synthesized sounds are evaluated by both the neural network and humans with recognition experiments. Our experimental results showed learning-to-synthesize and learning-to-recognize-through-synthesis for phonemes. This work corresponds to a key step toward our goal to close a great gap toward fully autonomous machine learning directly from the physical world.}
}
@article{LIU2021283,
title = {Adaptive neural network asymptotic tracking control for nonstrict feedback stochastic nonlinear systems},
journal = {Neural Networks},
volume = {143},
pages = {283-290},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002434},
author = {Yongchao Liu and Qidan Zhu},
keywords = {Asymptotic tracking control, Backstepping algorithm, Neural network, Stochastic nonlinear systems},
abstract = {The adaptive neural network asymptotic tracking control issue of nonstrict feedback stochastic nonlinear systems is studied in our article by adopting backstepping algorithm. Compared with the existing research, the hypothesis about unknown virtual control coefficients (UVCC) is overcome in the control design. By using the bound estimation scheme and some smooth functions, associating with approximation-based neural network, the asymptotic tracking controller is recursively constructed. With the aid of Lyapunov function and beneficial inequalities, the asymptotic convergence character and stability with stochastic disturbance and unknown UVCC can be ensured. Finally, the theoretical finding is verified via a simulation example.}
}
@article{LV2021515,
title = {Dynamical and static multisynchronization analysis for coupled multistable memristive neural networks with hybrid control},
journal = {Neural Networks},
volume = {143},
pages = {515-524},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002677},
author = {Xiaoxiao Lv and Jinde Cao and Leszek Rutkowski},
keywords = {Multistable memristive neural networks, Hybrid controller, Delayed impulsive control, Multisynchronization, Linear matrix inequalities},
abstract = {This paper investigates the dynamical multisynchronization (DMS) and static multisynchronization (SMS) problems for a class of delayed coupled multistable memristive neural networks (DCMMNNs) via a novel hybrid controller which includes delayed impulsive control and state feedback control. Based on the state–space partition method and the geometrical properties of the activation function, each subnetwork has multiple locally exponential stable equilibrium states. By employing a new Halanay-type inequality and the impulsive control theory, some new linear matrix inequalities (LMIs)-based sufficient conditions are proposed. It is shown that the delayed impulsive control with suitable impulsive interval and allowable time-varying delay can still guarantee the DMS and SMS of DCMMNNs. Finally, a numerical example is presented to illustrate the effectiveness of the hybrid controller.}
}
@article{HAO2021595,
title = {Multi-view spectral clustering via common structure maximization of local and global representations},
journal = {Neural Networks},
volume = {143},
pages = {595-606},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.020},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002835},
author = {Wenyu Hao and Shanmin Pang and Zhikai Chen},
keywords = {Multi-view, Spectral clustering, Affinity matrix},
abstract = {The essential problem of multi-view spectral clustering is to learn a good common representation by effectively utilizing multi-view information. A popular strategy for improving the quality of the common representation is utilizing global and local information jointly. Most existing methods capture local manifold information by graph regularization. However, once local graphs are constructed, they do not change during the whole optimization process. This may lead to a degenerated common representation in the case of existing unreliable graphs. To address this problem, rather than directly using fixed local representations, we propose a dynamic strategy to construct a common local representation. Then, we impose a fusion term to maximize the common structure of the local and global representations so that they can boost each other in a mutually reinforcing manner. With this fusion term, we integrate local and global representation learning in a unified framework and design an alternative iteration based optimization procedure to solve it. Extensive experiments conducted on a number of benchmark datasets support the superiority of our algorithm over several state-of-the-art methods.}
}
@article{AN2021355,
title = {IGAGCN: Information geometry and attention-based spatiotemporal graph convolutional networks for traffic flow prediction},
journal = {Neural Networks},
volume = {143},
pages = {355-367},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.035},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002318},
author = {Jiyao An and Liang Guo and Wei Liu and Zhiqiang Fu and Ping Ren and Xinzhi Liu and Tao Li},
keywords = {Spatiotemporal traffic data, Traffic flow prediction, Graph convolutional network, Information geometry, Attention mechanism},
abstract = {In this study, a novel spatiotemporal graph convolutional networks model is proposed for traffic flow prediction in urban road networks by fully considering an information geometry approach and attention-based mechanism. Accurate traffic flow prediction in real urban road networks is challenging due to the presence of dynamic spatiotemporal data and external factors in the urban environment. Moreover, the dynamic spatial and temporal dependencies of urban traffic flow data are very important for predicting traffic flow, and it has been shown that a recent attention mechanism has a relatively good ability to capture these dynamic dependencies, which are not fully considered by most existing algorithms. Therefore, in the novel model abbreviated as IGAGCN, the information geometry method is utilized to determine the dynamic data distribution difference between different sensors. The attention mechanism is employed with the information geometry method, in which a matrix is derived by analyzing the distributions of sensor data, and the spatiotemporal dynamic connections in traffic flow data features are better at capturing the spatial dependencies of traffic between different sensors in urban road networks. Furthermore, a parallel sub-model architecture is proposed to consider long time spans, where each dilated causal convolution sub-model is applied to short time spans. Two well-known data sets were employed to demonstrate that our proposed method obtains better performance and is better at capturing the dynamic spatial dependencies of traffic than the existing only-attention-based models. In addition a real-world urban road network in Shenzhen, China, was studied to test and verify the proposed model.}
}
@article{QING2021303,
title = {Label propagation via local geometry preserving for deep semi-supervised image recognition},
journal = {Neural Networks},
volume = {143},
pages = {303-313},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002392},
author = {Yuanyuan Qing and Yijie Zeng and Guang-Bin Huang},
keywords = {Semisupervised learning, Label propagation, Image classification},
abstract = {In this paper, we propose a novel transductive pseudo-labeling based method for deep semi-supervised image recognition. Inspired from the superiority of pseudo labels inferred by label propagation compared with those inferred from network, we argue that information flow from labeled data to unlabeled data should be kept noiseless and with minimum loss. Previous research works use scarce labeled data for feature learning and solely consider the relationship between two feature vectors to construct the similarity graph in feature space, which causes two problems that ultimately lead to noisy and incomplete information flow from labeled data to unlabeled data. The first problem is that the learned feature mapping is highly likely to be biased and can easily over-fit noise. The second problem is the loss of local geometry information in feature space during label propagation. Accordingly, we firstly propose to incorporate self-supervised learning into feature learning for cleaner information flow in feature space during subsequent label propagation. Secondly, we propose to use reconstruction concept to measure pairwise similarity in feature space, such that local geometry information can be preserved. Ablation study confirms synergistic effects from features learned with self-supervision and similarity graph with local geometry preserving. Extensive experiments conducted on benchmark datasets have verified the effectiveness of our proposed method.}
}
@article{SELVARAJ2021413,
title = {Equivalent-input-disturbance estimator-based event-triggered control design for master–slave neural networks},
journal = {Neural Networks},
volume = {143},
pages = {413-424},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.023},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002550},
author = {P. Selvaraj and O.M. Kwon and S.H. Lee and R. Sakthivel},
keywords = {Master–slave neural networks, Network-induced delays, Event-triggered control, Equivalent-input-disturbance estimator},
abstract = {This paper investigates the robust synchronization problem for a class of master–slave neural networks (MSNNs) subject to network-induced delays, unknown time-varying uncertainty, and exogenous disturbances. An equivalent-input-disturbance (EID) estimation technique is applied to compensate for the effects of unknown uncertainty and disturbances in the system output. In addition, to reduce the burden of the communication channel in the addressed MSNNs and improve the utilization of bandwidth an event-triggered control protocol is developed to obtain the synchronization of MSNNs. In particular, event-triggering conditions are verified periodically at every sampling instant in both sensors and actuators to avoid the Zeno behavior in the networks. By designing an appropriate low-pass filter in the EID estimator block, the accuracy of disturbance estimation performance is improved. Moreover, by concatenating the synchronization error, observer, and filter states as a single state vector, an augmented system is formulated. Then the tangible delay-dependent stability condition for that augmented system is established by employing the Lyapunov stability theory and reciprocally convex approach. Based on the feasible solutions of the derived stability conditions, the event-triggering parameters, controller, and observer gains are co-designed. Finally, two toy examples are given to illustrate the established theoretical findings.}
}
@article{HAIDER2021537,
title = {A distributed optimisation framework combining natural gradient with Hessian-free for discriminative sequence training},
journal = {Neural Networks},
volume = {143},
pages = {537-549},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002033},
author = {Adnan Haider and Chao Zhang and Florian L. Kreyssig and Philip C. Woodland},
keywords = {Second-order optimisation, Hessian-free, Natural gradient, Conjugate gradient, Discriminative sequence training},
abstract = {This paper presents a novel natural gradient and Hessian-free (NGHF) optimisation framework for neural network training that can operate efficiently in a distributed manner. It relies on the linear conjugate gradient (CG) algorithm to combine the natural gradient (NG) method with local curvature information from Hessian-free (HF). A solution to a numerical issue in CG allows effective parameter updates to be generated with far fewer CG iterations than usually used (e.g. 5-8 instead of 200). This work also presents a novel preconditioning approach to improve the progress made by individual CG iterations for models with shared parameters. Although applicable to other training losses and model structures, NGHF is investigated in this paper for lattice-based discriminative sequence training for hybrid hidden Markov model acoustic models using a standard recurrent neural network, long short-term memory, and time delay neural network models for output probability calculation. Automatic speech recognition experiments are reported on the multi-genre broadcast data set for a range of different acoustic model types. These experiments show that NGHF achieves larger word error rate reductions than standard stochastic gradient descent or Adam, while requiring orders of magnitude fewer parameter updates.}
}
@article{ANNABI2021638,
title = {Bidirectional interaction between visual and motor generative models using Predictive Coding and Active Inference},
journal = {Neural Networks},
volume = {143},
pages = {638-656},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002793},
author = {Louis Annabi and Alexandre Pitti and Mathias Quoy},
keywords = {Visuo-motor control, Predictive coding, Active inference, Developmental robotics, Embodiment},
abstract = {In this work, we build upon the Active Inference (AIF) and Predictive Coding (PC) frameworks to propose a neural architecture comprising a generative model for sensory prediction, and a distinct generative model for motor trajectories. We highlight how sequences of sensory predictions can act as rails guiding learning, control and online adaptation of motor trajectories. We furthermore inquire the effects of bidirectional interactions between the motor and the visual modules. The architecture is tested on the control of a simulated robotic arm learning to reproduce handwritten letters.}
}
@article{LIU2021108,
title = {Locality preserving dense graph convolutional networks with graph context-aware node representations},
journal = {Neural Networks},
volume = {143},
pages = {108-120},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.031},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002276},
author = {Wenfeng Liu and Maoguo Gong and Zedong Tang and A.K. Qin and Kai Sheng and Mingliang Xu},
keywords = {Representation learning, Graph convolutional network, Graph classification, Node representation, Locality preserving, Dense connection},
abstract = {Graph convolutional networks (GCNs) have been widely used for representation learning on graph data, which can capture structural patterns on a graph via specifically designed convolution and readout operations. In many graph classification applications, GCN-based approaches have outperformed traditional methods. However, most of the existing GCNs are inefficient to preserve local information of graphs — a limitation that is especially problematic for graph classification. In this work, we propose a locality-preserving dense GCN with graph context-aware node representations. Specifically, our proposed model incorporates a local node feature reconstruction module to preserve initial node features into node representations, which is realized via a simple but effective encoder–decoder mechanism. To capture local structural patterns in neighborhoods representing different ranges of locality, dense connectivity is introduced to connect each convolutional layer and its corresponding readout with all previous convolutional layers. To enhance node representativeness, the output of each convolutional layer is concatenated with the output of the previous layer’s readout to form a global context-aware node representation. In addition, a self-attention module is introduced to aggregate layer-wise representations to form the final graph-level representation. Experiments on benchmark datasets demonstrate the superiority of the proposed model over state-of-the-art methods in terms of classification accuracy.}
}
@article{2023ii,
title = {Editorial Board},
journal = {Neural Networks},
volume = {164},
pages = {ii},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(23)00314-3},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023003143}
}
@article{COUELLAN2021138,
title = {Probabilistic robustness estimates for feed-forward neural networks},
journal = {Neural Networks},
volume = {142},
pages = {138-147},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.04.037},
url = {https://www.sciencedirect.com/science/article/pii/S089360802100174X},
author = {Nicolas Couellan},
keywords = {Deep neural network, Neural network robustness, Random noise attacks, Regularization},
abstract = {Robustness of deep neural networks is a critical issue in practical applications. In the general case of feed-forward neural networks (including convolutional deep neural network architectures), under random noise attacks, we propose to study the probability that the output of the network deviates from its nominal value by a given threshold. We derive a simple concentration inequality for the propagation of the input uncertainty through the network using the Cramer–Chernoff method and estimates of the local variation of the neural network mapping computed at the training points. We further discuss and exploit the resulting condition on the network to regularize the loss function during training. Finally, we assess the proposed tail probability estimates empirically on various public datasets and show that the observed robustness is very well estimated by the proposed method.}
}
@article{JI202188,
title = {A semi-supervised zero-shot image classification method based on soft-target},
journal = {Neural Networks},
volume = {143},
pages = {88-96},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S089360802100215X},
author = {Zhong Ji and Qiang Wang and Biying Cui and Yanwei Pang and Xianbin Cao and Xuelong Li},
keywords = {Zero-shot learning, Image classification, Autoencoder, Soft-Target, Semi-supervised learning},
abstract = {Zero-shot learning (ZSL) aims at training a classification model with data only from seen categories to recognize data from disjoint unseen categories. Domain shift and generalization capability are two fundamental challenges in ZSL. In this paper, we address them with a novel Soft-Target Semi-supervised Classification (STSC) model. Specifically, an autoencoder network is leveraged, where both labeled seen data from the seen categories and unlabeled ancillary data collected from Internet or other datasets are employed as two branches, respectively. For the branch of labeled seen data, side information are employed as the latent vectors to separately connect the input of encoder and the output of decoder. In this way, visual and side information are implicitly aligned. For the branch of unlabeled ancillary data, it explicitly strengthens the reconstruction ability of the network. Meanwhile, these ancillary data can be viewed as a smooth to the domain distribution, which contributes to the alleviation of the domain shift problem. To further guarantee the generation ability, a Softmax-T loss function is proposed by making full use of the soft target. Extensive experiments on three benchmark datasets show the superiority of the proposed approach under tasks of both traditional zero-shot learning and generalized zero-shot learning.}
}
@article{WANG2021180,
title = {Two-timescale neurodynamic approaches to supervised feature selection based on alternative problem formulations},
journal = {Neural Networks},
volume = {142},
pages = {180-191},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.04.038},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021001751},
author = {Yadi Wang and Jun Wang and Hangjun Che},
keywords = {Feature selection, Recurrent neural networks, Neurodynamic optimization, Two-timescale neurodynamics},
abstract = {Feature selection is a crucial step in data processing and machine learning. While many greedy and sequential feature selection approaches are available, a holistic neurodynamics approach to supervised feature selection is recently developed via fractional programming by minimizing feature redundancy and maximizing relevance simultaneously. In view that the gradient of the fractional objective function is also fractional, alternative problem formulations are desirable to obviate the fractional complexity. In this paper, the fractional programming problem formulation is equivalently reformulated as bilevel and bilinear programming problems without using any fractional function. Two two-timescale projection neural networks are adapted for solving the reformulated problems. Experimental results on six benchmark datasets are elaborated to demonstrate the global convergence and high classification performance of the proposed neurodynamic approaches in comparison with six mainstream feature selection approaches.}
}
@article{YUAN2021509,
title = {Instance elimination strategy for non-convex multiple-instance learning using sparse positive bags},
journal = {Neural Networks},
volume = {142},
pages = {509-521},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002720},
author = {Min Yuan and Yitian Xu and Renxiu Feng and Zongmin Liu},
keywords = {Multiple-instance learning, Small sphere and large margin, Sparse, Safe screening, Non-convex hypersphere support vector machine},
abstract = {In some multiple instance learning (MIL) applications, positive bags are sparse (i.e. containing only a small fraction of positive instances). To deal with the imbalanced data caused by these situations, we present a novel MIL method based on a small sphere and large margin approach (SSLM-MIL). Due to the introduction of a large margin, SSLM-MIL enforces the desired constraint that for all positive bags, there is at least one positive instance in each bag. Moreover, our framework is flexible to incorporate the non-convex optimization problem. Therefore, we can solve it using the concave–convex procedure (CCCP). Still, CCCP may be computationally inefficient for the number of external iterations. Inspired by the existing safe screening rules, which can effectively reduce computational time by discarding some inactive instances. In this paper, we propose a strategy to reduce the scale of the optimization problem. Specifically, we construct a screening rule in the inner solver and another rule for propagating screened instances between iterations of CCCP. To the best of our knowledge, this is the first attempt to introduce safe instance screening to a non-convex hypersphere support vector machine. Experiments on thirty-one benchmark datasets demonstrate the safety and effectiveness of our approach.}
}
@article{SHI2021329,
title = {H-VECTORS: Improving the robustness in utterance-level speaker embeddings using a hierarchical attention model},
journal = {Neural Networks},
volume = {142},
pages = {329-339},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.024},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002203},
author = {Yanpei Shi and Qiang Huang and Thomas Hain},
keywords = {Speaker embeddings, Hierarchical attention, Speaker identification, Speaker verification, Attention mechanism},
abstract = {In this paper, a hierarchical attention network is proposed to generate robust utterance-level embeddings (H-vectors) for speaker identification and verification. Since different parts of an utterance may have different contributions to speaker identities, the use of hierarchical structure aims to learn speaker related information locally and globally. In the proposed approach, frame-level encoder and attention are applied on segments of an input utterance and generate individual segment vectors. Then, segment level attention is applied on the segment vectors to construct an utterance representation. To evaluate the quality of the learned utterance-level speaker embeddings on speaker identification and verification, the proposed approach is tested on several benchmark datasets, such as the NIST SRE2008 Part1, the Switchboard Cellular (Part1), the CallHome American English Speech ,the Voxceleb1 and Voxceleb2 datasets. In comparison with some strong baselines, the obtained results show that the use of H-vectors can achieve better identification and verification performances in various acoustic conditions.}
}
@article{AKAI202142,
title = {Experimental stability analysis of neural networks in classification problems with confidence sets for persistence diagrams},
journal = {Neural Networks},
volume = {143},
pages = {42-51},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021001994},
author = {Naoki Akai and Takatsugu Hirayama and Hiroshi Murase},
keywords = {Neural networks, Topological data analysis, Persistent homology},
abstract = {We investigate classification performance of neural networks (NNs) based on topological insight in an attempt to guarantee stability of their inference. NNs which can accurately classify a dataset map it into a hidden space while disentangling intertwined data. NNs sometimes acquire forcible mapping to disentangle the data, and this forcible mapping generates outliers. The mapping around the outliers is unstable because the outputs change drastically. Hence, we define stable NNs to mean that they do not generate outliers. To investigate the possibility of the existence of outliers, we use persistent homology and a method to estimate the confidence set for persistence diagrams. The combined use enables us to test whether the focused geometry is topologically simple, that is, no outliers. In this work, we use the MNIST and CIFAR-10 datasets and investigate the relationship between the classification performance and the topological characteristics with several NNs. Investigation results with the MNIST dataset show that the test accuracy of all the networks is superior, exceeding 98%, even though the transformed dataset is not topologically simple. Results with the CIFAR-10 dataset also show that the possibility of the existence of outliers is shown in the mapping by the accurate convolutional NNs. Therefore, we conclude that the presented investigation is necessary to guarantee that the NNs, in particular deep NNs, do not acquire unstable mapping for forcible classification.}
}
@article{AHMADIPOUR2021680,
title = {Subspace-based predictive control of Parkinson’s disease: A model-based study},
journal = {Neural Networks},
volume = {142},
pages = {680-689},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.025},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002963},
author = {Mahboubeh Ahmadipour and Mojtaba Barkhordari-Yazdi and Saeid R. Seydnejad},
keywords = {Parkinson’s disease, Basal ganglia, Deep brain stimulation, Predictive control, Subspace identification},
abstract = {Deep brain stimulation (DBS) of the Basal Ganglia (BG) is an effective treatment to suppress the symptoms of Parkinson’s disease (PD). Using a closed-loop scheme in DBS can not only improve its therapeutic effects but it can also reduce its energy consumption and possible side effects. In this paper, a predictive closed loop control strategy is employed to suppress the PD in real-time. A linear multi-input multi-output (MIMO) state-delayed system is considered as a simplified model of the BG neuronal network relating the stimulation signals as inputs to the beta power of local field potentials as PD biomarkers. The effect of time delay in different areas of the BG is incorporated into this model and a real-time subspace-based identification is implemented to continuously model the state of the BG neuronal network and drive the predictive control strategy. Simulation results show that the proposed MIMO subspace based predictive controller can suppress PD symptoms more effectively and with less power consumption compared to the conventional open-loop DBS and a recently proposed single-input single-output closed loop controller.}
}
@article{JI2021209,
title = {How to handle noisy labels for robust learning from uncertainty},
journal = {Neural Networks},
volume = {143},
pages = {209-217},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002446},
author = {Daehyun Ji and Dokwan Oh and Yoonsuk Hyun and Oh-Min Kwon and Myeong-Jin Park},
keywords = {Deep network, Noisy labels, Robust learning, Uncertain aware joint training},
abstract = {Most deep neural networks (DNNs) are trained with large amounts of noisy labels when they are applied. As DNNs have the high capacity to fit any noisy labels, it is known to be difficult to train DNNs robustly with noisy labels. These noisy labels cause the performance degradation of DNNs due to the memorization effect by over-fitting. Earlier state-of-the-art methods used small loss tricks to efficiently resolve the robust training problem with noisy labels. In this paper, relationship between the uncertainties and the clean labels is analyzed. We present novel training method to use not only small loss trick but also labels that are likely to be clean labels selected from uncertainty called “Uncertain Aware Co-Training (UACT)”. Our robust learning techniques (UACT) avoid over-fitting the DNNs by extremely noisy labels. By making better use of the uncertainty acquired from the network itself, we achieve good generalization performance. We compare the proposed method to the current state-of-the-art algorithms for noisy versions of MNIST, CIFAR-10, CIFAR-100, T-ImageNet and News to demonstrate its excellence.}
}
@article{YAN2021500,
title = {Novel methods to global Mittag-Leffler stability of delayed fractional-order quaternion-valued neural networks},
journal = {Neural Networks},
volume = {142},
pages = {500-508},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002689},
author = {Hongyun Yan and Yuanhua Qiao and Lijuan Duan and Jun Miao},
keywords = {Fractional-order, Mittag-Leffler stability, Quaternion-valued neural networks, Leakage delay, Time-varying delay},
abstract = {In this paper, a type of fractional-order quaternion-valued neural networks (FOQVNNs) with leakage and time-varying delays is established to simulate real-world situations, and the global Mittag-Leffler stability of the system is investigated by using the non-decomposition method. First, to avoid decomposing the system into two complex-valued systems or four real-valued systems, a new sign function for quaternion numbers is introduced based on the ones for real and complex numbers. And two novel lemmas for quaternion-valued sign function and Caputo fractional derivative are established in quaternion domain, which are used to investigate the stability of FOQVNNs. Second, a concise and flexible quaternion-valued state feedback controller is directly designed and a novel 1-norm Lyapunov function composed of the absolute values of real and imaginary parts is established. Then, based on the designed quaternion-valued state feedback controller and the proposed lemmas, some sufficient conditions are given to ensure the global Mittag-Leffler stability of the system. Finally, a numerical simulation is given to verify the theoretical results.}
}
@article{YUAN2021457,
title = {Capped L2,p-norm metric based robust least squares twin support vector machine for pattern classification},
journal = {Neural Networks},
volume = {142},
pages = {457-478},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.028},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002604},
author = {Chao Yuan and Liming Yang},
keywords = {Classification, Robustness, Capped -norm, LSTSVM, Iterative algorithm},
abstract = {Least squares twin support vector machine (LSTSVM) is an effective and efficient learning algorithm for pattern classification. However, the distance in LSTSVM is measured by squared L2-norm metric that may magnify the influence of outliers. In this paper, a novel robust least squares twin support vector machine framework is proposed for binary classification, termed as CL2,p-LSTSVM, which utilizes capped L2,p-norm distance metric to reduce the influence of noise and outliers. The goal of CL2,p-LSTSVM is to minimize the capped L2,p-norm intra-class distance dispersion, and eliminate the influence of outliers during training process, where the value of the metric is controlled by the capped parameter, which can ensure better robustness. The proposed metric includes and extends the traditional metrics by setting appropriate values of p and capped parameter. This strategy not only retains the advantages of LSTSVM, but also improves the robustness in solving a binary classification problem with outliers. However, the nonconvexity of metric makes it difficult to optimize. We design an effective iterative algorithm to solve the CL2,p-LSTSVM. In each iteration, two systems of linear equations are solved. Simultaneously, we present some insightful analyses on the computational complexity and convergence of algorithm. Moreover, we extend the CL2,p-LSTSVM to nonlinear classifier and semi-supervised classification. Experiments are conducted on artificial datasets, UCI benchmark datasets, and image datasets to evaluate our method. Under different noise settings and different evaluation criteria, the experiment results show that the CL2,p-LSTSVM has better robustness than state-of-the-art approaches in most cases, which demonstrates the feasibility and effectiveness of the proposed method.}
}
@article{COSSU2021607,
title = {Continual learning for recurrent neural networks: An empirical evaluation},
journal = {Neural Networks},
volume = {143},
pages = {607-627},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.021},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002847},
author = {Andrea Cossu and Antonio Carta and Vincenzo Lomonaco and Davide Bacciu},
keywords = {Continual learning, Recurrent neural networks, Benchmarks, Evaluation},
abstract = {Learning continuously during all model lifetime is fundamental to deploy machine learning solutions robust to drifts in the data distribution. Advances in Continual Learning (CL) with recurrent neural networks could pave the way to a large number of applications where incoming data is non stationary, like natural language processing and robotics. However, the existing body of work on the topic is still fragmented, with approaches which are application-specific and whose assessment is based on heterogeneous learning protocols and datasets. In this paper, we organize the literature on CL for sequential data processing by providing a categorization of the contributions and a review of the benchmarks. We propose two new benchmarks for CL with sequential data based on existing datasets, whose characteristics resemble real-world applications. We also provide a broad empirical evaluation of CL and Recurrent Neural Networks in class-incremental scenario, by testing their ability to mitigate forgetting with a number of different strategies which are not specific to sequential data processing. Our results highlight the key role played by the sequence length and the importance of a clear specification of the CL scenario.}
}
@article{WEN202152,
title = {A continuous-time neurodynamic approach and its discretization for distributed convex optimization over multi-agent systems},
journal = {Neural Networks},
volume = {143},
pages = {52-65},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002161},
author = {Xingnan Wen and Linhua Luan and Sitian Qin},
keywords = {Distributed optimization problem, Neurodynamic approach, Penalty method, Multi-agent systems, Convergence},
abstract = {Distributed optimization problem (DOP) over multi-agent systems, which can be described by minimizing the sum of agents’ local objective functions, has recently attracted widespread attention owing to its applications in diverse domains. In this paper, inspired by penalty method and subgradient descent method, a continuous-time neurodynamic approach is proposed for solving a DOP with inequality and set constraints. The state of continuous-time neurodynamic approach exists globally and converges to an optimal solution of the considered DOP. Comparisons reveal that the proposed neurodynamic approach can not only resolve more general convex DOPs, but also has lower dimension of solution space. Additionally, the discretization of the neurodynamic approach is also introduced for the convenience of implementation in practice. The iteration sequence of discrete-time method is also convergent to an optimal solution of DOP from any initial point. The effectiveness of the neurodynamic approach is verified by simulation examples and an application in L1-norm minimization problem in the end.}
}
@article{ATARASHI2021500,
title = {Sparse random feature maps for the item-multiset kernel},
journal = {Neural Networks},
volume = {143},
pages = {500-514},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.024},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002562},
author = {Kyohei Atarashi and Satoshi Oyama and Masahito Kurihara},
keywords = {Kernel method, Random feature},
abstract = {Random feature maps are a promising tool for large-scale kernel methods. Since most random feature maps generate dense random features causing memory explosion, it is hard to apply them to very-large-scale sparse datasets. The factorization machines and related models, which use feature combinations efficiently, scale well for large-scale sparse datasets and have been used in many applications. However, their optimization problems are typically non-convex. Therefore, although they are optimized by using gradient-based iterative methods, such methods cannot find global optimum solutions in general and require a large number of iterations for convergence. In this paper, we define the item-multiset kernel, which is a generalization of the itemset kernel and dot product kernels. Unfortunately, random feature maps for the itemset kernel and dot product kernels cannot approximate the item-multiset kernel. We thus develop a method that converts an item-multiset kernel into an itemset kernel, enabling the item-multiset kernel to be approximated by using a random feature map for the itemset kernel. We propose two random feature maps for the itemset kernel, which run faster and are more memory efficient than the existing feature map for the itemset kernel. They also generate sparse random features when the original (input) feature vector is sparse and thus linear models using proposed methods . Experiments using real-world datasets demonstrated the effectiveness of the proposed methodology: linear models using the proposed random feature maps ran from 10 to 100 times faster than ones based on existing methods.}
}
@article{GUIZZO2021238,
title = {Anti-transfer learning for task invariance in convolutional neural networks for speech processing},
journal = {Neural Networks},
volume = {142},
pages = {238-251},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002045},
author = {Eric Guizzo and Tillman Weyde and Giacomo Tarroni},
keywords = {Audio processing, Convolutional neural networks, Invariance transfer, Transfer learning},
abstract = {We introduce the novel concept of anti-transfer learning for speech processing with convolutional neural networks. While transfer learning assumes that the learning process for a target task will benefit from re-using representations learned for another task, anti-transfer avoids the learning of representations that have been learned for an orthogonal task, i.e., one that is not relevant and potentially confounding for the target task, such as speaker identity for speech recognition or speech content for emotion recognition. This extends the potential use of pre-trained models that have become increasingly available. In anti-transfer learning, we penalize similarity between activations of a network being trained on a target task and another one previously trained on an orthogonal task, which yields more suitable representations. This leads to better generalization and provides a degree of control over correlations that are spurious or undesirable, e.g. to avoid social bias. We have implemented anti-transfer for convolutional neural networks in different configurations with several similarity metrics and aggregation functions, which we evaluate and analyze with several speech and audio tasks and settings, using six datasets. We show that anti-transfer actually leads to the intended invariance to the orthogonal task and to more appropriate features for the target task at hand. Anti-transfer learning consistently improves classification accuracy in all test cases. While anti-transfer creates computation and memory cost at training time, there is relatively little computation cost when using pre-trained models for orthogonal tasks. Anti-transfer is widely applicable and particularly useful where a specific invariance is desirable or where labeled data for orthogonal tasks are difficult to obtain on a given dataset but pre-trained models are available.}
}
@article{ALSAEGH2021433,
title = {CutCat: An augmentation method for EEG classification},
journal = {Neural Networks},
volume = {141},
pages = {433-443},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.032},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002288},
author = {Ali Al-Saegh and Shefa A. Dawwd and Jassim M. Abdul-Jabbar},
keywords = {Augmentation, BCI, CNN, Deep learning, EEG, Motor imagery},
abstract = {The non-invasive electroencephalogram (EEG) signals enable humans to communicate with devices and have control over them, this process requires precise classification and identification of those signals. The recent revolution of deep learning has empowered both feature extraction and classification in a joint manner of different data types. However, deep learning is a data learning approach that demands a large number of training samples. Whilst, the EEG research field lacks a large amount of data which restricts the use of deep learning within this field. This paper proposes a novel augmentation method for enlarging EEG datasets. Our CutCat augmentation method generates trials from inter- and intra-subjects and trials. The method relies on cutting a specific period from an EEG trial and concatenating it with a period from another trial from the same subject or different subjects. The method has been tested on shallow and deep convolutional neural networks (CNN) for the classification of motor imagery (MI) EEG data. Two input formulation types images and time-series have been used as input to the neural networks. Short-time Fourier transform (STFT) is used for generating training images from the time-series signals. The experimental results demonstrate that the proposed augmentation method is a promising strategy for handling the classification of small-scale datasets. Classification results on two EEG datasets show advancement in comparison with the results of state-of-the-art researches.}
}
@article{AN2021250,
title = {Effective and direct control of neural TTS prosody by removing interactions between different attributes},
journal = {Neural Networks},
volume = {143},
pages = {250-260},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002380},
author = {Xiaochun An and Frank K. Soong and Shan Yang and Lei Xie},
keywords = {Neural TTS, Prosody control, Variational autoencoder (VAE), Prosodic features, Mutual information minimization},
abstract = {End-to-end TTS advancement has shown that synthesized speech prosody can be controlled by conditioning the decoder with speech prosody attribute labels. However, to annotate quantitatively the prosody patterns of a large set of training data is both time consuming and expensive. To use unannotated data, variational autoencoder (VAE) has been proposed to model individual prosody attribute as a random variable in the latent space. The VAE is an unsupervised approach and the corresponding latent variables are in general correlated with each other. For more effective and direct control of speech prosody along each attribute dimension, it is highly desirable to disentangle the correlated latent variables. Additionally, being able to interpret the disentangled attributes as speech perceptual cues is useful for designing more efficient prosody control of TTS. In this paper, we propose two attribute separation schemes: (1) using 3 separate VAEs to model the real-valued, different prosodic features, i.e., F0, energy and duration; (2) minimizing mutual information between different prosody attributes to remove their mutual correlations, for facilitating more direct prosody control. Experimental results confirm that the two proposed schemes can indeed make individual prosody attributes more interpretable and direct TTS prosody control more effective. The improvements are measured objectively by F0 Frame Error (FFE) and subjectively with MOS and A/B comparison listening tests, respectively. The scatter diagrams of t-SNE also demonstrate the correlations between prosody attributes, which are well disentangled by minimizing their mutual information. Synthesized TTS samples can be found at https://xiaochunan.github.io/prosody/index.html.}
}
@article{RACCA2021252,
title = {Robust Optimization and Validation of Echo State Networks for learning chaotic dynamics},
journal = {Neural Networks},
volume = {142},
pages = {252-268},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021001969},
author = {Alberto Racca and Luca Magri},
keywords = {Chaotic dynamical systems, Reservoir Computing, Robustness},
abstract = {An approach to the time-accurate prediction of chaotic solutions is by learning temporal patterns from data. Echo State Networks (ESNs), which are a class of Reservoir Computing, can accurately predict the chaotic dynamics well beyond the predictability time. Existing studies, however, also showed that small changes in the hyperparameters may markedly affect the network’s performance. The overarching aim of this paper is to improve the robustness in the selection of hyperparameters in Echo State Networks for the time-accurate prediction of chaotic solutions. We define the robustness of a validation strategy as its ability to select hyperparameters that perform consistently between validation and test sets. The goal is three-fold. First, we investigate routinely used validation strategies. Second, we propose the Recycle Validation, and the chaotic versions of existing validation strategies, to specifically tackle the forecasting of chaotic systems. Third, we compare Bayesian optimization with the traditional grid search for optimal hyperparameter selection. Numerical tests are performed on prototypical nonlinear systems that have chaotic and quasiperiodic solutions, such as the Lorenz and Lorenz-96 systems, and the Kuznetsov oscillator. Both model-free and model-informed Echo State Networks are analysed. By comparing the networks’ performance in learning chaotic (unpredictable) versus quasiperiodic (predictable) solutions, we highlight fundamental challenges in learning chaotic solutions. The proposed validation strategies, which are based on the dynamical systems properties of chaotic time series, are shown to outperform the state-of-the-art validation strategies. Because the strategies are principled – they are based on chaos theory such as the Lyapunov time – they can be applied to other Recurrent Neural Networks architectures with little modification. This work opens up new possibilities for the robust design and application of Echo State Networks, and Recurrent Neural Networks, to the time-accurate prediction of chaotic systems.}
}
@article{LUQUE2023676,
title = {Corrigendum to “Electrical coupling regulated by GABAergic nucleo-olivary afferent fibres facilitates cerebellar sensory–motor adaptation” [Neural Netw. 155 (2022) 422–438]},
journal = {Neural Networks},
volume = {165},
pages = {676},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.055},
url = {https://www.sciencedirect.com/science/article/pii/S089360802300299X},
author = {Niceto R. Luque and Francisco Naveros and Ignacio Abadía and Eduardo Ros and Angelo Arleo}
}
@article{YANG2021597,
title = {Non-native acoustic modeling for mispronunciation verification based on language adversarial representation learning},
journal = {Neural Networks},
volume = {142},
pages = {597-607},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S089360802100280X},
author = {Longfei Yang and Kaiqi Fu and Jinsong Zhang and Takahiro Shinozaki},
keywords = {Mispronunciation verification, Computer aided pronunciation training, Language adversarial training, Unsupervised learning, Non-native acoustic modeling},
abstract = {Non-native mispronunciation verification is designed to provide feedback to guide language learners to correct their pronunciation errors in their further learning and it plays an important role in the computer-aided pronunciation training (CAPT) system. Most existing approaches focus on establishing the acoustic model directly using non-native corpus thus they are suffering the data sparsity problem due to time-consuming non-native speech data collection and annotation tasks. In this work, to address this problem, we propose a pre-trained approach to utilize the speech data of two native languages (the learner’s native and target languages) for non-native mispronunciation verification. We set up an unsupervised model to extract knowledge from a large scale of unlabeled raw speech of the target language by making predictions about future observations in the speech signal, then the model is trained with language adversarial training using the learner’s native language to align the feature distribution of two languages by confusing a language discriminator. In addition, sinc filter is incorporated at the first convolutional layer to capture the formant-like feature. Formant is relevant to the place and manner of articulation. Therefore, it is useful not only for pronunciation error detection but also for providing instructive feedback. Then the pre-trained model serves as the feature extractor in the downstream mispronunciation verification task. Through the experiments on the Japanese part of the BLCU inter-Chinese speech corpus, the experimental results demonstrate that for the non-native phone recognition and mispronunciation verification tasks (1) the knowledge learned from two native languages speech with the proposed unsupervised approach is useful for these two tasks (2) our proposed language adversarial representation learning is effective to improve the performance (3) formant-like feature can be incorporated by introducing sinc filter to further improve the performance of mispronunciation verification.}
}
@article{2023I,
title = {CURRENT EVENTS},
journal = {Neural Networks},
volume = {164},
pages = {I},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(23)00317-9},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023003179}
}
@article{SALIMINEZHAD2021548,
title = {A neuromimetic realization of hippocampal CA1 for theta wave generation},
journal = {Neural Networks},
volume = {142},
pages = {548-563},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002653},
author = {Nima Salimi-Nezhad and Mohammad Hasanlou and Mahmood Amiri and Georgios A. Keliris},
keywords = {Hippocampus, CA1, Theta oscillations, Neuromorphic system, Spiking neural network},
abstract = {Recent advances in neural engineering allowed the development of neuroprostheses which facilitate functionality in people with neurological problems. In this research, a real-time neuromorphic system is proposed to artificially reproduce the theta wave and firing patterns of different neuronal populations in the CA1, a sub-region of the hippocampus. The hippocampal theta oscillations (4–12 Hz) are an important electrophysiological rhythm that contributes in various cognitive functions, including navigation, memory, and novelty detection. The proposed CA1 neuromimetic circuit includes 100 linearized Pinsky–Rinzel neurons and 668 excitatory and inhibitory synapses on a field programmable gate array (FPGA). The implemented spiking neural network of the CA1 includes the main neuronal populations for the theta rhythm generation: excitatory pyramidal cells, PV+ basket cells, and Oriens Lacunosum-Moleculare (OLM) cells which are inhibitory interneurons. Moreover, the main inputs to the CA1 region from the entorhinal cortex via the perforant pathway, the CA3 via Schaffer collaterals, and the medial septum via fimbria–fornix are also implemented on the FPGA using a bursting leaky-integrate and fire (LIF) neuron model. The results of hardware realization show that the proposed CA1 neuromimetic circuit successfully reconstructs the theta oscillations and functionally illustrates the phase relations between firing responses of the different neuronal populations. It is also evaluated the impact of medial septum elimination on the firing patterns of the CA1 neuronal population and the theta wave’s characteristics. This neuromorphic system can be considered as a potential platform that opens opportunities for neuroprosthetic applications in future works.}
}
@article{XIAN202197,
title = {Convolutional fusion network for monaural speech enhancement},
journal = {Neural Networks},
volume = {143},
pages = {97-107},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002136},
author = {Yang Xian and Yang Sun and Wenwu Wang and Syed Mohsen Naqvi},
keywords = {Convolutional neural network, Model capacity, Shuffle, Group convolutional fusion unit, Depth-wise separable convolution, Intra skip connection},
abstract = {Convolutional neural network (CNN) based methods, such as the convolutional encoder–decoder network, offer state-of-the-art results in monaural speech enhancement. In the conventional encoder–decoder network, large kernel size is often used to enhance the model capacity, which, however, results in low parameter efficiency. This could be addressed by using group convolution, as in AlexNet, where group convolutions are performed in parallel in each layer, before their outputs are concatenated. However, with the simple concatenation, the inter-channel dependency information may be lost. To address this, the Shuffle network re-arranges the outputs of each group before concatenating them, by taking part of the whole input sequence as the input to each group of convolution. In this work, we propose a new convolutional fusion network (CFN) for monaural speech enhancement by improving model performance, inter-channel dependency, information reuse and parameter efficiency. First, a new group convolutional fusion unit (GCFU) consisting of the standard and depth-wise separable CNN is used to reconstruct the signal. Second, the whole input sequence (full information) is fed simultaneously to two convolution networks in parallel, and their outputs are re-arranged (shuffled) and then concatenated, in order to exploit the inter-channel dependency within the network. Third, the intra skip connection mechanism is used to connect different layers inside the encoder as well as decoder to further improve the model performance. Extensive experiments are performed to show the improved performance of the proposed method as compared with three recent baseline methods.}
}
@article{WANG2021400,
title = {Enhanced image prior for unsupervised remoting sensing super-resolution},
journal = {Neural Networks},
volume = {143},
pages = {400-412},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002379},
author = {Jiaming Wang and Zhenfeng Shao and Xiao Huang and Tao Lu and Ruiqian Zhang and Jiayi Ma},
keywords = {Latent space, Satellite imagery, Unsupervised learning, Prior enhancement, Super resolution},
abstract = {Numerous approaches based on training low-high resolution image pairs have been proposed to address the super-resolution (SR) task. Despite their success, low-high resolution image pairs are usually difficult to obtain in certain scenarios, and these methods are limited in the actual scene (unknown or non-ideal image acquisition process). In this paper, we proposed a novel unsupervised learning framework, termed Enhanced Image Prior (EIP), which achieves SR tasks without low/high resolution image pairs. We first feed random noise maps into a designed generative adversarial network (GAN) for satellite image SR reconstruction. Then, we convert the reference image to latent space as the enhanced image prior. Finally, we update the input noise in the latent space with a recurrent updating strategy, and further transfer the texture and structured information from the reference image. Results on extensive experiments on the Draper dataset show that EIP achieves significant improvements over state-of-the-art unsupervised SR methods both quantitatively and qualitatively. Our experiments on satellite (SuperView-1) images reveal the potential of the proposed approach in improving the resolution of remote sensing imagery compared with the supervised algorithms. Source code is available at https://github.com/jiaming-wang/EIP.}
}
@article{YANG2021564,
title = {A CNN model embedded with local feature knowledge and its application to time-varying signal classification},
journal = {Neural Networks},
volume = {142},
pages = {564-572},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002811},
author = {Ruiping Yang and Xianyu Zha and Kun Liu and Shaohua Xu},
keywords = {Time-varying signal classification, CNN feature extraction, Local feature knowledge embedding, Imbalanced datasets, ECG signal classification},
abstract = {A novel convolutional neural network is proposed for local prior feature embedding and imbalanced dataset modeling for multi-channel time-varying signal classification. This model consists of a single-channel signal feature parallel extraction unit, a multi-channel signal feature integration unit, a local feature embedding and feature similarity measurement unit, a full connection layer, and a Softmax classifier. An algorithm combining dynamic clustering and sliding window was used to select segments signals with typical local features in each pattern class, forming a typical local feature set. The one-dimensional CNNs were used to extract features from the single-channel signal in parallel, a comprehensive feature matrix of the multi-channel signal and the local feature matrix templates were produced. Using the method of external embedding, based on the sliding window and dynamic time warping (DTW) algorithm, the local feature similarities between the local feature template of each pattern class and the comprehensive feature sub-matrix of the input signal were measured, and the maximum values were selected to construct a local feature similarity vector in order. The information fusion was realized through a full connection layer. The proposed methodology can extract and represent both global and local signals features, strengthen the role of prior local feature in classification and improve the modeling properties of imbalanced datasets. A comprehensive learning algorithm is presented in this paper. The classification diagnosis of cardiovascular disease based on 12-lead ECG signals was used as a verification experiment. Results showed that the accuracy and generalization for the proposed technique were significantly improved.}
}
@article{ZHANG2021386,
title = {A global neural network learning machine: Coupled integer and fractional calculus operator with an adaptive learning scheme},
journal = {Neural Networks},
volume = {143},
pages = {386-399},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.021},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002537},
author = {Huaqing Zhang and Yi-Fei Pu and Xuetao Xie and Bingran Zhang and Jian Wang and Tingwen Huang},
keywords = {Swarm intelligent, Fractional calculus, Global optimization, Neural network, Adaptive learning rate},
abstract = {Find the global optimal solution of the model is one promising research topic in computational intelligent community. Dependent on analogies to natural processes, the evolutionary swarm intelligent algorithms are widely used for solving global optimization problems which directed by the fitness values. In this paper, we propose one efficient fractional global learning machine (Fragmachine) which includes two stages (descending and ascending) to determine the optimal search path. The neural network model is used to approach the given fitness value. Specifically, for the descending stage, the integer gradient of the network output with respect the current location is employed to find the next descending point, while for the ascending stage, the fractional gradient is implemented to climb and escape from the local optimal point. We further propose one adaptive learning rate during training which relies on both the current gradient (integer or fractional) information and the fitness value. Finally, a series of numerical experiments verify the effectiveness of the proposed algorithm, Fragmachine.}
}
@article{XIE2021375,
title = {Self-spectral learning with GAN based spectral–spatial target detection for hyperspectral image},
journal = {Neural Networks},
volume = {142},
pages = {375-387},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.029},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002252},
author = {Weiying Xie and Jiaqing Zhang and Jie Lei and Yunsong Li and Xiuping Jia},
keywords = {Spatial–spectral target detection, Self-spectral learning, Feature extraction, Band selection, Hyperspectral image (HSI)},
abstract = {To alleviate the shortcomings of target detection in only one aspect and reduce redundant information among adjacent bands, we propose a spectral–spatial target detection (SSTD) framework in deep latent space based on self-spectral learning (SSL) with a spectral generative adversarial network (GAN). The concept of SSL is introduced into hyperspectral feature extraction in an unsupervised fashion with the purpose of background suppression and target saliency. In particular, a novel structure-to-structure selection rule that takes full account of the structure, contrast, and luminance similarity is established to interpret the mapping relationship between the latent spectral feature space and the original spectral band space, to generate the optimal spectral band subset without any prior knowledge. Finally, the comprehensive result is achieved by nonlinearly combining the spatial detection on the fused latent features with the spectral detection on the selected band subset and the corresponding selected target signature. This paper paves a novel self-spectral learning way for hyperspectral target detection and identifies sensitive bands for specific targets in practice. Comparative analyses demonstrate that the proposed SSTD method presents superior detection performance compared with CSCR, ACE, CEM, hCEM, and ECEM.}
}
@article{2023II,
title = {NN/ENNS/JNNS - Membership Applic. Form},
journal = {Neural Networks},
volume = {164},
pages = {II},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(23)00318-0},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023003180}
}
@article{LEONELLI2021314,
title = {On the effective initialisation for restricted Boltzmann machines via duality with Hopfield model},
journal = {Neural Networks},
volume = {143},
pages = {314-326},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002495},
author = {Francesca Elisa Leonelli and Elena Agliari and Linda Albanese and Adriano Barra},
keywords = {Hopfield model, Restricted Boltzmann machine, Statistical mechanics},
abstract = {Restricted Boltzmann machines (RBMs) with a binary visible layer of size N and a Gaussian hidden layer of size P have been proved to be equivalent to a Hopfield neural network (HNN) made of N binary neurons and storing P patterns ξ, as long as the weights w in the former are identified with the patterns. Here we aim to leverage this equivalence to find effective initialisations for weights in the RBM when what is available is a set of noisy examples of each pattern, aiming to translate statistical mechanics background available for HNN to the study of RBM’s learning and retrieval abilities. In particular, given a set of definite, structureless patterns we build a sample of blurred examples and prove that the initialisation where w corresponds to the empirical average ξ¯ over the sample is a fixed point under stochastic gradient descent. Further, as a toy application of the duality between HNN and RBM, we consider the simplest random auto-encoder (a three layer network made of two RBMs coupled by their hidden layer) and evidence that, as long as the parameter setting corresponds to the retrieval region of the dual HNN, reconstruction and denoising can be accomplished trivially, while when the system is in the spin-glass phase inference algorithms are necessary. This questions the need for larger retrieval regions which we obtain by applying a Gram–Schmidt orthogonalisation to the patterns: in fact, this procedure yields to a set of patterns devoid of correlations and for which the largest retrieval region can be accomplished. Finally we consider an application of duality also in a structured case: we test this approach on the MNIST dataset, and obtain that the network performs already ∼67% of successful classifications, suggesting it can be exploited as a computationally-cheap pre-training.}
}
@article{HUANG2021183,
title = {A novel density-based neural mass model for simulating neuronal network dynamics with conductance-based synapses and membrane current adaptation},
journal = {Neural Networks},
volume = {143},
pages = {183-197},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002410},
author = {Chih-Hsu Huang and Chou-Ching K. Lin},
keywords = {Mean-field network dynamics, Neural mass model, Spiking-adaptation, Conductance-based synaptic dynamics},
abstract = {Despite its success in understanding brain rhythms, the neural mass model, as a low-dimensional mean-field network model, is phenomenological in nature, so that it cannot replicate some of rich repertoire of responses seen in real neuronal tissues. Here, using a colored-synapse population density method, we derived a novel neural mass model, termed density-based neural mass model (dNMM), as the mean-field description of network dynamics of adaptive exponential integrate-and-fire (aEIF) neurons, in which two critical neuronal features, i.e., voltage-dependent conductance-based synaptic interactions and adaptation of firing rate responses, were included. Our results showed that the dNMM was capable of correctly estimating firing rate responses of a neuronal population of aEIF neurons receiving stationary or time-varying excitatory and inhibitory inputs. Finally, it was also able to quantitatively describe the effect of spike-frequency adaptation in the generation of asynchronous irregular activity of excitatory–inhibitory cortical networks. We conclude that in terms of its biological reality and calculation efficiency, the dNMM is a suitable candidate to build significantly large-scale network models involving multiple brain areas, where the neuronal population is the smallest dynamic unit.}
}
@article{KHAN2021479,
title = {An efficient encoder–decoder model for portrait depth estimation from single images trained on pixel-accurate synthetic data},
journal = {Neural Networks},
volume = {142},
pages = {479-491},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002707},
author = {Faisal Khan and Shahid Hussain and Shubhajit Basak and Joseph Lemley and Peter Corcoran},
keywords = {Depth estimation, Facial depth, 2.5D dataset, Hybrid loss function, Convolution neural network, Encoder–decoder architecture},
abstract = {Depth estimation from a single image frame is a fundamental challenge in computer vision, with many applications such as augmented reality, action recognition, image understanding, and autonomous driving. Large and diverse training sets are required for accurate depth estimation from a single image frame. Due to challenges in obtaining dense ground-truth depth, a new 3D pipeline of 100 synthetic virtual human models is presented to generate multiple 2D facial images and corresponding ground truth depth data, allowing complete control over image variations. To validate the synthetic facial depth data, we propose an evaluation of state-of-the-art depth estimation algorithms based on single image frames on the generated synthetic dataset. Furthermore, an improved encoder–decoder based neural network is presented. This network is computationally efficient and shows better performance than current state-of-the-art when tested and evaluated across 4 public datasets. Our training methodology relies on the use of synthetic data samples which provides a more reliable ground truth for depth estimation. Additionally, using a combination of appropriate loss functions leads to improved performance than the current state-of-the-art network performances. Our approach clearly outperforms competing methods across different test datasets, setting a new state-of-the-art for facial depth estimation from synthetic data.}
}
@article{SASAKI2021291,
title = {Variational policy search using sparse Gaussian process priors for learning multimodal optimal actions},
journal = {Neural Networks},
volume = {143},
pages = {291-302},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002422},
author = {Hikaru Sasaki and Takamitsu Matsubara},
keywords = {Reinforcement learning, Policy search, Gaussian processes, Multimodality, Mode-seeking},
abstract = {Policy search reinforcement learning has been drawing much attention as a method of learning a robot control policy. In particular, policy search using such non-parametric policies as Gaussian process regression can learn optimal actions with high-dimensional and redundant sensors as input. However, previous methods implicitly assume that the optimal action becomes unique for each state. This assumption can severely limit such practical applications as robot manipulations since designing a reward function that appears in only one optimal action for complex tasks is difficult. The previous methods might have caused critical performance deterioration because the typical non-parametric policies cannot capture the optimal actions due to their unimodality. We propose novel approaches in non-parametric policy searches with multiple optimal actions and offer two different algorithms commonly based on a sparse Gaussian process prior and variational Bayesian inference. The following are the key ideas: (1) multimodality for capturing multiple optimal actions and (2) mode-seeking for capturing one optimal action by ignoring the others. First, we propose a multimodal sparse Gaussian process policy search that uses multiple overlapped GPs as a prior. Second, we propose a mode-seeking sparse Gaussian process policy search that uses the student-t distribution for a likelihood function. The effectiveness of those algorithms is demonstrated through applications to object manipulation tasks with multiple optimal actions in simulations.}
}
@article{JALALI2021489,
title = {Low-shot transfer with attention for highly imbalanced cursive character recognition},
journal = {Neural Networks},
volume = {143},
pages = {489-499},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002665},
author = {Amin Jalali and Swathi Kavuri and Minho Lee},
keywords = {Traditional cursive character recognition, Highly imbalanced data samples, Low-shot regularizer, Attention transfer learning, Decoupled -normalized classifier},
abstract = {Recognition of ancient Korean–Chinese cursive character (Hanja) is a challenging problem mainly because of large number of classes, damaged cursive characters, various hand-writing styles, and similar confusable characters. They also suffer from lack of training data and class imbalance issues. To address these problems, we propose a unified Regularized Low-shot Attention Transfer with Imbalance τ-Normalizing (RELATIN) framework. This handles the problem with instance-poor classes using a novel low-shot regularizer that encourages the norm of the weight vectors for classes with few samples to be aligned to those of many-shot classes. To overcome the class imbalance problem, we incorporate a decoupled classifier to rectify the decision boundaries via classifier weight-scaling into the proposed low-shot regularizer framework. To address the limited training data issue, the proposed framework performs Jensen–Shannon divergence based data augmentation and incorporate an attention module that aligns the most attentive features of the pretrained network to a target network. We verify the proposed RELATIN framework using highly-imbalanced ancient cursive handwritten character datasets. The results suggest that (i) the extreme class imbalance has a detrimental effect on classification performance; (ii) the proposed low-shot regularizer aligns the norm of the classifier in favor of classes with few samples; (iii) weight-scaling of decoupled classifier for addressing class imbalance appeared to be dominant in all the other baseline conditions; (iv) further addition of the attention module attempts to select more representative features maps from base pretrained model; (v) the proposed (RELATIN) framework results in superior representations to address extreme class imbalance issue.}
}
@article{HEYDARI202166,
title = {Adversarial orthogonal regression: Two non-linear regressions for causal inference},
journal = {Neural Networks},
volume = {143},
pages = {66-73},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002148},
author = {M. Reza Heydari and Saber Salehkaleybar and Kun Zhang},
keywords = {Orthogonal regression, Adversarial models, Additive noise model, Structural equation model, Mutual information},
abstract = {We propose two nonlinear regression methods, namely, Adversarial Orthogonal Regression (AdOR) for additive noise models and Adversarial Orthogonal Structural Equation Model (AdOSE) for the general case of structural equation models. Both methods try to make the residual of regression independent from regressors, while putting no assumption on noise distribution. In both methods, two adversarial networks are trained simultaneously where a regression network outputs predictions and a loss network that estimates mutual information (in AdOR) and KL-divergence (in AdOSE). These methods can be formulated as a minimax two-player game; at equilibrium, AdOR finds a deterministic map between inputs and output and estimates mutual information between residual and inputs, while AdOSE estimates a conditional probability distribution of output given inputs. The proposed methods can be used as subroutines to address several learning problems in causality, such as causal direction determination (or more generally, causal structure learning) and causal model estimation. Experimental results on both synthetic and real-world data demonstrate that the proposed methods have remarkable performance with respect to previous solutions.}
}
@article{YANG2021231,
title = {H∞ synchronization of delayed neural networks via event-triggered dynamic output control},
journal = {Neural Networks},
volume = {142},
pages = {231-237},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S089360802100201X},
author = {Yachun Yang and Zhengwen Tu and Liangwei Wang and Jinde Cao and Lei Shi and Wenhua Qian},
keywords = {Delay partition, Dynamic output feedback, Event-triggered control,  synchronization, Neural network},
abstract = {This paper investigates H∞ exponential synchronization (ES) of neural networks (NNs) with delay by designing an event-triggered dynamic output feedback controller (ETDOFC). The ETDOFC is flexible in practice since it is applicable to both full order and reduced order dynamic output techniques. Moreover, the event generator reduces the computational burden for the zero-order-hold (ZOH) operator and does not induce sampling delay as many existing event generators do. To obtain less conservative results, the delay-partitioning method is utilized in the Lyapunov–Krasovskii functional (LKF). Synchronization criteria formulated by linear matrix inequalities (LMIs) are established. A simple algorithm is provided to design the control gains of the ETDOFC, which overcomes the difficulty induced by different dimensions of the system parameters. One numerical example is provided to demonstrate the merits of the theoretical analysis.}
}
@article{YU2021573,
title = {An interaction-modeling mechanism for context-dependent Text-to-SQL translation based on heterogeneous graph aggregation},
journal = {Neural Networks},
volume = {142},
pages = {573-582},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S089360802100277X},
author = {Wei Yu and Tao Chang and Xiaoting Guo and Mengzhu Wang and Xiaodong Wang},
keywords = {Interaction modeling, Context-dependent Text-to-SQL, Heterogeneous graph aggregation},
abstract = {For the context-dependent Text-to-SQL task, the generation of SQL query is placed in a multi-turn interaction scenario. Each turn of Text-to-SQL must take historical interactive information and database schema into account. Accordingly, how to encode and integrate these different types of texts (the question sentence, the corresponding SQL query, and database schema) is a tough problem. In previous work, these series of texts are usually concatenated into sequences and encoded by various variants of recurrent neural networks (RNN). However, the RNNs cannot model the intrinsic relationship of the text directly. To this end, we propose an interaction-modeling mechanism to represent and aggregate these texts. Firstly, different types of texts are represented as individual graphs. Then, heterogeneous graph aggregation is used to capture the interactions and aggregate graphs into a holistic representation. Finally, the corresponding SQL query is generated based on the current question and the aggregated information. We evaluate our model on the SparC and CoSQL dataset to demonstrate the benefits of interaction-modeling. Experimentally, our model has a competitive performance and space–time cost.}
}
@article{LEE2021420,
title = {QTTNet: Quantized tensor train neural networks for 3D object and video recognition},
journal = {Neural Networks},
volume = {141},
pages = {420-432},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.034},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002306},
author = {Donghyun Lee and Dingheng Wang and Yukuan Yang and Lei Deng and Guangshe Zhao and Guoqi Li},
keywords = {3DCNN, Tensor train decomposition, Neural network compression, Quantization, 8 bit inference},
abstract = {Relying on the rapidly increasing capacity of computing clusters and hardware, convolutional neural networks (CNNs) have been successfully applied in various fields and achieved state-of-the-art results. Despite these exciting developments, the huge memory cost is still involved in training and inferring a large-scale CNN model and makes it hard to be widely used in resource-limited portable devices. To address this problem, we establish a training framework for three-dimensional convolutional neural networks (3DCNNs) named QTTNet that combines tensor train (TT) decomposition and data quantization together for further shrinking the model size and decreasing the memory and time cost. Through this framework, we can fully explore the superiority of TT in reducing the number of trainable parameters and the advantage of quantization in decreasing the bit-width of data, particularly compressing 3DCNN model greatly with little accuracy degradation. In addition, due to the low bit quantization to all parameters during the inference process including TT-cores, activations, and batch normalizations, the proposed method naturally takes advantage in memory and time cost. Experimental results of compressing 3DCNNs for 3D object and video recognition on ModelNet40, UCF11, and UCF50 datasets verify the effectiveness of the proposed method. The best compression ratio we have obtained is up to nearly 180× with competitive performance compared with other state-of-the-art researches. Moreover, the total bytes of our QTTNet models on ModelNet40 and UCF11 datasets can be 1000× lower than some typical practices such as MVCNN.}
}
@article{ORKANOLCAY2021452,
title = {On the characterization of cognitive tasks using activity-specific short-lived synchronization between electroencephalography channels},
journal = {Neural Networks},
volume = {143},
pages = {452-474},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002549},
author = {B. {Orkan Olcay} and Murat Özgören and Bilge Karaçalı},
keywords = {EEG, Short-lived synchronization, Motor imagery activity characterization, Systematic timing organization, Synchronization measures},
abstract = {Accurate characterization of brain activity during a cognitive task is challenging due to the dynamically changing and the complex nature of the brain. The majority of the proposed approaches assume stationarity in brain activity and disregard the systematic timing organization among brain regions during cognitive tasks. In this study, we propose a novel cognitive activity recognition method that captures the activity-specific timing parameters from training data that elicits maximal average short-lived pairwise synchronization between electroencephalography signals. We evaluated the characterization power of the activity-specific timing parameter triplets in a motor imagery activity recognition framework. The activity-specific timing parameter triplets consist of latency of the maximally synchronized signal segments from activity onset Δt, the time lag between maximally synchronized signal segments τ, and the duration of the maximally synchronized signal segments w. We used cosine-based similarity, wavelet bi-coherence, phase-locking value, phase coherence value, linearized mutual information, and cross-correntropy to calculate the channel synchronizations at the specific timing parameters. Recognition performances as well as statistical analyses on both BCI Competition-III dataset IVa and PhysioNet Motor Movement/Imagery dataset, indicate that the inter-channel short-lived synchronization calculated using activity-specific timing parameter triplets elicit significantly distinct synchronization profiles for different motor imagery tasks and can thus reliably be used for cognitive task recognition purposes.}
}
@article{CATAL2021192,
title = {Robot navigation as hierarchical active inference},
journal = {Neural Networks},
volume = {142},
pages = {192-204},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002021},
author = {Ozan Çatal and Tim Verbelen and Toon {Van de Maele} and Bart Dhoedt and Adam Safron},
keywords = {Active inference, Robot navigation, SLAM, RatSLAM, Deep learning},
abstract = {Localization and mapping has been a long standing area of research, both in neuroscience, to understand how mammals navigate their environment, as well as in robotics, to enable autonomous mobile robots. In this paper, we treat navigation as inferring actions that minimize (expected) variational free energy under a hierarchical generative model. We find that familiar concepts like perception, path integration, localization and mapping naturally emerge from this active inference formulation. Moreover, we show that this model is consistent with models of hippocampal functions, and can be implemented in silico on a real-world robot. Our experiments illustrate that a robot equipped with our hierarchical model is able to generate topologically consistent maps, and correct navigation behaviour is inferred when a goal location is provided to the system.}
}
@article{DALBELLO2021583,
title = {Task-relevant and task-irrelevant variability causally shape error-based motor learning},
journal = {Neural Networks},
volume = {142},
pages = {583-596},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002781},
author = {Lucas Rebelo Dal’Bello and Jun Izawa},
keywords = {Motor learning, Computational model, Exploration, Redundancy, Error-based learning, Motor variability},
abstract = {Recent studies of motor learning show dissociable roles of reward- and sensory-prediction errors in updating motor commands by using typical adaptation paradigms where force or visual perturbations are imposed on hand movements. Such classic adaptation paradigms ignore a problem of redundancy inherently embedded in the motor pathways where the central nervous system has to find a unique solution in the high-dimensional motor command space. Computationally, a possible way of solving such a redundancy problem is exploring and updating motor commands based on the learned knowledge of the structures of both the motor pathways and the tasks. However, the effects of task-irrelevant motor command exploration in structure learning and its effects on reward-based and error-based learning have yet to be examined. Here, we used a redundant motor task where participants manipulated a cursor on a monitor screen with their hand gesture movements and then analyzed single-trial motor learning by fitting models consisting of reward-based and error-based learning contributions. We found that the error-based learning rate positively correlated with both task-relevant and task-irrelevant variability, likely reflecting the effect of motor exploration in structure learning. Further modeling results show that the effects of both task-relevant and task-irrelevant variability are simultaneous, and not mediated by one another. In contrast, the reward-based learning rate correlated with neither task-relevant nor task-irrelevant variability. Thus, although not having a direct influence on the task outcome, exploration in the task-irrelevant space late in training has a significant effect on the learning of a task structure used for error-based learning. This suggests that motor exploration, in both task-relevant and task-irrelevant spaces, has an essential role in error-based motor learning in a redundant motor mechanism.}
}
@article{OHTA2021218,
title = {The asymmetric learning rates of murine exploratory behavior in sparse reward environments},
journal = {Neural Networks},
volume = {143},
pages = {218-229},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.030},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002264},
author = {Hiroyuki Ohta and Kuniaki Satori and Yu Takarada and Masashi Arake and Toshiaki Ishizuka and Yuji Morimoto and Tatsuji Takahashi},
keywords = {Reinforcement learning, Dual learning rate, Behavior, Exploration, Multi-armed bandit problem},
abstract = {Goal-oriented behaviors of animals can be modeled by reinforcement learning algorithms. Such algorithms predict future outcomes of selected actions utilizing action values and updating those values in response to the positive and negative outcomes. In many models of animal behavior, the action values are updated symmetrically based on a common learning rate, that is, in the same way for both positive and negative outcomes. However, animals in environments with scarce rewards may have uneven learning rates. To investigate the asymmetry in learning rates in reward and non-reward, we analyzed the exploration behavior of mice in five-armed bandit tasks using a Q-learning model with differential learning rates for positive and negative outcomes. The positive learning rate was significantly higher in a scarce reward environment than in a rich reward environment, and conversely, the negative learning rate was significantly lower in the scarce environment. The positive to negative learning rate ratio was about 10 in the scarce environment and about 2 in the rich environment. This result suggests that when the reward probability was low, the mice tend to ignore failures and exploit the rare rewards. Computational modeling analysis revealed that the increased learning rates ratio could cause an overestimation of and perseveration on rare-rewarding events, increasing total reward acquisition in the scarce environment but disadvantaging impartial exploration.}
}
@article{BALDI202112,
title = {A theory of capacity and sparse neural encoding},
journal = {Neural Networks},
volume = {143},
pages = {12-27},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021001970},
author = {Pierre Baldi and Roman Vershynin},
keywords = {Neural capacity, Sparse representations, Sparse encoding, Neural maps, Threshold functions},
abstract = {Motivated by biological considerations, we study sparse neural maps from an input layer to a target layer with sparse activity, and specifically the problem of storing K input-target associations (x,y), or memories, when the target vectors y are sparse. We mathematically prove that K undergoes a phase transition and that in general, and somewhat paradoxically, sparsity in the target layers increases the storage capacity of the map. The target vectors can be chosen arbitrarily, including in random fashion, and the memories can be both encoded and decoded by networks trained using local learning rules, including the simple Hebb rule. These results are robust under a variety of statistical assumptions on the data. The proofs rely on elegant properties of random polytopes and sub-gaussian random vector variables. Open problems and connections to capacity theories and polynomial threshold maps are discussed.}
}
@article{GUO2021437,
title = {Selective ensemble-based online adaptive deep neural networks for streaming data with concept drift},
journal = {Neural Networks},
volume = {142},
pages = {437-456},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.027},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002598},
author = {Husheng Guo and Shuai Zhang and Wenjian Wang},
keywords = {Concept drift, Selective ensemble, Deep neural networks, Online learning, Adaptive method},
abstract = {Concept drift is an important issue in the field of streaming data mining. However, how to maintain real-time model convergence in a dynamic environment is an important and difficult problem. In addition, the current methods have limited ability to deal with the problem of streaming data classification for complex nonlinear problems. To solve these problems, a selective ensemble-based online adaptive deep neural network (SEOA) is proposed to address concept drift. First, the adaptive depth unit is constructed by combining shallow features with deep features and adaptively controls the information flow in the neural network according to changes in streaming data at adjacent moments, which improves the convergence of the online deep learning model. Then, the adaptive depth units of different layers are regarded as base classifiers for ensemble and weighted dynamically according to the loss of each classifier. In addition, a dynamic selection of base classifiers is adopted according to the fluctuation of the streaming data to achieve a balance between stability and adaptability. The experimental results show that the SEOA can effectively contend with different types of concept drift and has good robustness and generalization.}
}
@article{WANG2021271,
title = {Exponential passivity of discrete-time switched neural networks with transmission delays via an event-triggered sliding mode control},
journal = {Neural Networks},
volume = {143},
pages = {271-282},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.014},
url = {https://www.sciencedirect.com/science/article/pii/S089360802100246X},
author = {Jinling Wang and Haijun Jiang and Cheng Hu and Tianlong Ma},
keywords = {Discrete-time switched neural networks, Sliding mode control, Event-triggered mechanism, Exponential passivity},
abstract = {This paper investigates the exponential passivity of discrete-time switched neural networks (DSNNs) with transmission delays via an event-triggered sliding mode control (SMC). Firstly, a novel discrete-time switched SMC scheme is constructed on the basis of sliding mode control method and event-triggered mechanism. Next, a state observer with transmission delays is designed to estimate the system state. Moreover, some new weighted summation inequalities are further proposed to effectively evaluate the exponential passivity criteria for the closed-loop system. Finally, the effectiveness of theoretical results is showed through a simulative analysis on a multi-area power system.}
}
@article{SUN2021377,
title = {Neural adaptive fault-tolerant finite-time control for nonstrict feedback systems: An event-triggered mechanism},
journal = {Neural Networks},
volume = {143},
pages = {377-385},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002513},
author = {K. Sun and J. Qiu and H.R. Karimi},
keywords = {Neural networks, Event-triggered control, Finite time, Nonaffine nonlinear faults},
abstract = {The problem of event-triggered neural adaptive fault-tolerant finite-time control is investigated for a class of nonstrict feedback nonlinear systems in the presence of nonaffine nonlinear faults. The event-triggered signal is designed by using a relative-threshold to reduce communication burden. The dynamic surface control method is used to relax the assumption of the reference signal and deal with the computational complexity issue. Based on the finite-time stability, a new neural adaptive backstepping design method is developed. The event-triggered neural adaptive fault-tolerant control law is developed for the closed-loop system so that not only the semi-global practical finite-time stability is ensured, but also the tracking performance with a small residual set is guaranteed. Finally, the effectiveness of the proposed control law is verified via simulation results.}
}
@article{JI2021522,
title = {Deep attributed graph clustering with self-separation regularization and parameter-free cluster estimation},
journal = {Neural Networks},
volume = {142},
pages = {522-533},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002756},
author = {Junzhong Ji and Ye Liang and Minglong Lei},
keywords = {Attributed graph clustering, Graph convolutional networks, Parameter-free cluster estimation, Self-separation regularization},
abstract = {Detecting clusters over attributed graphs is a fundamental task in the graph analysis field. The goal is to partition nodes into dense clusters based on both their attributes and structures. Modern graph neural networks provide facilitation to jointly capture the above information in attributed graphs with a feature aggregation manner, and have achieved great success in attributed graph clustering. However, existing methods mainly focus on capturing the proximity information in graphs and often fail to learn cluster-friendly features during the training of models. Besides, similar to many deep clustering frameworks, current methods based on graph neural networks require a preassigned cluster number before estimating the clusters. To address these limitations, we propose in this paper a deep attributed clustering method based on self-separated graph neural networks and parameter-free cluster estimation. First, to learn cluster-friendly features, we jointly optimize a jumping graph convolutional auto-encoder with a self-separation regularizer, which learns clusters with changing sizes while keeping dense intra-cluster structures and sparse inter structures. Second, an additional softmax auto-encoder is trained to determine the natural cluster number from the data. The hidden units capture cluster structures and can be used to estimate the number of clusters. Extensive experiments show the effectiveness of the proposed model.}
}
@article{WANG2021121,
title = {Neural optimal tracking control of constrained nonaffine systems with a wastewater treatment application},
journal = {Neural Networks},
volume = {143},
pages = {121-132},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.027},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002239},
author = {Ding Wang and Mingming Zhao and Mingming Ha and Jin Ren},
keywords = {Adaptive critic, Actuator saturation, Optimal tracking control, Neural networks, Wastewater treatment},
abstract = {In this paper, we aim to solve the optimal tracking control problem for a class of nonaffine discrete-time systems with actuator saturation. First, a data-based neural identifier is constructed to learn the unknown system dynamics. Then, according to the expression of the trained neural identifier, we can obtain the steady control corresponding to the reference trajectory. Next, by involving the iterative dual heuristic dynamic programming algorithm, the new costate function and the tracking control law are developed. Two other neural networks are used to estimate the costate function and approximate the tracking control law. Considering approximation errors of neural networks, the stability analysis of the proposed algorithm for the specific systems is provided by introducing the Lyapunov approach. Finally, via conducting simulation and comparison, the superiority of the developed optimal tracking method is confirmed. Moreover, the trajectory tracking performance of the wastewater treatment application is also involved for further verifying the proposed approach.}
}
@article{TANG2021669,
title = {CommPOOL: An interpretable graph pooling framework for hierarchical graph representation learning},
journal = {Neural Networks},
volume = {143},
pages = {669-677},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.028},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002999},
author = {Haoteng Tang and Guixiang Ma and Lifang He and Heng Huang and Liang Zhan},
keywords = {Graph representation learning, Hierarchical graph pooling neural network, Community structure, Graph classification},
abstract = {Recent years have witnessed the emergence and flourishing of hierarchical graph pooling neural networks (HGPNNs) which are effective graph representation learning approaches for graph level tasks such as graph classification. However, current HGPNNs do not take full advantage of the graph’s intrinsic structures (e.g., community structure). Moreover, the pooling operations in existing HGPNNs are difficult to be interpreted. In this paper, we propose a new interpretable graph pooling framework — CommPOOL, that can capture and preserve the hierarchical community structure of graphs in the graph representation learning process. Specifically, the proposed community pooling mechanism in CommPOOL utilizes an unsupervised approach for capturing the inherent community structure of graphs in an interpretable manner. CommPOOL is a general and flexible framework for hierarchical graph representation learning that can further facilitate various graph-level tasks. Evaluations on five public benchmark datasets and one synthetic dataset demonstrate the superior performance of CommPOOL in graph representation learning for graph classification compared to the state-of-the-art baseline methods, and its effectiveness in capturing and preserving the community structure of graphs.}
}
@article{SONG2021205,
title = {Efficient learning with augmented spikes: A case study with image classification},
journal = {Neural Networks},
volume = {142},
pages = {205-212},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021001945},
author = {Shiming Song and Chenxiang Ma and Wei Sun and Junhai Xu and Jianwu Dang and Qiang Yu},
keywords = {Augmented spikes, Multi-spike learning, Image classification, Spiking neural networks, Temporal encoding, Neuromorphic computing},
abstract = {Efficient learning of spikes plays a valuable role in training spiking neural networks (SNNs) to have desired responses to input stimuli. However, current learning rules are limited to a binary form of spikes. The seemingly ubiquitous phenomenon of burst in nervous systems suggests a new way to carry more information with spike bursts in addition to times. Based on this, we introduce an advanced form, the augmented spikes, where spike coefficients are used to carry additional information. How could neurons learn and benefit from augmented spikes remains unclear. In this paper, we propose two new efficient learning rules to process spatiotemporal patterns composed of augmented spikes. Moreover, we examine the learning abilities of our methods with a synthetic recognition task of augmented spike patterns and two practical ones for image classification. Experimental results demonstrate that our rules are capable of extracting information carried by both the timing and coefficient of spikes. Our proposed approaches achieve remarkable performance and good robustness under various noise conditions, as compared to benchmarks. The improved performance indicates the merits of augmented spikes and our learning rules, which could be beneficial and generalized to a broad range of spike-based platforms.}
}
@article{SHIBATA2021436,
title = {Sensitivity – Local index to control chaoticity or gradient globally –},
journal = {Neural Networks},
volume = {143},
pages = {436-451},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002471},
author = {Katsunari Shibata and Takuya Ejima and Yuki Tokumaru and Toshitaka Matsuki},
keywords = {Sensitivity, Sensitivity adjustment learning (SAL), Edge of chaos, Recurrent neural network (RNN), Deep feedforward neural network (DFNN), Vanishing gradient problem},
abstract = {Here, we introduce a fully local index named “sensitivity” for each neuron to control chaoticity or gradient globally in a neural network (NN). We also propose a learning method to adjust it named “sensitivity adjustment learning (SAL)”. The index is the gradient magnitude of its output with respect to its inputs. By adjusting its time average to 1.0 in each neuron, information transmission in the neuron changes to be moderate without shrinking or expanding for both forward and backward computations. That results in moderate information transmission through a layer of neurons when the weights and inputs are random. Therefore, SAL can control the chaoticity of the network dynamics in a recurrent NN (RNN). It can also solve the vanishing gradient problem in error backpropagation (BP) learning in a deep feedforward NN or an RNN. We demonstrate that when applying SAL to an RNN with small and random initial weights, log-sensitivity, which is the logarithm of RMS (root mean square) sensitivity over all the neurons, is equivalent to the maximum Lyapunov exponent until it reaches 0.0. We also show that SAL works with BP or BPTT (BP through time) to avoid the vanishing gradient problem in a 300-layer NN or an RNN that learns a problem with a lag of 300 steps between the first input and the output. Compared with manually fine-tuning the spectral radius of the weight matrix before learning, SAL’s continuous nonlinear learning nature prevents loss of sensitivities during learning, resulting in a significant improvement in learning performance.}
}
@article{ROH2021397,
title = {Unsupervised multi-sense language models for natural language processing tasks},
journal = {Neural Networks},
volume = {142},
pages = {397-409},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.023},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002197},
author = {Jihyeon Roh and Sungjin Park and Bo-Kyeong Kim and Sang-Hoon Oh and Soo-Young Lee},
keywords = {Language model, Neural language processing (NLP), Multi-sense word modeling},
abstract = {Existing language models (LMs) represent each word with only a single representation, which is unsuitable for processing words with multiple meanings. This issue has often been compounded by the lack of availability of large-scale data annotated with word meanings. In this paper, we propose a sense-aware framework that can process multi-sense word information without relying on annotated data. In contrast to the existing multi-sense representation models, which handle information in a restricted context, our framework provides context representations encoded without ignoring word order information or long-term dependency. The proposed framework consists of a context representation stage to encode the variable-size context, a sense-labeling stage that involves unsupervised clustering to infer a probable sense for a word in each context, and a multi-sense LM (MSLM) learning stage to learn the multi-sense representations. Particularly for the evaluation of MSLMs with different vocabulary sizes, we propose a new metric, i.e., unigram-normalized perplexity (PPLu), which is also understood as the negated mutual information between a word and its context information. Additionally, there is a theoretical verification of PPLu on the change of vocabulary size. Also, we adopt a method of estimating the number of senses, which does not require further hyperparameter search for an LM performance. For the LMs in our framework, both unidirectional and bidirectional architectures based on long short-term memory (LSTM) and Transformers are adopted. We conduct comprehensive experiments on three language modeling datasets to perform quantitative and qualitative comparisons of various LMs. Our MSLM outperforms single-sense LMs (SSLMs) with the same network architecture and parameters. It also shows better performance on several downstream natural language processing tasks in the General Language Understanding Evaluation (GLUE) and SuperGLUE benchmarks.}
}
@article{GOELZ2021363,
title = {Classification of visuomotor tasks based on electroencephalographic data depends on age-related differences in brain activity patterns},
journal = {Neural Networks},
volume = {142},
pages = {363-374},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.04.029},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021001660},
author = {C. Goelz and K. Mora and J. Rudisch and R. Gaidai and E. Reuter and B. Godde and C. Reinsberger and C. Voelcker-Rehage and S. Vieluf},
keywords = {Dynamic mode decomposition, Fine motor control, Machine learning, Decoding, Dedifferentiation, Aging},
abstract = {Classification of physiological data provides a data driven approach to study central aspects of motor control, which changes with age. To implement such results in real-life applications for elderly it is important to identify age-specific characteristics of movement classification. We compared task-classification based on EEG derived activity patterns related to brain network characteristics between older and younger adults performing force tracking with two task characteristics (sinusoidal; constant) with the right or left hand. We extracted brain network patterns with dynamic mode decomposition (DMD) and classified the tasks on an individual level using linear discriminant analysis (LDA). Next, we compared the models’ performance between the groups. Studying brain activity patterns, we identified signatures of altered motor network function reflecting dedifferentiated and compensational brain activation in older adults. We found that the classification performance of the body side was lower in older adults. However, classification performance with respect to task characteristics was better in older adults. This may indicate a higher susceptibility of brain network mechanisms to task difficulty in elderly. Signatures of dedifferentiation and compensation refer to an age-related reorganization of functional brain networks, which suggests that classification of visuomotor tracking tasks is influenced by age-specific characteristics of brain activity patterns. In addition to insights into central aspects of fine motor control, the results presented here are relevant in application-oriented areas such as brain computer interfaces.}
}
@article{LIN202174,
title = {A brain-inspired computational model for spatio-temporal information processing},
journal = {Neural Networks},
volume = {143},
pages = {74-87},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002112},
author = {Xiaohan Lin and Xiaolong Zou and Zilong Ji and Tiejun Huang and Si Wu and Yuanyuan Mi},
keywords = {Spatio-temporal pattern, Brain-inspired, Reservoir computing, Decision-making},
abstract = {Spatio-temporal information processing is fundamental in both brain functions and AI applications. Current strategies for spatio-temporal pattern recognition usually involve explicit feature extraction followed by feature aggregation, which requires a large amount of labeled data. In the present study, motivated by the subcortical visual pathway and early stages of the auditory pathway for motion and sound processing, we propose a novel brain-inspired computational model for generic spatio-temporal pattern recognition. The model consists of two modules, a reservoir module and a decision-making module. The former projects complex spatio-temporal patterns into spatially separated neural representations via its recurrent dynamics, the latter reads out neural representations via integrating information over time, and the two modules are linked together using known examples. Using synthetic data, we demonstrate that the model can extract the frequency and order information of temporal inputs. We apply the model to reproduce the looming pattern discrimination behavior as observed in experiments successfully. Furthermore, we apply the model to the gait recognition task, and demonstrate that our model accomplishes the recognition in an event-based manner and outperforms deep learning counterparts when training data is limited.}
}
@article{ZHAO20211,
title = {P-DIFF+: Improving learning classifier with noisy labels by Noisy Negative Learning loss},
journal = {Neural Networks},
volume = {144},
pages = {1-10},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.024},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002872},
author = {QiHao Zhao and Wei Hu and Yangyu Huang and Fan Zhang},
keywords = {Classification, Noisy labels, Deep neural networks, Probability difference distribution, Noisy Negative Learning loss},
abstract = {Learning deep neural network (DNN) classifier with noisy labels is a challenging task because the DNN can easily over-fit on these noisy labels due to its high capability. In this paper, we present a very simple but effective training paradigm called P-DIFF+, which can train DNN classifiers but obviously alleviate the adverse impact of noisy labels. Our proposed probability difference distribution implicitly reflects the probability of a training sample to be clean, then this probability is employed to re-weight the corresponding sample during the training process. Moreover, Noisy Negative Learning(NNL) loss can be further employed to re-weight samples. P-DIFF+ can achieve good performance even without prior-knowledge on the noise rate of training samples. Experiments on benchmark datasets demonstrate that P-DIFF+ is superior to the state-of-the-art sample selection methods.}
}
@article{CHEN2021171,
title = {Correlating subword articulation with lip shapes for embedding aware audio-visual speech enhancement},
journal = {Neural Networks},
volume = {143},
pages = {171-182},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002355},
author = {Hang Chen and Jun Du and Yu Hu and Li-Rong Dai and Bao-Cai Yin and Chin-Hui Lee},
keywords = {Speech enhancement, Audio-visual, Representation learning, Deep learning, Universal attribute recognition},
abstract = {In this paper, we propose a visual embedding approach to improve embedding aware speech enhancement (EASE) by synchronizing visual lip frames at the phone and place of articulation levels. We first extract visual embedding from lip frames using a pre-trained phone or articulation place recognizer for visual-only EASE (VEASE). Next, we extract audio-visual embedding from noisy speech and lip frames in an information intersection manner, utilizing a complementarity of audio and visual features for multi-modal EASE (MEASE). Experiments on the TCD-TIMIT corpus corrupted by simulated additive noises show that our proposed subword based VEASE approach is more effective than conventional embedding at the word level. Moreover, visual embedding at the articulation place level, leveraging upon a high correlation between place of articulation and lip shapes, demonstrates an even better performance than that at the phone level. Finally the experiments establish that the proposed MEASE framework, incorporating both audio and visual embeddings, yields significantly better speech quality and intelligibility than those obtained with the best visual-only and audio-only EASE systems.}
}
@article{ZHOU2021316,
title = {Adaptive ensemble perception tracking},
journal = {Neural Networks},
volume = {142},
pages = {316-328},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021001957},
author = {Zikun Zhou and Nana Fan and Kai Yang and Hongpeng Wang and Zhenyu He},
keywords = {Visual tracking, Ensemble prediction, Receptive field adaption, Siamese network},
abstract = {Recently, tracking models based on bounding box regression (such as region proposal networks), built on the Siamese network, have attracted much attention. Despite their promising performance, these trackers are less effective in perceiving the target information in the following two aspects. First, existing regression models cannot take a global view of a large-scale target since the effective receptive field of a neuron is too small to cover the target with a large scale. Second, the neurons with a fixed receptive field (RF) size in these models cannot adapt to the scale and aspect ratio changes of the target. In this paper, we propose an adaptive ensemble perception tracking framework to address these issues. Specifically, we first construct a per-pixel prediction model, which predicts the target state at each pixel of the correlated feature. On top of the per-pixel prediction model, we then develop a confidence-guided ensemble prediction mechanism. The ensemble mechanism adaptively fuses the predictions of multiple pixels with the guidance of confidence maps, which enlarges the perception range and enhances the adaptive perception ability at the object-level. In addition, we introduce a receptive field adaption model to enhance the adaptive perception ability at the neuron-level, which adjusts the RF by adaptively integrating the features with different RFs. Extensive experimental results on the VOT2018, VOT2016, UAV123, LaSOT, and TC128 datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods in terms of accuracy and speed.}
}
@article{MA2021261,
title = {HiAM: A Hierarchical Attention based Model for knowledge graph multi-hop reasoning},
journal = {Neural Networks},
volume = {143},
pages = {261-270},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002409},
author = {Ting Ma and Shangwen Lv and Longtao Huang and Songlin Hu},
keywords = {Knowledge graph reasoning, Predecessor paths, Hierarchical Attention},
abstract = {Learning to reason in large-scale knowledge graphs has attracted much attention from research communities recently. This paper targets a practical task of multi-hop reasoning in knowledge graphs, which can be applied in various downstream tasks such as question answering, and recommender systems. A key challenge in multi-hop reasoning is to synthesize structural information (e.g., paths) in knowledge graphs to perform deeper reasoning. Existing methods usually focus on connection paths between each entity pair. However, these methods ignore predecessor paths before connection paths and regard entities and relations within every single path as equally important. With our observations, predecessor paths before connection paths can provide more accurate semantic representations. Furthermore, entities and relations in a single path contribute variously to the right answers. To this end, we propose a novel model HiAM (Hierarchical Attention based Model) for knowledge graph multi-hop reasoning. HiAM makes use of predecessor paths to provide more accurate semantics for entities and explores the effects of different granularities. Firstly, we extract predecessor paths of head entities and connection paths between each entity pair. Then, a hierarchical attention mechanism is designed to capture the information of different granularities, including entity/relation-level and path-level features. Finally, multi-granularity features are fused together to predict the right answers. We go one step further to select the most significant path as the explanation for predicted answers. Comprehensive experimental results demonstrate that our method achieves competitive performance compared with the baselines on three benchmark datasets.}
}
@article{CLONINGER2021404,
title = {A deep network construction that adapts to intrinsic dimensionality beyond the domain},
journal = {Neural Networks},
volume = {141},
pages = {404-419},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002367},
author = {Alexander Cloninger and Timo Klock},
keywords = {Deep neural networks, Approximation theory, Curse of dimensionality, Composite functions, Noisy manifold models},
abstract = {We study the approximation of two-layer compositions f(x)=g(ϕ(x)) via deep networks with ReLU activation, where ϕ is a geometrically intuitive, dimensionality reducing feature map. We focus on two intuitive and practically relevant choices for ϕ: the projection onto a low-dimensional embedded submanifold and a distance to a collection of low-dimensional sets. We achieve near optimal approximation rates, which depend only on the complexity of the dimensionality reducing map ϕ rather than the ambient dimension. Since ϕ encapsulates all nonlinear features that are material to the function f, this suggests that deep nets are faithful to an intrinsic dimension governed by f rather than the complexity of the domain of f. In particular, the prevalent assumption of approximating functions on low-dimensional manifolds can be significantly relaxed using functions of type f(x)=g(ϕ(x)) with ϕ representing an orthogonal projection onto the same manifold.}
}
@article{LIAN2021368,
title = {Distributed learning for sketched kernel regression},
journal = {Neural Networks},
volume = {143},
pages = {368-376},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002525},
author = {Heng Lian and Jiamin Liu and Zengyan Fan},
keywords = {Distributed learning, Kernel method, Optimal rate, Randomized sketches},
abstract = {We study distributed learning for regularized least squares regression in a reproducing kernel Hilbert space (RKHS). The divide-and-conquer strategy is a frequently used approach for dealing with very large data sets, which computes an estimate on each subset and then takes an average of the estimators. Existing theoretical constraint on the number of subsets implies the size of each subset can still be large. Random sketching can thus be used to produce the local estimators on each subset to further reduce the computation compared to vanilla divide-and-conquer. In this setting, sketching and divide-and-conquer are complementary to each other in dealing with the large sample size. We show that optimal learning rates can be retained. Simulations are performed to compare sketched and non-standard divide-and-conquer methods.}
}
@article{XU2021221,
title = {Graph embedding clustering: Graph attention auto-encoder with cluster-specificity distribution},
journal = {Neural Networks},
volume = {142},
pages = {221-230},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002008},
author = {Huiling Xu and Wei Xia and Quanxue Gao and Jungong Han and Xinbo Gao},
keywords = {Nodes clustering, Graph neural networks, Cluster-specificity distribution},
abstract = {Towards exploring the topological structure of data, numerous graph embedding clustering methods have been developed in recent years, none of them takes into account the cluster-specificity distribution of the nodes representations, resulting in suboptimal clustering performance. Moreover, most existing graph embedding clustering methods execute the nodes representations learning and clustering in two separated steps, which increases the instability of its original performance. Additionally, rare of them simultaneously takes node attributes reconstruction and graph structure reconstruction into account, resulting in degrading the capability of graph learning. In this work, we integrate the nodes representations learning and clustering into a unified framework, and propose a new deep graph attention auto-encoder for nodes clustering that attempts to learn more favorable nodes representations by leveraging self-attention mechanism and node attributes reconstruction. Meanwhile, a cluster-specificity distribution constraint, which is measured by ℓ1,2-norm, is employed to make the nodes representations within the same cluster end up with a common distribution in the dimension space while representations with different clusters have different distributions in the intrinsic dimensions. Extensive experiment results reveal that our proposed method is superior to several state-of-the-art methods in terms of performance.}
}
@article{GRANATO2021572,
title = {Internal manipulation of perceptual representations in human flexible cognition: A computational model},
journal = {Neural Networks},
volume = {143},
pages = {572-594},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002768},
author = {Giovanni Granato and Gianluca Baldassarre},
keywords = {Computational model, Goal-directed behaviour, Top-down representation manipulation, Selective attention, Cognitive flexibility},
abstract = {Executive functions represent a set of processes in goal-directed cognition that depend on integrated cortical-basal ganglia brain systems and form the basis of flexible human behaviour. Several computational models have been proposed for studying cognitive flexibility as a key executive function and the Wisconsin card sorting test (WCST) that represents an important neuropsychological tool to investigate it. These models clarify important aspects that underlie cognitive flexibility, particularly decision-making, motor response, and feedback-dependent learning processes. However, several studies suggest that the categorisation processes involved in the solution of the WCST include an additional computational stage of category representation that supports the other processes. Surprisingly, all models of the WCST ignore this fundamental stage and they assume that decision making directly triggers actions. Thus, we propose a novel hypothesis where the key mechanisms of cognitive flexibility and goal-directed behaviour rely on the acquisition of suitable representations of percepts and their top-down internal manipulation. Moreover, we propose a neuro-inspired computational model to operationalise this hypothesis. The capacity of the model to support cognitive flexibility was validated by systematically reproducing and interpreting the behaviour exhibited in the WCST by young and old healthy adults, and by frontal and Parkinson patients. The results corroborate and further articulate the hypothesis that the internal manipulation of representations is a core process in goal-directed flexible cognition.}
}
@article{TAMURA2021550,
title = {Transfer-RLS method and transfer-FORCE learning for simple and fast training of reservoir computing models},
journal = {Neural Networks},
volume = {143},
pages = {550-563},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.031},
url = {https://www.sciencedirect.com/science/article/pii/S089360802100263X},
author = {Hiroto Tamura and Gouhei Tanaka},
keywords = {Recurrent neural networks, Reservoir computing, Online supervised learning, Recursive least squares method, FORCE learning},
abstract = {Reservoir computing is a machine learning framework derived from a special type of recurrent neural network. Following recent advances in physical reservoir computing, some reservoir computing devices are thought to be promising as energy-efficient machine learning hardware for real-time information processing. To realize efficient online learning with low-power reservoir computing devices, it is beneficial to develop fast convergence learning methods with simpler operations. This study proposes a training method located in the middle between the recursive least squares (RLS) method and the least mean squares (LMS) method, which are standard online learning methods for reservoir computing models. The RLS method converges fast but requires updates of a huge matrix called a gain matrix, whereas the LMS method does not use a gain matrix but converges very slow. On the other hand, the proposed method called a transfer-RLS method does not require updates of the gain matrix in the main-training phase by updating that in advance (i.e., in a pre-training phase). As a result, the transfer-RLS method can work with simpler operations than the original RLS method without sacrificing much convergence speed. We numerically and analytically show that the transfer-RLS method converges much faster than the LMS method. Furthermore, we show that a modified version of the transfer-RLS method (called transfer-FORCE learning) can be applied to the first-order reduced and controlled error (FORCE) learning for a reservoir computing model with a closed-loop, which is challenging to train.}
}
@article{LIANG2021133,
title = {A hybrid quantum–classical neural network with deep residual learning},
journal = {Neural Networks},
volume = {143},
pages = {133-147},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.028},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002240},
author = {Yanying Liang and Wei Peng and Zhu-Jun Zheng and Olli Silvén and Guoying Zhao},
keywords = {Quantum computing, Quantum neural networks, Deep residual learning},
abstract = {Inspired by the success of classical neural networks, there has been tremendous effort to develop classical effective neural networks into quantum concept. In this paper, a novel hybrid quantum–classical neural network with deep residual learning (Res-HQCNN) is proposed. We firstly analyse how to connect residual block structure with a quantum neural network, and give the corresponding training algorithm. At the same time, the advantages and disadvantages of transforming deep residual learning into quantum concept are provided. As a result, the model can be trained in an end-to-end fashion, analogue to the backpropagation in classical neural networks. To explore the effectiveness of Res-HQCNN , we perform extensive experiments for quantum data with or without noisy on classical computer. The experimental results show the Res-HQCNN performs better to learn an unknown unitary transformation and has stronger robustness for noisy data, when compared to state of the arts. Moreover, the possible methods of combining residual learning with quantum neural networks are also discussed.}
}
@article{TANG2021327,
title = {Robust cost-sensitive kernel method with Blinex loss and its applications in credit risk evaluation},
journal = {Neural Networks},
volume = {143},
pages = {327-344},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002483},
author = {Jingjing Tang and Jiahui Li and Weiqi Xu and Yingjie Tian and Xuchan Ju and Jie Zhang},
keywords = {Credit risk evaluation, Imbalanced learning, Cost-sensitive support vector machine, Blinex loss function},
abstract = {Credit risk evaluation is a crucial yet challenging problem in financial analysis. It can not only help institutions reduce risk and ensure profitability, but also improve consumers’ fair practices. The data-driven algorithms such as artificial intelligence techniques regard the evaluation as a classification problem and aim to classify transactions as default or non-default. Since non-default samples greatly outnumber default samples, it is a typical imbalanced learning problem and each class or each sample needs special treatment. Numerous data-level, algorithm-level and hybrid methods are presented, and cost-sensitive support vector machines (CSSVMs) are representative algorithm-level methods. Based on the minimization of symmetric and unbounded loss functions, CSSVMs impose higher penalties on the misclassification costs of minority instances using domain specific parameters. However, such loss functions as error measurement cannot have an obvious cost-sensitive generalization. In this paper, we propose a robust cost-sensitive kernel method with Blinex loss (CSKB), which can be applied in credit risk evaluation. By inheriting the elegant merits of Blinex loss function, i.e., asymmetry and boundedness, CSKB not only flexibly controls distinct costs for both classes, but also enjoys noise robustness. As a data-driven decision-making paradigm of credit risk evaluation, CSKB can achieve the “win-win” situation for both the financial institutions and consumers. We solve linear and nonlinear CSKB by Nesterov accelerated gradient algorithm and Pegasos algorithm respectively. Moreover, the generalization capability of CSKB is theoretically analyzed. Comprehensive experiments on synthetic, UCI and credit risk evaluation datasets demonstrate that CSKB compares more favorably than other benchmark methods in terms of various measures.}
}
@article{CHATZIKONSTANTINOU2021475,
title = {Recurrent neural network pruning using dynamical systems and iterative fine-tuning},
journal = {Neural Networks},
volume = {143},
pages = {475-488},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002641},
author = {Christos Chatzikonstantinou and Dimitrios Konstantinidis and Kosmas Dimitropoulos and Petros Daras},
keywords = {Recurrent neural networks, Network pruning, Linear dynamical systems, Regularization},
abstract = {Network pruning techniques are widely employed to reduce the memory requirements and increase the inference speed of neural networks. This work proposes a novel RNN pruning method that considers the RNN weight matrices as collections of time-evolving signals. Such signals that represent weight vectors can be modelled using Linear Dynamical Systems (LDSs). In this way, weight vectors with similar temporal dynamics can be pruned as they have limited effect on the performance of the model. Additionally, during the fine-tuning of the pruned model, a novel discrimination-aware variation of the L2 regularization is introduced to penalize network weights (i.e., reduce the magnitude), whose impact on the output of an RNN network is minimal. Finally, an iterative fine-tuning approach is proposed that employs a bigger model to guide an increasingly smaller pruned one, as a steep decrease of the network parameters can irreversibly harm the performance of the pruned model. Extensive experimentation with different network architectures demonstrates the potential of the proposed method to create pruned models with significantly improved perplexity by at least 0.62% on the PTB dataset and improved F1-score by 1.39% on the SQuAD dataset, contrary to other state-of-the-art approaches that slightly improve or even deteriorate models’ performance.}
}
@article{TOKUDA2021269,
title = {Multiple clustering for identifying subject clusters and brain sub-networks using functional connectivity matrices without vectorization},
journal = {Neural Networks},
volume = {142},
pages = {269-287},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002124},
author = {Tomoki Tokuda and Okito Yamashita and Junichiro Yoshimoto},
keywords = {Functional connectivity, Clustering, Multiple clustering, Wishart mixture},
abstract = {In neuroscience, the functional magnetic resonance imaging (fMRI) is a vital tool to non-invasively access brain activity. Using fMRI, the functional connectivity (FC) between brain regions can be inferred, which has contributed to a number of findings of the fundamental properties of the brain. As an important clinical application of FC, clustering of subjects based on FC recently draws much attention, which can potentially reveal important heterogeneity in subjects such as subtypes of psychiatric disorders. In particular, a multiple clustering method is a powerful analytical tool, which identifies clustering patterns of subjects depending on their FC in specific brain areas. However, when one applies an existing multiple clustering method to fMRI data, there is a need to simplify the data structure, independently dealing with elements in a FC matrix, i.e., vectorizing a correlation matrix. Such a simplification may distort the clustering results. To overcome this problem, we propose a novel multiple clustering method based on Wishart mixture models, which preserves the correlation matrix structure without vectorization. The uniqueness of this method is that the multiple clustering of subjects is based on particular networks of nodes (or regions of interest, ROIs), optimized in a data-driven manner. Hence, it can identify multiple underlying pairs of associations between a subject cluster solution and a ROI sub-network. The key assumption of the method is independence among sub-networks, which is effectively addressed by whitening correlation matrices. We applied the proposed method to synthetic and fMRI data, demonstrating the usefulness and power of the proposed method.}
}
@article{KAZANOVICH2021628,
title = {A computational model of familiarity detection for natural pictures, abstract images, and random patterns: Combination of deep learning and anti-Hebbian training},
journal = {Neural Networks},
volume = {143},
pages = {628-637},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002859},
author = {Yakov Kazanovich and Roman Borisyuk},
keywords = {Recognition memory, Familiarity recognition, Deep learning, Anti-Hebbian rule, Memorization},
abstract = {We present a neural network model for familiarity recognition of different types of images in the perirhinal cortex (the FaRe model). The model is designed as a two-stage system. At the first stage, the parameters of an image are extracted by a pretrained deep learning convolutional neural network. At the second stage, a two-layer feed forward neural network with anti-Hebbian learning is used to make the decision about the familiarity of the image. FaRe model simulations demonstrate high capacity of familiarity recognition memory for natural pictures and low capacity for both abstract images and random patterns. These findings are in agreement with psychological experiments.}
}
@article{ARSLAN2021119,
title = {Novel criteria for global robust stability of dynamical neural networks with multiple time delays},
journal = {Neural Networks},
volume = {142},
pages = {119-127},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.04.039},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021001763},
author = {Emel Arslan},
keywords = {Time delayed systems, Uncertain neural networks, Robust stability, Lyapunov-type functionals},
abstract = {This research article considers the problem regarding global robust asymptotic stability of the general type of dynamical neural networks involving multiple constant time delays. Some new sufficient criteria are proposed for the existence, uniqueness and global asymptotic stability of the equilibrium point of this neural network model whose network parameters possess uncertainties. This paper will first address the existence and uniqueness problem for equilibrium points by utilizing the Homomorphic transformation theory. Secondly, by exploiting a novel Lyapunov functional candidate, the sufficient conditions for asymptotic stability of equilibrium points of this class of delayed neural networks will be established. The derived robust stability conditions are expressed independently of the constant time delay parameters and define some novel relationships among network parameters of the considered neural network. Thus, the applicability and validity of the obtained robust stability conditions for delayed-type neural networks can be easily tested. The comprehensive comparisons among the results of the current article and some of previously derived corresponding results will also be made by giving an illustrative numerical example.}
}
@article{OZYILDIRIM2021564,
title = {Levenberg–Marquardt multi-classification using hinge loss function},
journal = {Neural Networks},
volume = {143},
pages = {564-571},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002732},
author = {Buse Melis Ozyildirim and Mariam Kiran},
keywords = {Neural networks, Levenberg–Marquardt, Hinge loss, Loss functions, Classification},
abstract = {Incorporating higher-order optimization functions, such as Levenberg–Marquardt (LM) have revealed better generalizable solutions for deep learning problems. However, these higher-order optimization functions suffer from very large processing time and training complexity especially as training datasets become large, such as in multi-view classification problems, where finding global optima is a very costly problem. To solve this issue, we develop a solution for LM-enabled classification with, to the best of knowledge first-time implementation of hinge loss, for multiview classification. Hinge loss allows the neural network to converge faster and perform better than other loss functions such as logistic or square loss rates. We prove our method by experimenting with various multiclass classification challenges of varying complexity and training data size. The empirical results show the training time and accuracy rates achieved, highlighting how our method outperforms in all cases, especially when training time is limited. Our paper presents important results in the relationship between optimization and loss functions and how these can impact deep learning problems.}
}
@article{HUANG2021198,
title = {Fast mesh data augmentation via Chebyshev polynomial of spectral filtering},
journal = {Neural Networks},
volume = {143},
pages = {198-208},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.025},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002215},
author = {Shih-Gu Huang and Moo K. Chung and Anqi Qiu},
keywords = {Data augmentation, Signals on surfaces, Laplace–Beltrami operator, Cortical thickness, Graph-CNN},
abstract = {Deep neural networks have recently been recognized as one of the powerful learning techniques in computer vision and medical image analysis. Trained deep neural networks need to be generalizable to new data that are not seen before. In practice, there is often insufficient training data available, which can be solved via data augmentation. Nevertheless, there is a lack of augmentation methods to generate data on graphs or surfaces, even though graph convolutional neural network (graph-CNN) has been widely used in deep learning. This study proposed two unbiased augmentation methods, Laplace–Beltrami eigenfunction Data Augmentation (LB-eigDA) and Chebyshev polynomial Data Augmentation (C-pDA), to generate new data on surfaces, whose mean was the same as that of observed data. LB-eigDA augmented data via the resampling of the LB coefficients. In parallel with LB-eigDA, we introduced a fast augmentation approach, C-pDA, that employed a polynomial approximation of LB spectral filters on surfaces. We designed LB spectral bandpass filters by Chebyshev polynomial approximation and resampled signals filtered via these filters in order to generate new data on surfaces. We first validated LB-eigDA and C-pDA via simulated data and demonstrated their use for improving classification accuracy. We then employed brain images of the Alzheimer’s Disease Neuroimaging Initiative (ADNI) and extracted cortical thickness that was represented on the cortical surface to illustrate the use of the two augmentation methods. We demonstrated that augmented cortical thickness had a similar pattern to observed data. We also showed that C-pDA was faster than LB-eigDA and can improve the AD classification accuracy of graph-CNN.}
}
@article{ROY2021636,
title = {Spatial information transfer in hippocampal place cells depends on trial-to-trial variability, symmetry of place-field firing, and biophysical heterogeneities},
journal = {Neural Networks},
volume = {142},
pages = {636-660},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.026},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002975},
author = {Ankit Roy and Rishikesh Narayanan},
keywords = {Degeneracy, Ion channels, Mutual information, Stimulus specific information, Tuning curve},
abstract = {The relationship between the feature-tuning curve and information transfer profile of individual neurons provides vital insights about neural encoding. However, the relationship between the spatial tuning curve and spatial information transfer of hippocampal place cells remains unexplored. Here, employing a stochastic search procedure spanning thousands of models, we arrived at 127 conductance-based place-cell models that exhibited signature electrophysiological characteristics and sharp spatial tuning, with parametric values that exhibited neither clustering nor strong pairwise correlations. We introduced trial-to-trial variability in responses and computed model tuning curves and information transfer profiles, using stimulus-specific (SSI) and mutual (MI) information metrics, across locations within the place field. We found spatial information transfer to be heterogeneous across models, but to reduce consistently with increasing levels of variability. Importantly, whereas reliable low-variability responses implied that maximal information transfer occurred at high-slope regions of the tuning curve, increase in variability resulted in maximal transfer occurring at the peak-firing location in a subset of models. Moreover, experience-dependent asymmetry in place-field firing introduced asymmetries in the information transfer computed through MI, but not SSI, and the impact of activity-dependent variability on information transfer was minimal compared to activity-independent variability. We unveiled ion-channel degeneracy in the regulation of spatial information transfer, and demonstrated critical roles for N-methyl-d-aspartate receptors, transient potassium and dendritic sodium channels in regulating information transfer. Our results demonstrate that trial-to-trial variability, tuning-curve shape and biological heterogeneities critically regulate the relationship between the spatial tuning curve and spatial information transfer in hippocampal place cells.}
}
@article{CAI2021230,
title = {Periodicity and multi-periodicity generated by impulses control in delayed Cohen–Grossberg-type neural networks with discontinuous activations},
journal = {Neural Networks},
volume = {143},
pages = {230-245},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002458},
author = {Zuowei Cai and Lihong Huang and Zengyun Wang and Xianmin Pan and Shukun Liu},
keywords = {Cohen–Grossberg Neural Networks (CGNNs), Differential inclusions (DI), Impulses control, Time-delay, Discontinuous activations, Periodicity},
abstract = {This paper discusses the periodicity and multi-periodicity in delayed Cohen–Grossberg-type neural networks (CGNNs) possessing impulsive effects, whose activation functions possess discontinuities and are allowed to be unbounded or nonmonotonic. Based on differential inclusion and cone expansion–compression fixed-point theory of set-valued mapping, several improved criteria are given to derive the positive solution with ω-periodicity and ω-multi-periodicity for delayed CGNNs under impulsive control. These ω-periodicity/ω-multi-periodicity orbits are produced by impulses control. The analytical method and theoretical results presented in this paper are of certain significance to the design of neural network models or circuits possessing discontinuous neuron activation and impulsive effects in periodic environment.}
}
@article{CHEN2021246,
title = {Finite time convergence of pinning synchronization with a single nonlinear controller},
journal = {Neural Networks},
volume = {143},
pages = {246-249},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.036},
url = {https://www.sciencedirect.com/science/article/pii/S089360802100232X},
author = {Tianping Chen and Wenlian Lu and Xiwei Liu},
keywords = {Finite time convergence, Distributed algorithm, Pinning controller, Synchronization},
abstract = {In this paper, we discuss distributive synchronization of complex networks in finite time, with a single nonlinear pinning controller. The results apply to heterogeneous dynamic networks, too. Different from many models, which assume the coupling matrix being symmetric (or the connecting graph is undirected), here, the coupling matrix is asymmetric (or the connecting graph is directed).}
}
@article{XIONG2021525,
title = {Online sensorimotor learning and adaptation for inverse dynamics control},
journal = {Neural Networks},
volume = {143},
pages = {525-536},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.029},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002616},
author = {Xiaofeng Xiong and Poramate Manoonpong},
keywords = {Neural network, Robot control, Variable compliant control, Gaussian model},
abstract = {We propose a micro-data (< 10 trials) sensorimotor learning and adaptation (SEED) model for human-like arm inverse dynamics control. The SEED model consists of a feedforward Gaussian motor primitive (GATE) neural network and an adaptive feedback impedance (AIM) mechanism. Sensorimotor weights over trials are learned in the GATE network, while the AIM mechanism is used to online tune impedance gains in a trial. The model was validated by periodic and non-periodic tracking tasks on a two-joint robot arm. As a result, the proposed model enables the arm to stably learn the tasks within 10 trials, compared to thousands of trials required by state-of-art deep learning. This model facilitates the exploration of unknown arm dynamics, in which the elbow joint requires much less active control compared to the shoulder. This control goes below 3% of the overall effort. This finding complies with a proximal–distal control gradient in human arm control. Taken together, the proposed SEED model paves a way for implementing data-efficient sensorimotor learning and adaptation of human-like arm movement.}
}
@article{DIAS2021818,
title = {A full-parallel implementation of Self-Organizing Maps on hardware},
journal = {Neural Networks},
volume = {143},
pages = {818-827},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.021},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002173},
author = {Leonardo A. Dias and Augusto M.P. Damasceno and Elena Gaura and Marcelo A.C. Fernandes},
keywords = {Self-Organizing Map, Parallel design, Hardware, FPGA},
abstract = {Self-Organizing Maps (SOMs) are extensively used for data clustering and dimensionality reduction. However, if applications are to fully benefit from SOM based techniques, high-speed processing is demanding, given that data tends to be both highly dimensional and yet “big”. Hence, a fully parallel architecture for the SOM is introduced to optimize the system’s data processing time. Unlike most literature approaches, the architecture proposed here does not contain sequential steps — a common limiting factor for processing speed. The architecture was validated on FPGA and evaluated concerning hardware throughput and the use of resources. Comparisons to the state of the art show a speedup of 8.91× over a partially serial implementation, using less than 15% of hardware resources available. Thus, the method proposed here points to a hardware architecture that will not be obsolete quickly.}
}
@article{SUN2021410,
title = {Distributed-force-feedback-based reflex with online learning for adaptive quadruped motor control},
journal = {Neural Networks},
volume = {142},
pages = {410-427},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002331},
author = {Tao Sun and Zhendong Dai and Poramate Manoonpong},
keywords = {Reflexes, CPGs, Motor learning, Offset adaptation, Slope terrains, Quadruped robots},
abstract = {Biological motor control mechanisms (e.g., central pattern generators (CPGs), sensory feedback, reflexes, and motor learning) play a crucial role in the adaptive locomotion of animals. However, the interaction and integration of these mechanisms – necessary for generating the efficient, adaptive locomotion responses of legged robots to diverse terrains – have not yet been fully realized. One issue is that of achieving adaptive motor control for fast postural adaptation across various terrains. To address this issue, this study proposes a novel distributed-force-feedback-based reflex with online learning (DFRL). It integrates force-sensory feedback, reflexes, and learning to cooperate with CPGs in producing adaptive motor commands. The DFRL is based on a simple neural network that uses plastic synapses modulated online by a fast dual integral learner. Experimental results on different quadruped robots show that the DFRL can (1) automatically and rapidly adapt the CPG patterns (motor commands) of the robots, enabling them to realize appropriate body postures during locomotion and (2) enable the robots to effectively accommodate themselves to various slope terrains, including steep ones. Consequently, the DFRL-controlled robots can achieve efficient adaptive locomotion, to tackle complex terrains with diverse slopes.}
}
@article{NB2021425,
title = {When Noise meets Chaos: Stochastic Resonance in Neurochaos Learning},
journal = {Neural Networks},
volume = {143},
pages = {425-435},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.025},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002574},
author = {Harikrishnan N.B. and Nithin Nagaraj},
keywords = {Neurochaos Learning, , Machine learning, Stochastic Resonance, Hindmarsh–Rose neuronal model, Noise},
abstract = {Chaos and Noise are ubiquitous in the Brain. Inspired by the chaotic firing of neurons and the constructive role of noise in neuronal models, we for the first time connect chaos, noise and learning. In this paper, we demonstrate Stochastic Resonance (SR) phenomenon in Neurochaos Learning (NL). SR manifests at the level of a single neuron of NL and enables efficient subthreshold signal detection. Furthermore, SR is shown to occur in single and multiple neuronal NL architecture for classification tasks — both on simulated and real-world spoken digit datasets, and in architectures with 1D chaotic maps as well as Hindmarsh–Rose spiking neurons. Intermediate levels of noise in neurochaos learning enable peak performance in classification tasks thus highlighting the role of SR in AI applications, especially in brain inspired learning architectures.}
}
@article{IRANZOSANCHEZ2021303,
title = {Streaming cascade-based speech translation leveraged by a direct segmentation model},
journal = {Neural Networks},
volume = {142},
pages = {303-315},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002057},
author = {Javier Iranzo-Sánchez and Javier Jorge and Pau Baquero-Arnal and Joan Albert Silvestre-Cerdà and Adrià Giménez and Jorge Civera and Albert Sanchis and Alfons Juan},
keywords = {Streaming Cascade Speech Translation, Segmentation Model},
abstract = {The cascade approach to Speech Translation (ST) is based on a pipeline that concatenates an Automatic Speech Recognition (ASR) system followed by a Machine Translation (MT) system. Nowadays, state-of-the-art ST systems are populated with deep neural networks that are conceived to work in an offline setup in which the audio input to be translated is fully available in advance. However, a streaming setup defines a completely different picture, in which an unbounded audio input gradually becomes available and at the same time the translation needs to be generated under real-time constraints. In this work, we present a state-of-the-art streaming ST system in which neural-based models integrated in the ASR and MT components are carefully adapted in terms of their training and decoding procedures in order to run under a streaming setup. In addition, a direct segmentation model that adapts the continuous ASR output to the capacity of simultaneous MT systems trained at the sentence level is introduced to guarantee low latency while preserving the translation quality of the complete ST system. The resulting ST system is thoroughly evaluated on the real-life streaming Europarl-ST benchmark to gauge the trade-off between quality and latency for each component individually as well as for the complete ST system.}
}
@article{GUO2021657,
title = {Content-aware convolutional neural networks},
journal = {Neural Networks},
volume = {143},
pages = {657-668},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.030},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002628},
author = {Yong Guo and Yaofo Chen and Mingkui Tan and Kui Jia and Jian Chen and Jingdong Wang},
keywords = {Convolution, Neural networks, Redundancy reduction},
abstract = {Convolutional Neural Networks (CNNs) have achieved great success due to the powerful feature learning ability of convolution layers. Specifically, the standard convolution traverses the input images/features using a sliding window scheme to extract features. However, not all the windows contribute equally to the prediction results of CNNs. In practice, the convolutional operation on some of the windows (e.g., smooth windows that contain very similar pixels) can be very redundant and may introduce noises into the computation. Such redundancy may not only deteriorate the performance but also incur the unnecessary computational cost. Thus, it is important to reduce the computational redundancy of convolution to improve the performance. To this end, we propose a Content-aware Convolution (CAC) that automatically detects the smooth windows and applies a 1 ×1 convolutional kernel to replace the original large kernel. In this sense, we are able to effectively avoid the redundant computation on similar pixels. By replacing the standard convolution in CNNs with our CAC, the resultant models yield significantly better performance and lower computational cost than the baseline models with the standard convolution. More critically, we are able to dynamically allocate suitable computation resources according to the data smoothness of different images, making it possible for content-aware computation. Extensive experiments on various computer vision tasks demonstrate the superiority of our method over existing methods.}
}
@article{LI2021345,
title = {Graph routing between capsules},
journal = {Neural Networks},
volume = {143},
pages = {345-354},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002501},
author = {Yang Li and Wei Zhao and Erik Cambria and Suhang Wang and Steffen Eger},
keywords = {Capsule neural network, Text classification, Routing, Graph routing},
abstract = {Routing methods in capsule networks often learn a hierarchical relationship for capsules in successive layers, but the intra-relation between capsules in the same layer is less studied, while this intra-relation is a key factor for the semantic understanding in text data. Therefore, in this paper, we introduce a new capsule network with graph routing to learn both relationships, where capsules in each layer are treated as the nodes of a graph. We investigate strategies to yield adjacency and degree matrix with three different distances from a layer of capsules, and propose the graph routing mechanism between those capsules. We validate our approach on five text classification datasets, and our findings suggest that the approach combining bottom-up routing and top-down attention performs the best. Such an approach demonstrates generalization capability across datasets. Compared to the state-of-the-art routing methods, the improvements in accuracy in the five datasets we used were 0.82, 0.39, 0.07, 1.01, and 0.02, respectively.}
}
@article{GOULAS2021608,
title = {Bio-instantiated recurrent neural networks: Integrating neurobiology-based network topology in artificial networks},
journal = {Neural Networks},
volume = {142},
pages = {608-618},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002744},
author = {Alexandros Goulas and Fabrizio Damicelli and Claus C. Hilgetag},
keywords = {Network topology, Connectomes, Artificial networks},
abstract = {Biological neuronal networks (BNNs) are a source of inspiration and analogy making for researchers that focus on artificial neuronal networks (ANNs). Moreover, neuroscientists increasingly use ANNs as a model for the brain. Despite certain similarities between these two types of networks, important differences can be discerned. First, biological neural networks are sculpted by evolution and the constraints that it entails, whereas artificial neural networks are engineered to solve particular tasks. Second, the network topology of these systems, apart from some analogies that can be drawn, exhibits pronounced differences. Here, we examine strategies to construct recurrent neural networks (RNNs) that instantiate the network topology of brains of different species. We refer to such RNNs as bio-instantiated. We investigate the performance of bio-instantiated RNNs in terms of: (i) the prediction performance itself, that is, the capacity of the network to minimize the cost function at hand in test data, and (ii) speed of training, that is, how fast during training the network reaches its optimal performance. We examine bio-instantiated RNNs in working memory tasks where task-relevant information must be tracked as a sequence of events unfolds in time. We highlight the strategies that can be used to construct RNNs with the network topology found in BNNs, without sacrificing performance. Despite that we observe no enhancement of performance when compared to randomly wired RNNs, our approach demonstrates how empirical neural network data can be used for constructing RNNs, thus, facilitating further experimentation with biologically realistic network topologies, in contexts where such aspect is desired.}
}
@article{ZHANG2021388,
title = {Spectral embedding network for attributed graph clustering},
journal = {Neural Networks},
volume = {142},
pages = {388-396},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.026},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002227},
author = {Xiaotong Zhang and Han Liu and Xiao-Ming Wu and Xianchao Zhang and Xinyue Liu},
keywords = {Attributed graph clustering, Spectral embedding network, Graph structure improvement, Kernel matrix learning},
abstract = {Attributed graph clustering aims to discover node groups by utilizing both graph structure and node features. Recent studies mostly adopt graph neural networks to learn node embeddings, then apply traditional clustering methods to obtain clusters. However, they usually suffer from the following issues: (1) they adopt original graph structure which is unfavorable for clustering due to its noise and sparsity problems; (2) they mainly utilize non-clustering driven losses that cannot well capture the global cluster structure, thus the learned embeddings are not sufficient for the downstream clustering task. In this paper, we propose a spectral embedding network for attributed graph clustering (SENet), which improves graph structure by leveraging the information of shared neighbors, and learns node embeddings with the help of a spectral clustering loss. By combining the original graph structure and shared neighbor based similarity, both the first-order and second-order proximities are encoded into the improved graph structure, thus alleviating the noise and sparsity issues. To make the spectral loss well adapt to attributed graphs, we integrate both structure and feature information into kernel matrix via a higher-order graph convolution. Experiments on benchmark attributed graphs show that SENet achieves superior performance over state-of-the-art methods.}
}
@article{DUNG2021619,
title = {Deep ReLU neural networks in high-dimensional approximation},
journal = {Neural Networks},
volume = {142},
pages = {619-635},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.027},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002987},
author = {Dinh Dũng and Van Kien Nguyen},
keywords = {Deep ReLU neural network, Computation complexity, High-dimensional approximation, Sparse-grid sampling, Continuous piece-wise linear functions, Hölder–Zygmund space of mixed smoothness},
abstract = {We study the computation complexity of deep ReLU (Rectified Linear Unit) neural networks for the approximation of functions from the Hölder–Zygmund space of mixed smoothness defined on the d-dimensional unit cube when the dimension d may be very large. The approximation error is measured in the norm of isotropic Sobolev space. For every function f from the Hölder–Zygmund space of mixed smoothness, we explicitly construct a deep ReLU neural network having an output that approximates f with a prescribed accuracy ɛ, and prove tight dimension-dependent upper and lower bounds of the computation complexity of the approximation, characterized as the size and depth of this deep ReLU neural network, explicitly in d and ɛ. The proof of these results in particular, relies on the approximation by sparse-grid sampling recovery based on the Faber series.}
}
@article{ZHANG2021161,
title = {Synchronization of recurrent neural networks with unbounded delays and time-varying coefficients via generalized differential inequalities},
journal = {Neural Networks},
volume = {143},
pages = {161-170},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002185},
author = {Hao Zhang and Zhigang Zeng},
keywords = {Asymptotic synchronization, Unbounded delay, Time-varying neural networks, Generalized delay differential inequality},
abstract = {In this paper, we revisit the drive-response synchronization of a class of recurrent neural networks with unbounded delays and time-varying coefficients, contrary to usual in the literature about time-varying neural networks, the signs of self-feedback coefficients are permitted to be indefinite or the time-varying coefficients can be unbounded. A generalized scalar delay differential inequality considering indefinite self-feedback coefficient and unbounded delay simultaneously is established, which covers the existing result with bounded delay, the applicabilities of the sufficient conditions are discussed. Some novel criteria for network synchronization are then derived by constructing different candidate functions. These results have been improved in some aspects compared with the existing ones. Differential inequality in vector form is also derived to obtain a more refined synchronization criterion which removes some strong assumptions. Three examples are presented to verify the effectiveness and show the superiorities of our theoretical results.}
}
@article{WANG2021395,
title = {PC-GAIN: Pseudo-label conditional generative adversarial imputation networks for incomplete data},
journal = {Neural Networks},
volume = {141},
pages = {395-403},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S089360802100229X},
author = {Yufeng Wang and Dan Li and Xiang Li and Min Yang},
keywords = {Conditional, Generative adversarial network, Imputation, Missing data, Pseudo-label},
abstract = {Datasets with missing values are very common in real world applications. GAIN, a recently proposed deep generative model for missing data imputation, has been proved to outperform many state-of-the-art methods. But GAIN only uses a reconstruction loss in the generator to minimize the imputation error of the non-missing part, ignoring the potential category information which can reflect the relationship between samples. In this paper, we propose a novel unsupervised missing data imputation method named PC-GAIN, which utilizes potential category information to further enhance the imputation power. Specifically, we first propose a pre-training procedure to learn potential category information contained in a subset of low-missing-rate data. Then an auxiliary classifier is determined using the synthetic pseudo-labels. Further, this classifier is incorporated into the generative adversarial framework to help the generator to yield higher quality imputation results. The proposed method can improve the imputation quality of GAIN significantly. Experimental results on various benchmark datasets show that our method is also superior to other baseline approaches. Our code is available at https://github.com/WYu-Feng/pc-gain.}
}
@article{CAO2021288,
title = {Event-triggered adaptive neural networks control for fractional-order nonstrict-feedback nonlinear systems with unmodeled dynamics and input saturation},
journal = {Neural Networks},
volume = {142},
pages = {288-302},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002100},
author = {Boqiang Cao and Xiaobing Nie},
keywords = {Adaptive neural networks control, Unmodeled dynamics, ISpS, Adaptive event-triggered mechanism, Input saturation},
abstract = {The event-triggered adaptive neural networks control is investigated in this paper for a class of fractional-order systems (FOSs) with unmodeled dynamics and input saturation. Firstly, in order to obtain an auxiliary signal and then avoid the state variables of unmodeled dynamics directly appearing in the designed controller, the notion of exponential input-to-state practical stability (ISpS) and some related lemmas for integer-order systems are extended to the ones for FOSs. Then, based on the traditional event-triggered mechanism, we propose a novel adaptive event-triggered mechanism (AETM) in this paper, in which the threshold parameters can be adjusted dynamically according to the tracking performance. Besides, different from the previous works where the derivative of hyperbolic tangent function tanh(⋅) needs to have positive lower bound, a new type of auxiliary signal is introduced in this paper to handle the effect of input saturation and thus this limitation is released. Finally, two numerical examples and some comparisons are provided to illustrate our proposed controllers.}
}
@article{TONIN2021661,
title = {Unsupervised learning of disentangled representations in deep restricted kernel machines with orthogonality constraints},
journal = {Neural Networks},
volume = {142},
pages = {661-679},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.023},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002860},
author = {Francesco Tonin and Panagiotis Patrinos and Johan A.K. Suykens},
keywords = {Kernel methods, Unsupervised learning, Manifold learning, Learning disentangled representations},
abstract = {We introduce Constr-DRKM, a deep kernel method for the unsupervised learning of disentangled data representations. We propose augmenting the original deep restricted kernel machine formulation for kernel PCA by orthogonality constraints on the latent variables to promote disentanglement and to make it possible to carry out optimization without first defining a stabilized objective. After discussing a number of algorithms for end-to-end training, we quantitatively evaluate the proposed method’s effectiveness in disentangled feature learning. We demonstrate on four benchmark datasets that this approach performs similarly overall to β-VAE on several disentanglement metrics when few training points are available while being less sensitive to randomness and hyperparameter selection than β-VAE. We also present a deterministic initialization of Constr-DRKM’s training algorithm that significantly improves the reproducibility of the results. Finally, we empirically evaluate and discuss the role of the number of layers in the proposed methodology, examining the influence of each principal component in every layer and showing that components in lower layers act as local feature detectors capturing the broad trends of the data distribution, while components in deeper layers use the representation learned by previous layers and more accurately reproduce higher-level features.}
}
@article{KIM2021148,
title = {Periodic clustering of simple and complex cells in visual cortex},
journal = {Neural Networks},
volume = {143},
pages = {148-160},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002343},
author = {Gwangsu Kim and Jaeson Jang and Se-Bum Paik},
keywords = {Primary visual cortex, Simple cell, Complex cell, Orientation map, Retinal mosaic, Feedforward projection},
abstract = {Neurons in the primary visual cortex (V1) are often classified as simple or complex cells, but it is debated whether they are discrete hierarchical classes of neurons or if they represent a continuum of variation within a single class of cells. Herein, we show that simple and complex cells may arise commonly from the feedforward projections from the retina. From analysis of the cortical receptive fields in cats, we show evidence that simple and complex cells originate from the periodic variation of ON–OFF segregation in the feedforward projection of retinal mosaics, by which they organize into periodic clusters in V1. From data in cats, we observed that clusters of simple and complex receptive fields correlate topographically with orientation maps, which supports our model prediction. Our results suggest that simple and complex cells are not two distinct neural populations but arise from common retinal afferents, simultaneous with orientation tuning.}
}
@article{CHEN2021492,
title = {Predefined-time synchronization of competitive neural networks},
journal = {Neural Networks},
volume = {142},
pages = {492-499},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002586},
author = {Chuan Chen and Ling Mi and Zhongqiang Liu and Baolin Qiu and Hui Zhao and Lijuan Xu},
keywords = {Competitive neural networks, Predefined-time synchronization, The predefined time},
abstract = {In this paper, the predefined-time synchronization of competitive neural networks (CNNs) is researched based on two different predefined-time stability theorems. In view of the bilayer structure of CNNs, we design two bilayer predefined-time controllers. The first controller utilizes sign function, while the second controller utilizes exponential function and Lyapunov function. In these two controllers, the predefined time is set as a controller parameter, and it can be an arbitrary positive constant. Under these two controllers, the considered CNNs can achieve synchronization within the predefined time regardless of the initial values. A specific example is presented to validate the theoretical results.}
}