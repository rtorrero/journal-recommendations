@article{2023I,
title = {CURRENT EVENTS},
journal = {Neural Networks},
volume = {167},
pages = {I},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(23)00545-2},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023005452}
}
@article{2023I,
title = {CURRENT EVENTS},
journal = {Neural Networks},
volume = {166},
pages = {I},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(23)00476-8},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023004768}
}
@article{HAN2022175,
title = {Efficient joint model learning, segmentation and model updating for visual tracking},
journal = {Neural Networks},
volume = {147},
pages = {175-185},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004974},
author = {Wei Han and Chamara Kasun Liyanaarachchi Lekamalage and Guang-Bin Huang},
keywords = {Visual tracking, Extreme learning machine, Semi-supervised learning, Tracking-by-segmentation},
abstract = {The Tracking-by-segmentation framework is widely used in visual tracking to handle severe appearance change such as deformation and occlusion. Tracking-by-segmentation methods first segment the target object from the background, then use the segmentation result to estimate the target state. In existing methods, target segmentation is formulated as a superpixel labeling problem constrained by a target likelihood constraint, a spatial smoothness constraint and a temporal consistency constraint. The target likelihood is calculated by a discriminative part model trained independently from the superpixel labeling framework and updated online using historical tracking results as pseudo-labels. Due to the lack of spatial and temporal constraints and inaccurate pseudo-labels, the discriminative model is unreliable and may lead to tracking failure. This paper addresses the aforementioned problems by integrating the objective function of model training into the target segmentation optimization framework. Thus, during the optimization process, the discriminative model can be constrained by spatial and temporal constraints and provides more accurate target likelihoods for part labeling, and the results produce more reliable pseudo-labels for model learning. Moreover, we also propose a supervision switch mechanism to detect erroneous pseudo-labels caused by a severe change in data distribution and switch the classifier to a semi-supervised setting in such a case. Evaluation results on OTB2013, OTB2015 and TC-128 benchmarks demonstrate the effectiveness of the proposed tracking algorithm.}
}
@article{CHANG2022120,
title = {Event-centric multi-modal fusion method for dense video captioning},
journal = {Neural Networks},
volume = {146},
pages = {120-129},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004457},
author = {Zhi Chang and Dexin Zhao and Huilin Chen and Jingdan Li and Pengfei Liu},
keywords = {Dense video captioning, Event-centric, Multi-modal fusion},
abstract = {Dense video captioning aims to automatically describe several events that occur in a given video, which most state-of-the-art models accomplish by locating and describing multiple events in an untrimmed video. Despite much progress in this area, most current approaches only encode visual features in the event location phase and they neglect the relationships between events, which may degrade the consistency of the description in the identical video. Thus, in the present study, we attempted to exploit visual–audio cues to generate event proposals and enhance event-level representations by capturing their temporal and semantic relationships. Furthermore, to compensate for the major limitation of not fully utilizing multimodal information in the description process, we developed an attention-gating mechanism that dynamically fuses and regulates the multi-modal information. In summary, we propose an event-centric multi-modal fusion approach for dense video captioning (EMVC) to capture the relationships between events and effectively fuse multi-modal information. We conducted comprehensive experiments to evaluate the performance of EMVC based on the benchmark ActivityNet Caption and YouCook2 data sets. The experimental results showed that our model achieved impressive performance compared with state-of-the-art methods.}
}
@article{YAO2022206,
title = {GARAT: Generative Adversarial Learning for Robust and Accurate Tracking},
journal = {Neural Networks},
volume = {148},
pages = {206-218},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000107},
author = {Bowen Yao and Jing Li and Shan Xue and Jia Wu and Huanmei Guan and Jun Chang and Zhiquan Ding},
keywords = {Object tracking, Siamese network, Generative adversarial learning, Generalized intersection over union},
abstract = {Object tracking by the Siamese network has gained its popularity for its outstanding performance and considerable potential. However, most of the existing Siamese architectures are faced with great difficulties when it comes to the scenes where the target is going through dramatic shape or environmental changes. In this work, we proposed a novel and concise generative adversarial learning method to solve the problem especially when the target is going under drastic changes of appearance, illumination variations and background clutters. We consider the above situations as distractors for tracking and joint a distractor generator into the traditional Siamese network. The component can simulate these distractors, and more robust tracking performance is achieved by eliminating the distractors from the input instance search image. Besides, we use the generalized intersection over union (GIoU) as our training loss. GIoU is a more strict metric for the bounding box regression compared to the traditional IoU, which can be used as training loss for more accurate tracking results. Experiments on five challenging benchmarks have shown favorable and state-of-the-art results against other trackers in different aspects.}
}
@article{PENG202237,
title = {Finite-time synchronization of quaternion-valued neural networks with delays: A switching control method without decomposition},
journal = {Neural Networks},
volume = {148},
pages = {37-47},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004858},
author = {Tao Peng and Jie Zhong and Zhengwen Tu and Jianquan Lu and Jungang Lou},
keywords = {Finite-time synchronization, Quaternion-valued neural networks, Time delays, Switching control method without decomposition},
abstract = {For a class of quaternion-valued neural networks (QVNNs) with discrete and distributed time delays, its finite-time synchronization (FTSYN) is addressed in this paper. Instead of decomposition, a direct analytical method named two-step analysis is proposed. That method can always be used to study FTSYN, under either 1-norm or 2-norm of quaternion. Compared with the decomposing method, the two-step method is also suitable for models that are not easily decomposed. Furthermore, a switching controller based on the two-step method is proposed. In addition, two criteria are given to realize the FTSYN of QVNNs. At last, three numerical examples illustrate the feasibility, effectiveness and practicability of our method.}
}
@article{GUNASEKARAN2022137,
title = {Finite-time and sampled-data synchronization of complex dynamical networks subject to average dwell-time switching signal},
journal = {Neural Networks},
volume = {149},
pages = {137-145},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000466},
author = {Nallappan Gunasekaran and M. Syed Ali and Sabri Arik and H.I. Abdul Ghaffar and Ahmed A. Zaki Diab},
keywords = {Average dwell-time, Complex dynamical networks, Finite-time boundedness, Lyapunov method, Sampled-data control, Switching signal},
abstract = {This study deals with the finite-time synchronization problem of a class of switched complex dynamical networks (CDNs) with distributed coupling delays via sampled-data control. First, the dynamical model is studied with coupling delays in more detail. The sampling system is then converted to a continuous time-delay system using an input delay technique. We obtain some unique and less conservative criteria on exponential stability using the Lyapunov–Krasovskii functional (LKF), which is generated with a Kronecker product, linear matrix inequalities (LMIs), and integral inequality. Furthermore, some sufficient criteria are derived by an average dwell-time method and determine the finite-time boundedness of CDNs with switching signal. The proposed sufficient conditions can be represented in the form of LMIs. Finally, numerical examples are given to show that the suggested strategy is feasible.}
}
@article{XU2022142,
title = {Signed network representation with novel node proximity evaluation},
journal = {Neural Networks},
volume = {148},
pages = {142-154},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000223},
author = {Pinghua Xu and Wenbin Hu and Jia Wu and Weiwei Liu},
keywords = {Signed social network, Network representation, Node proximity},
abstract = {Currently, signed network representation has been applied to many fields, e.g., recommendation platforms. A mainstream paradigm of network representation is to map nodes onto a low-dimensional space, such that the node proximity of interest can be preserved. Thus, a key aspect is the node proximity evaluation. Accordingly, three new node proximity metrics were proposed in this study, based on the rigorous theoretical investigation on a new distance metric - signed average first-passage time (SAFT). SAFT derives from a basic random-walk quantity for unsigned networks and can capture high-order network structure and edge signs. We conducted network representation using the proposed proximity metrics and empirically exhibited our advantage in solving two downstream tasks — sign prediction and link prediction. The code is publicly available.}
}
@article{DEREICH2022121,
title = {On minimal representations of shallow ReLU networks},
journal = {Neural Networks},
volume = {148},
pages = {121-128},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000065},
author = {Steffen Dereich and Sebastian Kassing},
keywords = {Neural networks, Shallow networks, Minimal representations, ReLU activation},
abstract = {The realization function of a shallow ReLU network is a continuous and piecewise affine function f:Rd→R, where the domain Rd is partitioned by a set of n hyperplanes into cells on which f is affine. We show that the minimal representation for f uses either n, n+1 or n+2 neurons and we characterize each of the three cases. In the particular case, where the input layer is one-dimensional, minimal representations always use at most n+1 neurons but in all higher dimensional settings there are functions for which n+2 neurons are needed. Then we show that the set of minimal networks representing f forms a C∞-submanifold M and we derive the dimension and the number of connected components of M. Additionally, we give a criterion for the hyperplanes that guarantees that a continuous, piecewise affine function is the realization function of an appropriate shallow ReLU network.}
}
@article{DU2022386,
title = {Nostradamus: A novel event propagation prediction approach with spatio-temporal characteristics in non-Euclidean space},
journal = {Neural Networks},
volume = {145},
pages = {386-394},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004330},
author = {Haizhou Du and Yan Zhou},
keywords = {Event propagation prediction, Spatio-temporal characteristics, Hyperbolic graph convolutional recurrent neural network, Time series},
abstract = {The prediction of event propagation has received extensive attention from the knowledge discovery community for applications such as virus spread analytics, social network analysis, earthquake location prediction, and typhoon tracking. The data describing these phenomena are multidimensional asynchronous event data that affect each other and show complex dynamic patterns in the continuous-time domain. Unlike the discrete characteristics formed by sampling at equal intervals of asynchronous time series, the timestamps of asynchronous events are in the continuous-time field. The study of these dynamic processes and the mining of their potential correlations provide a foundation for applying event propagation prediction, traceability, and causal inference at both the micro and macro levels. Most of the existing methods represent data as being embedded in the Euclidean space. However, when embedding a real-world graph with a tree-likeliness graph, Euclidean space cannot visually represent a graph. Inspired by the characteristics of hyperbolic space, we propose a model called Nostradamus to capture the propagation of the events of interest from historical events in a graph via the hyperbolic graph neural Hawkes process with Spatio-temporal characteristics. The Nostradamus’ core concept is to integrate the Hawkes process’s conditional intensity function with a hyperbolic graph convolutional recurrent neural network.}
}
@article{ITOH2022356,
title = {Multi-level attention pooling for graph neural networks: Unifying graph representations with multiple localities},
journal = {Neural Networks},
volume = {145},
pages = {356-373},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004299},
author = {Takeshi D. Itoh and Takatomi Kubo and Kazushi Ikeda},
keywords = {Graph representation learning (GRL), Graph neural network (GNN), Multi-level attention pooling (MLAP), Multi-level locality},
abstract = {Graph neural networks (GNNs) have been widely used to learn vector representation of graph-structured data and achieved better task performance than conventional methods. The foundation of GNNs is the message passing procedure, which propagates the information in a node to its neighbors. Since this procedure proceeds one step per layer, the range of the information propagation among nodes is small in the lower layers, and it expands toward the higher layers. Therefore, a GNN model has to be deep enough to capture global structural information in a graph. On the other hand, it is known that deep GNN models suffer from performance degradation because they lose nodes’ local information, which would be essential for good model performance, through many message passing steps. In this study, we propose multi-level attention pooling (MLAP) for graph-level classification tasks, which can adapt to both local and global structural information in a graph. It has an attention pooling layer for each message passing step and computes the final graph representation by unifying the layer-wise graph representations. The MLAP architecture allows models to utilize the structural information of graphs with multiple levels of localities because it preserves layer-wise information before losing them due to oversmoothing. Results of our experiments show that the MLAP architecture improves the graph classification performance compared to the baseline architectures. In addition, analyses on the layer-wise graph representations suggest that aggregating information from multiple levels of localities indeed has the potential to improve the discriminability of learned graph representations.}
}
@article{VASCO2022238,
title = {Leveraging hierarchy in multimodal generative models for effective cross-modality inference},
journal = {Neural Networks},
volume = {146},
pages = {238-255},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.019},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004470},
author = {Miguel Vasco and Hang Yin and Francisco S. Melo and Ana Paiva},
keywords = {Multimodal representation learning, Cross-modality inference, Deep learning},
abstract = {This work addresses the problem of cross-modality inference (CMI), i.e., inferring missing data of unavailable perceptual modalities (e.g., sound) using data from available perceptual modalities (e.g., image). We overview single-modality variational autoencoder methods and discuss three problems of computational cross-modality inference, arising from recent developments in multimodal generative models. Inspired by neural mechanisms of human recognition, we contribute the Nexus model, a novel hierarchical generative model that can learn a multimodal representation of an arbitrary number of modalities in an unsupervised way. By exploiting hierarchical representation levels, Nexus is able to generate high-quality, coherent data of missing modalities given any subset of available modalities. To evaluate CMI in a natural scenario with a high number of modalities, we contribute the “Multimodal Handwritten Digit” (MHD) dataset, a novel benchmark dataset that combines image, motion, sound and label information from digit handwriting. We access the key role of hierarchy in enabling high-quality samples during cross-modality inference and discuss how a novel training scheme enables Nexus to learn a multimodal representation robust to missing modalities at test time. Our results show that Nexus outperforms current state-of-the-art multimodal generative models in regards to their cross-modality inference capabilities.}
}
@article{ZONG2022126,
title = {Observer-based adaptive neural tracking control for a class of nonlinear systems with prescribed performance and input dead-zone constraints},
journal = {Neural Networks},
volume = {147},
pages = {126-135},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004986},
author = {Guangdeng Zong and Yudi Wang and Hamid Reza Karimi and Kaibo Shi},
keywords = {Dynamic surface control, Input dead-zone, Neural network, Output feedback, Prescribed performance control},
abstract = {This paper investigates the problem of output feedback neural network (NN) learning tracking control for nonlinear strict feedback systems subject to prescribed performance and input dead-zone constraints. First, an NN is utilized to approximate the unknown nonlinear functions, then a state observer is developed to estimate the unmeasurable states. Second, based on the command filter method, an output feedback NN learning backstepping control algorithm is established. Third, a prescribed performance function is employed to ensure the transient performance of the closed-loop systems and forces the tracking error to fall within the prescribed performance boundary. It is rigorously proved mathematically that all the signals in the closed-loop systems are semi-globally uniformly ultimately bounded and the tracking error can converge to an arbitrarily small neighborhood of the origin. Finally, a numerical example and an application example of the electromechanical system are given to show effectiveness of the acquired control algorithm.}
}
@article{POLAP2022130,
title = {A hybridization of distributed policy and heuristic augmentation for improving federated learning approach},
journal = {Neural Networks},
volume = {146},
pages = {130-140},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004469},
author = {Dawid Połap and Marcin Woźniak},
keywords = {Image processing, Convolutional neural network, Classification problem, Federated learning},
abstract = {Modifying the existing models of classifiers’ operation is primarily aimed at increasing the effectiveness as well as minimizing the training time. An additional advantage is the ability to quickly implement a given solution to the real needs of the market. In this paper, we propose a method that can implement various classifiers using the federated learning concept and taking into account parallelism. Also, an important element is the analysis and selection of the best classifier depending on its reliability found for separated datasets extended by new, augmented samples. The proposed augmentation technique involves image processing techniques, neural architectures, and heuristic methods and improves the operation in federated learning by increasing the role of the server. The proposition has been presented and tested for the fruit image classification problem. The conducted experiments have shown that the described technique can be very useful as an implementation method even in the case of a small database. Obtained results are discussed concerning the advantages and disadvantages in the context of practical application like higher accuracy.}
}
@article{LIU20221,
title = {A one-layer recurrent neural network for nonsmooth pseudoconvex optimization with quasiconvex inequality and affine equality constraints},
journal = {Neural Networks},
volume = {147},
pages = {1-9},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004743},
author = {Na Liu and Jun Wang and Sitian Qin},
keywords = {Pseudoconvex optimization problem, Quasiconvex constraint functions, Neurodynamic optimization, Convergence analysis},
abstract = {As two important types of generalized convex functions, pseudoconvex and quasiconvex functions appear in many practical optimization problems. The lack of convexity poses some difficulties in solving pseudoconvex optimization with quasiconvex constraint functions. In this paper, we propose a one-layer recurrent neural network for solving such problems. We prove that the state of the proposed neural network is convergent from the feasible region to an optimal solution of the given optimization problem. We show that the proposed neural network has several advantages over the existing neural networks for pseudoconvex optimization. Specifically, the proposed neural network is applicable to optimization problems with quasiconvex inequality constraints as well as affine equality constraints. In addition, parameter matrix inversion is avoided and some assumptions on the objective function and inequality constraints in existing results are relaxed. We demonstrate the superior performance and characteristics of the proposed neural network with simulation results in three numerical examples.}
}
@article{NAGAYAMA202229,
title = {Detecting cell assemblies by NMF-based clustering from calcium imaging data},
journal = {Neural Networks},
volume = {149},
pages = {29-39},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000338},
author = {Mizuo Nagayama and Toshimitsu Aritake and Hideitsu Hino and Takeshi Kanda and Takehiro Miyazaki and Masashi Yanagisawa and Shotaro Akaho and Noboru Murata},
keywords = {Calcium imaging, Clustering, NMF},
abstract = {A large number of neurons form cell assemblies that process information in the brain. Recent developments in measurement technology, one of which is calcium imaging, have made it possible to study cell assemblies. In this study, we aim to extract cell assemblies from calcium imaging data. We propose a clustering approach based on non-negative matrix factorization (NMF). The proposed approach first obtains a similarity matrix between neurons by NMF and then performs spectral clustering on it. The application of NMF entails the problem of model selection. The number of bases in NMF affects the result considerably, and a suitable selection method is yet to be established. We attempt to resolve this problem by model averaging with a newly defined estimator based on NMF. Experiments on simulated data suggest that the proposed approach is superior to conventional correlation-based clustering methods over a wide range of sampling rates. We also analyzed calcium imaging data of sleeping/waking mice and the results suggest that the size of the cell assembly depends on the degree and spatial extent of slow wave generation in the cerebral cortex.}
}
@article{ZHENG2022166,
title = {Not every sample is efficient: Analogical generative adversarial network for unpaired image-to-image translation},
journal = {Neural Networks},
volume = {148},
pages = {166-175},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000211},
author = {Ziqiang Zheng and Jie Yang and Zhibin Yu and Yubo Wang and Zhijian Sun and Bing Zheng},
keywords = {Unpaired image-to-image translation, Generative adversarial network, Metric learning, Analogical learning},
abstract = {Image translation is to learn an effective mapping function that aims to convert an image from a source domain to another target domain. With the proposal and further developments of generative adversarial networks (GANs), the generative models have achieved great breakthroughs. The image-to-image (I2I) translation methods can mainly fall into two categories: Paired and Unpaired. The former paired methods usually require a large amount of input–output sample pairs to perform one-side image translation, which heavily limits its practicability. To address the lack of the paired samples, CycleGAN and its extensions utilize the cycle-consistency loss to provide an elegant and generic solution to perform the unpaired I2I translation between two domains based on unpaired data. This thread of dual learning-based methods usually adopts the random sampling strategy for optimizing and does not consider the content similarity between samples. However, not every sample is efficient and effective for the desired optimization and leads to optimal convergence. Inspired by analogical learning, which is to utilize the relationships and similarities between sample observations, we propose a novel generic metric-based sampling strategy to effectively select samples from different domains for training. Besides, we introduce a novel analogical adversarial loss to force the model to learn from the effective samples and alleviate the influence of the negative samples. Experimental results on various vision tasks have demonstrated the superior performance of the proposed method. The proposed method is also a generic framework that can be easily extended to other I2I translation methods and result in a performance gain.}
}
@article{SUN2022195,
title = {Sign Stochastic Gradient Descents without bounded gradient assumption for the finite sum minimization},
journal = {Neural Networks},
volume = {149},
pages = {195-203},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000454},
author = {Tao Sun and Dongsheng Li},
keywords = {Sign Stochastic Gradient Descent, Convergence, Unbounded gradient, Zeroth-order, Majority vote},
abstract = {Sign-based Stochastic Gradient Descents (Sign-based SGDs) use the signs of the stochastic gradients for communication costs reduction. Nevertheless, current convergence results of sign-based SGDs applied to the finite sum optimization are established on the bounded assumption of the gradient, which fails to hold in various cases. This paper presents a convergence framework about sign-based SGDs with the elimination of the bounded gradient assumption. The ergodic convergence rates are provided only with the smooth assumption of the objective functions. The Sign Stochastic Gradient Descent (signSGD) and its two variants, including majority vote and zeroth-order version, are developed for different application settings. Our framework also removes the bounded gradient assumption used in the previous analysis of these three algorithms.}
}
@article{LONG202286,
title = {Finite-time stabilization of complex-valued neural networks with proportional delays and inertial terms: A non-separation approach},
journal = {Neural Networks},
volume = {148},
pages = {86-95},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000053},
author = {Changqing Long and Guodong Zhang and Zhigang Zeng and Junhao Hu},
keywords = {Finite-time stabilization, Inertial terms, Proportional delays, Complex-valued neural networks, Lyapunov functions},
abstract = {This article mainly dedicates on the issue of finite-time stabilization of complex-valued neural networks with proportional delays and inertial terms via directly constructing Lyapunov functions without separating the original complex-valued neural networks into two real-valued subsystems equivalently. First of all, in order to facilitate the analysis of the second-order derivative caused by the inertial term, two intermediate variables are introduced to transfer complex-valued inertial neural networks (CVINNs) into the first-order differential equation form. Then, under the finite-time stability theory, some new criteria with less conservativeness are established to ensure the finite-time stabilizability of CVINNs by a newly designed complex-valued feedback controller. In addition, for reducing expenses of the control, an adaptive control strategy is also proposed to achieve the finite-time stabilization of CVINNs. At last, numerical examples are given to demonstrate the validity of the derived results.}
}
@article{RAJ202236,
title = {ARCNN framework for multimodal infodemic detection},
journal = {Neural Networks},
volume = {146},
pages = {36-68},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004342},
author = {Chahat Raj and Priyanka Meel},
keywords = {COVID-19 fake news, Infodemic, Deep learning, Multimodal fusion, Neural networks},
abstract = {Fake news and misinformation have adopted various propagation media over time, nowadays spreading predominantly through online social networks. During the ongoing COVID-19 pandemic, false information is affecting human life in many spheres The world needs automated detection technology and efforts are being made to meet this requirement with the use of artificial intelligence. Neural network detection mechanisms are robust and durable and hence are used extensively in fake news detection. Deep learning algorithms demonstrate efficiency when they are provided with a large amount of training data. Given the scarcity of relevant fake news datasets, we built the Coronavirus Infodemic Dataset (CovID), which contains fake news posts and articles related to coronavirus. This paper presents a novel framework, the Allied Recurrent and Convolutional Neural Network (ARCNN), to detect fake news based on two different modalities: text and image. Our approach uses recurrent neural networks (RNNs) and convolutional neural networks (CNNs) and combines both streams to generate a final prediction. We present extensive research on various popular RNN and CNN models and their performance on six coronavirus-specific fake news datasets. To exhaustively analyze performance, we present experimentation performed and results obtained by combining both modalities using early fusion and four types of late fusion techniques. The proposed framework is validated by comparisons with state-of-the-art fake news detection mechanisms, and our models outperform each of them.}
}
@article{CHEN202218,
title = {Anomalous diffusion dynamics of learning in deep neural networks},
journal = {Neural Networks},
volume = {149},
pages = {18-28},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000296},
author = {Guozhang Chen and Cheng Kevin Qu and Pulin Gong},
keywords = {Deep neural networks, Stochastic gradient descent, Complex systems},
abstract = {Learning in deep neural networks (DNNs) is implemented through minimizing a highly non-convex loss function, typically by a stochastic gradient descent (SGD) method. This learning process can effectively find generalizable solutions at flat minima. In this study, we present a novel account of how such effective deep learning emerges through the interactions of the SGD and the geometrical structure of the loss landscape. We find that the SGD exhibits rich, complex dynamics when navigating through the loss landscape; initially, the SGD exhibits superdiffusion, which attenuates gradually and changes to subdiffusion at long times when approaching a solution. Such learning dynamics happen ubiquitously in different DNN types such as ResNet, VGG-like networks and Vision Transformers; similar results emerge for various batch size and learning rate settings. The superdiffusion process during the initial learning phase indicates that the motion of SGD along the loss landscape possesses intermittent, big jumps; this non-equilibrium property enables the SGD to effectively explore the loss landscape. By adapting methods developed for studying energy landscapes in complex physical systems, we find that such superdiffusive learning processes are due to the interactions of the SGD and the fractal-like regions of the loss landscape. We further develop a phenomenological model to demonstrate the mechanistic role of the fractal-like loss landscape in enabling the SGD to effectively find flat minima. Our results reveal the effectiveness of SGD in deep learning from a novel perspective and have implications for designing efficient deep neural networks.}
}
@article{LI202213,
title = {Exponential synchronization for variable-order fractional discontinuous complex dynamical networks with short memory via impulsive control},
journal = {Neural Networks},
volume = {148},
pages = {13-22},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021005037},
author = {Ruihong Li and Huaiqin Wu and Jinde Cao},
keywords = {Fractional complex dynamical networks, Exponential synchronization, Impulsive control scheme, Short memory, Derivative couplings},
abstract = {This paper considers the exponential synchronization issue for variable-order fractional complex dynamical networks (FCDNs) with short memory and derivative couplings via the impulsive control scheme, where dynamical nodes are modeled to be discontinuous. Firstly, the mathematics model with respect to variable-order fractional systems with short memory is established under the impulsive controller, in which the impulse strength is not only determined by the impulse control gain, but also the order of the control systems. Secondly, the exponential stability criterion for variable-order fractional systems with short memory is developed. Thirdly, the hybrid controller, which consists of the impulsive coupling controller and the discontinuous feedback controller, is designed to realize the synchronization objective. In addition, by constructing Lyapunov functional and applying inequality analysis techniques, the synchronization conditions are achieved in terms of linear matrix inequalities (LMIs). Finally, two simulation examples are performed to verify the effectiveness of the developed synchronization scheme and the theoretical outcomes.}
}
@article{WATANABE2022303,
title = {Deep two-way matrix reordering for relational data analysis},
journal = {Neural Networks},
volume = {146},
pages = {303-315},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.028},
url = {https://www.sciencedirect.com/science/article/pii/S089360802100469X},
author = {Chihiro Watanabe and Taiji Suzuki},
keywords = {Matrix reordering, Relational data analysis, Neural network, Visualization},
abstract = {Matrix reordering is a task to permute the rows and columns of a given observed matrix such that the resulting reordered matrix shows meaningful or interpretable structural patterns. Most existing matrix reordering techniques share the common processes of extracting some feature representations from an observed matrix in a predefined manner, and applying matrix reordering based on it. However, in some practical cases, we do not always have prior knowledge about the structural pattern of an observed matrix. To address this problem, we propose a new matrix reordering method, called deep two-way matrix reordering (DeepTMR), using a neural network model. The trained network can automatically extract nonlinear row/column features from an observed matrix, which can then be used for matrix reordering. Moreover, the proposed DeepTMR provides the denoised mean matrix of a given observed matrix as an output of the trained network. This denoised mean matrix can be used to visualize the global structure of the reordered observed matrix. We demonstrate the effectiveness of the proposed DeepTMR by applying it to both synthetic and practical datasets.}
}
@article{GENEVA2022272,
title = {Transformers for modeling physical systems},
journal = {Neural Networks},
volume = {146},
pages = {272-289},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004500},
author = {Nicholas Geneva and Nicholas Zabaras},
keywords = {Transformers, Deep learning, Self-attention, Physics, Koopman, Surrogate modeling},
abstract = {Transformers are widely used in natural language processing due to their ability to model longer-term dependencies in text. Although these models achieve state-of-the-art performance for many language related tasks, their applicability outside of the natural language processing field has been minimal. In this work, we propose the use of transformer models for the prediction of dynamical systems representative of physical phenomena. The use of Koopman based embeddings provides a unique and powerful method for projecting any dynamical system into a vector representation which can then be predicted by a transformer. The proposed model is able to accurately predict various dynamical systems and outperform classical methods that are commonly used in the scientific machine learning literature.11Code available at: https://github.com/zabaras/transformer-physx.}
}
@article{2023II,
title = {INN/ENNS/JNNS - Membership Applic. Form},
journal = {Neural Networks},
volume = {166},
pages = {II},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(23)00478-1},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023004781}
}
@article{SHEN2023337,
title = {Corrigendum to “ Domain-adaptive Message Passing Graph Neural Network” [Neural Netw. 164 (2023) 439–454]},
journal = {Neural Networks},
volume = {168},
pages = {337-338},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.09.026},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023005233},
author = {Xiao Shen and Shirui Pan and Kup-Sze Choi and Xi Zhou}
}
@article{LI2022183,
title = {Deep Rival Penalized Competitive Learning for low-resolution face recognition},
journal = {Neural Networks},
volume = {148},
pages = {183-193},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000090},
author = {Peiying Li and Shikui Tu and Lei Xu},
keywords = {Face recognition, Low resolution, RPCL},
abstract = {Current face recognition tasks are usually carried out on high-quality face images, but in reality, most face images are captured under unconstrained or poor conditions, e.g., by video surveillance. Existing methods are featured by learning data uncertainty to avoid overfitting the noise, or by adding margins to the angle or cosine space of the normalized softmax loss to penalize the target logit, which enforces intra-class compactness and inter-class discrepancy. In this paper, we propose a deep Rival Penalized Competitive Learning (RPCL) for deep face recognition in low-resolution (LR) images. Inspired by the idea of the RPCL, our method further enforces regulation on the rival logit, which is defined as the largest non-target logit for an input image. Different from existing methods that only consider penalization on the target logit, our method not only strengthens the learning towards the target label, but also enforces a reverse direction, i.e., becoming de-learning, away from the rival label. Comprehensive experiments demonstrate that our method improves the existing state-of-the-art methods to be very robust for LR face recognition.}
}
@article{VERBEKE2022256,
title = {Using top-down modulation to optimally balance shared versus separated task representations},
journal = {Neural Networks},
volume = {146},
pages = {256-271},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.030},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004718},
author = {Pieter Verbeke and Tom Verguts},
keywords = {Cognitive control, Modulation, Neural representations, Generalization},
abstract = {Human adaptive behavior requires continually learning and performing a wide variety of tasks, often with very little practice. To accomplish this, it is crucial to separate neural representations of different tasks in order to avoid interference. At the same time, sharing neural representations supports generalization and allows faster learning. Therefore, a crucial challenge is to find an optimal balance between shared versus separated representations. Typically, models of human cognition employ top-down modulatory signals to separate task representations, but there exist surprisingly little systematic computational investigations of how such modulation is best implemented. We identify and systematically evaluate two crucial features of modulatory signals. First, top-down input can be processed in an additive or multiplicative manner. Second, the modulatory signals can be adaptive (learned) or non-adaptive (random). We cross these two features, resulting in four modulation networks which are tested on a variety of input datasets and tasks with different degrees of stimulus-action mapping overlap. The multiplicative adaptive modulation network outperforms all other networks in terms of accuracy. Moreover, this network develops hidden units that optimally share representations between tasks. Specifically, different than the binary approach of currently popular latent state models, it exploits partial overlap between tasks.}
}
@article{2023ii,
title = {Editorial Board},
journal = {Neural Networks},
volume = {166},
pages = {ii},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(23)00482-3},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023004823}
}
@article{HAO202240,
title = {Possibilistic classification by support vector networks},
journal = {Neural Networks},
volume = {149},
pages = {40-56},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000405},
author = {Pei-Yi Hao and Jung-Hsien Chiang and Yu-De Chen},
keywords = {Fuzzy set theory, Fuzzy classifier, Possibility measure, Support vector machines (SVMs)},
abstract = {In many real-world classification problems, the available information is often uncertain. In order to effectively describe the inherent vagueness and improve the classification performance, this paper proposes a novel possibilistic classification algorithm using support vector machines (SVMs). Based on possibility theory, the proposed algorithm aims at finding a maximal-margin fuzzy hyperplane by solving a fuzzy mathematical optimization problem Moreover, the decision function of the proposed approach is generalized such that the values assigned to the data vectors fall within a specified range and indicate the membership grade of these data vectors in the positive class. The proposed algorithm retains the advantages of fuzzy set theory and SVM theory. The proposed approach is more robust for handling data corrupted by outliers. Moreover, the structural risk minimization principle of SVMs enables the proposed approach to effectively classify the unseen data. Furthermore, the proposed algorithm has additional advantage of using vagueness parameter v for controlling the bounds on fractions of support vectors and errors. The extensive experiments performed on benchmark datasets and real applications demonstrate that the proposed algorithm has satisfactory generalization accuracy and better describes the inherent vagueness in the given dataset.}
}
@article{WEN202223,
title = {Multi-dimensional conditional mutual information with application on the EEG signal analysis for spatial cognitive ability evaluation},
journal = {Neural Networks},
volume = {148},
pages = {23-36},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004834},
author = {Dong Wen and Rou Li and Mengmeng Jiang and Jingjing Li and Yijun Liu and Xianling Dong and M. Iqbal Saripan and Haiqing Song and Wei Han and Yanhong Zhou},
keywords = {Multi-dimensional conditional mutual information, Multi-spectral image, Spatial cognition, Task-state EEG signal, Coupling feature extraction},
abstract = {This study aims to explore an effective method to evaluate spatial cognitive ability, which can effectively extract and classify the feature of EEG signals collected from subjects participating in the virtual reality (VR) environment; and evaluate the training effect objectively and quantitatively to ensure the objectivity and accuracy of spatial cognition evaluation, according to the classification results. Therefore, a multi-dimensional conditional mutual information (MCMI) method is proposed, which could calculate the coupling strength of two channels considering the influence of other channels. The coupled characteristics of the multi-frequency combination were transformed into multi-spectral images, and the image data were classified employing the convolutional neural networks (CNN) model. The experimental results showed that the multi-spectral image transform features based on MCMI are better in classification than other methods, and among the classification results of six band combinations, the best classification accuracy of Beta1–Beta2–Gamma combination is 98.3%. The MCMI characteristics on the Beta1–Beta2–Gamma band combination can be a biological marker for the evaluation of spatial cognition. The proposed feature extraction method based on MCMI provides a new perspective for spatial cognitive ability assessment and analysis.}
}
@article{JIAO2022331,
title = {Cycle-consistent Adversarial Adaptation Network and its application to machine fault diagnosis},
journal = {Neural Networks},
volume = {145},
pages = {331-341},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004317},
author = {Jinyang Jiao and Jing Lin and Ming Zhao and Kaixuan Liang and Chuancang Ding},
keywords = {Generative adversarial learning, Domain adaptation, Deep learning, Machine, Fault diagnosis},
abstract = {Driven by industrial big data and intelligent manufacturing, deep learning approaches have flourished and yielded impressive achievements in the community of machine fault diagnosis. Nevertheless, current diagnosis models trained on a specific dataset seldom work well on other datasets due to the domain discrepancy. Recently, adversarial domain adaptation-based approaches have become an emerging and compelling tool to address this issue. Nonetheless, existing methods still have a shortcoming since they cannot guarantee sufficient feature similarity between the source domain and the target domain after adaptation, resulting in unguaranteed performance. To this end, a Cycle-consistent Adversarial Adaptation Network (CAAN) is advanced to realize more effective fault diagnosis of machinery. In CAAN, specifically, an adversarial game formed by the feature extractor and the domain discriminator is constructed to induce transferable feature learning. Meanwhile, the feature translators and discriminators between source and target domains are further designed to build a more comprehensive cycle-consistent generative adversarial constrain, which can more reliably ensure domain-invariant and class-separate characteristics of learned features. Extensive experiments constructed on three datasets from different mechanical systems demonstrate the effectiveness and superiority of CAAN.}
}
@article{OJHA202266,
title = {Backpropagation Neural Tree},
journal = {Neural Networks},
volume = {149},
pages = {66-83},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000363},
author = {Varun Ojha and Giuseppe Nicosia},
keywords = {Stochastic gradient descent, RMSprop, Backpropagation, Minimal architecture, Neural networks, Neural trees},
abstract = {We propose a novel algorithm called Backpropagation Neural Tree (BNeuralT), which is a stochastic computational dendritic tree. BNeuralT takes random repeated inputs through its leaves and imposes dendritic nonlinearities through its internal connections like a biological dendritic tree would do. Considering the dendritic-tree like plausible biological properties, BNeuralT is a single neuron neural tree model with its internal sub-trees resembling dendritic nonlinearities. BNeuralT algorithm produces an ad hoc neural tree which is trained using a stochastic gradient descent optimizer like gradient descent (GD), momentum GD, Nesterov accelerated GD, Adagrad, RMSprop, or Adam. BNeuralT training has two phases, each computed in a depth-first search manner: the forward pass computes neural tree’s output in a post-order traversal, while the error backpropagation during the backward pass is performed recursively in a pre-order traversal. A BNeuralT model can be considered a minimal subset of a neural network (NN), meaning it is a “thinned” NN whose complexity is lower than an ordinary NN. Our algorithm produces high-performing and parsimonious models balancing the complexity with descriptive ability on a wide variety of machine learning problems: classification, regression, and pattern recognition.}
}
@article{SUN2022155,
title = {Low-degree term first in ResNet, its variants and the whole neural network family},
journal = {Neural Networks},
volume = {148},
pages = {155-165},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000120},
author = {Tongfeng Sun and Shifei Ding and Lili Guo},
keywords = {ResNets, DenseNets, Shallow subnetwork first, Low-degree term first, Taylor expansion},
abstract = {To explain the working mechanism of ResNet and its variants, this paper proposes a novel argument of shallow subnetwork first (SSF), essentially low-degree term first (LDTF), which also applies to the whole neural network family. A neural network with shortcut connections behaves as an ensemble of a number of subnetworks of differing depths. Among the subnetworks, the shallow subnetworks are trained firstly, having great effects on the performance of the neural network. The shallow subnetworks roughly correspond to low-degree polynomials, while the deep subnetworks are opposite. Based on Taylor expansion, SSF is consistent with LDTF. ResNet is in line with Taylor expansion: shallow subnetworks are trained firstly to keep low-degree terms, avoiding overfitting; deep subnetworks try to maintain high-degree terms, ensuring high description capacity. Experiments on ResNets and DenseNets show that shallow subnetworks are trained firstly and play important roles in the training of the networks. The experiments also reveal the reason why DenseNets outperform ResNets: The subnetworks playing vital roles in the training of the former are shallower than those in the training of the latter. Furthermore, LDTF can also be used to explain the working mechanism of other ResNet variants (SE-ResNets and SK-ResNets), and the common phenomena occurring in many neural networks.}
}
@article{ZHOU2022152,
title = {Command-filter-based adaptive neural tracking control for a class of nonlinear MIMO state-constrained systems with input delay and saturation},
journal = {Neural Networks},
volume = {147},
pages = {152-162},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004792},
author = {Yuhao Zhou and Xin Wang and Rui Xu},
keywords = {Adaptive neural control, Barrier Lyapunov function, Auxiliary system, Command filtering backstepping, Input delay and saturation},
abstract = {This paper investigates the problem of adaptive tracking control for a class of nonlinear multi-input and multi-output (MIMO) state-constrained systems with input delay and saturation. During the process of the control scheme, neural network is employed to approximate the unknown nonlinear uncertainties and the appropriate barrier Lyapunov function is introduced to prevent violation of the constraint. In addition, for the issue of input saturation with time delay, a smooth non-affine approximate function and a novel auxiliary system are utilized, respectively. Moreover, adaptive neural tracking control is developed by combining the command filtering backstepping approach, which effectively avoids the explosion of differentiation and reduces the computation burden. The introduced filtering error compensating system brings a significant improvement for the system tracking performance. Finally, the simulation result is presented to verify the feasibility of the proposed strategy.}
}
@article{LIU2022300,
title = {Convergence analysis of AdaBound with relaxed bound functions for non-convex optimization},
journal = {Neural Networks},
volume = {145},
pages = {300-307},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.10.026},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004275},
author = {Jinlan Liu and Jun Kong and Dongpo Xu and Miao Qi and Yinghua Lu},
keywords = {Non-convex optimization, Individual convergence, AdaBound, Bound functions, Deep learning},
abstract = {Clipping on learning rates in Adam leads to an effective stochastic algorithm—AdaBound. In spite of its effectiveness in practice, convergence analysis of AdaBound has not been fully explored, especially for non-convex optimization. To this end, we address the convergence of the last individual output of AdaBound for non-convex stochastic optimization problems, which is called individual convergence. We prove that, with the iteration of the AdaBound, the cost function converges to a finite value and the corresponding gradient converges to zero. The novelty of this proof is that the convergence conditions on the bound functions and momentum factors are much more relaxed than the existing results, especially when we remove the monotonicity and convergence of the bound functions, and only keep their boundedness. The momentum factors can be fixed to be constant, without the restriction of monotonically decreasing. This provides a new perspective on understanding the bound functions and momentum factors of AdaBound. At last, numerical experiments are provided to corroborate our theory and show that the convergence of AdaBound extends to more general bound functions.}
}
@article{YANG20221,
title = {Multi-layer information fusion based on graph convolutional network for knowledge-driven herb recommendation},
journal = {Neural Networks},
volume = {146},
pages = {1-10},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S089360802100438X},
author = {Yun Yang and Yulong Rao and Minghao Yu and Yan Kang},
keywords = {Traditional Chinese Medicine, Herb recommendation, Graph convolutional network, Representation learning},
abstract = {Prescription of Traditional Chinese Medicine (TCM) is a precious treasure accumulated in the long-term development of TCM. Artificial intelligence (AI) technology is used to build herb recommendation models to deeply understand regularities in prescriptions, which is of great significance to clinical application of TCM and discovery of new prescriptions. Most of herb recommendation models constructed in the past ignored the nature information of herbs, and most of them used statistical models based on bag-of-words for herb recommendation, which makes it difficult for the model to perceive the complex correlation between symptoms and herbs. In this paper, we introduce the properties of herbs as additional auxiliary information by constructing herb knowledge graph, and propose a graph convolution model with multi-layer information fusion to obtain symptom feature representations and herb feature representations with rich information and less noise. We apply the proposed model to the TCM prescription dataset, and the experiment results show that our model outperforms the baseline models in terms of Precision@5 by 6.2%, Recall@5 by 16.0% and F1-Score@5 by 12.0%.}
}
@article{TU2022350,
title = {Deep semi-supervised learning via dynamic anchor graph embedding in latent space},
journal = {Neural Networks},
volume = {146},
pages = {350-360},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.026},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004676},
author = {Enmei Tu and Zihao Wang and Jie Yang and Nikola Kasabov},
keywords = {Semi-supervised learning, Dynamic Anchor Graph Embedding, Grid-structured/graph-structured data, Image/text classification},
abstract = {Recently, deep semi-supervised graph embedding learning has drawn much attention for its appealing performance on the data with a pre-specified graph structure, which could be predefined or empirically constructed based on given data samples. However, the pre-specified graphs often contain considerable noisy/inaccurate connections and have a huge size for large datasets. Most existing embedding algorithms just take the graph off the shelf during the whole training stage and thus are easy to be misled by the inaccurate graph edges, as well as may result in large model size. In this paper, we attempt to address these issues by proposing a novel deep semi-supervised algorithm for simultaneous graph embedding and node classification, utilizing dynamic graph learning in neural network hidden layer space. Particularly, we construct an anchor graph to summarize the whole dataset using the hidden layer features of a consistency-constrained network. The anchor graph is used for sampling node neighborhood context, which is then presented together with node labels as contextual information to train an embedding network. The outputs of the consistency network and the embedding networks are finally concatenated together to pass a softmax function to perform node classification. The two networks are optimized jointly using both labeled and unlabeled data to minimize a single semi-supervised objective function, including a cross-entropy loss, a consistency loss and an embedding loss. Extensive experimental results on popular image and text datasets have shown that the proposed method is able to improve the performance of existing graph embedding and node classification methods, and outperform many state-of-the-art approaches on both types of datasets.}
}
@article{ZENG2022136,
title = {Fully corrective gradient boosting with squared hinge: Fast learning rates and early stopping},
journal = {Neural Networks},
volume = {147},
pages = {136-151},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004950},
author = {Jinshan Zeng and Min Zhang and Shao-Bo Lin},
keywords = {Boosting, Learning theory, Squared hinge, Fully corrective greedy, Early stopping},
abstract = {In this paper, we propose an efficient boosting method with theoretical guarantees for binary classification. There are three key ingredients of the proposed boosting method: a fully corrective greedy (FCG) update, a differentiable squared hinge (also called truncated quadratic) loss function, and an efficient alternating direction method of multipliers (ADMM) solver. Compared with traditional boosting methods, on one hand, the FCG update accelerates the numerical convergence rate, and on the other hand, the squared hinge loss inherits the robustness of the hinge loss for classification and maintains the theoretical benefits of the square loss in regression. The ADMM solver with guaranteed fast convergence then provides an efficient implementation for the proposed boosting method. We conduct both theoretical analysis and numerical verification to show the outperformance of the proposed method. Theoretically, a fast learning rate of order O((m/logm)−1/2) is proved under certain standard assumptions, where m is the size of sample set. Numerically, a series of toy simulations and real data experiments are carried out to verify the developed theory.}
}
@article{ZIRAKI2022174,
title = {Multiple-view flexible semi-supervised classification through consistent graph construction and label propagation},
journal = {Neural Networks},
volume = {146},
pages = {174-180},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004433},
author = {Najmeh Ziraki and Fadi Dornaika and Alireza Bosaghzadeh},
keywords = {Multi-view semi-supervised classification, Information fusion, Graph-based data smoothness, Graph construction},
abstract = {Graph construction plays an essential role in graph-based label propagation since graphs give some information on the structure of the data manifold. While most graph construction methods rely on predefined distance calculation, recent algorithms merge the task of label propagation and graph construction in a single process. Moreover, the use of several descriptors is proved to outperform a single descriptor in representing the relation between the nodes. In this article, we propose a Multiple-View Consistent Graph construction and Label propagation algorithm (MVCGL) that simultaneously constructs a consistent graph based on several descriptors and performs label propagation over unlabeled samples. Furthermore, it provides a mapping function from the feature space to the label space with which we estimate the label of unseen samples via a linear projection. The constructed graph does not rely on a predefined similarity function and exploits data and label smoothness. Experiments conducted on three face and one handwritten digit databases show that the proposed method can gain better performance compared to other graph construction and label propagation methods.}
}
@article{2023II,
title = {INN/ENNS/JNNS - Membership Applic. Form},
journal = {Neural Networks},
volume = {167},
pages = {II},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(23)00546-4},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023005464}
}
@article{2023ii,
title = {Editorial Board},
journal = {Neural Networks},
volume = {167},
pages = {ii},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(23)00550-6},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023005506}
}
@article{XIAN2022129,
title = {Dual Global Enhanced Transformer for image captioning},
journal = {Neural Networks},
volume = {148},
pages = {129-141},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000119},
author = {Tiantao Xian and Zhixin Li and Canlong Zhang and Huifang Ma},
keywords = {Image captioning, Transformer, Global information, Visual attention, Reinforcement learning},
abstract = {Transformer-based architectures have shown great success in image captioning, where self-attention module can model source and target interaction (e.g., object-to-object, object-to-word, word-to-word). However, the global information is not explicitly considered in the attention weight calculation, which is essential to understand the scene content. In this paper, we propose Dual Global Enhanced Transformer (DGET) to incorporate global information in the encoding and decoding stages. Concretely, in DGET, we regard the grid feature as the visual global information and adaptively fuse it into region features in each layer by a novel Global Enhanced Encoder (GEE). During decoding, we proposed Global Enhanced Decoder (GED) to explicitly utilize the textual global information. First, we devise the context encoder to encode the existing caption generated by classic captioner as a context vector. Then, we use the context vector to guide the decoder to generate accurate words at each time step. To validate our model, we conduct extensive experiments on the MS COCO image captioning dataset and achieve superior performance over many state-of-the-art methods.}
}
@article{POLI202357,
title = {Incorrect Application of Yilmaz–Poli (2022) Initialisation Method in dePater–Mitici 2023 paper entitled “A mathematical framework for improved weight initialization of neural networks using Lagrange multipliers”},
journal = {Neural Networks},
volume = {168},
pages = {57-58},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.09.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023005105},
author = {Riccardo Poli and Ahmet Yilmaz},
abstract = {In this letter to the editor we report on a methodological error made in the article entitled “A mathematical framework for improved weight initialization of neural networks using Lagrange multipliers” by dePater and Mitici recently appeared in this journal. The error relates to the incorrect application of a weight initialisation method we derived, published last year in this same journal.}
}
@article{YANG2022290,
title = {Efficient correntropy-based multi-view clustering with anchor graph embedding},
journal = {Neural Networks},
volume = {146},
pages = {290-302},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.027},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004688},
author = {Ben Yang and Xuetao Zhang and Badong Chen and Feiping Nie and Zhiping Lin and Zhixiong Nan},
keywords = {Multi-view clustering, Correntropy, Anchor graph, Matrix factorization},
abstract = {Although multi-view clustering has received widespread attention due to its far superior performance to single-view clustering, it still faces the following issues: (1) high computational cost, considering the introduction of multi-view information, reduces the clustering efficiency greatly; (2) complex noises and outliers, existed in real-world data, pose a huge challenge to the robustness of clustering algorithms. Currently, how to increase the efficiency and robustness has become two important issues of multi-view clustering. To cope with the above issues, an efficient correntropy-based multi-view clustering algorithm (ECMC) is proposed in this paper, which can not only improve clustering efficiency by constructing embedded anchor graph and utilizing nonnegative matrix factorization (NMF), but also enhance the robustness by exploring correntropy to suppress various noises and outliers. To further improve clustering efficiency, one of the factors of NMF is constrained to be an indicator matrix instead of a traditional non-negative matrix, so that the categories of samples can be obtained directly without any extra operation. Subsequently, a novel half-quadratic-based strategy is proposed to optimize the non-convex objective function of ECMC. Finally, extensive experiments on eight real-world datasets and eighteen noisy datasets show that ECMC can guarantee faster speed and better robustness than other state-of-the-art multi-view clustering algorithms.}
}
@article{ZHAO202295,
title = {Deep Bayesian Unsupervised Lifelong Learning},
journal = {Neural Networks},
volume = {149},
pages = {95-106},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S089360802200034X},
author = {Tingting Zhao and Zifeng Wang and Aria Masoomi and Jennifer Dy},
keywords = {Unsupervised Lifelong Learning, Bayesian Learning, Deep generative models, Deep Neural Networks, Sufficient statistics},
abstract = {Lifelong Learning (LL) refers to the ability to continually learn and solve new problems with incremental available information over time while retaining previous knowledge. Much attention has been given lately to Supervised Lifelong Learning (SLL) with a stream of labelled data. In contrast, we focus on resolving challenges in Unsupervised Lifelong Learning (ULL) with streaming unlabelled data when the data distribution and the unknown class labels evolve over time. Bayesian framework is natural to incorporate past knowledge and sequentially update the belief with new data. We develop a fully Bayesian inference framework for ULL with a novel end-to-end Deep Bayesian Unsupervised Lifelong Learning (DBULL) algorithm, which can progressively discover new clusters without forgetting the past with unlabelled data while learning latent representations. To efficiently maintain past knowledge, we develop a novel knowledge preservation mechanism via sufficient statistics of the latent representation for raw data. To detect the potential new clusters on the fly, we develop an automatic cluster discovery and redundancy removal strategy in our inference inspired by Nonparametric Bayesian statistics techniques. We demonstrate the effectiveness of our approach using image and text corpora benchmark datasets in both LL and batch settings.}
}
@article{HE2022113,
title = {Towards efficient network compression via Few-Shot Slimming},
journal = {Neural Networks},
volume = {147},
pages = {113-125},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004846},
author = {Junjie He and Yinzhang Ding and Ming Zhang and Dongxiao Li},
keywords = {Knowledge distillation, Network compression, Few-shot compression},
abstract = {While previous network compression methods achieve great success, most of them rely on the abundant training data which is, unfortunately, often unavailable in practice due to some reasons, e.g., privacy issues, storage constraints, and transmission limitations. A promising way to solve this problem is to perform compression with a few unlabeled data. Proceeding along this way, we propose a novel few-shot network compression framework named Few-Shot Slimming (FSS). FSS follows the student/teacher paradigm, and contains two steps: (1) construct the student by inheriting principal feature maps from the teacher; (2) refine the student feature representation by knowledge distillation with an enhanced mixing data augmentation method called GridMix. Specifically, in the first step, we employ normalized cross correlation to perform the principal feature analysis, and then theoretically construct a new indicator to select the most informative feature maps from the teacher for the student. The indicator is based on the variances of feature maps which can efficiently quantitate the information richness of the input feature maps in a feature-agnostic manner. In the second step, we perform the knowledge distillation for the initialized student in first step with a novel grid-based mixing data augmentation technique which greatly extends the limited sample dataset. In this way, the student is able to refine its feature representation and achieves a better result. Extensive experiments on multiple benchmarks demonstrate the state-of-the-art performance of FSS. For example, by using 0.2% label-free data of full training set, FSS yields a 60% FLOPs reduction for DenseNet-40 on CIFAR-10 with only a loss of 0.8% in top-1 accuracy, achieving a result on par with that obtained by the conventional full-data methods.}
}
@article{SEKER202222,
title = {Imitation and mirror systems in robots through Deep Modality Blending Networks},
journal = {Neural Networks},
volume = {146},
pages = {22-35},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004329},
author = {M. Yunus Seker and Alper Ahmetoglu and Yukie Nagai and Minoru Asada and Erhan Oztop and Emre Ugur},
keywords = {Robot learning, Imitation learning, Representation learning, Multimodal learning},
abstract = {Learning to interact with the environment not only empowers the agent with manipulation capability but also generates information to facilitate building of action understanding and imitation capabilities. This seems to be a strategy adopted by biological systems, in particular primates, as evidenced by the existence of mirror neurons that seem to be involved in multi-modal action understanding. How to benefit from the interaction experience of the robots to enable understanding actions and goals of other agents is still a challenging question. In this study, we propose a novel method, deep modality blending networks (DMBN), that creates a common latent space from multi-modal experience of a robot by blending multi-modal signals with a stochastic weighting mechanism. We show for the first time that deep learning, when combined with a novel modality blending scheme, can facilitate action recognition and produce structures to sustain anatomical and effect-based imitation capabilities. Our proposed system, which is based on conditional neural processes, can be conditioned on any desired sensory/motor value at any time step, and can generate a complete multi-modal trajectory consistent with the desired conditioning in one-shot by querying the network for all the sampled time points in parallel avoiding the accumulation of prediction errors. Based on simulation experiments with an arm-gripper robot and an RGB camera, we showed that DMBN could make accurate predictions about any missing modality (camera or joint angles) given the available ones outperforming recent multimodal variational autoencoder models in terms of long-horizon high-dimensional trajectory predictions. We further showed that given desired images from different perspectives, i.e. images generated by the observation of other robots placed on different sides of the table, our system could generate image and joint angle sequences that correspond to either anatomical or effect-based imitation behavior. To achieve this mirror-like behavior, our system does not perform a pixel-based template matching but rather benefits from and relies on the common latent space constructed by using both joint and image modalities, as shown by additional experiments. Moreover, we showed that mirror learning (in our system) does not only depend on visual experience and cannot be achieved without proprioceptive experience. Our experiments showed that out of ten training scenarios with different initial configurations, the proposed DMBN model could achieve mirror learning in all of the cases where the model that only uses visual information failed in half of them. Overall, the proposed DMBN architecture not only serves as a computational model for sustaining mirror neuron-like capabilities, but also stands as a powerful machine learning architecture for high-dimensional multi-modal temporal data with robust retrieval capabilities operating with partial information in one or multiple modalities.}
}
@article{LI2022176,
title = {Cross-modal distribution alignment embedding network for generalized zero-shot learning},
journal = {Neural Networks},
volume = {148},
pages = {176-182},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000077},
author = {Qin Li and Mingzhen Hou and Hong Lai and Ming Yang},
keywords = {Generalized zero-shot learning, Weakly-supervised learning, Image classification},
abstract = {Many approaches in generalized zero-shot learning (GZSL) rely on cross-modal mapping between the image feature space and the class embedding space, which achieves knowledge transfer from seen to unseen classes. However, these two spaces are completely different space and their manifolds are inconsistent, the existing methods suffer from highly overlapped semantic description of different classes, as in GZSL tasks unseen classes can be easily misclassified into seen classes. To handle these problems, we adopt a novel semantic embedding network which helps to encode more discriminative information from initial semantic attributes to semantic embeddings in visual space. Meanwhile, a distribution alignment constraint is adopted to help keep the distribution of the learned semantic embeddings consistent with the distribution of real image features. Moreover, an auxiliary classifier is adopted to strengthen the quality of the learned semantic embeddings. Finally, a relation network is used to classify the unseen images by computing the relation scores between the semantic embeddings and image features, which is much more flexible than the fixed distance metric functions. Experimental results demonstrate that our proposed method is superior to other state-of-the-arts.}
}
@article{UDHAYAKUMAR2022319,
title = {Fractional-order discontinuous systems with indefinite LKFs: An application to fractional-order neural networks with time delays},
journal = {Neural Networks},
volume = {145},
pages = {319-330},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.10.027},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004287},
author = {K. Udhayakumar and Fathalla A. Rihan and R. Rakkiyappan and Jinde Cao},
keywords = {Fixed-time synchronization, Signed neural networks, Lyapunov-Krasovskii functional, Fractional-order, Discontinuous activations},
abstract = {In this article, we discuss bipartite fixed-time synchronization for fractional-order signed neural networks with discontinuous activation patterns. The Filippov multi-map is used to convert the fixed-time stability of the fractional-order general solution into the zero solution of the fractional-order differential inclusions. On the Caputo fractional-order derivative, Lyapunov-Krasovskii functional is proved to possess the indefinite fractional derivatives for fixed-time stability of fragmentary discontinuous systems. Furthermore, the fixed-time stability of the fractional-order discontinuous system is achieved as well as an estimate of the new settling time.. The discontinuous controller is designed for the delayed fractional-order discontinuous signed neural networks with antagonistic interactions and new conditions for permanent fixed-time synchronization of these networks with antagonistic interactions are also provided, as well as the settling time for permanent fixed-time synchronization. Two numerical simulation results are presented to demonstrate the effectiveness of the main results}
}
@article{KORZENIEWSKA2022204,
title = {Significance of event related causality (ERC) in eloquent neural networks},
journal = {Neural Networks},
volume = {149},
pages = {204-216},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000351},
author = {Anna Korzeniewska and Takumi Mitsuhashi and Yujing Wang and Eishi Asano and Piotr J. Franaszczuk and Nathan E. Crone},
keywords = {Neural networks interactions, Multivariate autoregressive model, Granger causality, Short-time direct directed transfer function, Information flow, Time–frequency analysis},
abstract = {Neural activity emerges and propagates swiftly between brain areas. Investigation of these transient large-scale flows requires sophisticated statistical models. We present a method for assessing the statistical confidence of event-related neural propagation. Furthermore, we propose a criterion for statistical model selection, based on both goodness of fit and width of confidence intervals. We show that event-related causality (ERC) with two-dimensional (2D) moving average, is an efficient estimator of task-related neural propagation and that it can be used to determine how different cognitive task demands affect the strength and directionality of neural propagation across human cortical networks. Using electrodes surgically implanted on the surface of the brain for clinical testing prior to epilepsy surgery, we recorded electrocorticographic (ECoG) signals as subjects performed three naming tasks: naming of ambiguous and unambiguous visual objects, and as a contrast, naming to auditory description. ERC revealed robust and statistically significant patterns of high gamma activity propagation, consistent with models of visually and auditorily cued word production. Interestingly, ambiguous visual stimuli elicited more robust propagation from visual to auditory cortices relative to unambiguous stimuli, whereas naming to auditory description elicited propagation in the opposite direction, consistent with recruitment of modalities other than those of the stimulus during object recognition and naming. The new method introduced here is uniquely suitable to both research and clinical applications and can be used to estimate the statistical significance of neural propagation for both cognitive neuroscientific studies and functional brain mapping prior to resective surgery for epilepsy and brain tumors.}
}
@article{SARVANI2022186,
title = {HRel: Filter pruning based on High Relevance between activation maps and class labels},
journal = {Neural Networks},
volume = {147},
pages = {186-197},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004962},
author = {C.H. Sarvani and Mrinmoy Ghorai and Shiv Ram Dubey and S.H. Shabbeer Basha},
keywords = {Convolutional Neural Networks, Information Bottleneck theory, Filter pruning, Activation maps, Mutual information, Entropy},
abstract = {This paper proposes an Information Bottleneck theory based filter pruning method that uses a statistical measure called Mutual Information (MI). The MI between filters and class labels, also called Relevance, is computed using the filter’s activation maps and the annotations. The filters having High Relevance (HRel) are considered to be more important. Consequently, the least important filters, which have lower Mutual Information with the class labels, are pruned. Unlike the existing MI based pruning methods, the proposed method determines the significance of the filters purely based on their corresponding activation map’s relationship with the class labels. Architectures such as LeNet-5, VGG-16, ResNet-56, ResNet-110 and ResNet-50 are utilized to demonstrate the efficacy of the proposed pruning method over MNIST, CIFAR-10 and ImageNet datasets. The proposed method shows the state-of-the-art pruning results for LeNet-5, VGG-16, ResNet-56, ResNet-110 and ResNet-50 architectures. In the experiments, we prune 97.98%, 84.85%, 76.89%, 76.95%, and 63.99% of Floating Point Operation (FLOP)s from LeNet-5, VGG-16, ResNet-56, ResNet-110, and ResNet-50 respectively. The proposed HRel pruning method outperforms recent state-of-the-art filter pruning methods. Even after pruning the filters from convolutional layers of LeNet-5 drastically (i.e., from 20, 50 to 2, 3, respectively), only a small accuracy drop of 0.52% is observed. Notably, for VGG-16, 94.98% parameters are reduced, only with a drop of 0.36% in top-1 accuracy. ResNet-50 has shown a 1.17% drop in the top-5 accuracy after pruning 66.42% of the FLOPs. In addition to pruning, the Information Plane dynamics of Information Bottleneck theory is analyzed for various Convolutional Neural Network architectures with the effect of pruning. The code is available at https://github.com/sarvanichinthapalli/HRel.}
}
@article{LI202285,
title = {Functional connectivity inference from fMRI data using multivariate information measures},
journal = {Neural Networks},
volume = {146},
pages = {85-97},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004445},
author = {Qiang Li},
keywords = {Information transmission, Mutual information, Interaction information, Total correlation, Multivariate distribution, Functional connectivity},
abstract = {Shannon’s entropy or an extension of Shannon’s entropy can be used to quantify information transmission between or among variables. Mutual information is the pair-wise information that captures nonlinear relationships between variables. It is more robust than linear correlation methods. Beyond mutual information, two generalizations are defined for multivariate distributions: interaction information or co-information and total correlation or multi-mutual information. In comparison to mutual information, interaction information and total correlation are underutilized and poorly studied in applied neuroscience research. Quantifying information flow between brain regions is not explicitly explained in neuroscience by interaction information and total correlation. This article aims to clarify the distinctions between the neuroscience concepts of mutual information, interaction information, and total correlation. Additionally, we proposed a novel method for determining the interaction information between three variables using total correlation and conditional mutual information. On the other hand, how to apply it properly in practical situations. We supplied both simulation experiments and real neural studies to estimate functional connectivity in the brain with the above three higher-order information-theoretic approaches. In order to capture redundancy information for multivariate variables, we discovered that interaction information and total correlation were both robust, and it could be able to capture both well-known and yet-to-be-discovered functional brain connections.}
}
@article{ZHEN20231,
title = {RASP: Regularization-based Amplitude Saliency Pruning},
journal = {Neural Networks},
volume = {168},
pages = {1-13},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023004963},
author = {Chenghui Zhen and Weiwei Zhang and Jian Mo and Ming Ji and Hongbo Zhou and Jianqing Zhu},
keywords = {Model compression, Filter pruning, Pruning criterion, Regularization},
abstract = {Due to the prevalent data-dependent nature of existing pruning criteria, norm criteria with data independence play a crucial role in filter pruning criteria, providing promising prospects for deploying deep neural networks on resource-constrained devices. However, norm criteria based on amplitude measurements have long posed challenges in terms of theoretical feasibility. Existing methods rely on data-derived information such as derivatives to establish reasonable pruning standards. Nonetheless, achieving quantitative analysis of the “smaller-norm-less-important” notion remains elusive within the norm criterion context. To address the need for data independence and theoretical feasibility, we conducted saliency analysis on filters and proposed a regularization-based amplitude saliency pruning criterion (RASP). This amplitude saliency not only attains data independence but also establishes norm criteria for usage guidelines. Furthermore, we further investigated the amplitude saliency, addressing the issues of data dependency in model evaluation and inter-class filter selection. We introduced model saliency and an adaptive parameter group lasso (AGL) regularization approach sensitive to different layers. Theoretically, we thoroughly analyzed the feasibility of amplitude saliency and employed quantitative saliency analysis to validate the advantages of our method over previous approaches. Experimentally, conducted on the CIFAR-10 and ImageNet image classification benchmarks, we extensively validated the improved top-level performance of our method compared to previous methods. Even when the pruned model has the same or even smaller number of FLOP, our method can achieve equivalent or higher model accuracy. Notably, in our ImageNet experiment, RASP achieved a 51.9% reduction in FLOPs while maintaining an accuracy of 76.19% on ResNet-50.}
}
@article{DONG2022146,
title = {Zero-Hopf Bifurcation of a memristive synaptic Hopfield neural network with time delay},
journal = {Neural Networks},
volume = {149},
pages = {146-156},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000429},
author = {Tao Dong and Xiaomei Gong and Tingwen Huang},
keywords = {Memristive Hopfield neural network (MHNN), Memristive synapse, Zero-Hopf bifurcation, Stability, Periodic solutions},
abstract = {This paper proposes a novel memristive synaptic Hopfield neural network (MHNN) with time delay by using a memristor synapse to simulate the electromagnetic induced current caused by the membrane potential difference between two adjacent neurons. First, some sufficient conditions of zero bifurcation and zero-Hopf bifurcation are obtained by choosing time delay and coupling strength of memristor as bifurcation parameters. Then, the third-order normal form of zero-Hopf bifurcation is obtained. By analyzing the obtained normal form, six dynamic regions are found on the plane with coupling strength of memristor and time delay as abscissa and ordinate. There are some interesting dynamics in these areas, i.e., the coupling strength of memristor can affect the number and dynamics of system equilibrium, time delay can contribute to both trivial equilibrium and non-trivial equilibrium losing stability and generating periodic solutions.}
}
@article{WANG2022111,
title = {Joint learning adaptive metric and optimal classification hyperplane},
journal = {Neural Networks},
volume = {148},
pages = {111-120},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000028},
author = {Yidan Wang and Liming Yang},
keywords = {Metric learning, Optimal classification hyperplane, Maximum margin classification, Correntropy, Half quadratic optimization algorithm},
abstract = {Metric learning has attracted a lot of interest in classification tasks due to its efficient performance. Most traditional metric learning methods are based on k-nearest neighbors (kNN) classifiers to make decisions, while the choice k affects the generalization. In this work, we propose an end-to-end metric learning framework. Specifically, a new linear metric learning (LMML) is first proposed to jointly learn adaptive metrics and the optimal classification hyperplanes, where dissimilar samples are separated by maximizing classification margin. Then a nonlinear metric learning model (called RLMML) is developed based on a bound nonlinear kernel function to extend LMML. The non-convexity of the proposed models makes them difficult to optimize. The half-quadratic optimization algorithms are developed to solve iteratively the problems, by which the optimal classification hyperplane and adaptive metric are alternatively optimized. Moreover, the resulting algorithms are proved to be convergent theoretically. Numerical experiments on different types of data sets show the effectiveness of the proposed algorithms. Finally, the Wilcoxon test shows also the feasibility and effectiveness of the proposed models.}
}
@article{CHEN202253,
title = {UTRAD: Anomaly detection and localization with U-Transformer},
journal = {Neural Networks},
volume = {147},
pages = {53-62},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004810},
author = {Liyang Chen and Zhiyuan You and Nian Zhang and Juntong Xi and Xinyi Le},
keywords = {Anomaly detection, Image transformer, One-class learning},
abstract = {Anomaly detection is an active research field in industrial defect detection and medical disease detection. However, previous anomaly detection works suffer from unstable training, or non-universal criteria of evaluating feature distribution. In this paper, we introduce UTRAD, a U-TRansformer based Anomaly Detection framework. Deep pre-trained features are regarded as dispersed word tokens, and represented with transformer-based autoencoders. With reconstruction on more informative feature distribution instead of raw images, we achieve a more stable training process and a more precise anomaly detection and localization result. In addition, our proposed UTRAD has a multi-scale pyramidal hierarchy with skip connections that help detect both multi-scale structural and non-structural anomalies. As attention layers are decomposed to multi-level patches, UTRAD significantly reduces the computational cost and memory usage compared with the vanilla transformer. Experiments on industrial dataset MVtec AD and medical datasets Retinal-OCT, Brain-MRI, Head-CT have been conducted. Our proposed UTRAD out-performs all other state-of-the-art methods in the above datasets. Code released at https://github.com/gordon-chenmo/UTRAD.}
}
@article{XIU202257,
title = {Synchronization issue of coupled neural networks based on flexible impulse control},
journal = {Neural Networks},
volume = {149},
pages = {57-65},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000302},
author = {Ruihong Xiu and Wei Zhang and Zichuan Zhou},
keywords = {Global exponential synchronization, Average impulsive interval, Average impulsive delay, Neural networks},
abstract = {The global exponential synchronization issue of coupled neural networks with time-delayed impulses is investigated in this paper. On the basis of the characteristics of coupled neural networks and theorems, we have built a novel coupled systems model. In order to fit the real situation, the impulses are flexible and it can beyond the impulsive interval under certain conditions in this paper. Therefore, our results are less restrictive and more practical compared to existing research. Besides, by using average impulsive delay (AID) and average impulsive interval (AII), we investigate two different effects of impulses on synchronization respectively and get a few adequate conditions for different types of synchronization. Finally, there are two examples of numerical simulations presented to illustrate the efficiency of the conclusions.}
}
@article{FUJITA202210,
title = {Associative anticipatory learning and control of the cerebellar cortex based on the spike-timing-dependent plasticity of the parallel fiber-Purkinje cell synapses},
journal = {Neural Networks},
volume = {147},
pages = {10-24},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004779},
author = {Masahiko Fujita},
keywords = {Cerebellum, Motor learning, Time delay, Anticipation, Prediction, Inverted pendulum},
abstract = {Time delays are inevitable in the neural processing of sensorimotor systems; small delays can cause severe damage to movement accuracy and stability. It is strongly suggested that the cerebellum compensates for delays in neural signal processing and performs predictive control. Neural computational theories have explored concepts of the internal models of control objects—believed to avoid delays by providing internal feedback information—although there has been no clear relevance to neural processing. The timing-dependent plasticity of parallel fiber-Purkinje cell synapses is well known. The long-term depression of the synapse is observed when parallel fiber activation precedes climbing fiber activation within −50–300 ms, and is the greatest within 50–200 ms. This paper presents a theory that this temporal difference of 50–200 ms is the basis for an associative anticipation of as many milliseconds. Associative learning can theoretically connect an input signal to a desired signal; therefore, a 50–200 ms earlier input signal can be connected to a desired output signal through temporary asymmetric plasticity. After learning is completed, an input signal generates a desired output signal that appears 50–200 ms later. For the associative learning of temporally continuous signals, this study integrates the universal function approximation capability of the cerebellar cortex model and temporally asymmetric synaptic plasticity to create the theory of associative anticipatory learning of the cerebellum. The effective motor control of this learning is demonstrated by adaptively stabilizing an inverted pendulum with a delay similar to that done by humans.}
}
@article{MANOMAISAOWAPAK2022157,
title = {Joint learning of multiple Granger causal networks via non-convex regularizations: Inference of group-level brain connectivity},
journal = {Neural Networks},
volume = {149},
pages = {157-171},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000387},
author = {Parinthorn Manomaisaowapak and Jitkomut Songsiri},
keywords = {Granger causality, Effective brain connectivity, Non-convex penalty, Composite penalty},
abstract = {This paper considers joint learning of multiple sparse Granger graphical models to discover underlying common and differential Granger causality (GC) structures across multiple time series. This can be applied to drawing group-level brain connectivity inferences from a homogeneous group of subjects or discovering network differences among groups of signals collected under heterogeneous conditions. By recognizing that the GC of a single multivariate time series can be characterized by common zeros of vector autoregressive (VAR) lag coefficients, a group sparse prior is included in joint regularized least-squares estimations of multiple VAR models. Group-norm regularizations based on group- and fused-lasso penalties encourage a decomposition of multiple networks into a common GC structure, with other remaining parts defined in individual-specific networks. Prior information about sparseness and sparsity patterns of desired GC networks are incorporated as relative weights, while a non-convex group norm in the penalty is proposed to enhance the accuracy of network estimation in low-sample settings. Extensive numerical results on simulations illustrated our method’s improvements over existing sparse estimation approaches on GC network sparsity recovery. Our methods were also applied to available resting-state fMRI time series from the ADHD-200 data sets to learn the differences of causality mechanisms, called effective brain connectivity, between adolescents with ADHD and typically developing children. Our analysis revealed that parts of the causality differences between the two groups often resided in the orbitofrontal region and areas associated with the limbic system, which agreed with clinical findings and data-driven results in previous studies.}
}
@article{PALATNIKDESOUSA20221,
title = {Evolved explainable classifications for lymph node metastases},
journal = {Neural Networks},
volume = {148},
pages = {1-12},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.014},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004937},
author = {Iam {Palatnik de Sousa} and Marley M.B.R. Vellasco and Eduardo {Costa da Silva}},
keywords = {Artificial intelligence, Convolutional Neural Networks, Explainable AI, Multi-objective genetic algorithms},
abstract = {A novel evolutionary approach for Explainable Artificial Intelligence is presented: the “Evolved Explanations” model (EvEx). This methodology combines Local Interpretable Model Agnostic Explanations (LIME) with Multi-Objective Genetic Algorithms to allow for automated segmentation parameter tuning in image classification tasks. In this case, the dataset studied is Patch-Camelyon, comprised of patches from pathology whole slide images. A publicly available Convolutional Neural Network (CNN) was trained on this dataset to provide a binary classification for presence/absence of lymph node metastatic tissue. In turn, the classifications are explained by means of evolving segmentations, seeking to optimize three evaluation goals simultaneously. The final explanation is computed as the mean of all explanations generated by Pareto front individuals, evolved by the developed genetic algorithm. To enhance reproducibility and traceability of the explanations, each of them was generated from several different seeds, randomly chosen. The observed results show remarkable agreement between different seeds. Despite the stochastic nature of LIME explanations, regions of high explanation weights proved to have good agreement in the heat maps, as computed by pixel-wise relative standard deviations. The found heat maps coincide with expert medical segmentations, which demonstrates that this methodology can find high quality explanations (according to the evaluation metrics), with the novel advantage of automated parameter fine tuning. These results give additional insight into the inner workings of neural network black box decision making for medical data.}
}
@article{BLAKSETH2022181,
title = {Deep neural network enabled corrective source term approach to hybrid analysis and modeling},
journal = {Neural Networks},
volume = {146},
pages = {181-199},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004494},
author = {Sindre Stenen Blakseth and Adil Rasheed and Trond Kvamsdal and Omer San},
keywords = {Deep neural networks, Digital twins, Explainable AI, Hybrid analysis and modeling, Physics-based modeling, Corrective source term approach (CoSTA)},
abstract = {In this work, we introduce, justify and demonstrate the Corrective Source Term Approach (CoSTA)—a novel approach to Hybrid Analysis and Modeling (HAM). The objective of HAM is to combine physics-based modeling (PBM) and data-driven modeling (DDM) to create generalizable, trustworthy, accurate, computationally efficient and self-evolving models. CoSTA achieves this objective by augmenting the governing equation of a PBM model with a corrective source term generated using a deep neural network. In a series of numerical experiments on one-dimensional heat diffusion, CoSTA is found to outperform comparable DDM and PBM models in terms of accuracy – often reducing predictive errors by several orders of magnitude – while also generalizing better than pure DDM. Due to its flexible but solid theoretical foundation, CoSTA provides a modular framework for leveraging novel developments within both PBM and DDM. Its theoretical foundation also ensures that CoSTA can be used to model any system governed by (deterministic) partial differential equations. Moreover, CoSTA facilitates interpretation of the DNN-generated source term within the context of PBM, which results in improved explainability of the DNN. These factors make CoSTA a potential door-opener for data-driven techniques to enter high-stakes applications previously reserved for pure PBM.}
}
@article{JARUSEK2022342,
title = {FOREX rate prediction improved by Elliott waves patterns based on neural networks},
journal = {Neural Networks},
volume = {145},
pages = {342-355},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.10.024},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004251},
author = {Robert Jarusek and Eva Volna and Martin Kotyrba},
keywords = {FOREX, Prediction, Neural networks, Elliott wave, Fast Fourier Transform (FFT)},
abstract = {Financial market predictions represent a complex problem. Most prediction systems work with the term time window, which is represented by exchange rate values of a real financial commodity. Such values (time window) provide the base for prediction of future values. Real situations, however, prove that prediction of only a single time-series trend is insufficient. This article aims at suggesting a novelty and unconventional approach based on the use of several neural networks predicting probable courses of a future trend defined in a prediction time window. The basis of the proposed approach is a suitable representation of the training-set input data into the neural networks. It uses selected FFT coefficients as well as robust output indicators based on a histogram of the predicted course of the selected currency pair. At the same time, the given currency pair enters the prediction in a combination with another three mutually interconnected currency pairs. A significant output of the articles is, apart from the proposed methodology, confirmation that the Elliott wave theory is beneficial in the trading environment and provides a substantial profit compared with conventional prediction techniques. That was proved in the performed experimental study.}
}
@article{LUQUE2022316,
title = {Computational epidemiology study of homeostatic compensation during sensorimotor aging},
journal = {Neural Networks},
volume = {146},
pages = {316-333},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004652},
author = {Niceto R. Luque and Francisco Naveros and Denis Sheynikhovich and Eduardo Ros and Angelo Arleo},
keywords = {Vestibulo-ocular reflex (VOR), Aging, Cerebellar adaptation, Spike timing-dependent plasticity, Intrinsic plasticity, Electrical synapses},
abstract = {The vestibulo-ocular reflex (VOR) stabilizes vision during head motion. Age-related changes of vestibular neuroanatomical properties predict a linear decay of VOR function. Nonetheless, human epidemiological data show a stable VOR function across the life span. In this study, we model cerebellum-dependent VOR adaptation to relate structural and functional changes throughout aging. We consider three neurosynaptic factors that may codetermine VOR adaptation during aging: the electrical coupling of inferior olive neurons, the long-term spike timing-dependent plasticity at parallel fiber – Purkinje cell synapses and mossy fiber – medial vestibular nuclei synapses, and the intrinsic plasticity of Purkinje cell synapses Our cross-sectional aging analyses suggest that long-term plasticity acts as a global homeostatic mechanism that underpins the stable temporal profile of VOR function. The results also suggest that the intrinsic plasticity of Purkinje cell synapses operates as a local homeostatic mechanism that further sustains the VOR at older ages. Importantly, the computational epidemiology approach presented in this study allows discrepancies among human cross-sectional studies to be understood in terms of interindividual variability in older individuals. Finally, our longitudinal aging simulations show that the amount of residual fibers coding for the peak and trough of the VOR cycle constitutes a predictive hallmark of VOR trajectories over a lifetime.}
}
@article{MI2022194,
title = {Improving data augmentation for low resource speech-to-text translation with diverse paraphrasing},
journal = {Neural Networks},
volume = {148},
pages = {194-205},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000260},
author = {Chenggang Mi and Lei Xie and Yanning Zhang},
keywords = {Data augmentation, Speech translation, Paraphrasing},
abstract = {High quality end-to-end speech translation model relies on a large scale of speech-to-text training data, which is usually scarce or even unavailable for some low-resource language pairs. To overcome this, we propose a target-side data augmentation method for low-resource language speech translation. In particular, we first generate large-scale target-side paraphrases based on a paraphrase generation model which incorporates several statistical machine translation (SMT) features and the commonly used recurrent neural network (RNN) feature. Then, a filtering model which consists of semantic similarity and speech–word pair co-occurrence was proposed to select the highest scoring source speech–target paraphrase pairs from candidates. Experimental results on English, Arabic, German, Latvian, Estonian, Slovenian and Swedish paraphrase generation show that the proposed method achieves significant and consistent improvements over several strong baseline models on PPDB datasets (http://paraphrase.org/). To introduce the results of paraphrase generation into the low-resource speech translation, we propose two strategies: audio–text pairs recombination and multiple references training. Experimental results show that the speech translation models trained on new audio–text datasets which combines the paraphrase generation results lead to substantial improvements over baselines, especially on low-resource languages.}
}
@article{DAVIS2022200,
title = {NeuroLISP: High-level symbolic programming with attractor neural networks},
journal = {Neural Networks},
volume = {146},
pages = {200-219},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004378},
author = {Gregory P. Davis and Garrett E. Katz and Rodolphe J. Gentili and James A. Reggia},
keywords = {Programmable neural networks, Working memory, Symbolic processing, Cognitive control, Compositionality, Associative learning},
abstract = {Despite significant improvements in contemporary machine learning, symbolic methods currently outperform artificial neural networks on tasks that involve compositional reasoning, such as goal-directed planning and logical inference. This illustrates a computational explanatory gap between cognitive and neurocomputational algorithms that obscures the neurobiological mechanisms underlying cognition and impedes progress toward human-level artificial intelligence. Because of the strong relationship between cognition and working memory control, we suggest that the cognitive abilities of contemporary neural networks are limited by biologically-implausible working memory systems that rely on persistent activity maintenance and/or temporal nonlocality. Here we present NeuroLISP, an attractor neural network that can represent and execute programs written in the LISP programming language. Unlike previous approaches to high-level programming with neural networks, NeuroLISP features a temporally-local working memory based on itinerant attractor dynamics, top-down gating, and fast associative learning, and implements several high-level programming constructs such as compositional data structures, scoped variable binding, and the ability to manipulate and execute programmatic expressions in working memory (i.e., programs can be treated as data). Our computational experiments demonstrate the correctness of the NeuroLISP interpreter, and show that it can learn non-trivial programs that manipulate complex derived data structures (multiway trees), perform compositional string manipulation operations (PCFG SET task), and implement high-level symbolic AI algorithms (first-order unification). We conclude that NeuroLISP is an effective neurocognitive controller that can replace the symbolic components of hybrid models, and serves as a proof of concept for further development of high-level symbolic programming in neural networks.}
}
@article{YADAV202211,
title = {CSITime: Privacy-preserving human activity recognition using WiFi channel state information},
journal = {Neural Networks},
volume = {146},
pages = {11-21},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004391},
author = {Santosh Kumar Yadav and Siva Sai and Akshay Gundewar and Heena Rathore and Kamlesh Tiwari and Hari Mohan Pandey and Mohit Mathur},
keywords = {Human activity recognition, WiFi channel state information, Time series classification, Data augmentation},
abstract = {Human activity recognition (HAR) is an important task in many applications such as smart homes, sports analysis, healthcare services, etc. Popular modalities for human activity recognition involving computer vision and inertial sensors are in the literature for solving HAR, however, they face serious limitations with respect to different illumination, background, clutter, obtrusiveness, and other factors. In recent years, WiFi channel state information (CSI) based activity recognition is gaining momentum due to its many advantages including easy deployability, and cost-effectiveness. This work proposes CSITime, a modified InceptionTime network architecture, a generic architecture for CSI-based human activity recognition. We perceive CSI activity recognition as a multi-variate time series problem. The methodology of CSITime is threefold. First, we pre-process CSI signals followed by data augmentation using two label-mixing strategies — mixup and cutmix to enhance the neural network’s learning. Second, in the basic block of CSITime, features from multiple convolutional kernels are concatenated and passed through a self-attention layer followed by a fully connected layer with Mish activation. CSITime network consists of six such blocks followed by a global average pooling layer and a final fully connected layer for the final classification. Third, in the training of the neural network, instead of adopting general training procedures such as early stopping, we use one-cycle policy and cosine annealing to monitor the learning rate. The proposed model has been tested on publicly available benchmark datasets, i.e., ARIL, StanWiFi, and SignFi datasets. The proposed CSITime has achieved accuracy of 98.20%, 98%, and 95.42% on ARIL, StanWiFi, and SignFi datasets, respectively, for WiFi-based activity recognition. This is an improvement on state-of-the-art accuracies by 3.3%, 0.67%, and 0.82% on ARIL, StanWiFi, and SignFi datasets, respectively. In lab-5 users’ scenario of the SignFi dataset, which has the training and testing data from different distributions, our model achieved accuracy was 2.17% higher than state-of-the-art, which shows the comparative robustness of our model.}
}
@article{UTKIN202281,
title = {SurvNAM: The machine learning survival model explanation},
journal = {Neural Networks},
volume = {147},
pages = {81-102},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004949},
author = {Lev V. Utkin and Egor D. Satyukov and Andrei V. Konstantinov},
keywords = {Explainable AI, Survival analysis, The Cox model, The lasso method, Shortcut connection},
abstract = {An extension of the Neural Additive Model (NAM) called SurvNAM and its modifications are proposed to explain predictions of a black-box machine learning survival model. The method is based on applying the original NAM to solving the explanation problem in the framework of survival analysis. The basic idea behind SurvNAM is to train the network by means of a specific expected loss function which takes into account peculiarities of the survival model predictions. Moreover, the loss function approximates the black-box model by the extension of the Cox proportional hazards model, which uses the well-known Generalized Additive Model (GAM) in place of the simple linear relationship of covariates. The proposed method SurvNAM allows performing local and global explanations. The global explanation uses the whole training dataset. In contrast to the global explanation, a set of synthetic examples around the explained example are randomly generated for the local explanation. The proposed modifications of SurvNAM are based on using the Lasso-based regularization for functions from GAM and for a special representation of the GAM functions using their weighted linear and non-linear parts, which is implemented as a shortcut connection. Many numerical experiments illustrate efficiency of SurvNAM.}
}
@article{JI202284,
title = {Cross-domain heterogeneous residual network for single image super-resolution},
journal = {Neural Networks},
volume = {149},
pages = {84-94},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000417},
author = {Li Ji and Qinghui Zhu and Yongqin Zhang and Juanjuan Yin and Ruyi Wei and Jinsheng Xiao and Deqiang Xiao and Guoying Zhao},
keywords = {Neural networks, Neural network architecture, Image restoration, Image resolution},
abstract = {Single image super-resolution is an ill-posed problem, whose purpose is to acquire a high-resolution image from its degraded observation. Existing deep learning-based methods are compromised on their performance and speed due to the heavy design (i.e., huge model size) of networks. In this paper, we propose a novel high-performance cross-domain heterogeneous residual network for super-resolved image reconstruction. Our network models heterogeneous residuals between different feature layers by hierarchical residual learning. In outer residual learning, dual-domain enhancement modules extract the frequency-domain information to reinforce the space-domain features of network mapping. In middle residual learning, wide-activated residual-in-residual dense blocks are constructed by concatenating the outputs from previous blocks as the inputs into all subsequent blocks for better parameter efficacy. In inner residual learning, wide-activated residual attention blocks are introduced to capture direction- and location-aware feature maps. The proposed method was evaluated on four benchmark datasets, indicating that it can construct the high-quality super-resolved images and achieve the state-of-the-art performance. Code and pre-trained models are available at https://github.com/zhangyongqin/HRN.}
}
@article{CHEN2022219,
title = {Region-aware network: Model human’s Top-Down visual perception mechanism for crowd counting},
journal = {Neural Networks},
volume = {148},
pages = {219-231},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000259},
author = {Yuehai Chen and Jing Yang and Dong Zhang and Kun Zhang and Badong Chen and Shaoyi Du},
keywords = {Crowd counting, Top-Down visual perception mechanism, Priority map, Global context information},
abstract = {Background noise and scale variation are common problems that have been long recognized in crowd counting. Humans glance at a crowd image and instantly know the approximate number of human and where they are through attention the crowd regions and the congestion degree of crowd regions with a global receptive field. Hence, in this paper, we propose a novel feedback network with Region-Aware block called RANet by modeling human’s Top-Down visual perception mechanism. Firstly, we introduce a feedback architecture to generate priority maps that provide prior about candidate crowd regions in input images. The prior enables the RANet pay more attention to crowd regions. Then we design Region-Aware block that could adaptively encode the contextual information into input images through global receptive field. More specifically, we scan the whole input images and its priority maps in the form of column vector to obtain a relevance matrix estimating their similarity. The relevance matrix obtained would be utilized to build global relationships between pixels. Our method outperforms state-of-the-art crowd counting methods on several public datasets.}
}
@article{BINGHAM202248,
title = {Discovering Parametric Activation Functions},
journal = {Neural Networks},
volume = {148},
pages = {48-65},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000016},
author = {Garrett Bingham and Risto Miikkulainen},
keywords = {Activation functions, Evolutionary computation, Gradient descent, AutoML, Deep learning},
abstract = {Recent studies have shown that the choice of activation function can significantly affect the performance of deep learning networks. However, the benefits of novel activation functions have been inconsistent and task dependent, and therefore the rectified linear unit (ReLU) is still the most commonly used. This paper proposes a technique for customizing activation functions automatically, resulting in reliable improvements in performance. Evolutionary search is used to discover the general form of the function, and gradient descent to optimize its parameters for different parts of the network and over the learning process. Experiments with four different neural network architectures on the CIFAR-10 and CIFAR-100 image classification datasets show that this approach is effective. It discovers both general activation functions and specialized functions for different architectures, consistently improving accuracy over ReLU and other activation functions by significant margins. The approach can therefore be used as an automated optimization step in applying deep learning to new tasks.}
}
@article{SIVANGI202263,
title = {NoAS-DS: Neural optimal architecture search for detection of diverse DNA signals},
journal = {Neural Networks},
volume = {147},
pages = {63-71},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004822},
author = {Kaushik Bhargav Sivangi and Chandra Mohan Dasari and Santhosh Amilpur and Raju Bhukya},
keywords = {Neural architecture search, TFBS, RBP, Binding sites, LSTM},
abstract = {Neural network architectures are high-performing variable models that can solve many learning tasks. Designing architectures manually require substantial time and also prior knowledge and expertise to develop a high-accuracy model. Most of the architecture search methods are developed over the task of image classification resulting in the building of complex architectures intended for large data inputs such as images. Motivated by the applications of DNA computing in Neural Architecture Search (NAS), we propose NoAS-DS which is specifically built for the architecture search of sequence-based classification tasks. Furthermore, NoAS-DS is applied to the task of predicting binding sites. Unlike other methods that implement only Convolution layers, NoAS-DS, specifically combines Convolution and LSTM layers that helps in the process of automatic architecture building. This hybrid approach helped in achieving high accuracy results on TFBS and RBP datasets which outperformed other models in TF-DNA binding prediction tasks. The best architectures generated by the proposed model can be applied to other DNA datasets of similar nature using transfer learning technique that demonstrates its generalization capability. This greatly reduces the effort required to build new architectures for other prediction tasks.}
}
@article{TIAN202269,
title = {Multi-view Teacher–Student Network},
journal = {Neural Networks},
volume = {146},
pages = {69-84},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004305},
author = {Yingjie Tian and Shiding Sun and Jingjing Tang},
keywords = {Multi-view learning, Information fusion, Teacher–student Network, Knowledge distillation},
abstract = {Multi-view learning aims to fully exploit the view-consistency and view-discrepancy for performance improvement. Knowledge Distillation (KD), characterized by the so-called “Teacher–Student” (T-S) learning framework, can transfer information learned from one model to another. Inspired by knowledge distillation, we propose a Multi-view Teacher–Student Network (MTS-Net), which combines knowledge distillation and multi-view learning into a unified framework. We first redefine the teacher and student for the multi-view case. Then the MTS-Net is built by optimizing both the view classification loss and the knowledge distillation loss in an end-to-end training manner. We further extend MTS-Net to image recognition tasks and present a multi-view Teacher–Student framework with convolutional neural networks called MTSCNN. To the best of our knowledge, MTS-Net and MTSCNN bring a new insight to extend the Teacher–Student framework to tackle the multi-view learning problem. We theoretically verify the mechanism of MTS-Net and MTSCNN and comprehensive experiments demonstrate the effectiveness of the proposed methods.}
}
@article{JIANG2022161,
title = {A second-order accelerated neurodynamic approach for distributed convex optimization},
journal = {Neural Networks},
volume = {146},
pages = {161-173},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S089360802100441X},
author = {Xinrui Jiang and Sitian Qin and Xiaoping Xue and Xinzhi Liu},
keywords = {Second-order neurodynamic approach, Inertial systems, Convergence rate},
abstract = {Based on the theories of inertial systems, a second-order accelerated neurodynamic approach is designed to solve a distributed convex optimization with inequality and set constraints. Most of the existing approaches for distributed convex optimization problems are usually first-order ones, and it is usually hard to analyze the convergence rate for the state solution of those first-order approaches. Due to the control design for the acceleration, the second-order neurodynamic approaches can often achieve faster convergence rate. Moreover, the existing second-order approaches are mostly designed to solve unconstrained distributed convex optimization problems, and are not suitable for solving constrained distributed convex optimization problems. It is acquired that the state solution of the designed neurodynamic approach in this paper converges to the optimal solution of the considered distributed convex optimization problem. An error function which demonstrates the performance of the designed neurodynamic approach, has a superquadratic convergence. Several numerical examples are provided to show the effectiveness of the presented second-order accelerated neurodynamic approach.}
}
@article{CHEN2022124,
title = {Incremental learning algorithm for large-scale semi-supervised ordinal regression},
journal = {Neural Networks},
volume = {149},
pages = {124-136},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000375},
author = {Haiyan Chen and Yizhen Jia and Jiaming Ge and Bin Gu},
keywords = {Semi-supervised ordinal regression, Incremental learning, Concave–Convex procedure algorithm, Path following algorithm},
abstract = {As a special case of multi-classification, ordinal regression (also known as ordinal classification) is a popular method to tackle the multi-class problems with samples marked by a set of ranks. Semi-supervised ordinal regression (SSOR) is especially important for data mining applications because semi-supervised learning can make use of the unlabeled samples to train a high-quality learning model. However, the training of large-scale SSOR is still an open question due to its complicated formulations and non-convexity to the best of our knowledge. To address this challenging problem, in this paper, we propose an incremental learning algorithm for SSOR (IL-SSOR), which can directly update the solution of SSOR based on the KKT conditions. More critically, we analyze the finite convergence of IL-SSOR which guarantees that SSOR can converge to a local minimum based on the framework of concave–convex procedure. To the best of our knowledge, the proposed new algorithm is the first efficient on-line learning algorithm for SSOR with local minimum convergence guarantee. The experimental results show, IL-SSOR can achieve better generalization than other semi-supervised multi-class algorithms. Compared with other semi-supervised ordinal regression algorithms, our experimental results show that IL-SSOR can achieve similar generalization with less running time.}
}
@article{CHEN20221,
title = {Towards improving fast adversarial training in multi-exit network},
journal = {Neural Networks},
volume = {150},
pages = {1-11},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S089360802200048X},
author = {Sihong Chen and Haojing Shen and Ran Wang and Xizhao Wang},
keywords = {Adversarial robustness, Adversarial defense, Fast adversarial training, Multi-exit network},
abstract = {Adversarial examples are usually generated by adding adversarial perturbations on clean samples, designed to deceive the model to make wrong classifications. Adversarial robustness refers to the ability of a model to resist adversarial attacks. And currently, a mainstream method to enhance adversarial robustness is the Projected Gradient Descent (PGD). However, PGD is often criticized for being time-consuming during constructing adversarial examples. Fast adversarial training can improve the adversarial robustness in shorter time, but it only can train for a limited number of epochs, leading to sub-optimal performance. This paper demonstrates that the multi-exit network can reduce the impact of adversarial perturbations by outputting easily identified samples at early exits. Therefore, we can improve the adversarial robustness. Further, we find that the multi-exit network can prevent catastrophic overfitting existing in single-step adversarial training. Specifically, we find that, in the multi-exit network, (1) the norm of weights at a fully connected layer in a non-overfitted exit is much smaller than that in an overfitted exit; and (2) catastrophic overfitting occurs when the late exits have weight norms larger than the early exits. Based on these findings, we propose an approach to alleviating the catastrophic overfitting of the multi-exit network. Compared to PGD adversarial training, our approach can train a model with decreased time complexity and increased empirical robustness. Extensive experiments have been conducted to evaluate our approach against various adversarial attacks, and the experimental results demonstrate superior robustness accuracies on CIFAR-10, CIFAR-100 and SVHN.}
}
@article{TARTAGLIONE2022230,
title = {LOss-Based SensiTivity rEgulaRization: Towards deep sparse neural networks},
journal = {Neural Networks},
volume = {146},
pages = {230-237},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.029},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004706},
author = {Enzo Tartaglione and Andrea Bragagnolo and Attilio Fiandrotti and Marco Grangetto},
keywords = {Pruning, Regularization, Deep learning, Sparsity},
abstract = {LOBSTER (LOss-Based SensiTivity rEgulaRization) is a method for training neural networks having a sparse topology. Let the sensitivity of a network parameter be the variation of the loss function with respect to the variation of the parameter. Parameters with low sensitivity, i.e. having little impact on the loss when perturbed, are shrunk and then pruned to sparsify the network. Our method allows to train a network from scratch, i.e. without preliminary learning or rewinding. Experiments on multiple architectures and datasets show competitive compression ratios with minimal computational overhead.}
}
@article{EVANS202296,
title = {Biological convolutions improve DNN robustness to noise and generalisation},
journal = {Neural Networks},
volume = {148},
pages = {96-110},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004780},
author = {Benjamin D. Evans and Gaurav Malhotra and Jeffrey S. Bowers},
keywords = {Deep learning, Convolutional neural network, Biological constraint, Gabor filter, Noise tolerance, Generalisation},
abstract = {Deep Convolutional Neural Networks (DNNs) have achieved superhuman accuracy on standard image classification benchmarks. Their success has reignited significant interest in their use as models of the primate visual system, bolstered by claims of their architectural and representational similarities. However, closer scrutiny of these models suggests that they rely on various forms of shortcut learning to achieve their impressive performance, such as using texture rather than shape information. Such superficial solutions to image recognition have been shown to make DNNs brittle in the face of more challenging tests such as noise-perturbed or out-of-distribution images, casting doubt on their similarity to their biological counterparts. In the present work, we demonstrate that adding fixed biological filter banks, in particular banks of Gabor filters, helps to constrain the networks to avoid reliance on shortcuts, making them develop more structured internal representations and more tolerance to noise. Importantly, they also gained around 20–35% improved accuracy when generalising to our novel out-of-distribution test image sets over standard end-to-end trained architectures. We take these findings to suggest that these properties of the primate visual system should be incorporated into DNNs to make them more able to cope with real-world vision and better capture some of the more impressive aspects of human visual perception such as generalisation.}
}
@article{SHEN2022374,
title = {Event-based master–slave synchronization of complex-valued neural networks via pinning impulsive control},
journal = {Neural Networks},
volume = {145},
pages = {374-385},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.10.025},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004263},
author = {Yuan Shen and Xinzhi Liu},
keywords = {Complex-valued neural networks, Synchronization, Event-triggered, Pinning impulsive control, Time delays},
abstract = {This paper investigates the synchronization problem of complex-valued neural networks via event-triggered pinning impulsive control (ETPIC). A time-delayed pinning impulsive controller is proposed based on three levels of event-triggered conditions. By employing the Lyapunov functional method and differential inequality technique, sufficient delay-dependent synchronization criteria are derived under the proposed ETPIC scheme. The obtained result shows that synchronization of master and slave complex-valued neural networks can be achieved even if the sizes of delays exceed the length of intervals between any two consecutive impulsive instants determined by Lyapunov-based event-triggered conditions in the proposed control strategy. Moreover, the linear matrix inequality approach is utilized to exclude Zeno behavior. Numerical examples are provided to illustrate the effectiveness of the theoretical results.}
}
@article{WANG2022107,
title = {Dilated projection correction network based on autoencoder for hyperspectral image super-resolution},
journal = {Neural Networks},
volume = {146},
pages = {107-119},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004421},
author = {Xinya Wang and Jiayi Ma and Junjun Jiang and Xiao-Ping Zhang},
keywords = {Hyperspectral image, Super-resolution, Deep learning, Autoencoder},
abstract = {This paper focuses on improving the spatial resolution of the hyperspectral image (HSI) by taking the prior information into consideration. In recent years, single HSI super-resolution methods based on deep learning have achieved good performance. However, most of them only simply apply general image super-resolution deep networks to hyperspectral data, thus ignoring some specific characteristics of hyperspectral data itself. In order to make full use of spectral information of the HSI, we transform the HSI SR problem from the image domain into the abundance domain by the dilated projection correction network with an autoencoder, termed as aeDPCN. In particular, we first encode the low-resolution HSI to abundance representation and preserve the spectral information in the decoder network, which could largely reduce the computational complexity. Then, to enhance the spatial resolution of the abundance embedding, we super-resolve the embedding in a coarse-to-fine manner by the dilated projection correction network where the back-projection strategy is introduced to further eliminate spectral distortion. Finally, the predictive images are derived by the same decoder, which increases the stability of our method, even at a large upscaling factor. Extensive experiments on real hyperspectral image scenes demonstrate the superiority of our method over the state-of-the-art, in terms of accuracy and efficiency.}
}
@article{ABDULNABIALI2022334,
title = {Predictive accuracy of CNN for cortical oscillatory activity in an acute rat model of parkinsonism},
journal = {Neural Networks},
volume = {146},
pages = {334-340},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004664},
author = {Ali {Abdul Nabi Ali} and Mesbah Alam and Simon C. Klein and Nicolai Behmann and Joachim K. Krauss and Theodor Doll and Holger Blume and Kerstin Schwabe},
keywords = {Deep learning, Convolutional Neural Network, Parkinson’s disease, Electroencephalogram, Electrocorticogram, Acute rat model},
abstract = {In neurological and neuropsychiatric disorders neuronal oscillatory activity between basal ganglia and cortical circuits are altered, which may be useful as biomarker for adaptive deep brain stimulation. We investigated whether changes in the spectral power of oscillatory activity in the motor cortex (MCtx) and the sensorimotor cortex (SMCtx) of rats after injection of the dopamine (DA) receptor antagonist haloperidol (HALO) would be similar to those observed in Parkinson disease. Thereafter, we tested whether a convolutional neural network (CNN) model would identify brain signal alterations in this acute model of parkinsonism. A sixteen channel surface micro-electrocorticogram (ECoG) recording array was placed under the dura above the MCtx and SMCtx areas of one hemisphere under general anaesthesia in rats. Seven days after surgery, micro ECoG was recorded in individual free moving rats in three conditions: (1) basal activity, (2) after injection of HALO (0.5 mg/kg), and (3) with additional injection of apomorphine (APO) (1 mg/kg). Furthermore, a CNN-based classification consisting of 23,530 parameters was applied on the raw data. HALO injection decreased oscillatory theta band activity (4–8 Hz) and enhanced beta (12–30 Hz) and gamma (30–100 Hz) in MCtx and SMCtx, which was compensated after APO injection (P ¡ 0.001). Evaluation of classification performance of the CNN model provided accuracy of 92%, sensitivity of 90% and specificity of 93% on one-dimensional signals. The CNN proposed model requires a minimum of sensory hardware and may be integrated into future research on therapeutic devices for Parkinson disease, such as adaptive closed loop stimulation, thus contributing to more efficient way of treatment.}
}
@article{WANG202242,
title = {Fast writer adaptation with style extractor network for handwritten text recognition},
journal = {Neural Networks},
volume = {147},
pages = {42-52},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004755},
author = {Zi-Rui Wang and Jun Du},
keywords = {Fast writer adaptation, Style extractor network, Offline handwritten text recognition},
abstract = {Writing style is an abstract attribute in handwritten text. It plays an important role in recognition systems and is not easy to define explicitly. Considering the effect of writing style, a writer adaptation method is proposed to transform a writer-independent recognizer toward a particular writer. This transformation has the potential to significantly increase accuracy. In this paper, under the deep learning framework, we propose a general fast writer adaptation solution. Specifically, without depending on other complex skills, a well designed style extractor network (SEN) trained by identification loss (IDL) is introduced to explicitly extract personalized writer information. The architecture of SEN consists of a stack of convolutional layers followed by a recurrent neural network with gated recurrent units to remove semantic context and retain writer information. Then, the outputs of the GRU are further integrated into a one-dimensional vector that is adopted to represent writing style. Finally, the extracted style information is fed into the writer-independent recognizer to achieve adaptation. Validated on offline handwritten text recognition tasks, the proposed fast sentence-level adaptation achieves remarkable improvements in Chinese and English text recognition tasks. Specifically, in the HETR task, a multi-information fusion network that is equipped with a hybrid attention mechanism and that integrates visual features, context features and writing style is proposed. In addition, under the same condition (only one writer-specific text line used as adaptation data), the proposed solution, without consuming extra time, can significantly outperform the previous multiple-pass decoding method. The code is available at https://github.com/Wukong90/Handwritten-Text-Recognition.}
}
@article{DU202274,
title = {Noise-robust voice conversion with domain adversarial training},
journal = {Neural Networks},
volume = {148},
pages = {74-84},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S089360802200003X},
author = {Hongqiang Du and Lei Xie and Haizhou Li},
keywords = {Voice conversion, Noise-robust, Domain adversarial training},
abstract = {Voice conversion has made great progress in the past few years under the studio-quality test scenario in terms of speech quality and speaker similarity. However, in real applications, test speech from source speaker or target speaker can be corrupted by various environment noises, which seriously degrade the speech quality and speaker similarity. In this paper, we propose a novel encoder–decoder based noise-robust voice conversion framework, which consists of a speaker encoder, a content encoder, a decoder, and two domain adversarial neural networks. Specifically, we integrate disentangling speaker and content representation technique with domain adversarial training technique. Domain adversarial training makes speaker representations and content representations extracted by speaker encoder and content encoder from clean speech and noisy speech in the same space, respectively. In this way, the learned speaker and content representations are noise-invariant. Therefore, the two noise-invariant representations can be taken as input by the decoder to predict the clean converted spectrum. The experimental results demonstrate that our proposed method can synthesize clean converted speech under noisy test scenarios, where the source speech and target speech can be corrupted by seen or unseen noise types during the training process. Additionally, both speech quality and speaker similarity are improved.}
}
@article{WEI2022341,
title = {Fixed/Preassigned-time synchronization of quaternion-valued neural networks via pure power-law control},
journal = {Neural Networks},
volume = {146},
pages = {341-349},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.023},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004512},
author = {Wanlu Wei and Juan Yu and Leimin Wang and Cheng Hu and Haijun Jiang},
keywords = {Fixed-time synchronization, Preassigned-time synchronization, Quaternion-valued neural network, Pure power-law control},
abstract = {The fixed-time synchronization and preassigned-time synchronization of quaternion-valued neural networks are concerned in this article. By developing fixed-time stability and proposing a pure power-law control scheme, some simple conditions are obtained to realize fixed-time synchronization of quaternion-valued neural networks and the upper bound of the synchronized time is provided. Furthermore, the preassigned-time synchronization of quaternion-valued neural networks is investigated based on pure power-law control design, where the synchronization time is preassigned in advance and the control gains are finite. Note that the designed controllers in this paper are the pure power-law forms, which are simpler and more effective compared with the traditional design composed of the linear part and power-law part. Eventually, an example is given to illustrate the feasibility and validity of the results obtained.}
}
@article{SHEN20221,
title = {Transition dynamics and optogenetic controls of generalized periodic epileptiform discharges},
journal = {Neural Networks},
volume = {149},
pages = {1-17},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000326},
author = {Zhuan Shen and Honghui Zhang and Zilu Cao and Luyao Yan and Yuzhi Zhao and Lin Du and Zichen Deng},
keywords = {Mean field model, Generalized periodic epileptiform discharge (GPED), Optogenetic, Inhibitory population},
abstract = {This paper aims to analyze possible mechanisms underlying the generation of generalized periodic epileptiform discharges (GPEDs), especially to design targeted optogenetic regulation strategies. First and foremost, inspired by existing physiological experiments, we propose a new computational framework by introducing a second inhibitory neuronal population and related synaptic connections into the classic Liley mean field model. The improved model can simulate the basic normal and abnormal brain activities mentioned in previous studies, but much to our relief, it perfectly reproduces some types of GPEDs that match the clinical records. Specifically, results show that disinhibitory synaptic connections between inhibitory interneuronal populations are closely related to the occurrence, transition and termination of GPEDs, including delaying the occurrence of GPEDs caused by the excitatory AMPAergic autapses and regulating the transition process of GPEDs bidirectionally, which support the conjecture that selective changes of synaptic connections can trigger GPEDs. Additionally, we creatively offer six optogenetic strategies with dual targets. They can all control GPEDs well, just as experiments reveal that optogenetic stimulation of inhibitory interneurons can suppress abnormal activities in epilepsy or other brain diseases. More importantly, 1:1 coordinated reset stimulation with one period rest is concluded as the optimal strategy after taking into account the energy consumption and control effect. Hope these results provide feasible references for pathophysiological mechanisms of GPEDs.}
}
@article{LI2022103,
title = {DMPP: Differentiable multi-pruner and predictor for neural network pruning},
journal = {Neural Networks},
volume = {147},
pages = {103-112},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004998},
author = {Jiaxin Li and Bo Zhao and Derong Liu},
keywords = {Neural network pruning, Model compression, Differentiable structure search, Performance predictor, Multi-pruner},
abstract = {Neural network pruning can trim the over-parameterized neural networks effectively by removing a number of network parameters. However, the traditional rule-based approaches always depend on manual experience. Existing heuristic search methods in discrete search spaces are usually time consuming and sub-optimal. In this paper, we develop a differentiable multi-pruner and predictor (DMPP) to prune neural networks automatically. The pruner composed of learnable parameters generates the pruning ratios of all convolutional layers as the continuous representation of the network. The neural network-based predictor is employed to predict the performance of different structures, which can accelerate the search process. Pruner and predictor enable us to directly employ gradient-based optimization to find a better structure. In addition, multi-pruner is presented to improve the efficiency of search, and knowledge distillation is leveraged to improve the performance of the pruned network. To evaluate the effectiveness of the proposed method, extensive experiments are performed on CIFAR-10, CIFAR-100, and ImageNet datasets with VGGNet and ResNet. Results show that the present DMPP can achieve a better performance than many previous state-of-the-art methods.}
}
@article{HOU2022172,
title = {Deep adversarial transition learning using cross-grafted generative stacks},
journal = {Neural Networks},
volume = {149},
pages = {172-183},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000442},
author = {Jinyong Hou and Xuejie Ding and Jeremiah D. Deng and Stephen Cranefield},
keywords = {Domain adaptation, Variational auto-encoders, Generative adversarial networks, Transfer learning},
abstract = {As a common approach of deep domain adaptation in computer vision, current works have mainly focused on learning domain-invariant features from different domains, achieving limited success in transfer learning. In this paper, we present a novel “deep adversarial transition learning” (DATL) framework that bridges the domain gap by generating some intermediate, transitional spaces between the source and target domains through the employment of adjustable, cross-grafted generative network stacks and effective adversarial learning between transitions. Specifically, variational auto-encoders (VAEs) are constructed for the domains, and bidirectional transitions are formed by cross-grafting the VAEs’ decoder stacks. Generative adversarial networks are then employed to map the target domain data to the label space of the source domain, which is achieved by aligning the transitions initiated by different domains. This results in a new, effective learning paradigm, where training and testing are carried out in the associated transitional spaces instead of the original domains. Experimental results demonstrate that our method outperforms the state-of-the-art on a number of unsupervised domain adaptation benchmarks.}
}
@article{LIU2022163,
title = {Symmetric positive definite manifold learning and its application in fault diagnosis},
journal = {Neural Networks},
volume = {147},
pages = {163-174},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S089360802100486X},
author = {Yuanhong Liu and Zebiao Hu and Yansheng Zhang},
keywords = {Fault diagnosis, Riemannian manifold, Manifold learning, Locally linear embedding, Semi-supervised learning},
abstract = {Locally linear embedding (LLE) is an effective tool to extract the significant features from a dataset. However, most of the relevant existing algorithms assume that the original dataset resides on a Euclidean space, unfortunately nearly all the original data space is non-Euclidean. In addition, the original LLE does not use the discriminant information of the dataset, which will degrade its performance in feature extraction. To address these problems raised in the conventional LLE, we first employ the original dataset to construct a symmetric positive definite manifold, and then estimate the tangent space of this manifold. Furthermore, the local and global discriminant information are integrated into the LLE, and the improved LLE is operated in the tangent space to extract the important features. We introduce Iris dataset to analyze the capability of the proposed method to extract features. Finally, several experiments are performed on five machinery datasets, and experimental results indicate that our proposed method can extract the excellent low-dimensional representations of the original dataset. Compared with the state-of-the-art methods, the proposed algorithm shows a strong capability for fault diagnosis.}
}
@article{NEBLI2022254,
title = {Quantifying the reproducibility of graph neural networks using multigraph data representation},
journal = {Neural Networks},
volume = {148},
pages = {254-265},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000284},
author = {Ahmed Nebli and Mohammed Amine Gharsallaoui and Zeynep Gürler and Islem Rekik},
keywords = {Reproducibility, Graph neural networks, Brain connectivity multigraphs, Brain biomarkers},
abstract = {Graph neural networks (GNNs) have witnessed an unprecedented proliferation in tackling several problems in computer vision, computer-aided diagnosis and related fields. While prior studies have focused on boosting the model accuracy, quantifying the reproducibility of the most discriminative features identified by GNNs is still an intact problem that yields concerns about their reliability in clinical applications in particular. Specifically, the reproducibility of biological markers across clinical datasets and distribution shifts across classes (e.g., healthy and disordered brains) is of paramount importance in revealing the underpinning mechanisms of diseases as well as propelling the development of personalized treatment. Motivated by these issues, we propose, for the first time, reproducibility-based GNN selection (RG-Select), a framework for GNN reproducibility assessment via the quantification of the most discriminative features (i.e., biomarkers) shared between different models. To ascertain the soundness of our framework, the reproducibility assessment embraces variations of different factors such as training strategies and data perturbations. Despite these challenges, our framework successfully yielded replicable conclusions across different training strategies and various clinical datasets. Our findings could thus pave the way for the development of biomarker trustworthiness and reliability assessment methods for computer-aided diagnosis and prognosis tasks. RG-Select code is available on GitHub at https://github.com/basiralab/RG-Select.}
}
@article{WANG202266,
title = {Trade off analysis between fixed-time stabilization and energy consumption of nonlinear neural networks},
journal = {Neural Networks},
volume = {148},
pages = {66-73},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000041},
author = {Yuchun Wang and Song Zhu and Hu Shao and Li Wang and Shiping Wen},
keywords = {Nonlinear neural networks, Fixed-time stabilization, Energy consumption, Trade-off},
abstract = {This paper concentrates on trade off analysis between fixed-time stabilization and energy consumption for a type of nonlinear neural networks (NNs). By constructing a compound switching controller and utilizing inequality techniques, a sufficient condition is proposed to ensure the fixed-time stabilization. Then, an estimate of the upper bound of the energy consumed by the controller in the control process is given. Furthermore, the quantitative analysis of the trade-off between the control time and energy consumption is studied. This article reveals that appropriate control parameters can balance the above two indicators to achieve an optimal control state. Finally, the presented theoretical results are verified by two numerical examples.}
}
@article{LEE2022141,
title = {Stability and dissipativity criteria for neural networks with time-varying delays via an augmented zero equality approach},
journal = {Neural Networks},
volume = {146},
pages = {141-150},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004354},
author = {S.H. Lee and M.J. Park and D.H. Ji and O.M. Kwon},
keywords = {Stability, Neural Network, Time-varying delay, Dissipativity analysis, Lyapunov method},
abstract = {This work investigates the stability and dissipativity problems for neural networks with time-varying delay. By the construction of new augmented Lyapunov–Krasovskii functionals based on integral inequality and the use of zero equality approach, three improved results are proposed in the forms of linear matrix inequalities. And, based on the stability results, the dissipativity analysis for NNs with time-varying delays was investigated. Through some numerical examples, the superiority and effectiveness of the proposed results are shown by comparing the existing works.}
}
@article{ZHU202272,
title = {Approximation capabilities of measure-preserving neural networks},
journal = {Neural Networks},
volume = {147},
pages = {72-80},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004809},
author = {Aiqing Zhu and Pengzhan Jin and Yifa Tang},
keywords = {Measure-preserving, Neural networks, Dynamical systems, Approximation theory},
abstract = {Measure-preserving neural networks are well-developed invertible models, however, their approximation capabilities remain unexplored. This paper rigorously analyzes the approximation capabilities of existing measure-preserving neural networks including NICE and RevNets. It is shown that for compact U⊂RD with D≥2, the measure-preserving neural networks are able to approximate arbitrary measure-preserving map ψ:U→RD which is bounded and injective in the Lp-norm. In particular, any continuously differentiable injective map with ±1 determinant of Jacobian is measure-preserving, thus can be approximated.}
}
@article{AFEBU2022266,
title = {Feature-based intelligent models for optimisation of percussive drilling},
journal = {Neural Networks},
volume = {148},
pages = {266-284},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.021},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000314},
author = {Kenneth Omokhagbo Afebu and Yang Liu and Evangelos Papatheou},
keywords = {Vibro-impact drilling, Rotary-percussion, Bit-rock interaction, Impact motions, Multistability, Machine learning},
abstract = {As a rotary-percussion system, the vibro-impact drilling (VID) system utilises resonantly induced high frequency periodic impacts alongside existing drill-string rotation to cut through downhole rock layers. Due to the inhomogeneous nature of the rock layers, the system often experiences multi-stability which generates different categories of impact motions as drilling continues downhole. Some impact motions yield better drilling performance in terms of rate of penetration (ROP) and bit life-span when compared to others. As an optimisation strategy, the present study adopts feature-based classification algorithms including multi-layer perceptron, support vector machine and long short-term memory network as intelligent models for categorising impact motions from a one-degree-of-freedom impact oscillator representing the percussive bit-rock impacts of the VID system. This way, high-performance impacts can be easily detected and maintained while undesirable low-performance impacts are well avoided to increase ROP, improve bit life-span and save cost. In this study, scarce and limited classes of experimental impact data are merged with inexhaustibly simulated impact data to train different network models. By means of cross-validation, the trained networks were tested on separate sets of only-simulation and only-experimental data. Results show that extracting appropriate features from raw impact data is essential for optimising the performance of each network model. About 42% of the feature-based networks yield accuracies greater than 91% while about 67% yield accuracies greater than 77% on both simulation and experimental impact motion data.}
}
@article{LIN202225,
title = {Feature Correlation-Steered Capsule Network for object detection},
journal = {Neural Networks},
volume = {147},
pages = {25-41},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004767},
author = {Zhongqi Lin and Jingdun Jia and Feng Huang and Wanlin Gao},
keywords = {Capsule Network (CapsNet), Feature correlation, Part-object association, Expectation-maximum routing agreement, Object detection},
abstract = {Despite Convolutional Neural Networks (CNNs) based approaches have been successful in objects detection, they predominantly focus on positioning discriminative regions while overlooking the internal holistic part-whole associations within objects. This would ultimately lead to the neglect of feature relationships between object and its parts as well as among those parts, both of which are significantly helpful for detecting discriminative parts. In this paper, we propose to “look insider the objects” by digging into part-whole feature correlations and take the attempts to leverage those correlations endowed by the Capsule Network (CapsNet) for robust object detection. Actually, highly correlated capsules across adjacent layers share high familiarity, which will be more likely to be routed together. In light of this, we take such correlations between different capsules of the preceding training samples as an awareness to constrain the subsequent candidate voting scope during the routing procedure, and a Feature Correlation-Steered CapsNet (FCS-CapsNet) with Locally-Constrained Expectation-Maximum (EM) Routing Agreement (LCEMRA) is proposed. Different from conventional EM routing, LCEMRA stipulates that only those relevant low-level capsules (parts) meeting the requirement of quantified intra-object cohesiveness can be clustered to make up high-level capsules (objects). In doing so, part-object associations can be dug by transformation weighting matrixes between capsules layers during such “part backtracking” procedure. LCEMRA enables low-level capsules to selectively gather projections from a non-spatially-fixed set of high-level capsules. Experiments on VOC2007, VOC2012, HKU-IS, DUTS, and COCO show that FCS-CapsNet can achieve promising object detection effects across multiple evaluation metrics, which are on-par with state-of-the-arts.}
}
@article{VACHER2022107,
title = {Flexibly regularized mixture models and application to image segmentation},
journal = {Neural Networks},
volume = {149},
pages = {107-123},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000430},
author = {Jonathan Vacher and Claire Launay and Ruben Coen-Cagli},
keywords = {Unsupervised learning, Mixture models, Graphical model, Factor graph, Image segmentation, Convolutional neural networks},
abstract = {Probabilistic finite mixture models are widely used for unsupervised clustering. These models can often be improved by adapting them to the topology of the data. For instance, in order to classify spatially adjacent data points similarly, it is common to introduce a Laplacian constraint on the posterior probability that each data point belongs to a class. Alternatively, the mixing probabilities can be treated as free parameters, while assuming Gauss–Markov or more complex priors to regularize those mixing probabilities. However, these approaches are constrained by the shape of the prior and often lead to complicated or intractable inference. Here, we propose a new parametrization of the Dirichlet distribution to flexibly regularize the mixing probabilities of over-parametrized mixture distributions. Using the Expectation–Maximization algorithm, we show that our approach allows us to define any linear update rule for the mixing probabilities, including spatial smoothing regularization as a special case. We then show that this flexible design can be extended to share class information between multiple mixture models. We apply our algorithm to artificial and natural image segmentation tasks, and we provide quantitative and qualitative comparison of the performance of Gaussian and Student-t mixtures on the Berkeley Segmentation Dataset. We also demonstrate how to propagate class information across the layers of deep convolutional neural networks in a probabilistically optimal way, suggesting a new interpretation for feedback signals in biological visual systems. Our flexible approach can be easily generalized to adapt probabilistic mixture models to arbitrary data topologies.}
}
@article{WU2022184,
title = {Modeling learnable electrical synapse for high precision spatio-temporal recognition},
journal = {Neural Networks},
volume = {149},
pages = {184-194},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000399},
author = {Zhenzhi Wu and Zhihong Zhang and Huanhuan Gao and Jun Qin and Rongzhen Zhao and Guangshe Zhao and Guoqi Li},
keywords = {Electrical synapse/coupling, Leaky-integrate-and-fire model, Spatio-temporal information, Bio-plausible neuronal dynamics},
abstract = {Bio-inspired recipes are being introduced to artificial neural networks for the efficient processing of spatio-temporal tasks. Among them, Leaky Integrate and Fire (LIF) model is the most remarkable one thanks to its temporal processing capability, lightweight model structure, and well investigated direct training methods. However, most learnable LIF networks generally take neurons as independent individuals that communicate via chemical synapses, leaving electrical synapses all behind. On the contrary, it has been well investigated in biological neural networks that the inter-neuron electrical synapse takes a great effect on the coordination and synchronization of generating action potentials. In this work, we are engaged in modeling such electrical synapses in artificial LIF neurons, where membrane potentials propagate to neighbor neurons via convolution operations, and the refined neural model ECLIF is proposed. We then build deep networks using ECLIF and trained them using a back-propagation-through-time algorithm. We found that the proposed network has great accuracy improvement over traditional LIF on five datasets and achieves high accuracy on them. In conclusion, it reveals that the introduction of the electrical synapse is an important factor for achieving high accuracy on realistic spatio-temporal tasks.}
}
@article{XU202298,
title = {An inertial neural network approach for robust time-of-arrival localization considering clock asynchronization},
journal = {Neural Networks},
volume = {146},
pages = {98-106},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004408},
author = {Chentao Xu and Qingshan Liu},
keywords = {Time of arrival localization, Clock asynchronization, Inertial neural network, Constrained optimization},
abstract = {This paper presents an inertial neural network to solve the source localization optimization problem with l1-norm objective function based on the time of arrival (TOA) localization technique. The convergence and stability of the inertial neural network are analyzed by the Lyapunov function method. An inertial neural network iterative approach is further used to find a better solution among the solutions with different inertial parameters. Furthermore, the clock asynchronization is considered in the TOA l1-norm model for more general real applications, and the corresponding inertial neural network iterative approach is addressed. The numerical simulations and real data are both considered in the experiments. In the simulation experiments, the noise contains uncorrelated zero-mean Gaussian noise and uniform distributed outliers. In the real experiments, the data is obtained by using the ultra wide band (UWB) technology hardware modules. Whether or not there is clock asynchronization, the results show that the proposed approach always can find a more accurate source position compared with some of the existing algorithms, which implies that the proposed approach is more effective than the compared ones.}
}
@article{AGLIARI2022232,
title = {The emergence of a concept in shallow neural networks},
journal = {Neural Networks},
volume = {148},
pages = {232-253},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022000272},
author = {Elena Agliari and Francesco Alemanno and Adriano Barra and Giordano {De Marzo}},
keywords = {Neural networks, Machine learning, Glassy statistical mechanics},
abstract = {We consider restricted Boltzmann machine (RBMs) trained over an unstructured dataset made of blurred copies of definite but unavailable “archetypes” and we show that there exists a critical sample size beyond which the RBM can learn archetypes, namely the machine can successfully play as a generative model or as a classifier, according to the operational routine. In general, assessing a critical sample size (possibly in relation to the quality of the dataset) is still an open problem in machine learning. Here, restricting to the random theory, where shallow networks suffice and the “grandmother-cell” scenario is correct, we leverage the formal equivalence between RBMs and Hopfield networks, to obtain a phase diagram for both the neural architectures which highlights regions, in the space of the control parameters (i.e., number of archetypes, number of neurons, size and quality of the training set), where learning can be accomplished. Our investigations are led by analytical methods based on the statistical-mechanics of disordered systems and results are further corroborated by extensive Monte Carlo simulations.}
}
@article{CHUMACHENKO2022220,
title = {Feedforward neural networks initialization based on discriminant learning},
journal = {Neural Networks},
volume = {146},
pages = {220-229},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.020},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004482},
author = {Kateryna Chumachenko and Alexandros Iosifidis and Moncef Gabbouj},
keywords = {Neural networks initialization, Discriminant learning},
abstract = {In this paper, a novel data-driven method for weight initialization of Multilayer Perceptrons and Convolutional Neural Networks based on discriminant learning is proposed. The approach relaxes some of the limitations of competing data-driven methods, including unimodality assumptions, limitations on the architectures related to limited maximal dimensionalities of the corresponding projection spaces, as well as limitations related to high computational requirements due to the need of eigendecomposition on high-dimensional data. We also consider assumptions of the method on the data and propose a way to account for them in a form of a new normalization layer. The experiments on three large-scale image datasets show improved accuracy of the trained models compared to competing random-based and data-driven weight initialization methods, as well as better convergence properties in certain cases.}
}
@article{SEMENOVA2022151,
title = {Understanding and mitigating noise in trained deep neural networks},
journal = {Neural Networks},
volume = {146},
pages = {151-160},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004366},
author = {Nadezhda Semenova and Laurent Larger and Daniel Brunner},
keywords = {Artificial neural networks, Noise, Hardware neural networks, Deep neural networks, Analog neural networks, Noise reduction},
abstract = {Deep neural networks unlocked a vast range of new applications by solving tasks of which many were previously deemed as reserved to higher human intelligence. One of the developments enabling this success was a boost in computing power provided by special purpose hardware, such as graphic or tensor processing units. However, these do not leverage fundamental features of neural networks like parallelism and analog state variables. Instead, they emulate neural networks relying on binary computing, which results in unsustainable energy consumption and comparatively low speed. Fully parallel and analogue hardware promises to overcome these challenges, yet the impact of analogue neuron noise and its propagation, i.e. accumulation, threatens rendering such approaches inept. Here, we determine for the first time the propagation of noise in deep neural networks comprising noisy nonlinear neurons in trained fully connected layers. We study additive and multiplicative as well as correlated and uncorrelated noise, and develop analytical methods that predict the noise level in any layer of symmetric deep neural networks or deep neural networks trained with back propagation. We find that noise accumulation is generally bound, and adding additional network layers does not worsen the signal to noise ratio beyond a limit. Most importantly, noise accumulation can be suppressed entirely when neuron activation functions have a slope smaller than unity. We therefore developed the framework for noise in fully connected deep neural networks implemented in analog systems, and identify criteria allowing engineers to design noise-resilient novel neural network hardware.}
}