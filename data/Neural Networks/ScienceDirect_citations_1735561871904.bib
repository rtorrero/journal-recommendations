@article{KAMPFFMEYER201991,
title = {Deep divergence-based approach to clustering},
journal = {Neural Networks},
volume = {113},
pages = {91-101},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300292},
author = {Michael Kampffmeyer and Sigurd Løkse and Filippo M. Bianchi and Lorenzo Livi and Arnt-Børre Salberg and Robert Jenssen},
keywords = {Deep learning, Clustering, Unsupervised learning, Information-theoretic learning, Divergence},
abstract = {A promising direction in deep learning research consists in learning representations and simultaneously discovering cluster structure in unlabeled data by optimizing a discriminative loss function. As opposed to supervised deep learning, this line of research is in its infancy, and how to design and optimize suitable loss functions to train deep neural networks for clustering is still an open question. Our contribution to this emerging field is a new deep clustering network that leverages the discriminative power of information-theoretic divergence measures, which have been shown to be effective in traditional clustering. We propose a novel loss function that incorporates geometric regularization constraints, thus avoiding degenerate structures of the resulting clustering partition. Experiments on synthetic benchmarks and real datasets show that the proposed network achieves competitive performance with respect to other state-of-the-art methods, scales well to large datasets, and does not require pre-training steps.}
}
@article{FACHECHI201924,
title = {Dreaming neural networks: Forgetting spurious memories and reinforcing pure ones},
journal = {Neural Networks},
volume = {112},
pages = {24-40},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300176},
author = {Alberto Fachechi and Elena Agliari and Adriano Barra},
keywords = {Unlearning, Reinforcement learning, Statistical mechanics, Sleep&dream},
abstract = {The standard Hopfield model for associative neural networks accounts for biological Hebbian learning and acts as the harmonic oscillator for pattern recognition, however its maximal storage capacity is α∼0.14, far from the theoretical bound for symmetric networks, i.e. α=1. Inspired by sleeping and dreaming mechanisms in mammal brains, we propose an extension of this model displaying the standard on-line (awake) learning mechanism (that allows the storage of external information in terms of patterns) and an off-line (sleep) unlearning&consolidating mechanism (that allows spurious-pattern removal and pure-pattern reinforcement): this obtained daily prescription is able to saturate the theoretical bound α=1, remaining also extremely robust against thermal noise. The emergent neural and synaptic features are analyzed both analytically and numerically. In particular, beyond obtaining a phase diagram for neural dynamics, we focus on synaptic plasticity and we give explicit prescriptions on the temporal evolution of the synaptic matrix. We analytically prove that our algorithm makes the Hebbian kernel converge with high probability to the projection matrix built over the pure stored patterns. Furthermore, we obtain a sharp and explicit estimate for the “sleep rate” in order to ensure such a convergence. Finally, we run extensive numerical simulations (mainly Monte Carlo sampling) to check the approximations underlying the analytical investigations (e.g., we developed the whole theory at the so called replica-symmetric level, as standard in the Amit–Gutfreund–Sompolinsky reference framework) and possible finite-size effects, finding overall full agreement with the theory.}
}
@article{ZHENG2019116,
title = {Emergent neural turing machine and its visual navigation},
journal = {Neural Networks},
volume = {110},
pages = {116-130},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303198},
author = {Zejia Zheng and Xiang Wu and Juyang Weng},
keywords = {Universal turing machine, Hierarchical representation, Autonomous navigation, Neural network, General-purpose visual learning},
abstract = {Traditional Turing Machines (TMs) are symbolic whose hand-crafted representations are static and limited. Developmental Network 1 (DN-1) uses emergent representation to perform Turing Computation. But DN-1 lacks hierarchy in its internal representations, and is hard to handle the complex visual navigation tasks. In this paper, we improve DN-1 with several new mechanisms and presenta new emergent neural Turing Machine — Developmental Network 2 (DN-2). By neural, we mean that the control of the TM has neurons as basic computing elements. The major novelty of DN-2 over DN-1 is that the representational hierarchy inside DN-2 is emergent and fluid. DN-2 grows complex hierarchies by dynamically allowing initialization of neurons with different ranges of connection. We present a complex task — vision guided navigation in simulated and natural worlds using DN-2. A major function that has not been demonstrated before is that the hierarchy enables attention that disregards distracting features based on the navigation context. In simulated navigation experiments, DN-2 can perform with no errors, and in real-world navigation experiments, the error rate is only 0.78%. These experimental results showed that DN-2 successfully learned rules of navigation with image inputs.}
}
@article{2021iii,
title = {List of Editorial Board Members},
journal = {Neural Networks},
volume = {133},
pages = {iii-ix},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(20)30403-2},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020304032}
}
@article{ZHANG2019186,
title = {Synchronization in uncertain fractional-order memristive complex-valued neural networks with multiple time delays},
journal = {Neural Networks},
volume = {110},
pages = {186-198},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303356},
author = {Weiwei Zhang and Hai Zhang and Jinde Cao and Fuad E. Alsaadi and Dingyuan Chen},
keywords = {Memristor complex-valued neural networks, Comparison principle, Multiple time delays, Fractional order, Parameter uncertainties},
abstract = {This paper considers the global asymptotical synchronization of fractional-order memristive complex-valued neural networks (FOMCVNN), with both parameter uncertainties and multiple time delays. Sufficient conditions of uncertain FOMCVNN, with multiple time delays, are established through the employment of comparison principle and Lyapunov direct method. A numerical example is used to show the effectiveness of the proposed methods.}
}
@article{2020I,
title = {Current Events},
journal = {Neural Networks},
volume = {132},
pages = {I},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(20)30369-5},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020303695}
}
@article{WU201972,
title = {A structure–time parallel implementation of spike-based deep learning},
journal = {Neural Networks},
volume = {113},
pages = {72-78},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300218},
author = {Xi Wu and Yixuan Wang and Huajin Tang and Rui Yan},
keywords = {Neuromorphic computing, Parallel implementation, Spike-based deep learning, Deep spiking neural networks},
abstract = {Motivated by the recent progress of deep spiking neural networks (SNNs), we propose a structure–time parallel strategy based on layered structure and one-time computation over a time window to speed up the prominent spike-based deep learning algorithm named broadcast alignment. Furthermore, a well-designed deep hierarchical model based on the parallel broadcast alignment is proposed for object recognition. The parallel broadcast alignment achieves a significant 137× speedup compared to its original implementation on MNIST dataset. The object recognition model achieves higher accuracy than that of the latest spiking deep convolutional neural networks on the ETH-80 dataset. The proposed parallel strategy and the object recognition model will facilitate both the simulation of deep SNNs for studying spiking neural dynamics and also the applications of spike-based deep learning in real-world problems.}
}
@article{HU201931,
title = {A fully convolutional two-stream fusion network for interactive image segmentation},
journal = {Neural Networks},
volume = {109},
pages = {31-42},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302946},
author = {Yang Hu and Andrea Soltoggio and Russell Lock and Steve Carter},
keywords = {Interactive image segmentation, Fully convolutional network, Two-stream network},
abstract = {In this paper, we propose a novel fully convolutional two-stream fusion network (FCTSFN) for interactiveimage segmentation. The proposed network includes two sub-networks: a two-stream late fusion network (TSLFN) that predicts the foreground at a reduced resolution, and a multi-scale refining network (MSRN) that refines the foreground at full resolution. The TSLFN includes two distinct deep streams followed by a fusion network. The intuition is that, since user interactions are more direct information on foreground/background than the image itself, the two-stream structure of the TSLFN reduces the number of layers between the pure user interaction features and the network output, allowing the user interactions to have a more direct impact on the segmentation result. The MSRN fuses the features from different layers of TSLFN with different scales, in order to seek the local to global information on the foreground to refine the segmentation result at full resolution. We conduct comprehensive experiments on four benchmark datasets. The results show that the proposed network achieves competitive performance compared to current state-of-the-art interactive image segmentation methods. 1}
}
@article{2021I,
title = {Current Events},
journal = {Neural Networks},
volume = {140},
pages = {I},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00186-6},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021001866}
}
@article{2021ii,
title = {Editorial Board},
journal = {Neural Networks},
volume = {135},
pages = {ii},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00009-5},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021000095}
}
@article{ZHANG2021101,
title = {Advanced deep learning methods for biomedical information analysis: An editorial},
journal = {Neural Networks},
volume = {133},
pages = {101-102},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020303610},
author = {Yu-Dong Zhang and Francesco Carlo Morabito and Dinggang Shen and Khan Muhammad}
}
@article{RUIZGARCIA2021199,
title = {Deep neural network representation and Generative Adversarial Learning},
journal = {Neural Networks},
volume = {139},
pages = {199-200},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021000897},
author = {Ariel Ruiz-Garcia and Jürgen Schmidhuber and Vasile Palade and Clive Cheong Took and Danilo Mandic}
}
@article{2021ii,
title = {Editorial Board},
journal = {Neural Networks},
volume = {136},
pages = {ii},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00065-4},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021000654}
}
@article{LI2019102,
title = {Unsupervised robust discriminative manifold embedding with self-expressiveness},
journal = {Neural Networks},
volume = {113},
pages = {102-115},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303186},
author = {Jianwei Li},
keywords = {Unsupervised robust discriminative manifold embedding(URDME), Dimensionality reduction, Low-rank learning, Discriminative subspace, Robust affinity, Unsupervised leaning},
abstract = {Dimensionality reduction has obtained increasing attention in the machine learning and computer vision communities due to the curse of dimensionality. Many manifold embedding methods have been proposed for dimensionality reduction. Many of them are supervised and based on graph regularization whose weight affinity is determined by original noiseless data. When data are noisy, their performance may degrade. To address this issue, we present a novel unsupervised robust discriminative manifold embedding approach called URDME, which aims to offer a joint framework of dimensionality reduction, discriminative subspace learning , robust affinity representation and discriminative manifold embedding. The learned robust affinity not only captures the global geometry and intrinsic structure of underlying high-dimensional data, but also satisfies the self-expressiveness property. In addition, the learned projection matrix owns discriminative ability in the low-dimensional subspace. Experimental results on several public benchmark datasets corroborate the effectiveness of our approach and show its competitive performance compared with the related methods.}
}
@article{CARNEIRO2019243,
title = {Particle swarm optimization for network-based data classification},
journal = {Neural Networks},
volume = {110},
pages = {243-255},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303344},
author = {Murillo G. Carneiro and Ran Cheng and Liang Zhao and Yaochu Jin},
keywords = {Complex networks, Machine learning, Network structural optimization, Data classification, Graph optimization, Particle swarm},
abstract = {Complex networks provide a powerful tool for data representation due to its ability to describe the interplay between topological, functional, and dynamical properties of the input data. A fundamental process in network-based (graph-based) data analysis techniques is the network construction from original data usually in vector form. Here, a natural question is: How to construct an “optimal” network regarding a given processing goal? This paper investigates structural optimization in the context of network-based data classification tasks. To be specific, we propose a particle swarm optimization framework which is responsible for building a network from vector-based data set while optimizing a quality function driven by the classification accuracy. The classification process considers both topological and physical features of the training and test data and employing PageRank measure for classification according to the importance concept of a test instance to each class. Results on artificial and real-world problems reveal that data network generated using structural optimization provides better results in general than those generated by classical network formation methods. Moreover, this investigation suggests that other kinds of network-based machine learning and data mining tasks, such as dimensionality reduction and data clustering, can benefit from the proposed structural optimization method.}
}
@article{ZHANG201985,
title = {An unsupervised parameter learning model for RVFL neural network},
journal = {Neural Networks},
volume = {112},
pages = {85-97},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300188},
author = {Yongshan Zhang and Jia Wu and Zhihua Cai and Bo Du and Philip S. Yu},
keywords = {Random vector functional link network, Randomized feedforward neural networks, Autoencoder, -norm regularization, Pre-trained parameters, Classification applications},
abstract = {With the direct input–output connections, a random vector functional link (RVFL) network is a simple and effective learning algorithm for single-hidden layer feedforward neural networks (SLFNs). RVFL is a universal approximator for continuous functions on compact sets with fast learning property. Owing to its simplicity and effectiveness, RVFL has attracted significant interest in numerous real-world applications. In reality, the performance of RVFL is often challenged by randomly assigned network parameters. In this paper, we propose a novel unsupervised network parameter learning method for RVFL, named sparse pre-trained random vector functional link (SP-RVFL for short) network. The proposed SP-RVFL uses a sparse autoencoder with ℓ1-norm regularization to adaptively learn superior network parameters for specific learning tasks. By doing so, the learned network parameters in SP-RVFL are embedded with the valuable information of input data, which alleviate the randomly generated parameter issue and improve the algorithmic performance. Experiments and comparisons on 16 diverse benchmarks from different domains confirm the effectiveness of the proposed SP-RVFL. The corresponding results also demonstrate that RVFL outperforms extreme learning machine (ELM).}
}
@article{CAO2019159,
title = {Passivity analysis of delayed reaction–diffusion memristor-based neural networks},
journal = {Neural Networks},
volume = {109},
pages = {159-167},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302892},
author = {Yanyi Cao and Yuting Cao and Shiping Wen and Tingwen Huang and Zhigang Zeng},
keywords = {Neural network, Memristor, Passivity},
abstract = {This paper discusses the passivity of delayed reaction–diffusion memristor-based neural networks (RDMNNs). By exploiting inequality techniques and by constructing appropriate Lyapunov functional, several sufficient conditions are obtained in the form of linear matrix inequalities (LMIs), which can be used to ascertain the passivity, output and input strict passivity of delayed RDMNNs. In addition, the passivity of RDMNNs without any delay is also considered. These conditions, represented by LMIs, can be easily verified by virtue of the Matlab toolbox. Finally, some illustrative examples are provided to substantiate the effectiveness and validity of the theoretical results, and to present an application of RDMNN in pseudo-random number generation.}
}
@article{BRITODASILVA20191,
title = {Dual vigilance fuzzy adaptive resonance theory},
journal = {Neural Networks},
volume = {109},
pages = {1-5},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302776},
author = {Leonardo Enzo {Brito da Silva} and Islam Elnabarawy and Donald C. Wunsch},
keywords = {Clustering, Adaptive resonance theory, ART, Visual assessment of cluster tendency, Topology, Unsupervised},
abstract = {Clusters retrieved by generic Adaptive Resonance Theory (ART) networks are limited to their internal categorical representation. This study extends the capabilities of ART by incorporating multiple vigilance thresholds in a single network: stricter (data compression) and looser (cluster similarity) vigilance values are used to obtain a many-to-one mapping of categories-to-clusters. It demonstrates this idea in the context of Fuzzy ART, presented as Dual Vigilance Fuzzy ART (DVFA), to improve the ability to capture clusters with arbitrary geometry. DVFA outperformed Fuzzy ART for the datasets in our experiments while yielding a statistically-comparable performance to another more complex, multi-prototype Fuzzy ART-based architecture.}
}
@article{2021x,
title = {Neural Networks Referees in 2020},
journal = {Neural Networks},
volume = {133},
pages = {x-xv},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(20)30404-4},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020304044}
}
@article{ANTONOPOULOS201990,
title = {Evaluating performance of neural codes in model neural communication networks},
journal = {Neural Networks},
volume = {109},
pages = {90-102},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302934},
author = {Chris G. Antonopoulos and Ezequiel Bianco-Martinez and Murilo S. Baptista},
keywords = {Mutual information rate, Neural codes, Hindmarsh–Rose system, Neural networks, Interspike-intervals code, Firing-rate code},
abstract = {Information needs to be appropriately encoded to be reliably transmitted over physical media. Similarly, neurons have their own codes to convey information in the brain. Even though it is well-known that neurons exchange information using a pool of several protocols of spatio-temporal encodings, the suitability of each code and their performance as a function of network parameters and external stimuli is still one of the great mysteries in neuroscience. This paper sheds light on this by modeling small-size networks of chemically and electrically coupled Hindmarsh–Rose spiking neurons. We focus on a class of temporal and firing-rate codes that result from neurons’ membrane-potentials and phases, and quantify numerically their performance estimating the Mutual Information Rate, aka the rate of information exchange. Our results suggest that the firing-rate and interspike-intervals codes are more robust to additive Gaussian white noise. In a network of four interconnected neurons and in the absence of such noise, pairs of neurons that have the largest rate of information exchange using the interspike-intervals and firing-rate codes are not adjacent in the network, whereas spike-timings and phase codes (temporal) promote large rate of information exchange for adjacent neurons. If that result would have been possible to extend to larger neural networks, it would suggest that small microcircuits would preferably exchange information using temporal codes (spike-timings and phase codes), whereas on the macroscopic scale, where there would be typically pairs of neurons not directly connected due to the brain’s sparsity, firing-rate and interspike-intervals codes would be the most efficient codes.}
}
@article{LI20191,
title = {Robust dimensionality reduction via feature space to feature space distance metric learning},
journal = {Neural Networks},
volume = {112},
pages = {1-14},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300103},
author = {Bo Li and Zhang-Tao Fan and Xiao-Long Zhang and De-Shuang Huang},
keywords = {Dimensionality reduction, Discriminant graph embedding, Feature space to feature space distance metric, Manifold learning},
abstract = {Images are often represented as vectors with high dimensions when involved in classification. As a result, dimensionality reduction methods have to be developed to avoid the curse of dimensionality. Among them, Laplacian eigenmaps (LE) have attracted widespread concentrations. In the original LE, point to point (P2P) distance metric is often adopted for manifold learning. Unfortunately, they show few impacts on robustness to noises. In this paper, a novel supervised dimensionality reduction method, named feature space to feature space distance metric learning (FSDML), is presented. For any point, it can construct a feature space spanned by its k intra-class nearest neighbors, which results in a local projection on its nearest feature space. Thus feature space to feature space (S2S) distance metric will be defined to Euclidean distance between two corresponding projections. On one hand, the proposed S2S distance metric displays superiority on robustness by the local projection. On the other hand, the projection on the nearest feature space contributes to fully mining local geometry information hidden in the original data. Moreover, both class label similarity and dissimilarity are also measured, based on which an intra-class graph and an inter-class graph will be individually modeled. Finally, a subspace can be found for classification by maximizing S2S based manifold to manifold distance and preserving S2S based locality of manifolds, simultaneously. Compared to some state-of-art dimensionality reduction methods, experiments validate the proposed method’s performance either on synthesized data sets or on benchmark data sets.}
}
@article{2021II,
title = {INN/ENNS/JNNS - Membership Applic. Form},
journal = {Neural Networks},
volume = {140},
pages = {II},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00187-8},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021001878}
}
@article{2021I,
title = {Current Events},
journal = {Neural Networks},
volume = {134},
pages = {I},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(20)30431-7},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020304317}
}
@article{2020ii,
title = {Editorial Board},
journal = {Neural Networks},
volume = {132},
pages = {ii},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(20)30374-9},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020303749}
}
@article{UZUNTARLA2019131,
title = {Synchronization-induced spike termination in networks of bistable neurons},
journal = {Neural Networks},
volume = {110},
pages = {131-140},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303290},
author = {Muhammet Uzuntarla and Joaquin J. Torres and Ali Calim and Ernest Barreto},
keywords = {Synchronization, Spike-termination, Excitatory population},
abstract = {We observe and study a self-organized phenomenon whereby the activity in a network of spiking neurons spontaneously terminates. We consider different types of populations, consisting of bistable model neurons connected electrically by gap junctions, or by either excitatory or inhibitory synapses, in a scale-free connection topology. We find that strongly synchronized population spiking events lead to complete cessation of activity in excitatory networks, but not in gap junction or inhibitory networks. We identify the underlying mechanism responsible for this phenomenon by examining the particular shape of the excitatory postsynaptic currents that arise in the neurons. We also examine the effects of the synaptic time constant, coupling strength, and channel noise on the occurrence of the phenomenon.}
}
@article{SCARDAPANE201919,
title = {Kafnets: Kernel-based non-parametric activation functions for neural networks},
journal = {Neural Networks},
volume = {110},
pages = {19-32},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303174},
author = {Simone Scardapane and Steven {Van Vaerenbergh} and Simone Totaro and Aurelio Uncini},
keywords = {Neural networks, Activation functions, Kernel methods},
abstract = {Neural networks are generally built by interleaving (adaptable) linear layers with (fixed) nonlinear activation functions. To increase their flexibility, several authors have proposed methods for adapting the activation functions themselves, endowing them with varying degrees of flexibility. None of these approaches, however, have gained wide acceptance in practice, and research in this topic remains open. In this paper, we introduce a novel family of flexible activation functions that are based on an inexpensive kernel expansion at every neuron. Leveraging several properties of kernel-based models, we propose multiple variations for designing and initializing these kernel activation functions (KAFs), including a multidimensional scheme allowing to nonlinearly combine information from different paths in the network. The resulting KAFs can approximate any mapping defined over a subset of the real line, either convex or non-convex. Furthermore, they are smooth over their entire domain, linear in their parameters, and they can be regularized using any known scheme, including the use of ℓ1 penalties to enforce sparseness. To the best of our knowledge, no other known model satisfies all these properties simultaneously. In addition, we provide an overview on alternative techniques for adapting the activation functions, which is currently lacking in the literature. A large set of experiments validates our proposal.}
}
@article{OZCAN201920,
title = {Stability analysis of Cohen–Grossberg neural networks of neutral-type: Multiple delays case},
journal = {Neural Networks},
volume = {113},
pages = {20-27},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300310},
author = {Neyir Ozcan},
keywords = {Delayed neutral systems, Neural networks, Stability analysis, Lyapunov functionals},
abstract = {The essential purpose of this work is to conduct an investigation into stability problem for the class of neutral-type Cohen–Grossberg neural networks including multiple time delays in states and multiple neutral delays in time derivative of states. By proposing an appropriate Lyapunov functional, a new sufficient criterion is derived for global asymptotic stability of neutral-type neural networks. The obtained stability criterion is independent of time delay and neutral delay parameters, and it is completely stated in terms of the elements of interconnection matrices and other network parameters. Thus, this newly presented stability condition can be validated by simply examining some algebraic equations establishing some relationships among the system parameters and matrices of the neutral-type neural system. A constructive example is presented to indicate applicability of the obtained sufficient stability condition. Since stability analysis of the class of neutral-type neural networks considered in this work has not been given much attention due to the difficulty of developing efficient mathematical techniques and methods enabling to conduct a stability analysis of such neutral-type neural systems, the criterion proposed in this paper can be considered as a leading stability result for neutral-type Cohen–Grossberg neural systems including multiple time and multiple neutral delays.}
}
@article{TAVANAEI201947,
title = {Deep learning in spiking neural networks},
journal = {Neural Networks},
volume = {111},
pages = {47-63},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303332},
author = {Amirhossein Tavanaei and Masoud Ghodrati and Saeed Reza Kheradpisheh and Timothée Masquelier and Anthony Maida},
keywords = {Deep learning, Spiking neural network, Biological plausibility, Machine learning, Power-efficient architecture},
abstract = {In recent years, deep learning has revolutionized the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained, most often in a supervised manner using backpropagation. Vast amounts of labeled training examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and are arguably the only viable option if one wants to understand how the brain computes at the neuronal description level. The spikes of biological neurons are sparse in time and space, and event-driven. Combined with bio-plausible local learning rules, this makes it easier to build low-power, neuromorphic hardware for SNNs. However, training deep SNNs remains a challenge. Spiking neurons’ transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy and computational cost. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while SNNs typically require many fewer operations and are the better candidates to process spatio-temporal data.}
}
@article{SURYANARAYANA2019113,
title = {Roles for globus pallidus externa revealed in a computational model of action selection in the basal ganglia},
journal = {Neural Networks},
volume = {109},
pages = {113-136},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302880},
author = {Shreyas M. Suryanarayana and Jeanette {Hellgren Kotaleski} and Sten Grillner and Kevin N. Gurney},
keywords = {Action selection, Network models, Globus pallidus externa, Arkypallidal GPe neurons, Prototypical GPe neurons},
abstract = {The basal ganglia are considered vital to action selection - a hypothesis supported by several biologically plausible computational models. Of the several subnuclei of the basal ganglia, the globus pallidus externa (GPe) has been thought of largely as a relay nucleus, and its intrinsic connectivity has not been incorporated in significant detail, in any model thus far. Here, we incorporate newly revealed subgroups of neurons within the GPe into an existing computational model of the basal ganglia, and investigate their role in action selection. Three main results ensued. First, using previously used metrics for selection, the new extended connectivity improved the action selection performance of the model. Second, low frequency theta oscillations were observed in the subpopulation of the GPe (the TA or ‘arkypallidal’ neurons) which project exclusively to the striatum. These oscillations were suppressed by increased dopamine activity — revealing a possible link with symptoms of Parkinson’s disease. Third, a new phenomenon was observed in which the usual monotonic relationship between input to the basal ganglia and its output within an action ‘channel’ was, under some circumstances, reversed. Thus, at high levels of input, further increase of this input to the channel could cause an increase of the corresponding output rather than the more usually observed decrease. Moreover, this phenomenon was associated with the prevention of multiple channel selection, thereby assisting in optimal action selection. Examination of the mechanistic origin of our results showed the so-called ‘prototypical’ GPe neurons to be the principal subpopulation influencing action selection. They control the striatum via the arkypallidal neurons and are also able to regulate the output nuclei directly. Taken together, our results highlight the role of the GPe as a major control hub of the basal ganglia, and provide a mechanistic account for its control function.}
}
@article{2021ii,
title = {Editorial Board},
journal = {Neural Networks},
volume = {141},
pages = {ii},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00293-8},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002938}
}
@article{LIU201941,
title = {Deep associative neural network for associative memory based on unsupervised representation learning},
journal = {Neural Networks},
volume = {113},
pages = {41-53},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300152},
author = {Jia Liu and Maoguo Gong and Haibo He},
keywords = {Unsupervised representation learning, Deep neural network, Associative memory, Image recovery},
abstract = {This paper presents a deep associative neural network (DANN) based on unsupervised representation learning for associative memory. In brain, the knowledge is learnt by associating different types of sensory data, such as image and voice. The associative memory models which imitate such a learning process have been studied for decades but with simpler architectures they fail to deal with large scale complex data as compared with deep neural networks. Therefore, we define a deep architecture consisting of a perception layer and hierarchical propagation layers. To learn the network parameters, we define a probabilistic model for the whole network inspired from unsupervised representation learning models. The model is optimized by a modified contrastive divergence algorithm with a novel iterated sampling process. After training, given a new data or corrupted data, the correct label or corrupted part is associated by the network. The DANN is able to achieve many machine learning problems, including not only classification, but also depicting the data given a label and recovering corrupted images. Experiments on MNIST digits and CIFAR-10 datasets demonstrate the learning capability of the proposed DANN.}
}
@article{DOYA2021xvi,
title = {Maintaining the Publication Infrastructure in a Worldwide Pandemic},
journal = {Neural Networks},
volume = {133},
pages = {xvi-xvii},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(20)30405-6},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020304056},
author = {Kenji Doya and DeLiang Wang}
}
@article{ZHOU201955,
title = {Event-triggered impulsive control on quasi-synchronization of memristive neural networks with time-varying delays},
journal = {Neural Networks},
volume = {110},
pages = {55-65},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302764},
author = {Yufeng Zhou and Zhigang Zeng},
keywords = {Event triggered, Impulsive control, Quasi-synchronization, Memristive neural networks, Time-varying delays},
abstract = {This paper discusses the quasi-synchronization of memristive neural networks (MNNs) with time-varying delays via event-triggered impulsive and state feedback control approaches. The choice of different initial conditions may lead to the unexpected parameter mismatch in virtue of the state-dependent parameters of MNNs. Thus, the accurate synchronization error level and the exponential convergence rate are derived in view of the comparison principle of impulsive systems and the variable parameter formula. A co-design procedure that can be easily implemented is presented to make the synchronization error converge to a predetermined level. Then, no zeno-behavior is proved to exist in the controlled system with the proposed event-triggered condition. In addition, a self-triggered scheme is proposed to prevent continuous communication happening between the drive system and the response system. Finally, a numerical example is given to illustrate the availability of the proposed control scheme.}
}
@article{2021II,
title = {INN/ENNS/JNNS - Membership Applic. Form},
journal = {Neural Networks},
volume = {141},
pages = {II},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00289-6},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002896}
}
@article{2021ii,
title = {Editorial Board},
journal = {Neural Networks},
volume = {142},
pages = {ii},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00334-8},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021003348}
}
@article{IWAKI2019103,
title = {Implicit incremental natural actor critic algorithm},
journal = {Neural Networks},
volume = {109},
pages = {103-112},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302922},
author = {Ryo Iwaki and Minoru Asada},
keywords = {Reinforcement learning, Natural policy gradient, Natural actor critic, Incremental learning, Implicit update},
abstract = {Natural policy gradient (NPG) methods are promising approaches to finding locally optimal policy parameters. The NPG approach works well in optimizing complex policies with high-dimensional parameters, and the effectiveness of NPG methods has been demonstrated in many fields. However, the incremental estimation of the NPG is computationally unstable owing to its high sensitivity to the step-sizes values, especially to the one used to update the estimate of NPG. In this study, we propose a new incremental and stable algorithm for the NPG estimation. We call the proposed algorithm the implicit incremental natural actor critic (I2NAC), and it is based on the idea of the implicit update. The convergence analysis for I2NAC is provided. Theoretical analysis results indicate the stability of I2NAC and the instability of conventional incremental NPG methods. Numerical experiments were performed, and the results show that I2NAC is less sensitive to the values of the meta-parameters, including the step-size for the NPG update, compared to the existing incremental NPG method.}
}
@article{WEI20191,
title = {Fixed-time synchronization of quaternion-valued memristive neural networks with time delays},
journal = {Neural Networks},
volume = {113},
pages = {1-10},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300279},
author = {Ruoyu Wei and Jinde Cao},
keywords = {Memristor, Fixed-time synchronization, Quaternion, Time delays, Drive–response systems},
abstract = {In this paper, a new type of neural networks, quaternion-valued memristive neural networks (QVMNNs) is formulated. On the basis of the differential inclusion principle and the Lyapunov functional method, fixed-time synchronization problem is considered in the form of drive–response system for this type of neural networks. A novel fixed-time controller is designed to achieve the control goal. With the fixed-time stability theory and some inequality techniques, criterion of fixed-time synchronization for QVMNNs is given. Finally, corresponding simulation results are presented to show the effectiveness of the method derived in this paper.}
}
@article{2021II,
title = {Editorial Board},
journal = {Neural Networks},
volume = {134},
pages = {II},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(20)30432-9},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020304329}
}
@article{TANG201979,
title = {Finite-time cluster synchronization for a class of fuzzy cellular neural networks via non-chattering quantized controllers},
journal = {Neural Networks},
volume = {113},
pages = {79-90},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300280},
author = {Rongqiang Tang and Xinsong Yang and Xiaoxiao Wan},
keywords = {Finite-time cluster synchronization, Fuzzy cellular neural networks, Discontinuous activation functions, Delays, Markovian switching topology, Non-chattering quantized control},
abstract = {This paper considers the finite-time cluster synchronization (FTCS) of coupled fuzzy cellular neural networks (FCNNs) with Markovian switching topology, discontinuous activation functions, proportional leakage, and time-varying unbounded delays. Novel quantized controllers without the sign function are designed to avoid the chattering and save communication resources. Under the framework of Filippov solution, several sufficient conditions are derived to guarantee the FTCS by constructing new Lyapunov–Krasovskii functionals and utilizing M-matrix methods. The new analytical techniques skillfully overcome the difficulties caused by time-varying delays and cope with the uncertainties of both Filippov solution and Markov jumping, which enable us determine the settling time explicitly. Numerical simulations demonstrate the effectiveness of the theoretical analysis.}
}
@article{LERER201966,
title = {Luminance gradients and non-gradients as a cue for distinguishing reflectance and illumination in achromatic images: A computational approach},
journal = {Neural Networks},
volume = {110},
pages = {66-81},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303162},
author = {Alejandro Lerer and Hans Supèr and Matthias S. Keil},
keywords = {Reflectance, Illumination, Computational model, Lightness, Image processing, Brightness illusions},
abstract = {The brain analyses the visual world through the luminance patterns that reach the retina. Formally, luminance (as measured by the retina) is the product of illumination and reflectance. Whereas illumination is highly variable, reflectance is a physical property that characterizes each object surface. Due to memory constraints, it seems plausible that the visual system suppresses illumination patterns before object recognition takes place. Since many combinations of reflectance and illumination can give rise to identical luminance values, finding the correct reflectance value of a surface is an ill-posed problem, and it is still an open question how it is solved by the brain. Here we propose a computational approach that first learns filter kernels (“receptive fields”) for slow and fast variations in luminance, respectively, from achromatic real-world images. Distinguishing between luminance gradients (slow variations) and non-gradients (fast variations) could serve to constrain the mentioned ill-posed problem. The second stage of our approach successfully segregates luminance gradients and non-gradients from real-world images. Our approach furthermore predicts that visual illusions that contain luminance gradients (such as Adelson’s checker-shadow display or grating induction) may occur as a consequence of this segregation process.}
}
@article{WEN2019159,
title = {Estimating coupling strength between multivariate neural series with multivariate permutation conditional mutual information},
journal = {Neural Networks},
volume = {110},
pages = {159-169},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303289},
author = {Dong Wen and Peilei Jia and Sheng-Hsiou Hsu and Yanhong Zhou and Xifa Lan and Dong Cui and Guolin Li and Shimin Yin and Lei Wang},
keywords = {Multivariate permutation conditional mutual information, Coupling strength, Multivariate neural series, Resting state EEG signals, Multi-channel neural mass model, Amnestic mild cognitive impairment},
abstract = {Recently, coupling between groups of neurons or different brain regions has been widely studied to provide insights into underlying mechanisms of brain functions. To comprehensively understand the effect of such coupling, it is necessary to accurately extract the coupling strength information among multivariate neural signals from the whole brain. This study proposed a new method named multivariate permutation conditional mutual information (MPCMI) to quantitatively estimate the coupling strength of multivariate neural signals (MNS). The performance of the MPCMI method was validated on the simulated MNS generated by multi-channel neural mass model (MNMM). The coupling strength feature of simulated MNS extracted by MPCMI showed better performance compared with standard methods, such as permutation conditional mutual information (PCMI), multivariate Granger causality (MVGC), and Granger causality analysis (GCA). Furthermore, the MPCMI was applied to estimate the coupling strengths of two-channel resting-state electroencephalographic (rsEEG) signals from different brain regions of 19 patients with amnestic mild cognitive impairment (aMCI) with type 2 diabetes mellitus (T2DM) and 20 normal control (NC) with T2DM in Alpha1 and Alpha2 frequency bands. Empirical results showed that the MPCMI could effectively extract the coupling strength features that were significantly different between the aMCI and the NC. Hence, the proposed MPCMI method could be an effective estimate of coupling strengths of MNS, and might be a viable biomarker for clinical applications.}
}
@article{MORITANI2019213,
title = {A hypothetical neural network model for generation of human precision grip},
journal = {Neural Networks},
volume = {110},
pages = {213-224},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303320},
author = {Yuki Moritani and Naomichi Ogihara},
keywords = {Hand, Grasping, Simulation, Optimization, Motor control},
abstract = {Humans can stably hold and skillfully manipulate an object by coordinated control of a complex, redundant musculoskeletal system. However, how the human central nervous system actually accomplishes precision grip tasks by coordinated control of fingertip forces remains unclear. In the present study, we aimed to construct a hypothetical neural network model that can spontaneously generate humanlike precision grip. The nervous system was modeled as a recurrent neural network model prescribing kinematic and kinetic constraints that must be satisfied in precision grip tasks in the form of energy functions. The recurrent neural network autonomously behaves so as to decrease the energy functions; therefore, given the estimated mass and center-of-mass location of the target object, the nervous system model can spontaneously generate muscle activation signals that achieve stable precision grips due to dynamic relaxation of the energy functions embedded in the nervous system. Fingertip forces are modulated by sensory information about slip between the object and fingertips. A two-dimensional musculoskeletal model of the human hand with a thumb and an index finger was constructed. Forward dynamic simulation of the precision grip was performed using the proposed neural network model. Our results demonstrated that the proposed neural network model could stably pinch and successfully hold up the object in various conditions, including changes in friction, object shape, object mass, and center-of-mass location. The proposed hypothetical neuro-computational model may possibly explain some aspects of the control strategy humans use for precision grip.}
}
@article{ROMERO2019147,
title = {Weighted contrastive divergence},
journal = {Neural Networks},
volume = {114},
pages = {147-156},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302752},
author = {Enrique Romero and Ferran Mazzanti and Jordi Delgado and David Buchaca},
keywords = {Neural networks, Restricted Boltzmann machine, Contrastive divergence},
abstract = {Learning algorithms for energy based Boltzmann architectures that rely on gradient descent are in general computationally prohibitive, typically due to the exponential number of terms involved in computing the partition function. In this way one has to resort to approximation schemes for the evaluation of the gradient. This is the case of Restricted Boltzmann Machines (RBM) and its learning algorithm Contrastive Divergence (CD). It is well-known that CD has a number of shortcomings, and its approximation to the gradient has several drawbacks. Overcoming these defects has been the basis of much research and new algorithms have been devised, such as persistent CD. In this manuscript we propose a new algorithm that we call Weighted CD (WCD), built from small modifications of the negative phase in standard CD. However small these modifications may be, experimental work reported in this paper suggests that WCD provides a significant improvement over standard CD and persistent CD at a small additional computational cost.}
}
@article{MATTOS201954,
title = {A stochastic variational framework for Recurrent Gaussian Processes models},
journal = {Neural Networks},
volume = {112},
pages = {54-72},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300164},
author = {César Lincoln C. Mattos and Guilherme A. Barreto},
keywords = {Gaussian Processes, Dynamical modeling, Variational inference, Stochastic learning},
abstract = {Gaussian Processes (GPs) models have been successfully applied to the problem of learning from sequential observations. In such context, the family of Recurrent Gaussian Processes (RGPs) have been recently introduced with a specifically designed structure to handle dynamical data. However, RGPs present a limitation shared by most GP approaches: they become computationally infeasible when facing very large datasets. In the present work, with the aim of improving scalability, we modify the original variational approach used with RGPs in order to enable inference via stochastic mini-batch optimization, giving rise to the Stochastic Recurrent Variational Bayes (S-REVARB) framework. We review recent related literature and comprehensively contextualize it with our approach. Moreover, we propose two learning procedures, the Local and Global S-REVARB algorithms, which prevent computational costs from scaling with the number of training samples. The global variant permits even greater scalability by also preventing the number of variational parameters from increasing with the training set, through the use of neural networks as sequential recognition models. The proposed framework is evaluated in the task of dynamical system identification for large scale datasets, a scenario not readily supported by the standard batch inference for RGPs. The promising results indicate that the S-REVARB framework opens up the possibility of applying powerful hierarchical recurrent GP-based models to massive sequential data.}
}
@article{XIE2019300,
title = {A distributed semi-supervised learning algorithm based on manifold regularization using wavelet neural network},
journal = {Neural Networks},
volume = {118},
pages = {300-309},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302995},
author = {Jin Xie and Sanyang Liu and Hao Dai},
keywords = {Distributed learning (DL), Semi-supervised learning (SSL), Manifold regularization (MR), Wavelet neural network (WNN), Privacy preserving},
abstract = {This paper aims to propose a distributed semi-supervised learning (D-SSL) algorithm to solve D-SSL problems, where training samples are often extremely large-scale and located on distributed nodes over communication networks. Training data of each node consists of labeled and unlabeled samples whose output values or labels are unknown. These nodes communicate in a distributed way, where each node has only access to its own data and can only exchange local information with its neighboring nodes. In some scenarios, these distributed data cannot be processed centrally. As a result, D-SSL problems cannot be centrally solved by using traditional semi-supervised learning (SSL) algorithms. The state-of-the-art D-SSL algorithm, denoted as Distributed Laplacian Regularization Least Square (D-LapRLS), is a kernel based algorithm. It is essential for the D-LapRLS algorithm to estimate the global Euclidian Distance Matrix (EDM) with respect to total samples, which is time-consuming especially when the scale of training data is large. In order to solve D-SSL problems and overcome the common drawback of kernel based D-SSL algorithms, we propose a novel Manifold Regularization (MR) based D-SSL algorithm using Wavelet Neural Network (WNN) and Zero-Gradient-Sum (ZGS) distributed optimization strategy. Accordingly, each node is assigned an individual WNN with the same basis functions. In order to initialize the proposed D-SSL algorithm, we propose a centralized MR based SSL algorithm using WNN. We denote the proposed SSL and D-SSL algorithms as Laplacian WNN (LapWNN) and distributed LapWNN (D-LapWNN), respectively. The D-LapWNN algorithm works in a fully distributed fashion by using ZGS strategy, whose convergence is guaranteed by the Lyapunov method. During the learning process, each node only exchanges local coefficients with its neighbors rather than raw data. It means that the D-LapWNN algorithm is a privacy preserving method. At last, several illustrative simulations are presented to show the efficiency and advantage of the proposed algorithm.}
}
@article{OTSUKA2019137,
title = {Estimation of neuronal dynamics based on sparse modeling},
journal = {Neural Networks},
volume = {109},
pages = {137-146},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302910},
author = {Shinya Otsuka and Toshiaki Omori},
keywords = {Sparse modeling, Nonlinear neuronal dynamics, Conductance-based neuron model, Data-driven approach},
abstract = {Elucidating neural dynamics is one of the important subjects in neuroscience. To elucidate nonlinear dynamics of single neurons, it is important to extract nonlinear membrane currents from many types of membrane current candidates. In this study, we propose a sparse modeling method for estimating a conductance-based neuron model from observed data, by extracting necessary membrane currents from multiple candidates. We show using simulated data that our proposed sparse modeling approach with different sparsity levels for distinct membrane currents extracts only necessary membrane currents from candidates more accurately, compared with least-squares method and sparse method with uniform sparsity level.}
}
@article{SELVARAJ201973,
title = {Disturbance and uncertainty rejection performance for fractional-order complex dynamical networks},
journal = {Neural Networks},
volume = {112},
pages = {73-84},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300206},
author = {P. Selvaraj and O.M. Kwon and R. Sakthivel},
keywords = {Fractional-order system, Complex dynamical networks, Disturbance and uncertainty estimator},
abstract = {This paper investigates the synchronization issue for a family of time-delayed fractional-order complex dynamical networks (FCDNs) with time delay, unknown bounded uncertainty and disturbance. A novel fractional uncertainty and disturbance estimator (FUDE) based feedback control strategy is proposed to not only synchronize the considered FCDNs but also guaranteeing the precise rejection of unmodelled system uncertainty and external disturbance. Especially, in FUDE-based approach, model uncertainties and external disturbance are integrated as a lumped disturbance and it does not require a completely known system model or a disturbance model. On the other hand, the design algorithm for the proposed control strategy is based on the state-space framework, rather than frequency-based design methodologies in the literature, which helps for predominant comprehension of the inner system behaviour. Also, by the temperance of Lyapunov stability theory and fractional calculus, a set of adequate conditions in the linear matrix inequality framework is obtained, which guarantees the robust synchronization of the closed-loop system. Furthermore, an iterative optimization algorithm is proposed to improve control robustness against the external disturbance and model uncertainties. Finally, two numerical illustrations including financial network model, where the influence of adjustment of macro-economic policies in the entire financial system are given to exhibit the rightness and important features of the acquired theoretical results.}
}
@article{LIU201943,
title = {Variational inference with Gaussian mixture model and householder flow},
journal = {Neural Networks},
volume = {109},
pages = {43-55},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302879},
author = {GuoJun Liu and Yang Liu and MaoZu Guo and Peng Li and MingYu Li},
keywords = {Variational auto-encoder, Gaussian mixture model, Householder flow, Variational inference},
abstract = {The variational auto-encoder (VAE) is a powerful and scalable deep generative model. Under the architecture of VAE, the choice of the approximate posterior distribution is one of the crucial issues, and it has a significant impact on tractability and flexibility of the VAE. Generally, latent variables are assumed to be normally distributed with a diagonal covariance matrix, however, it is not flexible enough to match the true complex posterior distribution. We introduce a novel approach to design a flexible and arbitrarily complex approximate posterior distribution. Unlike VAE, firstly, an initial density is constructed by a Gaussian mixture model, and each component has a diagonal covariance matrix. Then this relatively simple distribution is transformed into a more flexible one by applying a sequence of invertible Householder transformations until the desired complexity has been achieved. Additionally, we also give a detailed theoretical and geometric interpretation of Householder transformations. Lastly, due to this change of approximate posterior distribution, the Kullback–Leibler distance between two mixture densities is required to be calculated, but it has no closed form solution. Therefore, we redefine a new variational lower bound by virtue of its upper bound. Compared with other generative models based on similar VAE architecture, our method achieves new state-of-the-art results on benchmark datasets including MNIST, Fashion-MNIST, Omniglot and Histopathology data a more challenging medical images dataset, the experimental results show that our method can improve the flexibility of posterior distribution more effectively.}
}
@article{2021II,
title = {INN/ENNS/JNNS - Membership Applic. Form},
journal = {Neural Networks},
volume = {136},
pages = {II},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00061-7},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021000617}
}
@article{ZHU201935,
title = {Learning a discriminant graph-based embedding with feature selection for image categorization},
journal = {Neural Networks},
volume = {111},
pages = {35-46},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303393},
author = {Ruifeng Zhu and Fadi Dornaika and Yassine Ruichek},
keywords = {Graph-based embedding, Discriminant embedding, Sparse regression, Feature selection, Semi-supervised learning, Image categorization},
abstract = {Graph-based embedding methods are very useful for reducing the dimension of high-dimensional data and for extracting their relevant features. In this paper, we introduce a novel nonlinear method called Flexible Discriminant graph-based Embedding with feature selection (FDEFS). The proposed algorithm aims to classify image sample data in supervised learning and semi-supervised learning settings. Specifically, our method incorporates the Manifold Smoothness, Margin Discriminant Embedding and the Sparse Regression for feature selection. The weights add ℓ2,1-norm regularization for local linear approximation. The sparse regression implicitly performs feature selection on the original features of data matrix and of the linear transform. We also provide an effective solution method to optimize the objective function. We apply the algorithm on six public image datasets including scene, face and object datasets. These experiments demonstrate the effectiveness of the proposed embedding method. They also show that proposed the method compares favorably with many competing embedding methods.}
}
@article{ECKLE2019232,
title = {A comparison of deep networks with ReLU activation function and linear spline-type methods},
journal = {Neural Networks},
volume = {110},
pages = {232-242},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303277},
author = {Konstantin Eckle and Johannes Schmidt-Hieber},
keywords = {Deep neural networks, Nonparametric regression, Splines, MARS, Faber–Schauder system, Rates of convergence},
abstract = {Deep neural networks (DNNs) generate much richer function spaces than shallow networks. Since the function spaces induced by shallow networks have several approximation theoretic drawbacks, this explains, however, not necessarily the success of deep networks. In this article we take another route by comparing the expressive power of DNNs with ReLU activation function to linear spline methods. We show that MARS (multivariate adaptive regression splines) is improper learnable by DNNs in the sense that for any given function that can be expressed as a function in MARS with M parameters there exists a multilayer neural network with O(Mlog(M∕ε)) parameters that approximates this function up to sup-norm error ε. We show a similar result for expansions with respect to the Faber–Schauder system. Based on this, we derive risk comparison inequalities that bound the statistical risk of fitting a neural network by the statistical risk of spline-based methods. This shows that deep networks perform better or only slightly worse than the considered spline methods. We provide a constructive proof for the function approximations.}
}
@article{ZENG201964,
title = {Classification of gait patterns between patients with Parkinson’s disease and healthy controls using phase space reconstruction (PSR), empirical mode decomposition (EMD) and neural networks},
journal = {Neural Networks},
volume = {111},
pages = {64-76},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300012},
author = {Wei Zeng and Chengzhi Yuan and Qinghui Wang and Fenglin Liu and Ying Wang},
keywords = {Parkinson’s diseases, Gait dynamics, Empirical mode decomposition (EMD), Phase space reconstruction (PSR), Euclidean distance (ED), Neural networks},
abstract = {Parkinson’s disease (PD) is a common neurodegenerative disorder that affects human’s quality of life, especially leading to locomotor deficits such as postural instability and gait disturbances. Gait signal is one of the best features to characterize and detect movement disorders caused by a malfunction in parts of the brain and nervous system of the patients with PD. Various classification approaches using spatiotemporal gait variables have been presented earlier to classify Parkinson’s gait. In this study we propose a novel method for gait pattern classification between patients with PD and healthy controls, based upon phase space reconstruction (PSR), empirical mode decomposition (EMD) and neural networks. First, vertical ground reaction forces (GRFs) at specific positions of human feet are captured and then phase space is reconstructed. The properties associated with the gait system dynamics are preserved in the reconstructed phase space. Three-dimensional (3D) PSR together with Euclidean distance (ED) has been used. These measured parameters demonstrate significant difference in gait dynamics between the two groups and have been utilized to form a reference variable set. Second, reference variables are decomposed into Intrinsic Mode Functions (IMFs) using EMD, and the third IMFs are extracted and served as gait features. Third, neural networks are then used as the classifier to distinguish between patients with PD and healthy controls based on the difference of gait dynamics preserved in the gait features between the two groups. Finally, experiments are carried out on 93 PD patients and 73 healthy subjects to assess the effectiveness of the proposed method. By using 2-fold, 10-fold and leave-one-out cross-validation styles, the correct classification rates are reported to be 91.46%, 96.99% and 98.80%, respectively. Compared with other state-of-the-art methods, the results demonstrate superior performance and the proposed method can serve as a potential candidate for the automatic and non-invasive classification between patients with PD and healthy subjects.}
}
@article{YI20198,
title = {Modulations of dendritic Ca2+ spike with weak electric fields in layer 5 pyramidal cells},
journal = {Neural Networks},
volume = {110},
pages = {8-18},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302983},
author = {Guosheng Yi and Xile Wei and Jiang Wang and Bin Deng and Yanqiu Che},
keywords = {Weak electric field, Layer 5 pyramidal cell, Dendritic  spike, Somato-dendritic coupling, Computational model},
abstract = {Weak electric fields (EFs) modulate input/output function of pyramidal cells. Dendritic Ca2+ spike is an important cellular mechanism for coupling synaptic inputs from different cortical layers, which plays a critical role in neuronal computation. This study aims to understand the effects of weak EFs on Ca2+ spikes initiated in the distal dendrites. We use a computational model to simulate dendritic Ca2+ spikes and backpropagating action potentials (APs) in layer 5 pyramidal cells. We apply uniform EFs (less than 20 mV/mm) to the model and examine how they affect the threshold for activation of Ca2+ spikes. We show that the effects of weak field on synaptically evoked Ca2+ spikes depend on the timing of synaptic inputs. When distal inputs coincide with the onset of EFs within a time window of several milliseconds, field-induced depolarization facilitates the initiation of Ca2+ spikes, while field-induced hyperpolarization suppresses dendritic APs. Sustained field-induced depolarization leads to the inactivation of Ca2+ channels and increases the threshold of Ca2+ spike. Sustained field-induced hyperpolarization de-inactivates Ca2+ channels and reduces the threshold of Ca2+ spike. By altering the threshold of backpropagation activated Ca2+ firing, field-induced depolarization increases the degree of coupling between inputs of the soma and distal dendrites, while field-induced hyperpolarization results in a decrease of coupling. The modulatory effects of weak EF are governed by the field direction with respect to the cell. Our study explains a fundamental link between field-induced polarization, dendritic Ca2+ spike, and somato-dendritic coupling. The findings are crucial to interpret how weak EFs achieve specific modulation of cellular activity.}
}
@article{CHEN2019170,
title = {Ensemble Neural Networks (ENN): A gradient-free stochastic method},
journal = {Neural Networks},
volume = {110},
pages = {170-185},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303319},
author = {Yuntian Chen and Haibin Chang and Jin Meng and Dongxiao Zhang},
keywords = {Inverse modeling, Gradient-free, Uncertainty quantification, Robust to small data size, Stochastic method},
abstract = {In this study, an efficient stochastic gradient-free method, the ensemble neural networks (ENN), is developed. In the ENN, the optimization process relies on covariance matrices rather than derivatives. The covariance matrices are calculated by the ensemble randomized maximum likelihood algorithm (EnRML), which is an inverse modeling method. The ENN is able to simultaneously provide estimations and perform uncertainty quantification since it is built under the Bayesian framework. The ENN is also robust to small training data size because the ensemble of stochastic realizations essentially enlarges the training dataset. This constitutes a desirable characteristic, especially for real-world engineering applications. In addition, the ENN does not require the calculation of gradients, which enables the use of complicated neuron models and loss functions in neural networks. We experimentally demonstrate benefits of the proposed model, in particular showing that the ENN performs much better than the traditional Bayesian neural networks (BNN). The EnRML in ENN is a substitution of gradient-based optimization algorithms, which means that it can be directly combined with the feed-forward process in other existing (deep) neural networks, such as convolutional neural networks (CNN) and recurrent neural networks (RNN), broadening future applications of the ENN.}
}
@article{2021I,
title = {Current Events},
journal = {Neural Networks},
volume = {133},
pages = {I},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(20)30406-8},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020304068}
}
@article{2021ii,
title = {Editorial Board},
journal = {Neural Networks},
volume = {137},
pages = {ii},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00102-7},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021001027}
}
@article{ZHAN201956,
title = {Unsupervised feature extraction by low-rank and sparsity preserving embedding},
journal = {Neural Networks},
volume = {109},
pages = {56-66},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302867},
author = {Shanhua Zhan and Jigang Wu and Na Han and Jie Wen and Xiaozhao Fang},
keywords = {Feature extraction, Sparse representation, Low-rank representation, Projection learning},
abstract = {Manifold based feature extraction has been proved to be an effective technique in dealing with the unsupervised classification tasks. However, most of the existing works cannot guarantee the global optimum of the learned projection, and they are sensitive to different noises. In addition, many methods cannot catch the discriminative information as much as possible since they only exploit the local structure of data while ignoring the global structure. To address the above problems, this paper proposes a novel graph based feature extraction method named low-rank and sparsity preserving embedding (LRSPE) for unsupervised learning. LRSPE attempts to simultaneously learn the graph and projection in a framework so that the global optimal projection can be obtained. Moreover, LRSPE exploits both global and local information of data for projection learning by imposing the low-rank and sparse constraints on the graph, which promotes the method to obtain a better performance. Importantly, LRSPE is more robust to noise by imposing the l2,1 sparsity norm on the reconstruction errors. Experimental results on both clean and noisy datasets prove that the proposed method can significantly improve classification accuracy and it is robust to different noises in comparison with the state-of-the-art methods.}
}
@article{2021II,
title = {INN/ENNS/JNNS - Membership Applic. Form},
journal = {Neural Networks},
volume = {139},
pages = {II},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00178-7},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021001787}
}
@article{2021I,
title = {Current Events},
journal = {Neural Networks},
volume = {137},
pages = {I},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00097-6},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021000976}
}
@article{TANNEBERG201967,
title = {Intrinsic motivation and mental replay enable efficient online adaptation in stochastic recurrent networks},
journal = {Neural Networks},
volume = {109},
pages = {67-80},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302909},
author = {Daniel Tanneberg and Jan Peters and Elmar Rueckert},
keywords = {Intrinsic motivation, Online learning, Experience replay, Autonomous robots, Spiking recurrent networks, Neural sampling},
abstract = {Autonomous robots need to interact with unknown, unstructured and changing environments, constantly facing novel challenges. Therefore, continuous online adaptation for lifelong-learning and the need of sample-efficient mechanisms to adapt to changes in the environment, the constraints, the tasks, or the robot itself are crucial. In this work, we propose a novel framework for probabilistic online motion planning with online adaptation based on a bio-inspired stochastic recurrent neural network. By using learning signals which mimic the intrinsic motivation signal cognitive dissonance in addition with a mental replay strategy to intensify experiences, the stochastic recurrent network can learn from few physical interactions and adapts to novel environments in seconds. We evaluate our online planning and adaptation framework on an anthropomorphic KUKA LWR arm. The rapid online adaptation is shown by learning unknown workspace constraints sample-efficiently from few physical interactions while following given way points.}
}
@article{2021II,
title = {INN/ENNS/JNNS - Membership Applic. Form},
journal = {Neural Networks},
volume = {135},
pages = {II},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00005-8},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021000058}
}
@article{KOU2019199,
title = {A compact network learning model for distribution regression},
journal = {Neural Networks},
volume = {110},
pages = {199-212},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303381},
author = {Connie Khor Li Kou and Hwee Kuan Lee and Teck Khim Ng},
keywords = {Supervised learning, Distribution regression},
abstract = {Despite the superior performance of deep learning in many applications, challenges remain in the area of regression on function spaces. In particular, neural networks are unable to encode function inputs compactly as each node encodes just a real value. We propose a novel idea to address this shortcoming: to encode an entire function in a single network node. To that end, we design a compact network representation that encodes and propagates functions in single nodes for the distribution regression task. Our proposed distribution regression network (DRN) achieves higher prediction accuracies while using fewer parameters than traditional neural networks.}
}
@article{2021II,
title = {INN/ENNS/JNNS - Membership Applic. Form},
journal = {Neural Networks},
volume = {133},
pages = {II},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(20)30408-1},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020304081}
}
@article{MERMILLOD201919,
title = {The importance of recurrent top-down synaptic connections for the anticipation of dynamic emotions},
journal = {Neural Networks},
volume = {109},
pages = {19-30},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302697},
author = {Martial Mermillod and Yannick Bourrier and Erwan David and Louise Kauffmann and Alan Chauvin and Nathalie Guyader and Frédéric Dutheil and Carole Peyrin},
keywords = {Predictive brain, Neural network modeling, Emotional facial expressions, Top-down processes},
abstract = {Different studies have shown the efficiency of a feed-forward neural network in categorizing basic emotional facial expressions. However, recent findings in psychology and cognitive neuroscience suggest that visual recognition is not a pure bottom-up process but likely involves top-down recurrent connectivity. In the present computational study, we compared the performances of a pure bottom-up neural network (a standard multi-layer perceptron, MLP) with a neural network involving recurrent top-down connections (a simple recurrent network, SRN) in the anticipation of emotional expressions. In two complementary simulations, results revealed that the SRN outperformed the MLP for ambiguous intensities in the temporal sequence, when the emotions were not fully depicted but when sufficient contextual information (related to previous time frames) was provided. Taken together, these results suggest that, despite the cost of recurrent connections in terms of energy and processing time for biological organisms, they can provide a substantial advantage for the fast recognition of uncertain visual signals.}
}
@article{MANUKIAN20191,
title = {Accelerating deep learning with memcomputing},
journal = {Neural Networks},
volume = {110},
pages = {1-7},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302971},
author = {Haik Manukian and Fabio L. Traversa and Massimiliano {Di Ventra}},
keywords = {Deep learning, Restricted Boltzmann machines, Memcomputing},
abstract = {Restricted Boltzmann machines (RBMs) and their extensions, often called “deep-belief networks”, are powerful neural networks that have found applications in the fields of machine learning and artificial intelligence. The standard way to train these models resorts to an iterative unsupervised procedure based on Gibbs sampling, called “contrastive divergence”, and additional supervised tuning via back-propagation. However, this procedure has been shown not to follow any gradient and can lead to suboptimal solutions. In this paper, we show an efficient alternative to contrastive divergence by means of simulations of digital memcomputing machines (DMMs) that compute the gradient of the log-likelihood involved in unsupervised training. We test our approach on pattern recognition using a modified version of the MNIST data set of hand-written numbers. DMMs sample effectively the vast phase space defined by the probability distribution of RBMs, and provide a good approximation close to the optimum. This efficient search significantly reduces the number of generative pretraining iterations necessary to achieve a given level of accuracy in the MNIST data set, as well as a total performance gain over the traditional approaches. In fact, the acceleration of the pretraining achieved by simulating DMMs is comparable to, in number of iterations, the recently reported hardware application of the quantum annealing method on the same network and data set. Notably, however, DMMs perform far better than the reported quantum annealing results in terms of quality of the training. Finally, we also compare our method to recent advances in supervised training, like batch-normalization and rectifiers, that seem to reduce the advantage of pretraining. We find that the memcomputing method still maintains a quality advantage (>1% in accuracy, corresponding to a 20% reduction in error rate) over these approaches, despite the network pretrained with memcomputing defines a more non-convex landscape using sigmoidal activation functions without batch-normalization. Our approach is agnostic about the connectivity of the network. Therefore, it can be extended to train full Boltzmann machines, and even deep networks at once.}
}
@article{KARIMPOULI201989,
title = {Image-based velocity estimation of rock using Convolutional Neural Networks},
journal = {Neural Networks},
volume = {111},
pages = {89-97},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S089360801830337X},
author = {Sadegh Karimpouli and Pejman Tahmasebi},
keywords = {Digital Rock Physics (DRP), Artificial intelligence, P- and S-wave velocities, HYPPS},
abstract = {Digital images of rock samples have been using extensively in Digital Rock Physics (DRP) to evaluate physical parameters of rock such as permeability, P- and S-wave velocities and formation factor. The parameters are numerically computed by simulation of the corresponding physical processes through segmented image of rock, which provide a direct and accurate evaluation of rock properties. However, recent advances in machine learning and Convolutional Neural Networks (CNN) allow using images as input. Such networks, however, require a considerable number of images as input. In this paper, CNNs are used to estimate the P- and S-wave velocities from images of rock medium. To deal with lack of input data, a hybrid pattern- and pixel-based simulation (HYPPS) is used as an efficient data augmentation method to increase the training data set. For each input image, 10 stochastic realizations are produced. Compare to the case wherein the stochastic models are not used, the new results from the enhanced network indicate a sharp improvement in the estimations such that R2 is increased to 0.94. Furthermore, the newly developed CNN network, unlike the one with the small data set (R2=0.75), manifests no over/underestimation. The estimated properties, in comparison with the computational results, indicate that CNNs perform outstandingly in predicting the physical parameters of rock without conducting any time-demanding forward modeling if enough input data are provided.}
}
@article{YAO201911,
title = {Prediction and identification of discrete-time dynamic nonlinear systems based on adaptive echo state network},
journal = {Neural Networks},
volume = {113},
pages = {11-19},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300140},
author = {Xianshuang Yao and Zhanshan Wang and Huaguang Zhang},
keywords = {Adaptive echo state network, Prediction and identification, Online learning algorithm, Discrete-time dynamic nonlinear systems},
abstract = {In this paper, a new prediction and identification method based on adaptive echo state network (AESN) is proposed to identify a class of discrete-time dynamic nonlinear systems (DDNS). Firstly, according to the characteristics of input signals, the reservoir state update equation of AESN can be adaptively adjusted. In order to guarantee the echo state property of AESN, a sufficient condition for echo state property is given. Secondly, the reservoir parameters of AESN are optimized to improve the identification and prediction performance of AESN. Thirdly, an improved online output weights learning method based on historical reservoir state and output error is given. Finally, the effectiveness of the proposed method is verified by simulation examples.}
}
@article{XU201977,
title = {NeuO: Exploiting the sentimental bias between ratings and reviews with neural networks},
journal = {Neural Networks},
volume = {111},
pages = {77-88},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303423},
author = {Yuanbo Xu and Yongjian Yang and Jiayu Han and En Wang and Fuzhen Zhuang and Jingyuan Yang and Hui Xiong},
keywords = {Opinion bias, Recommender systems, Convolutional neural network, Dual attention vectors},
abstract = {Traditional recommender systems rely on user profiling based on either user ratings or reviews through bi-sentimental analysis. However, in real-world scenarios, there are two common phenomena: (1) users only provide ratings for items but without detailed review comments. As a result, the historical transaction data available for recommender systems are usually unbalanced and sparse; (2) in many cases, users’ opinions can be better grasped in their reviews than ratings. For the reason that there is always a bias between ratings and reviews, it is really important that users’ ratings and reviews should be mutually reinforced to grasp the users’ true opinions. To this end, in this paper, we develop an opinion mining model based on convolutional neural networks for enhancing recommendation. Specifically, we exploit two-step training neural networks, which utilize both reviews and ratings to grasp users’ true opinions in unbalanced data. Moreover, we propose a Sentiment Classification scoring (SC) method, which employs dual attention vectors to predict the users’ sentiment scores of their reviews rather than using bi-sentiment analysis. Next, a combination function is designed to use the results of SC and user–item rating matrix to catch the opinion bias. It can filter the reviews and users, and build an enhanced user–item matrix. Finally, a Multilayer perceptron based Matrix Factorization (MMF) method is proposed to make recommendations with the enhanced user–item matrix. Extensive experiments on several real-world datasets (Yelp, Amazon, Taobao and Jingdong) demonstrate that (1) our approach can achieve a superior performance over state-of-the-art baselines; (2) our approach is able to tackle unbalanced data and achieve stable performances.}
}
@article{XU2019141,
title = {A density-based competitive data stream clustering network with self-adaptive distance metric},
journal = {Neural Networks},
volume = {110},
pages = {141-158},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303307},
author = {Baile Xu and Furao Shen and Jinxi Zhao},
keywords = {Unsupervised learning, Clustering methods, Competitive neural networks, Stream learning},
abstract = {Data stream clustering is a branch of clustering where patterns are processed as an ordered sequence. In this paper, we propose an unsupervised learning neural network named Density Based Self Organizing Incremental Neural Network(DenSOINN) for data stream clustering tasks. DenSOINN is a self organizing competitive network that grows incrementally to learn suitable nodes to fit the distribution of learning data, combining online unsupervised learning and topology learning by means of competitive Hebbian learning rule. By adopting a density-based clustering mechanism, DenSOINN discovers arbitrarily shaped clusters and diminishes the negative effect of noise. In addition, we adopt a self-adaptive distance framework to obtain good performance for learning unnormalized input data. Experiments show that the DenSOINN can achieve high standard performance comparing to state-of-the-art methods.}
}
@article{2021I,
title = {Current Events},
journal = {Neural Networks},
volume = {141},
pages = {I},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00288-4},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002884}
}
@article{2021ii,
title = {Editorial Board},
journal = {Neural Networks},
volume = {139},
pages = {ii},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00183-0},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021001830}
}
@article{2021I,
title = {Current Events},
journal = {Neural Networks},
volume = {139},
pages = {I},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00179-9},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021001799}
}
@article{CHEN201981,
title = {Fixed-time synchronization of inertial memristor-based neural networks with discrete delay},
journal = {Neural Networks},
volume = {109},
pages = {81-89},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S089360801830296X},
author = {Chuan Chen and Lixiang Li and Haipeng Peng and Yixian Yang},
keywords = {Inertial, Memristor, Neural networks, Feedback controllers, Fixed-time synchronization},
abstract = {This paper is concerned with the fixed-time synchronization control of inertial memristor-based neural networks with discrete delay. We design four different kinds of feedback controllers, under which the considered inertial memristor-based neural networks can realize fixed-time synchronization perfectly. Moreover, the obtained fixed-time synchronization criteria can be verified by algebraic operations. For any initial synchronization error, the settling time of fixed-time synchronization is bounded by a fixed constant, which can be calculated beforehand based on system parameters and controller parameters. Numerical simulations are given to illustrate the effectiveness of our theoretical results.}
}
@article{2020II,
title = {INN/ENNS/JNNS - Membership Applic. Form},
journal = {Neural Networks},
volume = {132},
pages = {II},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(20)30370-1},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020303701}
}
@article{2021II,
title = {INN/ENNS/JNNS - Membership Applic. Form},
journal = {Neural Networks},
volume = {137},
pages = {II},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00098-8},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021000988}
}
@article{2021I,
title = {Current Events},
journal = {Neural Networks},
volume = {142},
pages = {I},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00329-4},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021003294}
}
@article{ZHONG2019104,
title = {ADA-Tucker: Compressing deep neural networks via adaptive dimension adjustment tucker decomposition},
journal = {Neural Networks},
volume = {110},
pages = {104-115},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303010},
author = {Zhisheng Zhong and Fangyin Wei and Zhouchen Lin and Chao Zhang},
keywords = {Convolutional neural network, Compression, Tucker decomposition, Dimension adjustment},
abstract = {Despite recent success of deep learning models in numerous applications, their widespread use on mobile devices is seriously impeded by storage and computational requirements. In this paper, we propose a novel network compression method called Adaptive Dimension Adjustment Tucker decomposition (ADA-Tucker). With learnable core tensors and transformation matrices, ADA-Tucker performs Tucker decomposition of arbitrary-order tensors. Furthermore, we propose that weight tensors in networks with proper order and balanced dimension are easier to be compressed. Therefore, the high flexibility in decomposition choice distinguishes ADA-Tucker from all previous low-rank models. To compress more, we further extend the model to Shared Core ADA-Tucker (SCADA-Tucker) by defining a shared core tensor for all layers. Our methods require no overhead of recording indices of non-zero elements. Without loss of accuracy, our methods reduce the storage of LeNet-5 and LeNet-300 by ratios of 691× and 233 ×, respectively, significantly outperforming state of the art. The effectiveness of our methods is also evaluated on other three benchmarks (CIFAR-10, SVHN, ILSVRC12) and modern newly deep networks (ResNet, Wide-ResNet).}
}
@article{PARISI201954,
title = {Continual lifelong learning with neural networks: A review},
journal = {Neural Networks},
volume = {113},
pages = {54-71},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300231},
author = {German I. Parisi and Ronald Kemker and Jose L. Part and Christopher Kanan and Stefan Wermter},
keywords = {Continual learning, Lifelong learning, Catastrophic forgetting, Developmental systems, Memory consolidation},
abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.}
}
@article{2021ii,
title = {Editorial Board},
journal = {Neural Networks},
volume = {140},
pages = {ii},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00191-X},
url = {https://www.sciencedirect.com/science/article/pii/S089360802100191X}
}
@article{FERNANDEZDELGADO201911,
title = {An extensive experimental survey of regression methods},
journal = {Neural Networks},
volume = {111},
pages = {11-34},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303411},
author = {M. Fernández-Delgado and M.S. Sirsat and E. Cernadas and S. Alawadi and S. Barro and M. Febrero-Bande},
keywords = {Regression, UCI machine learning repository, Cubist, M5, Gradient boosted machine, Extremely randomized regression tree},
abstract = {Regression is a very relevant problem in machine learning, with many different available approaches. The current work presents a comparison of a large collection composed by 77 popular regression models which belong to 19 families: linear and generalized linear models, generalized additive models, least squares, projection methods, LASSO and ridge regression, Bayesian models, Gaussian processes, quantile regression, nearest neighbors, regression trees and rules, random forests, bagging and boosting, neural networks, deep learning and support vector regression. These methods are evaluated using all the regression datasets of the UCI machine learning repository (83 datasets), with some exceptions due to technical reasons. The experimental work identifies several outstanding regression models: the M5 rule-based model with corrections based on nearest neighbors (cubist), the gradient boosted machine (gbm), the boosting ensemble of regression trees (bstTree) and the M5 regression tree. Cubist achieves the best squared correlation ( R2) in 15.7% of datasets being very near to it, with difference below 0.2 for 89.1% of datasets, and the median of these differences over the dataset collection is very low (0.0192), compared e.g. to the classical linear regression (0.150). However, cubist is slow and fails in several large datasets, while other similar regression models as M5 never fail and its difference to the best R2 is below 0.2 for 92.8% of datasets. Other well-performing regression models are the committee of neural networks (avNNet), extremely randomized regression trees (extraTrees, which achieves the best R2 in 33.7% of datasets), random forest (rf) and ε-support vector regression (svr), but they are slower and fail in several datasets. The fastest regression model is least angle regression lars, which is 70 and 2,115 times faster than M5 and cubist, respectively. The model which requires least memory is non-negative least squares (nnls), about 2 GB, similarly to cubist, while M5 requires about 8 GB. For 97.6% of datasets there is a regression model among the 10 bests which is very near (difference below 0.1) to the best R2, which increases to 100% allowing differences of 0.2. Therefore, provided that our dataset and model collection are representative enough, the main conclusion of this study is that, for a new regression problem, some model in our top-10 should achieve R2 near to the best attainable for that problem.}
}
@article{SHI201947,
title = {Concept learning through deep reinforcement learning with memory-augmented neural networks},
journal = {Neural Networks},
volume = {110},
pages = {47-54},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.10.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303137},
author = {Jing Shi and Jiaming Xu and Yiqun Yao and Bo Xu},
keywords = {One-shot learning, Memory, Attention, Deep reinforcement learning, Neural networks},
abstract = {Deep neural networks have shown superior performance in many regimes to remember familiar patterns with large amounts of data. However, the standard supervised deep learning paradigm is still limited when facing the need to learn new concepts efficiently from scarce data. In this paper, we present a memory-augmented neural network which is motivated by the process of human concept learning. The training procedure, imitating the concept formation course of human, learns how to distinguish samples from different classes and aggregate samples of the same kind. In order to better utilize the advantages originated from the human behavior, we propose a sequential process, during which the network should decide how to remember each sample at every step. In this sequential process, a stable and interactive memory serves as an important module. We validate our model in some typical one-shot learning tasks and also an exploratory outlier detection problem. In all the experiments, our model gets highly competitive to reach or outperform those strong baselines.}
}
@article{2021ii,
title = {Editorial Board},
journal = {Neural Networks},
volume = {134},
pages = {ii},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(20)30436-6},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020304366}
}
@article{SIGAUD201928,
title = {Policy search in continuous action domains: An overview},
journal = {Neural Networks},
volume = {113},
pages = {28-40},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930022X},
author = {Olivier Sigaud and Freek Stulp},
keywords = {Policy search, Sample efficiency, Deep reinforcement learning, Deep neuroevolution},
abstract = {Continuous action policy search is currently the focus of intensive research, driven both by the recent success of deep reinforcement learning algorithms and the emergence of competitors based on evolutionary algorithms. In this paper, we present a broad survey of policy search methods, providing a unified perspective on very different approaches, including also Bayesian Optimization and directed exploration methods. The main message of this overview is in the relationship between the families of methods, but we also outline some factors underlying sample efficiency properties of the various approaches.}
}
@article{ZHAO20196,
title = {Neighborhood preserving neural network for fault detection},
journal = {Neural Networks},
volume = {109},
pages = {6-18},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302727},
author = {Haitao Zhao and Zhihui Lai},
keywords = {Statistical process monitoring, Fault detection, Feedforward neural network, Neighborhood preserving embedding},
abstract = {A novel statistical feature extraction method, called the neighborhood preserving neural network (NPNN), is proposed in this paper. NPNN can be viewed as a nonlinear data-driven fault detection technique through preserving the local geometrical structure of normal process data. The “local geometrical structure ” means that each sample can be constructed as a linear combination of its neighbors. NPNN is characterized by adaptively training a nonlinear neural network which takes the local geometrical structure of the data into consideration. Moreover, in order to extract uncorrelated and faithful features, NPNN adopts orthogonal constraints in the objective function. Through backpropagation and eigen decomposition (ED) technique, NPNN is optimized to extract low-dimensional features from original high-dimensional process data. After nonlinear feature extraction, Hotelling T2 statistic and the squared prediction error (SPE) statistic are utilized for the fault detection tasks. The advantages of the proposed NPNN method are demonstrated by both theoretical analysis and case studies on the Tennessee Eastman (TE) benchmark process. Extensive experimental results show the superiority of NPNN in terms of missed detection rate (MDR) and false alarm rate (FAR). The source code of NPNN can be found in https://github.com/htzhaoecust/npnn.}
}
@article{2021ii,
title = {Editorial Board},
journal = {Neural Networks},
volume = {133},
pages = {ii},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(20)30400-7},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020304007}
}
@article{ZHAO2019225,
title = {Research on a learning rate with energy index in deep learning},
journal = {Neural Networks},
volume = {110},
pages = {225-231},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S089360801830340X},
author = {Huizhen Zhao and Fuxian Liu and Han Zhang and Zhibing Liang},
keywords = {Deep learning, Convolutional neural network, Stochastic gradient algorithm, Learning rate, Energy index},
abstract = {The stochastic gradient descent algorithm (SGD) is the main optimization solution in deep learning. The performance of SGD depends critically on how learning rates are tuned over time. In this paper, we propose a novel energy index based optimization method (EIOM) to automatically adjust the learning rate in the backpropagation. Since a frequently occurring feature is more important than a rarely occurring feature, we update the features to different extents according to their frequencies. We first define an energy neuron model and then design an energy index to describe the frequency of a feature. The learning rate is taken as a hyperparameter function according to the energy index. To empirically evaluate the EIOM, we investigate different optimizers with three popular machine learning models: logistic regression, multilayer perceptron, and convolutional neural network. The experiments demonstrate the promising performance of the proposed EIOM compared with that of other optimization algorithms.}
}
@article{KAWAI201915,
title = {A small-world topology enhances the echo state property and signal propagation in reservoir computing},
journal = {Neural Networks},
volume = {112},
pages = {15-23},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300115},
author = {Yuji Kawai and Jihoon Park and Minoru Asada},
keywords = {Small-world network, Echo state network, Memory capacity, Complex network, Connectome},
abstract = {Cortical neural connectivity has been shown to exhibit a small-world (SW) network topology. However, the role of the topology in neural information processing remains unclear. In this study, we investigated the learning performance of an echo state network (ESN) that includes the SW topology as a reservoir. To elucidate the potential of the SW topology, we limited the numbers of the input and output nodes in the ESN and spatially segregated the output nodes from the input nodes. We tested the ESNs in two benchmark tasks: memory capacity and nonlinear time-series prediction. The SW-ESN exhibited the best learning performance when the spectral radius of the weight matrix was large and when the input and output nodes were segregated. That is, the SW topology provided the ESN with a stable echo state property over a broad range of the weight matrix and efficiently propagated input signals to the output nodes. This result is the same as that of the ESN using a real human cortical connectivity. Thus, the results suggest that the SW topology is essential for maintaining the echo state property, which is the appropriate neural dynamics between input and output brain regions.}
}
@article{2021I,
title = {Current Events},
journal = {Neural Networks},
volume = {138},
pages = {I},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00136-2},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021001362}
}
@article{2021II,
title = {INN/ENNS/JNNS - Membership Applic. Form},
journal = {Neural Networks},
volume = {142},
pages = {II},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00330-0},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021003300}
}
@article{2021I,
title = {Current Events},
journal = {Neural Networks},
volume = {136},
pages = {I},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00060-5},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021000605}
}
@article{WANG201941,
title = {A robust outlier control framework for classification designed with family of homotopy loss function},
journal = {Neural Networks},
volume = {112},
pages = {41-53},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300243},
author = {Yidan Wang and Liming Yang and Chao Yuan},
keywords = {Homotopy loss, Robustness classification, Fisher consistency, Re-weighted least square algorithm},
abstract = {We propose a new homotopy loss, where practitioners can tune the parameter to derive different loss functions such as l1-norm loss, logarithmic loss, Geman–Reynolds loss, Geman–McClure loss and correntropy-based loss. Moreover, we illustrate that the proposed loss satisfies Fisher consistency, and we analyze the robustness of the proposed homotopy loss from different perspectives: M-estimation and adversarial perturbations. Then, we represent a new evaluation standard to measure robustness and demonstrate its upper bound to ensure the validity of this measure. Applied the proposed homotopy loss to least square support vector machine (LSSVM) and the extreme learning machine (ELM), two robust models are presented to enhance the robustness. Furthermore, re-weighted least square algorithm is used to solve the problems, and the resulting algorithms converge globally. In addition, the proposed methods are implemented on various datasets with different levels of noise. Compared with traditional methods, experiment results on real-world datasets show that the proposed methods have superior anti-interference ability to outliers in most cases.}
}
@article{WU201991,
title = {Heterogeneity of synaptic input connectivity regulates spike-based neuronal avalanches},
journal = {Neural Networks},
volume = {110},
pages = {91-103},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303125},
author = {Shengdun Wu and Yangsong Zhang and Yan Cui and Heng Li and Jiakang Wang and Lijun Guo and Yang Xia and Dezhong Yao and Peng Xu and Daqing Guo},
keywords = {Self-organized activity, Neural avalanche, Structural heterogeneity, Criticality},
abstract = {Our mysterious brain is believed to operate near a non-equilibrium point and generate critical self-organized avalanches in neuronal activity. A central topic in neuroscience is to elucidate the underlying circuitry mechanisms of neuronal avalanches in the brain. Recent experimental evidence has revealed significant heterogeneity in both synaptic input and output connectivity, but whether the structural heterogeneity participates in the regulation of neuronal avalanches remains poorly understood. By computational modeling, we predict that different types of structural heterogeneity contribute distinct effects on avalanche neurodynamics. In particular, neuronal avalanches can be triggered at an intermediate level of input heterogeneity, but heterogeneous output connectivity cannot evoke avalanche dynamics. In the criticality region, the co-emergence of multi-scale cortical activities is observed, and both the avalanche dynamics and neuronal oscillations are modulated by the input heterogeneity. Remarkably, we show similar results can be reproduced in networks with various types of in- and out-degree distributions. Overall, these findings not only provide details on the underlying circuitry mechanisms of nonrandom synaptic connectivity in the regulation of neuronal avalanches, but also inspire testable hypotheses for future experimental studies.}
}
@article{KIM20191,
title = {Response prediction of nonlinear hysteretic systems by deep neural networks},
journal = {Neural Networks},
volume = {111},
pages = {1-10},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303368},
author = {Taeyong Kim and Oh-Sung Kwon and Junho Song},
keywords = {Deep learning, Convolutional neural network, Hysteretic behavior, Stochastic excitation, Nonlinear time history analysis, Single degree of freedom system (SDOF)},
abstract = {Nonlinear hysteretic systems are common in many engineering problems. The maximum response estimation of a nonlinear hysteretic system under stochastic excitations is an important task for designing and maintaining such systems. Although a nonlinear time history analysis is the most rigorous method to accurately estimate the responses in many situations, high computational costs and modelingtime hamper adoption of the approach in a routine engineering practice. Thus, various simplified regression equations are often introduced to replace a nonlinear time history analysis in engineering practices, but the accuracy of the estimated responses is limited. This paper proposes a deep neural network trained by the results of the nonlinear time history analyses as an alternative of such simplified regression equations. To this end, a convolutional neural network (CNN) which is usually applied to abstract features from visual imagery is introduced to analyze the information of the hysteretic behavior of the system, then, merged with neural networks representing the stochastic random excitation to predict the responses of a nonlinear hysteretic system. For verification, the proposed deep neural network is applied to the earthquake engineering area to predict the structural responses under earthquake excitations. The results confirm that the proposed deep neural network provides a superior performance compared to the simplified regression equations which are developed based on a limited dataset. Moreover, to give an insight of the proposed deep neural network, the extracted features from the deep neural network are investigated with various numerical examples. The method is expected to enable engineers to effectively predict the response of the hysteretic system without performing nonlinear time history analyses, and provide a new prospect in the relevant engineering fields. The supporting source code and data are available for download at https://github.com/TyongKim/ERD2.}
}
@article{WAGATSUMA201933,
title = {Saliency model based on a neural population for integrating figure direction and organizing Border Ownership},
journal = {Neural Networks},
volume = {110},
pages = {33-46},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303009},
author = {Nobuhiko Wagatsuma},
keywords = {Saliency map, Attentional selection, Border Ownership, Neural population, Figure-ground segregation},
abstract = {Attentional selection is a function of the brain that allocates computational resources momentarily to the most important part of a visual scene. Saliency map models have been used to predict the location of attentional selection and gaze. Border Ownership (BO) indicates the direction of the figure with respect to the border. I here propose a biologically plausible saliency model based on neural population for integrating the activities of intermediate-level visual areas with neurons selective to BO. A variety of BO organizations produces a population of model neurons that represent the grouping structure. In the model I propose, the interactions and the population responses of these model neurons underlie the determination of saliency and the accurate prediction of gaze location. I tested 100 patterns for BO organizations and found that the proposed saliency model not only reproduced the characteristics of perceptual organization but also captured object locations in natural images. Furthermore, the saliency model based on the population responses of the BO organization significantly improved the gaze prediction accuracy compared with previous saliency-based models. These results suggest a crucial role for a wide variety of BO organizations and neural population coding to determine saliency mediating attentional selection and to predict gaze location.}
}
@article{LIU2019147,
title = {A neurodynamic approach to nonlinear optimization problems with affine equality and convex inequality constraints},
journal = {Neural Networks},
volume = {109},
pages = {147-158},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302958},
author = {Na Liu and Sitian Qin},
keywords = {Nonlinear optimization problems, Recurrent neural network, Lyapunov function, Global convergence},
abstract = {This paper presents a neurodynamic approach to nonlinear optimization problems with affine equality and convex inequality constraints. The proposed neural network endows with a time-varying auxiliary function, which can guarantee that the state of the neural network enters the feasible region in finite time and remains there thereafter. Moreover, the state with any initial point is shown to be convergent to the critical point set when the objective function is generally nonconvex. Especially, when the objective function is pseudoconvex (or convex), the state is proved to be globally convergent to an optimal solution of the considered optimization problem. Compared with other neural networks for related optimization problems, the proposed neural network in this paper has good convergence and does not depend on some additional assumptions, such as the assumption that the inequality feasible region is bounded, the assumption that the penalty parameter is sufficiently large and the assumption that the objective function is lower bounded over the equality feasible region. Finally, some numerical examples and an application in real-time data reconciliation are provided to display the well performance of the proposed neural network.}
}
@article{KHAN201982,
title = {Regularization of deep neural networks with spectral dropout},
journal = {Neural Networks},
volume = {110},
pages = {82-90},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302715},
author = {Salman H. Khan and Munawar Hayat and Fatih Porikli},
keywords = {Regularization, Spectral analysis, Image classification, Deep learning},
abstract = {The big breakthrough on the ImageNet challenge in 2012 was partially due to the ‘Dropout’ technique used to avoid overfitting. Here, we introduce a new approach called ‘Spectral Dropout’ to improve the generalization ability of deep neural networks. We cast the proposed approach in the form of regular Convolutional Neural Network (CNN) weight layers using a decorrelation transform with fixed basis functions. Our spectral dropout method prevents overfitting by eliminating weak and ‘noisy’ Fourier domain coefficients of the neural network activations, leading to remarkably better results than the current regularization methods. Furthermore, the proposed is very efficient due to the fixed basis functions used for spectral transformation. In particular, compared to Dropout and Drop-Connect, our method significantly speeds up the network convergence rate during the training process (roughly ×2), with considerably higher neuron pruning rates (an increase of ∼30%). We demonstrate that the spectral dropout can also be used in conjunction with other regularization approaches resulting in additional performance gains.}
}
@article{DOYA2020iii,
title = {Announcement of the Neural Networks Best Paper Award},
journal = {Neural Networks},
volume = {132},
pages = {iii},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(20)30384-1},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020303841},
author = {Kenji Doya and DeLiang Wang}
}
@article{2021ii,
title = {Editorial Board},
journal = {Neural Networks},
volume = {138},
pages = {ii},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00141-6},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021001416}
}
@article{2021II,
title = {INN/ENNS/JNNS - Membership Applic. Form},
journal = {Neural Networks},
volume = {138},
pages = {II},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00137-4},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021001374}
}
@article{2021I,
title = {Current Events},
journal = {Neural Networks},
volume = {135},
pages = {I},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00004-6},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021000046}
}