@article{ZHANG2019181,
title = {A fusion method of 1D laser and vision based on depth estimation for pose estimation and reconstruction},
journal = {Robotics and Autonomous Systems},
volume = {116},
pages = {181-191},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S092188901830873X},
author = {Zhuang Zhang and Rujin Zhao and Enhai Liu and Kun Yan and Yuebo Ma and Yunze Xu},
abstract = {This paper presents a method that fuses a 1D laser range finder and monocular camera to restore unknown 3D structure and 6 degree-of-freedom camera poses. This method can overcome a known deficiency of the absolute scale of monocular cameras and avoid the baseline limitation of stereo vision to obtain a more robust result. The theoretical logic of the fusion of the two sensors is analyzed in detail, and the effectiveness of our method is demonstrated by simulation and experiments. The influential factors related to the measurement and reconstruction accuracy are analyzed through simulation, and the validity of the proposed method is verified by comparing the monocular and RGBD methods on open datasets and observational experiments.}
}
@article{PAOLANTI2019179,
title = {Robotic retail surveying by deep learning visual and textual data},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {179-188},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.021},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018304548},
author = {Marina Paolanti and Luca Romeo and Massimo Martini and Adriano Mancini and Emanuele Frontoni and Primo Zingaretti},
keywords = {Robotic vision, Shelf out of stock, Deep learning, Real time localisation system, Autonomous store mapping},
abstract = {Autonomous systems for monitoring and surveying are increasingly used in retail stores, since they improve the overall performance of the store and reduce the manpower cost. Moreover, an automated system improves the accuracy of collected data by avoiding human-related factors. This paper presents ROCKy, a mobile robot for data collection and surveying in a retail store that autonomously navigates and monitors store shelves based on real-time store heat maps; ROCKy is designed to automatically detect Shelf Out of Stock (SOOS) and Promotional Activities (PA) based on Deep Convolutional Neural Networks (DCNNs). The deep learning approach evaluates visual and textual content of an image simultaneously to classify and map SOOS and PA events during working hours. The proposed approach was applied and tested on several real scenarios, presenting a new public dataset with more than 14.000 annotated shelves images. Experimental results confirmed the effectiveness of the approach, showing high accuracy (up to 87%) in comparison with the existing state of the art SOOS and PA monitoring solutions, and a signification reduction of retail surveying time (45%).}
}
@article{LEVIN2019148,
title = {Agile maneuvering with a small fixed-wing unmanned aerial vehicle},
journal = {Robotics and Autonomous Systems},
volume = {116},
pages = {148-161},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018304305},
author = {Joshua M. Levin and Aditya A. Paranjape and Meyer Nahon},
keywords = {Fixed-wing, Trajectory generation, Unmanned aerial vehicles, Optimal control, Dynamic time warping},
abstract = {This paper presents a general and systematic approach to automating a variety of agile maneuvers with a small fixed-wing unmanned aerial vehicle. The methodology begins by numerically solving optimal control problems off-line to generate a small set of reference trajectories and feedforward control inputs for maneuvers. A dynamic time warping-based interpolation process parametrizes these solutions, adding robustness to the maneuver, whilst allowing the on-board library of state and control time histories to remain compact. To handle errors, inaccuracies, noise, and disturbances that are not accounted for by feedforward control, feedback control laws stabilize about the reference trajectories. The work focuses mainly on one agile maneuver: an aggressive turn-around, in which the aircraft undergoes a rapid 180 degree heading reversal, beginning and ending in straight and level flight. To establish the generality of the methodology, it is also applied to transition maneuvers between straight and level flight, and a nose-up hover. The proposed automation scheme is computationally light during flight, consisting of a simple feedforward/feedback controller coupled to a compact library of maneuvers that are optimized over the full flight envelope. The methodology is validated in simulations and flight tests. The automation scheme is implemented successfully on a small fixed-wing unmanned aerial vehicle.}
}
@article{OKAMOTO2019155,
title = {Data-driven human driver lateral control models for developing haptic-shared control advanced driver assist systems},
journal = {Robotics and Autonomous Systems},
volume = {114},
pages = {155-171},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018302008},
author = {Kazuhide Okamoto and Panagiotis Tsiotras},
keywords = {Advanced driver assist system, Human driver control model, Driving simulator study, Driver–vehicle interaction, Unknown parameter and input estimation},
abstract = {In order to improve road safety, many advanced driver assist systems (ADAS) have been developed to support human-decision making and reduce driver workload. Currently, the majority of ADAS employ a single, often very simple, driver model to predict human-driver interaction in the immediate future (e.g., next few seconds). However, there is tremendous variability in how each individual drives, necessitating personalized driver models, based on data collected from observed actual driver actions. Yet, because we currently lack sufficient knowledge of the high-level cognitive brain functions, traditional control-theoretic driver models have difficulty accurately predicting driver actions. Recently, machine-learning algorithms have been utilized to predict future driver control actions. We compare several of these algorithms used to predict the lateral control actions of human drivers. Specifically, we compare these algorithms in terms of their suitability to develop haptic-shared ADAS, which share the control force with the human driver. To this end, we need to know how the steering torque is provided by the driver. However, low-cost driving simulators typically measure steering angle but not steering torque. Thus, this work also proposes a methodology to estimate the steering-wheel torque. Using the estimated steering torque, we train several machine learning driver control models and compare the performance using both simulated and real human-driving data sets.}
}
@article{DUPEYROUX201940,
title = {An ant-inspired celestial compass applied to autonomous outdoor robot navigation},
journal = {Robotics and Autonomous Systems},
volume = {117},
pages = {40-56},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S092188901830263X},
author = {Julien Dupeyroux and Stéphane Viollet and Julien R. Serres},
keywords = {Non-conventional vision, Optic flow, Hexapod, Homing, Odometry, Multiple sensory fusion, Bio-inspiration, Biomimetics, Bionics, Biorobotics},
abstract = {Desert ants use the polarization of skylight and a combination of stride and ventral optic flow integration processes to track the nest and food positions when traveling, achieving outstanding performances. Navigation sensors such as global positioning systems and inertial measurement units still have disadvantages such as their low resolution and drift. Taking our inspiration from ants, we developed a 2-pixel celestial compass which computes the heading angle of a mobile robot in the ultraviolet range. The output signals obtained with this optical compass were investigated under various weather and ultraviolet conditions and compared with those obtained on a magnetometer in the vicinity of our laboratory. After being embedded on-board the robot, the sensor was first used to compensate for random yaw disturbances. We then used the compass to keep the Hexabot robot’s heading angle constant in a straight forward walking task over a flat terrain while its walking movements were imposing yaw disturbances. Experiments performed under various meteorological conditions showed the occurrence of steady state heading angle errors ranging from 0.3∘ (with a clear sky) to 2.9∘ (under changeable sky conditions). The compass was also tested under canopies and showed a strong ability to determine the robot’s heading while most of the sky was hidden by the foliage. Lastly, a waterproof, mono-pixel version of the sensor was designed and successfully tested in a preliminary underwater benchmark test. These results suggest this new optical compass shows great precision and reliability in a wide range of outdoor conditions, which makes it highly suitable for autonomous robotic outdoor navigation tasks. A celestial compass and a minimalistic optic flow sensor called M2APix (based on Michaelis–Menten Auto-adaptive Pixels) were therefore embedded on-board our latest insectoid robot called AntBot, to complete the previously mentioned ant-like homing navigation processes. First the robot was displaced manually and made to return to its starting-point on the basis of its absolute knowledge of the coordinates of this point. Lastly, AntBot was tested in fully autonomous navigation experiments, in which it explored its environment and then returned to base using the same sensory modes as those on which desert ants rely. AntBot produced robust, precise localization performances with a homing error as small as 0.7% of the entire trajectory.}
}
@article{ZHANG2019189,
title = {Planning coordinated motions for tethered planar mobile robots},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {189-203},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018309710},
author = {Xu Zhang and Quang-Cuong Pham},
keywords = {Multi-robot motion planning, Tethered robots},
abstract = {This paper considers the motion planning problem for multiple tethered planar mobile robots. Each robot is attached to a fixed base by a flexible cable. Since the robots share a common workspace, the interactions amongst the robots, cables, and obstacles pose significant difficulties for planning. Previous works have studied the problem of detecting whether a target cable configuration is intersecting (or entangled). Here, we are interested in the motion planning problem: how to plan and coordinate the robot motions to realize a given non-intersecting target cable configuration. We identify four possible modes of motion, depending on whether (i) the robots move in straight lines or following their cable lines; (ii) the robots move sequentially or concurrently. We present an in-depth analysis of Straight/Concurrent, which is the most practically-interesting mode of motion. In particular, we propose algorithms that (a) detect whether a given target cable configuration is realizable by a Straight/Concurrent motion, and (b) return a valid coordinated motion plan. The algorithms are analyzed in detail and validated in simulations and in a hardware experiment.}
}
@article{SCHUMACHER2019185,
title = {An introductory review of active compliant control},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {185-200},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018307772},
author = {Marie Schumacher and Janis Wojtusch and Philipp Beckerle and Oskar {von Stryk}},
keywords = {Compliant control, Impedance control, Admittance control, Hybrid force/position control, Parallel force/position control, Force control},
abstract = {Active compliant control enables to quickly and freely adjust the properties and dynamic behavior of interactions of mechanisms within certain limits. According to the emerging applications in many robotic fields and related areas, the number of publications has also strongly increased. This paper meets the need for a recent comprehensive review, including a profound and concise characterization and classification of compliant control approaches extending the basic concepts, hybrid and parallel force/position, impedance and admittance control, by a survey of their variants and combinations. It mainly focuses on individually operating, stiff, non-redundant systems. Unlike previous reviews, this work is based on a transparent and systematic literature search methodology, which can easily be adapted or updated by any reader, hence remaining enduringly up-to-date over time. Also, a novel selection scheme is proposed, which facilitates the choice of appropriate control approaches for given requirements, particularly for newcoming researchers to the field.}
}
@article{QIAN201947,
title = {Pillar Networks: Combining parametric with non-parametric methods for action recognition},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {47-54},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018302483},
author = {Yu Qian and Biswa Sengupta},
keywords = {Action recognition, dCNN, Multi-kernel SVM, Gaussian process},
abstract = {Image understanding using deep convolutional network has reached human-level performance, yet the closely related problem of video understanding, especially action recognition, has not reached the same required level of maturity. As a solution we propose two independent architectures for action recognition using meta-classifiers – the first is based on combining kernels of support-vector-machines (SVM) and the second is based on distributed Gaussian Processes (GP). Both receive features that are computed using a multi-stream deep convolutional neural network, enabling the achievement of state-of-the-art performance on a 51 and a 101-class action recognition problem (HMDB-51/UCF-101 dataset). We have named the resulting architecture ‘pillar networks’ as each (very) deep neural network acts as a pillar for the meta-classifiers. In addition, we illustrate that hand-crafted features such as the improved dense trajectories (iDT) and Multi-skip Feature Stacking (MIFS), when used as additional pillars, can further supplement the performance.}
}
@article{YANG201980,
title = {A distributed and parallel self-assembly approach for swarm robotics},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {80-92},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018307899},
author = {Hong-an Yang and Shuai Cao and Luoyu Bai and Zhaoqi Zhang and Jie Kong},
keywords = {Swarm robotics, Self-assembly, Stratified mechanism, Motion-chain, Parallel planning},
abstract = {This paper presents a novel distributed and parallel self-assembly approach, which uses the lattice system as a systematic structure and homogeneous robots as shaping carriers to form a two-dimensional user-specified shape autonomously. Given a desired shape to be formed, the initial shape matches with it to execute the initialization of all individuals to allow each of them gets its location and status. Based on that, with the stratified mechanism, the macro-level behavior of large-scale group formation in swarm robotics is transformed to local formation action of individuals within the edge layers of the current aggregate, which makes complex shape formation possible. Then two motion-chains, a collection of individuals that have priority to move currently, are autonomously planning in parallel through local interaction and collaboration. Once all robots within a motion-chain are activated, each of them will move along the outer edge of the current aggregate orderly to fill the edge-filling layer efficiently. The motion-chains will be iteratively generated until the desired shape is formed. We evaluate the feasibility and scalability of this novel approach in simulation-based experiments, and implement the self-assembly algorithm on the Rubik robot, a hardware system developed in our lab.}
}
@article{TERUEL201951,
title = {A distributed robot swarm control for dynamic region coverage},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {51-63},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018309436},
author = {Enrique Teruel and Rosario Aragues and Gonzalo López-Nicolás},
keywords = {Autonomous robots, Swarm, Coverage, Tracking},
abstract = {We propose a new distributed method for coverage of a moving deformable convex region with a team of robots in a communication network. Robots execute a distributed self-deployment strategy based on Centroidal Voronoi Tessellations (CVT) to cover the region evenly while preventing collisions. The main contribution is the addition of a feedforward action to overcome the well-known slow convergence issue of the basic CVT algorithms. This action is derived by each robot from the information about the region that floods through the network from a few selected leaders. The method allows to quickly adapt to the fastly changing working area in spite of the light communication requirements, and it is well suited for large teams of expendable robots.}
}
@article{CHEN201931,
title = {A distributed method for dynamic multi-robot task allocation problems with critical time constraints},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {31-46},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018308182},
author = {Xinye Chen and Ping Zhang and Guanglong Du and Fang Li},
keywords = {Adaptive system, Distributed task allocation, Multi-robot system, Overall objective},
abstract = {This paper considers the task allocation problems in a distributed multi-robot system under critical time constraints. Considering the requirement of distributed computing, many existing distributed heuristic task allocation approaches tend to trap in local optimal and cannot obtain high-quality solutions. For a dynamic task allocation problem in a multi-robot system, not only the task information and the robot state may be subject to changes, but also the network status. That is, robots that each robot can communicate with may change over time, and sometimes there may even be no robots that it can communicate with. To solve these problems, a dynamic grouping allocation method is proposed. It builds upon the state-of-the-art consensus-based auction algorithms, extending them in both task inclusion phase and consensus phase. First, a cluster-first strategy and a task inclusion procedure that can be easily applied to the task inclusion phase of the algorithms are proposed, so that the solution quality of each iteration of the algorithms are significantly improved with a reasonable amount of computation. In addition, to increase the exploration capabilities, a proportional selection method is used in the task inclusion procedure when it is likely to trap in a local optimal. Second, the block-information-sharing strategy is used to avoid the possible conflicts that dynamic changes may bring. Numerical simulations demonstrate that the proposed method can provide conflict-free solutions in dynamic environments and can achieve outstanding performance in comparison with the state-of-the-art algorithms.}
}
@article{BAYOUMI201940,
title = {Speeding up person finding using hidden Markov models},
journal = {Robotics and Autonomous Systems},
volume = {115},
pages = {40-48},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018306043},
author = {AbdElMoniem Bayoumi and Philipp Karkowski and Maren Bennewitz},
keywords = {Modeling human motion, Motion prediction, Hidden Markov models, Information gain based search},
abstract = {The ability of mobile service robots to efficiently search for a person is needed in a vast domain of applications. The search problem is especially challenging when the user is freely moving across the environment, the robot has only a constrained field of view, and visibility constraints arise from the environment. We propose in this article a novel approach that simulates the user’s presence at different locations in the environment based on a hidden Markov model (HMM). The HMM predicts the user’s motion and computes the observability likelihood at the different locations given the predictions. Our approach then selects effective search locations that maximize the user’s expected observability. The selection criterion hereby considers the visibility constraints along the robot’s path as well as the robot’s travel time to reach the search location. We performed both real-world and extensive simulated experiments to evaluate our method. In comparison to a greedy maximum coverage approach as well as to a greedy strategy that uses background information, we show that our framework leads to a significant reduction of the time needed to find the user.}
}
@article{TANG2019247,
title = {DCBot: An autonomous hot-line working robot for 110 kV substation},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {247-262},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018308340},
author = {Mingdong Tang and Youlin Gu and Shigang Wang and Qinghua Liang},
keywords = {Hot-line working robotics, Extreme environments, Machine vision, Manipulator},
abstract = {This paper presents the design of DCBot, a hot-line working robot that can replace workers to assemble and disassemble the connect fittings of the disconnecting circuit breaker (DCB) in 110 kV substations without needing to be powered off. Robotic assembly in electrical substations requires a robot system with high flexibility and the capability to deal with uncertainties. A robust mobile manipulation system is a new and promising technology that could overcome these challenges. The highlights of our work include (1) a reference for designing hot-line working robots, which can not only guarantee the electrical safety clearance but also minimize the robots’ influence on the electric field; (2) an electromagnetic shielding method applicable to the electrical components on the equipotential operation platform; (3) a method for precise outdoor visual positioning for objects with worn surfaces in outdoor environments; (4) a control strategy for hole-searching on complex contact surfaces during the assembly of connect fittings; and (5) a method of planning the safe working space of the hot-line working robot. The experiments and the field tests carried out in 110 kV energized substation environments show that the electrical safety of the substation was guaranteed during the whole process of operation. The success rate of 3 times assembly and 3 times disassembly is 100%.}
}
@article{LLOPHARILLO2019103259,
title = {The Anthropomorphic Hand Assessment Protocol (AHAP)},
journal = {Robotics and Autonomous Systems},
volume = {121},
pages = {103259},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.103259},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019300946},
author = {Immaculada Llop-Harillo and Antonio Pérez-González and Julia Starke and Tamim Asfour},
keywords = {Assessment, Benchmark, Grasping, Prosthetic hand, Robotic hand, Test protocol},
abstract = {The progress in the development of anthropomorphic hands for robotic and prosthetic applications has not been followed by a parallel development of objective methods to evaluate their performance. The need for benchmarking in grasping research has been recognized by the robotics community as an important topic. In this study we present the Anthropomorphic Hand Assessment Protocol (AHAP) to address this need by providing a measure for quantifying the grasping ability of artificial hands and comparing hand designs. To this end, the AHAP uses 25 objects from the publicly available Yale-CMU-Berkeley Object and Model Set thereby enabling replicability. It is composed of 26 postures/tasks involving grasping with the eight most relevant human grasp types and two non-grasping postures. The AHAP allows to quantify the anthropomorphism and functionality of artificial hands through a numerical Grasping Ability Score (GAS). The AHAP was tested with different hands, the first version of the hand of the humanoid robot ARMAR-6 with three different configurations resulting from attachment of pads to fingertips and palm as well as the two versions of the KIT Prosthetic Hand. The benchmark was used to demonstrate the improvements of these hands in aspects like the grasping surface, the grasp force and the finger kinematics. The reliability, consistency and responsiveness of the benchmark have been statistically analyzed, indicating that the AHAP is a powerful tool for evaluating and comparing different artificial hand designs.}
}
@article{DENG2019103240,
title = {A Compiler for Scalable Construction by the TERMES Robot Collective},
journal = {Robotics and Autonomous Systems},
volume = {121},
pages = {103240},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019301897},
author = {Yawen Deng and Yiwen Hua and Nils Napp and Kirstin Petersen},
keywords = {Multi-robot systems, Assembly, Construction, Autonomy, Compiler},
abstract = {The TERMES system is a robot collective capable of autonomous construction of 3D user-specified structures. A key component of the framework is an off-line compiler which takes in a structure blueprint and generates a directed map, in turn permitting an arbitrary number of robots to perform decentralized construction in a provably correct manner. In past work, this compiler was limited to a non-optimized search approach which scaled poorly with the structure size. Here, we first recast the process as a constraint satisfaction problem (CSP) to apply well-known optimizations for solving CSP and present new scalable compiler schemes and the ability to quickly generate provably correct maps (or find that none exist) of structures with up to 1 million bricks. We compare the performance of the compilers on a range of structures, and show how the completion time is related to the inter-dependencies between built locations. Second, we show how the transition probability between locations in the structure affect assembly time. While the exact solution for the expected completion time is difficult to compute, we evaluate different objective functions for the transition probabilities and show that these optimizations can drastically improve overall efficiency. This work represents an important step towards collective robotic construction of real-world structures.}
}
@article{BRAHMI201992,
title = {Compliant adaptive control of human upper-limb exoskeleton robot with unknown dynamics based on a Modified Function Approximation Technique (MFAT)},
journal = {Robotics and Autonomous Systems},
volume = {117},
pages = {92-102},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018307802},
author = {Brahim Brahmi and Mohamed Hamza Laraki and Maarouf Saad and M.H. Rahman and Cristobal Ochoa-Luna and Abdelkrim Brahmi},
keywords = {Rehabilitation robots, Function Approximation Technique, Active assistive motion, Human inverse kinematics},
abstract = {Rehabilitation robots have shown a high potential for improving the patients’ mobility, improving their functional movements and assisting in daily activities. However, this technology is still an emerging field and suffers from several challenges like compliance control and dynamic uncertain caused by the human–robot collaboration. The main challenge addressed in this paper is to ensure that the exoskeleton robot provides a suitable compliance control that allows it to cooperate perfectly with humans even if the dynamic model of the exoskeleton robot is uncertain. To achieve that, an adaptive tracking controller based on Modified Function Approximation Technique (FAT) is proposed to approximate the dynamic model of the exoskeleton robot. Unlike a conventional FAT, the required use of basis functions in estimations law of dynamic model and the acceleration feedback is eliminated in the proposed modified FAT. Additionally, the desired trajectory is designed based on the designer’s prediction of the motion intention of the human, using the Damped Least Square method in order to reduce the error between the actual position of the robot and the motion intention of the human which help the subject move the exoskeleton arm easily in active rehabilitation tasks. The stability analysis is formulated and demonstrated based on Lyapunov function. An experimental physiotherapy session and comparison study with a healthy subject was performed to test the effectiveness and robustness of the proposed adaptive control.}
}
@article{ELZAATARI2019162,
title = {Cobot programming for collaborative industrial tasks: An overview},
journal = {Robotics and Autonomous Systems},
volume = {116},
pages = {162-180},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S092188901830602X},
author = {Shirine {El Zaatari} and Mohamed Marei and Weidong Li and Zahid Usman},
keywords = {Human–robot collaboration, Intuitive programming, Human-awareness, Cobot},
abstract = {Collaborative robots (cobots) have been increasingly adopted in industries to facilitate human–robot collaboration. Despite this, it is challenging to program cobots for collaborative industrial tasks as the programming has two distinct elements that are difficult to implement: (1) an intuitive element to ensure that the operations of a cobot can be composed or altered dynamically by an operator, and (2) a human-aware element to support cobots in producing flexible and adaptive behaviours dependent on human partners. In this area, some research works have been carried out recently, but there is a lack of a systematic summary on the subject. In this paper, an overview of collaborative industrial scenarios and programming requirements for cobots to implement effective collaboration is given. Then, detailed reviews on cobot programming, which are categorised into communication, optimisation, and learning, are conducted. Additionally, a significant gap between cobot programming implemented in industry and in research is identified, and research that works towards bridging this gap is pinpointed. Finally, the future directions of cobots for industrial collaborative scenarios are outlined, including potential points of extension and improvement.}
}
@article{LUCET2019106,
title = {Stabilization of a road-train of articulated vehicles},
journal = {Robotics and Autonomous Systems},
volume = {114},
pages = {106-123},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S092188901730893X},
author = {Eric Lucet and Alain Micaelli},
keywords = {Articulated vehicle road-train, Linear quadratic regulator, Dynamic stabilization, ELK-test},
abstract = {This paper deals with the stable navigation of up to eight articulated vehicles, coupled together to form a road-train. Based on kinematic and dynamic models, three control approaches are proposed for dynamic stabilization in road-train configuration, as well as a methodology for setting control gains, using three possible actuators: damper at the vehicle articulation, front steering or rear drive wheels. Implementation on a 3D simulator, representative of the dynamics of the real system with a high degree of fidelity, demonstrates the controller’s performance and robustness in critical scenario conditions. Tests are then conducted in real conditions to validate the new strategy.}
}
@article{TANG2019231,
title = {Omni-directional gait of a passive-spine hexapod},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {231-246},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018309667},
author = {Yongchen Tang and Guoteng Zhang and Dingxin Ge and Shugen Ma},
keywords = {Multi-legged robot, Passive-spine hexapod robot, Passive body segment joint, Geometric analysis, Omni-directional gait},
abstract = {In this paper, we discuss the omni-directional gait of a passive-spine hexapod robot, which consists of three body segments connected by passive body joints. To design the turning and rotating gaits for this robot, firstly, we analyze a geometric model to demonstrate how segment rotation can result from abduction of the legs. Next, we obtain the principle of the rotation and direction of the body segments, which are analyzed by determining the geometric features of different configurations. The turning and rotating gaits are then attained by varying only the duration of the transition sequence of the passive-spine hexapod robot. The capability of sideways motion is also given in this paper. Finally, this proposed method for designing the omni-directional gait of a passive-spine hexapod is demonstrated by simulations and experiments.}
}
@article{2021103883,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {144},
pages = {103883},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(21)00168-8},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021001688}
}
@article{LIU2019124,
title = {Error modeling and extrinsic–intrinsic calibration for LiDAR-IMU system based on cone-cylinder features},
journal = {Robotics and Autonomous Systems},
volume = {114},
pages = {124-133},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S092188901730636X},
author = {W.I. Liu and Yunwang Li},
keywords = {Error modeling, LiDAR-IMU, Extrinsic–intrinsic calibration, Cone-cylinder feature},
abstract = {The extrinsic and intrinsic calibration of light detection and ranging (LiDAR) and inertial measurement unit (IMU) system is an essential prerequisite for its using in robots navigation or localization tasks. However, the existing LiDAR-IMU calibration method usually based on either point or planar features and existed adjustment parameter correlation limitations, which great restrict the extrinsic–intrinsic calibration flexibility and versatility. For this reason, a novel calibration technique that incorporates cone and cylinder features is proposed to overcome these drawbacks. The basic principle of our proposed method is that, first of all, we establish the transformation relationship between LiDAR and IMU coordinate frame, the LiDAR-IMU system calibration parameters and cone-cylinder geometric constrained. Secondly, the LiDAR extrinsic parameters are calibrated by estimating the pose of each scanned point datasets lies on the cone-cylinder surface and then solving the cone-cylinder geometric constrained optimization problem. Thirdly, the restricted maximum likelihood estimation (RMLE) algorithm is used to solve the optimization of IMU intrinsic parameters calibration Finally, intensive experimental studies are conducted to check the validity of our proposed method, the experimental results are presented that validate the proposed method and demonstrate the overall performances of LiDAR-IMU system obviously improved compared to plane based calibration method.}
}
@article{QI20191,
title = {Decoupled modeling and model predictive control of a hybrid cable-driven robot (HCDR)},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {1-12},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019300533},
author = {Ronghuai Qi and Mitchell Rushton and Amir Khajepour and William W. Melek},
keywords = {Hybrid cable-driven robot, Dynamics, Control, Vibration},
abstract = {This paper presents modeling and model predictive control (MPC) of a hybrid cable-driven robot (HCDR). First of all, a whole-body model is developed for the HCDR. To simplify the system modeling in practical applications, a decoupled model is then proposed to divide the whole system into two subsystems: in-plane and out-plane systems, where the former indicates a kinematic constraint vibration model and the latter indicates an underactuated dynamic model. Control design, simulations and experiments are developed to validate the models and control strategies. To overcome the inaccurate limitation of Inertial Measurement Unit (IMU) to observe states, new in-plane and out-plane state estimation methods are also proposed. Based on these state observers, experiments are implemented in different cases (e.g., no-load and 6 kg load) to evaluate control performance, and results are satisfactory.}
}
@article{ASIF201951,
title = {Whole-body motion and footstep planning for humanoid robots with multi-heuristic search},
journal = {Robotics and Autonomous Systems},
volume = {116},
pages = {51-63},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018306547},
author = {Rizwan Asif and Ali Athar and Faisal Mehmood and Fahad Islam and Yasar Ayaz},
keywords = {Robotic motion planning, Humanoid robots, Multi-heuristic A*, Graph search},
abstract = {In this paper, we present a motion planning framework for humanoid robots that combines whole-body motions as well as footsteps under a quasi-static flat ground plane assumption. Traditionally, these two have been treated as separate research domains. One of the major challenges behind whole-body motion planning is the high DoF (Degrees of Freedom) nature of the problem, in addition to strict constraints on obstacle avoidance and stability. On the other hand footstep planning on its own is a comparatively simpler problem due to the low DoF search space, but coalescing it into a larger framework that includes whole-body motion planning adds further complexity in reaching a solution within a suitable time frame that satisfies all the constraints. In this work, we treat motion planning as a graph search problem, and employ Shared Multi-heuristic A* (SMHA*) to generate efficient, stable and collision-free motion plans given only the starting state of the robot and the desired end-effector pose.}
}
@article{SAVAGE201977,
title = {Semantic reasoning in service robots using expert systems},
journal = {Robotics and Autonomous Systems},
volume = {114},
pages = {77-92},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018302501},
author = {Jesus Savage and David A. Rosenblueth and Mauricio Matamoros and Marco Negrete and Luis Contreras and Julio Cruz and Reynaldo Martell and Hugo Estrada and Hiroyuki Okada},
keywords = {Service robots, Semantic reasoning, Knowledge representation},
abstract = {This paper presents the semantic-reasoning module of VIRBOT, our proposed architecture for service robots. We show that by combining symbolic AI with digital-signal processing techniques this module achieves competitive performance. Our system translates a voice command into an unambiguous representation that helps an inference engine, built around an expert system, to perform action and motion planning. First, in the natural-language interpretation process, the system generates two outputs: (1) conceptual dependence, expressing the linguistic meaning of the statement, and (2) verbal confirmation, a paraphrase in natural language that is repeated to the user to confirm that the command has been correctly understood. Then, a conceptual-dependency interpreter extracts semantic role structures from the input sentence and looks for such structures in a set of known interpretation patterns. We evaluate this approach in a series of skill-specific semantic-reasoning experiments. Finally, we demonstrate our system in the general-purpose service robot test of the RoboCup-at-Home international competition, where incomplete information is given to a robot and the robot must recognize and request the missing information, and we compare our results with a series of baselines from the competition where our proposal performed best.}
}
@article{K2019263,
title = {Optimal whole-body motion planning of humanoids in cluttered environments},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {263-277},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017307108},
author = {Hari Teja K. and Abhilash Balachandran and S.V. Shah},
keywords = {Optimal motion planning, Humanoid, Sampling-based motion planning, Task solver, Inverse kinematics, Closed kinematic loops},
abstract = {Due to high degrees-of-freedom of humanoids and induced redundancy, there are multiple ways of performing a given manipulation task. Finding optimal ways of performing tasks is one desirable property of any motion planning framework. This includes optimizing both the path with respect to a certain objective function and also the final pre-grasp or goal position. Additionally, a variety of constraints need to be satisfied such as stability, self-collision and collision with objects in the environment and also kinematic loop-closure formed during the task. In this paper, an asymptotically optimal sampling based approach for generating motion plans is presented. A novel constraint solver extension to the bidirectional Fast Marching Trees (BFMT*) algorithm in the form of a way-point generator is proposed such that it can be applied for whole-body motion planning of humanoids. Moreover, a comparison of the performance of the proposed extension of BFMT* and the state-of-art RRT* based motion planner is shown. A gradient based inverse kinematics solver has also been implemented in combination with an optimization technique to generate goal configurations in order to ensure optimality in the pre-grasp position. The efficacy of the proposed approach is evaluated in a simulation environment on Hubo+ robot model. The results show a significant improvement in path costs, as well as overall optimality of given tasks for the proposed approach. Additionally, rigorous analysis over the choice of planning algorithms considered in this paper is present for the considered scenarios.}
}
@article{KUHNER201998,
title = {A service assistant combining autonomous robotics, flexible goal formulation, and deep-learning-based brain–computer interfacing},
journal = {Robotics and Autonomous Systems},
volume = {116},
pages = {98-113},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018302227},
author = {D. Kuhner and L.D.J. Fiederer and J. Aldinger and F. Burget and M. Völker and R.T. Schirrmeister and C. Do and J. Boedecker and B. Nebel and T. Ball and W. Burgard},
keywords = {EEG, Co-adaptive brain–computer-interface, Realtime deep learning, Autonomous robotics, Referring expression generation, High-level task planning, Computer vision},
abstract = {As autonomous service robots become more affordable and thus available for the general public, there is a growing need for user-friendly interfaces to control these systems. Control interfaces typically get more complicated with increasing complexity of robotic tasks and environments. Traditional control modalities such as touch, speech or gesture are not necessarily suited for all users. While some users can make the effort to familiarize themselves with a robotic system, users with motor disabilities may not be capable of controlling such systems even though they need robotic assistance most. In this paper, we present a novel framework that allows these users to interact with a robotic service assistant in a closed-loop fashion, using only thoughts. The system is composed of several interacting components: a brain–computer interface (BCI) that uses non-invasive neuronal signal recording and co-adaptive deep learning, high-level task planning based on referring expressions, navigation and manipulation planning as well as environmental perception. We extensively evaluate the BCI in various tasks, determine the performance of the goal formulation user interface and investigate its intuitiveness in a user study. Furthermore, we demonstrate the applicability and robustness of the system in real-world scenarios, considering fetch-and-carry tasks, close human–robot interactions and in presence of unexpected changes. As our results show, the system is capable of adapting to frequent changes in the environment and reliably accomplishes given tasks within a reasonable amount of time. Combined with high-level task planning based on referring expressions and an autonomous robotic system, interesting new perspectives open up for non-invasive BCI-based human–robot interactions.}
}
@article{FIGUEIREDO201917,
title = {A robust and efficient framework for fast cylinder detection},
journal = {Robotics and Autonomous Systems},
volume = {117},
pages = {17-28},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017308710},
author = {Rui Figueiredo and Atabak Dehban and Plinio Moreno and Alexandre Bernardino and José Santos-Victor and Helder Araújo},
abstract = {In this work, a complete solution is provided for detecting and identifying cylindrical shapes, which are commonly found in household and industrial environments, using consumer-grade RGB-D cameras. Most standard approaches to detect and identify cylinders are not robust to outliers (e.g. points on other objects in the scene), which limits their applicability in realistic scenes. In addition, these methods fail to benefit from environmental constraints, e.g. the fact that cylinders often lie or stand on flat surfaces. To tackle the aforementioned limitations, we introduce three main novelties: (i) a point cloud soft voting scheme with curvature information that reduces the influence of outliers and noise, (ii) a selective sampling of the orientation space that favorsorientations known a priori, and (iii) a deep-learning based classifier to filter out objects with non-cylindrical appearance in the 2D images, thus further improving robustness to outliers. A set of experiments with synthetically generated data are used to assess the robustness of our fitting method to different levels of outliers and noise. The results demonstrate that incorporating the principal curvature direction within the orientation voting process allows for large improvements on cylinders parameters estimation. Furthermore, we demonstrate that combining the 2D deep-learning cylinder classifier with the 3D orientation voting scheme allows for large speed-up and accuracy improvements on cylinder identification. The qualitative and quantitative results with real data acquired from a consumer RGB-D camera, confirm the advantages of the proposed framework.}
}
@article{SULLIVAN2019130,
title = {Sequential single-item auction improvements for heterogeneous multi-robot routing},
journal = {Robotics and Autonomous Systems},
volume = {115},
pages = {130-142},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018306663},
author = {Nick Sullivan and Steven Grainger and Ben Cazzolato},
keywords = {Multi-robot, Path planning, Routing, Sequential auction},
abstract = {We introduce new auction bidding and resolution algorithms to improve multi-robot sequential single-item auctions for heterogeneous systems. We consider two objectives, minimising the energy usage and time required to complete all tasks. Sequential single-item auctions are computationally inexpensive while producing efficient task allocations for homogeneous robots, but produce less efficient allocations for heterogeneous robots. Our algorithms provide consistent and significant (up to 20%) improvements for both objectives for a number of scenarios relative to the standard auction process, as tested in MATLAB simulations. Interestingly, our algorithms produce faster task completion even in homogeneous systems. We also introduce a new algorithm for sequential single-item auctions when robots have partial knowledge of their environment. We illustrate its improved performance and analyse its sensitivity, showing that precise tuning is not essential for faster and more efficient task completion. These improvements can reduce energy usage and task completion times for both indoor and outdoor robots in a variety of fields.}
}
@article{KOBAYASHI201955,
title = {Automatic controller generation based on dependency network of multi-modal sensor variables for musculoskeletal robotic arm},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {55-65},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018307474},
author = {Yuichi Kobayashi and Kentaro Harada and Kentaro Takagi},
keywords = {Adaptivity, Control, Mutual information, Structure estimation},
abstract = {Autonomous robots that work in the same environment as humans are preferred to ensure mechanical safety with respect to soft contact with their surroundings and adaptivity to handle various tools and to manage partial malfunctions. To ensure that these requirements for robots are satisfied, this study proposes an approach for obtaining a robot structure and its application to building controller for dynamic motion of a robot. It is assumed that the physical relations between the sensor variables are unknown. On the basis of dependency network construction using mutual information, controllers are generated and tested by finding appropriate causal chains of the sensor variables. The proposed controller generation methods were tested using the control tasks of a musculoskeletal robotic arm. Thus, the proposed controller generation algorithm finds appropriate controllers, and the framework of this generation is robust to the changes in the body of the body.}
}
@article{LIM201938,
title = {Shared representations of actions for alternative suggestion with incomplete information},
journal = {Robotics and Autonomous Systems},
volume = {116},
pages = {38-50},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303397},
author = {Gi Hyun Lim},
keywords = {Alternative actions, Weighted semantic networks, Incomplete knowledge, Robot knowledge representation, Spreading activation},
abstract = {Common coding theory posits there are cycles between perceptions and actions at the fundamental logic of the nervous system. An action performed by an agent modifies its external environment and/or internal states. The agent is able to complete a given goal by performing a sequence of actions. In this work, shared representations for both perception and action and their processing algorithm are presented that suggest goal-oriented alternative actions even with incomplete information. The alternative actions provide more opportunities for a service robot to achieve goals. Knowledge plays significant roles to successfully complete service tasks. Most knowledge inference mechanisms assume complete and correct knowledge about the environment. Real world environments are often uncertain and only partially observable. Thus, intelligent service robots may have an incomplete knowledge base which includes false negatives and false positives as well as true positives. False negatives and false positives can prevent service robots from completing their service tasks. A case study reveals that the proposed method has proved valuable to suggest alternatives for false information as well as to build reactive plans with customary efficiency.}
}
@article{CAPPELLO2019167,
title = {Multistable series elastic actuators: Design and control},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {167-178},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S092188901830900X},
author = {Leonardo Cappello and Michele Xiloyannis and Binh Khanh Dinh and Alberto Pirrera and Filippo Mattioni and Lorenzo Masia},
keywords = {Series elastic actuators, Multistability, System identification, Robust position control, Linear Kalman filter},
abstract = {In this paper we propose a novel actuation concept, consisting of a conventional DC motor in series with a compliant element having multiple configurations of equilibrium. The proposed device works similarly to a traditional series elastic actuator, where the elasticity increases safety and force control accuracy, but presents the possibility of achieving higher efficiency and releasing energy at a higher bandwidth. An introduction on the mechanical properties of the multistable element explains its working principle and provides simple model-based guidelines to its design. We characterize the actuator and propose a robust algorithm to control both storage and rate of release of its elastic energy. Using only an incremental encoder on the motor’s axis, we show that we can reliably control the position of the actuator and its convergence towards a state of stable equilibrium. The proposed robust control architecture sensibly improves the tracking accuracy with respect to conventional PID controllers. Once reconfigured, no additional energy from the motor is required to hold the position, making the actuator appealing for energy-efficient systems. We conclude with a discussion on the limitations and advantages of such technology, suggesting avenues for its application in the field of assistive robotics.}
}
@article{SEO201964,
title = {Learning 3D local surface descriptor for point cloud images of objects in the real-world},
journal = {Robotics and Autonomous Systems},
volume = {116},
pages = {64-79},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018307231},
author = {Ju-Hwan Seo and Dong-Soo Kwon},
keywords = {3D local surface descriptor, RGB-D sensor, Point cloud, Convolutional neural network},
abstract = {Surface descriptors, which represent the surface characteristics of an image numerically, are the fundamental elements in many vision applications. Although traditional surface descriptors that are handcrafted or learned using machine learning techniques have been applied in many different vision applications, some difficulty remains in handling large amounts of noise and variance in 3D data. To resolve this difficulty, recent studies have applied deep learning techniques for the development of surface descriptors. Unlike other techniques based on the complete 3D CAD model or pre-known mesh information of the object, we consider the constraint of the robotic applications in which the information mentioned above is difficult to preload. In this paper, we propose a new 3D surface descriptor that does not require any pre-loaded topological information of the objects or a mesh construction, which may occasionally fail with new or previously unknown objects. Further, we propose a voxel representation that is adaptive to the density of the points, resolving the problem of varying densities of the point cloud data. Finally, we adopt domain-adversarial learning that leads a network to learn the features discriminative for similarity measurements while remaining invariant to different point densities. We gathered approximately 5,000 point-cloud images of objects along with their position and orientation information. We then constructed approximately half a million pairs of point clouds indicating the identical and different parts of the objects, which are labeled as true and false, respectively. The dataset of constructed pairs was used for the learning of 3D surface descriptors using a Siamese convolutional neural network (SCNN) with a domain-adversarial characteristic. The results indicate that the proposed descriptor outperforms other descriptors.}
}
@article{LOW2019143,
title = {Solving the optimal path planning of a mobile robot using improved Q-learning},
journal = {Robotics and Autonomous Systems},
volume = {115},
pages = {143-161},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018308285},
author = {Ee Soong Low and Pauline Ong and Kah Chun Cheah},
keywords = {Flower pollination algorithm, Obstacle avoidance, Path planning, Robot, Q-learning, Robot navigation},
abstract = {Q-learning, a type of reinforcement learning, has gained increasing popularity in autonomous mobile robot path planning recently, due to its self-learning ability without requiring a priori model of the environment. Yet, despite such advantage, Q-learning exhibits slow convergence to the optimal solution. In order to address this limitation, the concept of partially guided Q-learning is introduced wherein, the flower pollination algorithm (FPA) is utilized to improve the initialization of Q-learning. Experimental evaluation of the proposed improved Q-learning under the challenging environment with a different layout of obstacles shows that the convergence of Q-learning can be accelerated when Q-values are initialized appropriately using the FPA. Additionally, the effectiveness of the proposed algorithm is validated in a real-world experiment using a three-wheeled mobile robot.}
}
@article{FOSCHVILLARONGA201977,
title = {Cloud robotics law and regulation: Challenges in the governance of complex and dynamic cyber–physical ecosystems},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {77-91},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S092188901930051X},
author = {Eduard Fosch-Villaronga and Christopher Millard},
keywords = {Cloud, Cloud computing, Cloud robotics, Robots, Cyber–physical system, Law, Technology, Policy, Human–robot interaction, Privacy, Data protection, Product safety},
abstract = {This paper assesses some of the key legal and regulatory questions arising from the integration of physical robotic systems with cloud-based services, also called “cloud robotics.” The literature on legal and ethical issues in robotics has a strong focus on the robot itself, but largely ignores the background information processing. Conversely, the literature on cloud computing rarely addresses human–machine interactions, which raise distinctive ethical and legal concerns. In this paper, we investigate, from a European legal and regulatory perspective, the growing interdependence and interactions of tangible and virtual elements in cloud robotics environments. We highlight specific problems and challenges in regulating such complex and dynamic ecosystems and explore potential solutions. To illustrate practical challenges, we consider several examples of cloud robotics ecosystems involving multiple parties, various physical devices, and various cloud services. These examples illuminate the complexity of interactions between relevant parties. By identifying pressing legal and regulatory issues in relation to cloud robotics, we hope to inform the policy debate and set the scene for further research.}
}
@article{SAFEEA2019278,
title = {On-line collision avoidance for collaborative robot manipulators by adjusting off-line generated paths: An industrial use case},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {278-288},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019300648},
author = {Mohammad Safeea and Pedro Neto and Richard Bearee},
keywords = {Collision avoidance, Collaborative robotics, Industry, Assembly},
abstract = {Human–robot collision avoidance is a key in collaborative robotics and in the framework of Industry 4.0. It plays an important role for achieving safety criteria while having humans and machines working side-by-side in unstructured and time-varying environment. This study introduces the subject of manipulator’s on-line collision avoidance into a real industrial application implementing typical sensors and a commonly used collaborative industrial manipulator, KUKA iiwa. In the proposed methodology, the human co-worker and the robot are represented by geometric primitives (capsules). The minimum distance and relative velocity between them is calculated, when human/obstacles are nearby the concept of hypothetical repulsion and attraction vectors is used. By coupling this concept with a mathematical representation of robot’s kinematics, a task level control with collision avoidance capability is achieved. Consequently, the off-line generated nominal path of the industrial task is modified on-the-fly so the robot is able to avoid collision with the co-worker safely while being able to fulfill the industrial operation. To guarantee motion continuity when switching between different tasks, the notion of repulsion-vector-reshaping is introduced. Tests on an assembly robotic cell in automotive industry show that the robot moves smoothly and avoids collisions successfully by adjusting the off-line generated nominal paths.}
}
@article{QUANN20191,
title = {Chance constrained reachability in environments with spatially varying energy costs},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {1-12},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018310212},
author = {Michael Quann and Lauro Ojeda and William Smith and Denise Rizzo and Matthew Castanier and Kira Barton},
keywords = {Reachability, Spatial energy mapping, Gaussian process regression, Planning under uncertainty},
abstract = {In off-road environments, energy costs are highly uncertain and variable due to unknown terrains. To plan missions for robots with limited energy storage capacity, a robot’s reachable set must be determined. This work presents a novel approach for learning reachable sets based on data collected during a mission. Leveraging the authors’ previous work, a spatial energy map of an unknown environment, built with data collected by a robot, can be used to compute a chance constrained reachable set (CCRS) based on a user-defined confidence level. Simulations demonstrate that as a robot collects more data on an energy map, the true positive rate of a computed CCRS improves significantly while the false positive rate remains low, implying that a robot’s reachability can be robustly determined for use in future missions.}
}
@article{HUANG2019103237,
title = {Design, simulation and experimental study of a force observer for a flying–perching quadrotor},
journal = {Robotics and Autonomous Systems},
volume = {120},
pages = {103237},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018308716},
author = {Chengwei Huang and Yong Liu and Xi Ye},
keywords = {Flying-perching quadrotor, Force observer},
abstract = {It is significant to know the information of the contact force for the control of the flying–perching quadrotor when it is switching from flying state to perching state. The triaxial force sensor is not only difficult to install but also increases the weight and cost of the flying–perching quadrotor. To overcome these limits, this paper presents a force observer to estimate the contact force. We first build a detailed model of the flying–perching quadrotor in free space and when it gets into contact with the perching surface. The model is modified by considering the proximity effect. According to the model, a force observer was proposed. To demonstrate the performance of the proposed observer, simulations are carried out in ideal environment and with consideration of sensor noises. Moreover, we built an experimental platform and carried out experiments to verify the effectiveness of the designed observer in practical situations.}
}
@article{RAJA2019103245,
title = {Learning framework for inverse kinematics of a highly redundant mobile manipulator},
journal = {Robotics and Autonomous Systems},
volume = {120},
pages = {103245},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018306778},
author = {R. Raja and A. Dutta and B. Dasgupta},
keywords = {Kohonen self organizing map, Redundancy resolution, Mobile manipulators, Rough terrain, Manipulability},
abstract = {A learning framework is proposed to solve the inverse kinematic problems of a highly redundant mobile manipulator designed to traverse on rough terrains. The problem is not trivial to solve and there does not exist a closed form solution. The learning framework is designed based on the Kohonen Self Organizing Map (KSOM) to establish the mapping between the task-space and joint-space while resolving redundancy exist in the system. The standard KSOM learning architecture is modified to ensure proper coordination between the mobile base and arm satisfying multiple constraints such as wheels of the mobile robot always remain in contact with the terrain and maximize the manipulability for the robot arm while solving inverse kinematics. The network is trained for 14 DoF mobile manipulator traverse on uneven terrain. This method can be extended to other types of robot, such as high degrees of the manipulator. To validate the proposed network architecture several simulations have been performed and experimented on a mobile manipulator considering robot traverse on different types of terrains. The results show the effectiveness of the proposed framework.}
}
@article{YUAN2019119,
title = {End-to-end nonprehensile rearrangement with deep reinforcement learning and simulation-to-reality transfer},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {119-134},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018304913},
author = {Weihao Yuan and Kaiyu Hang and Danica Kragic and Michael Y. Wang and Johannes A. Stork},
keywords = {Nonprehensile rearrangement, Deep reinforcement learning, Transfer learning},
abstract = {Nonprehensile rearrangement is the problem of controlling a robot to interact with objects through pushing actions in order to reconfigure the objects into a predefined goal pose. In this work, we rearrange one object at a time in an environment with obstacles using an end-to-end policy that maps raw pixels as visual input to control actions without any form of engineered feature extraction. To reduce the amount of training data that needs to be collected using a real robot, we propose a simulation-to-reality transfer approach. In the first step, we model the nonprehensile rearrangement task in simulation and use deep reinforcement learning to learn a suitable rearrangement policy, which requires in the order of hundreds of thousands of example actions for training. Thereafter, we collect a small dataset of only 70 episodes of real-world actions as supervised examples for adapting the learned rearrangement policy to real-world input data. In this process, we make use of newly proposed strategies for improving the reinforcement learning process, such as heuristic exploration and the curation of a balanced set of experiences. We evaluate our method in both simulation and real setting using a Baxter robot to show that the proposed approach can effectively improve the training process in simulation, as well as efficiently adapt the learned policy to the real world application, even when the camera pose is different from simulation. Additionally, we show that the learned system not only can provide adaptive behavior to handle unforeseen events during executions, such as distraction objects, sudden changes in positions of the objects, and obstacles, but also can deal with obstacle shapes that were not present in the training process.}
}
@article{LIU2019209,
title = {Real-time mode recognition based assistive torque control of bionic knee exoskeleton for sit-to-stand and stand-to-sit transitions},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {209-220},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S092188901830705X},
author = {Xiuhua Liu and Zhihao Zhou and Jingeng Mai and Qining Wang},
keywords = {STS transitions, Knee exoskeleton, Mode recognition, Real-time, Assistive torque control},
abstract = {Assistive torque control is important for people who cannot fully accomplish sit-to-stand and stand-to-sit (STS) transitions. In this paper, we proposed a three-level control strategy for a bionic knee exoskeleton based on real-time STS transition recognition. Motion features were obtained from one potentiometer and two inertial measurement units integrated in the exoskeleton. A multi-class support vector classifier was utilized to infer the subject’s real-time motion intent. Twelve able-bodied subjects were recruited in experiments. Mean accuracy across subjects was 97.63%±0.01%. Once STS transition was detected, the proposed control system could add assistive torque in time to assist the subject to accomplish the transition.}
}
@article{PAZ201961,
title = {Online optimization of humanoid walking trajectories for passing through a door},
journal = {Robotics and Autonomous Systems},
volume = {115},
pages = {61-72},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018306377},
author = {Alvaro Paz and Gustavo Arechavaleta},
keywords = {Humanoid walking, Optimal observer planning, Stochastic optimal control},
abstract = {In this work we propose an online trajectory planner for humanoid walking. It is based on the observer trajectory planning problem where the moving observer (humanoid) should maneuver optimally to better estimate the position of the target (door). Thus, the uncertainty of the door location within the humanoid’s field of view is considered. In particular, we propose a terminal stochastic controller that recursively applies the Unscented Kalman Filter (UKF) over a time horizon for evaluating a set of objective functions. The aim is to minimize the estimation error of the door location while maintaining it inside the humanoid’s field of view, and avoiding collisions between the humanoid and the door frame. The output is the sequence of walking velocity references. In addition, we compute the humanoid’s odometric localization based on the UKF to provide the instantaneous humanoid location, which is an input to the trajectory planner. We validate the stochastic controller with a set of experiments in a scenario with doors using a real humanoid NAO, equipped with an RGB-D sensor mounted on its head.1 1The source code in ROS C++ is available at: https://sites.google.com/site/gustavoarechavaleta/sochum.}
}
@article{HAWLEY20191,
title = {Control framework for cooperative object transportation by two humanoid robots},
journal = {Robotics and Autonomous Systems},
volume = {115},
pages = {1-16},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303543},
author = {Louis Hawley and Wael Suleiman},
keywords = {Humanoid robot, Control, Legged locomotion, Robot–robot cooperation},
abstract = {This paper aims at proposing a comprehensive control framework designed for cooperative transportation of a heavy load by two humanoid robots. First, a simplified dynamic model of the cooperative task is developed and the system stability is rigorously analyzed. Next, a centralized controller is formulated, this formulation provides an optimal control of the system by considering the robots dynamical stability while satisfying the robot–object–robot constraints. Finally, the controller is integrated with an arm controller and a local planner module forming a complete framework for cooperative transportation tasks. The approach is thoroughly analyzed and validated in simulation, and experiments are carried out on a team of two Nao humanoid robots transporting a range of objects placed on a small table. The experimental results pointed out the robustness of the approach as the robots successfully accomplished the transportation tasks in a stable way, moreover the transported objects masses were up to half the mass of one of the robot. Besides increasing the robot payload, some of the transported objects are relatively large and it is simply impossible for a single robot to transport them.}
}
@article{MILLINGTON2019144,
title = {Innovative mechanism to identify robot alignment in an automation system},
journal = {Robotics and Autonomous Systems},
volume = {114},
pages = {144-154},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S092188901830527X},
author = {Joseph Millington and Radmehr P Monfared and Daniel Vera},
keywords = {Robot alignment, Automated systems, Robot calibration, Forward and inverse modelling},
abstract = {Robotic applications are commonly used in industrial automation systems. Such systems are often comprised of a series of equipment, including robotic arms, conveyors, a workspace, and fixtures. While each piece of equipment may be calibrated with the highest precision, their alignment in relation to each other is an important issue in defining the accuracy of the system. Currently, a variety of complex automated and manual methods are used to align a robotic arm to a workspace. These methods often use either expensive equipment or are slow and skill-dependent. This paper presents a novel low-cost method for aligning an industrial robot to its workcell at 6 degrees of freedom (DoF). The solution is new, simple and easy to use and intended for the SMEs dealing with low volume, high complexity automated systems. The proposed method uses three dial indicators mounted to a robot end effector and a fixed measurement cube, positioned on a workcell. The robot is pre-programmed for a procedure around the cube. The changes on the dial indicators are used to calculate the misalignment between the robot and the workcell. Despite simplicity of the design, the solution is supported with complex real-time mathematical calculations and proven to identify and eliminate misalignment up to 3 mm and 5 degrees to an accuracy of 0.003 mm and 0.002 degrees: much higher than the precision required for a conventional industrial robot. In this article, the authors describe a proposed solution, validate the computation both theoretically and through a laboratory test rig and simulation.}
}
@article{DEMIDIO2019151,
title = {Collision-free allocation of temporally constrained tasks in multi-robot systems},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {151-172},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018306493},
author = {Mattia D’Emidio and Imran Khan},
keywords = {Multi-robot systems, Distributed algorithms, Task allocation, Path planning, Algorithm engineering, Scheduling},
abstract = {Multi-robot systems (MRS) are a reference solution for many prominent real-world applications, e.g. management of warehouses or exploration of unknown environments. One of the most fundamental computational problems in MRS is that of planning the assignment of tasks to robots when such tasks have deadlines, i.e. constraints on when the execution must take place. The problem, when multiple objective functions of interest need to be optimized, is both NP-Hard and hard to approximate, and few heuristics are known in the literature to handle it. Unfortunately, none of them guarantees that the trajectories used by the robots when moving between tasks’ locations are collision-free at planning time. Rather, they implement a reactive behavior, i.e. they abort the execution of a planned task whenever something goes wrong, e.g. trajectories of robots intersect or a deadline is missed due to some obstacle. This approach induces negative effects on the global performance of the system in the form of waste of energy, due to high distances traveled by the fleet members, or in the form of high convergence time to execute tasks. Therefore, planning the assignments of temporally constrained tasks with the guarantee of avoiding collisions can be a desirable feature for multi-robot systems. In this paper, we present CFAT-D (Collision-Free Allocation of Tasks having Deadlines), a new algorithm that can allocate temporally constrained tasks while guaranteeing that used trajectories are collision-free at planning time. We prove CFAT-D to be correct and showcase its effectiveness through an extensive experimental evaluation. Finally, we provide a roadmap toward the practical implementation of the new strategy in real-world environments.}
}
@article{WIEDEMANN201966,
title = {Model-based gas source localization strategy for a cooperative multi-robot system—A probabilistic approach and experimental validation incorporating physical knowledge and model uncertainties},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {66-79},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303816},
author = {Thomas Wiedemann and Dmitriy Shutin and Achim J. Lilienthal},
keywords = {Robotic exploration, Gas source localization, Multi-agent-system, Partial differential equation, Mobile robot olfaction, Sparse Bayesian learning, Factor graph, Message passing},
abstract = {Sampling gas distributions by robotic platforms in order to find gas sources is an appealing approach to alleviate threats for a human operator. Different sampling strategies for robotic gas exploration exist. In this paper we investigate the benefit that could be obtained by incorporating physical knowledge about the gas dispersion. By exploring a gas diffusion process using a multi-robot system. The physical behavior of the diffusion process is modeled using a Partial Differential Equation (PDE) which is integrated into the exploration strategy. It is assumed that the diffusion process is driven by only a few spatial sources at unknown locations with unknown intensity. The objective of the exploration strategy is to guide the robots to informative measurement locations and by means of concentration measurements estimate the source parameters, in particular, their number, locations and magnitudes. To this end we propose a probabilistic approach towards PDE identification under sparsity constraints using factor graphs and a message passing algorithm. Moreover, message passing schemes permit efficient distributed implementation of the algorithm, which makes it suitable for a multi-robot system. We designed an experimental setup that allows us to evaluate the performance of the exploration strategy in hardware-in-the-loop experiments as well as in experiments with real ethanol gas under laboratory conditions. The results indicate that the proposed exploration approach accelerates the identification of the source parameters and outperforms systematic sampling.}
}
@article{2021103926,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {146},
pages = {103926},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(21)00211-6},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021002116}
}
@article{MARTINEZ2019220,
title = {Continuous perception for deformable objects understanding},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {220-230},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019300417},
author = {Luz Martínez and Javier Ruiz-del-Solar and Li Sun and J. Paul Siebert and Gerardo Aragon-Camarasa},
keywords = {Deformable object classification, Continuous perception, Robot vision},
abstract = {We present a robot vision approach to deformable object classification, with direct application to autonomous service robots. Our approach is based on the assumption that continuous perception provides robots with greater visual competence for deformable objects interpretation and classification. Our approach classifies the category of clothing items by continuously perceiving the dynamic interactions of the garment’s material and shape as it is being picked up. For this, we extract continuously visual features of a RGB-D video sequence and we fuse features by means of the Locality Constrained Group Sparse Representation (LGSR) algorithm. To evaluate the performance of our approach, we created a fully annotated database featuring 150 garment videos in random configurations. Experiments demonstrate that by continuously observing an object deform, our approach achieves a classification score of 66.7%, outperforming state-of-the-art approaches by a ∼27.3% increase.}
}
@article{PENAFERNANDEZ2019231,
title = {Nonlinear trajectory tracking controller for wheeled mobile robots by using a flexible auxiliary law based on slipping and skidding variations},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {231-250},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018304986},
author = {C.A. {Peña Fernández} and J.J.F. Cerqueira and A.M.N. Lima},
keywords = {Slipping, Skidding, Flexibility, Singular perturbation, Wheeled mobile robots, Trajectory tracking},
abstract = {In order to synthesize controllers for wheeled mobile robots (WMRs), some design techniques are usually based on the assumption that the set of kinematic constraints are fully satisfied. Nevertheless, when a WMR moves on a trajectory, phenomena such as slipping, skidding and deformability (also related to flexibility) can cause violation of kinematic constraints as well as system instability. Previous research has shown that to make the tracking error converge toward a small neighborhood containing the origin, in WMRs that moving at different speeds, it can be used an outer closed-loop scheme based on an auxiliary control law with aid of estimations of the slipping and skidding variations. But, although these previous works use slipping/skidding variations, the flexibility parameter is not taken into account to compute the auxiliary control law, making the outer closed-loop less robust for different flexibility values in the dynamic. The work reported here proposes a control scheme fully influenced by the flexibility parameter and whose robustness will be based on an auxiliary control law that uses a nonlinear function in terms of flexibility and slipping/skidding variations. The results obtained from experimental and simulations tests will show that the flexible auxiliary control law not only ensures that the error converges to a small neighborhood close to origin but also it is more insensitive to the speed increasing of the WMR.}
}
@article{XIONG201990,
title = {Path planning of multiple autonomous marine vehicles for adaptive sampling using Voronoi-based ant colony optimization},
journal = {Robotics and Autonomous Systems},
volume = {115},
pages = {90-103},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018304469},
author = {Chengke Xiong and Danfeng Chen and Di Lu and Zheng Zeng and Lian Lian},
keywords = {Path planning, Multiple autonomous marine vehicles, Adaptive ocean sampling, Voronoi diagram, Ant colony optimization},
abstract = {In this paper, a hybrid Voronoi-based ant colony optimization (V-ACO) technique for multiple autonomous marine vehicles (AMVs) is proposed to solve adaptive ocean sampling problem. The Voronoi-based scheme utilizes Voronoi partition with tournament selection method that enables more Voronoi edges lie in higher scientific interest regions. This scheme is then combined with ant colony optimization (ACO) using modified heuristic function, to find collision-free optimal trajectories for multiple AMVs to collect ocean measurements. For comparison, conventional ACO, rapidly-exploring random tree star (RRT*) and Dijkstra’s algorithm are also applied and tested for adaptive ocean sampling. Results of simulation tests specifically highlight the effectiveness and robustness of the proposed V-ACO path planner in generating trajectories of multi-AMVs that maximize data collection for adaptive ocean sampling in high scientific interest areas while considering specified mission time, inter-vehicle and obstacles avoidance constraints. Furthermore, field experiments validate the capability of the proposed V-ACO path planner in finding optimal solutions for adaptive ocean sampling.}
}
@article{RAMIREZAMARO201931,
title = {A survey on semantic-based methods for the understanding of human movements},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {31-50},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303932},
author = {Karinne Ramirez-Amaro and Yezhou Yang and Gordon Cheng},
keywords = {Semantic representations, Understanding human movements, Human activity recognition, Robot action execution, Intelligent systems},
abstract = {This paper presents semantic-based methods for the understanding of human movements in robotic applications. To understand human movements, robots need to first, recognize the observed or demonstrated human activities, and secondly, learn different parameters to execute an action or robot behavior. In order to achieve that, several challenges need to be addressed such as the automatic segmentation of human activities, identification of important features of actions, determine the correct sequencing between activities, and obtain the correct mapping between the continuous data and the symbolic and semantic interpretations of the human movements. This paper aims to present state-of-the-art semantic-based approaches, especially the new emerging approaches that tackle the challenges of finding generic and compact semantic models for the robotics domain. Finally, we will highlight potential breakthroughs and challenges for the next years such as achieving scalability, better generalization, compact and flexible models, and higher system accuracy.}
}
@article{SONG2019103247,
title = {Modeling and control of inherently safe robots with variable stiffness links},
journal = {Robotics and Autonomous Systems},
volume = {120},
pages = {103247},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019300752},
author = {Siyang Song and Xianpai Zeng and Yu She and Junmin Wang and Hai-Jun Su},
keywords = {Variable stiffness link, Physical human–robot interaction, Robust control, Vibration suppression, Safe trajectory planning, Impact test},
abstract = {In this paper, the modeling, control design, and trajectory planning for inherently safe robots with variable stiffness links (VSL) are investigated. Firstly, a dynamic model of VSL robots is developed using the pseudo-rigid-body model (PRBM). Based on PRBM, a feedback-linearization based controller is proposed. Extended state observer and deflection feedback are designed to improve the robustness and vibration suppression. To keep the inherent safety, a safe trajectory planning problem is formulated and the safety criterion is converted to a velocity constraint. With constraints on the jerk, acceleration, and velocity, the trajectory-planning problem is formulated as a time-optimal problem. The analytical solution of this problem is derived by optimal control theory. Experiments show the performances of motion control and vibration suppression of the proposed controller. The impact test results indicate the potential of VSL robots for applications with physical human–robot interaction.}
}
@article{ZAARE2019204,
title = {Voltage based sliding mode control of flexible joint robot manipulators in presence of uncertainties},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {204-219},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018309916},
author = {Saeed zaare and Mohammad Reza Soltanpour and Mazda Moattari},
keywords = {Robot manipulator, Joint flexibility, Sliding mode control, Voltage-based, Structured uncertainty, Unstructured uncertainty},
abstract = {In this paper, a voltage-based sliding mode control (SMC) is presented to control the position of n rigid-link flexible-joint (RLFJ) serial robot manipulator in the presence of structured and unstructured uncertainties. In addition to good performance, the proposed method has three attracting properties: First, it has been developed for a class of RLFJ robot manipulators with a n degree of freedom (DOF). Second, the design process includes all the manipulator dynamical equations, the mechanical and the electrical part of the actuator. Third, the simple structure and low volume of computing make practical implementation possible. Using the idea of the independent joint control strategy, the system dynamic equations are decomposed into n independent subsystems. For each subsystem, three sliding surfaces are defined. Then using these sliding surfaces, control input laws are designed for all subsystems simultaneously. It is proved that the proposed method can guarantee the asymptotic stability of each subsystems and the global asymptotic stability of the closed-loop system in the presence of uncertainties. The results of simulations, as well as experimental results produced using MATLAB/Simulink external mode control on a flexible-joint electrically driven robot manipulator, illustrate high efficiency of the proposed control schemes.}
}
@article{2021103895,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {145},
pages = {103895},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(21)00180-9},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021001809}
}
@article{CASTRO2019192,
title = {Efficient on-board Stereo SLAM through constrained-covisibility strategies},
journal = {Robotics and Autonomous Systems},
volume = {116},
pages = {192-205},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018304500},
author = {Gastón Castro and Matías A. Nitsche and Taihú Pire and Thomas Fischer and Pablo {De Cristóforis}},
keywords = {Stereo SLAM, Constrained covisibility, Loop closure, Real-time embedded SLAM, MAVs},
abstract = {Visual SLAM is a computationally expensive task, with a complexity that grows unbounded as the size of the explored area increases. This becomes an issue when targeting embedded applications such as on-board localization on Micro Aerial Vehicles (MAVs), where real-time execution is mandatory and computational resources are a limiting factor. The herein proposed method introduces a covisibility-graph based map representation which allows a visual SLAM system to execute with a complexity that does not depend on the size of the map. The proposed structure allows to efficiently select locally relevant portions of the map to be optimized in such a way that the results resemble performing a full optimization on the whole trajectory. We build on S-PTAM (Stereo Parallel Tracking and Mapping), yielding an accurate and robust stereo SLAM system capable to work in real-time under limited hardware constraints such as those present in MAVs. The developed SLAM system in assessed using the EuRoC dataset. Results show that covisibility-graph based map culling allows the SLAM system to run in real-time even on a low-resource embedded computer. The impact of each SLAM task on the overall system performance is analyzed in detail and the SLAM system is compared with state-of-the-art methods to validate the presented approach.}
}
@article{XIAO20191,
title = {Dynamic-SLAM: Semantic monocular visual localization and mapping based on deep learning in dynamic environment},
journal = {Robotics and Autonomous Systems},
volume = {117},
pages = {1-16},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018308029},
author = {Linhui Xiao and Jinge Wang and Xiaosong Qiu and Zheng Rong and Xudong Zou},
keywords = {Simultaneous localization and mapping (SLAM), Semantics, Object detection, Dynamic environment},
abstract = {When working in dynamic environment, traditional SLAM framework performs poorly due to interference from dynamic objects. By taking advantages of deep learning in object detection, a semantic simultaneous localization and mapping framework named Dynamic-SLAM is proposed, in order to solve the problem of SLAM in dynamic environment. First, based on the convolutional neural network, an SSD object detector which combines prior knowledge is constructed to detect dynamic objects in the newly detection thread at semantic level. Then, in view of low recall rate of the existing SSD object detection network, a missed detection compensation algorithm based on the speed invariance in adjacent frames is proposed, which greatly improves the recall rate of detection. Finally, a feature-based visual SLAM system is constructed, which processes the feature points of dynamic objects through a selective tracking algorithm in the tracking thread, to significantly reduce the error of pose estimation caused by incorrect matching. The recall rate of the system is increased from 82.3% to 99.8% compared with the original SSD network. Several experiments show that the localization accuracy of Dynamic-SLAM is higher than the state-of-the-art systems. The system successfully localizes and constructs an accurate environmental map in real-world dynamic environment by using a mobile robot. In sum, our experimental demonstrations verify that Dynamic-SLAM shows improved accuracy and robustness in robot localization and mapping comparing to the state-of-the-art SLAM system in dynamic environment.}
}
@article{LUDDECKE201992,
title = {Context-based affordance segmentation from 2D images for robot actions},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {92-107},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018309990},
author = {Timo Lüddecke and Tomas Kulvicius and Florentin Wörgötter},
abstract = {Affordances play a crucial role in robotics since they allow developing truly autonomous robots, which can freely explore and interact with the environment. Most of the existing approaches for analyzing affordances in a scene consider only one or few types of affordance, e.g., grasping points, object manipulation or locomotion. In many cases only whole objects are considered. In our study we include in total 12 affordances of object-related, manipulation and locomotion affordances, considering affordances of both objects and/or their parts. We design a system that can densely predict affordances given only a single 2D RGB image. For this, we propose a method that transfers object class labels to affordances. This enables us to train convolutional neural networks, a PSPNet-based network and a U-Net-style network, to directly predict affordances from an image using a selective binary cross entropy loss function. The method is able to handle (potentially multiple) affordances of objects and their parts in a pixel-wise manner even in the case of incomplete data. We perform qualitative as well as quantitative evaluations with simulated and real data including robot experiments. In general, we find that frequent affordances are recognized with a substantial fraction of correctly assigned pixels, while this is harder for infrequent affordances and small objects. In addition, we demonstrate that our method performs better than a recent competitive approach. As the proposed method operates on 2D images, it is easier to implement than competing 3D methods and it could therefore more easily provide useful affordance estimates for robotic actions as demonstrated experimentally.}
}
@article{XU2019121,
title = {Shared control of a robotic arm using non-invasive brain–computer interface and computer vision guidance},
journal = {Robotics and Autonomous Systems},
volume = {115},
pages = {121-129},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018306080},
author = {Yang Xu and Cheng Ding and Xiaokang Shu and Kai Gui and Yulia Bezsudnova and Xinjun Sheng and Dingguo Zhang},
keywords = {Brain–computer interface, Motor imagery, Computer vision, Shared control, Robotic arm},
abstract = {Control of a robotic arm using a brain–computer interface (BCI) for reach and grasp activities is one of the most fascinating applications for some severely disabled people, which is especially challenging for the non-invasive BCIs based on electroencephalography (EEG). In this paper, shared control is applied to realize the control of a dexterous robotic arm with a motor imagery-based (MI-based) BCI and computer vision guidance. With the utilization of the shared control, the subjects just need to move the robotic arm by performing only two different mental tasks to the surrounding area of the target. The accurate pose of the target is estimated by a depth camera equipped in the robot system. Once the endpoint of the robotic arm enters the pre-defined vision-guided region, the robotic arm will grasp the target autonomously. Five healthy and inexperienced subjects participated in the online experiments and the average success rate is above 70% even with no specific user training. The results show that the shared control can make the robotic arm accomplish the complex tasks (reach and grasp) with the simple two-class MI-based BCIs.}
}
@article{WEN201928,
title = {Observer-based control of vehicle platoons with random network access},
journal = {Robotics and Autonomous Systems},
volume = {115},
pages = {28-39},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018300939},
author = {Shixi Wen and Ge Guo},
keywords = {Vehicle platoons control, Vehicular ad hoc networks, Markov MAC protocol, Observer-based control, String stability, Zero steady-state velocity errors},
abstract = {This paper investigates cooperative control of vehicle platoons, focusing on the effect of medium access control (MAC) protocol and unreliable measurement on acceleration information. A Markov chain is used to describe the randomness in vehicular network access under a MAC protocol; a reduced-order observer is proposed to estimate the relative acceleration of neighboring vehicles. Based on stochastic system techniques, a series of sufficient conditions for the existence of the observer-based controller are given in the form of backward recursive Riccati Difference Equations (RDE). A controller–estimator design algorithm is derived to determine the controller and observer gains with which the disturbance of the preceding vehicle acceleration can be eliminated and string stability and zero steady-state velocity error performance can be guaranteed. Both numerical simulations and experiments with laboratory-scale Arduino cars are given to verify the effectiveness and practicability of the proposed algorithm.}
}
@article{QI201964,
title = {Autonomous landing solution of low-cost quadrotor on a moving platform},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {64-76},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019300508},
author = {Yuhua Qi and Jiaqi Jiang and Jin Wu and Jianan Wang and Chunyan Wang and Jiayuan Shan},
keywords = {Autonomous landing, Low-cost quadrotor, Backstepping method},
abstract = {In this paper, a complete solution for autonomous landing of a low-cost quadrotor on a moving platform is presented. First, the dynamic model of the quadrotor is described and simplified for landing task. Second, a novel landing pad associated with detection algorithm is designed for robust detection by a low-cost monocular camera. In order to deal with mirror effect and occasional misidentification, a 3D point cluster algorithm for relative position estimation is proposed. Third, using the backstepping approach, an adaptive position controller is designed to calculate the desired attitude for attitude loop. The overall stability of the position loop is proven by a Lyapunov method. Meanwhile, some landing strategies are presented to achieve a safe and stable landing task. Finally, the low-cost system architecture of the quadrotor and the experiment results are both demonstrated to showcase the effectiveness of the proposed method.}
}
@article{ANTUNES201983,
title = {Implementation and testing of a sideslip estimation for a formula student prototype},
journal = {Robotics and Autonomous Systems},
volume = {115},
pages = {83-89},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.018},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018301908},
author = {André Antunes and Pedro Outeiro and Carlos Cardeira and Paulo Oliveira},
keywords = {Kalman Filters, Estimation, System implementation, Real-time systems, Vehicle dynamics},
abstract = {This document details the implementation and test of a self-calibrating estimation architecture for the sideslip of a Formula Student prototype. The proposed algorithm fuses several sensors being the most relevant an Inertial Measurement Unit (IMU) and a Global Positioning System (GPS). It is presented a comparison between a linear and a non-linear estimators, and their consequences. The algorithm is tested with real data from a Formula Student vehicle, and validated with a differential GPS. It is also reported an implementation of the proposed algorithm in a micro-controller, and tested with a radio-controlled (RC) vehicle. These results are also validated with the data from a more accurate indoor motion system.}
}
@article{THALAMY2019103242,
title = {A survey of autonomous self-reconfiguration methods for robot-based programmable matter},
journal = {Robotics and Autonomous Systems},
volume = {120},
pages = {103242},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019301459},
author = {Pierre Thalamy and Benoît Piranda and Julien Bourgeois},
keywords = {Self-reconfiguration, Modular robots, Programmable matter, Distributed algorithms, Self-organizing particle systems},
abstract = {While researchers envision exciting applications for metamorphic systems like programmable matter, current solutions to the shape formation problem are still a long way from meeting their requirements. To dive deeper into this issue, we propose an extensive survey of the current state of the art of self/reconfiguration algorithms and underlying models in modular robotic and self-organizing particle systems. We identify three approaches for solving this problem and we compare the different solutions using a synoptic graphical representation. We then close this survey by confronting existing methods to our vision of programmable matter, and by discussing a number of future research directions that would bring us closer to making it a reality.}
}
@article{KAZEMI20191,
title = {Real-time walking pattern generation for a lower limb exoskeleton, implemented on the Exoped robot},
journal = {Robotics and Autonomous Systems},
volume = {116},
pages = {1-23},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018307966},
author = {Jafar Kazemi and Sadjaad Ozgoli},
keywords = {Exoskeleton, Walking pattern, Optimal control, Center of mass, Exoped},
abstract = {Lower extremity exoskeletons have been developed as a motion assistive technology in recent years. Walking pattern generation is a fundamental topic in the design of these robots. The usual approach with most exoskeletons is to use a pre-recorded pattern as a look-up table. There are some deficiencies with this method, including data storage limitation and poor regulation relating to the walking parameters. In addition, the walking parameters can be taken in hand very hard. Therefore modeling the human walking pattern is required. The few existing models provide piece by piece walking patterns, only generating at the beginning of each stride. In this paper, a real-time walking pattern generation method is provided which enables changing the parameters during the stride. For this purpose, two feedback controlled third order systems are proposed as real-time trajectory planners for generating the trajectories of the x and y components of each joint’s position. The boundary conditions of the trajectories are determined to prevent backward balance loss by appropriate displacement of the center of mass. In addition, a cost function is intended for each trajectory planner in order to increase the smoothness of trajectories. Optimization technique is used to design the feedback controller for tracking the boundary conditions in such a way that the cost function is minimized. Finally, the proper joint angles are generated using inverse kinematics transformation. The performance of the proposed pattern generator is verified via real experiments on the Exoped robot.}
}
@article{FOLGHERAITER2019103244,
title = {A neuromorphic control architecture for a biped robot},
journal = {Robotics and Autonomous Systems},
volume = {120},
pages = {103244},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017301793},
author = {Michele Folgheraiter and Amina Keldibek and Bauyrzhan Aubakir and Giuseppina Gini and Alessio Mauro Franchi and Matteo Bana},
keywords = {Neuromorphic controller, Real time trajectory generation, Chaotic Recurrent Neural Network, Humanoid robotics, Biped robot, Neurodynamics},
abstract = {A neuromorphic control architecture is introduced to govern the motion of a lightweight humanoid robot. The reference trajectories necessary to perform stable gaits are generated by neural modules represented by Chaotic Recurrent Neural Networks CRNN organized in a hierarchical fashion. In the higher layer a body-coordination module generates the trajectories for the central parts of the robot body, in the middle layer the limb-coordination modules generate the Cartesian trajectories for the end effector of each limb, finally in the lower layer the limb modules control the position of the robot joints. Each neural module consists of a reservoir of N=200 leaky-integrator neurons randomly and sparsely connected with fixed synapses. The adaptation occurs in the synapses of readout units by online learning techniques like the delta rule and the Recursive Least Square algorithm RLS. It is demonstrated that the neural modules can learn and reproduce with enough accuracy the trajectories acquired from the simulation of a humanoid robot in V-REP software. With an optimal initialization of the reservoir connection matrix and by using a low computationally expensive learning algorithm such as the delta rule, Θ(N), the average of MSE over all lower limbs joints is in the order of 0.1. For the lower-limbs-coordination-module the MSE drops to 0.004 by using the more computational expensive RLS, Θ(N2). In case the neural module needs to learn how to adapt the trajectories according to a specific step length and frequency the MSE is 0.06. A comparison between different learning algorithms applied on the CRNN showed better performances by using RLS. This result is confirmed also by a direct comparison with a different neural architecture, the PCPG, however at the expense of a bigger computational complexity. A real test conducted on a small computational unit (Raspberry Pi2) demonstrated that the CRNN can be executed at a frequency of 142 Hz which suffices to feed a PID feedback control loop at the joint level.}
}
@article{OSUNAIBARRA2019201,
title = {Tracking control using optimal discrete-time H∞ for mechanical systems: Applied to Robotics},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {201-208},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017302920},
author = {L. Osuna-Ibarra and H. Caballero-Barragán and A.G. Loukianov and E. Bayro-Corrochano},
keywords = {Robotics, Discrete-time systems, Mechanical systems,  controller, Tracking control},
abstract = {In this work, the H∞ control for mechanical systems and its application in Robotics is discussed. The controller is designed in discrete time and it is synthesized for mechanical systems that are modeled by means of the Euler–Lagrange formulation. Making use of the discrete Hamilton–Jacobi–Isaacs equation the control law is derived. The discrete control law is then applied to a continuous-time 6-DoF bipedal robot model in order to track the walking pattern references for each link. The system along with the control law is simulated, with the system subjected to an external disturbance that emulates the action of a group of unknown bounded forces over the links of the bipedal robot. Furthermore, an algorithm to diminish the effect of the disturbance is proposed such that the full knowledge of the plant is not needed but only the linear part of the mass and inertia matrix; this algorithm is combined with the H∞ controller and applied to a robotic arm. Finally, this work is compared to a similar approach that uses H∞ technique in continuous time.}
}
@article{RAJ2019144,
title = {Dynamically feasible trajectory planning for Anguilliform-inspired robots in the presence of steady ambient flow},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {144-158},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018307565},
author = {Aditi Raj and Atul Thakur},
keywords = {Fish-inspired robots, Trajectory planning, Motion planning, Flow, Anguilliform-inspired robot, Heuristic function},
abstract = {A crucial requirement for imparting autonomy to the fish-inspired robots is the trajectory planning capability that can generate obstacle-free and optimal trajectories to a commanded goal location. Research in the area of dynamics-aware trajectory planning, especially in the presence of environmental disturbances, for fish-inspired robots is scarce. Trajectory-following for fish-inspired robots, which is different from trajectory planning, has been well explored wherein feedback controllers that reject disturbances are employed to follow the pre-specified waypoints. This paper reports an optimal trajectory planning approach for Anguilliform-inspired robots based on model predictive planning framework. The robot’s dynamics constraints, as well as its interaction with the surrounding flow conditions, are captured via dynamics simulations and are expressed using discretized motion primitives. The motion primitives are then used for generating a search-tree and an A*-based algorithm is used for searching the optimal trajectory. We developed a simulation-based admissible heuristic function that is used for improving the computational performance of the developed trajectory planner. The developed simulation-based heuristic function provided a computational speed-up by a factor of up to 10.3 with respect to that of the Euclidean heuristic for test cases comprising of a variety of obstacle densities, ambient flow conditions, and goal locations. The trajectories generated by the developed approach have been found to be dynamically feasible, collision-free, and optimal for the experiments reported in this paper. We believe that the developed approach can impart autonomy to the Anguilliform-inspired robots that, due to their slender and hyper-redundant structures, can perform autonomous inspection and maintenance of sub-sea structures having narrow regions, obstacles, and ambient flow and can be useful in various civil and defense applications.}
}
@article{ZAHRA2019135,
title = {Synthesis of a hybrid brain for a humanoid robot},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {135-150},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017307819},
author = {Omar Zahra and Mohamed Fanni and Abdelfatah M. Mohamed},
keywords = {Computer vision, Invariant object-recognition, Brain-Based Device, Perceptual categorization, Neural simulation},
abstract = {This article comprehends the design of a Brain-Based Robot (BBR) using hybrid techniques that incorporate both Brain-Based Device (BBD) and computational algorithms. BBDs are biologically inspired machines which have its behavior guided by a simulated nervous system. This nervous system follows detailed neuroanatomy of different brain areas. BBDs tend to have a nervous system with a large number of neurons and synapses. Thus, a huge computational power is required to simulate the nervous system of a BBD. Nevertheless, some of the tasks carried out by the simulated nervous system can be accomplished using computational algorithms which can help reducing the required computational power greatly. In this article, a BBR is built which combines some subsystems from BBD with computer vision algorithms. Computer vision algorithms are applied using OpenCV to extract some features from images, while neuronal-areas are connected together based on a detailed neuroanatomical structure to mimic the human learning process. Nengo python package is used for simulating neuronal areas in the system and monitoring activities of neuronal units. Moreover, the successful integration of the BBD’s subsystems with computer vision leads to the perceptual categorization based on invariant object-recognition of various visual cues. To make a fair comparison with BBD, the nervous system of a BBD is built on the same computer used to build the hybrid brain for the proposed BBR. The proposed hybrid brain is then applied to a Nao humanoid robot in V-REP simulation environment to test it. The results obtained through this article prove that the proposed hybrid brain possesses the same intelligence of the BBD and requires much less computational power that it can run on an on-board computer of a robot, which makes it plausible for engineering applications.}
}
@article{LI2019103253,
title = {Muscle tension training method for series elastic actuator (SEA) based on gain-scheduled method},
journal = {Robotics and Autonomous Systems},
volume = {121},
pages = {103253},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.103253},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019303197},
author = {Jian Li and Siqi Li and Guihua Tian and Hongcai Shang},
keywords = {Series elastic actuator, Gain-scheduled, Muscle tension training},
abstract = {A muscle tension training device that contains series elastic actuators (SEAs) has high safety and control performance in human–machine interaction equipment. Based on the cascade impedance controller and the electromyographic (EMG) sensor signal, this paper proposes a self-adaptive gain-scheduled algorithm. The algorithm automatically adjusts the stiffness gain value according to the muscle force. Simultaneously the stable gain function of the passivity condition can ensure the interaction stability. A cascade impedance controller is the basis for ensuring the stiffness of the port and the stability of the interaction; the gain-scheduled function is derived based on the acquired EMG signal and the pre-set muscle exercise mode. Therefore, the control structure is highly efficient, safe to use and offers diverse strength training modes. The simulation and experimental results show that the stiffness gain-scheduled controller can accurately achieve matching of the force and port stiffness. Furthermore, the interaction process ensures precise stability. The gain-scheduled method can adjust the contact stiffness in real time according to the needs of the experimenter. It changes the way muscles exercise under the original constant stiffness. This method that has a personalized exercise feature provides a new solution for improving dynamic training.}
}
@article{JANGSHER2019263,
title = {An embodied, platform-invariant architecture for connecting high-level spatial commands to platform articulation},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {263-277},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017301835},
author = {A. {Jang Sher} and U. Huzaifa and J. Li and V. Jain and A. Zurawski and A. LaViers},
abstract = {In contexts such as teleoperation, robot reprogramming, human–robot-interaction, and neural prosthetics, conveying movement commands to a robotic platform is often a limiting factor. Currently, many applications rely on joint-angle-by-joint-angle prescriptions. This inherently requires a large number of parameters to be specified by the user that scales with the number of degrees of freedom on a platform, creating high bandwidth requirements for interfaces. This paper presents an efficient representation of high-level, spatial commands that specifies many joint angles with relatively few parameters based on a spatial architecture that is judged favorably by human viewers. In particular, a general method for labeling connected platform linkages, generating a databank of user-specified poses, and mapping between high-level spatial commands and specific platform static configurations are presented. Thus, this architecture is “platform-invariant” where the same high-level, spatial command can be executed on any platform. This has the advantage that our commands have meaning for human movers as well. In order to achieve this, we draw inspiration from Laban/Bartenieff Movement Studies, an embodied taxonomy for movement description. The architecture is demonstrated through implementation on 26 spatial directions for a Rethink Robotics Baxter, an Aldebaran NAO, and a KUKA youBot. User studies are conducted to validate the claims of the proposed framework.}
}
@article{VALLVE2019108,
title = {Pose-graph SLAM sparsification using factor descent},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {108-118},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303002},
author = {Joan Vallvé and Joan Solà and Juan Andrade-Cetto},
keywords = {Mobile robotics, SLAM, Sparsification, Factor recovery, Topology},
abstract = {Since state of the art simultaneous localization and mapping (SLAM) algorithms are not constant time, it is often necessary to reduce the problem size while keeping as much of the original graph’s information content. In graph SLAM, the problem is reduced by removing nodes and rearranging factors. This is normally faced locally: after selecting a node to be removed, its Markov blanket sub-graph is isolated, the node is marginalized and its dense result is sparsified. The aim of sparsification is to compute an approximation of the dense and non-relinearizable result of node marginalization with a new set of factors. Sparsification consists on two processes: building the topology of new factors, and finding the optimal parameters that best approximate the original dense distribution. This best approximation can be obtained through minimization of the Kullback–Liebler divergence between the two distributions. Using simple topologies such as Chow–Liu trees, there is a closed form for the optimal solution. However, a tree is oftentimes too sparse and produces bad distribution approximations. On the contrary, more populated topologies require nonlinear iterative optimization. In the present paper, the particularities of pose-graph SLAM are exploited for designing new informative topologies and for applying the novel factor descent iterative optimization method for sparsification. Several experiments are provided comparing the proposed topology methods and factor descent optimization with state-of-the-art methods in synthetic and real datasets with regards to approximation accuracy and computational cost.}
}
@article{WANG201993,
title = {A submap joining algorithm for 3D reconstruction using an RGB-D camera based on point and plane features},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {93-111},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018302033},
author = {Jun Wang and Jingwei Song and Liang Zhao and Shoudong Huang and Rong Xiong},
keywords = {RGB-D, SLAM, 3D reconstruction, Indoor mapping},
abstract = {In standard point-based methods, the depth measurements of the point features suffer from noise, which will lead to incorrect global structure of the environment. This paper presents a submap joining based SLAM with an RGB-D camera by introducing planes as well as points as features.This work is consisted of two steps: submap building and submap joining. Several adjacent keyframes, with the corresponding small patches, visual feature points, and planes observed from these keyframes, are used to build a submap. We fuse the submaps into a global map in a sequential fashion, such that, the global structure is recovered gradually through plane feature associations and optimization. We also show that the proposed algorithm can handle plane association problem incrementally in submap level, as the plane covariance can be obtained in each submap. The use of submap significantly reduces the computational cost during the optimization process, while keeping all information about planes. The proposed method is validated using both publicly available RGB-D benchmarks and datasets collected by authors. The algorithm can produce accurate trajectories and high quality 3D models on these challenging datasets, which are difficult for existing RGB-D SLAM or SFM algorithms.}
}
@article{MOLINOS2019112,
title = {Dynamic window based approaches for avoiding obstacles in moving},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {112-130},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018309746},
author = {Eduardo J. Molinos and Ángel Llamazares and Manuel Ocaña},
keywords = {Dynamic obstacle avoidance, Dynamic window, Mapping},
abstract = {In recent years, Unmanned Ground Vehicles (UGVs) have been widely used as service robots. Unlike industrial robots, which are situated in fixed and controlled positions, UGVs work in dynamic environments, sharing the environment with other vehicles and humans. These robots should be able to move without colliding with any obstacle, assuring its integrity and the environment safety. In this paper, we propose two adaptations of the classical Dynamic Window algorithm for dealing with dynamic obstacles like Dynamic Window for Dynamic Obstacles (DW4DO) and Dynamic Window for Dynamic Obstacles Tree (DW4DOT). These new algorithms are compared with our previous algorithms based on Curvature Velocity Methods: Predicted Curvature Velocity Method (PCVM) and Dynamic Curvature Velocity Method (DCVM). Proposals have been validated in both simulated and real environment using several robotic platforms.}
}
@article{YAZDANI201980,
title = {Query-based integration of heterogeneous knowledge bases for search and rescue tasks},
journal = {Robotics and Autonomous Systems},
volume = {117},
pages = {80-91},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303488},
author = {Fereshta Yazdani and Sebastian Blumenthal and Nico Huebel and Asil Kaan Bozcuoğlu and Michael Beetz and Herman Bruyninckx},
keywords = {Querying big data, Knowledge sharing, Knowledge management, Knowledge maintenance},
abstract = {Recently, advances in robotics’ technology and research focus on complex scenarios. In these scenarios, robots have to act and respond fast to situational demands. First, they require heterogeneous knowledge from various sources. Then, they need to integrate this knowledge with their reasoning methodologies. These reasoning methodologies are typically different for every domain. This paper introduces an integrated knowledge processing methodology. This methodology uses query mechanisms and model-to-model transformations. Combining these two mechanisms enables processing of heterogeneous knowledge bases. The methodology is demonstrated for an outdoor scenario with diverse systems. In this scenario knowledge and reasoning methods from various sources are integrated. This includes static knowledge from. Open Sreet Map and Digital Elevation Models. The Robot Scene Graph tracks changes in the world and provides geometric reasoning. KnowRob with its Sherpa ontology and openEASE provide further reasoning capabilities.}
}
@article{DAI2019114,
title = {A novel STSOSLAM algorithm based on strong tracking second order central difference Kalman filter},
journal = {Robotics and Autonomous Systems},
volume = {116},
pages = {114-125},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018305025},
author = {Jiahui Dai and Xiaobo Li and Kequan Wang and Yunpei Liang},
keywords = {FastSLAM, CDFastSLAM, STSOSLAM, Particle filter, Second order central difference particle filter, Strong tracking filter},
abstract = {Simultaneous Localization and Mapping (SLAM) is an effective technique in the field of robot location and navigation. However, when the existing SLAM algorithm is applied in harsh terrain, such as the terrain found in coal mines, accuracy suffers, and on-line adaptive adjustment capability is poor. Furthermore, the system suffers from low robustness and is susceptible to random noise. In order to solve these problems, we propose an innovative Strong Tracking Second Order Central Difference SLAM (STSOSLAM) algorithm that combines a Strong Tracking Filter (STF), a Second-Order Central Differential Filter (SOCDF), and a Particle Filter (PF). The new algorithm utilizes the second order sterling interpolation formula to deal with the nonlinear system problem using the Cholesky decomposition technique, which propagates directly by using the covariance square root factor in the SLAM probabilistic estimation. This technique not only guarantees the positive definite property of the covariance matrix, but also reduces the truncation error of local linearization. In addition, STF is introduced into the algorithm. It updates every sigma point using an adaptive algorithm and obtains optimized filter gain through the STF online adjustment factor and suppresses uncertain noise and the influence of initial value selection. Through simulation and experiments, STSOSLAM algorithm shows much better performance in terms of estimation accuracy, robustness and reliability than FastSLAM2.0 and Central Difference FastSLAM (CDFastSLAM) algorithms, establishing the foundation of applying the STSOSLAM algorithm in the harsh terrain of coal mines.}
}
@article{HYUN201924,
title = {Walking propulsion generation in double stance by powered exoskeleton for paraplegics},
journal = {Robotics and Autonomous Systems},
volume = {116},
pages = {24-37},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018306511},
author = {Dong Jin Hyun and Hyunseop Lim and SangIn Park and JuYoung Yoon and Kyungmo Jung and KiHyeon Bae and Inju Lee},
keywords = {Lower-limb exoskeleton, Impedance control, Gait trajectory planning, Double-stance phase, Human locomotion},
abstract = {This paper introduces an electric Hyundai Medical Exoskeleton (H-MEX). It is specially designed to enable disabled people (e.g. spinal cord injury individuals below T10, stroke patients) to walk again, according to a basic walking control strategy. H-MEX is easily assembled with mechanically/electrically block-by-block connections, and its built-in control framework provides an unique control interface. Through this interface, the H-MEX wearer can customize gait parameters (viz., the step length, step period, and default torso tilt angle). With the proposed framework, trajectories for each active joint are planned for generating propulsion (i.e., angular momentum) in double-stance gait. This facilitates stability and convenience for H-MEX wearers. A dynamic simulation was conducted on a simplified planar model that describes an average human body: the intended propulsion generation during the double-stance phase was verified to lead to angular momentum with respect to a leading stance leg, for more stable and convenient step walking. Also, the degree of propulsion was shown to be adjusted by setting kinematic percentage of intended double-stance motion. The proposed control method was evaluated with five healthy subjects on a treadmill as one of initial performance tests: kinematic data on subjects’ torsos given from basic walking at a velocity of 0.7 km/h and 1.2 km/h indicated the effectiveness of the proposed control strategy.}
}
@article{LING2019134,
title = {Dual-arm cooperation and implementing for robotic harvesting tomato using binocular vision},
journal = {Robotics and Autonomous Systems},
volume = {114},
pages = {134-143},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S092188901830808X},
author = {Xiao Ling and Yuanshen Zhao and Liang Gong and Chengliang Liu and Tao Wang},
keywords = {Binocular vision, Dual-arm cooperation, Tomato detection, Three-dimensional scene reconstruction, Harvesting cycle time},
abstract = {Dual-arm cooperation is considered as an available approach to improve the poor efficiency by autonomous robotic harvesting. While, cooperating arm movements using visual information is a key challenge for harvesting robots working in non-structured environments. In this paper, we develop a dual-arm cooperative approach for a tomato harvesting robot using a binocular vision sensor. Firstly, a tomato detection algorithm combining AdaBoost classifier and color analysis is proposed and employed by the harvesting robot. Then, a fast three-dimensional scene reconstruction method is obtained in the simulation environment by using point clouds acquired from a stereo camera. Integration of tomato detection, target localization, motion planning and real-time control for dual-arm movements, the dual-arm cooperation for robotic harvesting can be implemented. To validate the proposed approach, field experiments were conducted with the potted tomatoes in greenhouse. Over 96% of target tomatoes were correctly detected with the speed of about 10 fps. The positioning error of robot end-point of less than 10 mm was achieved for large scale direct positioning of the harvesting robot. With the vacuum cup grasping and wide-range cutting, the success rate of robotic harvesting achieved 87.5%. Meanwhile, the harvesting cycle time excluding cruise time was less than 30 s. These results indicate that the dual-arm cooperative approach is feasible and practical for robotic harvesting in non-structured environments.}
}
@article{SUBBURAMAN2019103257,
title = {Human inspired fall prediction method for humanoid robots},
journal = {Robotics and Autonomous Systems},
volume = {121},
pages = {103257},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.103257},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018309187},
author = {Rajesh Subburaman and Dimitrios Kanoulas and Luca Muratore and Nikos G. Tsagarakis and Jinoh Lee},
keywords = {Humanoid robots, Fall prediction, Multi-sensor fusion},
abstract = {Humanoid robots are anticipated to work like humans in unstructured environments, and in such cases, falling over is inevitable due to the inherent postural instabilities and external disturbances arising from the environments. Since falling over may annihilate both the robot and its surroundings, we introduce in this work a generic method to predict the falling over of humanoid robots in a reliable, robust, and agile manner across various terrains, and also amidst arbitrary disturbances. The aforementioned characteristics have been strived to attain by proposing a prediction principle inspired by the human balance sensory systems. Accordingly, the fusion of multiple sensors such as inertial measurement unit and gyroscope (IMU), foot pressure sensor (FPS), joint encoders, and stereo vision sensor, which are equivalent to the human’s vestibular, proprioception, and vision systems are considered. For the prediction process, we first define a set of feature-based fall indicator variables (FIVs) from the different sensors, introduce prediction window parameters, and the thresholds for the FIVs are extracted using those parameters for four major disturbance scenarios. Further, an online threshold interpolation technique and an impulse adaptive counter limit are proposed to manage more generic disturbances. Finally, an instantaneous integer value is computed for each FIVs using their respective thresholds and the cumulative sum of them are normalized to predict the fall over by setting a suitable value as the critical limit. To determine the best combination and the usefulness of multiple sensors, the prediction performance is evaluated on four different types of terrains, in three unique combinations: first, each feature individually with their respective FIVs; second, an intuitive performance-based (PF); and finally, Kalman filter based (KF) techniques, which involve the usage of multiple features. For PF and KF techniques, prediction performance evaluations are carried out with and without adding noise to ascertain the influence of sensor noise. Overall, it is reported that KF performed better than PF and individual sensor features under different conditions. Also, the method’s ability to predict fall overs during the robot’s simple dynamic motion is also tested and verified through simulations. Experimental verification of the proposed prediction method on flat and uneven terrains is carried out with the WALK-MAN humanoid robot.}
}
@article{OZBILGE201968,
title = {Experiments in online expectation-based novelty-detection using 3D shape and colour perceptions for mobile robot inspection},
journal = {Robotics and Autonomous Systems},
volume = {117},
pages = {68-79},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019300636},
author = {Emre Özbilge},
keywords = {Online learning, Novelty detection, Robot learning, Mobile robot inspection, Image processing, Neural network},
abstract = {Novelty detection is a very useful function for detecting abnormal data in any application. An expectation-based novelty-detection approach has been introduced that learns the dynamic relationship model among normal data in order to predict the next expected data. Most novelty-detection systems use an offline approach with a fixed structure, a system type that has limitations when the data count in the environment is unknown. A new expectation-based novelty-detection system features an online recurrent neural network approach that learns the data by inserting new nodes or deleting unused nodes from its structure. Generally, to detect novelties, a global novelty threshold is defined to filter out all input data as novel whenever the prediction error of the network exceeds the threshold. However, because a neural network cannot learn to predict all classes of input data perfectly, using a global novelty threshold leads to the misclassification of the insufficiently learned normal data as novel. To overcome this problem, the novelty-detection system has been improved to learn local novelty thresholds alongside its learning to predict expectations. The proposed algorithm is applied to an online novelty detection using colour and depth data obtained from a Kinect sensor on a mobile robot. The performance of the expected novelty detector and its limitations during experiments are analysed and shown. Furthermore, colour and depth data as inputs into the novelty filter are separately analysed and their contributions on the overall novelty detection highlighted. In conclusion, the performance of the novelty filter could further be improved by applying a better feature-selection technique to extract more interesting features from high-dimensional input data.}
}
@article{LIU2019104,
title = {Review, classification and structural analysis of downhole robots: Core technology and prospects for application},
journal = {Robotics and Autonomous Systems},
volume = {115},
pages = {104-120},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018306109},
author = {Qingyou Liu and Jianguo Zhao and Haiyan Zhu and Guorong Wang and John D. McLennan},
keywords = {Downhole robot, Drilling robot, Traction robot, Tractor, Downhole tool, Horizontal well},
abstract = {In this paper, downhole robots are classified into two categories according to their structures and applications. And, two categories are downhole traction robots and downhole drilling robots. Meanwhile, attributes of the downhole robots, such as the working parameters, applications, environmental tolerances and design principles for each core technology are analyzed. According to the analysis parameters, the downhole robots can be divided into several small parts, such as wheeled type, telescopic type, pedrail type and so on. In addition, four key technical criteria (support structure, drive structure, control system and power supply method) are summarized for the downhole robots. And, four new application schemes are proposed for the downhole robots. Thus, the purpose of this review is to provide a concise reference, which summarizes the development and research directions, as well as the design and the application of downhole robots.}
}
@article{MARINPLAZA2019251,
title = {iCab Use Case for ROS-based Architecture},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {251-262},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S092188901830201X},
author = {Pablo Marin-Plaza and Ahmed Hussein and David Martin and Arturo {de la Escalera}},
keywords = {Autonomous vehicles, Icab, Ros, Software architecture, Path planning},
abstract = {In the Intelligent Transportation Systems Community, the research interest in intelligent and autonomous vehicles is increasing during the last few years. Accordingly, this paper presents the advances in the development of a ROS-based (Robot Operating System) software architecture for intelligent vehicles. The main contribution of the architecture is its powerfulness (in the aspect of complete architecture managing the navigation, synchronization, communication, cooperation, ground station, web server, sound system), flexibility (due to the fact that autonomous vehicles evolve exponentially where new sensors or algorithms emerge every day), and modularity (in the form of interchangeability and replaceability for new modules), which allow researchers to develop and test different algorithms easily and faster. The architecture has been tested on two real platforms such as the iCab (Intelligent Campus Automobile) and IvvI 2.0 vehicle (Intelligent Vehicle based on Visual Information) The first one is a full y autonomous vehicle and the second one provides Advance Driver Assistance. Additionally, it is shown the use case for the iCab project where the scope is focused on the critical element of navigation. The use case exposed proves the benefits of using this architecture over a centralized one.}
}
@article{HU201957,
title = {Trajectory generation with multi-stage cost functions learned from demonstrations},
journal = {Robotics and Autonomous Systems},
volume = {117},
pages = {57-67},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303075},
author = {Jin Hu and Rong Xiong},
keywords = {Trajectory generation, Learning from demonstration, Cost learning, Dynamic programming},
abstract = {Learning from demonstration provides an effective method to resolve the problem of teaching robot to execute complex motions without expert knowledge about the robot system. In this paper, we present a novel learning from demonstration method based on multi-stage cost learning. The recorded demonstrations are assumed to be composed of several substages chained together. Leveraging this assumption, a segmentation and cost learning framework is proposed to search for the cutting points that divide the unsegmented demonstrations into multiple substages and retrieve cost function for each substage. To the best of our knowledge, it is the first solution to learn multi-stage cost functions in continuous domain without restricting the possible cost functions into limited numbers or simple forms. To generate new trajectory, a complete objective functional is constructed based on the learned multi-stage cost functions plus other constraints like obstacle avoidance and is optimized with functional gradient method. The generated trajectory can adapt to new environments while maintain the specific properties of each substage as demonstrations. The effectiveness of the proposed method is verified through simulation study and experiments conducted on a real robot manipulator.}
}
@article{WILDE2019159,
title = {A robotics-oriented taxonomy of how ethologists characterize the traversability of animal environments},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {159-166},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018306468},
author = {Grant A. Wilde and Robin R. Murphy},
abstract = {This article surveys 21 studies of how ethologists characterize the environment for arthropods, reptiles, mammals, and birds traversing above ground, below ground, and burrowing in order to provide insights in selecting or designing a robot for a complex work envelope, for example, the 2018 Thailand Cave rescue. Roboticists are currently forced to rely on empirical expertise to select or design robots and to construct ad hoc testbeds for expected environments due to the lack of comprehensive quantitative metrics. Fortunately, ethologists have been grappling with how to quantify environmental factors that impact the traversability of different animals. That community has collectively identified 21 characteristics which this article discusses and groups into a novel taxonomy of three broad categories: local navigational constraints, surface properties, and global layout properties. One conclusion is that the set of appropriate characteristics for a specific environment depends on the scale of the environment to the agent. The article also makes four recommendations to aid roboticists in a) selecting a particular robot suitable for the environmental characteristics, b) building testbeds that are more representative of the target environment or to objectively compare different robotics, and c) collecting data about an environment for use cases or work analyses. It also discusses the limitations of the ethological studies for robotics and the remaining gaps.}
}
@article{LIM2019103227,
title = {Skill-based anytime agent architecture for European Robotics Challenges in realistic environments: EuRoC Challenge 2, Stage II — realistic labs},
journal = {Robotics and Autonomous Systems},
volume = {120},
pages = {103227},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017307947},
author = {Gi Hyun Lim and Eurico Pedrosa and Filipe Amaral and Artur Pereira and Nuno Lau and José Luís Azevedo and Bernardo Cunha and Simone Badini},
keywords = {Skill-based, Anytime agent architecture, Mobile manipulation, Autonomous packaging, European robotics challenges (EuRoC)},
abstract = {As demands on pragmatic solutions of robotics technology increase in the manufacturing industry, deep affinities between research experts and industry users are required. The European Robotics Challenges (EuRoC) research project has proposed a scientific competition and matched up research labs with industrial end users to establish challenger teams to develop and test solutions that will be applied in the real context of the industrial end-users. The paper reports the result of TIMAIRIS who is one of 6 challenger teams to advance to the final stage out of 103 teams and technical details used in the Challenge 2 - Shop Floor Logistics and Manipulation. To address the requirements and achieve the objectives of the challenge, a skill-based anytime agent architecture has been developed and extended to make the team focus on the challenging research that addresses real issues in the user environments. Finally, shop floor logistics and manipulation scenarios have been developed and demonstrated in a realistic environment for autonomous packaging.}
}
@article{RODRIGUEZ201957,
title = {Spontaneous talking gestures using Generative Adversarial Networks},
journal = {Robotics and Autonomous Systems},
volume = {114},
pages = {57-65},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018304445},
author = {Igor Rodriguez and José María Martínez-Otzeta and Itziar Irigoien and Elena Lazkano},
keywords = {Social robotics, Generative learning models, Motion generation, Principal coordinate analysis, Body language expression, Generative adversarial networks},
abstract = {This paper presents a talking gesture generation system based on Generative Adversarial Networks, along with an evaluation of its adequateness. The talking gesture generation system produces a sequence of joint positions of the robot’s upper body which keeps in step with an uttered sentence. The suitability of the approach is demonstrated with a real robot. Besides, the motion generation method is compared with other (non-deep) generative approaches. A two-step comparison is made. On the one hand, a statistical analysis is performed over movements generated by each approach by means of Principal Coordinate Analysis. On the other hand, the robot motion adequateness is measured by calculating the end effectors’ jerk, path lengths and 3D space coverage.}
}
@article{TONUTTI2019162,
title = {Robust and subject-independent driving manoeuvre anticipation through Domain-Adversarial Recurrent Neural Networks},
journal = {Robotics and Autonomous Systems},
volume = {115},
pages = {162-173},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018301209},
author = {Michele Tonutti and Emanuele Ruffaldi and Alessandro Cattaneo and Carlo Alberto Avizzano},
keywords = {Manoeuvre anticipation, ADAS, Deep learning, LSTM, Recurrent neural networks, Domain adaptation},
abstract = {Through deep learning and computer vision techniques, driving manoeuvres can be predicted accurately a few seconds in advance. Even though adapting a learned model to new drivers and different vehicles is key for robust driver-assistance systems, this problem has received little attention so far. This work proposes to tackle this challenge through domain adaptation, a technique closely related to transfer learning. A proof of concept for the application of a Domain-Adversarial Recurrent Neural Network (DA-RNN) to multi-modal time series driving data is presented, in which domain-invariant features are learned by maximising the loss of an auxiliary domain classifier. Our implementation is evaluated using a leave-one-driver-out approach on individual drivers from the Brain4Cars dataset, as well as using a new dataset acquired through driving simulations, yielding an average increase in performance of 30% and 114% respectively compared to no adaptation. We also show the importance of fine-tuning sections of the network to optimise the extraction of domain-independent features. The results demonstrate the applicability of the approach to driver-assistance systems as well as training and simulation environments.}
}
@article{TANG2019103251,
title = {Dynamic target searching and tracking with swarm robots based on stigmergy mechanism},
journal = {Robotics and Autonomous Systems},
volume = {120},
pages = {103251},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.103251},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019301538},
author = {Qirong Tang and Zhipeng Xu and Fangchao Yu and Zhongqun Zhang and Jingtao Zhang},
keywords = {Stigmergy, Swarm robots, Indirect communication, Vectorial pheromone, Dynamic target},
abstract = {A strategy based on stigmergy mechanism for swarm robots interaction is proposed in this paper. The stigmergy mechanism refers to an interactive method in which robots communicate indirectly through the pheromone left in the environment by robots. The proposed stigmergic strategy that uses a vectorial pheromone model is applied for swarm robots to search and track a dynamic target. Some simulations and experiments are implemented to observe the performance of the strategy. In these investigations, RFID tags which are very cheap and convenient are arranged in the environment as carriers of the pheromones for indirect communication among robots. Robots read and write the vectorial pheromones while moving in the environment, and determine their behaviors through the vectorial pheromones. The results from simulations and experiments of using different numbers of robots show the swarm robots are able to find a dynamic target quickly and then keep a close track to the target, which demonstrates the feasibility and scalability of this strategy.}
}
@article{YE2019126,
title = {Robot learning of manipulation activities with overall planning through precedence graph},
journal = {Robotics and Autonomous Systems},
volume = {116},
pages = {126-135},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303622},
author = {Xin Ye and Zhe Lin and Yezhou Yang},
keywords = {Manipulation precedence graph, Understanding human activities, Intelligent systems, AI and robotics},
abstract = {One critical aspect of robotic visual learning is to capture the precedence relations among primitive actions from observing human performing manipulation activities. Current state-of-the-art spatial–temporal representations do not fully capture the precedence relations. In this paper, we present a novel activity representation: Manipulation Precedence Graph (MPG) and its associated overall planning module, with the goal to enable robot to learn manipulation activities from human demonstrations with overall planning. Experiments conducted on three publicly available manipulation activity video corpora as well as on a simulation platform validate that (1) the generated MPG from our system is robust given noisy detections from perception modules; (2) the overall planning module is able to generate parallel action sequences for robot to execute them in parallel; (3) the overall system improves robots’ manipulation execution efficiency.}
}
@article{WANG201949,
title = {Towards a 3D passive dynamic walker to study ankle and toe functions during walking motion},
journal = {Robotics and Autonomous Systems},
volume = {115},
pages = {49-60},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018306857},
author = {Kunyang Wang and Pablo Tena Tobajas and Jing Liu and Tao Geng and Zhihui Qian and Lei Ren},
keywords = {Biologically inspired robot, Passive walker, Biomechanics, Ankle, Toe},
abstract = {The ankle–foot complex in the human body is one of the major determinants in normal human walking. Most of the research study ankle and foot motion by observing people as they move, measuring desired kinematic and kinetic data indoor or outdoor, and numerical simulations in computer. However, very few studies are able to explore the fundamental mechanical principles underlying human musculoskeletal system. In this paper, we developed a three-dimension (3D) passive dynamic walker with flat feet, toes and ankle springs to investigate the impact of the ankle and toe stiffness in the walking motion. The results suggest that the ankle springs have a main impact on the walking motion, where the anterior spring, over any other position, plays a main role in providing sagittal stability. The springs from the sagittal plane control the pitch angle of the robot which impacts on its velocity and step length. The stability got worse along with the step length and velocity increasing especially when the step length overcame 8 cm. The fact that the best configuration of the ankle joint has stiffer stiffness in the sagittal plane than the coronal plane complies in nature with humans where Tibialis Anterior, Soleus and Gastrocnemius muscles are much stronger than other muscles around the ankle. Furthermore, it can be stated that the medial toe plays a more important role than the lateral one, as blocking the medial toe with the stiffest joint (rigid joint) has a negative effect on walking motion. In conclusion, we show that the ankle stiffness of the robot in anterior–posterior position should be higher than that in medial–lateral position and the stiffness in any position should exceed a minimum level to maintain walking stability. Also, adding toes (medial one should be softer than the lateral) in the foot of the robot may benefit biped locomotion especially when taking longer step length.}
}
@article{ZHAO2019103234,
title = {Learning Kalman Network: A deep monocular visual odometry for on-road driving},
journal = {Robotics and Autonomous Systems},
volume = {121},
pages = {103234},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S092188901930154X},
author = {Cheng Zhao and Li Sun and Zhi Yan and Gerhard Neumann and Tom Duckett and Rustam Stolkin},
keywords = {Monocular visual odometry, Learning Kalman Filter, Vehicle driving},
abstract = {This paper proposes a Learning Kalman Network (LKN) based monocular visual odometry (VO), i.e. LKN-VO, for on-road driving. Most existing learning-based VO focus on ego-motion estimation by comparing the two most recent consecutive frames. By contrast, the LKN-VO incorporates a learning ego-motion estimation through the current measurement, and a discriminative state estimator through a sequence of previous measurements. Superior to the model-based monocular VO, a more accurate absolute scale can be learned by LKN without any geometric constraints. In contrast to the model-based Kalman Filter (KF), the optimal model parameters of LKN can be obtained from dynamic and deterministic outputs of the neural network without elaborate human design. LKN is a hybrid approach where we achieve the non-linearity of the observation model and the transition model though deep neural networks, and update the state following the Kalman probabilistic mechanism. In contrast to the learning-based state estimator, a sparse representation is further proposed to learn the correlations within the states from the car’s movement behaviour, thereby applying better filtering on the 6DOF trajectory for on-road driving. The experimental results show that the proposed LKN-VO outperforms both model-based and learning state-estimator-based monocular VO on the most well-cited on-road driving datasets, i.e. KITTI and Apolloscape. In addition, LKN-VO is integrated with dense 3D mapping, which can be deployed for simultaneous localization and mapping in urban environments.}
}
@article{PATEL201980,
title = {A deep learning gated architecture for UGV navigation robust to sensor failures},
journal = {Robotics and Autonomous Systems},
volume = {116},
pages = {80-97},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018305645},
author = {Naman Patel and Anna Choromanska and Prashanth Krishnamurthy and Farshad Khorrami},
keywords = {Robustness to sensor failures, Deep learning for autonomous navigation, Vision/LiDAR based navigation, Learning from demonstration, Sensor fusion, Autonomous vehicles},
abstract = {In this paper, we introduce a novel methodology for fusing sensors and improving robustness to sensor failures in end-to-end learning based autonomous navigation of ground vehicles in unknown environments. We propose the first learning based camera–LiDAR fusion methodology for autonomous in-door navigation. Specifically, we develop a multimodal end-to-end learning system, which maps raw depths and pixels from LiDAR and camera, respectively, to the steering commands. A novel gating based dropout regularization technique is introduced which effectively performs multimodal sensor fusion and reliably predicts steering commands even in the presence of various sensor failures. The robustness of our network architecture is demonstrated by experimentally evaluating its ability to autonomously navigate in the indoor corridor environment. Specifically, we show through various empirical results that our framework is robust to sensor failures, partial image occlusions, modifications of the camera image intensity, and the presence of noise in the camera or LiDAR range images. Furthermore, we show that some aspects of obstacle avoidance are implicitly learned (while not being specifically trained for it); these learned navigation capabilities are shown in ground vehicle navigation around static and dynamic obstacles.}
}
@article{RIVA2019221,
title = {Algorithms for limited-buffer shortest path problems in communication-restricted environments},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {221-230},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303865},
author = {Alessandro Riva and Arlind Rufi and Jacopo Banfi and Francesco Amigoni},
keywords = {Path planning, Communication constraints, Limited memory},
abstract = {In several applications, a robot moving from a start to a goal location is required to gather data along its path (e.g., a video feed in a monitoring scenario). The robot can have at its disposal only a limited amount of memory to store the collected data, in order to contain costs or to avoid that sensitive data fall into the hands of an attacker. This poses the need of periodically delivering the data to a Base Station (BS) through a deployed communication infrastructure that, in general, is not available everywhere. In this paper, we study this scenario by considering a variant of the shortest path problem (which we prove to be NP-hard) where the robot acquires information along its path, stores it into a limited memory buffer, and ensures that no information is lost by periodically communicating data to the BS. We present and evaluate an optimal exponential-time algorithm, an efficient feasibility test, and a polynomial-time heuristic algorithm.}
}
@article{FAIGL2019136,
title = {Adaptive locomotion control of hexapod walking robot for traversing rough terrains with position feedback only},
journal = {Robotics and Autonomous Systems},
volume = {116},
pages = {136-147},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018306614},
author = {Jan Faigl and Petr Čížek},
keywords = {Multi-legged robot, Locomotion control, Rough terrains},
abstract = {Traversing rough terrains is one of the domains where multi-legged walking robots benefit from their relatively more complex kinematics in comparison to wheeled robots. The complexity of walking robots is usually related not only to mechanical parts but also to servomotors and the necessary electronics to efficiently control such a robotic system. Therefore, large, middle, but even small walking robots capable of traversing rough terrains can be very costly because of all the required equipment. On the other hand, using intelligent servomotors with the position control and feedback, affordable hexapod walking robots are becoming increasingly available. However, additional sensors may still be needed to stabilize the robot motion on rough terrains, e.g., inclinometers or inertial measurement units, force or tactile sensors to detect the ground contact point of the leg foot-tip. In this work, we present a minimalistic approach for adaptive locomotion control using only the servomotors position feedback. Adaptive fine-tuning of the proposed controller is supported by a dynamic model of the robot leg accompanied by the model of the internal servomotor controller. The models enable timely detection of the leg contact point with the ground and reduce developed stress and torques applied to the robot construction and servomotors without any additional sensor feedback. The presented results support that the proposed approach reliably detects the ground contact point, and thus enable traversing rough terrains with small, affordable hexapod walking robot.}
}
@article{DONG201973,
title = {An optimal curvature smoothing method and the associated real-time interpolation for the trajectory generation of flying robots},
journal = {Robotics and Autonomous Systems},
volume = {115},
pages = {73-82},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017301173},
author = {Wei Dong and Ye Ding and Jie Huang and Luo Yang and Xiangyang Zhu},
keywords = {Trajectory generation, Curvature smoothing, Real-time interpolation, Flying robot},
abstract = {This paper presents a trajectory smoothing approach and corresponding real-time interpolation for the flying robot. To smoothly transit between straight line segments, the Bézier curve is introduced to guarantee continuous curvature. Subsequently, considering the constraints on approximation errors and lengths of the original straight line segments, an optimization problem pursuing maximal curvature radius of the Bézier curves is proposed to reduce the potential fluctuation in the real-time flights. With the established geometric profile of continuous curvature, a fast real-time interpolation approach that ensures smooth acceleration profile in real-time flights is proposed. To verify the effectiveness of this development, extensive simulations and experiments are conducted at last. The results show that the proposed trajectory generation approach can effectively generate reference trajectories in real-time both at two-dimensional and three-dimensional spaces with continuous curvature and smooth acceleration. With well-generated trajectory, the flying robot can closely track the reference with maximum cross-tracking error of 0.05 m.}
}
@article{SEKER2019173,
title = {Deep effect trajectory prediction in robot manipulation},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {173-184},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019300740},
author = {M. Yunus Seker and Ahmet E. Tekden and Emre Ugur},
keywords = {Robot learning, Predictive models, Long short term memory, Shape context, Manipulation, Convolutional neural networks},
abstract = {Imagining the consequences of one’s own actions, before and during their execution, allows the agents to choose actions based on their simulated performance, and to monitor the progress by comparing observed to simulated behavior. In this study, we propose a deep model that enables a robot to learn to predict the consequences of its manipulation actions from its own interaction experience on objects of various shapes. Given the top-down image of the object, the robot learns to predict the movement trajectory of the object during execution of a lever-up action performed with a screwdriver in a physics-based simulator. The prediction is realized in two stages; the system first computes a number of features from the object and then generates the complete motion trajectory of the center of mass of the object using Long Short Term Memory (LSTM) models. In the first step, we investigated use of various feature descriptors such as shape context that encodes a distributed representation of positions of the object boundary points, unsupervised features that are extracted from autoencoders, Convolutional Neural Network (CNN) based features that are conjointly trained with the LSTMs, and finally task-specific supervised features that are engineered to well-encode the underlying dynamics of the lever-up action. The models are trained in simulation with objects of varying edge numbers and tested in the simulated and the real world. Our deep and generic CNN-based LSTM model outperformed the predictors that use unsupervised representations such as shape descriptors or autoencoder features in the simulated test set. Additionally, it was shown to generalize well to novel object shapes that were not experienced during model training. Finally, our model was shown to perform well in predicting the consequences of lever-up actions generated by a screwdriver that was attached to the gripper of the real UR10 robot. We further showed that our system can predict qualitatively different trajectories of objects that roll off the table or tumble over as the result of lever-up action.}
}
@article{HOSSEININAVEHAHMADABADIAN201929,
title = {An automatic 3D reconstruction system for texture-less objects},
journal = {Robotics and Autonomous Systems},
volume = {117},
pages = {29-39},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017307431},
author = {Ali {Hosseininaveh Ahmadabadian} and Ali Karami and Rouhallah Yazdan},
keywords = {Structure from motion (SfM), Dense multi-view stereo (DMVS), 3D reconstruction system, Texture-less objects, Autonomous systems},
abstract = {Structure from Motion (SfM) is an image based method for 3D reconstruction of objects. This method coupled with Dense Multi-View Stereo (DMVS) can be used to generate an accurate point cloud of texture-full objects. Although this process is fully automatic, capturing images in proper locations is hard especially for texture-less objects which need a pattern projection procedure to have a successful matching step in SfM. This study aims to propose an automatic and portable system which can provide a pattern on objects and capture a set of high quality images in a way that a complete and accurate 3D model can be generated by the captured images using SFM and DMVS method. The system consists of three parts including a glassy turntable with a novel pattern projection system, a digital camera located on a mono-pod mounted on a length adjustable bar attached to the box of turntable and a controller system to control two other parts. Given the speed and step parameters for the system in a smart phone as the controller system, the digital camera automatically captures an image after every rotation step of the table. To evaluate the system, five different objects were tested under four criteria including plane fitting, structural resolution test, scale resolving test and comparing with a reference 3D model obtained with a commercial accurate laser scanner known as GOM ATOS Compact laser scanner. The average standard deviation for all the cited criteria was around 0.2 mm which illustrates the ability of the proposed system to capture high quality images for 3D reconstruction of texture-less objects.}
}
@article{ARBABMIR2019103249,
title = {Visual–inertial state estimation with camera and camera–IMU calibration},
journal = {Robotics and Autonomous Systems},
volume = {120},
pages = {103249},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.103249},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019300466},
author = {Mohammadvali Arbabmir and Masoud Ebrahimi},
keywords = {Visual–inertial odometry, Sensor fusion, Sensors calibration, State estimation, Optimization},
abstract = {In the last two decades, the Visual–Inertial Odometry (VIO) has recently received much attention for efficient and accurate ego-motion estimation of unmanned vehicle systems. In particular, the VIO includes only an Inertial Measurement Unit (IMU) and a camera. In this paper, we present a novel calibration approach for accurate deployment of monocular VIO. For this purpose, the hybrid optimization algorithm is used for calibrating the camera intrinsic and camera–IMU extrinsic calibration, automatically and without knowing the mechanical configuration. It is a professional work to carefully calibrate the intrinsic and extrinsic parameters, and it is required to repeat this work when the mechanical configuration of the sensor suite changes. Quantitative comparisons our method with the offline conventional calibration method on the KITTI dataset verify the efficacy and accuracy of the proposed method. We also demonstrate the performance of the proposed approach in large scale outdoor experiments.}
}
@article{PAULIUS201913,
title = {A Survey of Knowledge Representation in Service Robotics},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {13-30},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303506},
author = {David Paulius and Yu Sun},
keywords = {Knowledge representation, Robot learning, Task planning, Domestic robots, Service robotics},
abstract = {Within the realm of service robotics, researchers have placed a great amount of effort into learning, understanding, and representing motions as manipulations for task execution by robots. The task of robot learning and problem-solving is very broad, as it integrates a variety of tasks such as object detection, activity recognition, task/motion planning, localization, knowledge representation and retrieval, and the intertwining of perception/vision and machine learning techniques. In this paper, we solely focus on knowledge representations and notably how knowledge is typically gathered, represented, and reproduced to solve problems as done by researchers in the past decades. In accordance with the definition of knowledge representations, we discuss the key distinction between such representations and useful learning models that have extensively been introduced and studied in recent years, such as machine learning, deep learning, probabilistic modeling, and semantic graphical structures. Along with an overview of such tools, we discuss the problems which have existed in robot learning and how they have been built and used as solutions, technologies or developments (if any) which have contributed to solving them. Finally, we discuss key principles that should be considered when designing an effective knowledge representation.}
}
@article{KARIMOV201917,
title = {Advanced tone rendition technique for a painting robot},
journal = {Robotics and Autonomous Systems},
volume = {115},
pages = {17-27},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018309321},
author = {Artur I. Karimov and Ekaterina E. Kopets and Vyacheslav G. Rybin and Sergey V. Leonov and Anzhelika I. Voroshilova and Denis N. Butusov},
keywords = {Robotics, Robotic art, Painterly rendering, Color rendition, Brushstroke rendering, Computer creativity},
abstract = {One of the unresolved problems in the field of computer creativity is developing a robot capable of creating full-color images with artistic paints in a human-like manner. While several advanced painting machines have been presented up to date, high-grade color rendition remains one of the main bottlenecks in robotic painting. In this paper, we present a robotic setup for realistic grayscale painting. The key feature of our robot is a special paint mixing device aimed at improving tone rendition in comparison to previously reported approaches. We describe the main algorithmic and hardware solutions implemented in our robot as well as the first experimental results. The robot represents a 3-DoF CNC machine equipped with a brush, the paint mixing device and a syringe pump block for paint supplement. In our study, we focus on monochrome painting with black and white acrylic paints. A mathematical model of primary paint mixing is described. Two realistic artworks have been created during test runs, and their reproductions together with the source images are given. The accuracy of tone rendition was experimentally tested. Further research will be aimed at high-grade color rendition and creating full-color paintings.}
}
@article{SOROUR2019131,
title = {Complementary-route based ICR control for steerable wheeled mobile robots},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {131-143},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018305086},
author = {Mohamed Sorour and Andrea Cherubini and Abdellah Khelloufi and Robin Passama and Philippe Fraisse},
keywords = {Steerable mobile robot, Pseudo-omni mobile robot, Nonholonomic omnidirectional mobile robot},
abstract = {Emerging industrial applications involving mobile manipulation in the presence of humans is driving attention towards steerable wheeled mobile robots (SWMR), since these can perform arbitrary 2D planar trajectories, providing a reasonable compromise between maneuverability (necessary for human avoiding algorithms) and effectiveness. Instantaneous center of rotation (ICR) based kinematic models and controllers are the most suited for such robots, as they assure the existence of a unique ICR point at all times. However, unsatisfactory behavior do exist in numerous applications requiring frequent changes in the sign of the angular velocity command. This is typically the case for robot heading control: moving the ICR point from one border of the 2D ICR space to the other makes it pass by the robot geometric center, where only pure rotations are feasible. This behavior is not desirable and should be avoided. In this paper, we propose a novel complementary route ICR controller, where the ICR can go from one extreme to the other by means of border switching in one sample period. Thanks to this approach, fast response to the velocity commands is achieved with little steering motion. The new algorithm has been tested successfully in simulations and experiments, and is more time efficient with far more satisfactory behavior than the state-of-art direct route based controllers. These results have been also confirmed quantitatively, using a newly developed metric, the command fulfillment index (CFI).}
}
@article{KANOULAS201913,
title = {Curved patch mapping and tracking for irregular terrain modeling: Application to bipedal robot foot placement},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {13-30},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017308242},
author = {Dimitrios Kanoulas and Nikos G. Tsagarakis and Marsette Vona},
keywords = {Irregular surface modeling, Foothold contact modeling, Bounded curved patch modeling, Curved patch fitting and tracking, 3D perception for bipedal robots, Bipedal robot foot placement, Rough terrain stepping, Legged robot locomotion},
abstract = {Legged robots need to make contact with irregular surfaces, when operating in unstructured natural terrains. Representing and perceiving these areas to reason about potential contact between a robot and its surrounding environment, is still largely an open problem. This paper introduces a new framework to model and map local rough terrain surfaces, for tasks such as bipedal robot foot placement. The system operates in real-time, on data from an RGB-D and an IMU sensor. We introduce a set of parametrized patch models and an algorithm to fit them in the environment. Potential contacts are identified as bounded curved patches of approximately the same size as the robot’s foot sole. This includes sparse seed point sampling, point cloud neighborhood search, and patch fitting and validation. We also present a mapping and tracking system, where patches are maintained in a local spatial map around the robot as it moves. A bio-inspired sampling algorithm is introduced for finding salient contacts. We include a dense volumetric fusion layer for spatiotemporally tracking, using multiple depth data to reconstruct a local point cloud. We present experimental results on a mini-biped robot that performs foot placements on rocks, implementing a 3D foothold perception system, that uses the developed patch mapping and tracking framework.}
}
@article{MATSUDA2019103231,
title = {Resident autonomous underwater vehicle: Underwater system for prolonged and continuous monitoring based at a seafloor station},
journal = {Robotics and Autonomous Systems},
volume = {120},
pages = {103231},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018310029},
author = {Takumi Matsuda and Toshihiro Maki and Kotohiro Masuda and Takashi Sakamaki},
keywords = {Autonomous underwater vehicle (AUV), Docking, Resident AUV, Underwater monitoring, Wireless charging},
abstract = {This paper proposes a resident autonomous underwater vehicle (AUV) system for monitoring around an underwater infrastructure using an AUV and a seafloor station (SS), where the SS serves as the positioning reference and a battery charging station for the AUV. Docking to the SS is a key technology in the system. The proposed method utilizes acoustic and visual positioning based on the SS. To increase the robustness of the method, further methods for visual positioning using limited information and autonomous judging of docking success are implemented. After completing docking, the AUV charges its batteries wirelessly. The process of balancing battery consumption and the amount of charge is formulized to enable prolonged and continuous surveying. The proposed system was evaluated in sea and tank environments using the AUV Tri-TON 2 (TT2) and a test-bed SS. In the sea experiments, TT2 succeeded in docking to the SS under the condition of low visibility which was within 2 m and complicated sea current induced by waves which was around 0.5 m/s. To enhance the success rate of docking, a control method considering the resistance due to sea currents is formulated. It was verified that this control method can ensure a position of TT2 maintained with 0.1 m vibration throughout the control simulation using the velocity of sea currents obtained in the sea experiments. In the tank trials, TT2 succeeded in operating continuously during three days while performing autonomous docking to the SS and charging its batteries.}
}