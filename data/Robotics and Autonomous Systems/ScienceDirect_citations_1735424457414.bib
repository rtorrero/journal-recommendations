@article{QIN2020103561,
title = {Appearance-invariant place recognition by adversarially learning disentangled representation},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103561},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103561},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304012},
author = {Cao Qin and Yunzhou Zhang and Yan Liu and Sonya Coleman and Dermot Kerr and Guanghao Lv},
keywords = {Visual place recognition, Changing environment, Adversarial learning, Representation disentanglement},
abstract = {Place recognition is an essential component to address the problem of visual navigation and SLAM. The long-term place recognition is challenging as the environment exhibits significant variations across different times of the days, months, and seasons. In this paper, we view appearance changes as multiple domains and propose a Feature Disentanglement Network (FDNet) based on a convolutional auto-encoder and adversarial learning to extract two independent deep features — content and appearance. In our network, the content feature is learned which only retains the content information of images through the competition with the discriminators and content encoder. Besides, we utilize the triplets loss to make the appearance feature encode the appearance information. The generated content features are directly used to measure the similarity of images without dimensionality reduction operations. We use datasets that contain extreme appearance changes to carry out experiments, which show how meaningful recall at 100% precision can be achieved by our proposed method where existing state-of-art approaches often get worse performance.}
}
@article{KOSARNOVSKY2020103609,
title = {Geometric and constrained control for a string of tethered drones},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103609},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103609},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304498},
author = {Benny Kosarnovsky and Shai Arogeti},
abstract = {In this study, we present a novel concept of a multi tethered drone system. The system includes an arbitrary number of drones connected serially to an active ground station. The considered drones are of quadrotor type. Utilizing a unique pulley–gimbal mechanism, each drone can freely move along the tether, and its state is measured with respect to the ground station without the use of standard onboard inertial sensors or GPS. The proposed system can be thought of as a robotic arm where each tether section acts as a variable-length link and each drone is a joint actuator. We model the coupled behavior of the ground station and the string, taking into account an arbitrary number of drones. Then, a controller that combines tools from geometric-control and Model Predictive Control is suggested. The developed model and control approach are also applicable for other swarm applications where the position of agents is to be controlled to a string-like form. Finally, the concept is demonstrated using numerical simulations and an initial experiment, which illustrate its potential effectiveness.}
}
@article{DIAB2020103653,
title = {SkillMaN — A skill-based robotic manipulation framework based on perception and reasoning},
journal = {Robotics and Autonomous Systems},
volume = {134},
pages = {103653},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103653},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304930},
author = {Mohammed Diab and Mihai Pomarlan and Daniel Beßler and Aliakbar Akbari and Jan Rosell and John Bateman and Michael Beetz},
keywords = {Manipulation planning, Semantic Skill, Navigation, Every-day tasks, Adaptation, Knowledge-based Reasoning},
abstract = {One of the problems that service robotics deals with is to bring mobile manipulators to work in semi-structured human scenarios, which requires an efficient and flexible way to execute every-day tasks, like serve a cup in a cluttered environment. Usually, for those tasks, the combination of symbolic and geometric levels of planning is necessary, as well as the integration of perception models with knowledge to guide both planning levels, resulting in a sequence of actions or skills which, according to the current knowledge of the world, may be executed. This paper proposes a planning and execution framework, called SkillMaN, for robotic manipulation tasks, which is equipped with a module with experiential knowledge (learned from its experience or given by the user) on how to execute a set of skills, like pick-up, put-down or open a drawer, using workflows as well as robot trajectories. The framework also contains an execution assistant with geometric tools and reasoning capabilities to manage how to actually execute the sequence of motions to perform a manipulation task (which are forwarded to the executor module), as well as the capacity to store the relevant information to the experiential knowledge for further usage, and the capacity to interpret the actual perceived situation (in case the preconditions of an action do not hold) and to feed back the updated state to the planner to resume from there, allowing the robot to adapt to non-expected situations. To evaluate the viability of the proposed framework, an experiment has been proposed involving different skills performed with various types of objects in different scene contexts.}
}
@article{BARADARANKHALKHALI2020103596,
title = {Vehicle tracking with Kalman filter using online situation assessment},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103596},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103596},
url = {https://www.sciencedirect.com/science/article/pii/S092188902030436X},
author = {Maryam {Baradaran Khalkhali} and Abedin Vahedian and Hadi {Sadoghi Yazdi}},
keywords = {Vehicle tracking, Kalman filter, Situation assessment},
abstract = {Vehicle tracking is an attractive problem in the field of public transportation with several research projects conducted using Kalman filter (KF) to tackle this. While a driver may act on his own decision, there exist parameters affecting his behavior so called situation assessment such as neighboring drivers, possible obstacles, or alternative routes changing over time. In this paper, utilizing online situation assessment (SA) inside Kalman filter is studied. Motion History Graph is used as online modeling of the history of the vehicle motions and is used to augment the estimation. Experimental results on video sequences from different datasets show an average 25 percent performance improvement when using online SA inside KF.}
}
@article{MENG2020103556,
title = {Safeguarding against prefix interception attacks via online learning},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103556},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103556},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020303961},
author = {Meng Meng and Ruijuan Zheng and Ruxi Peng and Junlong Zhu and Mingchuan Zhang and Qingtao Wu},
keywords = {Online learning, Prefix interception, Routing attacks, Secure route},
abstract = {In human–robot cooperation, the information interaction plays a key role. Most of the information interaction rely on Border Gateway Protocol (BGP), which is a vital route protocol on networks. However, the BGP is susceptible to the prefix interception attacks because the rightful origin of each prefix cannot be verified in BGP. For this reason, we propose a novel and effective route selection method against prefix interception attacks, which combines the resilience of routers and the historical performance of routers to choose a secure route. Moreover, we estimate the performance of BGP by introducing the definition of resilience and the historical performance of routers via online learning against the prefix interception attack. Furthermore, we analyze the bound of regret and obtain O(T) regret, where T denotes the time horizon. In addition, the proposed method is verified both on synthetic data and network simulations. The results show that the proposed method has more resilience against prefix interception attacks than Counter-Raptor.}
}
@article{FASS2020103601,
title = {On planar self-folding magnetic chains: Comparison of Newton–Euler dynamics and internal energy optimisation},
journal = {Robotics and Autonomous Systems},
volume = {132},
pages = {103601},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103601},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304413},
author = {T.H. Fass and Guangbo Hao and Pádraig Cantillon-Murphy},
keywords = {Magnetic self-folding chain, Self-assembly,  dynamics, Magnetic surgery},
abstract = {Within the wide field of self-assembly, the self-folding chain has the unique capability to pass through narrow openings, too small for the assembled structure, yet consists in one connected body. This paper presents a novel analytical framework and corresponding experimental setup to quantify the results of a self-folding process using magnetic forces at the centimetre-scale, with the aim to put experimental results and prediction methods in the context of surgical anchoring and therapy. Two possibilities to predict the folding of a chain of magnetic components in 2D are compared and investigated in an experimental setup. Folding prediction by system Coulomb energy, neglecting folding dynamics, is compared with a simulation of the system dynamics using a novel approach for 2D folding chains, derived from the Newton–Euler equations. The presented algorithm is designed for the parallel computation architecture of modern computer systems to be easily applicable and to achieve an improved simulation speed. The experimental setup for the self-folding chain used to validate the simulation results consists of a chain of magnetic components where movement is limited to one plane and the chain is agitated by the magnetic forces between the chain components. The folding process of the experimental setup is validated for its stability and predictability under different deployment modes. Finally, the results are discussed in light of the folding prediction of longer chains. The implications of the presented findings for a 3D folding chain are discussed together with the challenges to apply the novel dynamics simulation algorithm to the 3D case. The work clearly demonstrates the potential for this novel approach for complex self-folding applications such as magnetic compression anastomosis and anchoring in minimally invasive surgery.}
}
@article{2022104110,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {152},
pages = {104110},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(22)00061-6},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022000616}
}
@article{ZHAO2020103597,
title = {A Robust Stereo Feature-aided Semi-direct SLAM System},
journal = {Robotics and Autonomous Systems},
volume = {132},
pages = {103597},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103597},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304371},
author = {Xiangrui Zhao and Lina Liu and Renjie Zheng and Wenlong Ye and Yong Liu},
keywords = {Visual SLAM, Hybrid method, Image brightness rectification},
abstract = {In autonomous driving, many intelligent perception technologies have been put in use. However, visual SLAM still has problems with robustness, which limits its application, although it has been developed for a long time. We propose a feature-aided semi-direct approach to combine the direct and indirect methods in visual SLAM to allow robust localization under various situations, including large-baseline motion, textureless environment, and great illumination changes. In our approach, we first calculate inter-frame pose estimation by feature matching. Then we use the direct alignment and a multi-scale pyramid, which employs the previous coarse estimation as a priori, to obtain a more precise result. To get more accurate photometric parameters, we combine the online photometric calibration method with visual odometry. Furthermore, we replace the Shi–Tomasi corner with the ORB feature, which is more robust to illumination. For extreme brightness change, we employ the dark channel prior to weaken the halation and maintain the consistency of the image. To evaluate our approach, we build a full stereo visual SLAM system. Experiments on the publicly available dataset and our mobile robot dataset indicate that our approach improves the accuracy and robustness of the SLAM system.}
}
@article{LOPEZ2020103569,
title = {Obstacle avoidance in dynamic environments based on velocity space optimization},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103569},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103569},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304097},
author = {Joaquín López and Pablo Sanchez-Vilariño and Miguel Díaz Cacho and Elena López Guillén},
keywords = {Collision avoidance, Dynamic environment, Robot motion control, Reactive control},
abstract = {Robotic obstacle avoidance is an important issue in robotic navigation for unknown or partially known, dynamic environments. A good number of techniques have already been proposed to navigate obstacles in this kind of environment. They include a series of velocity space methods that have been successful implemented in several applications. They formulate the problem as one of constrained optimization in the velocity space of the robot. The constraints include the obstacles in the environment assuming they are static. In this paper, we present an efficient, real-time method (BCM-DO) to include the restrictions imposed by dynamic objects. The optimization function has also been adapted to include these new restrictions. The new function is evaluated in two sets of points. A first set is obtained from a coarse sampling in the reachable window of velocities and a second set is selected in the limits of each curvature interval to avoid missing small openings between static objects. The whole system has first been extensively tested in several simulated robots and finally applied to a hotel assistant robot (BellBot) resulting in an efficient, real-time obstacle avoidance method that produces smooth and reliable routes.}
}
@article{XIE2020103649,
title = {Map merging with terrain-adaptive density using mobile 3D laser scanner},
journal = {Robotics and Autonomous Systems},
volume = {134},
pages = {103649},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103649},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304899},
author = {Yangmin Xie and Yujie Tang and Rui Zhou and Yukun Guo and Hang Shi},
keywords = {Map merging, ICP algorithm, Point cloud simplification, Terrain-adaptive mapping},
abstract = {Building 3D maps of various terrains is a necessary approach to gain environmental information for mobile robots when they are exploring in unknown territories. In this paper, we propose a method to construct a point cloud map with laser-measured data as the robot moves around. A terrain-adaptive density mapping technique is used to balance the demands of small data size and high terrain accuracy by utilizing the local curvatures as the simplification criteria. The adaptive density mapping technique is further integrated within the map merging framework to improve the matching speed and accuracy. Indoor and outdoor experiments are proceeded, which verifies the effects of using terrain-adaptive density point cloud on controlling the map size and decreasing the map alignment error.}
}
@article{CAMARA2020103625,
title = {Visual Place Recognition by spatial matching of high-level CNN features},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103625},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103625},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304656},
author = {Luis G. Camara and Libor Přeučil},
keywords = {Visual Place Recognition, Convolutional neural networks, Autonomous navigation, Image retrieval, Computer vision, Deep learning},
abstract = {We present a Visual Place Recognition (VPR) pipeline that achieves substantially improved precision as compared with approaches commonly appearing in the literature. It is based on a standard image retrieval configuration, with an initial stage that retrieves the closest candidates to a query from a database and a second stage where the list of candidates is re-ranked. The latter is realized by the introduction of a novel geometric verification procedure that uses the activations of a pre-trained convolutional neural network. It is both remarkably simple and robust to viewpoint and condition changes. As a stand-alone, general spatial matching methodology, it could be easily added and used to enhance existing VPR approaches whose output is a ranked list of candidates. The proposed two-stage pipeline is also improved through extensive optimization of hyperparameters and by the implementation of a frame-based temporal filter that takes into account past recognition results.}
}
@article{YAZDANI2020103641,
title = {Feasibility analysis of using the hp-adaptive Radau pseudospectral method for minimum-effort collision-free docking operations of AUV},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103641},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103641},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304814},
author = {A.M. Yazdani and K. Sammut and O.A. Yakimenko and A. Lammas},
keywords = {Underwater docking, Optimal trajectory generation, Two-point boundary-value problem, hp-adaptive Radau method},
abstract = {This paper continues the previous effort on the development of a trajectory generation platform that assures minimum-control expenditure and collision-free manoeuvre of a torpedo-shaped autonomous underwater vehicle (AUV) into a funnel-shaped stationary docking station (DS). The earlier-developed guidance system was based on the Inverse Dynamics in the Virtual Domain (IDVD) method accounting for AUV’s dynamics and producing a smooth trackable trajectory, thus guaranteeing safe arrival to DS. The optimality of the real-time generated solutions has been assessed via comparing them with the Legendre–Gauss–Lobatto pseudo-spectral (PS) method solutions that could only be obtained off-line. This paper explores a possibility of employing a more advanced hp-adaptive Radau (hp-AR) PS method for the same Hamiltonian two-point boundary-value problem. The considered approach explicitly encapsulates all realistic vehicular and environmental constraints such as the AUV’s dynamics, ocean current disturbances, no-fly zones, and DS pose while minimizing the vehicle’s controls expenditure and permitting precise manoeuvring into DS. The performance evaluation of the hp-AR PS based optimization routine is carried out through extensive software-in-the-loop simulations. For completeness, computational effectiveness and solution optimality of the trajectory generator engine based on the hp-AR method is compared with two other well-known PS methods based on Legendre and Chebyshev polynomial approximation. The results of this study show the superb performance of the hp-AR method-based trajectory generator among all other PS methods and a possibility of using it along with IDVD in the real-time implementation.}
}
@article{ZHANG2020103555,
title = {WAGNN: A Weighted Aggregation Graph Neural Network for robot skill learning},
journal = {Robotics and Autonomous Systems},
volume = {130},
pages = {103555},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103555},
url = {https://www.sciencedirect.com/science/article/pii/S092188902030395X},
author = {Fengyi Zhang and Zhiyong Liu and Fangzhou Xiong and Jianhua Su and Hong Qiao},
keywords = {Skill transfer learning, Serial structures, Robot skill learning, Graph Neural Network},
abstract = {Robotic skill learning suffers from the diversity and complexity of robotic tasks in continuous domains, making the learning of transferable skills one of the most challenging issues in this area, especially for the case where robots differ in terms of structure. Aiming at making the policy easier to be generalized or transferred, the graph neural networks (GNN) was previously employed to incorporate explicitly the robot structure into the policy network. In this paper, with the help of graph neural networks, we further investigate the problem of efficient learning transferable policies for robots with serial structure, which commonly appears in various robot bodies, such as robotic arms and the leg of centipede. Based on a kinematics analysis on the serial robotic structure, the policy network is improved by proposing a weighted information aggregation strategy. It is experimentally shown on different robotics structures that in a few-shot policy learning setting, the new aggregation strategy significantly improves the performance not only on the learning speed, but also on the control accuracy.}
}
@article{WANG2020103560,
title = {Coupled task scheduling for heterogeneous multi-robot system of two robot types performing complex-schedule order fulfillment tasks},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103560},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103560},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304000},
author = {Hanfu Wang and Weidong Chen and Jingchuan Wang},
keywords = {Task scheduling, Heterogeneous multi-robot system, Complex-schedule constraints, Block sequence graph},
abstract = {This paper addresses multi-robot task scheduling for two robot types arising from heterogeneous robotic order fulfillment systems. The heterogeneous multi-robot system comprises two types of robots with specialized and complementary capabilities to achieve long-cycle and multi-station order fulfillment tasks on a logistic network. This problem is extremely challenging because of innate complex-schedule constraints of tasks and coupled temporal–spatial relations between all robots. After set-theoretic and mixed integer linear programming problem formulations, we use coupled approach, instead of decoupled approaches to explore the synergy between heterogeneous robots, which is different from most existing similar works. To model the structural (complex-schedule) and quantitative (temporal–spatial) coupledness of robots’ time-extended task schedules, an edge-weighted and vertex-weighted block sequence graph is introduced. Based on this model, time-extended task scheduling is achieved using rank-minimal heuristic and genetic algorithm metaheuristic. Theoretically, this model is complete and non-redundant. Empirically, compared with decoupled approach, optimality and efficiency of the proposed methods are evaluated on designed instances. The results demonstrate that coupled methods can achieve near-optimal solutions with higher performance ratio than decoupled methods in moderate time. At the same time, coupled methods can leverage spatial and temporal properties of miscellaneous tasks, and balance instantaneous and time-extended decisions to achieve tight collective synergy in the long run.}
}
@article{GUNEY2021103534,
title = {Dynamic prioritized motion coordination of multi-AGV systems},
journal = {Robotics and Autonomous Systems},
volume = {139},
pages = {103534},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103534},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018305505},
author = {Mehmet Ali Guney and Ioannis A. Raptis},
keywords = {Multi-robot systems, Motion coordination, Autonomous Guided Vehicles},
abstract = {The motion coordination problem for a fleet of Autonomous Guided Vehicles (AGVs) in a confined industrial facility is addressed. The working scenario involves a group of AGVs that is tasked to transport without collisions to predefined locations within the industrial facility. We introduce a centralized motion coordination controller that utilizes a dynamic priority logic to resolve motion conflicts between AGVs as they appear. The controller relies on the implementation of a predefined, virtual transportation network that is comparable to a conventional right-handed bidirectional traffic system. The construction of the transportation network considers the physical and motion characteristics of the AGVs (dimensions and maximum speed). The high-level function of the controller is to detect imminent collisions and determine the right-of-way of conflicting AGVs in same-directional routes and intersection junctions of the transportation network. The priority update logic is inspired by the traffic control of conventional four-way stop-controlled intersections. Based on the updated priorities, the motion coordinator adjusts the advancement of the AGVs to eliminate collisions. The proposed formulation combines a high-level event-driven logic for collision avoidance with low-level feedback control laws for guidance and navigation. As a result, the controller relies only on real-time measurements, removing the need for computationally demanding look-ahead predictions (heuristics) of the AGVs’ motion. It is shown that the proposed method ensures collision- and blockage-free motion of a large number of AGVs. Extensive numerical simulations validate the performance of the motion coordination algorithm.}
}
@article{HUNG2020103608,
title = {Range-based target localization and pursuit with autonomous vehicles: An approach using posterior CRLB and model predictive control},
journal = {Robotics and Autonomous Systems},
volume = {132},
pages = {103608},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103608},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304486},
author = {Nguyen T. Hung and N. Crasta and David Moreno-Salinas and António M. Pascoal and Tor A. Johansen},
keywords = {Range-based target localization, Target tracking, Target pursuit, MPC, Fisher information matrix, Posterior CRLB, Autonomous vehicle},
abstract = {We address the general problem of multiple target localization and pursuit using measurements of the ranges from the targets to a set of autonomous pursuing vehicles, referred to as trackers. We develop a general framework for targets with models exhibiting uncertainty in the initial state, process, and measurement noise. The main objective is to compute optimal motions for the trackers that maximize the range-based information available for target localization and at the same time yield good target pursuit performance. The solution proposed is rooted in an estimation-theoretical setting that involves the computation of an appropriately defined Bayesian Fisher Information Matrix (FIM). The inverse of the latter yields a posterior Cramér–Rao Lower Bound (CRLB) on the covariance of the targets’ state estimation errors that can be possibly achieved with any estimator. Using the FIM, sufficient conditions on the trackers’ motions are derived for the ideal relative geometry between the trackers and the targets for which the range information acquired is maximal. This allows for an intuitive understanding of the types of ideal tracker trajectories. To deal with realistic constraints on the trackers’ motions and the requirement that the trackers pursue the targets, we then propose a model predictive control (MPC) framework for optimal tracker motion generation with a view to maximizing the predicted range information for target localization while taking explicitly into account the trackers’ dynamics, strict constraints on the trackers’ states and inputs, and prior knowledge about the targets’ states. The efficacy of the MPC is assessed in simulation through the help of representative examples motivated by operational scenarios involving single and multiple targets and trackers.}
}
@article{FERNANDEZ2020103624,
title = {Associated Reality: A cognitive Human–Machine Layer for autonomous driving},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103624},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103624},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304644},
author = {Felipe Fernandez and Angel Sanchez and Jose F. Velez and Belen Moreno},
keywords = {ADAS and autonomous vehicle, Cooperative-ITS, Human–Machine​ system, Augmented Local Dynamic Map, Cognitive system, Cognitive landmark},
abstract = {Advanced Driver Assistance Systems (ADAS) and Automated and Autonomous Vehicles (AV) are cooperative systems and processes that use: artificial intelligence, cognitive methods, cloud technologies, cooperative vehicle-to-everything-communications (V2X), software–hardwareplatforms, sensor platforms and incipient intelligent transport infrastructures, to get self-driving systems and smart connected mobility services. This paper, to support automated driving systems (assisted, semi-autonomous and fully autonomous vehicles), introduces a cognitive layer called Associated Reality to enhance the involved information, knowledge and communication processes. The architecture defined includes an augmented Local Dynamic Map, with complementary layers, and an augmented Graph Database, with complementary semantic–cognitive relations, for the considered purpose, in cooperative human–machine and machine–machine systems. Virtual augmented landmarks are defined to improve the connectivity and intelligence of the involved spatial-information systems. Different structure landmarks and sequence landmarks (which includes regular, repetitive and periodic landmarks) are defined, categorized and used in diverse visual localization and mapping scenarios, for autonomous driving. In this paper, it is also shown, as a proof-of-concept for vehicle localization and mapping in road tunnels, the visual detection of different sequences of periodic luminaires, using YOLO v3 for the corresponding LED lights detection, or a specific alternative procedure developed with very low computational cost.}
}
@article{ANSARI2020103600,
title = {Human grasp position estimation for human–robot cooperative object manipulation},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103600},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103600},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304401},
author = {Ramin Jaberzadeh Ansari and Giuseppe Giordano and Jonas Sjöberg and Yiannis Karayiannidis},
keywords = {Physical human–robot interaction, Collaborative robots, Object manipulation, Estimation},
abstract = {This paper addresses the problem of human grasp position estimation in a physical human–robot object handling scenario. The problem is formulated as a linear regression by considering the human grasp position and their exerted torque as unknown parameters. We propose a modified least-squares algorithm to estimate the parameters by evaluating the quality of the estimates based on the assumption that the parameters should remain constant for a period of time. The solution is model-agnostic in terms of the human force/torque model – requiring only force/torque measurements on the robot side and proprioception – and is model-based in terms of the object model. The proposed grasp position estimation method is compared statistically with a conventional contact point estimation method using the collected experimental data. Moreover, the performance of the developed method is evaluated through various scenarios of physical human–robot interaction.}
}
@article{ANDRADE2020103499,
title = {A minimal biologically-inspired algorithm for robots foraging energy in uncertain environments},
journal = {Robotics and Autonomous Systems},
volume = {128},
pages = {103499},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103499},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019304002},
author = {Gabriela R. Andrade and Jordan H. Boyle},
abstract = {This work details the design and simulation results of a bioinspired minimalist algorithm based on C. elegans, using autonomous agents to forage for attractant energy sources. The robotic agents are energy-constrained and depend on the energy they forage to recharge their batteries, which is significant as the foraging task is one of the canonical testbeds for cooperative robotics. The algorithm consists of 6 input parameters which were simulated and optimised in 9 unbounded environments of varying difficulty levels, containing attractant sources which robots would then have to forage from to maintain energy levels and survive the entirety of the simulation. The robots running the algorithm were then optimised using Evolutionary Algorithms and the best solutions in all 9 environments were categorised with the use of clustering techniques. The clustering results highlighted the different strategies which emerged. Ultimately across the 9 environments, 6 different strategies have been identified. The results demonstrate the applicability of the proposed algorithm to localise attractant sources and harvest energy in different scenarios using the same core algorithm.}
}
@article{LIKMETA2020103568,
title = {Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103568},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103568},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304085},
author = {Amarildo Likmeta and Alberto Maria Metelli and Andrea Tirinzoni and Riccardo Giol and Marcello Restelli and Danilo Romano},
keywords = {Autonomous driving, Decision making, Interpretability, Reinforcement learning, Parameter-based exploration},
abstract = {The design of high-level decision-making systems is a topical problem in the field of autonomous driving. In this paper, we combine traditional rule-based strategies and reinforcement learning (RL) with the goal of achieving transparency and robustness. On the one hand, the use of handcrafted rule-based controllers allows for transparency, i.e., it is always possible to determine why a given decision was made, but they struggle to scale to complex driving scenarios, in which several objectives need to be considered. On the other hand, black-box RL approaches enable us to deal with more complex scenarios, but they are usually hardly interpretable. In this paper, we combine the best properties of these two worlds by designing parametric rule-based controllers, in which interpretable rules can be provided by domain experts and their parameters are learned via RL. After illustrating how to apply parameter-based RL methods (PGPE) to this setting, we present extensive numerical simulations in the highway and in two urban scenarios: intersection and roundabout. For each scenario, we show the formalization as an RL problem and we discuss the results of our approach in comparison with handcrafted rule-based controllers and black-box RL techniques.}
}
@article{KANAPRAM2020103652,
title = {Self-awareness in intelligent vehicles: Feature based dynamic Bayesian models for abnormality detection},
journal = {Robotics and Autonomous Systems},
volume = {134},
pages = {103652},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103652},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304929},
author = {Divya Thekke Kanapram and Pablo Marin-Plaza and Lucio Marcenaro and David Martin and Arturo {de la Escalera} and Carlo Regazzoni},
keywords = {Intelligent Transportation System (ITS), Autonomous vehicles, Dynamic Bayesian Network (DBN), Hellinger distance, Abnormality detection},
abstract = {The evolution of Intelligent Transportation Systems in recent times necessitates the development of self-awareness in agents. Before the intensive use of Machine Learning, the detection of abnormalities was manually programmed by checking every variable and creating huge nested conditions that are very difficult to track. This paper aims to introduce a novel method to develop self-awareness in autonomous vehicles that mainly focuses on detecting abnormal situations around the considered agents. Multi-sensory time-series data from the vehicles are used to develop the data-driven Dynamic Bayesian Network (DBN) models used for future state prediction and the detection of dynamic abnormalities. Moreover, an initial level collective awareness model that can perform joint anomaly detection in co-operative tasks is proposed. The GNG algorithm learns the DBN models’ discrete node variables; probabilistic transition links connect the node variables. A Markov Jump Particle Filter (MJPF) is applied to predict future states and detect when the vehicle is potentially misbehaving using learned DBNs as filter parameters. In this paper, datasets from real experiments of autonomous vehicles performing various tasks used to learn and test a set of switching DBN models.}
}
@article{DELAFUENTE2020103514,
title = {A robust phase oscillator design for wearable robotic systems},
journal = {Robotics and Autonomous Systems},
volume = {128},
pages = {103514},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103514},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019305287},
author = {Juan {De La Fuente} and Susheelkumar C. Subramanian and Thomas G. Sugar and Sangram Redkar},
keywords = {Phase oscillator, Wearable robot, Hip exoskeleton, Nonlinear controller},
abstract = {The design of a phase-based robust oscillator for wearable robots, that could assist humans performing periodic or repetitive tasks, is presented in this paper. The bounds on perturbations, that guaranteed the stability of the output for the phase oscillator controller, were identified and the Lyapunov redesign method was applied to construct a robust controller using a bounding function. The robust controller produced a bounded control signal to modify the amplitude and frequency of the resulting second-order oscillator to modulate the stiffness and damping properties. In this paper, the focus is on the mathematical modeling of the controller, its dynamic stability and robustness for human–robot application. The proposed approach was verified through a simple pendulum experiment. The results provided evidence that a better limit cycle, with a controlled radial spread of the steady state, was obtained with Lyapunov redesigned phase oscillator. Finally, the potential of the proposed approach for hip assistance in a healthy subject wearing HeSa (Hip Exoskeleton for Superior Assistance) during periodic activities are discussed with preliminary results.}
}
@article{FU2020103519,
title = {LiDAR-based vehicle localization on the satellite image via a neural network},
journal = {Robotics and Autonomous Systems},
volume = {129},
pages = {103519},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103519},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019305202},
author = {Mengyin Fu and Minzhao Zhu and Yi Yang and Wenjie Song and Meiling Wang},
keywords = {Localization, Deep learning, LiDAR, Satellite image matching},
abstract = {We present a novel method to localize the vehicle on an easily accessible geo-referenced satellite image based on LiDAR. We first design a neural network to extract and compare the spatial-discriminative feature maps of the satellite image patch and the LiDAR points, and obtain the probability of correspondence. Then based on the outputs of the network, a particle filter is used to obtain the probability distribution of the vehicle pose. This method can use LiDAR points and any type of odometry as input to localize the vehicle. The experimental results show that our model can generalize well on several datasets. Compared with other methods, ours is more robust in some challenging scenarios such as the occluded or shadowed area on the satellite image.}
}
@article{YANG2020103505,
title = {Multi-camera visual SLAM for off-road navigation},
journal = {Robotics and Autonomous Systems},
volume = {128},
pages = {103505},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103505},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019308711},
author = {Yi Yang and Di Tang and Dongsheng Wang and Wenjie Song and Junbo Wang and Mengyin Fu},
keywords = {Multi-camera, Panorama, Off-road, Simultaneous localization and mapping},
abstract = {With the rapid development of computer vision, vision-based simultaneous localization and mapping (vSLAM) plays an increasingly important role in the field of unmanned driving. However, traditional SLAM methods based on a monocular camera only perform well in simple indoor environments or urban environments with obvious structural features. In off-road environments, the situation that SLAM encounters could be complicated by problems such as direct sunlight, leaf occlusion, rough roads, sensor failure, sparsity of stably trackable texture. Traditional methods are highly susceptible to these factors, which lead to compromised stability and reliability. To counter such problems, we propose a panoramic vision SLAM method based on multi-camera collaboration, aiming at utilizing the characters of panoramic vision and stereo perception to improve the localization precision in off-road environments. At the same time, the independence and information sharing of each camera in multi-camera system can improve its ability to resist bumps, illumination, occlusion and sparse texture in an off-road environment, and enable our method to recover the metric scale. These characters ensure unmanned ground vehicles (UGVs) to locate and navigate safely and reliably in complex off-road environments.}
}
@article{SUN2020103648,
title = {Shared mixed reality-bilateral telerobotic system},
journal = {Robotics and Autonomous Systems},
volume = {134},
pages = {103648},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103648},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304887},
author = {Da Sun and Qianfang Liao and Andrey Kiselev and Todor Stoyanov and Amy Loutfi},
keywords = {Virtual reality, Bilateral teleoperation, Shared control},
abstract = {This study proposes a new shared mixed reality (MR)-bilateral telerobotic system. The main contribution of this study is to combine MR teleoperation and bilateral teleoperation, which takes advantage of the two types of teleoperation and compensates for each other’s drawbacks. With this combination, the proposed system can address the asymmetry issues in bilateral teleoperation, such as kinematic redundancy and workspace inequality, and provide force feedback, which is lacking in MR teleoperation. In addition, this system effectively supports long-distance movements and fine movements. In this system, a new MR interface is developed to provide the operator with an immersive visual feedback of the workspace, in which a useful virtual controller known as an interaction proxy—is designed. Compared with previous virtual reality-based teleoperation systems, this interaction proxy can freely decouple the operator from the control loop, such that the operational burden can be substantially alleviated. Additionally, the force feedback provided by the bilateral teleoperation gives the operator an advanced perception about the remote workspace and can improve task performance. Experiments on multiple pick-and-place tasks are provided to demonstrate the feasibility and effectiveness of the proposed system.}
}
@article{CHEN2020103599,
title = {A lobster-inspired articulated shaft for minimally invasive surgery},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103599},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103599},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304395},
author = {Yaohui Chen and Hoam Chung and Bernard Chen and  Baoyinjiya},
keywords = {Soft pneumatic actuation, Hybrid robots, Minimally invasive surgery, Soft robotics},
abstract = {Novel applications of soft pneumatic actuation in minimally invasive surgery (MIS) are proposed due to its relatively safe robot–environment interactions. Although the inherent compliance of soft robots makes them suitable for surgery, their low force output and complicated system response and behavior may limit their potential as practical MIS instruments. In this paper, three lobster-inspired antagonistic modules are proposed to realize bidirectional translational, bending and rotational motions and variable stiffness in centimeter scale. Their modular design enables flexible combinations of articulated shafts to satisfy end-effector workspace requirements in MIS. Theoretical models are proposed to relate the input pressure, deformation, output force/torque and stiffness, which provide quantitative solutions for independent adjustment on the deformation and stiffness of each module. A series of experimental results show that the proposed modules can deliver sufficient force and torque output for MIS applications, and they can be conveniently assembled into articulated shafts featuring safe actuation, high dexterity, stiffness tuning and reconfigurability.}
}
@article{KURTSER2020103591,
title = {Planning the sequence of tasks for harvesting robots},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103591},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103591},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304310},
author = {Polina Kurtser and Yael Edan},
keywords = {Harvesting robot, Task sequencing, Traveling salesman problem, Sweet pepper, Agriculture robotics},
abstract = {A methodology for planning the sequence of tasks for a harvesting robot is presented. The fruit targets are situated at unknown locations and must be detected by the robot through a sequence of sensing tasks. Once the targets are detected, the robot must execute a harvest action at each target location. The traveling salesman paradigm (TSP) is used to plan the sequence of sensing and harvesting tasks taking into account the costs of the sensing and harvesting actions and the traveling times. Sensing is planned online. The methodology is validated and evaluated in both laboratory and greenhouse conditions for a case study of a sweet pepper harvesting robot. The results indicate that planning the sequence of tasks for a sweet pepper harvesting robot results in 12% cost reduction. Incorporating the sensing operation in the planning sequence for fruit harvesting is a new approach in fruit harvesting robots and is important for cycle time reduction. Furthermore, the sequence is re-planned as sensory information becomes available and the costs of these new sensing operations are also considered in the planning.}
}
@article{LEE2020103563,
title = {Robust image completion and masking with application to robotic bin picking},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103563},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103563},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304036},
author = {Sukhan Lee and Naeem Ul Islam and Soojin Lee},
keywords = {Generative adversarial network, Latent space association network, Image completion, Image masking, Robotic bin picking},
abstract = {Automated image completion and masking have been emerged as a subject of keen interest due to their impact on image modification and interpretation. The current state-of-the-art approaches require a fixed format of missing parts and are ineffective for handling corrupted images. Besides, they focus exclusively on the image completion without taking into consideration the image masking as an inverse process of completion. This paper proposes a deep learning approach to an integrated framework of image completion and masking based on the cross-mapping generative adversarial network or CM-GAN, in short. CM-GAN offers the robustness in image completion under corruptions as well as the capability of synthesizing various masked images with arbitrary mask locations and shapes. In particular, the capability of CM-GAN in image masking is shown to be extended into the removal of unwanted backgrounds in images. We verify the superior performance of CM-GAN for image completion and masking based on extensive experiments. Furthermore, we implement a deep learning based robotic bin picking to demonstrate that the background removal capability of CM-GAN plays a key role for estimating the 3D pose of randomly filed multiple industrial parts in a bin.}
}
@article{HAFEZ2020103630,
title = {Improving robot dual-system motor learning with intrinsically motivated meta-control and latent-space experience imagination},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103630},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103630},
url = {https://www.sciencedirect.com/science/article/pii/S092188902030470X},
author = {Muhammad Burhan Hafez and Cornelius Weber and Matthias Kerzel and Stefan Wermter},
keywords = {Meta-control, Arbitration, Experience imagination, Intrinsic motivation, Reinforcement learning, Robotic grasping},
abstract = {Combining model-based and model-free learning systems has been shown to improve the sample efficiency of learning to perform complex robotic tasks. However, dual-system approaches fail to consider the reliability of the learned model when it is applied to make multiple-step predictions, resulting in a compounding of prediction errors and performance degradation. In this paper, we present a novel dual-system motor learning approach where a meta-controller arbitrates online between model-based and model-free decisions based on an estimate of the local reliability of the learned model. The reliability estimate is used in computing an intrinsic feedback signal, encouraging actions that lead to data that improves the model. Our approach also integrates arbitration with imagination where a learned latent-space model generates imagined experiences, based on its local reliability, to be used as additional training data. We evaluate our approach against baseline and state-of-the-art methods on learning vision-based robotic grasping in simulation and real world. The results show that our approach outperforms the compared methods and learns near-optimal grasping policies in dense- and sparse-reward environments.}
}
@article{XU2020103548,
title = {Kinematic and dynamic manipulability analysis for free-floating space robots with closed chain constraints},
journal = {Robotics and Autonomous Systems},
volume = {130},
pages = {103548},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103548},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019309236},
author = {Ruonan Xu and Jianjun Luo and Mingming Wang},
keywords = {Cooperative manipulation, Free-floating, Multi-arm space robot, Closed chain system, Manipulability analysis},
abstract = {This paper presents the manipulability analysis of free-floating multi-arm space robots. Evaluation of manipulator capability is useful both in the design and in the operation phase. After capturing a target, closed kinematic chains are formed with multi-arm cooperative manipulating a common object. Owing to the dynamic coupling effect, the manipulability analysis of free-floating systems is more complex than that of ground-fixed closed chain systems. To analyze the cooperative manipulability, kinematic and dynamic formulations for the free-floating closed chain systems are firstly derived. The formulations describe the mapping of joint velocities and torques, respectively, to task velocities and forces, as well as joint torques to task accelerations and forces, by using the generalized Jacobian matrices. Next, the well-known concepts of manipulability ellipsoid, manipulability measure and task compatibility of the free-floating closed chain system are formally extended. Besides, a new approach called scaling factor method is used in the analysis of the task compatibility, which is more accurate compared with the manipulability ellipsoid method. Three applications of the performance indices are considered: (1) the feasibility analysis for a given task, (2) the trajectory planing giving a desired task path, and (3) configuration optimization with different task requirements. The proposed index is proved a very efficient tool that can be utilized in the cooperative manipulation tasks for free-floating space robotic systems.}
}
@article{LOURENCO2020103552,
title = {Earth-fixed trajectory and map online estimation: Building on GES sensor-based SLAM filters},
journal = {Robotics and Autonomous Systems},
volume = {130},
pages = {103552},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103552},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019305275},
author = {Pedro Lourenço and Bruno J. Guerreiro and Pedro Batista and Paulo Oliveira and Carlos Silvestre},
keywords = {SLAM, Procrustes problem, Perturbation theory, Mapping, Robotics},
abstract = {This paper addresses the problem of obtaining an Earth-fixed trajectory and map (ETM), with the associated uncertainty, using the sensor-based map provided by a globally asymptotically/exponentially stable (GES) SLAM filter. The algorithm builds on an optimization problem with a closed-form solution, and its uncertainty description is derived resorting to perturbation theory. The combination of the algorithm proposed in this paper with sensor-based SLAM filtering results in a complete SLAM methodology, which is directly applied to the three main different formulations: range-and-bearing, range-only, and bearing-only. Simulation and experimental results for all these formulations are included in this work to illustrate the performance of the proposed algorithm under realistic conditions. The ETM algorithm proposed in this paper is truly sensor-agnostic, as it only requires a sensor-based map and imposes no constraints on how this map is acquired nor how egomotion is captured. However, in the experiments presented herein, all the sensor-based filters use a sensor to measure the angular velocity and, for the range-only and bearing-only formulations, a sensor to measure the linear velocity.}
}
@article{OLIVEIRA2020103558,
title = {A ROS framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103558},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103558},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020303985},
author = {Miguel Oliveira and Afonso Castro and Tiago Madeira and Eurico Pedrosa and Paulo Dias and Vítor Santos},
keywords = {Extrinsic calibration, ROS, Optimization, Bundle adjustment, Intelligent vehicles, OpenCV},
abstract = {This paper proposes a general approach to the problem of extrinsic calibration of multiple sensors of varied modalities. This is of particular relevance for intelligent vehicles, which are complex systems that often encompass several sensors of different modalities. Our approach is seamlessly integrated with the Robot Operating System (ROS) framework, and allows for the interactive positioning of sensors and labelling of data, facilitating the calibration procedure. The calibration is formulated as a simultaneous optimization for all sensors, in which the objective function accounts for the various sensor modalities. Results show that the proposed procedure produces accurate calibrations, on par with state of the art approaches which operate only for pairwise setups.}
}
@article{GODOY2020103631,
title = {C-Nav: Distributed coordination in crowded multi-agent navigation},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103631},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103631},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304711},
author = {Julio Godoy and Stephen J. Guy and Maria Gini and Ioannis Karamouzas},
keywords = {Multi-agent navigation, Multi-agent coordination, Robotics},
abstract = {In crowded multi-agent navigation, the motion of the agents is significantly constrained by the motion of the nearby agents. This makes planning paths very difficult and leads to inefficient global motion. To address this problem, we propose a distributed approach, which we call C-Nav, that introduces politeness into multi agent navigation. With our approach, agents take into account the velocities and goals of their neighbors and optimize their motion accordingly and in real-time. Further, we perform a theoretical analysis of the algorithm, and experimentally demonstrate its advantages in simulation, with hundreds of agents in a variety of scenarios, and in real world navigation tasks with several mobile robots.}
}
@article{YOROZU2020103603,
title = {Estimation of body direction based on gait for service robot applications},
journal = {Robotics and Autonomous Systems},
volume = {132},
pages = {103603},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103603},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304437},
author = {Ayanori Yorozu and Masaki Takahashi},
keywords = {Service robots, Human–robot interaction, Pedestrian tracking, Kalman filter, Laser range sensor},
abstract = {Recently, there have been several studies on the research and development of service robots, such as reception or waiter robots for facilities and companion robots for support of baggage transportation or guidance in public spaces. Several experimental results in real environments have been reported. To realize socially acceptable human–robot interaction for service robots, human recognition, including not only position but also body direction, around the robot is important. Using an RGB-D camera, it is possible to detect the posture of a person. However, because the viewing angle of the camera is narrow, it is difficult to recognize the environment around the robot with a single device. This study proposes the estimation of the body direction based on the gait, that is, not only the position and velocity, but also the state of the legs (stance or swing phase), using laser range sensors installed at shin height. We verify the effectiveness of the proposed method for several patterns of movement, which are seen when a person interacts with the service robot and evaluate measurement accuracy.}
}
@article{KAHRAMAN2020103643,
title = {Fuzzy controlled humanoid robots: A literature review},
journal = {Robotics and Autonomous Systems},
volume = {134},
pages = {103643},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103643},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304838},
author = {Cengiz Kahraman and Muhammet Deveci and Eda Boltürk and Seda Türk},
keywords = {Humanoid robots, Robots, Fuzzy control, Fuzzy sets, Classification},
abstract = {Humanoid robots generated by inspiring by human appearances and abilities have became essential in human society to improve the quality of their life. All over the world, there have been many researchers who have focused on humanoid robots to develop the capabilities of humanoid robots. Generally, humanoid robot systems include mechanisms of decision making and information processing. Because of the uncertainty behind decision making and information processes, fuzzy sets are used most commonly. This study investigates a comprehensive literature review about humanoid robots that presents the recent technological developments and the theories associated with fuzzy set models. The basic principles and concepts of fuzzy sets for humanoid robots are presented.}
}
@article{NAGHAVI2020103667,
title = {Assist-As-Needed control of a hip exoskeleton based on a novel strength index},
journal = {Robotics and Autonomous Systems},
volume = {134},
pages = {103667},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103667},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305078},
author = {Naeim Naghavi and Alireza Akbarzadeh and S. Mohammad Tahamipour-Z. and Iman Kardan},
keywords = {Adaptive torque controller, Assist-As-Needed controller, Generalized fuzzy hyperbolic model, Hip assistive exoskeleton, Strength index},
abstract = {This paper addresses the challenging concept of Assist-As-Needed control of exoskeleton robots. The proposed controller boosts the voluntary participation of the patient by providing assistance according to the ability of the wearer in performing the assigned task. A novel strength index is presented that combines interaction force and position-tracking error into a single quantity to estimate the physical strength of the wearer during the therapy. The estimated strength is used to adjust the boundaries of a virtual tunnel around the desired trajectory, defined to assume a degree of freedom for the wearer’s motions and to compensate for asymmetric gait patterns. The required assistance is then defined by an adaptive impedance controller according to the distance of the tracking error from the tunnel boundaries. To ensure that the assistance is accurately supplied to the patient, an adaptive torque controller is integrated into the control loop. The adaptive torque controller uses a generalized fuzzy hyperbolic model to compensate for the inherent impedance of the exoskeleton. Simulation results on a hemiplegic model show that the proposed index can estimate the wearer’s strength properly and the proposed assist-as-needed controller can reduce the tracking error. The performance of the proposed method is also evaluated experimentally on a healthy subject wearing a hip exoskeleton. The results verify that the proposed method can be used in a variety of therapeutic applications where it is important to track the desired trajectory with minimum interventions.}
}
@article{PREUCIL2022104050,
title = {Special Issue on the 9th European Conference on Mobile Robots (ECMR 2019)},
journal = {Robotics and Autonomous Systems},
volume = {150},
pages = {104050},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104050},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022000203},
author = {Libor Přeučil and Sven Behnke and Miroslav Kulich}
}
@article{SEKIGUCHI2020103562,
title = {Human-friendly control system design for two-wheeled service robot with optimal control approach},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103562},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103562},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304024},
author = {Shunichi Sekiguchi and Ayanori Yorozu and Kazuhiro Kuno and Masaki Okada and Yutaka Watanabe and Masaki Takahashi},
keywords = {Autonomous mobile service robot, Nonlinear model predictive control, Human-friendly control system},
abstract = {This paper proposes a novel control system design for a two-wheeled service robot that follows a person as an assistant without knowing the person’s destination. For this kind of service robot, the key skill is to realize human-friendly movement. However, appropriate motion always changed depending on the situation. For instance, when the robot is close and person turns toward it, it is important to suppress the robot’s acceleration. Likewise, if the person turns away from the robot, the robot should maintain its position within an appropriate area. Therefore, to deal with various required movements, our control system is able to change its properties automatically and suitably depending on the situation by using weights of the cost function in nonlinear model predictive control (NMPC) as a function of the relative distance between the person and the robot. Unlike previous methods, our design includes only one controller. Consequently, we are able to take into account system stability. Moreover, owing to proposing in NMPC framework, it is easy to extend our method by adopting other recognition or goal-setting methods. We conducted simulations using actual human walking data taken by the robot’s laser range sensors. The experiments demonstrate that the robot can follow a person who performs U-turn, confirming that our method can produce human-friendly robot movement in a practical scene.}
}
@article{LI2020103557,
title = {Real-time 3D object proposal generation and classification using limited processing resources},
journal = {Robotics and Autonomous Systems},
volume = {130},
pages = {103557},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103557},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020303973},
author = {Xuesong Li and Jose Guivant and Subhan Khan},
keywords = {Point cloud segmentation, 3D object detection, Optimization, CNN},
abstract = {The task of detecting 3D objects is important in various robotic applications. The existing deep learning-based detection techniques have achieved impressive performances. However, these techniques are limited to being run on a graphics processing unit (GPU) in a real-time environment. To achieve real-time 3D object detection with limited computational resources, we propose an efficient detection method based on 3D proposal generation and classification. The proposal generation is based mainly on point segmentation, while proposal classification is performed by a lightweight convolution neural network (CNN). KITTI datasets are then used to validate our method. It takes only 0.082 s for our method to process one point block with one core of the central processing unit (CPU). In addition to efficiency, the experimental results also demonstrate the capability of the proposed method of producing a competitive performance in object recall and classification.}
}
@article{MALTAR2020103598,
title = {Visual place recognition using directed acyclic graph association measures and mutual information-based feature selection},
journal = {Robotics and Autonomous Systems},
volume = {132},
pages = {103598},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103598},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304383},
author = {Jurica Maltar and Ivan Marković and Ivan Petrović},
keywords = {Visual place recognition, Localization, Deep convolutional neural networks, Mutual information-based feature selection, SeqSLAM},
abstract = {Visual localization is a challenging problem, especially over the long run, since places can exhibit significant variation due to dynamic environmental and seasonal changes. To tackle this problem, we propose a visual place recognition method based on directed acyclic graph matching and feature maps extracted from deep convolutional neural networks (DCNN). Furthermore, in order to find the best subset of DCNN feature maps with minimal redundancy, we propose to form probability distributions on image representation features and leverage the Jensen–Shannon divergence to rank features. We evaluate the proposed approach on two challenging public datasets, namely the Bonn and the Freiburg datasets, and compare it to the state-of-the-art methods. For image representations, we evaluated the following DCNN architectures: AlexNet, OverFeat, ResNet18 and ResNet50. Due to the proposed graph structure, we are able to account for any kind of correlations in image sequences, and therefore dub our approach NOSeqSLAM. Algorithms with and without feature selection were evaluated based on precision–recall curves, area under the curve score, best recall at 100% precision score and running time, with NOSeqSLAM outperforming the counterpart approaches. Furthermore, by formulating the mutual information-based feature selection specifically for visual place recognition and by selecting the feature percentile with the best score, all the algorithms, and not just NOSeqSLAM, exhibited enhanced performance with the reduced feature set.}
}
@article{ZHANG2020103503,
title = {A scene comprehensive safety evaluation method based on binocular camera},
journal = {Robotics and Autonomous Systems},
volume = {128},
pages = {103503},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103503},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019309108},
author = {Xinyu Zhang and Wenbo Shao and Mo Zhou and Qifan Tan and Jun Li},
abstract = {Intelligent vehicle is an inevitable trend in urban transportation development. Whether for driver assistance systems or advanced driverless systems, safety issues are one of the cores of these systems. This paper proposes a scene safety evaluation method based on binocular camera. By proposing a comprehensive safety evaluation model, the uncertainty of the scene can be represented with a single value, which can be expressed as a quantitative evaluation of the safety of intended functionality (SOTIF). It provides real-time safety monitoring and protection for the driving process of drivers and occupants by the human–computer interaction. Based on the calculation results, different monitoring methods can be adopted for different levels of autonomous driving to further improve the safety of autonomous driving.}
}
@article{DUFOUR2020103626,
title = {Visual–spatial attention as a comfort measure in human–robot collaborative tasks},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103626},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103626},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304668},
author = {Kévin Dufour and Jorge Ocampo-Jimenez and Wael Suleiman},
keywords = {Collaborative robotics, Visual–spatial attention, Inverse kinematics, QP solver},
abstract = {In this paper, we propose a new formulation to consider visual–spatial attention in order to improve the comfort of a human standing or operating near a collaborative robot. This formulation is based on the principle of having the robot’s end-effector in the human visual–spatial attention as much as possible. The integration of this new constraint into the Inverse Kinematics (IK) problem is thoroughly studied and efficient solutions are proposed. Moreover, to allow the robot to react rapidly in the case of unforeseen events, adding the manipulability index to the IK problem is also studied and its impact is analyzed. The proposed method is then extensively tested in simulation and verified on the real Baxter research robot, these experiments pointed out the method efficiency to improve the task visibility while avoiding self-occlusion and singular configurations. Moreover, real experiments revealed the method robustness to uncertainties such as imprecision of detecting human gaze direction.}
}
@article{HAMADI2020103602,
title = {Comparative study of self tuning, adaptive and multiplexing FTC strategies for successive failures in an Octorotor UAV},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103602},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103602},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304425},
author = {Hussein Hamadi and Benjamin Lussier and Isabelle Fantoni and Clovis Francis and Hassan Shraim},
keywords = {Fault-tolerant control, Sliding mode control, Robust control, Multiplexing, Control allocation, Actuator redundancy, Adaptive control, UAV},
abstract = {This paper presents three fault-tolerant control (FTC) strategies for a coaxial octorotor unmanned aerial vehicle (UAV) regarding motor failures. The first FTC is based on a control mixing strategy which consists of a set of control laws designed offline, each one dedicated to a specific fault situation. The second FTC, a robust adaptive sliding mode control allocation is presented, where the control gains of the controller are adjusted online in order to redistribute the control signals among the healthy motors in order to stabilize the overall system. The third FTC strategy is a new strategy proposed in this article, which is based on a self-tuning sliding mode control (STSMC) where the control gains are readjusted based on the detected error to maintain the stability of the system. Multiple indoor experiments on an octorotor UAV are conducted to show and compare the effectiveness and the behavior of each FTC scheme after successive faults are injected. More specifically, we inject complete actuator’s failures into the top four motors of our octorotor. Every strategies show good fault tolerance results, although the control mixing method performs slightly better overall while the adaptive method performs slightly worse. However, the control mixing method requires a huge design effort to take into account as much situations as possible, while the adaptive method and the STSMC only require to determine a few gains. The adaptive method do not need fault detection to operate, but it thus does not provide information on the system’s health without an additional fault identification and diagnosis mechanism, while both the control mixing method and the STSMC provide such information.}
}
@article{DOGAN2020103654,
title = {The impact of adding perspective-taking to spatial referencing during human–robot interaction},
journal = {Robotics and Autonomous Systems},
volume = {134},
pages = {103654},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103654},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304942},
author = {Fethiye Irmak Doğan and Sarah Gillet and Elizabeth J. Carter and Iolanda Leite},
keywords = {Perspective-taking, Spatial referring expressions},
abstract = {For effective verbal communication in collaborative tasks, robots need to account for the different perspectives of their human partners when referring to objects in a shared space. For example, when a robot helps its partner find correct pieces while assembling furniture, it needs to understand how its collaborator perceives the world and refer to objects accordingly. In this work, we propose a method to endow robots with perspective-taking abilities while spatially referring to objects. To examine the impact of our proposed method, we report the results of a user study showing that when the objects are spatially described from the users’ perspectives, participants take less time to find the referred objects, find the correct objects more often and consider the task easier.}
}
@article{ZHONG2020103642,
title = {Fuzzy logic compliance adaptation for an assist-as-needed controller on the Gait Rehabilitation Exoskeleton (GAREX)},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103642},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103642},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304826},
author = {Bin Zhong and Jinghui Cao and Kaiqi Guo and Andrew McDaid and Yuxin Peng and Qing Miao and ShengQ. Xie and Mingming Zhang},
keywords = {Fuzzy logic, Pneumatic muscle, Compliance adaptation, Gait rehabilitation, Assist-as-needed},
abstract = {Assist-as-needed control strategy is an emerging approach to improve the effectiveness of gait rehabilitation training. We have proposed a pneumatic muscle (PM) driven Gait Rehabilitation Exoskeleton (GAREX) implemented with a multi-input–multi-output (MIMO) sliding mode control system to actively adjust the assistance level provided during gait rehabilitation. To realize the assist-as-needed control strategy, a specific algorithm is imperative to assess the active participation or effort of wearers and adapt the amount of assistance accordingly. We sought to establish a fuzzy logic compliance adaptation (FLCA) controller to form a novel cascade control system. We evaluated the feasibility of implemented FLCA controller on the performance of adjusting the compliance of GAREX’s knee joint according to the online assessment of the wear’s active participation level once in every gait cycle. Using controlled, treadmill-based walking tests involved three healthy subjects, we demonstrate that FLCA controller could effectively distinguish the capability/effort levels of wearers and enable the exoskeleton to adapt the knee joint compliance accordingly. Obtained results reveal that FLCA controller can collaborate well with MIMO sliding mode controller in a system and indicate the novel method of realizing assist-as-needed concept with the pneumatic muscle powered mechanisms.}
}
@article{OGUZEKIM2020103590,
title = {TDOA based localization and its application to the initialization of LiDAR based autonomous robots},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103590},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103590},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304309},
author = {Pınar Oğuz-Ekim},
keywords = {Squared range difference-based robot localization, TDOA, Least squares, LiDAR, Scan matching, Initialization},
abstract = {This work considers the problem of locating a single robot given a set of squared noisy range difference measurements to a set of points (anchors) whose positions are known. In the sequel, localization problem is solved in the Least-Squares (LS) sense by writing the robot position in polar/spherical coordinates. This representation transforms the original nonconvex/multimodal cost function into the quotient of two quadratic forms, whose constrained maximization is more tractable than the original problem. Simulation results indicate that the proposed method has similar accuracy to state-of-the-art optimization-based localization algorithms in its class, and the simple algorithmic structure and computational efficiency makes it appealing for applications with strong computational constraints. Additionally, location information is used to find the initial orientation of the robot with respect to the previously obtained map in scan matching. Thus, the crucial problem of the autonomous initialization and localization in robotics is solved.}
}
@article{TOMY2020103629,
title = {Battery charge scheduling in long-life autonomous mobile robots via multi-objective decision making under uncertainty},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103629},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103629},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304693},
author = {Milan Tomy and Bruno Lacerda and Nick Hawes and Jeremy L. Wyatt},
keywords = {Mobile service robots, Markov decision processes, Multi-objective reasoning, Long term autonomy},
abstract = {The daily working hours of mobile robots are limited primarily by battery life. Most systems use a combination of thresholds and fixed periods to decide when to charge. This produces charging behaviour that ignores high-value tasks that must be performed within time-windows or by deadlines. Instead the robot should schedule charging adaptively, taking into account the times of day when it is expected to be given more valuable tasks to perform. This paper proposes an approach that exploits the fact that, during long-term deployments, the robot can learn when it is most probable that valuable tasks are added to the system, enabling it to schedule charging at times that are expected to be less busy. We pose the problem of scheduling battery charging as a multi-objective sequential decision making problem over a time-dependent Markov decision process model of expected task rewards and battery dynamics. We evaluate the scalability and solution quality of our multi-objective scheduler, and compare it with a typical rule-based approach. Empirical results show that our approach enables more flexible and efficient robot behaviour, which takes into account both the value of current available tasks and the predicted value of future tasks to decide whether to charge at a given time.}
}
@article{ALMEIDA2020103605,
title = {Road detection based on simultaneous deep learning approaches},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103605},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103605},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304450},
author = {Tiago Almeida and Bernardo Lourenço and Vitor Santos},
keywords = {Visual perception, Data combination, Deep learning, Computer vision, Road map detection, Road lane lines detection, Road segmentation, Driving assistance},
abstract = {One of the most important challenges for Autonomous Driving and Driving Assistance systems is the detection of the road to perform or monitor navigation. Many works can be found in the literature to perform road and lane detection, using both algorithmic processing and learning based techniques. However, no single solution is mentioned to be applicable in any circumstance of mixed scenarios of structured, unstructured, lane based, line based or curb based limits, and other sorts of boundaries. So, one way to embrace this challenge is to have multiple techniques, each specialized on a different approach, and combine them to obtain the best solution from individual contributions. That is the central concern of this paper. By improving a previously developed architecture to combine multiple data sources, a solution is proposed to merge the outputs of two Deep Learning based techniques for road detection. A new representation for the road is proposed along with a workflow of procedures for the combination of two simultaneous Deep Learning models, based on two adaptations of the ENet model. The results show that the overall solution copes with the alternate failures or under-performances of each model, producing a road detection result that is more reliable than the one given by each approach individually.}
}
@article{NITSCHE2020103577,
title = {Visual-inertial teach and repeat},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103577},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103577},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304176},
author = {Matías Nitsche and Facundo Pessacg and Javier Civera},
keywords = {UAV, Embedded, Navigation, Visual-inertial, Stereo, Teach-and-replay, Relative},
abstract = {Teach and Repeat (T&R) refers to the technology that allows a robot to autonomously follow a previously traversed route, in a natural scene and using only its onboard sensors. In this paper we present a Visual-Inertial Teach and Repeat (VI-T&R) algorithm that uses stereo and inertial data and targets Unmanned Aerial Vehicles with limited on-board computational resources. We propose a tightly-coupled relative formulation of the visual-inertial constraints that is tailored to the T&R application. In order to achieve real-time operation on limited hardware, we reduce the problem to motion-only visual-inertial Bundle Adjustment. In the repeat stage, we detail how to generate a trajectory and smoothly follow it with a constantly changing relative frame. The proposed method is validated in simulated environments, using real sensor data from the public EuRoC dataset, and using our own robotic setup and closed-loop control. Our experimental results demonstrate high accuracy and real-time performance both on a standard desktop system and on a low-cost Odroid X-U4 embedded computer.}
}
@article{ZHANG2020103645,
title = {An iterative optimization approach for multi-robot pattern formation in obstacle environment},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103645},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103645},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304851},
author = {Fangfang Zhang and Tingting Wang and Qiyan Li and Jianbin Xin},
keywords = {Multi-robot systems, Pattern formation, Convex quadratic programming, Collision avoidance},
abstract = {Pattern formation for multi-robot systems has received increasing attention in different scenarios. However, existing methods cannot efficiently optimize pattern formation in the obstacle environment. To address this limitation, this paper proposes a new planning method that assigns the optimal goals to the robots and iteratively computes collision-free paths to reach goal positions. Firstly, according to the random initial position of the group robot and the arbitrary shape, convex quadratic programming is used to minimize the distance to obtain the optimal pattern parameters under certain constraints. Secondly, the iterative controller plans the collision-free path of each robot to the goal considering a preferred velocity. Simulation results verified the effectiveness of the proposed methodology for scenarios of letter formation, in comparison to a commonly-used method.}
}
@article{KHNISSI2020103593,
title = {A smart mobile robot commands predictor using recursive neural network},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103593},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103593},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304334},
author = {Khaled Khnissi and Chiraz {Ben Jabeur} and Hassene Seddik},
keywords = {Virtual simulator, Smart navigation, Mobile robot, RNNC, Prediction controller, TurtleBot},
abstract = {Autonomous navigation of mobile robot via classic neural network (NN) models are no more valid in terms of efficiency and accuracy due to the development of new advanced techniques. However, the necessity of finding an implementable Recursive Neural Network (RNN) model to predict the motor control of the robot with both speed and accuracy constraints still remains stagnant because of the nonlinearity and complexity of the trajectories. To provide new solutions for smart navigation problems, this paper proposes a new implementable recursive neural network controller (RNNC) predictor that calculates the Pulse Width Modulation (PMW) signals of the motors. Such proposed Multi-input Multi-output (MIMO) Controller succeeded to solve the problem of speed and accuracy of autonomous navigation. The Smart RNNC model design is illustrated with its architecture in details. Due to the complexity and the non-efficiency of the training process in real-world, a 3D Simulator was developed to create all possible scenarios. The machine learning and navigation predictions processes for designing the new RNNC model are presented together in details. In addition, the motor commands generation speed and accuracy as well as their efficiency are theoretically and practically proven. Moreover, numerical studies, 3D scenarios of trajectory tracking and obstacle avoidance prove the effectiveness and robustness of the proposed technique.}
}
@article{BERSANI2021103662,
title = {An integrated algorithm for ego-vehicle and obstacles state estimation for autonomous driving},
journal = {Robotics and Autonomous Systems},
volume = {139},
pages = {103662},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103662},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305029},
author = {Mattia Bersani and Simone Mentasti and Pragyan Dahal and Stefano Arrigoni and Michele Vignati and Federico Cheli and Matteo Matteucci},
keywords = {Obstacles tracking, Sensor fusion, State estimation, Autonomous driving},
abstract = {Understanding of the driving scenario represents a necessary condition for autonomous driving. Within the control routine of an autonomous vehicle, it represents the preliminary step for the motion planning system. Estimation algorithms hence need to handle a considerable number of information coming from multiple sensors, to provide estimates regarding the motion of ego-vehicle and surrounding obstacles. Furthermore, tracking is crucial in obstacles state estimation, because it ensures obstacles recognition during time. This paper presents an integrated algorithm for the estimation of ego-vehicle and obstacles’ positioning and motion along a given road, modeled in curvilinear coordinates. Sensor fusion deals with information coming from two Radars and a Lidar to identify and track obstacles. The algorithm has been validated through experimental tests carried on a prototype of an autonomous vehicle.}
}
@article{HAUGALOKKEN2020103589,
title = {Monocular vision-based gripping of objects},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103589},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103589},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304292},
author = {Bent Oddvar Arnesen Haugaløkken and Martin Breivik Skaldebø and Ingrid Schjølberg},
keywords = {Underwater robotics, Object detection, Autonomy, Dynamic positioning, Manipulator},
abstract = {Optics-based systems may provide high spatial and temporal resolution for close range object detection in underwater environments. By using a monocular camera on a low cost underwater vehicle manipulator system, objects can be tracked by the vehicle and handled by the manipulator. In this paper, a monocular camera is used to detect an object of interest through object detection. Spatial features of the object are extracted, and a dynamic positioning system is designed for the underwater vehicle in order for it to maintain a desired position relative to the object. A manipulator mounted under the vehicle is used to retrieve the object through a developed kinematic control system. Experimental tests verify the proposed methodology. A stability analysis proves asymptotic stability properties for the chosen sliding mode controller and exponential stability for the task error.}
}
@article{SHIBATA2020103527,
title = {Robust shape estimation with false-positive contact detection},
journal = {Robotics and Autonomous Systems},
volume = {129},
pages = {103527},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103527},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019307791},
author = {Kazuki Shibata and Tatsuya Miyano and Tomohiko Jimbo and Takamitsu Matsubara},
keywords = {Tactile sensing, Shape estimation, Gaussian processes},
abstract = {We propose a means of omni-directional contact detection using accelerometers instead of tactile sensors for object shape estimation using touch. Unlike tactile sensors, our contact-based detection method tends to induce a degree of uncertainty with false-positive contact data because the sensors may react not only to actual contact but also to the unstable behavior of the robot. Therefore, it is crucial to consider a robust shape estimation method capable of handling such false-positive contact data. To realize this, we introduce the concept of heteroscedasticity into the contact data and propose a robust shape estimation algorithm based on Gaussian process implicit surfaces (GPIS). We confirmed that our algorithm not only reduces shape estimation errors caused by false-positive contact data but also distinguishes false-positive contact data more clearly than the GPIS through simulations and actual experiments using a quadcopter.}
}
@article{LIU2020103515,
title = {Skill transfer learning for autonomous robots and human–robot cooperation: A survey},
journal = {Robotics and Autonomous Systems},
volume = {128},
pages = {103515},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103515},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019309972},
author = {Yueyue Liu and Zhijun Li and Huaping Liu and Zhen Kan},
keywords = {Skill transfer, Robot learning, Human demonstrations, Neurophysiological skill acquisition},
abstract = {Designing a robot system with reasoning and learning ability has gradually become a research focus in robotics research field. Recently, Skill Transfer Learning (STL), i.e., the ability of transferring human skills to robots, has become a research thrust for autonomous robots and human–robot cooperation. It provides the following benefits: (i) the skill transfer learning system with independent decision-making and learning ability enables the robot to learn and acquire manipulation skills in a complex and dynamic environment, which can overcome the shortages of conventional methods such as traditional programming, and greatly improve the adaptability of the robot to complex environments and (ii) human physiological signals allow us to extract motion control characteristics from physiological levels which create a rich sensory signal. In this survey, we provide an overview of the most important applications of STL by analyzing and categorizing existing works in autonomous robots and human–robot cooperation area. We close this survey by discussing remaining open challenges and promising research topics in future.}
}
@article{LIU2020103532,
title = {A new approach for the estimation of non-cooperative satellites based on circular feature extraction},
journal = {Robotics and Autonomous Systems},
volume = {129},
pages = {103532},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103532},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019307158},
author = {Yang Liu and Zongwu Xie and Qi Zhang and Xiaoyu Zhao and Hong Liu},
keywords = {Pose estimation, Non-cooperative target, Adapter ring, Ellipse detection, Space robot, On-orbit service},
abstract = {Pose estimation of non-cooperative satellites has been a hot topic in the study of astronautics as the visual feedback will highly enhance the safety of on-orbit services. A stereo vision system is proposed in this paper. It works as an eye-to-hand vision camera in the final approach phase Based on circular feature extraction, a closed-form solution is presented. The position and orientation of the adapter ring can be figured out in real-time as well as the unknown radius. Neither additional sensors nor prior knowledge is required, and the orientation-duality problem has been solved. It works well on the partial ellipses and is robust to outliers, noise and occlusions. Experimental results on both synthetic and real images have demonstrated the effectiveness and efficiency of the proposed method.}
}
@article{LUDDECKE2020103511,
title = {Fine-grained action plausibility rating},
journal = {Robotics and Autonomous Systems},
volume = {129},
pages = {103511},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103511},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019305536},
author = {Timo Lüddecke and Florentin Wörgötter},
abstract = {An essential capability of humans is the effortless identification of useful tasks based on visual cues in everyday situations. Objects and their surroundings are integrated and processed to differentiate plausible from implausible actions. In this work, we study how to teach this ability to robots. In contrast to many tasks in computer vision where the goal is an accurate description (object labels, caption, scene class) of the present situation here the challenge is to make reasonable guesses about which forms of plausible and implausible actions can be conducted. To this end, we collect a dataset that associates images with probabilities over a set of actions. A convolutional neural network is trained to match these ground truth plausibility scores using this dataset. We compare the performance of state-of-the-art encoder architectures and specifically analyze the role of contextual cues quantitatively. While the object recognition capabilities of the encoder have a strong impact on performance, using context did not lead to substantial improvements. We show qualitatively the utility of such a system for robotic action selection in a household setting.}
}
@article{SAHIN2020103633,
title = {Spatiotemporal chaotification of delta robot mixer for homogeneous graphene nanocomposite dispersing},
journal = {Robotics and Autonomous Systems},
volume = {134},
pages = {103633},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103633},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304735},
author = {Savas Sahin and Ali Emre Kavur and Sibel {Demiroglu Mustafov} and Ozgur Seydibeyoglu and Ozgun Baser and Yalcin Isler and Cuneyt Guzelis},
keywords = {Delta robot, Chaotification, Robustness, Sliding mode control, Polymer nanocomposites mixing, Graphene},
abstract = {This paper presents the design, implementation and polymer nanocomposite mixing application of a robust spatiotemporal chaotic delta robot. Blending fluids efficiently is a vital process for the preparation of graphene nanocomposite mixing. The most commonly used mixing materials are polymeric materials that need to be blended in non-Newtonian fluids. To achieve a superior blending performance over the conventional ones, it is used two different chaotification mechanisms for the realization of the spatiotemporal chaotic delta robot mixer system. One of them is for the chaotification of the mixer propeller while the second one is for the chaotification of the three-dimensional position of the endpoint of the delta robot. The model-based robust chaotification scheme based on sliding mode control is applied to chaotify the speed of the delta robot-mixer via dynamical state-feedback chaotification method. The chaotification of 3D position of the mixer is realized in a feedforward way by producing chaotic input signals. The implemented robust chaotic delta robot mixer exploits the efficacy of chaotic mixing in obtaining homogeneity in the mixture with less operation time, and hence reduced electrical energy consumption. In these performance evaluations, energy consumption and material characterization, which are measured by reliable material characterization methods such as X-ray diffraction, Fourier-transform-infrared spectroscopy, water contact angle, dynamical mechanical analysis, atomic force microscopy, Raman and field emission-scanning electron microscope analyses, are used as criteria. The obtained results show that, for the delta robot, the proposed chaotic-speed together with 3D chaotic-movement operation mode provides a better mixing performance than other mixing operation modes.}
}
@article{VANA2020103644,
title = {Surveillance planning with safe emergency landing guarantee for fixed-wing aircraft},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103644},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103644},
url = {https://www.sciencedirect.com/science/article/pii/S092188902030484X},
author = {Petr Váňa and Jakub Sláma and Jan Faigl},
keywords = {Unmanned aerial vehicle, Surveillance planning, Emergency landing guarantee},
abstract = {In this paper, we study Emergency Landing Aware Surveillance Planning (ELASP) to determine a cost-efficient trajectory to visit a given set of target locations such that a safe emergency landing is possible at any point of the multi-goal trajectory. The problem is motivated to guarantee a safe mission plan in a case of loss of thrust for which it is desirable to have a safe gliding trajectory to a nearby airport. The problem combines computational challenges of the combinatorial multi-goal planning with demanding motion planning to determine safe landing trajectories for the curvature-constrained aerial vehicle. The crucial property of safe landing is a minimum safe altitude of the vehicle that can be found by trajectory planning to nearby airports using sampling-based motion planning such as RRT*. A trajectory is considered safe if the vehicle is at least at the minimum safe altitude at any point of the trajectory. Thus, a huge number of samples have to be evaluated to guarantee the safety of the trajectory, and an evaluation of all possible multi-goal trajectories is quickly computationally intractable. Therefore, we propose to utilize a roadmap of safe altitudes combined with the estimation of the trajectory lengths to evaluate only the most promising candidate trajectories. Based on the reported results, the proposed approach significantly reduces the computational burden and enables a solution of ELASP instances with tens of locations in units of minutes using standard single-core computational resources.}
}
@article{LI2020103621,
title = {Autonomous drone race: A computationally efficient vision-based navigation and control strategy},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103621},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103621},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304619},
author = {Shuo Li and Michaël M.O.I. Ozo and Christophe {De Wagter} and Guido C.H.E. {de Croon}},
keywords = {Micro aerial vehicle, Visual navigation, Autonomous drone race},
abstract = {Drone racing is becoming a popular sport where human pilots have to control their drones to fly at high speed through complex environments and pass a number of gates in a pre-defined sequence. In this paper, we develop an autonomous system for drones to race fully autonomously using only onboard resources. Instead of commonly used visual navigation methods, such as simultaneous localization and mapping and visual inertial odometry, which are computationally expensive for micro aerial vehicles (MAVs), we developed the highly efficient snake gate detection algorithm for visual navigation, which can detect the gate at 20 HZ on a Parrot Bebop drone. Then, with the gate detection result, we developed a robust pose estimation algorithm which has better tolerance to detection noise than a state-of-the-art perspective-n-point method. During the race, sometimes the gates are not in the drone’s field of view. For this case, a state prediction-based feed-forward control strategy is developed to steer the drone to fly to the next gate. Experiments show that the drone can fly a half-circle with 1.5 m radius within 2 s with only 30cm error at the end of the circle without any position feedback. Finally, the whole system is tested in a complex environment (a showroom in the faculty of Aerospace Engineering, TU Delft). The result shows that the drone can complete the track of 15 gates with a speed of 1.5m∕s which is faster than the speeds exhibited at the 2016 and 2017 IROS autonomous drone races.}
}
@article{ALDABBAGH2020103628,
title = {A review of terrain detection systems for applications in locomotion assistance},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103628},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103628},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304681},
author = {Ali H.A. Al-dabbagh and Renaud Ronsse},
keywords = {Terrain detection systems, Human locomotion assistance, Intent detection, Stairs detection, Ramp detection, Prostheses, Locomotion modes prediction, Environmental features recognition, Vision, Visually impaired, Wearable sensors},
abstract = {Terrain detection systems have been developed for a large body of applications. For instance, a bionic leg prosthesis would have to adapt its behavior as a function of the terrain, in order to restore a sound lower-limb biomechanics to the amputee. Visually impaired people benefit from such systems in order to collect information about their locomotion environment and avoid obstacles. Finally, mobile robots use them for estimating terrain traversability, and adjusting control algorithms as a function of the surface type. This diversity of applications led to a large repertoire of systems, regarding both hardware (sensors, processing unit) and software used for classification. This paper provides an extended review of these systems, with a specific focus on the assistance of disabled walker. More precisely, it overviews the sensory systems and algorithms that were implemented to identify different locomotion terrains in indoor or urban environments (flat ground, stairs, slopes) in a way that they are or can be worn by a human user, and running in real-time. Contributions from mobile robotics are also included, pending that they could be adapted to a scenario of locomotion assistance. The systems are classified into two categories: these relying on proprioceptive sensors only and those further using exteroceptive sensors. Contributions from both categories are then compared according to their main specifications, such as accuracy and prediction time. The paper unambiguously shows that systems with exteroceptive sensors have higher prediction capability than systems with proprioceptive sensors only, and should thus be favored for assistive devices requiring predicting transitions between locomotion tasks.}
}
@article{FANG2020103592,
title = {Vision-based posture-consistent teleoperation of robotic arm using multi-stage deep neural network},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103592},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103592},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304322},
author = {Bin Fang and Xiao Ma and Jiachun Wang and Fuchun Sun},
keywords = {Visual teleoperation, Deep neural networks, Human–robot posture-consistent mapping, Data generator},
abstract = {This paper proposes a visual teleoperation with human–robot posture-consistent based on deep neural network. A multi-stage structure of visual teleoperation network, in which the angles of robotic joints are obtained from human, is deduced. Furthermore, a novel human–robot posture-consistent mapping method is developed to generate dataset of the visual teleoperation network by solving constrained nonlinear matrix functions. Based on the designed framework, the data generator and a well trained multi-stage visual teleoperation network are presented. Finally teleoperation experiments are implemented to demonstrate that the proposed method is effectiveness and reliable.}
}
@article{GIORDANO2020103564,
title = {Coordination of thrusters, reaction wheels, and arm in orbital robots},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103564},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103564},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304048},
author = {Alessandro M. Giordano and Alexander Dietrich and Christian Ott and Alin Albu-Schäffer},
keywords = {Space robotics, Thrusters, Reaction wheels, Stability analysis, Nonlinear control, Hardware-in-the-loop},
abstract = {A fuel-efficient control strategy for a manipulator-equipped spacecraft is presented. The strategy uses the thrusters, the reaction wheels, and the arm drives in a coordinated way to limit the use of the thrusters and achieve ideally zero fuel consumption in contact-free maneuvering. The thrusters are activated automatically only after contact, to stabilize the inertial motion of the system. The controller regulates the translation of the center-of-mass (CoM) of the whole space robot, the rotation of the spacecraft, and the pose of the end-effector (EE) in a decoupled way, utilizing the thrusters to control the CoM translation only and the remaining actuators to control the rotation and end-effector coordinately. The method is validated experimentally using a hardware-in-the-loop simulator composed of a seven degrees-of-freedom (DOF) arm mounted on a 6DOF simulated spacecraft. Numerical simulations with discrete thrusters assess the fuel efficiency of the proposed strategy.}
}
@article{TANG2020103661,
title = {An improved H-infinity unscented FastSLAM with adaptive genetic resampling},
journal = {Robotics and Autonomous Systems},
volume = {134},
pages = {103661},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103661},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305017},
author = {Ming Tang and Zhe Chen and Fuliang Yin},
keywords = {Simultaneous localization and mapping (SLAM), FastSLAM, Unscented Kalman filter, Particle filter, Time varying noise estimator, Adaptive genetic algorithm},
abstract = {The FastSLAM is a typical tracking algorithm for SLAM, but it often suffers from the low tracking accuracy. To mitigate the problem, an improved H-Infinity unscented FastSLAM (IHUFastSLAM) with adaptive genetic resampling is proposed in this paper. Specifically, the H-Infinity unscented Kalman filter algorithm is improved using an adaptive factor and is employed as importance sampling in particle filter. Next, the process noise and the measurement noise are estimated by a time varying noise estimator. Moreover, an adaptive genetic algorithm is used to complete the resampling of particle filter. Finally, the improved H-Infinity UFastSLAM with adaptive genetic resampling is proposed to complete robot tracking. The proposed algorithm can track robot with good accuracy, and obtain reliable state estimation in SLAM. Simulation results reveal the validity of the proposed algorithm.}
}
@article{SU2020103588,
title = {Trajectory coordination for a cooperative multi-manipulator system and dynamic simulation error analysis},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103588},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103588},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304280},
author = {Chunjian Su and Shuai Zhang and Shumei Lou and Rui Wang and Gaohua Cao and Longyun Yang and Qing Wang},
keywords = {Multi-manipulator, Track coordination, Dynamic simulation},
abstract = {To achieve temporal and spatial correspondence between multiple robotic manipulators, the system must correctly analyze the coordinated path required for a specific task. Based on manipulator kinematics analysis, we first studied the kinematic constraints between the end-effectors of cooperative manipulators, and deduced the multi-manipulator cooperative kinematics constraint equations in the Cartesian coordinate system space under different motion modes. Then, we created two MD-6 manipulator models to simulate the trajectory simulation of the synchronous and relative motion of the manipulator. This allowed us to verify the correctness of the proposed trajectory coordination method, and analyze the influence of external loads on the position and posture of the end-effector of the manipulator, to effectively predict the cooperative motion error of the manipulator system. Finally, in order to verify the effectiveness of the proposed trajectory coordination method, we established a robotic experimental platform and conducted experimental research. The results show that the multi-manipulator trajectory coordination method studied in this paper can make multi-manipulators effectively achieve the target requirements of tasks such as time and space cooperative handling and circular drawing operations.}
}
@article{KHALILI2020103509,
title = {Tuning and sensitivity analysis of a hexapod state estimator},
journal = {Robotics and Autonomous Systems},
volume = {129},
pages = {103509},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103509},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019304166},
author = {Hassan H. Khalili and Wei Cheah and Tomas B. Garcia-Nathan and Joaquin Carrasco and Simon Watson and Barry Lennox},
keywords = {Extended Kalman filter, Indirect EKF, EKF tuning, Hexapod, State estimation, Particle swarm optimisation (PSO)},
abstract = {An important envisaged application of legged robots is the exploration and mapping of extreme environments with an unknown terrain. Corin is a hexapod designed at the University of Manchester, which is able to perform motions using footholds on surfaces perpendicular to the ground plane. This allows it to be able to navigate through confined and narrow spaces. The hexapod requires an accurate estimate of its pose in order to be able to perform these motions. Current state-of-the-art state estimators for legged robots that solely use proprioceptive sensors, fuse inertial and leg kinematic measurements through an extended Kalman filter (EKF). This paper describes the implementation and validation of a state estimator on the Corin hexapod whilst performing motions using surface perpendicular to the ground plane. Another novelty of the work is the analysis of the algorithm sensitivity to the filter parameters and motion variables, and the tuning of the EKF using particle swarm optimisation (PSO). The results show that the average error achieved was below 6% for both position and orientation estimates.}
}
@article{ZHANG2020103554,
title = {A novel coordinated motion planner based on capability map for autonomous mobile manipulator},
journal = {Robotics and Autonomous Systems},
volume = {129},
pages = {103554},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103554},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019310553},
author = {Heng Zhang and Qi Sheng and Yuxin Sun and Xinjun Sheng and Zhenhua Xiong and Xiangyang Zhu},
keywords = {Autonomous mobile manipulator, Coordinated motion, Capability map, Motion planning},
abstract = {With the development of the robotic technology, Autonomous Mobile Manipulator (AMM) is increasingly used in more applications. Reasonable motion planning for AMM to maintain high manipulation capability is the prerequisite for the success of the mobile manipulation task. In this paper, the Capability Map (CM) of AMM that gives the distribution of the manipulability in cartesian space is first built. Then given the path of the end effector, we design a novel path planner for the mobile robot by querying CM online so that AMM keeps high manipulability. Moreover, a task-priority coordinated motion controller is developed to control the mobile robot and the manipulator to track their trajectories. In this controller, the trajectory of the manipulator is used as the primary task, and the trajectory of the mobile robot is treated as the constrained task. Simulation results show that the path of the mobile robot can be found online, and AMM follows the trajectories well.}
}
@article{HOANG2020103632,
title = {Object-RPE: Dense 3D reconstruction and pose estimation with convolutional neural networks},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103632},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103632},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304723},
author = {Dinh-Cuong Hoang and Achim J. Lilienthal and Todor Stoyanov},
keywords = {Object pose estimation, 3D reconstruction, Semantic mapping, 3D registration},
abstract = {We present an approach for recognizing objects present in a scene and estimating their full pose by means of an accurate 3D instance-aware semantic reconstruction. Our framework couples convolutional neural networks (CNNs) and a state-of-the-art dense Simultaneous Localization and Mapping (SLAM) system, ElasticFusion (Whelan et al., 2016), to achieve both high-quality semantic reconstruction as well as robust 6D pose estimation for relevant objects. We leverage the pipeline of ElasticFusion as a backbone, and propose a joint geometric and photometric error function with per-pixel adaptive weights. While the main trend in CNN-based 6D pose estimation has been to infer object’s position and orientation from single views of the scene, our approach explores performing pose estimation from multiple viewpoints, under the conjecture that combining multiple predictions can improve the robustness of an object detection system. The resulting system is capable of producing high-quality instance-aware semantic reconstructions of room-sized environments, as well as accurately detecting objects and their 6D poses. The developed method has been verified through extensive experiments on different datasets. Experimental results confirmed that the proposed system achieves improvements over state-of-the-art methods in terms of surface reconstruction and object pose prediction. Our code and video are available at https://sites.google.com/view/object-rpe.}
}
@article{MERTYUZ2020103627,
title = {FUHAR: A transformable wheel-legged hybrid mobile robot},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103627},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103627},
url = {https://www.sciencedirect.com/science/article/pii/S092188902030467X},
author = {İrem Mertyüz and Alper K. Tanyıldızı and Beyda Taşar and Ahmet B. Tatar and Oğuz Yakut},
keywords = {Mobile robot, Transformable wheel-legged robot, Motion analysis, Obstacle avoidance, Dynamic model},
abstract = {This paper introduces a mobile robot with a new type of transformable wheel legs that can be used for flat and rough terrain. It integrates the stability and maneuverability of a wheeled robot and the legged robot’s obstacle climbing capacity using a transformable mechanism with wheel legs. With a transformation structure based on a four-bar mechanism, these two modes can be easily changed. This paper analyzes the movements for the proposed robot in wheeled and legged mode. Dynamic modeling and design of a control system were obtained. Then, the obstacle climbing strategies under legged modes were carried out. Finally, on the basis of the simulation, a prototype of the proposed robot was designed and produced. The results from the experiments validate the efficiency of the designed hybrid mobile robot.}
}
@article{CHEN2020103559,
title = {Advanced mapping robot and high-resolution dataset},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103559},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103559},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020303997},
author = {Hongyu Chen and Zhijie Yang and Xiting Zhao and Guangyuan Weng and Haochuan Wan and Jianwen Luo and Xiaoya Ye and Zehao Zhao and Zhenpeng He and Yongxia Shen and Sören Schwertfeger},
keywords = {Mobile robot, Sensor synchronization, Sensor calibration, Robotic datasets, Simultaneous Localization and Mapping (SLAM)},
abstract = {This paper presents a fully hardware synchronized mapping robot with support for a hardware synchronized external tracking system, for super-precise timing and localization. Nine high-resolution cameras and two 32-beam 3D Lidars were used along with a professional, static 3D scanner for ground truth map collection. With all the sensors calibrated on the mapping robot, three datasets are collected to evaluate the performance of mapping algorithms within a room and between rooms. Based on these datasets we generate maps and trajectory data, which is then fed into evaluation algorithms. We provide the datasets for download and the mapping and evaluation procedures are made in a very easily reproducible manner for maximum comparability. We have also conducted a survey on available robotics-related datasets and compiled a big table with those datasets and a number of properties of them.}
}
@article{OLCAY2020103604,
title = {Collective navigation of a multi-robot system in an unknown environment},
journal = {Robotics and Autonomous Systems},
volume = {132},
pages = {103604},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103604},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304449},
author = {Ertug Olcay and Fabian Schuhmann and Boris Lohmann},
keywords = {Swarm intelligence, Cooperative control, Autonomous systems, Collective navigation},
abstract = {The navigation of autonomous, mobile multi-robot systems in changing environments is a challenging problem investigated over the past years. Cooperative, multiple robots are employed for many different tasks to increase the efficiency and success of a mission. However, many of the existing collective path planning approaches do not guarantee a reliable escape in environments with complex, non-convex obstacles without any prior knowledge. In this study, we developed a navigation framework for multi-robot systems in unknown areas that solely exploit the sensing information and shared data among the agents. The key contribution of this paper is the simultaneous, collision-free motion planning for fully autonomous robots in a collective manner. Furthermore, our communication architecture enables the robots to find an appropriate path to a desired, joint target position, despite the limited sensing and communication range.}
}
@article{YU2020103507,
title = {Design of transfer learning structure for slot wedge tightness inspection robot},
journal = {Robotics and Autonomous Systems},
volume = {128},
pages = {103507},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103507},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019308759},
author = {Wenbin Yu and Yingjie Zhao and Lu Ding and Lei Song and Dan Huang},
keywords = {Transfer learning, Slot wedge inspection robot, CNN, RNN, Frequency domain feature},
abstract = {The tightness inspection for the slot wedges is significant for the safe operation of large generators. One of the traditional methods is analysis of the acoustic signals of knocking on the surface of the slot wedge by inspection experts. Nowadays the slot wedge inspecting robot is an effective way to measure the tightness of the slot wedges and classify the level of the slot wedges into different groups. However, there are many types of generators and the precision cannot be guaranteed if the model of one type of generators is applied to another. Although the machine learning methods such as CNN (Convolutional Neural Networks) and RNN (Recurrent Neural Networks) are widely used for classification, they are not suitable for model transfer between different generators. In this paper, a transfer learning based structure is introduced to solve the problem and also the mixture of RNN and CNN is designed to fulfill the system. The structure is tested to transfer models with the acoustic signal sampled by the inspecting robot between the 500 MW and 600 MW generators. Experiment results show that the transfer learning structure can transfer models from one type of generators to another. Compared with the state-of-the-art methods, the proposed structure can improve the inspection precision by at least 36.7% and obtain the average precision over 79.0%.}
}
@article{RODRIGUES2020103620,
title = {Three level sequence-based Loop Closure Detection},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103620},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103620},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304607},
author = {Fernanda Rodrigues and Renata Neuland and Mathias Mantelli and Diego Pittol and Renan Maffei and Edson Prestes and Mariana Kolberg},
keywords = {Loop Closure Detection, SLAM, Mobile robotics},
abstract = {The recognition of previously visited places, known as Loop Closure Detection (LCD), composes one of the problems widely studied in robotics: simultaneous localization and mapping (SLAM). In this paper we propose a three level hierarchy based LCD method. In our serialized approach, in the First Level, a sequence of the most recently visited places is used as query to search for candidate sequences in our topological map composed by previously visited places. After that, at the Second Level, the method selects the most similar sequence to the query among all candidate sequences which is temporally consistent with the previous LCD method response. Then, at the Third Level, we match the image sequences belonging to the query sequence to the candidate sequence selected in the Second Level. The method is evaluated in different and challenging public datasets, and presents expressive results that overcome the LCD state-of-the-art methods.}
}
@article{ALVESNETO2020103607,
title = {On the consensus of nonlinear agents in unknown cluttered environments using random planning},
journal = {Robotics and Autonomous Systems},
volume = {132},
pages = {103607},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103607},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304474},
author = {Armando {Alves Neto} and Leonardo A. Mozelli and Douglas G. Macharet},
keywords = {Multi-agent consensus, Rapidly-exploring random trees, Probabilistic completeness, Rendezvous},
abstract = {The consensus of multi-agent dynamic systems is a metaphor for many different tasks involving group agreement. However, ensuring consensus in real-world scenarios, with non-convex obstacles and kinodynamic motion constraints, proves to be a hard task, since it is quite difficult to model such a problem analytically. Therefore, this paper studies the problem of state agreement for Multi-Robot Systems (MRS) in unknown cluttered complex environments. Here, we propose and analyze a distributed consensus algorithm combined with a Rapidly-exploring Random Tree-based planner, which allows linear and nonlinear systems to reach a common target on their states inside bi- or three-dimensional spaces filled with static obstacles. We demonstrate that, with enough time, our planning strategy ensures probabilistic completeness convergence independently of the topological communication network employed, since some connectivity constraints are observed. Simulated results with linear and nonlinear models are provided, showing the effectiveness of our proposed method in comparison with the state-of-the-art literature for the specific case of position consensus (rendezvous) missions.}
}
@article{2022104091,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {151},
pages = {104091},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(22)00047-1},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022000471}
}
@article{SHAN2020103647,
title = {Simulation-based lidar super-resolution for ground vehicles},
journal = {Robotics and Autonomous Systems},
volume = {134},
pages = {103647},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103647},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304875},
author = {Tixiao Shan and Jinkun Wang and Fanfei Chen and Paul Szenher and Brendan Englot},
keywords = {Lidar super-resolution, Range sensing, Perception & driving systems},
abstract = {We propose a methodology for lidar super-resolution with ground vehicles driving on roadways, which relies completely on a driving simulator to enhance, via deep learning, the apparent resolution of a physical lidar. To increase the resolution of the point cloud captured by a sparse 3D lidar, we convert this problem from 3D Euclidean space into an image super-resolution problem in 2D image space, which is solved using a deep convolutional neural network. By projecting a point cloud onto a range image, we are able to efficiently enhance the resolution of such an image using a deep neural network. Typically, the training of a deep neural network requires vast real-world data. Our approach does not require any real-world data, as we train the network purely using computer-generated data. Thus our method is applicable to the enhancement of any type of 3D lidar theoretically. By novelly applying Monte-Carlo dropout in the network and removing the predictions with high uncertainty, our method produces high accuracy point clouds comparable with the observations of a real high resolution lidar. We present experimental results applying our method to several simulated and real-world datasets. We argue for the method’s potential benefits in real-world robotics applications such as occupancy mapping and terrain modeling.}
}
@article{LOBIANCO2020103623,
title = {Joint semantic segmentation of road objects and lanes using Convolutional Neural Networks},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103623},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103623},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304632},
author = {Leonardo Cabrera {Lo Bianco} and Jorge Beltrán and Gerardo Fernández López and Fernando García and Abdulla Al-Kaff},
keywords = {Semantic segmentation, Neural networks, Lane segmentation, Object segmentation},
abstract = {This paper presents a multi-task instance segmentation neural network able to provide both road lane and road participants detection. The multi-task approach, ERFNet-based, allows feature sharing and reduces the computational requirements of the overall detection architecture, allowing real time performance even in configurations with limited hardware. The proposed method includes an ad-hoc training procedure and automatic dataset creation mechanism that is also introduced in this paper. The proposed solution has been tested and validated through a newly generated public dataset derived from the BDD100K of 19K images, and in real scenarios. The results obtained prove the viability of the work for road application and its real time performance.}
}
@article{YUAN2020103595,
title = {An efficient RRT cache method in dynamic environments for path planning},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103595},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103595},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304358},
author = {Chengren Yuan and Guifeng Liu and Wenqun Zhang and Xinglong Pan},
keywords = {Dynamic obstacle avoidance, Path planning, Multiple-query, Cache information, ROS},
abstract = {This paper is concentrated on path planning for robots working in a dynamic environment to satisfy real-time needs. An efficient bias-goal factor RRT (EBG-RRT), which is multiple-query sampling-based replanning algorithm, is proposed with rapid response and high success rate. Specifically, a relay node method is proposed to get a position where the robot and dynamic obstacles will be no-collision and help robots to move without suspended. Based on the relay node method, Connection strategy performs minimal modifications to maintain the interrupted path. In order to overcome the short of Waypoint Cache method, an efficient and optimal Waypoint Cache (EOWC) method is proposed to make use of potential cache information and find an optimal path to repair. The EOWC method is combined with the BG-RRT algorithm according to the iterative characteristics. Finally, the EBG-RRT algorithm is verified on ROS with Aubo-i5 manipulator. Simulation results provide the EBG-RRT algorithm is outperformed both in static and dynamic environments.}
}
@article{XU2020103501,
title = {Grappling claws for a robot to climb rough wall surfaces: Mechanical design, grasping algorithm, and experiments},
journal = {Robotics and Autonomous Systems},
volume = {128},
pages = {103501},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103501},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018310091},
author = {Fengyu Xu and Fanchang Meng and Quansheng Jiang and Gaoliang Peng},
keywords = {Flexible grasping claw, Grasping discrimination algorithm, 3D wall, Asperity of rough wall surfaces},
abstract = {Wall-climbing robots have been widely applied in the inspection of smooth walls. However, only a few adhesion methods have been developed for robots that will allow them to climb cliffs and dusty, high-altitude, rough walls (constructed using coarse concrete, bricks, and stones, etc.) that may be subjected to vibrations. This paper proposes a suitable adhesion method that employs grappling-hook-like claws arranged in a cross shape. First, we address the implementation mechanism required. Then, a method of extracting the characteristic parameter is revealed rough wall was devised, 3D profiles of rough walls were simulated, and the discriminant conditions necessary for the claws to stably grasp the wall were provided. A method of triangulation is proposed to judge which regions of a 3D wall can be gripped, and we subsequently present a grasping discrimination algorithm for the interaction between the miniature claws and 3D wall profile. Finally, a prototype of the grappling-hook-like claw system was fabricated. A test platform was built to test the robot which incorporates an electromagnetic vibration shaker to simulate a vibrating wall. Experiments were then carried out on the robot using the vibrating wall and a random outdoor wall. The results verified the feasibility of the proposed claws and the validity of the discriminant algorithm for gripping 3D walls. Compared with traditional adhesion approaches, the proposed method (based on hook-like claws) is more adaptable to suit various types of wall. It also has higher resistance to disturbances and so provides a more reliable method of adhesion for robots on rough walls.}
}
@article{ZHU2020103619,
title = {A novel odor source localization system based on particle filtering and information entropy},
journal = {Robotics and Autonomous Systems},
volume = {132},
pages = {103619},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103619},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304590},
author = {Hongbiao Zhu and Yibo Wang and Chengjin Du and Quan Zhang and Weidong Wang},
keywords = {Source localization, Particle filtering, Information entropy, Pseudo source filtering, Artificial potential field},
abstract = {So far, gas leakage caused by natural or human factors has led to serious consequences in terms of social security. Previous strategies for locating the odor sources appear to be either defective or incomplete. For enhancing the success rate and rapidity, this paper aims to present a novel and complete strategy in search of lurking gas sources. Particle filtering and information entropy are both employed to track the plume information. To improve the tracking efficiency in this process, a novel objective function is designed by considering the entropy gains of the suspected targets as well as the repeated exploration scores. Considering the pseudo sourced caused by obstacles, a statistics-based source determine algorithm is proposed to confirm the source’s authenticity, while the artificial potential field method is subsequently applied to eliminate the distractions introduced by the pseudo sources. Simulations and on-site tests are both carried out while results showed that the proposed scheme is competent to complete sources localization task in the scene that contains randomly distributed obstacles and pseudo source.}
}
@article{BALASKA2020103567,
title = {Unsupervised semantic clustering and localization for mobile robotics tasks},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103567},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103567},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304073},
author = {Vasiliki Balaska and Loukas Bampis and Moses Boudourides and Antonios Gasteratos},
keywords = {Topological mapping, Illumination invariance, Community detection, Robot localization},
abstract = {Due to its vast applicability, the semantic interpretation of regions or entities increasingly attracts the attention of scholars within the robotics community. The paper at hand introduces a novel unsupervised technique to semantically identify the position of an autonomous agent in unknown environments. When the robot explores a certain path for the first time, community detection is achieved through graph-based segmentation. This allows the agent to semantically define its surroundings in future traverses even if the environment’s lighting conditions are changed. The proposed semantic clustering technique exploits the Louvain community detection algorithm, which constitutes a novel and efficient method for identifying groups of measurements with consistent similarity. The produced communities are combined with metric information, as provided by the robot’s odometry through a hierarchical agglomerative clustering method. The suggested algorithm is evaluated in indoors and outdoors datasets creating topological maps capable of assisting semantic localization. We demonstrate that the system categorizes the places correctly when the robot revisits an environment despite the possible lighting variation.}
}
@article{SONG2021103651,
title = {Skill learning for robotic assembly based on visual perspectives and force sensing},
journal = {Robotics and Autonomous Systems},
volume = {135},
pages = {103651},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103651},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304917},
author = {Rui Song and Fengming Li and Wei Quan and Xuting Yang and Jie Zhao},
keywords = {Skill learning, Reinforcement learning, Robot assembly, Visual perspectives, Force sensing},
abstract = {An environment cannot be effectively described with a single perception form in skill learning for robotic assembly. The visual perception may provide the object’s apparent characteristics and the softness or stiffness of the object could be detected using the contact force/torque information during the assembly process. In the process of inserting assembly strategy learning, most of the work takes the contact force information as the current observation state of the assembly process, ignoring the influence of visual information on the assembly state. This paper proposes robotic assembly skill learning with deep Q-learning using visual perspectives and force sensing to learn an assembly policy. The reward system is designed with an image template matching for assembly state, which is used to judge whether the process is completed successfully. The observations of assembly state are described by force/torque information and the pose of the end effector. To evaluate the performance of the proposed skill learning method, experiments with a KUKA iiwa robot are performed for a plastic fasten assembly in a low-voltage apparatus. The results indicate that the robot can complete the plastic fasten assembly using the learned inserting assembly strategy with visual perspectives and force sensing.}
}
@article{JAFARZADEH2021103536,
title = {A wearable sensor vest for social humanoid robots with GPGPU, IoT, and modular software architecture},
journal = {Robotics and Autonomous Systems},
volume = {139},
pages = {103536},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103536},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019306323},
author = {Mohsen Jafarzadeh and Stephen Brooks and Shimeng Yu and Balakrishnan Prabhakaran and Yonas Tadesse},
keywords = {Social robot, Internet of Things, GPGPU, Human–robot interaction, Software architecture, Senor vest},
abstract = {Currently, most social robots interact with their surroundings and humans through sensors that are integral parts of the robots, which limits the usability of the sensors, human–robot interaction, and interchangeability. A wearable sensor garment that fits many robots is needed in many applications. This article presents an affordable wearable sensor vest, and an open-source software architecture with the Internet of Things (IoT) for social humanoid robots. The vest consists of touch, temperature, gesture, distance, vision sensors, and a wireless communication module. The IoT feature allows the robot to interact with humans locally and over the Internet. The designed architecture works for any social robot that has a general-purpose graphics processing unit (GPGPU), I2C/SPI buses, Internet connection, and the Robotics Operating System (ROS). The modular design of this architecture enables developers to easily add/remove/update complex behaviors. The proposed software architecture provides IoT technology, GPGPU nodes, I2C and SPI bus mangers, audio-visual interaction nodes (speech to text, text to speech, and image understanding), and isolation between behavior nodes and other nodes. The proposed IoT solution consists of related nodes in the robot, a RESTful web service, and user interfaces. We used the HTTP protocol as a means of two-way communication with the social robot over the Internet. Developers can easily edit or add nodes in C, C++, and Python programming languages. Our architecture can be used for designing more sophisticated behaviors for social humanoid robots.}
}
@article{KHATERI2020103540,
title = {A connectivity preserving node permutation local method in limited range robotic networks},
journal = {Robotics and Autonomous Systems},
volume = {129},
pages = {103540},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103540},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019309959},
author = {Koresh Khateri and Mahdi Pourgholi and Mohsen Montazeri and Lorenzo Sabattini},
keywords = {Connectivity maintenance, Local connectivity, Multi-robot network, Potential function},
abstract = {Limited communication range, together with mobility of robots, makes it crucial to design the control plans such that connectivity of a multi-robot network is maintained. Recently, many local and global connectivity maintenance schemes have been proposed to preserve connectivity of a robotic network. The traditional local connectivity maintenance method (LCM) is known to preserve every existing link, even though some of the existing connections might not be necessary for maintaining a path between each pair of robots, which is the aim of global connectivity maintenance (GCM) methods. However, the flexibility of movement provided by the global method costs restriction on speed and bandwidth. In this paper, a modified local connectivity maintenance method is provided to gain more flexibility of movement, while preserving the properties and simplicity of a local method. The proposed method is based on traditional local connectivity maintenance equipped with a basic operation to exchange the neighbors between two adjacent robots. Permutation of robots could be beneficial in many robotic applications such as exchanging the leader role in a V-formed robotic group or providing a path for a robot to reach its desired position while preserving the networks connectivity.}
}
@article{SHIRAFUJI2020103538,
title = {Development of a robotic finger with a branching tendon mechanism and sensing based on the moment-equivalent point},
journal = {Robotics and Autonomous Systems},
volume = {129},
pages = {103538},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103538},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019302908},
author = {Shouhei Shirafuji and Jun Ota},
keywords = {Wire-driven robotic finger, Branching tendon mechanism, Force sensing, Moment-equivalent point},
abstract = {In this paper, we developed an underactuated robotic finger with three joints having a branching tendon mechanism and a sensing system to estimate the wrench applied to the fingertip based on the moment-equivalent point (MEP). We proposed the design to combine the branching tendon mechanism and the principle of wrench sensing based on the MEP. The proposed system realized the measurement of the wrench applied to the fingertip using a simple force sensor and a wire-driven system. Furthermore, we experimentally confirmed their functioning.}
}
@article{QIN2020103606,
title = {A novel path planning methodology for automated valet parking based on directional graph search and geometry curve},
journal = {Robotics and Autonomous Systems},
volume = {132},
pages = {103606},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103606},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304462},
author = {Zhaobo Qin and Xin Chen and Manjiang Hu and Liang Chen and Jingjing Fan},
keywords = {AVP, Path planning, Path coordination, Directional graph search, Geometry curve},
abstract = {The paper presents a novel path planning methodology based on the directional graph search and the geometry curve for the Automated Valet Parking (AVP) system. The whole path planning methodology is divided into three parts including the global path planning, the path coordination strategy and the parking path planning. Firstly, the global path planning is triggered to find a path from the parking slot entrance to the rough location of the assigned parking spot. A novel directional Hybrid A* algorithm is proposed to generate the global path efficiently without redundant searches, such as the dead end. Afterwards, the path coordination strategy gives a transitional path to connect the end node of the global path to the parking planning start node. The transitional path is composed of geometry curves including arcs and line segments based on the optimal parking start node. Finally, the parking path planning generates a parking path to guide the vehicle from parking start node to the parking space. A modified C-type vertical parking path planning algorithm is utilized to generate the parking path, offering flexibility for choosing the parking start node. Simulation results based on Matlab and PreScan show that it takes less time for the proposed path planning algorithm to generate a feasible path for the AVP system compared with the general planning algorithm. The novel AVP path planning algorithm also has the potential for practical use.}
}
@article{LI2020103578,
title = {Real-time topological localization using structured-view ConvNet with expectation rules and training renewal},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103578},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103578},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304188},
author = {Chih-Hung G. Li and Yi-Feng Hong and Po-Kai Hsu and Thavida Maneewarn},
abstract = {Mobile service robots possess high potential of providing numerous assistances in the working areas. In an attempt to develop a mobile service robot which is dynamically balanced for faster movement and taller manipulation capability, we designed and prototyped J4.alpha, which is intended for swift navigation and nimble manipulation. Previously, we devised a pure visual method based on a supervised deep learning model for real-time recognition of nodal locations. Four low-resolution RGB cameras are installed around J4.alpha to capture the surrounding visual features for training and detection. As the method is developed for ease of implementation, fast real-time application, accurate detection, and low cost, we further improve the accuracy and the practicality of the method in this study. Specifically, a set of expectation rules are introduced to reject outlier detections, and a scheme of training renewal is devised to effectively react to environmental modifications. In our previous tests, precision and recall rates of the location coordinate detection by the ConvNet models were generally between 0.78 and 0.91; by introducing the expectation rules, precision and recall are improved by approximately 10%. A large scale field test is also carried out here for both corridor and factory scenarios; the performance of the proposed method was tested for detection accuracy and verified for 2 m and 0.5 m nodal intervals. The scheme of training renewal designed for capturing and reflecting environmental modifications was also proved to be effective.}
}
@article{LUPERTO2020103622,
title = {Robot exploration of indoor environments using incomplete and inaccurate prior knowledge},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103622},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103622},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304620},
author = {Matteo Luperto and Michele Antonazzi and Francesco Amigoni and N. Alberto Borghese},
keywords = {Robot exploration, Floor plan, Exploration strategy, Prior knowledge},
abstract = {Exploration is a task in which autonomous mobile robots incrementally discover features of interest in initially unknown environments. We consider the problem of exploration for map building, in which a robot explores an indoor environment in order to build a metric map. Most of the current exploration strategies used to select the next best locations to visit ignore prior knowledge about the environments to explore that, in some practical cases, could be available. In this paper, we present an exploration strategy that evaluates the amount of new areas that can be perceived from a location according to a priori knowledge about the structure of the indoor environment being explored, like the floor plan or the contour of external walls. Although this knowledge can be incomplete and inaccurate (e.g., a floor plan typically does not represent furniture and objects and consequently may not fully mirror the structure of the real environment), we experimentally show, both in simulation and with real robots, that employing prior knowledge improves the exploration performance in a wide range of settings.}
}
@article{ZHANG2020103565,
title = {Rapidly-exploring Random Trees multi-robot map exploration under optimization framework},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103565},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103565},
url = {https://www.sciencedirect.com/science/article/pii/S092188902030405X},
author = {Liwei Zhang and Zhibin Lin and Jie Wang and Bingwei He},
keywords = {Rapidly-exploring Randomized Trees, Multiple robots, Map exploration, Optimization framework},
abstract = {Rapidly-exploring Randomized Trees (RRT) is a kind of probabilistically complete exploration algorithm based on the tree structure. It has been widely used in the robotic navigation since it guarantees the complete discovery and the exploration of environment maps through robots. In the present study, the RRT algorithm is extended to propose an optimization-based map exploration strategy for multiple robots to actively explore and build environment maps. The present study adopts a market-based task allocation strategy, which to maximize the profit, for the coordination between robots. In the extension of the RRT, the cost function consists the unknown region and the passed unknown region. The unknown region is explored for a given frontier point, while the passed unknown region is the area, where the robot moves towards the target frontier point. When the robot moves from the start position to the target frontier point, the trajectory length is defined as a constraint for the optimization. The main contributions of the present study can be summarized in optimizing the frontier points, defining a new task allocation strategy and applying different evaluation rules, including the running time and the trajectory length. These rules are applied to explore the multi-robot map in simulated and practical environments. Then the Robot Operating System (ROS) is utilized to evaluate the application of the proposed exploration strategy on Turtlebots in a 270 m2 room. Obtained results from the simulation and the experiment demonstrate that the proposed method outperforms the Umari’s approach from both the running time and the trajectory length aspects.}
}
@article{SHI2020103570,
title = {Planning the trajectory of an autonomous wheel loader and tracking its trajectory via adaptive model predictive control},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103570},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103570},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304103},
author = {Junren Shi and Dongye Sun and Datong Qin and Minghui Hu and Yingzhe Kan and Ke Ma and Ruibo Chen},
keywords = {Automatic drive, Wheel loader, Trajectory planning, Trajectory tracking, Model predictive control},
abstract = {In a typical operation mode, a wheel loader frequently accelerates and decelerates, and the curvature of the driving path is inconsistent. In the past, autonomous vehicle trajectory planning has not considered the related changes in the velocity of the vehicle. Therefore, the trajectory tracking control process has seldom considered the impact of curving paths on the trajectory tracking performance. To address these problems, this study evaluated an autonomous wheel loader based on the trajectory of its non-uniform driving motion and constructed an adaptive model predictive control (AMPC) trajectory tracking system that considers disturbances in the path curvature. The trajectory of the autonomous wheel loader was then tracked using the proposed AMPC system with a planned non-uniform motion trajectory as the target. Its performance was then compared with that of a conventional model predictive control (MPC) trajectory tracking system that does not consider any path curvature disturbances. The maximum displacement error and heading error obtained by the proposed AMPC system were found to be 65.7% and 60%, respectively, smaller than those obtained by the MPC system. The desired trajectory can also be tracked well under different curvature amplitudes using the AMPC trajectory tracking system, ensuring active safety performance of an autonomous wheel loader in the process of trajectory tracking.}
}
@article{2022104054,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {150},
pages = {104054},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(22)00024-0},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022000240}
}
@article{PETROVIC2020103618,
title = {Cross-entropy based stochastic optimization of robot trajectories using heteroscedastic continuous-time Gaussian processes},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103618},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103618},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304589},
author = {Luka Petrović and Juraj Peršić and Marija Seder and Ivan Marković},
keywords = {Robot motion planning, Trajectory optimization, Continuous-time Gaussian processes, Stochastic optimization, Cluttered environments},
abstract = {High dimensional robot motion planning has recently been approached with trajectory optimization methods that efficiently minimize a suitable objective function in order to generate robot trajectories that are both optimal and feasible. However, finding a globally optimal solution is often an insurmountable problem in practice and state-of-the-art trajectory optimization methods are thus prone to local minima, mainly in cluttered environments. In this paper, we propose a novel trajectory planning algorithm that employs stochastic optimization in order to find a collision-free trajectory generated from a continuous-time Gaussian process (GP). The contributions of the proposed motion planning method stem from introducing the heteroscedasticity of the GP, together with exploited sparsity for efficient covariance estimation, and a cross-entropy based stochastic optimization for importance sampling based trajectory optimization. We evaluate the proposed method on three simulated scenarios: a maze benchmark, a 7DOF robot arm planning benchmark and a 10DOF mobile manipulator trajectory planning example and compare it to a state-of-the-art GP trajectory optimization method, namely the Gaussian process motion planner 2 algorithm (GPMP2). Our results demonstrate the following: (i) the proposed method yields a more thorough exploration of the solution space in complex environments than GPMP2, while having comparable execution time, (ii) the introduced heteroscedasticity generates GP priors better suited for collision avoidance and (iii) the proposed method has the ability to efficiently tackle high-dimensional trajectory planning problems.}
}
@article{YAN2020103594,
title = {Fixed-Wing UAVs flocking in continuous spaces: A deep reinforcement learning approach},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103594},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103594},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304346},
author = {Chao Yan and Xiaojia Xiang and Chang Wang},
keywords = {Fixed-wing UAV, Flocking, Reinforcement learning, Actor–critic},
abstract = {Fixed-Wing UAVs (Unmanned Aerial Vehicles) flocking is still a challenging problem due to the kinematics complexity and environmental dynamics. In this paper, we solve the leader–followers flocking problem using a novel deep reinforcement learning algorithm that can generate roll angle and velocity commands by training an end-to-end controller in continuous state and action spaces. Specifically, we choose CACLA (Continuous Actor–Critic Learning Automation) as the base algorithm and we use the multi-layer perceptron to represent both the actor and the critic. Besides, we further improve the learning efficiency by using the experience replay technique that stores the training data in the experience memory and samples from the memory as needed. We have compared the performance of the proposed CACER (Continuous Actor–Critic with Experience Replay) algorithm with benchmark algorithms such as DDPG and double DQN in numerical simulation, and we have demonstrated the performance of the learned optimal policy in semi-physical simulation without any parameter tuning.}
}
@article{MITTAL2020103646,
title = {Rapid path planning for Dubins vehicles under environmental currents},
journal = {Robotics and Autonomous Systems},
volume = {134},
pages = {103646},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103646},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304863},
author = {Khushboo Mittal and Junnan Song and Shalabh Gupta and Thomas A. Wettergren},
keywords = {Dubins paths, Path planning, Environmental currents, Curvature-constrained vehicles},
abstract = {This paper presents a rapid (real time) solution to the minimum-time path planning problem for Dubins vehicles under environmental currents (wind or ocean currents). Real-time solutions are essential in time-critical situations (such as replanning under dynamically changing environments or tracking fast moving targets). Typically, Dubins problem requires to solve for six path types; however, due to the presence of currents, four of these path types require to solve the root-finding problem involving transcendental functions. Thus, the existing methods result in high computation times and their applicability for real-time applications is limited. In this regard, in order to obtain a real-time solution, this paper proposes a novel approach where only a subset of two Dubins path types (LSL and RSR) are used which have direct analytical solutions in the presence of currents. However, these two path types do not provide full reachability. We show that by extending the feasible range of circular arcs in the LSL and RSR path types from 2π to 4π: (1) full reachability of any goal pose is guaranteed, and (2) paths with lower time costs as compared to the corresponding 2π-arc paths can be produced. Theoretical properties are rigorously established, supported by several examples, and evaluated in comparison to the Dubins solutions by extensive Monte-Carlo simulations.}
}
@article{AKKAWUTVANICH2020103663,
title = {Adaptive parallel reflex- and decoupled CPG-based control for complex bipedal locomotion},
journal = {Robotics and Autonomous Systems},
volume = {134},
pages = {103663},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103663},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305030},
author = {Chaicharn Akkawutvanich and Frederik Ibsgaard Knudsen and Anders Falk Riis and Jørgen Christian Larsen and Poramate Manoonpong},
keywords = {Bipedal walking robots, Central pattern generator, Adaptive online learning, Asymmetric gait},
abstract = {The achievement of adaptive, stable, and robust locomotion and dealing with asymmetrical conditions for bipedal robots remain a challenging problem. To address the problem, this paper introduces adaptive parallel reflex- and decoupled central pattern generator (CPG)-based control for a planar bipedal robot. The control has modular structure consisting of two parallel modules that work together. Firstly, as the main controller, the reflex-based control module inspired by an agonist–antagonist model, utilizes proprioceptive sensory feedback to adaptively generate various stable gaits. In parallel, as an auxiliary controller, the decoupled CPG-based control units individually governing the robot legs have the ability to learn the generated gaits in an online manner. Using the proposed framework, our study shows that this real-time control approach contributes to stable gait generation with robustness toward sensory feedback malfunction and adaptability to deal with environmental and morphological changes. Herein this study, we demonstrate the planar bipedal robot control functionality on a variable speed treadmill, dealing with asymmetric conditions such as weight imbalance and asymmetrical elastic resistance in the legs. However, the approach does not require robot kinematic and dynamic models as well as an environmental model and is therefore flexible. As such, it can be used as a basis for controlling other bipedal locomotion systems, like lower-limb exoskeletons.}
}
@article{TIECK2020103566,
title = {A spiking network classifies human sEMG signals and triggers finger reflexes on a robotic hand},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103566},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103566},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304061},
author = {J. Camilo Vasquez Tieck and Sandro Weber and Terrence C. Stewart and Jacques Kaiser and Arne Roennau and Rüdiger Dillmann},
keywords = {Neurorobotics, Human–robot-interaction, Neural control system, Humanoid robot, Motion representation, sEMG classification, Spiking neural networks, Anthropomorphic robot hand},
abstract = {The interaction between robots and humans is of great relevance for the field of neurorobotics as it can provide insights on how humans perform motor control and sensor processing and on how it can be applied to robotics. We propose a spiking neural network (SNN) to trigger finger motion reflexes on a robotic hand based on human surface Electromyography (sEMG) data. The first part of the network takes sEMG signals to measure muscle activity, then classify the data to detect which finger is being flexed in the human hand. The second part triggers single finger reflexes on the robot using the classification output. The finger reflexes are modeled with motion primitives activated with an oscillator and mapped to the robot kinematic. We evaluated the SNN by having users wear a non-invasive sEMG sensor, record a training dataset, and then flex different fingers, one at a time. The muscle activity was recorded using a Myo sensor with eight different channels. The sEMG signals were successfully encoded into spikes as input for the SNN. The classification could detect the active finger and trigger the motion generation of finger reflexes. The SNN was able to control a real Schunk SVH 5-finger robotic hand online. Being able to map myo-electric activity to functions of motor control for a task, can provide an interesting interface for robotic applications, and a platform to study brain functioning. SNN provide a challenging but interesting framework to interact with human data. In future work the approach will be extended to control also a robot arm at the same time.}
}
@article{KIM2020103517,
title = {Optimal communication relay positioning in mobile multi-node networks},
journal = {Robotics and Autonomous Systems},
volume = {129},
pages = {103517},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103517},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019309145},
author = {Jongyun Kim and Pawel Ladosz and Hyondong Oh},
keywords = {Communication relay, Gaussian process regression, Indoor communication model, Wireless mesh network},
abstract = {This paper presents an optimal communication relay positioning method to improve the communication performance of mobile multi-node networks in complex environments. The communication channel quality prediction between nodes is of primary concern to find the optimal relay node positions while considering the uncertain and dynamic nature of environments. To this end, the learning-based or the distance model-based method is used for the channel prediction depending on the mobility of the communication nodes of interest. The global message connectivity and the worst case connectivity are introduced as the communication performance metric of networked agents. The optimal relay positions are found by maximizing the performance with respect to the relay positions through a heuristic optimization technique. This algorithm outperforms a recently-developed relay positioning algorithm in the simulations. The indoor experiments are conducted to show that the proposed approach using mobile relays improves the communication performance of the complex network significantly with the accurate channel prediction.}
}
@article{ZHAO2020103550,
title = {Grasp prediction and evaluation of multi-fingered dexterous hands using deep learning},
journal = {Robotics and Autonomous Systems},
volume = {129},
pages = {103550},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103550},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019309947},
author = {Zengzhi Zhao and Weiwei Shang and Haoyuan He and Zhijun Li},
keywords = {Grasp prediction, Grasp evaluation, Multi-fingered dexterous hands, Convolutional neural networks, Mixture density networks},
abstract = {Learning from human skills has become one of the popular inspirations in grasp prediction and evaluation, but lack of effective methods on groups of grasp points for multi-fingered dexterous hands yields an open challenge. When facing an object, humans firstly predict a variety of options for grasps, which can be concerned as a complex multi-valued problem. After prediction, humans evaluate grasps and then choose the optimal one. Inspired by human skills, we propose Grasp Prediction Networks (GPNs) based on Convolutional Neural Networks (CNNs) and Mixture Density Networks (MDNs). The proposed GPNs map from a depth image to a set of parameters for Gaussian Mixture Model (GMM), from which candidate groups of grasp points can be sampled for prediction. Besides, we also propose Grasp Evaluation Networks (GENs) to evaluate candidate groups and then choose the optimal group of grasp points. The proposed GENs consider force-closure metric as grasp quality for evaluation. Different from other related work, our method (1) utilizes a probabilistic model to predict multiple groups of grasp points from a monocular depth image and (2) evaluates grasp quality with force-closure metric given a monocular depth image and a group of grasp points. Furthermore, we built a grasp dataset which consists of depth images, groups of grasp points and each group’s grasp quality. Herein, three different experiments were designed to validate our approach. The first one was a comparative experiment and revealed that GPNs show equivalent performance as GraspIt! in terms of high-quality grasp planning. The second one was also a comparative experiment, which validated that GENs can evaluate grasps as precisely as GraspIt!. Moreover, the last one was an actual experiment implemented on Shadow Hand Lite, and experimental results indicated that our approach achieved finely grasp of novel objects.}
}
@article{ALI2020103610,
title = {FinnForest dataset: A forest landscape for visual SLAM},
journal = {Robotics and Autonomous Systems},
volume = {132},
pages = {103610},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103610},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304504},
author = {Ihtisham Ali and Ahmed Durmush and Olli Suominen and Jari Yli-Hietanen and Sari Peltonen and Jussi Collin and Atanas Gotchev},
keywords = {Forest, Dataset, SLAM, Visual odometry, Navigation, Localization, Mapping, Stereo, Autonomous driving, Mobile robotics, Field robotics, Computer vision},
abstract = {This paper presents a novel challenging dataset that offers a new landscape of testing material for mobile robotics, autonomous driving research, and forestry operation. In contrast to common urban structures, we explore an unregulated natural environment to exemplify sub-urban and forest environment. The sequences provide two-natured data where each place is visited in summer and winter conditions. The vehicle used for recording is equipped with a sensor rig that constitutes four RGB cameras, an Inertial Measurement Unit, and a Global Navigation Satellite System receiver. The sensors are synchronized based on non-drifting timestamps. The dataset provides trajectories of varying complexity both for the state of the art visual odometry approaches and visual simultaneous localization and mapping algorithms. The full dataset and toolkits are available for download at: http://urn.fi/urn:nbn:fi:att:9b8157a7-1e0f-47c2-bd4e-a19a7e952c0d. As an alternative, you can browse for the dataset using the article title at: http://etsin.fairdata.fi.}
}
@article{GHAFOORI2020103650,
title = {Modeling and experimental analysis of a multi-rod parallel continuum robot using the Cosserat theory},
journal = {Robotics and Autonomous Systems},
volume = {134},
pages = {103650},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103650},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304905},
author = {Morteza Ghafoori and Ali {Keymasi Khalaji}},
keywords = {Cosserat theory, Rod, Modeling, Parallel robot, Continuum robot, Elasticity},
abstract = {Parallel continuum robots get their compliance and compactness from continuum rods while also having the stability and strength of parallel robots. Their potential to provide multi-degree-of-freedom articulation gives them versatility and makes them very useful in various applications. The purpose of this paper is to model a six link parallel continuum robot using the Cosserat theory. The single rod model is initially derived and experimentally verified with an average error of 12%. Then, the parallel continuum robot model is obtained by combining six elastic links, and eventually, some experiments are done on a real system to analyze the results of the obtained model.}
}