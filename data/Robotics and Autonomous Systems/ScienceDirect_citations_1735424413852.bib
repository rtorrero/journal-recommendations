@article{TSURUMINE2022104264,
title = {Goal-aware generative adversarial imitation learning from imperfect demonstration for robotic cloth manipulation},
journal = {Robotics and Autonomous Systems},
volume = {158},
pages = {104264},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104264},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001543},
author = {Yoshihisa Tsurumine and Takamitsu Matsubara},
keywords = {Generative adversarial imitation learning, Robotic cloth manipulation, Deep reinforcement learning},
abstract = {Generative Adversarial Imitation Learning (GAIL) can learn policies without explicitly defining the reward function from demonstrations. GAIL has the potential to learn policies with high-dimensional observations as input, e.g., images. By applying GAIL to a real robot, perhaps robot policies can be obtained for daily activities like washing, folding clothes, cooking, and cleaning. However, human demonstration data are often imperfect due to mistakes, which degrade the performance of the resulting policies. We address this issue by focusing on the following features: (1) many robotic tasks are goal-reaching tasks, and (2) labeling such goal states in demonstration data is relatively easy. With these in mind, this paper proposes Goal-Aware Generative Adversarial Imitation Learning (GA-GAIL), which trains a policy by introducing a second discriminator to distinguish the goal state in parallel with the first discriminator that indicates the demonstration data. This extends a standard GAIL framework to more robustly learn desirable policies even from imperfect demonstrations through a goal-state discriminator that promotes achieving the goal state. Furthermore, GA-GAIL employs the Entropy-maximizing Deep P-Network (EDPN) as a generator, which considers both the smoothness and causal entropy in the policy update, to achieve stable policy learning from two discriminators. Our proposed method was successfully applied to two real-robotic cloth-manipulation tasks: turning a handkerchief over and folding clothes. We confirmed that it learns cloth-manipulation policies without task-specific reward function design. Video of the real experiments are available at this URL.}
}
@article{JIMENEZ2023104363,
title = {Object-wise comparison of LiDAR occupancy grid scan rendering methods},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104363},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104363},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000027},
author = {Víctor Jiménez and Jorge Godoy and Antonio Artuñedo and Jorge Villagra},
keywords = {Occupancy grid, Evaluation method, LiDAR, Perception, Autonomous vehicles},
abstract = {Computing Occupancy grids with LiDAR data, is a popular strategy for environment representation. In the last two decades, several authors have proposed different methods to render the sensed information into the grids, seeking to obtain computational efficiency or accurate environment modeling. However, no comparison regarding their performance under object detection in autonomous driving applications has been found in the literature. As a result, this work compares six representative LiDAR scan rendering strategies in a quantitative manner. To that end, a novel quantitative evaluation framework for occupancy grids is proposed. It addresses the two main steps of object detection: object segmentation and features estimation, proposing a meaningful procedure, repeatable with other OG approaches. The code of this evaluation framework is available in https://git-autopia.car.upm-csic.es/open_source/occupancy_grid_object_detection_evaluation.git.}
}
@article{LIU2023104289,
title = {Balanced task allocation and collision-free scheduling of multi-robot systems in large spacecraft structure manufacturing},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104289},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104289},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001786},
author = {Shaorui Liu and Jianxin Shen and Wei Tian and Jiamei Lin and Pengcheng Li and Bo Li},
keywords = {Multi-robot manufacturing, Workload balancing, Collision-free scheduling},
abstract = {The use of multiple cooperating industrial robots provides efficient and flexible solutions to the manufacturing of complex aerospace structures. Such applications require the workloads to be sufficiently shared between neighboring robots, this entails the collision-free scheduling of many discrete tasks, where precedence orders need to be assigned for specific tasks. In this paper, we first present a two-step task allocation method that handles workload balancing, then a scheduling algorithm combining construction heuristic with iterated local search to provide efficient schedules. Our key innovation is a collision model that encodes precedence constraints and a fast heuristic that constructs collision-free schedule under given constraints, the optimization of the schedule is then addressed by an iterated local search. The advantage in terms of minimizing makespan under different problem scales and conditions is validated by computational experiments. Finally, the use of our method is demonstrated by a physical multi-robot system.}
}
@article{LUPERTO2023104346,
title = {Seeking at-home long-term autonomy of assistive mobile robots through the integration with an IoT-based monitoring system},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104346},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104346},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002354},
author = {Matteo Luperto and Javier Monroy and Francisco-Angel Moreno and Francesca Lunardini and Jennifer Renoux and Andrej Krpic and Cipriano Galindo and Simona Ferrante and Nicola Basilico and Javier Gonzalez-Jimenez and N. Alberto Borghese},
keywords = {Socially assistive robots, IoT, Internet of robotic things},
abstract = {In this paper, we propose a system that stems from the integration of an autonomous mobile robot with an IoT-based monitoring system to provide monitoring, assistance, and stimulation to older adults living alone in their own houses. The creation of an Internet of Robotics Things (IoRT) based on the interplay between pervasive smart objects and autonomous robotic systems is claimed to enable the creation of innovative services conceived for assisting the final user, especially in elderly care. The synergy between IoT and a Socially Assistive Robot (SAR) was conceived to offer robustness, reconfiguration, heterogeneity, and scalability, by bringing a strong added value to both the current SAR and IoT technologies. First, we propose a method to achieve the synergy and integration between the IoT system and the robot; then, we show how our method increases the performance and effectiveness of both to provide long-term support to the older adults. To do so, we present a case-study, where we focus on the detection of signs of the frailty syndrome, a set of vulnerabilities typically conveyed by a cognitive and physical decline in older people that concur in amplifying the risks of major diseases hindering the capabilities of independent living. Experimental evaluation is performed in both controlled settings and in a long-term real-world pilot study with 9 older adults in their own apartments, where the system was deployed autonomously for, on average, 12 weeks.}
}
@article{CORRADINI2023104322,
title = {A BPMN-driven framework for Multi-Robot System development},
journal = {Robotics and Autonomous Systems},
volume = {160},
pages = {104322},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104322},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002111},
author = {Flavio Corradini and Sara Pettinari and Barbara Re and Lorenzo Rossi and Francesco Tiezzi},
keywords = {Multi-robot systems, BPMN, Model-driven development, Enactment framework},
abstract = {Programming robotic systems is often a challenging task requiring advanced skills, especially when the goal is to ensure loosely-coupled coordination in heterogeneous Multi-Robot Systems (MRSs). Model-driven approaches for robotic system engineering have shown their benefits in facilitating the development of robots’ behavior, controllers, and system components. However, the state of the art still lacks contributions addressing crucial aspects of the model-driven approach applied to MRSs, such as developing robots’ distributed cooperation through models supporting the communication among robots. In this paper, we present a novel framework for modeling, configuring and enacting the cooperative behaviors of MRSs through collaboration diagrams as provided by the BPMN 2.0 standard. The advantages of our solution lie, indeed, in the use of BPMN, which provides easily understandable and highly expressive diagrams for representing the cooperation among distributed robots, and benefits from a wide list of supporting tools. Starting from the selection of BPMNelements, we define a set of guidelines for driving the developer in modeling an MRS mission using BPMN. The developer configures the resulting collaboration diagram to link elements in the model to the robotic middleware, ROS2 in the toolchain we implemented. Finally, the configured model is enacted by BPMN engines integrated into the ROS2 middleware run by each robot involved in the MRS, thus obtaining a fully distributed cooperation. We assess our framework’s effectiveness through experiments in simulated and real environments.}
}
@article{SARKHEL2023104293,
title = {Robust deflection control and analysis of a fishing rod-type flexible robotic manipulator for collaborative robotics},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104293},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104293},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001828},
author = {Prasenjit Sarkhel and Mithilesh K. Dikshit and Vimal Kumar Pathak and Kuldeep K. Saxena and C. Prakash and Dharam Buddhi},
keywords = {Flexible rod, Manipulator, Sim-mechanics, Deflection, PID controller},
abstract = {Due to high-speed operation at low inertia, flexible manipulators are becoming more and more popular in today’s world. These manipulators produce excessive vibration, which must be reduced using an efficient control technique for the manipulator to function well. In the present study, a flexible manipulator model with a single link is built in such a manner that it can be considered as a flexible fishing rod. The free end of the flexible rod has a provision for applying a payload, which serves to give a deflection of the flexible rod. A lumped parameter method is adopted for modelling the flexible rod as well as the string using the Sim-Mechanics tool in MATLAB. Simulations were carried out for sudden and sinusoidal loading. It has been found that the flexible link produces excessive vibration under both sudden and sinusoidal loading. A proportional integral derivative (PID) controller is used to suppress the excessive vibration generated in the simulation model. Four different locations (Location 1: 15 cm; Location 2: 30 cm; Location 3: 45 cm; and Location 4: 60 cm) are selected for controller positioning. Simulation revealed that the minimum deflection was observed at location 4, i.e., at the tip for both sudden and sinusoidal loading. The developed model is validated using two loading conditions, viz., the beam’s self-weight and a point load of 30 N at the free end. It has been found that the simulation results resemble the analytical results with an error of 0.44% and 0.36% for both the loading conditions.}
}
@article{ZHAO2023104321,
title = {Skill generalization of tubular object manipulation with tactile sensing and Sim2Real learning},
journal = {Robotics and Autonomous Systems},
volume = {160},
pages = {104321},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104321},
url = {https://www.sciencedirect.com/science/article/pii/S092188902200210X},
author = {Yongqiang Zhao and Xingshuo Jing and Kun Qian and Daniel Fernandes Gomes and Shan Luo},
keywords = {Skill generalization, Sim-to-real learning, Tactile sensing, Reinforcement learning, Unsupervised domain adaptation, Robot manipulation},
abstract = {Tubular objects such as test tubes are common in chemistry and life sciences research laboratories, and robots that can handle them have the potential to accelerate experiments. Moreover, it is expected to train a robot to manipulate tubular objects in a simulator and then deploy it in a real-world environment. However, it is still challenging for a robot to learn to handle tubular objects through single sensing and bridge the gap between simulation and reality. In this paper, we propose a novel tactile–motor policy learning method to generalize tubular object manipulation skills from simulation to reality. In particular, we propose a Sim-to-Real transferable in-hand pose estimation network that generalizes to unseen tubular objects. The network utilizes a novel adversarial domain adaptation network to narrow the pixel-level domain gap for tactile tasks by introducing the attention mechanism and a task-related constraint. The in-hand pose estimation network is further implemented in a Reinforcement Learning-based policy learning framework for robotic insert-and-pullout manipulation tasks. The proposed method is applied to a human–robot collaborative tube placing scenario and a robotic pipetting scenario. The experimental results demonstrate the generalization capability of the learned tactile–motor policy toward tubular object manipulation in research laboratories.}
}
@article{FANG2023104281,
title = {Consistency optimal coordination control of underground heavy-load robot in nonstructural environment},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104281},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104281},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001701},
author = {Lixia Fang and Tong Wang and Weixiong Zheng and Zhigang Liu and Liu Ming and Xiaowen Zheng and Miao Wu},
keywords = {Underground heavy-load robot, Multi-body coupled system, Non-structural environment, Dynamic stability, Consistency optimal coordination control},
abstract = {In order to guarantee the dynamic stability of robots in nonstructural environment, this paper proposes a new consistency optimal coordination (COC) control strategy. Firstly, by analyzing the topological structure, motion correlation and control coupling relationship among subsystems, the general expression of multi-body coupling system (MCS) for underground heavy-load robot is realized. Then, the transient spatial output deviations are proposed as the characterization of dynamic stability for the underground heavy-load robot. Afterwards, in order to reduce the strong coupling relationship among subsystems, the underground heavy-load robot is decoupled into fixed-point and non-fixed-point operation modes, the Nyquist stability and Lyapunov stability are applied to discriminate the dynamic stability of two working modes respectively. Finally, based on the idea of minimum loop gain compensation of mechanism, the COC control strategy is proposed, which can coordinate the subsystems to be consistent, so as to realize the dynamic stability of overall system in nonstructural environment. Both the experimental and situational results verify that the COC control strategy proposed in this paper not only realizes stable output of robot, but also effectively reduces the lag caused by instability. The work of this paper improves the dynamic stability analysis theory for robots, and promotes adaptability and dynamic response capability of moving robots as well.}
}
@article{AHN2023104399,
title = {Robotic assembly strategy via reinforcement learning based on force and visual information},
journal = {Robotics and Autonomous Systems},
volume = {164},
pages = {104399},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104399},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000386},
author = {Kuk-Hyun Ahn and Minwoo Na and Jae-Bok Song},
keywords = {Robotic assembly, Learning-based method},
abstract = {Since assembly tasks are frequently performed in a wide range of industries, there have been many efforts to develop robotic assembly strategies. However, robotic assembly is only applicable in structured environments wherein a target object is placed in a fixed position, because the occurrence of a large error substantially degrades performance. Thus, there is still a need for a generalized assembly strategy that can cope with a large position/orientation error regardless of the shape. To this end, this study presents an assembly strategy based on both the force and visual information. Specifically, the trajectory of the robot is obtained by combining the output of two neural-network-based trajectory generators that receive the force and image information, respectively, and then a deep reinforcement learning algorithm is applied to obtain the optimal strategy. In this process, imitation learning is applied to train the force-based network using the demonstration data collected with the suggested hand-guiding method, and the probability distribution of the feature is introduced in the image-based network to enable a robot to quickly adapt to assembly parts with different shapes. The performance of the proposed assembly strategy is experimentally verified using various peg-in-hole tasks, and the results confirm that the robot can successfully accomplish an assembly task regardless of the shapes of the assembly parts, even when the initial position/orientation error is large.}
}
@article{YUAN2023104305,
title = {Improvement of Strong Tracking UKF-SLAM Approach using Three-position Ultrasonic Detection},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104305},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104305},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001944},
author = {Shuai Yuan and Jian Wu and Fangjun Luan and Lili Zhang and Jiaqi Lv},
keywords = {Ultrasonic detection, Strong tracking UKF, Simultaneous Localization and Mapping (SLAM), Omni-Wheel mobile robot},
abstract = {As for the uncertainty problem of detection and estimation of robot localization and mapping using ultrasonic sensor, this paper proposes the improvement of Strong Tracking UKF-SLAM approach using three-position ultrasonic detection. A three-position ultrasonic detection model is first of all built for reducing these uncertainties through topological relationship screening and environmental contour estimation. Then Strong Tracking UKF-SLAM approach is improved by using multiple fade factors to fuse the ultrasonic measurement data and motion model information of robot for obtaining more accurate localization and mapping. Finally, we construct simulation and indoor experimental environments and design the mobile robot system with ultrasonic sensor for verification. The simulation represents that the improved algorithm has less error and more accurate effect than original algorithms in localization and mapping of mobile robot. The indoor environmental experiment is performed for illustrating the feasibility and effectiveness of the proposed method. The proposed method has certain reference value for research of Simultaneous Localization and Mapping.}
}
@article{LIU2022104246,
title = {Performance analysis and trajectory planning of multi-locomotion mode ankle rehabilitation robot},
journal = {Robotics and Autonomous Systems},
volume = {157},
pages = {104246},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104246},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001427},
author = {Ya Liu and Wenjuan Lu and Huafang Wu and Yici Xia and Bo Hu and Daxing Zeng},
keywords = {Ankle rehabilitation robot, Multi-locomotion mode, Parallel mechanism, Performance analysis, Trajectory planning},
abstract = {A multi-locomotion mode ankle rehabilitation robot (MLMARR) based on the 2-UPU/RPU (U: universal; P: prismatic; R: revolute) parallel mechanism with actuators above the end effector is proposed. In addition to the rehabilitation training of the basic motion orientation of the ankle, the MLMARR enables up/down or back/forth traction rehabilitation training, ensuring the training of muscle groups and ligaments related to the ankle motion. First, degrees-of-freedom analysis is conducted based on the screw theory. Subsequently, using the closed-loop vector method and coordinate system rotation transformation, inverse position analysis is performed and the Jacobian matrix is described. In addition, three types of kinematic singularities are identified by analyzing the Jacobian matrix. Moreover, the workspace is determined by the limit boundary method. Three rehabilitation training modes are set and dynamic simulations are performed according to the ankle rehabilitation requirements; on this basis, the linear actuators can be selected reasonably. Finally, the effectiveness and accuracy of rehabilitation training are evaluated based on experimental data obtained using an MLMARR prototype. This research reveals the characteristics and superiority of the proposed MLMARR and offers the basis for the future improvement of the device.}
}
@article{MAZHITOV2023104311,
title = {Human–robot handover with prior-to-pass soft/rigid object classification via tactile glove},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104311},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104311},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002007},
author = {Ayan Mazhitov and Togzhan Syrymova and Zhanat Kappassov and Matteo Rubagotti},
keywords = {Tactile sensors, Human–robot handover, Human–robot interaction, Deep learning},
abstract = {Human–robot handovers constitute a challenging and fundamental aspect of physical human–robot interaction. This paper describes the design and implementation of a human–robot handover pipeline in the case in which both soft and rigid objects are passed by the human to the robot. These objects require different profiles of grasping torques by the robot hand fingers, so as to avoid damaging them. As a viable solution to this problem, a tactile glove worn by the human is used to provide real-time information to a deep neural network, which classifies each object as soft or rigid in the pre-handover phase: this information is passed to the robot, which applies the grasping torque profile suitable for the specific type of object. The proposed method is designed and validated based on experiments with eight human participants and 24 objects. The outcomes of these experiments regarding classification accuracy, force and torque profiles, and evaluation of the subjective experiences via questionnaires, are described and discussed.}
}
@article{HERNANDEZ2023104290,
title = {Exploiting the confusions of semantic places to improve service robotic tasks in indoor environments},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104290},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104290},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001798},
author = {Alejandra C. Hernandez and Clara Gomez and Ramon Barber and Oscar Martinez Mozos},
keywords = {Semantic understanding, Service robots},
abstract = {A significant challenge in service robots is the semantic understanding of their surrounding areas. Traditional approaches addressed this problem by segmenting the environment into regions corresponding to full rooms that are assigned labels consistent with human perception, e.g. office or kitchen. However, different areas inside the same room can be used in different ways: Could the table and the chair in my kitchen become my office ? What is the category of that area now? office or kitchen? To adapt to these circumstances we propose a new paradigm where we intentionally relax the resulting labeling of place classifiers by allowing confusions, and by avoiding further filtering leading to clean full room classifications. Our hypothesis is that confusions can be beneficial to a service robot and, therefore, they can be kept and better exploited. Our approach creates a subdivision of the environment into different regions by maintaining the confusions which are due to the scene appearance or to the distribution of objects. In this paper, we present a proof of concept implemented in simulated and real scenarios, that improves efficiency in the robotic task of searching for objects by exploiting the confusions in place classifications.}
}
@article{GIL2023104386,
title = {Mission specification and decomposition for multi-robot systems},
journal = {Robotics and Autonomous Systems},
volume = {163},
pages = {104386},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104386},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000258},
author = {Eric Bernd Gil and Genaína Nunes Rodrigues and Patrizio Pelliccione and Radu Calinescu},
keywords = {Multi-robot systems, Mission specification, Hierarchical planning, Modeling, Mission decomposition},
abstract = {Service robots are increasingly being used to perform missions comprising dangerous or tedious tasks previously executed by humans. However, their users—who know the environment and requirements for these missions—have limited or no robotics experience. As such, they often find the process of allocating concrete tasks to each robot within a multi-robot system (MRS) very challenging. Our paper introduces a framework for Multi-Robot mission Specification and decomposition (MutRoSe) that simplifies and automates key activities of this process. To that end, MutRoSe allows an MRS mission designer to define all relevant aspects of a mission and its environment in a high-level specification language that accounts for the variability of real-world scenarios, the dependencies between task instances, and the reusability of task libraries. Additionally, MutRoSe automates the decomposition of MRS missions defined in this language into task instances, which can then be allocated to specific robots for execution—with all task dependencies appropriately taken into account. We illustrate the application of MutRoSe and show its effectiveness for four missions taken from a recently published repository of MRS applications.}
}
@article{GRILLO2022104266,
title = {Trust as a metric for auction-based task assignment in a cooperative team of robots with heterogeneous capabilities},
journal = {Robotics and Autonomous Systems},
volume = {157},
pages = {104266},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104266},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001555},
author = {Alberto Grillo and Stefano Carpin and Carmine Tommaso Recchiuto and Antonio Sgorbissa},
keywords = {Trust, Robot–robot interaction, Task assignment, Social robotics},
abstract = {As robots become part of our everyday lives, they may be required to cooperate without being aware of each other’s capabilities (e.g., because different teams have developed them), and therefore will have to trust each other to work together safely and efficiently. Starting from this premise, this work identifies trust as an essential metric to assign tasks to robots using auction-based mechanisms. We model trust by taking inspiration from popular models in the literature and adapting them to an open environment in which heterogeneous robots may dynamically enter or exit, execute assigned tasks, or verify the correct execution of tasks by other robots. Robots are considered to be heterogeneous in the sense that they may have different capabilities in executing and verifying the execution of actions. In the proposed model, “doing an action” and “verifying the execution of an action” are distinct, not necessarily overlapping, capabilities. Some robots may be able to do an action, whereas some robots may not be able to do it but only to observe and judge the ability of other robots to do it. After introducing the relevant formalism, the article describes the system’s architecture implemented in ROS and multiple experiments performed in simulation and with real robots (one NAO and two Pepper robots by SoftBank Robotics), providing a proof-of-concept for broader utilization of the system in cooperative robotic scenarios.}
}
@article{ALBORE2023104318,
title = {Skill-based design of dependable robotic architectures},
journal = {Robotics and Autonomous Systems},
volume = {160},
pages = {104318},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104318},
url = {https://www.sciencedirect.com/science/article/pii/S092188902200207X},
author = {Alexandre Albore and David Doose and Christophe Grand and Jérémie Guiochet and Charles Lesire and Augustin Manecy},
keywords = {Skill-based architecture, Dependability, Development process, Model-checking, Fault-Tree Analysis},
abstract = {Software architectures for autonomous systems are generally structured with 3 layers: a decisional layer managing autonomous reasoning, a functional layer managing reactive tasks and processing, and an executive layer bridging the gap between both. The executive layer plays a central role, as it links high-level tasks with low-level processing, and is generally responsible for the robustness or the fault-tolerance of the overall system. In this paper, we propose a development process for such an executive layer that emphasizes on the dependability of this layer. To do so, we structure the executive layer using skills, that are formally defined using a specific language, and we then provide some tools to verify these models, generate some code, and a methodology to assess the fault-tolerance of the resulting architecture.}
}
@article{LUO2022104278,
title = {A balanced jumping control algorithm for quadruped robots},
journal = {Robotics and Autonomous Systems},
volume = {158},
pages = {104278},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104278},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001671},
author = {Bende Luo and Yinlin Luo},
keywords = {Quadruped robot, Balanced jumping algorithm, Trajectory planning, Spatial mechanics decoupling, Soft landing},
abstract = {The jumping movement of quadruped robots is crucial, so a complete set of balanced jumping algorithms is proposed in this paper due to the flaws of the current jumping algorithms. The proposed algorithm includes trajectory planning of the Center of Mass(CoM) and four jumping phases, illustrating the jumping process in detail. Tasks that a quadruped robot with the height of 0.6 m jumps up a step with the height of 0.3 m, 0.4 m, 0.5 m and 0.6 m are the research object. Optimal Parabola Trajectory of CoM(OPTC) is solved by about ten iterations based on the fastest approaching strategy before jumping. During the first phase of jumping, Ground Reaction Forces(GRFs) are precisely distributed to control six Degrees of Freedom(DoFs) based on symmetric six-dimensional spatial mechanics decoupling solution, controlling the robot to adjust itself to the best ejecting posture. The maximum displacement error is less than 0.005 m. During the second phase, full-leg ejection is implemented to eject, guaranteeing that the robot accurately tracks the OPTC after takeoff by updating proportional virtual forces. The tracking Mean Square Error(MSE) is less than 0.06. During the third phase, the flying attitude is adjusted by swinging leg theory summarized in the paper, with the maximum pitch angle less than 4.5°. Meanwhile, the theoretical landing points of feet are calculated to lead the movement of feet, ensuring a soft landing to reduce touchdown impact. The momentary landing velocities of feet are less than 0.09 m/s. During the fourth phase, the robot buffers and brakes to a static state based on the algorithm used in the first phase. Eventually, the proposed algorithms are verified through simulating experiments on Webots physical engine, and the effectiveness and feasibility are validated by the experimental results.}
}
@article{FRASHERI2023104347,
title = {Addressing time discrepancy between digital and physical twins},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104347},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104347},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002366},
author = {Mirgita Frasheri and Henrik Ejersbo and Casper Thule and Cláudio Gomes and Jakob Levisen Kvistgaard and Peter Gorm Larsen and Lukas Esterle},
keywords = {Cyber–physical systems, Digital twins, Modelling, Time delays, Synchronisation},
abstract = {Digital twins (DTs) represent a key technology in the development, real-time monitoring and optimisation of cyber–physical systems (CPSs). Such potential emerges as a result of the real-time coupling between DTs and their physical counterparts, where it is possible to make use of operational data as it is being generated in order to aid decision-making. Harnessing this potential presents several design challenges, such as the parallel operation of the DT and its physical twin (PT), and the necessary synchronisation thereof, to ensure coherent execution of the system in ensemble. In this paper we present an approach that handles situations where a DT and its PT get out of sync as a result of disturbances in the normal operational conditions of the DT–PT system, e.g., due to network degradation or temporary network drop. The purpose is to provide a best-effort functionality covering: user notification, degradation of DT to digital shadow (DS), with recovery mechanisms to re-establish the synchronisation between DT and PT.}
}
@article{XUE2023104323,
title = {A robotic learning and generalization framework for curved surface based on modified DMP},
journal = {Robotics and Autonomous Systems},
volume = {160},
pages = {104323},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104323},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002123},
author = {Xianfa Xue and Jiale Dong and Zhenyu Lu and Ning Wang},
keywords = {Continuous drag demonstration, Modified DMP, Curve drawing experiments, Learning and generalization framework},
abstract = {How to reproduce and generalize the skills acquired by demonstrating is a hot topic for researchers. (1) A compliant continuous drag demonstration system based on discrete admittance model was designed to continuously and smoothly drag or demonstrate. (2) The modified DMP including the scaling factor and the force coupling term was used to improve the poor generalization ability of the classical DMP. (3) Curve drawing experiments were carried out to show the effectiveness of our proposed learning and generalization framework.}
}
@article{CLARO2023104398,
title = {ArTuga: A novel multimodal fiducial marker for aerial robotics},
journal = {Robotics and Autonomous Systems},
volume = {163},
pages = {104398},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104398},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000374},
author = {Rafael Marques Claro and Diogo Brandão Silva and Andry Maykol Pinto},
keywords = {Fiducial markers, Unmanned Aerial Vehicle, Precise landing, Multimodal data},
abstract = {For Vertical Take-Off and Landing Unmanned Aerial Vehicles (VTOL UAVs) to operate autonomously and effectively, it is mandatory to endow them with precise landing abilities. The UAV has to be able to detect the landing target and to perform the landing maneuver without compromising its own safety and the integrity of its surroundings. However, current UAVs do not present the required robustness and reliability for precise landing in highly demanding scenarios, particularly due to their inadequacy to perform accordingly under challenging lighting and weather conditions, including in day and night operations. This work proposes a multimodal fiducial marker, named ArTuga (Augmented Reality Tag for Unmanned vision-Guided Aircraft), capable of being detected by an heterogeneous perception system for accurate and precise landing in challenging environments and daylight conditions. This research combines photometric and radiometric information by proposing a real-time multimodal fusion technique that ensures a robust and reliable detection of the landing target in severe environments. Experimental results using a real multicopter UAV show that the system was able to detect the proposed marker in adverse conditions (such as at different heights, with intense sunlight and in dark environments). The obtained average accuracy for position estimation at 1 m height was of 0.0060 m with a standard deviation of 0.0003 m. Precise landing tests obtained an average deviation of 0.027 m from the proposed marker, with a standard deviation of 0.026 m. These results demonstrate the relevance of the proposed system for the precise landing in adverse conditions, such as in day and night operations with harsh weather conditions.}
}
@article{TIAN2023104351,
title = {Reinforcement learning under temporal logic constraints as a sequence modeling problem},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104351},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104351},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002408},
author = {Daiying Tian and Hao Fang and Qingkai Yang and Haoyong Yu and Wenyu Liang and Yan Wu},
keywords = {Temporal logic, Reinforcement learning, Trajectory transformer, Sparse attention},
abstract = {Reinforcement learning (RL) under temporal logic typically suffers from slow propagation for credit assignment. Inspired by recent advancements called trajectory transformer in machine learning, the reinforcement learning under Temporal Logic (TL) is modeled as a sequence modeling problem in this paper, where an agent utilizes the transformer to fit the optimal policy satisfying the Finite Linear Temporal Logic (LTLf) tasks. To combat the sparse reward issue, dense reward functions for LTLf are designed. For the sake of reducing the computational complexity, a sparse transformer with local and global attention is constructed to automatically conduct credit assignment, which removes the time-consuming value iteration process. The optimal action is found by the beam search performed in transformers. The proposed method generates a series of policies fitted by sparse transformers, which has sustainably high accuracy in fitting the demonstrations. At last, the effectiveness of the proposed method is demonstrated by simulations in Mini-Grid environments.}
}
@article{CHELLA2023104362,
title = {Quantum planning for swarm robotics},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104362},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104362},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000015},
author = {Antonio Chella and Salvatore Gaglio and Maria Mannone and Giovanni Pilato and Valeria Seidita and Filippo Vella and Salvatore Zammuto},
keywords = {Grover search, Quantum decision-making, Foraging-ant behavior},
abstract = {Computational resources of quantum computing can enhance robotic motion, decision making, and path planning. While the quantum paradigm is being applied to individual robots, its approach to swarms of simple and interacting robots remains largely unexplored. In this paper, we attempt to bridge the gap between swarm robotics and quantum computing, in the framework of a search and rescue mission. We focus on a decision-making and path-planning collective task. Thus, we present a quantum-based path-planning algorithm for a swarm of robots. Quantization enters position and reward information (measured as a robot’s proximity to the target) and path-planning decisions. Pairwise information-exchange is modeled through a logic gate, implemented with a quantum circuit. Path planning draws upon Grover’s search algorithm, implemented with another quantum circuit. Our case study involves a search and rescue scenario, inspired by ant-foraging behavior in nature, as an example of swarm intelligence. We show that our method outperforms two ant-behavior simulations, in NetLogo and Java, respectively, presenting a faster convergence to the target, represented here by the source of food. This study can shed light on future applications of quantum computing to swarm robotics.}
}
@article{EFFATI2023104306,
title = {Systematic solution for optimally energy-efficient turning radius for wheeled skid-steer rovers},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104306},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104306},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001956},
author = {Meysam Effati and Krzysztof Skonieczny},
keywords = {Energy-efficient paths, Skid-steer rovers, Optimization, KKT conditions},
abstract = {A skid-steer rover’s power consumption is highly dependent on the turning radius of its path, with a point turn consuming much more power compared to straight line motion. As energy is the integration of instantaneous power over time, a trade-off between arcs’ turning radii and lengths should be made to minimize energy consumption. Because of the skid-steer rovers’ ability to do point turns the simplest and shortest way to traverse a distance between two points is by doing a point turn-line-point turn (PLP) maneuver. However, we show that wheeled skid-steer rovers there are scenarios where optimal Circle–Line–Circle (CLC) paths consume less energy than PLP paths. Therefore, the goal in this work is to find the best path from among CLC paths; Karush–Kuhn–Tucker (KKT) conditions are used to systematically obtain the optimally energy-efficient answer for the CLC paths. It is assumed that the rovers move forward on hard flat ground. For solving the problem, a new practical constraint constant-vc is suggested. In this paper, comparing the KKT conditions and experimental results reveals that the lowest total energy consumption for CLC paths with or without considering constant-vc constraint is obtained by selecting turning radii equal to R′ (the half of slip-track).}
}
@article{LUPERTO2023104282,
title = {Mapping beyond what you can see: Predicting the layout of rooms behind closed doors},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104282},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104282},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001713},
author = {Matteo Luperto and Federico Amadelli and Moreno {Di Berardino} and Francesco Amigoni},
keywords = {Mapping, Map prediction, Doors},
abstract = {The availability of maps of indoor environments is often fundamental for autonomous mobile robots to efficiently operate in industrial, office, and domestic applications. When robots build such maps, some areas of interest could be inaccessible, for instance, due to closed doors. As a consequence, these areas are not represented in the maps, possibly causing limitations in robot localization and navigation. In this paper, we provide a method that completes 2D grid maps by adding the predicted layout of the rooms behind closed doors. The main idea of our approach is to exploit the underlying geometrical structure of indoor environments to estimate the shape of unobserved rooms. Results show that our method is accurate in completing maps also when large portions of environments cannot be accessed by the robot during map building. We experimentally validate the quality of the completed maps by using them to perform path planning tasks.}
}
@article{MOHAMAD2023104319,
title = {Online gait generator for lower limb exoskeleton robots: Suitable for level ground, slopes, stairs, and obstacle avoidance},
journal = {Robotics and Autonomous Systems},
volume = {160},
pages = {104319},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104319},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002081},
author = {Habib Mohamad and Sadjaad Ozgoli},
keywords = {Gait generator, Optimization, Wearable robots, Obstacle avoidance, Exoped®},
abstract = {The development of lower limb exoskeletons has seen significant interest in recent times. Two types of them are more used that cover two types of needs: gait rehabilitation and human locomotion assistance. An essential subject in controlling the latter kind is trajectory generation, in which there are still challenges. For online controlling of the exoskeleton, gait parameters must have the ability to change at any moment during walking, taking into account human intention and particular conditions. In this paper, an online gait generation method is provided that is suitable for different walking modes. For this purpose, three trajectory generator blocks are proposed. The first block is for the center of mass (CoM) in the double support phase, where the trajectory is generated to make the patient feel more comfortable. The second block is for the support leg in the single support phase. The trajectory is generated using the center of pressure (CoP) criterion to secure backward balance and reduce the forces applied to the arms. The last block is for the swing leg in the single support phase, where a cost function is proposed to minimize the torques of the motors. The performance analysis of the proposed trajectory generator blocks was evaluated, and walking patterns were examined via simulations. Finally, three experimental tests were implemented with a healthy subject wearing Exoped® exoskeleton on level-ground with an obstacle, and stairs.}
}
@article{MEHTA2023104332,
title = {UV Disinfection Robots: A Review},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104332},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104332},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002214},
author = {Ishaan Mehta and Hao-Ya Hsueh and Sharareh Taghipour and Wenbin Li and Sajad Saeedi},
keywords = {UV disinfection, Autonomous robots, UVC, COVID-19, Disinfection robots},
abstract = {The novel coronavirus (COVID-19) pandemic has completely changed our lives and how we interact with the world. The pandemic has brought about a pressing need to have effective disinfection practices that can be incorporated into daily life. They are needed to limit the spread of infections through surfaces and air, particularly in public settings. Most of the current methods utilize chemical disinfectants, which can be laborious and time-consuming. Ultraviolet (UV) irradiation is a proven and powerful means of disinfection. There has been a rising interest in the implementation of UV disinfection robots by various public institutions, such as hospitals, long-term care homes, airports, and shopping malls. The use of UV-based disinfection robots could make the disinfection process faster and more efficient. The objective of this review is to equip readers with the necessary background on UV disinfection and provide relevant discussion on various aspects of UV robots.}
}
@article{ARORA2023104287,
title = {Static map generation from 3D LiDAR point clouds exploiting ground segmentation},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104287},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104287},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001762},
author = {Mehul Arora and Louis Wiesmann and Xieyuanli Chen and Cyrill Stachniss},
keywords = {Map cleaning, Ground segmentation},
abstract = {A clean and reliable map of the environment is key for a variety of robotic tasks including localization, path planning, and navigation. Dynamic objects are an inherent part of our world, but their presence often deteriorates the performance of various mapping algorithms. This not only makes it important but necessary to remove these dynamic points from the map before they can be used for other tasks such as path planning. In this paper, we address the problem of building maps of the static aspects of the world by detecting and removing dynamic points from the source point clouds. We target a map cleaning approach that removes the dynamic points and maintains a high quality map of the static part of the world. To this end, we propose a novel offline ground segmentation method and integrate it into the OctoMap to better distinguish between the moving objects and static road backgrounds. We evaluate our approach using SemanticKITTI for both, dynamic object removal and ground segmentation algorithms as well as on the Apollo dataset. The evaluation results show that our method outperforms the baseline methods in both tasks and achieves good performance in generating clean maps over different datasets without any change in the parameters.}
}
@article{YAMAZAKI2022104232,
title = {Approaching motion planning for mobile manipulators considering the uncertainty of self-positioning and object’s pose estimation},
journal = {Robotics and Autonomous Systems},
volume = {158},
pages = {104232},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104232},
url = {https://www.sciencedirect.com/science/article/pii/S092188902200135X},
author = {Kimitoshi Yamazaki and Satoshi Suzuki and Yusuke Kuribayashi},
keywords = {Mobile manipulators, Uncertainty, Pose evaluation, Posture selection},
abstract = {This study focuses on the problem of determining the posture of a mobile manipulator aiming to reach a target end-effector pose. Any posture of a mobile manipulator can be described using two parameters: the pose of the mobile platform and the joint angles of the manipulator. This study proposes a posture evaluation method for determining the pose of the mobile platform while focusing on two types of uncertainty: (i) the pose error associated with the end-effector caused by the accumulated positioning error of the mobile platform and (ii) the pose estimation error of objects to be grasped. In addition to using pose errors, the proposed method considers the manipulation ability of the end-effector and the tolerance region for grasping an object by the end-effector. Furthermore, this study proposes a method based on Bayes optimization for finding acceptable moving paths for the mobile platform in environments with sparse obstacles. The effectiveness of the proposed methods was demonstrated through simulations using a mobile manipulator. According to simulation results, the proposed methods can find both moving trajectories for the mobile platform and postures for the mobile manipulator within several seconds.}
}
@article{FAIZULLIN2023104316,
title = {Best Axes Composition extended: Multiple gyroscopes and accelerometers data fusion to reduce systematic error},
journal = {Robotics and Autonomous Systems},
volume = {160},
pages = {104316},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104316},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002056},
author = {Marsel Faizullin and Gonzalo Ferrer},
keywords = {Multiple IMU, Virtual IMU, Data fusion, State estimation},
abstract = {Multiple rigidly attached Inertial Measurement Unit (IMU) sensors provide a richer flow of data compared to a single IMU. State-of-the-art methods follow a probabilistic model of IMU measurements based on the random nature of errors combined under a Bayesian framework. However, affordable low-grade IMUs, in addition, suffer from systematic errors due to their imperfections not covered by their corresponding probabilistic model. In this paper, we propose a method, the Best Axes Composition (BAC) of combining Multiple IMU (MIMU) sensors data for accurate 3D-pose estimation that takes into account both random and systematic errors by dynamically choosing the best IMU axes from the set of all available axes. We evaluate our approach on our MIMU visual–inertial sensor and compare the performance of the method with a purely probabilistic state-of-the-art approach of MIMU data fusion. We show that BAC outperforms the latter and achieves up to 20% accuracy improvement for both orientation and position estimation in open loop, but needs proper treatment to keep the obtained gain.}
}
@article{HU2023104381,
title = {Model predictive optimization for imitation learning from demonstrations},
journal = {Robotics and Autonomous Systems},
volume = {163},
pages = {104381},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104381},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000209},
author = {Yingbai Hu and Mingyang Cui and Jianghua Duan and Wenjun Liu and Dianye Huang and Alois Knoll and Guang Chen},
keywords = {Imitation learning, Model predictive control, Dynamic movement primitives},
abstract = {“Motion generation by imitating” enables a robot to generate its trajectory in a new environment. Research works on dynamic movement primitives (DMP) has reported promising results, with good imitation effect and convergence to the target. However, DMP still has issues such as learning from multiple demonstrations for different initial conditions and achieving obstacle avoidance considering the distribution and motion of obstacles. One of the effective solutions is combining DMP and model predictive control (MPC). The imitation process was transformed into a receding horizon planning procedure, letting the robot to learn more from nearer demonstrations. It is solved as an optimization problem with obstacles modeled as constraints. However, its drawback includes the heavy computation burden, which can be even aggravated in a multi-obstacle scenario where complicated constraints occur. Thus, in this paper, we propose an enhanced MPDMP+ method that combines the advantages of MPC with potential function for both multi-demonstration imitation and multi-obstacle avoidance effect. A proximal augmented Lagrangian method is proposed to solve the optimization problem. This proposed method has a faster convergence rate and small errors. We conducted the simulation and robot experiments for imitation learning for obstacle avoidance scenarios. Our results illustrate the superior performance of the proposed method.}
}
@article{CHIEN2023104344,
title = {Design and control of an aerial-ground tethered tendon-driven continuum robot with hybrid routing},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104344},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104344},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002330},
author = {Jer Luen Chien and Clarissa Leong and Jingmin Liu and Shaohui Foong},
keywords = {Cosserat rod, Tendon-driven continuum robot, Aerial robot, Robot control, Aerial manipulation},
abstract = {Combining aerial and continuum robots harnesses both their flexibility and manoeuvrability to potentially perform dangerous maintenance tasks. However, such systems require heavy payloads to interact with its environment. An aerial-ground tethered tendon-driven continuum robot is thus proposed to tackle the limitations of on-board payload aerial systems and the underactuation of multirotors. Due to the natural limitation on the tendons used in the implementation of aerial ground tethered continuum robots, we explore the use of hybrid polynomial and parallel routes to achieve desired workspace profiles, while providing intuition on choosing suitable tendon routes. In this work, we leverage on geometrically exact methods to derive the differential kinematics of the aerial continuum robot using actuation sensors particularly for polynomial tendon routes. We demonstrate that both position and orientation can be controlled using a single stage continuum robot with hybrid tendon routing. Finally, a simple manoeuvre is executed by the aerial continuum robot prototype to validate the proposed proof of concept.}
}
@article{PRASAD2023104340,
title = {Design and development of software stack of an autonomous vehicle using robot operating system},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104340},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104340},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002299},
author = {Abhisek Omkar Prasad and Pradumn Mishra and Urja Jain and Anish Pandey and Anushka Sinha and Anil Singh Yadav and Rajan Kumar and Abhishek Sharma and Gaurav Kumar and Karrar {Hazim Salem} and Avdhesh Sharma and Anil Kumar Dixit},
keywords = {Autonomous vehicle, Robot operating system, Simultaneous localization and mapping, Computer vision-based controller, Intelligent object avoidance},
abstract = {In recent research activities, autonomous vehicles and self-driving technology have gained lot of attention among scientists. The idea of autonomous vehicles can be anticipated in the 1920s when the design of the first radio-controlled vehicles was in progress. Autonomous vehicles are going to be the trend of the future in this modern era of automation and technology. In this paper various autonomous driving aspects, highlighting the software stack and hardware components are discussed. The software architecture covers mainly robot operating system (ROS), machine learning (ML), deep learning (DL), and OpenCV frameworks, along with the calibration of sensors and cameras. The paper also discussed about simultaneous localization and mapping (SLAM) based-path tracking, computer vision-based controller, and intelligent object avoidance. Further, point cloud, ground, radius, and raycast filters was implemented to distinguish between the real-time objects, ground, and its own parts or obstacle shadows. The paper highlights the overall hardware modules responsible for controlling the car.}
}
@article{GAO2023104291,
title = {A non-potential orthogonal vector field method for more efficient robot navigation and control},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104291},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104291},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001804},
author = {Yan Gao and Chenggang Bai and Rao Fu and Quan Quan},
keywords = {Artificial potential field, Non-potential orthogonal vector field, Robotic system, Distributed control},
abstract = {To navigate and control a single mobile robot or a robotic swarm with higher efficiency, a novel non-potential orthogonal vector field method is proposed in this paper, which is modified from the traditional artificial potential field method. The improvement strategy aims at making the overall repulsive vector field orthogonal to the attractive vector field in some conditions. And the same potential field function is still applied to the Lyapunov-based stability analysis. In short, such an improvement strategy combines the advantages of the artificial potential field method with the non-potential vector field method, namely a combination of theoretical completeness and control efficiency. Finally, the effectiveness of the proposed method is validated both by numerical simulations with statistical significance and real experiments. The comparisons between our method and other methods are also presented.}
}
@article{DINAKARAN2023104303,
title = {A novel multi objective constraints based industrial gripper design with optimized stiffness for object grasping},
journal = {Robotics and Autonomous Systems},
volume = {160},
pages = {104303},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104303},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001920},
author = {Venkatesa Prabu Dinakaran and Meenakshi Priya Balasubramaniyan and Quynh Hoang Le and Ali Jawad Alrubaie and Ameer Al-khaykan and Suresh Muthusamy and Hitesh Panchal and Mustafa Musa Jaber and Anil Kumar Dixit and Chander Prakash},
keywords = {Soft gripper, Robotic manipulator, Multiple degrees of freedom, Optimization, Variable stiffness, Object grasping},
abstract = {Soft gripper design is a rising area of research due of its great possibilities in automation. One difficult problem in robot design is the ability to grasp a broader variety of items with variable stiffness, forms, and sizes in a single gripper. An ideal soft robotic gripper design with variable stiffness was designed in this research as a grasping model. Its distinctiveness is found in the methods utilized for modelling actuators and in the shifting stiffness characteristics of silicon soft gripper. When modelling the actuator in this case, multi-objective functions like gripping displacement and force transmission ratio are taken into account, and the actuator functions are controlled by the MDF (multiple degrees of freedom). The precise stiffness needed to grasp the item is then chosen using an adaptive optimization method. This enhanced weight-based horse herd (IHH) optimization method carries out the stiffness adjustment based on actuation pressure. Additionally, the suggested soft robotic gripper with variable stiffness employs the adaptive level set (ALS) technique to build the gripping force model. Additionally, several validations are offered in relation to the outcomes for item gripping by the suggested soft gripper. This shown that the results of the created soft gripper excelled those of other methods. The developed ABBIRB 1410 robot gripper type may enhance the work cycle in industrial applications and performs object grabbing with dependability and speed. The experimental validations show that the developed gripper model provides an enhanced object grasp with a range of curvatures, delivering a maximum pulling force of 121.07 kPa at 50 kpa, 119.15 kPa for patterned pulling, and 45.05 kPa for non-patterned pulling. The designed gripper type has a curve with a minimum size of 1.1 mm and a maximum size of 218 mm. Additionally, the soft gripper for industrial applications is examined with variously sized and weighted items. The suggested gripper model achieved an RMSE performance of 2.9 and a Pearson correlation of 0.993.}
}
@article{CHEN2023104377,
title = {Kinematics optimization of a novel 7-DOF redundant manipulator},
journal = {Robotics and Autonomous Systems},
volume = {163},
pages = {104377},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104377},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000167},
author = {Yanlin Chen and Xianmin Zhang and Yanjiang Huang and Yanbin Wu and Jun Ota},
keywords = {Kinematics analysis, Spherical parallel mechanism, Redundant manipulator, LDWPSO},
abstract = {Redundant manipulators can accomplish complex tasks due to the redundant degree of freedom (DOF). At the same time, their kinematics calculations are complicated. In this study, the kinematics solution for a self-designed 7-DOF redundant anthropomorphic manipulator is obtained, and an optimization method is provided to optimize the motion time of the manipulator in the same trajectory. Kinematics analysis of the spherical parallel shoulder joint was performed by using a geometric method and coordinate transformation. Kinematics analysis of the redundant manipulator was performed by a geometric method with an optimizing arm angle. Linearly decreasing weight particle swarm optimization (LDWPSO) was used to optimize the arm angle to minimize the motion time of the manipulator. The kinematics calculation of the shoulder joint was verified by a combination of SOLIDWORKSTM and MATLABTM software. The kinematics calculation of the redundant manipulator was verified by MATLABTM. The linear, circular and 8-shaped motion trajectories were used to evaluate the proposed method in the simulation. The simulation results showed that the motion time with optimization of the arm angle was only 16%–30% of that without optimization. Furthermore, the proposed method was evaluated through real manipulator experiments, and the experimental results were similar to those in the simulation.}
}
@article{BONNET2023104365,
title = {Practical whole-body elasto-geometric calibration of a humanoid robot: Application to the TALOS robot},
journal = {Robotics and Autonomous Systems},
volume = {164},
pages = {104365},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104365},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000040},
author = {Vincent Bonnet and Joseph Mirabel and David Daney and Florent Lamiraux and Maxime Gautier and Olivier Stasse},
keywords = {Elasto-geometric calibration, Humanoid robot},
abstract = {The whole-body elasto-geometrical calibration of humanoid robots is critical particularly for their control and accurate simulation. However, it is often not considered probably since it is a nontrivial task due to the mechanical complexity and inherent constraints of anthropomorphic structures. Also, humanoid robots have to sustain great efforts on their support legs, leading to link and joint being deformed, and are prone to auto-collision. Thus, elastic parameters have to be factored in addition to the geometric ones and to improve the precision of the pose of all robot segments. This is much more cumbersome and time consuming than the classical calibration of serial manipulators that deals solely with the estimation of the pose of the end-effector. Finally, due to the complexity of the task, a manual intervention in several steps of the calibration is no longer possible and a thorough automation of the approach is needed. Therefore, we propose to use a stereophotogrammetric system along with embedded joint torque sensors to calibrate the pose of all robot links with a fully automatic procedure. The generation of the minimal set of optimal calibration postures is based on a new iterative optimization process that leads to a stable maximum of an observability index. Then full set of geometrical parameters but also joint and base elastic parameters were calibrated using a single least-square optimization program. The proposed method was validated on a TALOS humanoid robot allowing to obtain an accurate whole-body calibration in less than 10 min. The proposed approach was cross-validated experimentally and showed an average RMS error of the tracked markers of 2.2 mm.}
}
@article{SINGH2023104302,
title = {Caster Walker GAIT Trainer (CGT): A robotic assistive device},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104302},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104302},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001919},
author = {Ramanpreet Singh and Vimal Kumar Pathak and Abhishek Sharma and Debaditya Chakraborty and Kuldeep K. Saxena and C. Prakash and Dharam Buddhi and Karrar hazim Salem},
keywords = {Defect-free, Mechanism synthesis, Gait rehabilitation, Caster Walker Gait Trainer, Lower limb, CGT, TLBO, Teaching–Learning-Based-Optimization, Stephenson III six-bar mechanism},
abstract = {Stroke has become one of the leading causes of lower limb paresis. Costing the patients, a fortune for its diagnosis and prognosis. Clinical experimentations have proven that one can regain ambulation if the rehabilitation is started in the acute or sub-acute stage. Traditional mode of rehabilitation include manual therapies which are labor-intensive and time consuming. Therefore, robotic training are preferred over manual therapies. Nevertheless, there are some limitations such as devices are bulky and complex, some are not portable, others need body weight support system, and costly. To address such issues, this paper proposes development of a new Caster Walker Gait Trainer (CGT) for gait rehabilitation. CGT is an end-effector based passive device in which Stephenson III six-bar linkage has been implemented to mimic the kinematics of a healthy gait. The trainer device uses a belt-pulley system for providing motion to the linkage. The lower limb of the patient gets the drive as he/she pushes the cater walker forward or backward. The paper also proposed the optimal design of defect-free Stephenson III six-bar linkage using loop-by-loop approach. To design the linkage, an optimal problem is formulated, and tear drop ankle trajectory is desired. The optimization problem is solved using a nature-inspired algorithm and it is found that the trajectory generated by the synthesized mechanism is able to mimic the desired trajectory. Then using the notion of inverse kinematics, hip and knee trajectories are obtained from the generated ankle trajectory for validation.}
}
@article{STACHE2023104288,
title = {Adaptive path planning for UAVs for multi-resolution semantic segmentation},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104288},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104288},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001774},
author = {Felix Stache and Jonas Westheider and Federico Magistri and Cyrill Stachniss and Marija Popović},
keywords = {Unmanned aerial vehicles, Semantic segmentation, Planning, Terrain monitoring},
abstract = {Efficient data collection methods play a major role in helping us better understand the Earth and its ecosystems. In many applications, the usage of unmanned aerial vehicles (UAVs) for monitoring and remote sensing is rapidly gaining momentum due to their high mobility, low cost, and flexible deployment. A key challenge is planning missions to maximize the value of acquired data in large environments given flight time limitations. This is, for example, relevant for monitoring agricultural fields. This paper addresses the problem of adaptive path planning for accurate semantic segmentation of using UAVs. We propose an online planning algorithm which adapts the UAV paths to obtain high-resolution semantic segmentations necessary in areas with fine details as they are detected in incoming images. This enables us to perform close inspections at low altitudes only where required, without wasting energy on exhaustive mapping at maximum image resolution. A key feature of our approach is a new accuracy model for deep learning-based architectures that captures the relationship between UAV altitude and semantic segmentation accuracy. We evaluate our approach on different domains using real-world data, proving the efficacy and generability of our solution.}
}
@article{NUTALAPATI2022104262,
title = {A generalized framework for autonomous calibration of wheeled mobile robots},
journal = {Robotics and Autonomous Systems},
volume = {158},
pages = {104262},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104262},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001531},
author = {Mohan Krishna Nutalapati and Lavish Arora and Anway Bose and Ketan Rajawat and Rajesh M. Hegde},
keywords = {Calibration and identification, Kinematics, Wheeled robots, Sensor fusion, Extrinsic calibration},
abstract = {Robotic calibration allows for the fusion of data from multiple sensors such as odometers, cameras etc., by providing appropriate transformational relationships between the corresponding reference frames. For wheeled robots equipped with exteroceptive sensors, calibration entails learning the motion model of the sensor or the robot in terms of the odometric data, and must generally be performed prior to performing tasks such as simultaneous localization and mapping (SLAM). Within this context, the current trend is to carry out simultaneous calibration of odometry and exteroceptive sensors without using additional hardware. Building upon the existing simultaneous calibration algorithms, we put forth a generalized calibration framework that can not only handle robots operating in 2D with arbitrary or unknown motion models but also handle outliers in an automated manner. We first propose an algorithm based on the alternating minimization framework applicable to two-wheel differential drive. Subsequently, for arbitrary but known drive configurations we put forth an iteratively re-weighted least squares methodology leveraging an intelligent weighing scheme. Different from the existing works, these proposed algorithms require no manual intervention and seamlessly handle outliers that arise due to both systematic and non-systematic errors. Finally, we put forward a novel Gaussian Process-based non-parametric approach for calibrating wheeled robots with unknown or un-modeled drive configurations. Detailed experiments are performed to demonstrate the accuracy, usefulness, and flexibility of the proposed algorithms. 11This paper has supplementary downloadable material (available at http://tinyurl.com/simultaneous-calibration) that includes raw-data of all the experiments and implementation codes for the proposed methodologies.}
}
@article{WENZHU2023104364,
title = {Smart agriculture: Development of a skid-steer autonomous robot with advanced model predictive controllers},
journal = {Robotics and Autonomous Systems},
volume = {162},
pages = {104364},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104364},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000039},
author = {Cesar {Wen Zhu} and Elyse Hill and Mohammad Biglarbegian and S. Andrew Gadsden and John A. Cline},
keywords = {Mobile robots, Skid-steer, MPSMC, SMC, MPC, Tube-based MPC, Lyapunov, Agricultural applications},
abstract = {The agricultural domain has been experiencing extensive automation interest over the past decade. The established process for measuring physiological and morphological traits (phenotypes) of crops is labour-intensive and error-prone. In this paper, a mobile robotic platform, namely The Autonomous Robot for Orchard Surveying (AROS), was developed to automate the process of collecting spatial and visual data autonomously. Furthermore, six different control frameworks are presented to evaluate the feasibility of using a kinematic model in agricultural environments. The kinematic model does not consider wheel slippage or any forces associated with dynamic motion. Thus, the following six controllers are evaluated: Proportional-Derivative (PD) controller, Sliding Mode Controller (SMC), Control-Lyapunov Function (CLF), Nonlinear Model Predictive Controller (NMPC), Tube-Based Nonlinear Model Predictive Controller (TBNMPC), and Model Predictive Sliding Mode Control (MPSMC). This paper provides insight into the degree of disturbance rejection that the mentioned control architectures can achieve in outdoor environments. Experimental results validate that all control architectures are capable of rejecting the present disturbances associated with unmodelled dynamics and wheel slip on soft ground conditions. Additionally, the optimal-based controllers managed to perform better than the non-optimal controllers. Performance improvements of the TBNMPC of up to 209.72% are realized when compared to non-optimal methods. Results also show that the non-optimal controllers had low performance due to the underactuated constraint present in the kinematic model.}
}
@article{BEDARD2023104361,
title = {Message flow analysis with complex causal links for distributed ROS 2 systems},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104361},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104361},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002500},
author = {Christophe Bédard and Pierre-Yves Lajoie and Giovanni Beltrame and Michel Dagenais},
keywords = {Software tools for robot programming, Distributed robot systems, Robot Operating System (ROS), Performance analysis, Tracing},
abstract = {Distributed robotic systems rely heavily on the publish–subscribe communication paradigm and middleware frameworks that support it, such as the Robot Operating System (ROS), to efficiently implement modular computation graphs. The ROS 2 executor, a high-level task scheduler which handles ROS 2 messages, is a performance bottleneck. We extend ros2_tracing, a framework with instrumentation and tools for real-time tracing of ROS 2, with the analysis and visualization of the flow of messages across distributed ROS 2 systems. Our method detects one-to-many and many-to-many causal links between input and output messages, including indirect causal links through simple user-level annotations. We validate our method on both synthetic and real robotic systems, and demonstrate its low runtime overhead. Moreover, the underlying intermediate execution representation database can be further leveraged to extract additional metrics and high-level results. This can provide valuable timing and scheduling information to further study and improve the ROS 2 executor as well as optimize any ROS 2 system. The source code is available at: github.com/christophebedard/ros2-message-flow-analysis.}
}
@article{BEDNAREK2022104236,
title = {HAPTR2: Improved Haptic Transformer for legged robots’ terrain classification},
journal = {Robotics and Autonomous Systems},
volume = {158},
pages = {104236},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104236},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001373},
author = {Michał Bednarek and Michał R. Nowicki and Krzysztof Walas},
keywords = {Legged robots, Deep learning methods, Data sets for robot learning},
abstract = {The haptic terrain classification is an essential component of a mobile walking robot control system, ensuring proper gait adaptation to the changing environmental conditions. In practice, such components are a part of an autonomous system and thus have to be lightweight, provide fast inference time, and guarantee robustness to minor changes in recorded sensory data. We propose transformer-based HAPTR and HAPTR2 terrain classification methods that use force and torque measurements from feet to meet these requirements. For reliable comparison of the proposed solutions, we adapt two classical machine learning algorithms (DTW-KNN and ROCKET), one temporal convolution network (TCN), and use the state-of-the-art CNN-RNN. The experiments are performed on publicly available PUTany and QCAT datasets. We show that the proposed HAPTR and HAPTR2 methods achieve accuracy on par or better than state-of-the-art approaches with a lower number of parameters, faster inference time, and improved robustness to input signal distortions. These features make HAPTR and HAPTR2 excel in terrain recognition tasks when considering real-world requirements.}
}
@article{TUGAL2023104345,
title = {Contact-based object inspection with mobile manipulators at near-optimal base locations},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104345},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104345},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002342},
author = {Harun Tugal and Kamil Cetin and Yvan Petillot and Matthew Dunnigan and Mustafa Suphi Erden},
keywords = {Particle swarm optimization, Mobile robots, Underwater vehicle manipulator system, Forward dynamics control},
abstract = {This paper presents a control and motion planning algorithm for a mobile vehicle-manipulator system such that the mobile vehicle and the manipulator mounted on it work in harmony to inspect unknown objects. Forward Dynamic Control method is used for the manipulator to accomplish a stable interaction with the environment and constrained particle swarm optimization is applied so that the vehicle can be localized at the estimated points maximizing the dexterity of the manipulator. Quartic splines are implemented to generate a smooth path for the vehicle in between the optimal locations. The proposed architecture is validated via an experimental setup consisting of a robotic arm with a force sensor at its end-effector mounted on a parallel manipulator. These experiments emulate an underwater vehicle-manipulator system, where the mobile base is subject to disturbances due to the physical interaction of the end-effector with the environment, typically a pipe. The advantage of the proposed approach is that it allows continuous and smooth movement of the base in harmony with the robotic manipulator while executing a task on a large surface (larger than the manipulator workspace can cover from a fixed position) and maintains a high level of dexterity index for the manipulator.}
}
@article{TOAN2023104317,
title = {The human-following strategy for mobile robots in mixed environments},
journal = {Robotics and Autonomous Systems},
volume = {160},
pages = {104317},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104317},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002068},
author = {Nguyen Van Toan and Minh {Do Hoang} and Phan Bui Khoi and Soo-Yeong Yi},
keywords = {Mixed environments, Human-following mobile robots, Robot behavior strategy, Human-like inference mechanism, Hedge algebras fuzzy},
abstract = {The robot behavior strategy is considered as a crucial part in the human-following task to help the robot maintain an appropriate distance and orientation to the selected target person (STP) with a smooth and safe manner. As usual, the robot is uniquely considered to follow the STP in a specific class of environments, such as unknown environments (non-mapped environments) or known environments (mapped environments). However, in real-life applications, the robot is sometimes requested to follow the STP in various types of environments, both in known and unknown ones. This observation raises the need to propose an alternative method to challenge the mentioned issue, as well as to break the current limit of the human-following function. In this paper, a new approach for the human-following strategy is proposed in which the mobile robot is enabled to follow the STP in mixed environments (non-mapped and mapped). In non-mapped environments, only the STP and the obstacle information with respect to the robot local coordinates are considered, whose purpose is to make the robot work without any prior understandings about its working environment. However, after the robot entered mapped environments, its prior knowledge of the working environment is leveraged to fulfill some additional requirements during the cooperation, such as the mobile robot in factories is not allowed to enter some specific areas even when the STP is executing technical tasks inside. Additionally, in this paper, a human-like inference mechanism is also introduced for the human-following strategy by using an extended hedge algebras. The proposed method is experimentally verified both in factories and laboratories. Demo Video Link: https://www.youtube.com/watch?v=YGrWU6ldKuw Since real videos in the factory are not allowed to publish, only visualization (in Rviz) is presented for demos in such kinds of environments. The visualization is synchronous with the real executions of the human–robot interactions. The robot used in the factory is an autonomous mobile robot (dimension 0.5 (m) ×1.0 (m), weight 120 (kg), carrying a tool cabinet around 300(kg))). The mobile robot is following the worker to support them during the technical processes in the car production line. In the video, the robot is represented by a green rectangular, and the STP is represented by a cylinder (with a sphere on its head) The events in the demo video are described more clearly in Appendix A.}
}
@article{ZHAO2022104260,
title = {Gait rehabilitation training robot: A motion-intention recognition approach with safety and convenience},
journal = {Robotics and Autonomous Systems},
volume = {158},
pages = {104260},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104260},
url = {https://www.sciencedirect.com/science/article/pii/S092188902200152X},
author = {A. Donghui Zhao and B. Tianqi Zhang and C. Houde Liu and D. Junyou Yang and E. Hiroshi Yokoi},
keywords = {Directional Intention Recognition (DIR), Gait rehabilitation training robot, Safety and accuracy, Speed Intention Recognition (SIR)},
abstract = {Motion-intention recognition is a vital prerequisite in active training when employing a gait rehabilitation training robot. To accurately recognize the motion intention of the elderly and the people with inconveniences in the human–robot interaction process, a novel approach of motion intention recognition with safety, accuracy, and convenience, including directional intention recognition (DIR) and speed intention recognition (SIR) is proposed in this paper. Firstly, the structures of the gait rehabilitation training robot and its motion-intention recognition system are illustrated. To ensure that the user walks in any desired direction safely and naturally, an improved distance-type fuzzy reasoning algorithm combined with a shake-intent filter and second-order optimization algorithm is proposed. It effectively eliminates the control error caused by body shaking and usage habits in human–robot interaction. Furthermore, by extracting from the pressure sensor data, a novel algorithm, taking advantage of the Gaussian probability density function (PDF)’s characteristics, is proposed for SIR, which does not increase the system complexity. Finally, a multi-directional fuzzy reasoning experiment and a human–robot interaction experiment are implemented. The results show that the algorithm can accurately recognize the motion direction intention and motion speed intention of people with weak motion capability, which also improves the safety and convenience of the interaction approach. The proposed method can be integrated into a walker with similar structures, and the whole system can be applied in hospitals, families, and other places for assisting the elderly and the disabled.}
}
@article{YANG2023104292,
title = {INDI-based aggressive quadrotor flight control with position and attitude constraints},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104292},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104292},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001816},
author = {Jiesong Yang and Zhihao Cai and Jiang Zhao and Zexin Wang and Yongfei Ding and Yingxun Wang},
keywords = {Quadrotor, Position and attitude control, Constrained trajectory planning, Incremental nonlinear dynamic inversion (INDI), Gap traverse flight},
abstract = {Recent studies have significantly contributed to the extensive use of quadrotors for delivery, mapping, and inspection. To further increase the versatility of quadrotors under confined environments, we focus on the precise trajectory tracking problem with position and attitude constraints. In tightly constrained scenarios, any slight error will infect flight security, especially in a large attitude maneuver. We utilize the incremental nonlinear dynamic inversion (INDI) method to precisely linearize the nonlinearities in the system and generalize it to the entire rotation space to reach a globally expressed control law. Meanwhile, the thrust alignment is introduced to improve the robustness against the mismatch between actuator dynamics and rotational dynamics, guaranteeing higher tracking accuracy. Improvements over a conventional geometry tracking controller are demonstrated in experiments where the quadrotor flies through an inclined narrow gap with orientation up to 90°. Flight tests also indicate the high disturbance rejection capabilities with the thrust alignment in gap traverse flight.}
}
@article{LESTINGI2023104387,
title = {Specification, stochastic modeling and analysis of interactive service robotic applications},
journal = {Robotics and Autonomous Systems},
volume = {163},
pages = {104387},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104387},
url = {https://www.sciencedirect.com/science/article/pii/S092188902300026X},
author = {Livia Lestingi and Davide Zerla and Marcello M. Bersani and Matteo Rossi},
keywords = {Service robotics, Human–robot interaction, Formal methods for robotics, Statistical model-checking, Model-driven engineering, Domain-specific languages for robotics, Stochastic hybrid automata, Models of human behavior},
abstract = {Assistive robotic systems are quickly becoming a core technology for the service sector as they are understood capable of supporting people in need of assistance in a wide variety of tasks. This step poses a number of ethical and technological questions. The research community is wondering how service robotics can be a step forward in human care and aid, and how robotics applications can be realized in order to put the human role at the forefront. Therefore, there is a growing demand for frameworks supporting robotic application designers in a “human-aware” development process. This paper presents a model-driven framework for analyzing and developing human–robot interactive scenarios in non-industrial settings with significant sources of uncertainty. The framework’s core is a formal model of the agents at play – the humans and the robot – and the robot’s mission, which is then put through verification to estimate the probability of completing the mission. The model captures non-trivial features related to human behavior, specifically the unpredictability of human choices and physiological aspects tied to their state of health. To foster the framework’s accessibility, we present a verification tool-agnostic Domain-Specific Language that allows designers lacking expertise in formal modeling to configure the interactive scenarios in a user-friendly manner. We compare the formal analysis outputs with results obtained by deploying benchmark scenarios in the physical environment with a real mobile robot to assess whether the formal model adheres to reality and whether the verification results are accurate. The entire development pipeline is then tested on several scenarios from the healthcare setting to assess its flexibility and effectiveness in the application design process.}
}
@article{AZPURUA2023104304,
title = {A Survey on the autonomous exploration of confined subterranean spaces: Perspectives from real-word and industrial robotic deployments},
journal = {Robotics and Autonomous Systems},
volume = {160},
pages = {104304},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104304},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001932},
author = {Héctor Azpúrua and Maíra Saboia and Gustavo M. Freitas and Lillian Clark and Ali-akbar Agha-mohammadi and Gustavo Pessin and Mario F.M. Campos and Douglas G. Macharet},
keywords = {Robot exploration, Confined and subterranean spaces, GPS-denied scenarios, Multi-robot exploration},
abstract = {Confined and subterranean areas are common in many civilian and industrial sites, although they are hazardous for humans given the presence of noxious gases, extreme temperatures, narrow spaces, unhealthy oxygen levels, flooding, and collapsing structures. Therefore, exploration, routine inspections, and surveillance tasks can benefit from using autonomous mobile robots to improve safety by reducing the presence of humans in those scenarios. However, despite advances in the field, there are still challenges to overcome for confined and subterranean robot operation. Real word robotic exploration requires robust and reliable map generation, precise localization, safe navigation, and efficient path planning. These requirements make exploration in complex 3D environments with rugged terrain difficult. The challenge is increased when considering multi-robot teams, as there is no guarantee of a functional network infrastructure. Despite consistent increasing interest in the area, there is a lack of research summarizing the results and best practices for exploring such environments. Therefore, in this paper, we provide a review and discuss state-of-the-art robotic exploration techniques, including single and cooperative approaches with homogeneous and heterogeneous teams, with a focus on complex subterranean and confined 3D scenarios. We also present a comprehensive list of insights on open challenges and possible directions for future investigation in the topic.}
}
@article{AUGELLO2023104400,
title = {Roboception and adaptation in a cognitive robot},
journal = {Robotics and Autonomous Systems},
volume = {164},
pages = {104400},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104400},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000398},
author = {Agnese Augello and Salvatore Gaglio and Ignazio Infantino and Umberto Maniscalco and Giovanni Pilato and Filippo Vella},
keywords = {Somatosensory system, Soft sensors, Cognitive architecture, Humanoid robot, Roboception, Reinforcement learning},
abstract = {In robotics, perception is usually oriented at understanding what is happening in the external world, while few works pay attention to what is occurring in the robot’s body. In this work, we propose an artificial somatosensory system, embedded in a cognitive architecture, that enables a robot to perceive the sensations from its embodiment while executing a task. We called these perceptions roboceptions, and they let the robot act according to its own physical needs in addition to the task demands. Physical information is processed by the robot to behave in a balanced way, determining the most appropriate trade-off between the achievement of the task and its well being. The experiments show the integration of information from the somatosensory system and the choices that lead to the accomplishment of the task.}
}
@article{DIEHL2023104376,
title = {A causal-based approach to explain, predict and prevent failures in robotic tasks},
journal = {Robotics and Autonomous Systems},
volume = {162},
pages = {104376},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104376},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000155},
author = {Maximilian Diehl and Karinne Ramirez-Amaro},
keywords = {Causality in robotics, Failure prediction and prevention, Explainable AI},
abstract = {Robots working in human environments need to adapt to unexpected changes to avoid failures. This is an open and complex challenge that requires robots to timely predict and identify the causes of failures in order to prevent them. In this paper, we present a causal-based method that will enable robots to predict when errors are likely to occur and prevent them from happening by executing a corrective action. Our proposed method is able to predict immediate failures and also failures that will occur in the future. The latter type of failure is very challenging, and we call them timely-shifted action failures (e.g., the current action was successful but will negatively affect the success of future actions). First, our method detects the cause–effect relationships between task executions and their consequences by learning a causal Bayesian network (BN). The obtained model is transferred from simulated data to real scenarios to demonstrate the robustness and generalization of the obtained models. Based on the causal BN, the robot can predict if and why the executed action will succeed or not in its current state. Then, we introduce a novel method that finds the closest success state through a contrastive Breadth-First-Search if the current action was predicted to fail. We evaluate our approach for the problem of stacking cubes in two cases; (a) single stacks (stacking one cube) and; (b) multiple stacks (stacking three cubes). In the single-stack case, our method was able to reduce the error rate by 97%. We also show that our approach can scale to capture various actions in one model, allowing us to measure the impact of an imprecise stack of the first cube on the stacking success of the third cube. For these complex situations, our model was able to prevent around 95% of the stacking errors. Thus, demonstrating that our method is able to explain, predict, and prevent execution failures, which even scales to complex scenarios that require an understanding of how the action history impacts future actions.}
}
@article{WONG2023104349,
title = {Human-assisted robotic detection of foreign object debris inside confined spaces of marine vessels using probabilistic mapping},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104349},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104349},
url = {https://www.sciencedirect.com/science/article/pii/S092188902200238X},
author = {Benjamin Wong and Wade Marquette and Nikolay Bykov and Tyler M. Paine and Ashis G. Banerjee},
keywords = {Robotics in hazardous fields, Mapping, Human-in-the-loop decision making},
abstract = {Many complex vehicular systems, such as large marine vessels, contain confined spaces like water tanks, which are critical for the safe functioning of the vehicles. It is particularly hazardous for humans to inspect such spaces due to limited accessibility, poor visibility, and unstructured configuration. While robots provide a viable alternative, they encounter the same set of challenges in realizing robust autonomy. In this work, we specifically address the problem of detecting foreign object debris (FODs) left inside the confined spaces using a visual mapping-based system that relies on Mahalanobis distance-driven comparisons between the nominal and online maps for local outlier identification. The identified outliers, corresponding to candidate FODs, are used to generate waypoints that are fed to a mobile ground robot to take camera photos. The photos are subsequently labeled by humans for final identification of the presence and types of FODs, leading to high detection accuracy while mitigating the effect of recall–precision tradeoff. Preliminary simulation studies, followed by extensive physical trials on a prototype tank, demonstrate the capability and potential of our FOD detection system.}
}
@article{LENZ2023104338,
title = {Bimanual telemanipulation with force and haptic feedback through an anthropomorphic avatar system},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104338},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104338},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002275},
author = {Christian Lenz and Sven Behnke},
keywords = {Force feedback control, Teleoperation, Dual arm manipulation, Human–robot interaction, Real-time control, Haptic interface},
abstract = {Robotic teleoperation is a key technology for a wide variety of applications. It allows sending robots instead of humans in remote, possibly dangerous locations while still using the human brain with its enormous knowledge and creativity, especially for solving unexpected problems. A main challenge in teleoperation consists of providing enough feedback to the human operator for situation awareness and thus create full immersion, as well as offering the operator suitable control interfaces to achieve efficient and robust task fulfillment. We present a bimanual telemanipulation system consisting of an anthropomorphic avatar robot and an operator station providing force and haptic feedback to the human operator. The avatar arms are controlled in Cartesian space with a direct mapping of the operator movements. The measured forces and torques on the avatar side are haptically displayed to the operator. We developed a predictive avatar model for limit avoidance which runs on the operator side, ensuring low latency. The system was successfully evaluated during the ANA Avatar XPRIZE competition semifinals. In addition, we performed in lab experiments and carried out a small user study with mostly untrained operators.}
}
@article{PEREZVILLEDA2023104309,
title = {Learning and extrapolation of robotic skills using task-parameterized equation learner networks},
journal = {Robotics and Autonomous Systems},
volume = {160},
pages = {104309},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104309},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001981},
author = {Hector Perez-Villeda and Justus Piater and Matteo Saveriano},
keywords = {Learning from demonstration, Learning parameterized skills, Skill generalization and extrapolation, Equation learner neural networks},
abstract = {Imitation learning approaches achieve good generalization within the range of the training data, but tend to generate unpredictable motions when querying outside this range. We present a novel approach to imitation learning with enhanced extrapolation capabilities that exploits the so-called Equation Learner Network (EQLN). Unlike conventional approaches, EQLNs use supervised learning to fit a set of analytical expressions that allows them to extrapolate beyond the range of the training data. We augment the task demonstrations with a set of task-dependent parameters representing spatial properties of each motion and use them to train the EQLN. At run time, the features are used to query the Task-Parameterized Equation Learner Network (TP-EQLN) and generate the corresponding robot trajectory. The set of features encodes kinematic constraints of the task such as desired height or a final point to reach. We validate the results of our approach on manipulation tasks where it is important to preserve the shape of the motion in the extrapolation domain. Our approach is also compared with existing state-of-the-art approaches, in simulation and in real setups. The experimental results show that TP-EQLN can respect the constraints of the trajectory encoded in the feature parameters, even in the extrapolation domain, while preserving the overall shape of the trajectory provided in the demonstrations.}
}
@article{FAROOQ2023104285,
title = {Power solutions for autonomous mobile robots: A survey},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104285},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104285},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001749},
author = {Muhammad Umar Farooq and Amre Eizad and Hyun-Ki Bae},
keywords = {Autonomous mobile robots, Automated guided vehicles, Energy solutions, Operating range, Operational endurance},
abstract = {Autonomous mobile robots are a special class of robotic systems that can move a payload from one location to the other or perform a specific task. They allow efficient, precise, and streamlined workflow that makes human work less arduous. The market and research work related to these robots is increasing in anticipation of industry 5.0, where humans and machines are expected to co-exist and co-work. The future mobile robots are desired to have clean and cost-effective energy sources to have longer operation times and compliance with environmental requirements to allow application in diverse fields. The research on mechanical design, perception, navigation and control has carved out many commercially viable solutions for mobile robots. However, their widespread application is still limited due to the lack of efficient power systems for use in diverse and largely unknown/uncontrolled environments. The current power solutions incur high initial costs and require recharging or refuelling, which makes them unsuitable for unattended long-haul worktimes and cost-effective applications. These drawbacks are major hurdles in the wider applicability of terrain-based mobile robots to new domains and daily life scenarios, which are possible with the existing mechanical, perception, and control technologies. Keeping in view the need for advancement in this field and to gain a better understanding of the current state of the art and future directions, this work summarizes and reviews the energy solutions presented in the literature and used in notable commercially available terrain-based mobile robots. The provided solutions are categorized and discussed while the prospects and research gaps are also highlighted. A comparison of discussed power techniques is also provided, which can serve as a guideline for selecting a robot’s energy source according to the desired requirements.}
}
@article{CHOUDHARY2022104256,
title = {Spatial and temporal features unified self-supervised representation learning networks},
journal = {Robotics and Autonomous Systems},
volume = {157},
pages = {104256},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104256},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001506},
author = {Rahul Choudhary and Rahee Walambe and Ketan Kotecha},
keywords = {Self-supervised learning, Reinforcement learning, Representation learning, Imitation learning, Manipulation},
abstract = {Robot manipulation tasks can be carried out effectively, provided the state representation is satisfactorily detailed. Embodiment difference, Viewpoint difference, and Domain difference are some of the challenges in learning from human demonstration. This work proposes a self-supervised and multi-viewpoint spatial and temporal features unified representation learning method. The algorithm consists of two components: (a) Spatial Component, which learns the setting of the environment, i.e., on which pixels to focus on most to get the best representation of the image regardless of point of view, and (b) Temporal Component that learns how snapshots taken from multiple viewpoints simultaneously (i.e., at the same time-step but from a different viewpoint) are similar and how these snaps are different from snaps taken at a different time-step but same viewpoint. Further, these representations are integrated with the Reinforcement Learning (RL) framework to learn accurate behaviors from videos of humans performing the manipulation task. The effectiveness of this approach is illustrated by training the robots to learn various manipulation tasks i.e., (a) grab objects (b) lift objects (c) open and close drawers from expert demonstrations provided by humans. The algorithm shows great promise and is highly successful across all the manipulation tasks. The robot learns to pick up objects of various shapes, sizes and colors having different orientations and placements on the table. The robot also successfully learns how to open and close drawers. The method is highly sample efficient and addresses the challenges of embodiment, viewpoint, and domain difference.}
}
@article{EIBAND2023104367,
title = {Online task segmentation by merging symbolic and data-driven skill recognition during kinesthetic teaching},
journal = {Robotics and Autonomous Systems},
volume = {162},
pages = {104367},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104367},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000064},
author = {Thomas Eiband and Johanna Liebl and Christoph Willibald and Dongheui Lee},
keywords = {Learning from demonstration, Programming by demonstration, Robot, Symbolic, Subsymbolic, Data-driven, Task segmentation, Action segmentation, Skill recognition, Task representation, Interactive robot programming, Intuitive robot programming, Force-based, Tactile, Online segmentation, Kinesthetic teaching},
abstract = {Programming by Demonstration (PbD) is used to transfer a task from a human teacher to a robot, where it is of high interest to understand the underlying structure of what has been demonstrated. Such a demonstrated task can be represented as a sequence of so-called actions or skills. This work focuses on the recognition part of the task transfer. We propose a framework that recognizes skills online during a kinesthetic demonstration by means of position and force–torque (wrench) sensing. Therefore, our framework works independently of visual perception. The recognized skill sequence constitutes a task representation that lets the user intuitively understand what the robot has learned. The skill recognition algorithm combines symbolic skill segmentation, which makes use of pre- and post-conditions, and data-driven prediction, which uses support vector machines for skill classification. This combines the advantages of both techniques, which is inexpensive evaluation of symbols and usage of data-driven classification of complex observations. The framework is thus able to detect a larger variety of skills, such as manipulation and force-based skills that can be used in assembly tasks. The applicability of our framework is proven in a user study that achieves a 96% accuracy in the online skill recognition capabilities and highlights the benefits of the generated task representation in comparison to a baseline representation. The results show that the task load could be reduced, trust and explainability could be increased, and, that the users were able to debug the robot program using the generated task representation.}
}
@article{SONUGUR2023104342,
title = {A Review of quadrotor UAV: Control and SLAM methodologies ranging from conventional to innovative approaches},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104342},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104342},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002317},
author = {Güray SONUGÜR},
keywords = {UAV, Quadrotor, Flight control, SLAM},
abstract = {There are two indispensable methodologies for autonomous flights performed by unmanned aerial vehicles (UAV). The first is flight control, and the other is simultaneous localization and mapping (SLAM). In the literature, these two issues are generally considered separately. However, they have very close relationships with each other. In this study, both methods were extensively examined in the literature, especially for quadrotors. Quadrotors, also known as quadrotors, are rotary-wing UAVs capable of vertical take-off and landing. As their use becomes widespread worldwide, the number of studies conducted to enable autonomous tasks is growing. The study was prepared under three subtitles. First, a fast and simple introduction of quadrotors was made, and the advances in this area were discussed. In the next section, studies on the position, attitude, and altitude control methods required for the autonomous use of such aircraft are analyzed based on linear, nonlinear, and intelligent methods. As the third subheading, research on SLAM techniques was widely discussed. Frequently used performance metrics, application environments, and results were presented in detailed tables for studies in both areas. Comparative studies were particularly emphasized, and the best results obtained were expressed in tables. The hardware implementations of the mentioned applications were also reviewed. Thus, hardware and method-based quick reference resource were created for researchers. As a consequence, the objective of this research is to provide a comprehensive resource for researchers working on quadrotor navigation systems to effectively select the flight control and SLAM methods they will employ.}
}
@article{HU2023104383,
title = {Iterative reward shaping for non-overshooting altitude control of a wing-in-ground craft based on deep reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {163},
pages = {104383},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104383},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000222},
author = {Huan Hu and Guiyong Zhang and Lichao Ding and Kuikui Jiao and Zhifan Zhang and Ji Zhang},
keywords = {Wing-in-ground craft, Non-overshoot, Deep reinforcement learning, Reward shaping, Path following},
abstract = {When a wing-in-ground craft (WIG) adjusts its flying altitude, overshooting behavior may occur, which weakens the safety and stealth ability. In previous studies on path following, cross-track error was used in company with other indicators to indirectly suppress overshoot. This paper proposes a method for direct and gradual suppression of the overshoot via deep reinforcement learning (DRL), which iterates the reward function by introducing a partial one based on the current overshoot magnitude. Each time the overshoot is obtained by DRL, a function about this overshoot is added to the reward function for retraining. The function is defined as a type of cross-track error within a range of the current overshoot magnitude to the target altitude, and it counts the partial reward before the WIG gets the worse overshoot during training. The methodological feasibility is proved by mathematical reasoning, and an example of a virtual WIG changing the altitude is taken to validate the method. Assuming that the added partial function is in a basic 1-order fractional form of cross-track error and multiplied by a factor, the implementation of iterative reward shaping decreases overshoot to a minimal level, with the overshoot down to over 99.8% when compared to the initial one. Moreover, when introducing the partial reward function in the first iteration, influence of the factor on overshoot is analyzed. For a WIG’s adjustment of altitude, the method can monotonically reduce overshoot within tolerance.}
}
@article{WANG2023104308,
title = {Evaluation of safety-related performance of wearable lower limb exoskeleton robot (WLLER): A systematic review},
journal = {Robotics and Autonomous Systems},
volume = {160},
pages = {104308},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104308},
url = {https://www.sciencedirect.com/science/article/pii/S092188902200197X},
author = {Duojin Wang and Xiaoping Gu and Wenzhuo Li and Yaoxiang Jin and Maisi Yang and Hongliu Yu},
keywords = {Wearable lower limb exoskeleton robot, Test, Safety, Performance indicator},
abstract = {Wearable lower limb exoskeleton robots (WLLER) have broad development prospects in the military, industrial and medical fields. The intelligent device comes into intimate contact with the human body, and its safety is an essential factor that developers must consider. With the increasing research on the safety of WLLER, its safety test methods and indicators should gradually improve. By examining current test methods and indicators, this study aims to mobilize this information and summarize the most recent safety research. The safety-related studies reviewed in this paper are not limited to evaluating subjects in clinical trials but are concerned with extensive safety research. The focus of our analysis is the test performance indicators. Some functional evaluation indicators are also summarized to explore a broader and more applicable approach on the safety metrics. We found that, in general, most researchers pay attention to the power-assisting performance of WLLER, but the stability and comfort have been largely ignored. At the same time, our analysis also reveals that although there are a wide variety of existing evaluation indicators, uniform and standard test methods and indicators for safety testing of WLLER are still deficient. Based on these results, we identified and discussed several promising research directions that may help the community to attain a widely accepted test method that can objectively evaluate the safety of WLLER.}
}
@article{LAURENZI2023104379,
title = {The XBot2 real-time middleware for robotics},
journal = {Robotics and Autonomous Systems},
volume = {163},
pages = {104379},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104379},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000180},
author = {Arturo Laurenzi and Davide Antonucci and Nikos G. Tsagarakis and Luca Muratore},
keywords = {Real-time, Middleware, ROS, Hardware abstraction layer, Simulation},
abstract = {This paper introduces XBot2, a novel real-time middleware for robotic applications with a strong focus on modularity and reusability of components, and seamless support for multi-threaded, mixed real-time (RT) and non-RT architectures. Compared to previous works, XBot2 focuses on providing a dynamic, ready-to-use hardware abstraction layer that allows users to make run-time queries about the robot topology, and act consequently, by leveraging an easy-to-use API that is fully RT-compatible. We provide an extensive description about implementation challenges and design decisions, and finally validate our architecture with multiple use-cases. These range from the integration of three popular simulation tools (i.e. Gazebo, PyBullet, and MuJoCo), to real-world tests involving complex, hybrid robotic platforms such as IIT’s CENTAURO and MoCA robots.}
}
@article{MARTIN2023104314,
title = {Multi-robot task allocation clustering based on game theory},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104314},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104314},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002032},
author = {Javier G. Martin and Francisco Javier Muros and José María Maestre and Eduardo F. Camacho},
keywords = {Multi-robot systems (MRS), Multi-robot task allocation (MRTA), Clustering, Task planning, Cooperative game theory, Shapley value},
abstract = {A cooperative game theory framework is proposed to solve multi-robot task allocation (MRTA) problems. In particular, a cooperative game is built to assess the performance of sets of robots and tasks so that the Shapley value of the game can be used to compute their average marginal contribution. This fact allows us to partition the initial MRTA problem into a set of smaller and simpler MRTA subproblems, which are formed by ranking and clustering robots and tasks according to their Shapley value. A large-scale simulation case study illustrates the benefits of the proposed scheme, which is assessed using a genetic algorithm (GA) as a baseline method. The results show that the game theoretical approach outperforms GA both in performance and computation time for a range of problem instances.}
}
@article{MISHRA2022104269,
title = {Footstep planning of humanoid robot in ROS environment using Generative Adversarial Networks (GANs) deep learning},
journal = {Robotics and Autonomous Systems},
volume = {158},
pages = {104269},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104269},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001580},
author = {Pradumn Mishra and Urja Jain and Siddharth Choudhury and Surjeet Singh and Anish Pandey and Abhishek Sharma and Ramanpreet Singh and Vimal Kumar Pathak and Kuldeep K. Saxena and Anita Gehlot},
keywords = {Footstep planning, Deep learning, Generative Adversarial Networks, Humanoid robots, Path planning, Robot Operating System},
abstract = {This paper proposes deep learning-based footstep planning using Generative Adversarial Networks (GANs) for the indoor navigation of humanoid robots. The GAN-based architecture presents an efficient and accurate path to plan the footsteps of a humanoid robot on Robot Operating System (ROS) based architecture. During navigation, humanoid robots must understand their surroundings and be able to generate footsteps accurately. Although some algorithms that are based on sampling, such as Rapidly Exploring Random Tree (RRT*) and A*, are widely used for path planning, they fail to perform in narrow paths, especially for the footstep generation of humanoid robots. The widely growing deep learning approaches such as GANs are now producing extremely surprising results in solving real-life problems. The experiments conclude that GAN based approach performs better than conventional Dijkstra’s or A* algorithms. The accuracy of the generated footsteps from the GAN-based path planner comes out to be approximately 93%.}
}
@article{MITREVSKI2023104350,
title = {A hybrid skill parameterisation model combining symbolic and subsymbolic elements for introspective robots},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104350},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104350},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002391},
author = {Alex Mitrevski and Paul G. Plöger and Gerhard Lakemeyer},
keywords = {Robot introspection, Robot execution failures, Skill execution models},
abstract = {In the design of robot skills, the focus generally lies on increasing the flexibility and reliability of the robot execution process; however, typical skill representations are not designed for analysing execution failures if they occur or for explicitly learning from failures. In this paper, we describe a learning-based hybrid representation for skill parameterisation called an execution model, which considers execution failures to be a natural part of the execution process. We then (i) demonstrate how execution contexts can be included in execution models, (ii) introduce a technique for generalising models between object categories by combining generalisation attempts performed by a robot with knowledge about object similarities represented in an ontology, and (iii) describe a procedure that uses an execution model for identifying a likely hypothesis of a parameterisation failure. The feasibility of the proposed methods is evaluated in multiple experiments performed with a physical robot in the context of handle grasping, object grasping, and object pulling. The experimental results suggest that execution models contribute towards avoiding execution failures, but also represent a first step towards more introspective robots that are able to analyse some of their execution failures in an explicit manner.}
}
@article{CHRISTIDILOUMPASEFSKI2023104310,
title = {On the parameter identification of free-flying space manipulator systems},
journal = {Robotics and Autonomous Systems},
volume = {160},
pages = {104310},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104310},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001993},
author = {Olga-Orsalia Christidi-Loumpasefski and Evangelos Papadopoulos},
keywords = {Parameter identification, Space manipulator systems, Space robotics, On-orbit servicing},
abstract = {A novel parameter identification method is proposed, which identifies all the parameters required for the reconstruction of free-flying space manipulator system dynamics. Its key advantage is that it does not use acceleration measurements; thus, it is less sensitive to sensor noise than other methods. The method is based on the conservation of angular momentum and on a kinematic equation including a Jacobian. To apply the method, all manipulator joints are commanded to follow optimized exciting trajectories, while the system is in free-floating mode. The estimated parameters render the free-flying system dynamics fully identified and available to model-based control. The method applies to multi-arm systems and is validated by simulation and experiments with excellent results.}
}
@article{SINGH2023104339,
title = {Efficient surface detection for assisting Collaborative Robots},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104339},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104339},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002287},
author = {Simranjit Singh and Mohit Sajwan and Gurbhej Singh and Anil Kumar Dixit and Amrinder Mehta},
keywords = {COBOTs, Robotics, Deep learning, CNN, LSTM},
abstract = {Collaborative Robots need to read the surfaces they are walking on to keep their dynamic equilibrium, regardless of whether the ground is flat or uneven. Although accelerometers are frequently employed for this task, previous efforts have centered on retrofitting the quadruped robots with new sensors. The second technique is to collect lots of samples for machine learning algorithms, which are not widely implemented. Learning-based approaches altered the traditional way of data analytics. The advanced deep learning algorithms provide better accuracy and prove more efficient when the data size is large. This paper introduced a novel architecture of Convolutional Neural Network, a deep learning-based approach for efficiently classifying the surface on which the robots are walking. The dataset contains reading captured by Inertia Measurement Unit sensors. The proposed model achieved an overall classification accuracy of 88%. The proposed architecture is compared with the existing deep and machine learning techniques to show its effectiveness. The proposed model can be installed on collaborative robots’ onboard processors to identify the surfaces effectively.}
}
@article{PRITZL2023104315,
title = {Adaptive estimation of UAV altitude in complex indoor environments using degraded and time-delayed measurements with time-varying uncertainties},
journal = {Robotics and Autonomous Systems},
volume = {160},
pages = {104315},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104315},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002044},
author = {Václav Pritzl and Matouš Vrba and Claudio Tortorici and Reem Ashour and Martin Saska},
keywords = {Unmanned Aerial Vehicle, UAV, Altitude estimation, State estimation, LIDAR, Laser, Indoor},
abstract = {A novel approach for robust Unmanned Aerial Vehicle (UAV) altitude estimation relying on laser measurements that is designed for use in complex indoor environments is proposed in this paper. Specifically, we aim to design a system with general usability inside multi-floor buildings. The multi-floor buildings are characterized by areas lacking distinct vertical geometric features to be used as reference by 3D Light Detection and Ranging (LiDAR) localization algorithms, and by areas with either flat floors or limited areas with inconsistent ground elevation. The proposed approach solves the problem of adaptive fusion of data from multiple sources with apriori-unknown confidence dependent on the current environmental properties. Whenever the environment contains enough geometric structure, altitude data from a 3D LiDAR-based Simultaneous Localization and Mapping (SLAM) algorithm are utilized. In environments that are too symmetrical for reliable SLAM operation, the approach relies mostly on measurements from a downward-facing 1D laser rangefinder, while simultaneously detecting inconsistent ground elevation areas. These measurements are fused with barometer data, Inertial Measurement Unit (IMU) data, and information from the UAV position controllers. Furthermore, our approach correctly handles the measurement delay caused by 3D LiDAR data processing that significantly differs from other sensor delays. The performance of the proposed approach has been validated in complex simulations and real-world experiments with the produced altitude estimate utilized in the control loop of the UAV. The proposed approach is released as open-source as part of the MRS UAV System.}
}
@article{BULTMANN2023104286,
title = {Real-time multi-modal semantic fusion on unmanned aerial vehicles with label propagation for cross-domain adaptation},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104286},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104286},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001750},
author = {Simon Bultmann and Jan Quenzel and Sven Behnke},
keywords = {Robot perception, Sensor fusion, Unmanned aerial vehicles, Semantic segmentation, Label propagation, Object detection, Deep learning},
abstract = {Unmanned aerial vehicles (UAVs) equipped with multiple complementary sensors have tremendous potential for fast autonomous or remote-controlled semantic scene analysis, e.g., for disaster examination. Here, we propose a UAV system for real-time semantic inference and fusion of multiple sensor modalities. Semantic segmentation of LiDAR scans and RGB images, as well as object detection on RGB and thermal images, run online onboard the UAV computer using lightweight CNN architectures and embedded inference accelerators. We follow a late fusion approach where semantic information from multiple sensor modalities augments 3D point clouds and image segmentation masks while also generating an allocentric semantic map. Label propagation on the semantic map allows for sensor-specific adaptation with cross-modality and cross-domain supervision. Our system provides augmented semantic images and point clouds with ≈ 9Hz. We evaluate the integrated system in real-world experiments in an urban environment and at a disaster test site.}
}
@article{YANG2022104258,
title = {Learning differentiable dynamics models for shape control of deformable linear objects},
journal = {Robotics and Autonomous Systems},
volume = {158},
pages = {104258},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104258},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001518},
author = {Yuxuan Yang and Johannes A. Stork and Todor Stoyanov},
keywords = {Deformable linear object, Model learning, Parameter identification, Model predictive control},
abstract = {Robots manipulating deformable linear objects (DLOs) – such as surgical sutures in medical robotics, or cables and hoses in industrial assembly – can benefit substantially from accurate and fast differentiable predictive models. However, the off-the-shelf analytic physics models fall short of differentiability. Recently, neural-network-based data-driven models have shown promising results in learning DLO dynamics. These models have additional advantages compared to analytic physics models, as they are differentiable and can be used in gradient-based trajectory planning. Still, the data-driven approaches demand a large amount of training data, which can be challenging for real-world applications. In this paper, we propose a framework for learning a differentiable data-driven model for DLO dynamics with a minimal set of real-world data. To learn DLO twisting and bending dynamics in a 3D environment, we first introduce a new suitable DLO representation. Next, we use a recurrent network module to propagate effects between different segments along a DLO, thereby addressing a critical limitation of current state-of-the-art methods. Then, we train a data-driven model on synthetic data generated in simulation, instead of foregoing the time-consuming and laborious data collection process for real-world applications. To achieve a good correspondence between real and simulated models, we choose a set of simulation model parameters through parameter identification with only a few trajectories of a real DLO required. We evaluate several optimization methods for parameter identification and demonstrate that the differential evolution algorithm is efficient and effective for parameter identification. In DLO shape control tasks with a model-based controller, the data-driven model trained on synthetic data generated by the resulting models performs on par with the ones trained with a comparable amount of real-world data which, however, would be intractable to collect.}
}
@article{ZHANG2023104366,
title = {Energy efficient path planning for autonomous ground vehicles with ackermann steering},
journal = {Robotics and Autonomous Systems},
volume = {162},
pages = {104366},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104366},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000052},
author = {Haojie Zhang and Yudong Zhang and Chuankai Liu and Zuoyu Zhang},
keywords = {Energy efficient path planning, Ackermann steering, Energy cost model, Motion primitive},
abstract = {The autonomous ground vehicles have attracted a great deal of attention as viable solutions to a wide variety of military and civilian applications. However, the energy consumption plays a major role in the navigation of autonomous ground vehicles in challenging environments, especially if they are left to operate unattended under limited on-board power, such as planetary exploration, border patrol, etc. The autonomous ground vehicles are expected to perform more tasks more efficiently with limited power in these scenarios. Although plenty of research has developed an effective methodology for generating dynamically feasible and energy efficient trajectories for skid steering or differential steering vehicles, few studies on path planning for ackermann steering autonomous ground vehicles are available. In this study, an energy efficient path planning method with guarantee on completeness is proposed for autonomous ground vehicle with ackermann steering which is based on A∗ search algorithm. Firstly, the energy cost model is established for the autonomous ground vehicle using its kinematic constraints. Then, given the start and goal states, the energy-aware motion primitives are generated offline using the energy cost model to calculate the cost of each primary trajectory. Lastly, the energy efficient path planner is proposed and the analysis for completeness properties is given. The effectiveness of the proposed energy efficient path planner is verified by simulation over 150 randomly generated maps and real vehicle tests. The results show that a small increase in the distance of a path over the distance optimal path can result in a reduction of energy cost by nearly 26.9% in simulation and 21.09% in real test scenario for autonomous ground vehicles with ackermann steering.}
}
@article{DEFAGO2023104378,
title = {Using model checking to formally verify rendezvous algorithms for robots with lights in Euclidean space},
journal = {Robotics and Autonomous Systems},
volume = {163},
pages = {104378},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104378},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000179},
author = {Xavier Défago and Adam Heriban and Sébastien Tixeuil and Koichi Wada},
keywords = {Autonomous mobile robots, Rendezvous, Lights, Model checking, Verification, Asynchrony, Continuous space},
abstract = {The paper details the first successful attempt at using model checking techniques to verify the correctness of distributed algorithms for robots evolving in a continuous environment. The study focuses on the problem of rendezvous of two robots with lights. There exist many different rendezvous algorithms that aim at finding the minimal number of colors needed to solve rendezvous in various synchrony models (e.g., FSYNC, SSYNC, ASYNC). While these rendezvous algorithms are typically very simple, their analysis and proof of correctness tend to be extremely complex, tedious, and error-prone as impossibility results are based on subtle interactions between the activation schedules of the robots. The paper presents a generic verification model that can be concretely expressed in available software model-checkers. In particular, we explain the subtle design decisions that allow to keep the search space finite and tractable, as well as prove several important theorems that support them. As a sanity check, we use the model to verify several known rendezvous algorithms in six different models of synchrony. In each case, we find that the results obtained from the model checker are consistent with the results known in the literature. The model checker outputs a counter-example execution in every case that is known to fail. In the course of developing and proving the validity of the model, we identified several fundamental theorems, including the ability for a well chosen algorithm and ASYNC scheduler to produce an emerging property of memory in a system of oblivious mobile robots, and why it is not a problem when robots executing the gathering algorithms are equipped with lights.}
}
@article{DOSSANTOS2023104382,
title = {Parallel multi-speed Pursuit-Evasion Game algorithms},
journal = {Robotics and Autonomous Systems},
volume = {163},
pages = {104382},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104382},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000210},
author = {Renato F. {dos Santos} and Ragesh K. Ramachandran and Marcos A.M. Vieira and Gaurav S. Sukhatme},
keywords = {Pursuit-Evasion Game, Heterogeneous robots, Optimal path, Multi-agent systems},
abstract = {Pursuit-Evasion Game (PEG) consists of a team of pursuers trying to capture one or more evaders. PEG is important due to its application in surveillance, search and rescue, disaster robotics, boundary defense and so on. In general, PEG requires exponential time to compute the minimum number of pursuers to capture an evader. To mitigate this, we have designed a parallel optimal algorithm to minimize the capture time in PEG. Given a discrete topology, this algorithm also outputs the minimum number of pursuers to capture an evader. We also extended parallel algorithm to consider other versions as: heterogeneous/multi-speed players; the pac-dot technique to increase evader lifetime in a game; and a pruning strategy for pac-dot technique to increase the scalability. The parallel algorithm performance was evaluated by speedup metric and this algorithm and its extensions were simulated and evaluated on many different topologies to validate the viability of our algorithms by discussing and evaluating a set of simulation results. The parallel algorithm enables us to scale up to 8.13 times with 8 cores compared to state-of-the-art. Considering the complexity of the state space growing up, pruning technique to pac-dot algorithm minimizes the state space and generated transition, and can handle a large number of states (≈830 M) and transitions (≈11 G) generated. In general, our algorithms increase the scalability and make it feasible to compute the PEG optimal strategy for more realistic cases.}
}
@article{KHEIRANDISH2023104343,
title = {A fault-tolerant sensor fusion in mobile robots using multiple model Kalman filters},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104343},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104343},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002329},
author = {M. Kheirandish and E. Azadi Yazdi and H. Mohammadi and M. Mohammadi},
keywords = {Fault-tolerant, Localization, Sensor fusion, Mobile robots, Kalman filters, Multiple model Kalman filters, IMU, Encoders, Fault detection, Isolation},
abstract = {Accurate localization is crucial in the navigation of mobile robots. However, in other circumstances, single-sensor localization faces different challenges, including software and hardware problems or data outages. Sensor fusion is used in most autonomous vehicles (including aerial and ground vehicles) to overcome such challenges. In this paper, the localization of a mobile robot is studied in the presence of sensor faults. The mobile robot has two sensors: two Inertial Measurement Units (IMU) and wheel encoders. Regarding the fault-tolerant scheme, measurements of both sets of sensors are fused using an Interacting Multiple Model (IMM) Kalman filter based on both unscented and extended Kalman filters (UKF and EKF). UKF and EKF-based IMM are chosen for this study since the dynamic model of the localization is highly nonlinear. Regarding contributions, it should be noted that this scheme eliminates the need to model every single fault scenario and use an additional sensor to oversee the performance of the sensing system. Also, comparing this method with similar approaches adopted by other studies shows better performance regarding the cost of computations and RMSE. To evaluate performance, the outputs of the proposed filters are simulated and compared for different trajectories where the data of each sensor is intentionally corrupted to observe the fault detection capability. Simulations are performed for different trajectories and noises to demonstrate this method’s efficiency in different situations. In addition, the results of unscented and extended Kalman filter-based IMM are compared in terms of error and computational costs to evaluate their performance. Overall, simulation and experiments indicate accurate 3D estimations in all cases. Moreover, designated weights vividly show that sensor fault detection is achieved by both unscented and extended IMM Kalman filters, which enable complete fault isolation consequently. This approach provides mobile robots with a reliable and straightforward sensor fault detection and localization solution.}
}
@article{DALZILIO2023104301,
title = {A formal toolchain for offline and run-time verification of robotic systems},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104301},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104301},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001907},
author = {Silvano {Dal Zilio} and Pierre-Emmanuel Hladik and Félix Ingrand and Anthony Mallet},
keywords = {Robotic software engineering, Formal verification, Model checking, Runtime verification, UAV controller},
abstract = {Validation and Verification (V&V) of autonomous robotic system software is becoming a critical issue. Among the V&V techniques at our disposal, formal approaches are among the most rigorous and trustworthy ones. Yet, the level of skills and knowledge required to use and deploy formal methods is usually quite high and rare. In this paper, we describe an approach that starts from a regular, but rigorous, framework to specify and deploy robotic software components, which can also automatically synthesize a formal model of these components. We describe how we can execute the resulting formal model, in place of a traditional implementation, and show how this provides the opportunity to add powerful monitoring and runtime verification capabilities to a system, e.g., to prevent collisions, or trigger an emergency landing. Since the runtime used to execute formal models is specifically designed to be faithful to their semantics, every execution (in the implementation) can be mapped to a trace in the specification. As a result, we can also prove many interesting properties offline, using model-checking techniques. We give several examples, such as properties about schedulability, worst-case traversal time, or mutual exclusion. We believe that having a consistent workflow, from an initial specification of our system, down to a formal, executable specification is a major advance in robotics and opens the way for verification of functional components of autonomous robots and beyond. We illustrate this claim by describing a complete example based on a genuine drone flight controller.}
}
@article{MAHATO2023104270,
title = {Consensus-based fast and energy-efficient multi-robot task allocation},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104270},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104270},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001592},
author = {Prabhat Mahato and Sudipta Saha and Chayan Sarkar and Md. Shaghil},
keywords = {Multi-robot task allocation, Synchronous transmission, Multi-robot communication, Consensus-based MRTA, Energy-efficient MRTA},
abstract = {In a multi-robot system, the appropriate allocation of the tasks to the individual robots is a very significant component. The availability of a centralized infrastructure can guarantee an optimal allocation of the tasks. However, in many important scenarios such as search and rescue, exploration, disaster-management, war-field, etc., on-the-fly allocation of the dynamic tasks to the robots in a decentralized fashion is the only possible option. Efficient communication among the robots plays a crucial role in any such decentralized setting. Existing works on distributed Multi-Robot Task Allocation (MRTA) either assume that the network is available or a naive communication paradigm is used. On the contrary, in most of these scenarios, the network infrastructure is either unstable or unavailable and ad-hoc networking is the only resort. Recent developments in synchronous-transmission (ST) based wireless communication protocols are shown to be more efficient than the traditional asynchronous transmission-based protocols in ad hoc networks such as Wireless Sensor Network (WSN)/Internet of Things (IoT) applications. The current work is the first effort that utilizes ST for MRTA. Specifically, we propose an algorithm that efficiently adapts ST-based many-to-many interaction and minimizes the information exchange to reach a consensus for task allocation. We showcase the efficacy of the proposed algorithm through an extensive simulation-based study of its latency and energy-efficiency under different settings.}
}
@article{ROJASRODRIGUEZ2023104401,
title = {Whole body motion generation with centroidal dynamics of legged robots using sequential bounds tightening of McCormick envelopes},
journal = {Robotics and Autonomous Systems},
volume = {164},
pages = {104401},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104401},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000404},
author = {Jose C. Rojas-Rodriguez and Ana Y. Aguilar-Bustos and Eusebio Bugarin},
keywords = {Centroidal dynamics, Legged robots, Sequential convex programming, McCormick envelopes, Bounds tightening, Quadratic programming},
abstract = {In this paper we introduce a Sequential Convex Programming (SCP) algorithm for the motion generation with the centroidal dynamics of legged robots using a sequential bounds tightening of McCormick envelopes strategy to cope with the nonconvexity of the problem (related to bilinear terms). Therefore, the proposed SCP algorithm is initialized with relaxed McCormick envelopes and then their bounds are sequentially tightened around the current estimate of the solution enforcing this way convergence to a feasible point. The SCP algorithm solves a quadratic program at each iteration by an interior point method. Additionally, the proposed SCP algorithm is alternated with an inverse kinematics algorithm to achieve the whole body motion generation. Finally, extensive numerical experiments show the effectiveness of the proposed algorithm in generating highly agile motions such as trotting, bounding, stotting and running for humanoid and quadruped robots.}
}
@article{QIU2023104348,
title = {Precision fingertip grasp: A human-inspired grasp planning and inverse kinematics approach for integrated arm–hand systems},
journal = {Robotics and Autonomous Systems},
volume = {162},
pages = {104348},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104348},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002378},
author = {Shuwei Qiu and Mehrdad R. Kermani},
keywords = {Grasp planning, Inverse kinematics, Integrated arm–hand systems},
abstract = {In this paper, we present a human-inspired approach to solve grasp planning and inverse kinematics (IK) problems, simultaneously. Our proposed solution is for integrated arm–hand systems. Conventional approaches consider the robot manipulator (arm) and the robotic hand separately and solve the problems of grasp planning and IK in sequence. Such separate considerations of the arm and hand often introduce errors in the IK solution. The sequential approaches waste significant computational power in searching for infeasible grasps. To address these issues, we propose to consider the robotic arm and the hand as a kinematically integrated system. We then introduce a coarse-to-fine strategy to solve grasp planning and IK problems simultaneously. The proposed approach achieves force-closure fingertip grasping without using reachability information a priori. Instead, through an integrated grasp planning and IK solution, the reachability information is obtained from the IK solution and is used to filter out infeasible grasps. This strategy will dramatically reduce the search space and save significant computational power. Numerical examples will be used to demonstrate the efficiency of the proposed approach, in comparison to a sequential solution of the grasp planning and IK for integrated arm–hand systems.}
}
@article{TAN2022104267,
title = {Path tracking control strategy for off-road 4WS4WD vehicle based on robust model predictive control},
journal = {Robotics and Autonomous Systems},
volume = {158},
pages = {104267},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104267},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001567},
author = {Qifan Tan and Cheng Qiu and Jing Huang and Yue Yin and Xinyu Zhang and Huaping Liu},
keywords = {Mobile robot, 4WS4WD vehicle, Path tracking, Model Predictive Control, Robustness},
abstract = {With the increasing requirements for vehicle adaptability and maneuverability in various road environments, four-wheel-steering four-wheel-drive (4WS4WD) vehicles have attracted wider attention. This paper presents a robust model predictive control-based strategy for the path tracking of 4WS4WD vehicles considering external disturbances. The strategy combines model predictive control (MPC) and control allocation under an upper–lower structure. The main objective of the present work is to improve the robustness and stability of path tracking by developing an MPC algorithm in the upper layer. The controller design considers general disturbances caused by allocation errors and sudden disturbances caused by an outer force in the offset model. Based on the offset model, a robust MPC control law is obtained by converting the robustness constraints into a linear matrix inequality. The control law is mathematically demonstrated to be stable in multidisturbed conditions via the Lyapunov stability theorem. Through comparison with a similar control algorithm of path tracking and applying it on different uneven ground conditions, the proposed robust algorithm is found to effectively overcome disturbances on the system.}
}
@article{DAHIYA2023104335,
title = {A survey of multi-agent Human–Robot Interaction systems},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104335},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104335},
url = {https://www.sciencedirect.com/science/article/pii/S092188902200224X},
author = {Abhinav Dahiya and Alexander M. Aroyo and Kerstin Dautenhahn and Stephen L. Smith},
keywords = {Human–Robot Interaction (HRI), Multi-agent system, Robots in groups, Human–robot teams},
abstract = {This article presents a survey of literature in the area of Human–Robot Interaction (HRI), specifically on systems containing more than two agents (i.e., having multiple humans and/or multiple robots). We identify three core aspects of “Multi-agent” HRI systems that are useful for understanding how these systems differ from dyadic systems and from one another. These are the Team structure, Interaction style among agents, and the system’s Computational characteristics. Under these core aspects, we present five attributes of HRI systems, namely Team size, Team composition, Interaction model, Communication modalities, and Robot control. These attributes are used to characterize and distinguish one system from another. We populate resulting categories with examples from the recent literature along with a brief discussion of their applications. We also analyze how these attributes in multi-agent systems differ from the case of dyadic human–robot systems. Through this survey, we summarize key observations from the current literature, and identify challenges and promising areas for future research in this domain. In order to realize the vision of robots being part of the society and interacting seamlessly with humans, there is a need to expand research on multi-human–multi-robot systems. Not only do these systems require coordination among several agents, they also involve multi-agent and indirect interactions which are absent from dyadic HRI systems. Including multiple agents in HRI systems requires more advanced interaction schemes, behavior understanding and control methods to allow natural interactions among humans and robots. In addition, research on human behavioral understanding in mixed human–robot teams also requires more attention. This will help formulate and implement effective robot control policies in HRI systems with large numbers of heterogeneous robots and humans; a team composition reflecting many real-world scenarios.}
}
@article{MARTINELLI2023104324,
title = {A resilient solution to Range-Only SLAM based on a decoupled landmark range and bearing reconstruction},
journal = {Robotics and Autonomous Systems},
volume = {160},
pages = {104324},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104324},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002135},
author = {Francesco Martinelli and Simone Mattogno and Fabrizio Romanelli},
keywords = {Resilience, Sensor fusion, RO-SLAM, UWB, EKF},
abstract = {A Range Only Simultaneous Localization and Mapping (RO-SLAM) problem is considered in this paper. The robot is a unicycle like vehicle equipped with encoders on the actuated wheels, which measures the distance to a set of UWB landmarks located in unknown position in the surrounding. A Multi Hypotheses Extended Kalman Filter (MHEKF), one for each landmark, is designed to dynamically estimate the range and the bearing of the observed landmark. These estimates, regarded as measurements with a proper covariance matrix, are used in an EKF SLAM algorithm, endowed with a resilient module to discern and possibly to temporarily discard landmarks with an unreliable range and bearing estimate. This allows to cope with the initial uncertainty characterizing the bearing reconstruction, but also to resist the effects of outliers and to detect possible abnormal situations. Simulation and experimental results illustrate the effectiveness of the proposed approach compared to other methods available in the literature, especially in case of significant perturbations, like the sudden and unmodeled shift of the landmarks.}
}
@article{KAWAMURA2023104369,
title = {Hierarchical mixture of experts for autonomous unmanned aerial vehicles utilizing thrust models and acoustics},
journal = {Robotics and Autonomous Systems},
volume = {162},
pages = {104369},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104369},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000088},
author = {Evan Kawamura and Dilmurat Azimov and John S. Allen and Corey Ippolito},
keywords = {State estimation, Acoustics, Hierarchical mixture of experts, Unmanned aerial vehicle},
abstract = {Accurate position, velocity, attitude, and angular velocity state estimation is crucial for unmanned aerial vehicles, especially in enabling them with autonomous capabilities. It is necessary to adequately model and account for all the environmental and dynamic flight parameters. A hierarchical mixture of experts (HME) framework has been viable in improving state estimation accuracy in interplanetary orbit determination problems, and this paper proposes an extension for quadcopters. It is shown that the state and motor angular velocity estimation accuracy can be significantly improved by processing different thrust models, and acoustic parameters have an important, previously unreported, role in this improvement. Higher motor angular velocities produce higher noise levels, and thus, the relationships of the onboard acoustic measurements to the vehicle state parameters play an essential part in estimation. The motor angular velocities’ estimations depend on the extended Kalman filter solutions or an acoustic curve fit. The experts in the HME framework utilize the state estimation solutions from the extended Kalman filters and the motor angular velocity estimations to compare against the telemetry data as truth. The overall HME solution is compared against a non-acoustic static thrust model. Illustrative examples and analysis presented in this paper reveal that the proposed estimation solutions can also apply to other flight vehicles for onboard real-time implementation to leverage autonomy.}
}
@article{SHIBATA2023104307,
title = {Deep reinforcement learning of event-triggered communication and consensus-based control for distributed cooperative transport},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104307},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104307},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001968},
author = {Kazuki Shibata and Tomohiko Jimbo and Takamitsu Matsubara},
keywords = {Cooperative transport, Multi-agent reinforcement learning, Event-triggered control, Consensus algorithm},
abstract = {In this paper, we present a solution to a design problem of control strategies for multi-agent cooperative transport. Although existing learning-based methods assume that the number of agents is the same as that in the training environment, the number might differ in reality considering that the robots’ batteries may completely discharge, or additional robots may be introduced to reduce the time required to complete a task. Therefore, it is crucial that the learned strategy be applicable to scenarios wherein the number of agents differs from that in the training environment. In this paper, we propose a novel multi-agent reinforcement learning framework of event-triggered communication and consensus-based control for distributed cooperative transport. The proposed policy model estimates the resultant force and torque in a consensus manner using the estimates of the resultant force and torque with the neighborhood agents. Moreover, it computes the control and communication inputs to determine when to communicate with the neighboring agents under local observations and estimates of the resultant force and torque. Therefore, the proposed framework can balance the control performance and communication savings in scenarios wherein the number of agents differs from that in the training environment. We confirm the effectiveness of our approach by using a maximum of eight and six robots in the simulations and experiments, respectively.}
}
@article{LIU2023104294,
title = {A survey of Semantic Reasoning frameworks for robotic systems},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104294},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104294},
url = {https://www.sciencedirect.com/science/article/pii/S092188902200183X},
author = {Weiyu Liu and Angel Daruna and Maithili Patel and Kartik Ramachandruni and Sonia Chernova},
keywords = {Semantic reasoning, Robotics, Knowledge bases},
abstract = {Robots are increasingly transitioning from specialized, single-task machines to general-purpose systems that operate in diverse and dynamic environments. To address the challenges associated with operation in real-world domains, robots must effectively generalize knowledge, learn, and be transparent in their decision making. This survey examines Semantic Reasoning techniques for robotic systems, which enable robots to encode and use semantic knowledge, including concepts, facts, ideas, and beliefs about the world. Continually perceiving, understanding, and generalizing semantic knowledge allows a robot to identify the meaningful patterns shared between problems and environments, and therefore more effectively perform a wide range of real-world tasks. We identify the three common components that make up a computational Semantic Reasoning Framework: knowledge sources, computational frameworks, and world representations. We analyze the existing implementations and the key characteristics of these components, highlight the many interactions that occur between them, and examine their integration for solving robotic tasks related to five aspects of the world, including objects, spaces, agents, tasks, and actions. By analyzing the computational formulation and underlying mechanisms of existing methods, we provide a unified view of the wide range of semantic reasoning techniques and identify open areas for future research.}
}
@article{LAN2023104385,
title = {Efficient reinforcement learning with least-squares soft Bellman residual for robotic grasping},
journal = {Robotics and Autonomous Systems},
volume = {164},
pages = {104385},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104385},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000246},
author = {Yixing Lan and Junkai Ren and Tao Tang and Xin Xu and Yifei Shi and Zixin Tang},
keywords = {Reinforcement learning, Policy evaluation, Robotic grasping, Sparse kernel},
abstract = {Grasping control of intelligent robots has to deal with the difficulties of model uncertainties and nonlinearities. In this paper, we propose the Kernel-based Least-Squares Soft Bellman residual Actor–Critic (KLSAC) algorithm for robotic grasping. In the proposed approach, a novel linear temporal-difference learning algorithm using the least-squares soft Bellman residual (LS2BR) method is designed for policy evaluation. In addition, KLSAC adopts a sparse-kernel feature representation method based on approximate linear dependency (ALD) analysis to construct features for continuous state–action space. Compared with typical deep reinforcement learning algorithms, KLSAC has two main advantages: firstly, the critic module has the capacity for rapid convergence by computing the fixed point of the linear soft Bellman equation via the least-squares optimization method. Secondly, the kernel-based features construction approach only requires predefining the basic kernel function and can improve the generalization ability of KLSAC. The simulation studies on robotic grasping control were conducted in the V-REP simulator. The results demonstrate that compared with other typical RL algorithms (e.g., SAC and BMPO), the proposed KLSAC algorithm can achieve better performance in terms of sample efficiency and asymptotic convergence property. Furthermore, experimental results on a real UR5 robot validated that KLSAC performed well in the real world.}
}
@article{ZHANG2023104312,
title = {A neural network based framework for variable impedance skills learning from demonstrations},
journal = {Robotics and Autonomous Systems},
volume = {160},
pages = {104312},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104312},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002019},
author = {Yu Zhang and Long Cheng and Ran Cao and Houcheng Li and Chenguang Yang},
keywords = {Variable impedance skill, Learning from demonstrations, Skills learning, Human–robot interaction},
abstract = {Robots are becoming standard collaborators not only in factories, hospitals, and offices, but also in people’s homes, where they can play an important role in situations where a human cannot complete a task alone or needs the help of another person (i.e., collaborative tasks). Variable impedance control with contact forces is critical for robots to successfully perform such manipulation tasks, and robots should be equipped with adaptive capabilities because conditions vary significantly for different robotic tasks in dynamic environments. This can be achieved by learning human motion capabilities and variable impedance skills. In this paper, a neural-network-based framework for learning variable impedance skills is proposed. The proposed approach builds the full stiffness function with the acquired forces and position learned from demonstrations, and then is used together with the sensed data to achieve the variable impedance control. The proposed algorithm can adapt to unknown situations that change the learned motion skill as needed (e.g., adapt to intermediate via-points or avoid obstacles). The proposed framework consists of two parts: Learning motion features and learning impedance features. The motion features learning is validated by reproducing, generalizing, and adapting to transit points and avoiding obstacles in the LASA dataset. Impedance features learning is validated based on a virtual variable stiffness system that achieves higher accuracy (approximately 90%) compared to traditional methods in a manual dataset, and the whole framework is validated through a co-manipulation task between a person and the Franka Emika robot.}
}
@article{DONG2023104283,
title = {Online pole segmentation on range images for long-term LiDAR localization in urban environments},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104283},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104283},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001725},
author = {Hao Dong and Xieyuanli Chen and Simo Särkkä and Cyrill Stachniss},
keywords = {Localization, Pole, LiDAR, Range image, Mapping, Autonomous driving, Deep learning, Semantic segmentation},
abstract = {Robust and accurate localization is a basic requirement for mobile autonomous systems. Pole-like objects, such as traffic signs, poles, and lamps are frequently used landmarks for localization in urban environments due to their local distinctiveness and long-term stability. In this paper, we present a novel, accurate, and fast pole extraction approach based on geometric features that runs online and has little computational demands. Our method performs all computations directly on range images generated from 3D LiDAR scans, which avoids processing 3D point clouds explicitly and enables fast pole extraction for each scan. We further use the extracted poles as pseudo labels to train a deep neural network for online range image-based pole segmentation. We test both our geometric and learning-based pole extraction methods for localization on different datasets with different LiDAR scanners, routes, and seasonal changes. The experimental results show that our methods outperform other state-of-the-art approaches. Moreover, boosted with pseudo pole labels extracted from multiple datasets, our learning-based method can run across different datasets and achieve even better localization results compared to our geometry-based method. We released our pole datasets to the public for evaluating the performance of pole extractors, as well as the implementation of our approach.}
}
@article{SZOTS2023104360,
title = {Optimal strategies of a pursuit-evasion game with three pursuers and one superior evader},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104360},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104360},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002494},
author = {János Szőts and István Harmati},
keywords = {Game theory, Differential games, Pursuit-evasion, Optimal trajectories, Optimal strategies},
abstract = {We attempt to solve the pursuit-evasion game of a faster evader being surrounded by three pursuers. The complexity of the game under study stems from the holonomic motion of the agents. This game has not been solved either in the sense of presenting optimal trajectories or in the sense of feedback strategies. There exist heuristic strategies, and solutions to similar but simpler games which will be of use. We present a solution for the optimal trajectories of the game, but we do not prove optimality. Then, we synthesize feedback strategies for both parties based on the proposed trajectories.}
}
@article{CHAKRABORTY2023104336,
title = {External six-bar mechanism rehabilitation device for index finger: Development and shape synthesis},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104336},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104336},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002251},
author = {Debaditya Chakraborty and Ayush Rathi and Ramanpreet Singh and Vimal Kumar Pathak and Ashish Kumar Srivastava and Abhishek Sharma and Kuldeep K. Saxena and Gaurav Kumar and Sandeep Kumar},
keywords = {Rehabilitation, Shape synthesis, FEA, Flexion/extension, Biomechanics, CAE (Computer Aided Engineering)},
abstract = {This work proposes a novel external Stephenson-III six-bar mechanism-based rehabilitation device. This device has been designed to rehabilitate the patient’s finger for the action of grabbing, also known as flexion and extension. A predefined trajectory is used to synthesize the mechanism using TeachingLearning-Optimization algorithm (TLBO) and Particle swarm optimization algorithm (PSO). The trajectory data was obtained after image processing by grabbing a particular object 30 times to record the flexion and extension motion of the index finger. An optimization problem was formulated and results of TLBO and PSO were compared. The mechanism obtained from TLBO algorithm was deemed better in terms of precision and feasible configuration. Using clinical biomechanical data for flexion/extension of index finger, position and static force analysis are performed. The CAD model of the mechanism was then tested for feasibility in a CAD/Software. Excess mass was removed using topology optimization and a 20% mean reduction for every link was achieved. An index finger rehabilitation device employing an external six-bar mechanism was obtained, that would help a patient with motor control loss to rehabilitate and bring normalcy to life. The design of exoskeleton was able to match the trajectory of the index. Shape synthesis ensured a 20% reduction in overall mass of the linkages.}
}
@article{WAKABAYASHI2023104320,
title = {Dynamic obstacle avoidance for Multi-rotor UAV using chance-constraints based on obstacle velocity},
journal = {Robotics and Autonomous Systems},
volume = {160},
pages = {104320},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104320},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002093},
author = {Takumi Wakabayashi and  {Yukimasa Suzuki} and Satoshi Suzuki},
keywords = {Chance constraints, Velocity obstacles, Collision avoidance, Formation flight, Path planning},
abstract = {To ensure the safety of autonomous Multi-rotor UAVs flying in urban airspace, they should be capable of avoiding collisions with unpredictable dynamic obstacles, such as birds. UAVs must consider both relative position and relative velocity to avoid moving obstacles. Model predictive control (MPC) can consider the multiple collision avoidance constraints in a constrained optimisation framework. This study proposes a chance-constraints based on obstacle velocity (CCOV) method, which can be combined with previous positional chance constraint methods to account for uncertainty in both position and velocity. This effectively prevents collision with high-velocity obstacles, even in a noisy environment. The proposed method has been performed on a numerical simulation built in MATLAB.}
}
@article{MUNOZ2023104325,
title = {OGATE: A framework for autonomous controllers assessment},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104325},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104325},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002147},
author = {Pablo Muñoz and Amedeo Cesta and Andrea Orlandini and María D. R-Moreno},
keywords = {Autonomous robots, Planning & scheduling, Planning & execution, Performance metrics},
abstract = {Autonomous robots can face a variety of applications integrating Artificial Intelligence (AI) techniques, for instance Planning & Scheduling (P&S). While robotics and planning systems are commonly well assessed through test benchs and metrics, in autonomous robots literature it is usual to present isolated case studies for evaluating such works. For instance, experiments are presented in very specific circumstances and often the data provided is not enough to enable a characterization of the autonomous features performance, providing only a demonstration of effectiveness. The main issue is the absence of a framework to assess autonomous controllers from a general perspective. We propose a research focused on a set of general applicable metrics to enable assessment of autonomous controllers. In this way, our objective is to analyse the deliberation and reaction capabilities of an autonomous robot in real operative scenarios. Such metrics are implemented in OGATE, a domain independent tool that automatically carries on with controllers’ testing, generating objective and reproducible performance assessments. To test this framework we have used two autonomous controllers that rely on different technologies for P&S. Results show that we are able to obtain relevant data, enabling the characterization of different P&S integration in robotics.}
}
@article{MARTINEZHERNANDEZ2023104353,
title = {Human-in-the-loop layered architecture for control of a wearable ankle–foot robot},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104353},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104353},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002421},
author = {Uriel Martinez-Hernandez and Sina Firouzy and Pouyan Mehryar and Lin Meng and Craig Childs and Arjan Buis and Abbas A. Dehghani-Sanij},
keywords = {Layered architectures, Autonomous systems, Bayesian inference, Sensorimotor control},
abstract = {Intelligent wearable robotics is a promising approach for the development of devices that can interact with people and assist them in daily activities. This work presents a novel human-in-the-loop layered architecture to control a wearable robot while interacting with the human body. The proposed control architecture is composed of high-, mid- and low-level computational and control layers, together with wearable sensors, for the control of a wearable ankle–foot robot. The high-level layer uses Bayesian formulation and a competing accumulator model to estimate the human posture during the gait cycle. The mid-level layer implements a Finite State Machine (FSM) to prepare the control parameters for the wearable robot based on the decisions from the high-level layer. The low-level layer is responsible for the precise control of the wearable robot over time using a cascade proportional–integral–derivative (PID) control approach. The human-in-the-loop layered architecture is systematically validated with the control of a 3D printed wearable ankle–foot robot to assist the human foot while walking. The assistance is applied lifting up the human foot when the toe-off event is detected in the walking cycle, and the assistance is removed allowing the human foot to move down and contact the ground when the heel-contact event is detected. Overall, the experiments in offline and real-time modes, undertaken for the validation process, show the potential of the human-in-the-loop layered architecture to develop intelligent wearable robots capable of making decisions and responding fast and accurately based on the interaction with the human body.}
}
@article{QUAN2023104368,
title = {Distributed control for a robotic swarm to pass through a curve virtual tube},
journal = {Robotics and Autonomous Systems},
volume = {162},
pages = {104368},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104368},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000076},
author = {Quan Quan and Yan Gao and Chenggang Bai},
keywords = {Virtual tube, Passing-through control, Robotic swarm, Vector field},
abstract = {To guide a robotic swarm in a cluttered environment, a curve virtual tube is designed in this paper. There is no obstacle within the curve virtual tube, and the area inside can be seen as a safety zone. Then, a distributed swarm controller is proposed with three elaborate control terms. Formal analyses and proofs show that the curve virtual tube passing-through control problem can be solved in a finite time. For convenience in practical use, a modified controller with an approximate control performance is put forward. Some control laws for different types of robots to track their vector fields are also presented. The effectiveness of the proposed method is validated by numerical simulations and real experiments. To show the advantages of the proposed method, the comparisons between our method and other methods are also presented.}
}
@article{PASSOS2023104284,
title = {Congestion control algorithms for robotic swarms with a common target based on the throughput of the target area},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104284},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104284},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001737},
author = {Yuri Tavares dos Passos and Xavier Duquesne and Leandro Soriano Marcolino},
keywords = {Robotic swarm, Common target, Throughput, Congestion, Traffic control},
abstract = {When a large number of robots try to reach a common area, congestions happen, causing severe delays. To minimise congestion in a robotic swarm system, traffic control algorithms must be employed in a decentralised manner. Based on strategies aimed to maximise the throughput of the common target area, we developed two novel algorithms for robots using artificial potential fields for obstacle avoidance and navigation. One algorithm is inspired by creating a queue to get to the target area (Single Queue Former — SQF), while the other makes the robots touch the boundary of the circular area by using vector fields (Touch and Run Vector Fields — TRVF). We performed simulation experiments to show that the proposed algorithms are bounded by the throughput of their inspired theoretical strategies and compare the two novel algorithms with state-of-art algorithms for the same problem (PCC, EE and PCC–EE). The SQF algorithm significantly outperforms all other algorithms for a large number of robots or when the circular target region radius is small. TRVF, on the other hand, is better than SQF only for a limited number of robots and outperforms only PCC for numerous robots. However, it allows us to analyse the potential impacts on the throughput when transferring an idea from a theoretical strategy to a concrete algorithm that considers changing linear speeds and distances between robots.}
}
@article{PARK2023104396,
title = {Heel-strike and toe-off walking of humanoid robot using quadratic programming considering the foot contact states},
journal = {Robotics and Autonomous Systems},
volume = {163},
pages = {104396},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104396},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000350},
author = {Beomyeong Park and Jaeheung Park},
keywords = {Humanoid robots, Legged locomotion, Quadratic programming, Model predictive control, Optimal control, Trajectory optimization},
abstract = {Heel-strike and toe-off (heel-toe) walking has been studied to increase step length, reduce the torque of the leg joints, or make robots walk similarly to humans. To realize heel-toe walking, it is necessary to determine the foot angle and ensure contact between the ground and the heel and toe. The foot angle for heel-toe walking can be analytically calculated considering the position of the center of mass (CoM) before the foot lands or rises. Therefore, the trajectory of the CoM of one cycle of walking must be known in advance. However, this method cannot be easily incorporated with model predictive control (MPC) to generate the trajectory of CoM in real time. This paper proposes a heel-toe walking method that can be used with the CoM trajectory generated using the MPC scheme. The CoM trajectory generation method that reduced the velocity fluctuation using MPC, which was proposed in a previous study, was used. The stability of the MPC scheme is proved in this paper. The quadratic programming is used to generate the heel-toe walking by considering the foot contact states as the constraints. The increase in step length and the decrease in singularity occurrence due to heel-toe walking were compared and analyzed in the simulation. The experiment verified the proposed heel-toe method.}
}
@article{CHEHELGAMI2023104384,
title = {Safe deep learning-based global path planning using a fast collision-free path generator},
journal = {Robotics and Autonomous Systems},
volume = {163},
pages = {104384},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104384},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000234},
author = {Shirin Chehelgami and Erfan Ashtari and Mohammad Amin Basiri and Mehdi {Tale Masouleh} and Ahmad Kalhor},
keywords = {Mobile robots, Deep learning in robotics and automation, Recurrent neural network, Fast global path planner, Safe path generator},
abstract = {In this research, a global path planning method based on recurrent neural networks by means of a new Loss function is presented, which regardless of the complexity of the configuration space, generates the path in a relatively constant time. The new Loss function is defined in such a way that in addition to learning the input data of the network, it creates an adjustable safety margin around the obstacles and ultimately creates a safe path. Moreover, a new global path planning method is also introduced, which is used to create the dataset required to train the proposed neural network. The convergence of this method is mathematically proven and it is shown that this method can also produce a suboptimal path in a much shorter time than the common methods of global path planning reported in the literature. In short, the main purpose of this research consists in providing a method which can create a suboptimal, fast and safe path for a mobile robot from any random starting point to any random destination in a known environment. First, the proposed methods will be implemented for different two-dimensional environments consisting of convex and non-convex obstacles, considering the robot as a point-mass, and then it will be implemented in a simulation environment, AI2THOR. Compared to classical global path planning algorithms, such as RRT and A*, the proposed approach demonstrates better performance in complex and challenging environments.}
}
@article{PI2023104352,
title = {Social interaction model enhanced with speculation stage for human trajectory prediction},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104352},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104352},
url = {https://www.sciencedirect.com/science/article/pii/S092188902200241X},
author = {Lei Pi and Qiang Zhang and Lingfang Yang and Zhi Huang},
keywords = {Human trajectory prediction, Social interactive feature, STGCNN, Attention mechanism, Speculation},
abstract = {Accurate human trajectory prediction is still challenging due to the complicated interactions with surroundings. A Spatio-Temporal Graph Convolution Neural Network based Social Interaction Model (STGCNN-SIM) is proposed to address this challenge. In addition to historical trajectory information, the presented method employs the speculated trajectories in the future to extract social interactive features and model interaction behaviors. Three social interactive features are extracted explicitly from the observed and speculated trajectories: (1) the relative distance, (2) the angle between the velocity vectors of two interacting partners, and (3) the angles between the velocity vectors of interacting partners and the distance vector. STGCNN-SIM utilizes these social interactive features to model interactions with surroundings in the historical and speculated stages. Then an attention mechanism is adopted to improve the model by focusing on more relevant features. Experimental results on three public datasets demonstrate that STGCNN-SIM achieves higher accuracy and stability than the state-of-the-art methods.}
}
@article{LU2023104341,
title = {Disturbance-aware reinforcement learning for rejecting excessive disturbances},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104341},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104341},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002305},
author = {Wenjie Lu and Manman Hu},
keywords = {Reinforcement learning, Disturbance rejection, Disturbance observer},
abstract = {This paper presents a disturbance-aware Reinforcement Learning (RL) approach for stabilizing a free-floating platform under excessive external disturbances. In particular, we consider the scenarios where disturbances frequently exceed actuator limits and largely affect the dynamics characterizing the disturbed platform. This stabilization problem is better described by a set of Unknown Partially Observable Markovian Decision Processes (POMDPs), as opposed to a single-POMDP formulation, making online disturbance awareness necessary. This paper proposes a new Disturbance-Observer network (DO-net) that mimics prediction procedures through an auxiliary Gated Recurrent Unit (GRU), for the purpose of estimating and encoding the disturbance states and the disturbance transition functions, respectively. Then the controller subnetwork is trained with joint optimization of the observer subnetwork in an RL manner for mutual robustness and runtime efficiency. Numerical simulations on position regulation tasks have demonstrated that the DO-net outperforms the DOB-net and reduces the gap with an ideal performance estimate, the latter of which is obtained by a commercial solver given precise disturbance knowledge.}
}
@article{YIN2022104268,
title = {Spring-linkage integrated mechanism design for jumping robots},
journal = {Robotics and Autonomous Systems},
volume = {158},
pages = {104268},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104268},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001579},
author = {Xuanchun Yin and Jinchun Yan and Sheng Wen and Jiantao Zhang},
keywords = {Jumping robots, Dynamic modeling, Hart linkage, Multiple springs, Legged robots},
abstract = {Legged robots can negotiate unstructured environments and have applications in education, environmental inspection, space exploration, and cargo transportation. As a powerful form of legged robots, jumping robots have attractive features due to their high efficiency, mobility, traverse obstacles, and low cost of transport. Although spring is the core component of the energy system, little related work explores the value selection of spring stiffness and the effect of different spring placements for jumping robots. In this article, we present a systematic method to select the stiffness of spring based on static analysis, jumping linkage configuration and multi-objective optimization for jumping robots. Also, to predict the motion behavior of jumping robots, we provide a comprehensive dynamic model of the robotic jumping in different phases according to the Lagrange method and the principle of virtual work, which considers the motion constraints and configuration constraints simultaneously. The proposed method and dynamic model can validate by designing a spring-linkage-based jumping robot as a showcase. The experimental results show performance improvements in jumping height in terms of both different springs’ stiffness and arrangement, which is possible to appraise a maximal enhancement of 57.88%.}
}
@article{GUO2023104271,
title = {Audio–visual language instruction understanding for robotic sorting},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104271},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104271},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022001609},
author = {Di Guo and Huaping Liu and Fuchun Sun},
keywords = {Audio–visual perception, Referring expression, Robotic sorting},
abstract = {For robot in human environment, it has always been expected that the robot can execute specified tasks following language instructions. Most current methods only rely on visual perception to understand the language instruction, while it may be not sufficient to fully interpret some language instructions when visually identical objects exist. In this paper, we propose a task of audio–visual language instruction understanding for robotic sorting, in which the robot is able to use both the visual and audio information to fully understand and execute the given instruction. To solve the proposed task, an audio–visual fusion framework is developed, which combines the visual localization and audio recognition models together for the robotic sorting task following language instruction. We have also collected a multimodal dataset for evaluation, and extensive experiments are conducted within the dataset and generalized to new scenarios in physical world demonstrating the effectiveness of the proposed framework.}
}
@article{SPLIETKER2023104337,
title = {Rendering the Directional TSDF for Tracking and Multi-Sensor Registration with Point-To-Plane Scale ICP},
journal = {Robotics and Autonomous Systems},
volume = {162},
pages = {104337},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104337},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002263},
author = {Malte Splietker and Sven Behnke},
keywords = {SLAM, TSDF, Surface orientation, ICP, Frame-to-keyframe, Point-to-plane (3)},
abstract = {Dense real-time tracking and mapping from RGB-D images is an important tool for many robotic applications, such as navigation and manipulation. The recently presented Directional Truncated Signed Distance Function (DTSDF) is an augmentation of the regular TSDF that shows potential for more coherent maps and improved tracking performance. In this work, we present methods for rendering depth- and color images from the DTSDF, making it a true drop-in replacement for the regular TSDF in established trackers. We evaluate the algorithm on well-established datasets and observe that our method improves tracking performance and increases re-usability of mapped scenes. Furthermore, we add color integration which notably improves color-correctness at adjacent surfaces. Our novel formulation of combined ICP with frame-to-keyframe photometric error minimization further improves tracking results. Lastly, we introduce Sim(3) point-to-plane ICP for refining pose priors in a multi-sensor scenario with different scale factors.}
}
@article{KASAEI2023104313,
title = {MVGrasp: Real-time multi-view 3D object grasping in highly cluttered environments},
journal = {Robotics and Autonomous Systems},
volume = {160},
pages = {104313},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104313},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002020},
author = {Hamidreza Kasaei and Mohammadreza Kasaei},
keywords = {Multi-view object grasping, Object manipulation, Human-robot interaction, Service robots},
abstract = {Nowadays robots play an increasingly important role in our daily life. In human-centered environments, robots often encounter piles of objects, packed items, or isolated objects. Therefore, a robot must be able to grasp and manipulate different objects in various situations to help humans with daily tasks. In this paper, we propose a multi-view deep learning approach to handle robust object grasping in human-centric domains. In particular, our approach takes a point cloud of an arbitrary object as an input, and then, generates orthographic views of the given object. The obtained views are finally used to estimate pixel-wise grasp synthesis for each object. We train the model end-to-end using a synthetic object grasp dataset and test it on both simulation and real-world data without any further fine-tuning. To evaluate the performance of the proposed approach, we performed extensive sets of experiments in four everyday scenarios, including isolated objects, packed items, pile of objects, and highly cluttered scenes. Experimental results show that our approach performed very well in all simulation and real-robot scenarios. More specifically, the proposed approach outperforms previous state-of-the-art approaches and achieves a success rate of >90% in all simulated and real scenarios, except for the pile of objects which is 82%. Additionally, our method demonstrated reliable closed-loop grasping of novel objects in a variety of scene configurations. The video of our experiments can be found here: https://youtu.be/c-4lzjbF7fY.}
}