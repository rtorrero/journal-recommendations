@article{BELTER2019110,
title = {Optimization-based legged odometry and sensor fusion for legged robot continuous localization},
journal = {Robotics and Autonomous Systems},
volume = {111},
pages = {110-124},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S092188901830126X},
author = {Dominik Belter and Michał R. Nowicki},
keywords = {Walking robot, Legged odometry, SLAM, Mapping, Navigation},
abstract = {In this manuscript, we tackle the problem of a continuous localization of a legged robot. We propose a novel, optimization-based procedure for the state estimation of the robot using measurements from internal sensors (legged odometry). Then, we propose the optimization-based integration of the legged odometry and the visual SLAM output. The proposed multi-modal localization system can continuously estimate the pose of the robot in various conditions despite fast motions of the robot, slippages or image motion blur. We provide the results of the real-time implementation of the proposed method on a multi-legged walking robot. We compare the proposed localization method to other state of the art localization systems and provide the dataset for future comparisons.}
}
@article{MARANGOZ2019174,
title = {More scalable solution for multi-robot–multi-target assignment problem},
journal = {Robotics and Autonomous Systems},
volume = {113},
pages = {174-185},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018301830},
author = {Salih Marangoz and Mehmet Fatih Amasyalı and Erkan Uslu and Furkan Çakmak and Nihal Altuntaş and Sırma Yavuz},
keywords = {Exploration, Multi-robot, Multi-target, Assignment, Target elimination},
abstract = {During multi-robot exploration, the calculation cost of the target assignment increases significantly with an increase in the map size and number of robots. In this study, a target elimination method is proposed to overcome this challenge. The benefit of the proposed method increases as the map size and number of robots increase, which is demonstrated both empirically and theoretically. Moreover, it is established that the proposed method does not rely on a specific map representation or robot–target assignment method.}
}
@article{SANTOS2021103832,
title = {Editorial: Special issue on Autonomous Driving and Driver Assistance Systems — Some main trends},
journal = {Robotics and Autonomous Systems},
volume = {144},
pages = {103832},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103832},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021001172},
author = {Vitor Santos and Angel D. Sappa and Miguel Oliveira and Arturo {de la Escalera}}
}
@article{2019ii,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {116},
pages = {ii},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(19)30301-X},
url = {https://www.sciencedirect.com/science/article/pii/S092188901930301X}
}
@article{ZHENG201944,
title = {New trajectory control method for robot with flexible bar-groups based on workspace lattices},
journal = {Robotics and Autonomous Systems},
volume = {111},
pages = {44-61},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018302768},
author = {Kunming Zheng and Youmin Hu and Bo Wu and Xuexing Guo},
keywords = {Flexible bar-group, Trajectory control, Comprehensive dynamics, Comprehensive dexterity, Workspace lattice},
abstract = {For robot with flexible bar-groups that comprise flexible link, flexible joint and joint clearance, it is difficult to guarantee highly accurate and efficient work. To address this problem, dynamic models of flexible links and flexible joints are studied in this paper, and a comprehensive dynamics model is established by constraints. Based on this comprehensive dynamics model, the comprehensive dexterity is defined by the index functions of condition number, minimum singular and dynamics operand, which can reflect the actual situation. Taking the comprehensive dexterity as the main basis, the novel idea of workspace lattice is put forward, and based on the Fourier transform and the principles of probability and statistics, the workspace lattice of robot system can be obtained. The proposed method is experimentally applied to a light-weight parallel robot, and its feasibility and effectiveness are verified. Moreover, extensive actual operation is performed by the light-weight parallel robot, which demonstrates the adaptive capability and the robustness of the proposed method.}
}
@article{PENA201998,
title = {Feasibility of an optimal EMG-driven adaptive impedance control applied to an active knee orthosis},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {98-108},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018304263},
author = {Guido G. Peña and Leonardo J. Consoni and Wilian M. {dos Santos} and Adriano A.G. Siqueira},
keywords = {Robotics rehabilitation, Impedance control, Adaptive controller, EMG-driven controller},
abstract = {This paper deals with EMG-driven torque estimation and optimal adaptive impedance control during robot-aided rehabilitation. In this preliminary and feasibility study, the proposed framework was evaluated considering an active knee orthosis and only healthy subjects. First, a simplified and optimized musculoskeletal model is used to compute the estimate of user’s torque considering electromyographic (EMG) signals taken from selected muscles acting during flexion and extension movements. The model optimization is performed by comparing the estimated torque from EMG with the torque generated by the inverse dynamics tool of the OpenSim software. As an alternative solution, a multilayer perceptron neural network (NN) is proposed to map the EMG signals to the user’s torque. The proposed approaches are evaluated by a set of healthy subjects wearing the knee orthosis and performing a protocol created for user–robot interaction analysis. Then, an EMG-driven adaptive impedance control is proposed to improve the user participation during the rehabilitation session. The approach is based on an optimal solution which considers the position error and the robot assistance level. The experimental results indicate the use of EMG signals is feasible for adaptive control strategies, taking into account the current condition of the user and optimizing the robot assistance.}
}
@article{EKMEKCI2019137,
title = {A context aware model for autonomous agent stochastic planning},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {137-153},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303294},
author = {Omer Ekmekci and Faruk Polat},
keywords = {Markov decision processes, Stochastic planning},
abstract = {Markov Decision Processes (MDPs) are not able to make use of domain information effectively due to their representational limitations. The lacking of elements which enable the models be aware of context, leads to unstructured representation of that problem such as raw probability matrices or lists. This causes these tools significantly less efficient at determining a useful policy as the state space of a task grows, which is the case for more realistic problems having localized dependencies between states and actions. In this paper, we present a new state machine, called Context-Aware Markov Decision Process (CA-MDP) based on MDP for the purpose of representing Markovian sequential decision making problems in a more structured manner. CA-MDP changes and augments MDP facilities by integrating causal relationships between actions and states thereby enabling structural, hence compact if possible, representation of the tasks. To show the expressive power of CA-MDP, we give the theoretical bounds for complexity of conversion between MDP and CA-MDP to demonstrate the expressive power of CA-MDP. Next, to generate an optimal policy from CA-MDP encoding by exploiting those newly defined facilities, we devised a new solver algorithm based on value iteration (VI), called Context-Aware Value Iteration (CA-VI). Although regular dynamic programming (DP) based algorithms is successful at effectively determining optimal policies, they do not scale well with respect to state–action space, making both the MDP encoding and related solver mechanism practically unusable for real-life problems. Our solver algorithm gets the power of overcoming the scalability problem by integrating the structural information provided in CA-MDP. First, we give theoretical analysis of CA-VI by examining the expected number of Bellman updates being performed on arbitrary tasks. Finally, we present our conducted experiments on numerous problems, with important remarks and discussions on certain aspects of CA-VI and CA-MDP, to justify our theoretical analyses empirically and to assess the real performance of CA-VI with CA-MDP formulation by analysing the execution time by checking how close it gets to the practical minimum runtime bound with respect to VI performance with MDP encoding of the same task.}
}
@article{CHINIMILLI201966,
title = {Automatic virtual impedance adaptation of a knee exoskeleton for personalized walking assistance},
journal = {Robotics and Autonomous Systems},
volume = {114},
pages = {66-76},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S092188901830558X},
author = {Prudhvi Tej Chinimilli and Zhi Qiao and Seyed Mostafa {Rezayat Sorkhabadi} and Vaibhav Jhawar and Iat Hou Fong and Wenlong Zhang},
keywords = {Wearable sensors, Assistive robotics, Human intention estimation, Impedance control, Rehabilitation},
abstract = {This paper attempts to address the problem of online modulation of virtual impedance for an assistive robot based on real-time gait and activity measurements to personalize the assistance for different users at different states. In this work, smart shoes and inertial sensors are introduced to measure ground contact forces and knee joint kinematics, respectively. An automatic impedance tuning (AIT) approach is presented for a knee assistive device (KAD) based on real-time activity recognition and gait phase detection. The activities considered in this paper are level, uphill, and downhill walking. A Gaussian mixture model (GMM) is employed to map the fuzzy likelihood of various activities and gait phases to the desired virtual impedance of the KAD. The prior estimate of virtual impedance is defined using human knee impedance identified with the walking data collected on different users. The AIT approach is integrated into the high-level impedance-based controller of the KAD for assistance during the stance phase. Finally, to evaluate the benefit of the proposed algorithm in stance phase, an EMG sensor is placed on the vastus medialis muscle group of three participants. The proposed approach is compared with two baseline approaches: constant impedance and finite state machine, and the results demonstrate that the profiles of impedance parameters and robot assistive torque are smoother and the muscle activity of vastus medialis is reduced. It is also noticed that the participants reduce their step lengths and increase walking cadence with assistance from the KAD.}
}
@article{2019103279,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {120},
pages = {103279},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(19)30719-5},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019307195}
}
@article{CAMARGOFORERO2019187,
title = {The ARCHADE: Ubiquitous Supercomputing for robotics. Part I: Philosophy},
journal = {Robotics and Autonomous Systems},
volume = {114},
pages = {187-198},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018305190},
author = {Leonardo Camargo-Forero and Pablo Royo and Xavier Prats},
keywords = {Ubiquitous supercomputing, The ARCHADE, Ubiquitous supercomputing ontology, High performance robotic computing - HPRC, HPRC cluster, Parallel robotic computing node - PRCN, General-purpose computing mission},
abstract = {In this work, we introduce Ubiquitous Supercomputing for robotics with the objective of opening our imagination to the development of new powerful heterogeneous multi-robot systems able to perform all kind of missions. Supercomputing, also known as High Performance computing (HPC) is the tool that allows us to predict the weather, understand the origins of the universe, create incredibly realistic fantasy movies, send personalized advertisement to millions of users worldwide and much more. Robotics has been mostly absent in its use of HPC but some previous works have lightly flirted with it. With the findings presented in here, we propose a ubiquitous supercomputing ontology, which allows describing systems made up of robots, traditional HPC infrastructures, sensors, actuators and people and exhibiting scalability, user-transparency and ultimately higher computing efficiency. Moreover, we present a technology called The ARCHADE, which facilitates the development, implementation and operation of such systems, and we propose a mechanism to define and automatize missions carried out by ubiquitous supercomputing systems. As a proof of concept, we present a system depicted as Tigers VS Hunters, which illustrates the potential of this technology. The results presented in here are part of a two series work introducing The ARCHADE. This first delivery presents its philosophy and main features. Correspondingly the second part will present a set of use cases and a complete performance benchmark. Supercomputing is part of our lives and it can be found in many research and industrial endeavors. With the ubiquitous supercomputing ontology and The ARCHADE, supercomputing will become part of robotics as well, bringing it therefore everywhere.}
}
@article{2021103791,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {140},
pages = {103791},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(21)00076-2},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000762}
}
@article{2019ii,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {117},
pages = {ii},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(19)30380-X},
url = {https://www.sciencedirect.com/science/article/pii/S092188901930380X}
}
@article{FANG2019145,
title = {A2ML: A general human-inspired motion language for anthropomorphic arms based on movement primitives},
journal = {Robotics and Autonomous Systems},
volume = {111},
pages = {145-161},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018301829},
author = {Cheng Fang and Xilun Ding and Chengxu Zhou and Nikos Tsagarakis},
keywords = {Anthropomorphic arm, Motion language, Task-motion planning, Movement primitive},
abstract = {The recent increasing demands on accomplishing complicated manipulation tasks necessitate the development of effective task-motion planning techniques. To help understand robot movement intention and avoid causing unease or discomfort to nearby humans toward safe human–robot interaction when these tasks are performed in the vicinity of humans by those robot arms that resemble an anthropomorphic arrangement, a dedicated and unified anthropomorphism-aware task-motion planning framework for anthropomorphic arms is at a premium. A general human-inspired four-level Anthropomorphic Arm Motion Language (A2ML) is therefore proposed for the first time to serve as this framework. First, six hypotheses/rules of human arm motion are extracted from the literature in neurophysiological field, which form the basis and guidelines for the design of A2ML. Inspired by these rules, a library of movement primitives and related motion grammar are designed to build the complete motion language. The movement primitives in the library are designed from two different but associated representation spaces of arm configuration: Cartesian-posture-swivel-angle space and human arm triangle space. Since these two spaces can be always recognized for all the anthropomorphic arms, the designed movement primitives and consequent motion language possess favorable generality. Decomposition techniques described by the A2ML grammar are proposed to decompose complicated tasks into movement primitives. Furthermore, a quadratic programming based method and a sampling based method serve as powerful interfaces for transforming the decomposed tasks expressed in A2ML to the specific joint trajectories of different arms. Finally, the generality and advantages of the proposed motion language are validated by extensive simulations and experiments on two different anthropomorphic arms.}
}
@article{DEALMEIDA201932,
title = {Bio-inspired on-line path planner for cooperative exploration of unknown environment by a Multi-Robot System},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {32-48},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303567},
author = {João Paulo Lima Silva {de Almeida} and Renan Taizo Nakashima and Flávio Neves-Jr and Lúcia Valéria Ramos {de Arruda}},
keywords = {Multi-robot system, Path planning, Indirect communication, Distributed control},
abstract = {This paper aims to present a cooperative and distributed navigation strategy, that is an on-line path planner, for an autonomous multi-robot system. The robots are intended to navigate and explore an unknown environment in order to find and reach obligatory passage points or way-points (goals), and then achieve a known final position. All robots in the team are homogeneous, independent and have limited communication skills. However they interact among them and with the environment to autonomously decide about their paths and tasks: if they should explore the environment, or avoid visiting a previously explored region, or to reach a discovered goal. Information sharing is directly carried out when the robots are into a communication area and/or indirectly by stigmergy. In this case, artificial pheromone, as a repulsive field, is used to mark regions that have already been explored by other members of the team, therefore avoiding redundant exploration and time waste. Fuzzy controllers are used for robots’ motion. The proposed on line path planner performance is evaluated in different simulated environment scenarios and the main results are presented.}
}
@article{LOURENCO201938,
title = {Strategies for uncertainty optimization through motion planning in GES sensor-based SLAM},
journal = {Robotics and Autonomous Systems},
volume = {113},
pages = {38-55},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303841},
author = {Pedro Lourenço and Pedro Batista and Paulo Oliveira and Carlos Silvestre},
keywords = {Simultaneous localization and mapping, Optimal control, Uncertainty optimization},
abstract = {This paper addresses the problem of minimizing the uncertainty through motion planning in a globally exponentially stable sensor-based simultaneous localization and mapping algorithm, with the objective of performing active exploitation. This is done by designing an optimization problem that weighs the final uncertainty, the overall uncertainty in the horizon considered, and the cost of the control. Using the Pontryagin minimum principle and building on the derivation of the Kalman filter by Athans and Tse as well as on Hussein’s extension for motion planning, the optimization problem is transformed into a two-point boundary value problem that encodes necessary conditions for the input that minimizes the uncertainty. A strategy is proposed to solve this problem numerically, and particular examples are analysed. Following the shortcomings identified in this procedure, the original optimization problem is modified assuming that the input velocities are piecewise constant functions. A direct approach is used to solve this new optimization problem, allowing the in-depth analysis of more realistic scenarios.}
}
@article{PRAKASH2019172,
title = {Robust obstacle detection for advanced driver assistance systems using distortions of inverse perspective mapping of a monocular camera},
journal = {Robotics and Autonomous Systems},
volume = {114},
pages = {172-186},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018301787},
author = {Charan D. Prakash and Farshad Akhbari and Lina J. Karam},
keywords = {Obstacle detection, Inverse Perspective Mapping (IPM), Selective Edge Filtering (SEF), Proposal assessment, Fused statistical properties, Occlusion, Advanced Driver Assistance Systems (ADAS)},
abstract = {This paper presents a robust method for generic obstacle detection and collision warning in Advanced Driver Assistance Systems (ADAS). The highlight of our method is the ability to detect all obstacles without prior knowledge and detect partially occluded obstacles including the obstacles that have not completely appeared in the frame (truncated obstacles). Our results show an improvement of 90% more true positives per frame compared to one of the state-of-the-art methods. Our proposed method is robust to variations in illumination and to a wide variety of vehicles and obstacles that are encountered while driving. Distortions are introduced when Inverse Perspective Mapping (IPM) projects the camera image onto the road surface plane. In this paper, we first show that the angular distortion in the IPM domain belonging to obstacle edges varies as a function of their corresponding 2D location in the camera plane. We use this information to perform proposal generation. We propose a novel proposal assessment method based on fusing statistical properties from both the IPM image and the camera image to perform robust outlier elimination and false positive reduction. We also present an annotated obstacle detection dataset derived from various source videos that can serve as a benchmark for the evaluation of future obstacle detection algorithms. The source videos containing diverse illumination and traffic conditions are derived from multiple publicly available datasets.}
}
@article{2019ii,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {113},
pages = {ii},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(19)30060-0},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019300600}
}
@article{PENG201999,
title = {Feature fusion based automatic aesthetics evaluation of robotic dance poses},
journal = {Robotics and Autonomous Systems},
volume = {111},
pages = {99-109},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018305220},
author = {Hua Peng and Jing Li and Huosheng Hu and Liping Zhao and Sheng Feng and Keli Hu},
keywords = {Robotic dance pose, Feature fusion, Machine learning, Automatic aesthetics estimation},
abstract = {Inspired by human dancers who make a comprehensive aesthetic judgement of their own dance poses by using both visual and non-visual information, this paper presents a novel feature fusion based approach to automatic aesthetics evaluation of robotic dance poses in order to improve the performance of robotic choreography creation. Four kinds of features are extracted, namely kinematic feature, region feature, contour feature, and spatial distribution feature of colour block. Based on different feature combinations, machine learning is deployed to train aesthetics models for the automatic judgement on robotic dance poses. The proposed approach has been implemented on a simulated robot environment, and experimental results are presented to verify its feasibility and good performance.}
}
@article{2019ii,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {114},
pages = {ii},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(19)30109-5},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019301095}
}
@article{PASQUALE2019260,
title = {Are we done with object recognition? The iCub robot’s perspective},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {260-281},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018300332},
author = {Giulia Pasquale and Carlo Ciliberto and Francesca Odone and Lorenzo Rosasco and Lorenzo Natale},
keywords = {Humanoid robotics, Robot vision, Visual object recognition, Machine learning, Deep learning, Transfer learning, Image dataset, Dataset collection, Representation invariance, iCub},
abstract = {We report on an extensive study of the benefits and limitations of current deep learning approaches to object recognition in robot vision scenarios, introducing a novel dataset used for our investigation. To avoid the biases in currently available datasets, we consider a natural human–robot interaction setting to design a data-acquisition protocol for visual object recognition on the iCub humanoid robot. Analyzing the performance of off-the-shelf models trained off-line on large-scale image retrieval datasets, we show the necessity for knowledge transfer. We evaluate different ways in which this last step can be done, and identify the major bottlenecks affecting robotic scenarios. By studying both object categorization and identification problems, we highlight key differences between object recognition in robotics applications and in image retrieval tasks, for which the considered deep learning approaches have been originally designed. In a nutshell, our results confirm the remarkable improvements yield by deep learning in this setting, while pointing to specific open challenges that need be addressed for seamless deployment in robotics.}
}
@article{2021103805,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {141},
pages = {103805},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(21)00090-7},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000907}
}
@article{2019103340,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {122},
pages = {103340},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(19)30879-6},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019308796}
}
@article{MAZINAN2022102587,
title = {On comprehensive cascade control strategy considering a class of overactuated autonomous non-rigid space systems with model uncertainties},
journal = {Robotics and Autonomous Systems},
volume = {155},
pages = {102587},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2015.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0921889015301056},
author = {A.H. Mazinan},
keywords = {Comprehensive cascade control strategy, Propellant engine modes control, Autonomous non-rigid space systems, Model uncertainties},
abstract = {With a focus on a number of state-of-the-art techniques in the area of autonomous non-rigid space systems control, realization of a comprehensive strategy is worthy of investigation to handle a set of parameters of the present overactuated processes with model uncertainties. In a word, the subject behind the research is to guarantee the desirable performance of a class of the autonomous space systems, which can be considered through the moments of inertia, the central of mass, the profile of the thrust vector and the misalignments of the propellant engine to deal with mission operation plans. There is the attitude cascade strategy including the low thrust three-axis engine off mode control, the low thrust x-axis engine on mode control and finally the high thrust y,z-axis engine on mode control, respectively. The control strategy is realized in a number of loops, as long as the on and off modes of the propellant engine are focused on the Euler angles control, in finite burn time, and quaternion vector control, in non-burn time, respectively, in line with parameters variations. It is to note that the parameters variations are coherently different in each one of the engine modes. The dynamics of high-low thrusters are taken into real consideration, where the control allocations in association with the pulse-width pulse-frequency modulators are employed to cope with a set of on–off reaction thrusters. The investigated results are finally analyzed in line with some related well-known benchmarks to verify the approach performance. The main contribution and motivation of the strategy investigated here is to propose a novel three-axis comprehensive cascade robust control solution to be able to deal with the parameters of autonomous non-rigid space systems under control with model uncertainties, in a synchronous manner, once the results regarding the tracking of the three-axis referenced commands are efficient with high accuracy along with the recent potential outcomes, researched in this area.}
}
@article{2019103299,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {121},
pages = {103299},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(19)30786-9},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019307869}
}
@article{2021103819,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {142},
pages = {103819},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(21)00104-4},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021001044}
}
@article{2020103459,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {125},
pages = {103459},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(20)30116-0},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020301160}
}
@article{YABUKI201931,
title = {Development of new cosmetic gloves for myoelectric prosthetic hand using superelastic rubber},
journal = {Robotics and Autonomous Systems},
volume = {111},
pages = {31-43},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017306772},
author = {Yoshiko Yabuki and Kazumasa Tanahashi and Yasuhiro Mouri and Yuta Murai and Shunta Togo and Ryu Kato and Yinlai Jiang and Hiroshi Yokoi},
keywords = {Cosmetic gloves, Myoelectric prosthetic hand, Gripping performance, Skin texture, Superelastic rubber, Thermoplastic styrene elastomer, Silicone elastomer},
abstract = {This paper reports on the design and development of new cosmetic gloves made of two different superelastic rubbers – thermoplastic styrene elastomer (TSE) and silicone rubber (TSG silicone) – and compares them with gloves made of polyvinyl chloride (PVC) for myoelectric prosthetic hands to realize a realistic appearance and flexible motion. The materials are compared in terms of their appearance, material, mechanical, and sensing properties. Appearance properties include the shape, wrinkles, fingerprints, texture, nail, and color of the hand; these properties are designed so as to produce a prosthetic hand that looks similar to a human hand. The material properties are evaluated in terms of adaptability for daily living without preventing finger motions of the powered hand by performing a tear strength test. Mechanical properties are improved by designing the thickness of the palm to grip an object. The sensing properties are essential for acquiring information about the object and the environment. The overall performance is evaluated through a material engineering test and a pick-and-place test with a powered prosthetic hand. Tear strength comparisons showed that TSE and TSG silicone could respectively withstand 5–7 and 3 times the strain that PVC could withstand before breakage. The TSE glove shows the highest stretching length before breaking and shows high flexibility even after breaking. The electric currents during EMG prosthetic hand motion showed that TSE and TSG silicone gloves successfully reduced energy consumption by around one-third for many hand movements. Flexibility test results for the maximum opening posture showed that the PVC glove greatly restricted the hand opening width. However, the differences between the cases without and with TSE gloves were very small; therefore, both cases show the same range of motion. The flexible TSE facilitated easy fitting and therefore had the lowest fitting time; in fact, it can be worn in one-third the time required for wearing PVC or TSG silicone gloves. In pick-and-place experiments, TSG silicone and TSE gloves both showed similar results for successfully grasping objects. The TSE glove is hard to break and has high elasticity; therefore, nails can be added to it. Furthermore, TSG resin is thermosetting and can be processed at room temperature, making it easy to impart conductivity. Therefore, the TSG silicone material is more suitable for implementing a sensor.}
}
@article{2020103615,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103615},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(20)30455-3},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304553}
}
@article{HONARPARDAZ2019120,
title = {Fast finger design automation for industrial robots},
journal = {Robotics and Autonomous Systems},
volume = {113},
pages = {120-131},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018304123},
author = {Mohammadali Honarpardaz and Johan Ölvander and Mehdi Tarkian},
keywords = {Design automation, Fingers design, Multi-function fingers, Industrial grippers, Optimization, Robotics},
abstract = {Finger design automation is highly demanded from robot industries to fulfill the requirements of the agile market. Nevertheless, literature lacks a promising approach to automate the design process of reliable fingers for industrial robots. Hence, this work proposes the generic optimized finger design (GOFD) method which automates the design process of single- and multi-function finger grippers. The proposed method includes an optimization algorithm to minimize the design process time. The method is utilized to generate fingers for several groups of objects. Results show that the GOFD method outperforms existing methods and is able to reduce the design time by an average of 16,600 s. While the proposed method substantially reduces the design process time of fingers, the quality of grasps is comparable to the traditional exhaustive search method. The grasp quality of GOFD deviates only 0.47% from the absolute best grasp known from the exhaustive search method in average. The designed fingers are lastly manufactured and experimentally verified.}
}
@article{LI2019201,
title = {Semi-direct monocular visual and visual-inertial SLAM with loop closure detection},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {201-210},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018301374},
author = {Shao-peng Li and Tao Zhang and Xiang Gao and Duo Wang and Yong Xian},
keywords = {Robot vision, Simultaneous localization and mapping (SLAM), Loop closure detection},
abstract = {A novel semi-direct monocular visual simultaneous localization and mapping (SLAM) system is proposed to maintain the fast performance of a direct method and the high precision and loop closure capability of a feature-based method. This system extracts and matches Oriented FAST and Rotated BRIEF features in a keyframe and tracks a non-keyframe via a direct method without the requirement of extracting and matching features. A keyframe is used for global or local optimization and loop closure, whereas a non-keyframe is used for fast tracking and localization, thereby combining the advantages of direct and feature-based methods. A monocular visual-inertial SLAM system that fuses inertial measurement data with visual SLAM is also proposed. This system successfully recovers the metric scale successfully. The evaluation on datasets shows that the proposed approach accomplishes loop closure detection successfully and requires less time to achieve accuracy comparable with that of feature-based method. The physical experiment demonstrates the feasibility and robustness of the proposed SLAM. The approach achieves good balance between speed and accuracy and provides valuable references for design and improvement of other SLAM methods.}
}
@article{2020103358,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {123},
pages = {103358},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(19)30907-8},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019309078}
}
@article{LEIDNER2019199,
title = {Cognition-enabled robotic wiping: Representation, planning, execution, and interpretation},
journal = {Robotics and Autonomous Systems},
volume = {114},
pages = {199-216},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.018},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303312},
author = {Daniel Leidner and Georg Bartels and Wissam Bejjani and Alin Albu-Schäffer and Michael Beetz},
keywords = {AI reasoning methods, Action and effect representation, Compliant manipulation, Service robotics},
abstract = {Advanced cognitive capabilities enable humans to solve even complex tasks by representing and processing internal models of manipulation actions and their effects. Consequently, humans are able to plan the effect of their motions before execution and validate the performance afterwards. In this work, we derive an analog approach for robotic wiping actions which are fundamental for some of the most frequent household chores including vacuuming the floor, sweeping dust, and cleaning windows. We describe wiping actions and their effects based on a qualitative particle distribution model. This representation enables a robot to plan goal-oriented wiping motions for the prototypical wiping actions of absorbing, collecting and skimming. The particle representation is utilized to simulate the task outcome before execution and infer the real performance afterwards based on haptic perception. This way, the robot is able to estimate the task performance and schedule additional motions if necessary. We evaluate our methods in simulated scenarios, as well as in real experiments with the humanoid service robot Rollin’ Justin.}
}
@article{CARNOVALE2019103282,
title = {Introducing article numbering to Robotics and Autonomous Systems},
journal = {Robotics and Autonomous Systems},
volume = {120},
pages = {103282},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.103282},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019307298},
author = {Catherine Carnovale}
}
@article{2019ii,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {111},
pages = {ii},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(18)30925-4},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018309254}
}
@article{BAMPIS2019104,
title = {Revisiting the Bag-of-Visual-Words model: A hierarchical localization architecture for mobile systems},
journal = {Robotics and Autonomous Systems},
volume = {113},
pages = {104-119},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018305293},
author = {Loukas Bampis and Antonios Gasteratos},
keywords = {Localization, Visual place recognition, Mobile systems, Parallel programming},
abstract = {In this paper, an enhanced visual place recognition system is proposed aiming to improve the localization performance of a mobile platform. Our technique takes full advantage of the continuous input image stream in order to provide additional knowledge to the matching functionality. The well-established Bag-of-Visual-Words model is adapted into a hierarchical design that derives the visual information from the full entity of a natural scene into the description, while it additionally preserves the geometric structure of the explored world. Our approach is evaluated as part of a state-of-the-art Simultaneous-Localization-and-Mapping algorithm, and parallelization techniques are exploited utilizing every available hardware module in a low-power device. The implemented algorithm has been tested on several publicly available datasets offering consistently accurate localization results and preventing the majority of redundant computations that the additional geometrical verifications can induce.}
}
@article{2021103853,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {143},
pages = {103853},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(21)00138-X},
url = {https://www.sciencedirect.com/science/article/pii/S092188902100138X}
}
@article{SAJADIALAMDARI2019291,
title = {Nonlinear Model Predictive Control for Ecological Driver Assistance Systems in Electric Vehicles},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {291-303},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S092188901830191X},
author = {Seyed Amin Sajadi-Alamdari and Holger Voos and Mohamed Darouach},
keywords = {Nonlinear Model Predictive Control, Ecological Driver Assistance Systems, Electric vehicles, Optimal Energy Management, Real-time Systems},
abstract = {Battery Electric Vehicle (BEV) has one of the most promising drivetrain technology. However, the BEVs are facing the limited cruising range which generally reduces their share in the automotive market. Velocity profile, acceleration characteristics, road gradients, and drive techniques around curves have significant impacts on the energy consumption of the BEVs. A semi-autonomous ecological driver assistance system to regulate the velocity with energy-efficient techniques is proposed to address the limitation. The main contribution of this paper is the design of a real-time nonlinear model predictive controller with improved inequality constraints handling and economic penalty function to plan the online cost-effective cruising velocity. This system is based on the extended cruise control driver assistance system which controls the longitudinal velocity of the BEV in a safe and energy efficient manner by taking advantage of road slopes, effective drive around curves, and respecting the traffic regulation. A real-time optimisation algorithm is adapted and extended with economic objective function. Instead of the conventional Euclidean norms, deadzone penalty functions are proposed to achieve the economic objectives. In addition, the states inequality constraints are handled based on the proposed soft nonlinear complementarity function aimed to preserve the relaxed complementary slackness to enhance the Pontryagin’s Minimum Principle (PMP) method. Obtained numerical simulation and field experimental results demonstrate the effectiveness of the proposed method for a semi-autonomous electric vehicle in terms of real-time energy-efficient velocity regulation and constraints satisfaction intended to improve the cruising range capability of the BEVs.}
}
@article{2019ii,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {118},
pages = {ii},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(19)30460-9},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019304609}
}
@article{WIETRZYKOWSKI2019160,
title = {PlaneLoc: Probabilistic global localization in 3-D using local planar features},
journal = {Robotics and Autonomous Systems},
volume = {113},
pages = {160-173},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303701},
author = {Jan Wietrzykowski and Piotr Skrzypczyński},
keywords = {Global localization, SLAM, Planar segments, RGB-D data},
abstract = {The global localization problem concerns situations when a map of the environment is known but there is no initial guess of the agent position. Whereas the ability to perform global localization is required in many practical situations, it is still an open problem, particularly if the agent requires to find an accurate estimate of its 3-D pose. In this article, we describe PlaneLoc, a novel probabilistic approach to 3-D global localization, which integrates multiple local cues to construct a probability distribution that describes the likelihood of the agent pose. This framework enables to incorporate various types of localization cues but we demonstrate its feasibility using segmented planes abstracted from RGB-D data. We use multiple triplets of planar segments to generate candidate probability distribution and employ it to find the most probable pose with respect to a global map of planar segments. The PlaneLoc implementation uses the ORB-SLAM2 system that serves as visual odometry and makes it possible to generate observation in a form of sets of local segments online. The proposed approach can be used for global localization with a known map or for loop closing and re-localization in Simultaneous Localization and Mapping. The implemented system is validated in experiments using publicly available RGB-D data sets, including our own data set acquired specifically for testing localization methods based on planar features.}
}
@article{2021103720,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {136},
pages = {103720},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(21)00005-1},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000051}
}
@article{BOZCAN2019132,
title = {COSMO: Contextualized scene modeling with Boltzmann Machines},
journal = {Robotics and Autonomous Systems},
volume = {113},
pages = {132-148},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303427},
author = {İlker Bozcan and Sinan Kalkan},
keywords = {Scene modeling, Context, Boltzmann Machines},
abstract = {Scene modeling is very crucial for robots that need to perceive, reason about and manipulate the objects in their environments. In this paper, we adapt and extend Boltzmann Machines (BMs) for contextualized scene modeling. Although there are many models on the subject, ours is the first to bring together objects, relations, and affordances in a highly-capable generative model. For this end, we introduce a hybrid version of BMs where relations and affordances are incorporated with shared, tri-way connections into the model. Moreover, we introduce a dataset for relation estimation and modeling studies. We evaluate our method in comparison with several baselines on object estimation, out-of-context object detection, relation estimation, and affordance estimation tasks. Moreover, to illustrate the generative capability of the model, we show several example scenes that the model is able to generate, and demonstrate the benefits of the model on a humanoid robot. The code and the dataset are publicly made available at: https://github.com/bozcani/COSMO.}
}
@article{STEINER2019211,
title = {Open-sector rapid-reactive collision avoidance: Application in aerial robot navigation through outdoor unstructured environments},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {211-220},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303944},
author = {Jake A. Steiner and Xiang He and Joseph R. Bourne and Kam K. Leang},
abstract = {A new reactive collision avoidance method for navigation of aerial robots (such as unmanned aerial vehicles (UAVs)) in unstructured urban/suburban environments is presented. Small form-factor aerial robots, such as quadcopters, often have limited payload capacity, flight time, processing power, and sensing capabilities. To enhance the capabilities of such vehicles without increasing weight or computing power, a reactive collision avoidance method based on open sectors is described. The method utilizes information from a two-dimensional laser scan of the environment and a short-term memory of past actions and can rapidly circumvent obstacles in outdoor urban/suburban environments. With no map required, the method enables the robot to react quickly and navigate even when the enivornment changes. Furthermore, the low computational requirement of the method allows the robot to quickly react to unknown obstacles that may be poorly represented in the scan, such as trees with branches and leaves. The method is validated in simulation results and through physical experiments on a prototype quadcopter system, where results show the robot flying smoothly around obstacles at a relatively high speed (3 m/s).}
}
@article{2020103638,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {132},
pages = {103638},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(20)30478-4},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304784}
}
@article{MATEUS201923,
title = {Efficient and robust Pedestrian Detection using Deep Learning for Human-Aware Navigation},
journal = {Robotics and Autonomous Systems},
volume = {113},
pages = {23-37},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017306784},
author = {André Mateus and David Ribeiro and Pedro Miraldo and Jacinto C. Nascimento},
keywords = {Pedestrian detection, Convolutional Neural Network, Human-Aware Navigation},
abstract = {This paper addresses the problem of Human-Aware Navigation (HAN), using multi camera sensors to implement a vision-based person tracking system. The main contributions of this paper are as follows: a novel and efficient Deep Learning person detection and a standardization of human-aware constraints. In the first stage of the approach, we propose to cascade the Aggregate Channel Features (ACF) detector with a deep Convolutional Neural Network (CNN) to achieve fast and accurate Pedestrian Detection (PD). Regarding the human awareness (that can be defined as constraints associated with the robot’s motion), we use a mixture of asymmetric Gaussian functions, to define the cost functions associated to each constraint. Both methods proposed herein are evaluated individually to measure the impact of each of the components. The final solution (including both the proposed pedestrian detection and the human-aware constraints) is tested in a typical domestic indoor scenario, in four distinct experiments. The results show that the robot is able to cope with human-aware constraints, defined after common proxemics and social rules.}
}
@article{2020103574,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {129},
pages = {103574},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(20)30414-0},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304140}
}
@article{CHEN201994,
title = {Convolutional multi-grasp detection using grasp path for RGBD images},
journal = {Robotics and Autonomous Systems},
volume = {113},
pages = {94-103},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018307346},
author = {Lu Chen and Panfeng Huang and Zhongjie Meng},
keywords = {Multi-grasp detection, Grasp path, Convolutional neural network},
abstract = {Generally, most grasp detection models follow the similar frameworks as that in object detection, which use the convolutional neural network to regress the grasp parameters directly. However, grasp detection and object detection are actually different, for the ground truths in object detection are unique while that in grasp detection are not exhaustive. A predicted grasp could still be applicable despite it does not coincide well with ground truth. In this paper, a novel grasp detection model is constructed to make a fairer evaluation on grasp candidate. Instead of using isolated ground truths, the grasp path is introduced to reveal the possible consequent distribution of ground truths. The grasp candidate is first mapped to grasp path, generating the mapped grasp, and the bias between them works as the estimated error for back-propagation. Experiments deployed on grasping dataset as well as real-world scenarios show that our proposed method could improve the detection accuracy. In addition, it can be well-generalized to detect unseen objects.}
}
@article{BONIARDI201984,
title = {A pose graph-based localization system for long-term navigation in CAD floor plans},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {84-97},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018306092},
author = {Federico Boniardi and Tim Caselitz and Rainer Kümmerle and Wolfram Burgard},
keywords = {Mobile robotics, Localization, Mapping, SLAM, Adaptive systems},
abstract = {Accurate localization is an essential technology for flexible automation. Industrial applications require mobile platforms to be precisely localized in complex environments, often subject to continuous changes and reconfiguration. Most of the approaches use precomputed maps both for localization and for interfacing robots with workers and operators. This results in increased deployment time and costs as mapping experts are required to setup the robotic systems in factory facilities. Moreover, such maps need to be updated whenever significant changes in the environment occur in order to be usable within commanding tools. To overcome those limitations, in this work we present a robust and highly accurate method for long-term LiDAR-based indoor localization that uses CAD-based architectural floor plans. The system leverages a combination of graph-based mapping techniques and Bayes filtering to maintain a sparse and up-to-date globally consistent map that represents the latest configuration of the environment. This map is aligned to the CAD drawing using prior constraints and is exploited for relative localization, thus allowing the robot to estimate its current pose with respect to the global reference frame of the floor plan. Furthermore, the map helps in limiting the disturbances caused by structures and clutter not represented in the drawing. Several long-term experiments in changing real-world environments show that our system outperforms common state-of-the-art localization methods in terms of accuracy and robustness while remaining memory and computationally efficient.}
}
@article{2021103748,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {138},
pages = {103748},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(21)00033-6},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000336}
}
@article{GUINDEL2019109,
title = {Traffic scene awareness for intelligent vehicles using ConvNets and stereo vision},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {109-122},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018301751},
author = {Carlos Guindel and David Martín and José María Armingol},
keywords = {Object detection, Pose estimation, Deep learning, Intelligent vehicles},
abstract = {In this paper, we propose an efficient approach to perform recognition and 3D localization of dynamic objects on images from a stereo camera, with the goal of gaining insight into traffic scenes in urban and road environments. We rely on a deep learning framework able to simultaneously identify a broad range of entities, such as vehicles, pedestrians or cyclists, with a frame rate compatible with the strict requirements of onboard automotive applications. Stereo information is later introduced to enrich the knowledge about the objects with geometrical information. The results demonstrate the capabilities of the perception system for a wide variety of situations, thus providing valuable information for a higher-level understanding of the traffic situation.}
}
@article{HIRABAYASHI201962,
title = {Traffic light recognition using high-definition map features},
journal = {Robotics and Autonomous Systems},
volume = {111},
pages = {62-72},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018301234},
author = {Manato Hirabayashi and Adi Sujiwo and Abraham Monrroy and Shinpei Kato and Masato Edahiro},
keywords = {Autonomous vehicles, Vehicle environment perception, Information fusion},
abstract = {Accurate recognition of traffic lights in public roads is a critical step to deploy automated driving systems. Camera sensors are widely used for the object detection task. It might seem natural to employ them to traffic signal detection. However, images as captured by cameras contain a broad number of unrelated objects, causing a significant reduction in the detection accuracy. This paper presents an innovative, yet reliable method to recognize the state of traffic lights in images. With the help of accurate 3D maps and a self-localization technique in it, elements already being used in autonomous driving systems, we propose a method to improve the traffic light detection accuracy. Using the current location and looking for the traffic signals in the road, we extract the region related only to the traffic light (ROI, region of interest) in images captured by a vehicle-mounted camera, then we feed the ROIs to custom classifiers to recognize the state. Evaluation of our method was carried out in two datasets recorded during our urban public driving experiments, one taken during day light and the other obtained during sunset. The quantitative evaluations indicate that our method achieved over 97% average precision for each state and approximately 90% recall as far as 90 meters under preferable condition.}
}
@article{PERSIC2019217,
title = {Extrinsic 6DoF calibration of a radar–LiDAR–camera system enhanced by radar cross section estimates evaluation},
journal = {Robotics and Autonomous Systems},
volume = {114},
pages = {217-230},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.023},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018301994},
author = {Juraj Peršić and Ivan Marković and Ivan Petrović},
keywords = {Sensor calibration, Radar, LiDAR, Camera, Radar cross section},
abstract = {Autonomous navigation of mobile robots is often based on information from a variety of heterogeneous sensors; hence, extrinsic sensor calibration is a fundamental step in the fusion of such information. In this paper, we address the problem of extrinsic calibration of a radar–LiDAR–camera sensor system. This problem is primarily challenging due to sparse informativeness of radar measurements. Namely, radars cannot extract rich structural information about the environment, while their lack of elevation resolution, that is nevertheless accompanied by substantial elevation field of view, introduces uncertainty in the origin of the measurements. We propose a novel calibration method which involves a special target design and two-step optimization procedure to solve the aforementioned challenges. First step of the optimization is minimization of a reprojection error based on an introduced point–circle geometric constraint. Since the first step is not able to provide reliable estimates of all the six extrinsic parameters, we introduce a second step to refine the subset of parameters with high uncertainty. We exploit a pattern discovered in the radar cross section estimation that is correlated to the missing elevation angle. Additionally, we carry out identifiability analysis based on the Fisher Information Matrix to show minimal requirements on the dataset and to verify the method through simulations. We test the calibration method on a variety of sensor configurations and address the problem of radar vertical misalignment. In the end, we show via extensive experiment analysis that the proposed method is able to reliably estimate all the six parameters of the extrinsic calibration.}
}
@article{YAN201941,
title = {Efficient decision-making for multiagent target searching and occupancy in an unknown environment},
journal = {Robotics and Autonomous Systems},
volume = {114},
pages = {41-56},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018306882},
author = {Fuhan Yan and Kai Di and Jiuchuan Jiang and Yichuan Jiang and Hui Fan},
keywords = {Multiagent, Target searching and occupancy, Decision-making, Unknown environment},
abstract = {Target searching in an unknown environment is a traditional research issue in the multiagent area. In some real cases, the agents do not only discover the targets; instead, they have subsequent tasks that must be completed before a deadline. In this paper, these cases are abstracted as the agents searching for target locations in an unknown environment and then occupying these target locations within a limited time. The agents can obtain rewards by occupying the target locations, and the goal of this problem is to maximize net income, defined as total reward minus the moving cost of the agents. This problem can be transformed into the traditional problems, and then be solved by previous related algorithms. However, this approach is not optimal. In this paper, we present a method that combines previous algorithms and a decision-making algorithm. The experiments demonstrate that the method containing our decision-making algorithm can lead to higher net income than simply using previous algorithms.}
}
@article{2020103523,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {127},
pages = {103523},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(20)30308-0},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020303080}
}
@article{SUN2019190,
title = {Optimal control of intelligent vehicle longitudinal dynamics via hybrid model predictive control},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {190-200},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.020},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018306122},
author = {Xiaoqiang Sun and Yingfeng Cai and Shaohua Wang and Xing Xu and Long Chen},
keywords = {Intelligent vehicle, Longitudinal dynamics, Hybrid system, Mixed logical dynamical model, Experimental tests},
abstract = {This paper presents an innovative application of the hybrid model predictive control (HMPC) scheme to optimally regulate the intelligent vehicle longitudinal velocity. For autonomous velocity regulation, the intelligent vehicle needs to be operated in two distinct modes (drive and brake) and because of the mode-dependent constraints on accelerations and decelerations by considering the comfort of passengers, the intelligent vehicle longitudinal dynamics control process can be regarded as a constrained hybrid dynamical system. Thus, in this study, the intelligent vehicle longitudinal dynamics is approximated as a two-mode discrete-time mixed logical dynamical (MLD) system. Using this approximation, a hybrid model predictive controller, which allows us to optimize the switching sequences of the operation modes (binary control inputs) and the torques acted on the wheels (continuous control inputs), is tuned based on online mixed-integer quadratic programming. Numerical simulation analysis is conducted for a sport utility vehicle to demonstrate the effectiveness of the proposed control method. Finally, the explicit representation of the HMPC is computed to control the intelligent vehicle in real-time, and the experimental results are presented to show the applicability of the proposed controller.}
}
@article{DELEONGOMEZ2019229,
title = {An essential model for generating walking motions for humanoid robots},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {229-243},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303373},
author = {Víctor De-León-Gómez and Qiuyue Luo and Anne Kalouguine and J. Alfonso Pámanes and Yannick Aoustin and Christine Chevallereau},
keywords = {Modeling, Biped walking, Periodic motions, Humanoid robots},
abstract = {The modeling of humanoid robots with many degrees-of-freedom (DoF) can be done via the complete dynamic model. However, the complexity of the model can hide the essential factor of the walking, i.e. the equilibrium of the robot. One alternative is to simplify the model by neglecting some dynamical effects like in the 3D Linear Inverted Pendulum (LIP) model. Nonetheless, the assumption that the ZMP will be at the base of the pendulum is not ensured and the resulting walking gaits can make the Zero Moment Point (ZMP) evolve outside of the convex hull of support when they are replicated on the complete model of any humanoid robot. The objective of this paper is to propose a new model for walking that has the same dimension as the 3D LIP model but considers the complete dynamics of the humanoid. The proposed model is called essential model and it can be written based on the internal states of the robot and possible external information, thereby generating models for different purposes. The main advantage of the essential model is that it allows to generate walking gaits that ensure that the Zero Moment Point (ZMP) is kept in a desired position or it follows a desired path while the gait is performed. Furthermore, impacts of the swing foot with the ground can be considered to compute periodic walking gaits. In order to show the advantages of the proposed model, numerical studies are performed to design periodic walking gaits for the humanoid robot ROMEO.}
}
@article{LEE201960,
title = {Time-dependent genetic algorithm and its application to quadruped’s locomotion},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {60-71},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018300149},
author = {Jeong Hoon Lee and Jong Hyeon Park},
keywords = {Genetic algorithm, Quadruped robots, Slope, Admittance control, Impedance parameter, Stable locomotion},
abstract = {Genetic algorithms (GAs) are widely used in machine learning and optimization. This paper proposes a time-dependent genetic algorithm (TDGA) based on real-coded genetic algorithm (RCGA) to improve the convergence performance of functions over time such as a foot trajectory. TDGA has several distinguishing features when compared with traditional RCGA. First, individuals are arranged over time, and then the individuals are optimized in sequence. Second, search spaces of design variables are newly comprised of processes of reductions for search spaces. Third, the search space for crossover operations is expanded to avoid local minima traps that can occur in new search spaces up to the previous search space before performing any reduction of search space, and boundary mutation operation is performed to the new search spaces. Computer simulations are implemented to verify the convergence performance of the robot locomotion optimized by TDGA. Then, TDGA optimizes the desired feet trajectories of quadruped robots that climb up a slope and the impedance parameters of admittance control so that quadruped robots can trot stably over irregular terrains. Simulation results clearly represent that the convergence performance is improved by TDGA, which also shows that TDGA could be broadly used in robot locomotion research.}
}
@article{GONGORA20191,
title = {Olfactory telerobotics. A feasible solution for teleoperated localization of gas sources?},
journal = {Robotics and Autonomous Systems},
volume = {113},
pages = {1-9},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018306523},
author = {Andres Gongora and Javier Gonzalez-Jimenez},
keywords = {Mobile robotics, Telerobotics, Artificial olfaction, Gas source localization, Electronic nose},
abstract = {Olfactory telerobotics consists in augmenting the sensing capabilities of a conventional teleoperated mobile-robot to acquire information about the surrounding air (i.e. smell, wind-speed, etc.) in addition to the usual audio and video streams. Conceptually, this allows for new and improved applications, among which the most relevant are those related to gas-source localization (GSL). That is, searching through telerobotics for one or multiple gas-emission sources, such as hazardous gas-leaks in industrial facilities or the CO2 signature of trapped survivors in collapsed buildings. Notwithstanding, both the needed sensing-technology for the robot as well as the olfactory feedback-interfaces for the human operator are relatively recent, and might still not meet all the requirements of such applications. This work is therefore meant to assess the current feasibility of olfactory telerobotics to address real-world GSL problems, and accordingly, to determine which aspects play the most important role for its success, or otherwise, might be constraining its usefulness. We have collected to this end a dataset composed of 60 experiments where volunteer operators had to locate and identify hidden gas-source among several identical candidates with an olfaction-enabled robot and under realistic environmental conditions (i.e. uncontrolled and natural gas-distributions). We analyse this data to determine the overall search accuracy and intuitiveness of the system, considering that none of the operators had any prior experience with it, and study the importance of the employed sensory-feedback and how they were employed during the experiments. We finally report different findings, from which we highlight that the tested telerobotics system allowed the operators to correctly identify the source in 3 out of 4 attempts, and that the underlying human search-strategy appears to be a probabilistic-driven behaviour that favours semantic and visual information over the robot’s gas and wind measurements.}
}
@article{AMANATIADIS2019282,
title = {ViPED: On-road vehicle passenger detection for autonomous vehicles},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {282-290},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018302045},
author = {Angelos Amanatiadis and Evangelos Karakasis and Loukas Bampis and Stylianos Ploumpis and Antonios Gasteratos},
keywords = {Autonomous vehicles, Safety systems, Driver information systems, ADAS},
abstract = {This paper is about detecting and counting the passengers of a tracking vehicle using on-car monocular vision. By having a model of nearby vehicle occupants, intelligent reasoning systems of autonomous cars will be provided with this additional knowledge needed in emergency situations such as those that many philosophers have recently raised. The on-road Vehicle PassengEr Detection (ViPED) system is based on the human perception model in terms of spatio-temporal reasoning, namely the slight movements of passenger shape silhouettes inside the cabin. The main challenges we face are the low light conditions of the cabin (no feature points), the subtle non-rigid motions of the occupants (possible artifactual transitions), and the puzzling discrimination problem of back or front seat occupants (lack of depth information inside the cabin). To overcome these challenges, we first track the detected car windshield and find the optimal affine warp. The registered windshield images are preprocessed in order to extract a feature matrix, which serves as input to a Convolutional Neural Network (CNN) for inferring the number and position of passengers. We demonstrate that our low-cost sensor system is able to detect in most cases successfully all the passengers in preceding moving vehicles at various distances and occupancies. Metrics and datasets are included for possible community future work on this new challenging task.}
}
@article{WU20191,
title = {Coordinated control of a dual-arm robot for surgical instrument sorting tasks},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {1-12},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018304068},
author = {Qihan Wu and Meng Li and Xiaozhi Qi and Ying Hu and Bing Li and Jianwei Zhang},
keywords = {Surgical instrument sorting, Dual-arm robot, Recognition and location, Constraint analysis, Hybrid fuzzy control},
abstract = {Surgical instrument sorting tasks are usually performed by medical staff. Because of the large number and special structure of surgical instruments, the manual processing method has certain drawbacks: it is time-consuming, poses an infection risk and has potential for errors. Moreover, a relatively sharp surgical instrument poses a potential threat to the staff’s health. In this paper, to complete the sorting task accurately and avoid these threats, a coordinated control strategy based on fuzzy hybrid control is proposed for dual-arm coordinated operations. First, sorting information is obtained through identification, location and grasping pose detection of surgical instruments. Second, according to the characteristics of the dual-arm coordinated operation, a kinematics model of three tight coordination tasks is established and a passive compliant structure is applied to simplify the motion of surgical scissor stretching. Then, a hybrid fuzzy control strategy is proposed for dual-arm coordinated operations. Using this strategy, the contact force in the process of unlocking surgical scissors is adaptively adjusted. Finally, the proposed control strategy is validated using comparative analysis and in robotic experiments. The experimental results demonstrate that the proposed control strategy is suitable for a dual-arm robot to efficiently implement instrument sorting tasks.}
}
@article{KOLLMITZ201929,
title = {Deep 3D perception of people and their mobility aids},
journal = {Robotics and Autonomous Systems},
volume = {114},
pages = {29-40},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303257},
author = {Marina Kollmitz and Andreas Eitel and Andres Vasquez and Wolfram Burgard},
keywords = {Mobile robot, Service robotics, Convolutional neural networks, Object detection, Object tracking, People detection, Multi-class detection},
abstract = {Robots operating in populated environments, such as hospitals, office environments or airports, encounter a large variety of people with some of them having an advanced need for cautious interaction because of their advanced age or motion impairments. To provide appropriate assistance and support robot helpers require the ability to recognize people and their potential requirements. In this article, we present a people detection framework that distinguishes people according to the mobility aids they use. Our framework uses a deep convolutional neural network for detecting people in image data. For human-aware robots it is necessary to know where people are in a 3D world reference frame instead of only locating them in a 2D image, therefore we add a 3D centroid regression output to the network to predict the Cartesian position of people. We further use a probabilistic class, position and velocity tracker to account for false detections and occlusions. Our framework comes in two variants: The depth only variant targets high privacy demands, while the RGB only framework provides improved detection performance for non-critical applications. Both variants do not require additional geometric information about the environment. We demonstrate our approach using a dedicated dataset acquired with the support of a mobile robotic platform. The dataset contains five classes: pedestrian, person in wheelchair, pedestrian pushing a person in a wheelchair, person using crutches and person using a walking frame. Our framework achieves an mAP of 0.87 for RGB and 0.79 for depth images at a detection distance threshold of 0.5m on our dataset, with a runtime of 53ms per image. The annotated dataset is publicly available and our framework is made open source as a ROS people detector.}
}
@article{CHEN2019123,
title = {Odor source localization algorithms on mobile robots: A review and future outlook},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {123-136},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303014},
author = {Xin-xing Chen and Jian Huang},
keywords = {Odor source localization, Chemical plume tracking, Mobile robot},
abstract = {When applied in some harsh environments (e.g. in poisonous atmosphere or underwater), odor source localization robots are able to perform better than animals without being hurt. During the past three decades, robotic odor source localization has become a popular research field with various algorithms being proposed. These algorithms can be roughly divided into four categories: gradient-based algorithms, bio-inspired algorithms, multi-robot algorithms and probabilistic and map-based algorithms. In this paper, we present a literature review of these four categories and discuss their pros and cons. We also discuss the current trends and some future challenges according to some research papers published in recent years.}
}
@article{TARDIOLI201973,
title = {Pound: A multi-master ROS node for reducing delay and jitter in wireless multi-robot networks},
journal = {Robotics and Autonomous Systems},
volume = {111},
pages = {73-87},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017309144},
author = {Danilo Tardioli and Ramviyas Parasuraman and Petter Ögren},
keywords = {Multi-robot systems, Wireless multi-hop networks, Robot operating system, Jitter, Delay},
abstract = {The Robot Operating System (ROS) is a popular and widely used software framework for building robotics systems. With the growth of its popularity, it has started to be used in multi-robot systems as well. However, the TCP connections that the platform relies on for connecting the so-called ROS nodes presents several issues regarding limited-bandwidth, delays, and jitter, when used in wireless multi-hop networks. In this paper, we present a thorough analysis of the problem and propose a new ROS node called Pound to improve the wireless communication performance by reducing delay and jitter in data exchanges, especially in multi-hop networks. Pound allows the use of multiple ROS masters (roscores), features data compression, and importantly, introduces a priority scheme that allows favoring more important flows over less important ones. We compare Pound to the state-of-the-art solutions through extensive experiments and show that it performs equally well, or better in all the test cases, including a control-over-network example.}
}
@article{ALLEN2019174,
title = {A real-time framework for kinodynamic planning in dynamic environments with application to quadrotor obstacle avoidance},
journal = {Robotics and Autonomous Systems},
volume = {115},
pages = {174-193},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017308692},
author = {Ross E. Allen and Marco Pavone},
keywords = {Motion planning, Kinodynamic, Real-time, Obstacle avoidance, Quadrotor, Unmanned aerial vehicle, Machine learning, Human–robot interaction},
abstract = {The objective of this paper is to present a full-stack, real-time motion planning framework for kinodynamic robots and then show how it is applied and demonstrated on a physical quadrotor system operating in a laboratory environment. The proposed framework utilizes an offline–online computation paradigm, neighborhood classification through machine learning, sampling-based motion planning with an optimal cost distance metric, and trajectory smoothing to achieve real-time planning for aerial vehicles. This framework accounts for dynamic obstacles with an event-based replanning structure and a locally reactive control layer that minimizes replanning events. The approach is demonstrated on a quadrotor navigating moving obstacles in an indoor space and stands as, arguably, one of the first demonstrations of full-online kinodynamic motion planning, with execution cycles of 3 Hz to 5 Hz. For the quadrotor, a simplified dynamics model is used during the planning phase to accelerate online computation. A trajectory smoothing phase, which leverages the differentially flat nature of quadrotor dynamics, is then implemented to guarantee a dynamically feasible trajectory.}
}
@article{2020103490,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {126},
pages = {103490},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(20)30230-X},
url = {https://www.sciencedirect.com/science/article/pii/S092188902030230X}
}
@article{CALTAGIRONE2019125,
title = {LIDAR–camera fusion for road detection using fully convolutional neural networks},
journal = {Robotics and Autonomous Systems},
volume = {111},
pages = {125-131},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018300496},
author = {Luca Caltagirone and Mauro Bellone and Lennart Svensson and Mattias Wahde},
keywords = {Intelligent vehicles, Deep learning, Sensor fusion, Road detection},
abstract = {In this work, a deep learning approach has been developed to carry out road detection by fusing LIDAR point clouds and camera images. An unstructured and sparse point cloud is first projected onto the camera image plane and then upsampled to obtain a set of dense 2D images encoding spatial information. Several fully convolutional neural networks (FCNs) are then trained to carry out road detection, either by using data from a single sensor, or by using three fusion strategies: early, late, and the newly proposed cross fusion. Whereas in the former two fusion approaches, the integration of multimodal information is carried out at a predefined depth level, the cross fusion FCN is designed to directly learn from data where to integrate information; this is accomplished by using trainable cross connections between the LIDAR and the camera processing branches. To further highlight the benefits of using a multimodal system for road detection, a data set consisting of visually challenging scenes was extracted from driving sequences of the KITTI raw data set. It was then demonstrated that, as expected, a purely camera-based FCN severely underperforms on this data set. A multimodal system, on the other hand, is still able to provide high accuracy. Finally, the proposed cross fusion FCN was evaluated on the KITTI road benchmark where it achieved excellent performance, with a MaxF score of 96.03%, ranking it among the top-performing approaches.}
}
@article{2021103697,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {135},
pages = {103697},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(20)30537-6},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305376}
}
@article{MATHEWS2019154,
title = {Supervised morphogenesis: Exploiting morphological flexibility of self-assembling multirobot systems through cooperation with aerial robots},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {154-167},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S092188901730372X},
author = {Nithin Mathews and Anders Lyhne Christensen and Alessandro Stranieri and Alexander Scheidler and Marco Dorigo},
keywords = {Self-assembling robots, Heterogeneous multirobot teams, Distributed systems, Air/ground robot teams, Robot coordination, Modular robots},
abstract = {Self-assembling robots have the potential to undergo autonomous morphological adaptation. However, due to the simplicity in their hardware makeup and their limited perspective of the environment, self-assembling robots are often not able to reach their potential and adapt their morphologies to tasks or environments without external cues or prior information. In this paper, we present supervised morphogenesis — a control methodology that makes self-assembling robots truly flexible by enabling aerial robots to exploit their elevated position and better view of the environment to initiate and control (hence supervise) morphology formation on the ground. We present results of two case studies in which we assess the feasibility of the presented methodology using real robotic hardware. In the case studies, we implemented supervised morphogenesis using two different aerial platforms and up to six self-assembling autonomous robots. We furthermore quantify the benefits attainable for self-assembling robots through cooperation with aerial robots using simulation-based studies. The research presented in this paper is a significant step towards realizing the true potential of self-assembling robots by enabling autonomous morphological adaptation to a priori unknown tasks and environments.}
}
@article{FUKUI20191,
title = {Autonomous gait transition and galloping over unperceived obstacles of a quadruped robot with CPG modulated by vestibular feedback},
journal = {Robotics and Autonomous Systems},
volume = {111},
pages = {1-19},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018300137},
author = {Takahiro Fukui and Hisamu Fujisawa and Kotaro Otaka and Yasuhiro Fukuoka},
keywords = {Central pattern generator, Gait transition, Quadrupedal locomotion, Robust galloping},
abstract = {The aim of this paper is to demonstrate, based on robot results, the effectiveness and practicability of vestibular feedback to central pattern generators (CPG) employed for the locomotion of quadruped robots. We build a new quadruped robot with mechanisms enabling walking to running and apply CPGs modulated by simple vestibular sensory feedback (a body tilt multiplied by a fixed gain). As a result, the robot safely locomotes at a variety of speeds by autonomously changing the gait from walking to trotting to galloping according to speed, despite the fact that the walk and gallop are not preprogrammed. In addition, as this paper’s major contribution, we discover and demonstrate that the robot robustly runs with an emergent gallop while stepping on and over several types of unperceived obstacles, while being suddenly pulled forward, and while the physical balance is changed (i.e., a weight is put forward on the robot), by autonomously modifying the phase differences between the four legs from the basic gallop. To our knowledge, no other galloping robots have been reported that can adapt to an unperceived obstacle. We conclude that CPGs modulated by vestibular feedback is effective and practical as a gait generator for bio-inspired robots that are expected to have both the abilities of “speed-based autonomous gait transition” and “autonomous robust running”.}
}
@article{TSURUMINE201972,
title = {Deep reinforcement learning with smooth policy update: Application to robotic cloth manipulation},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {72-83},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303245},
author = {Yoshihisa Tsurumine and Yunduan Cui and Eiji Uchibe and Takamitsu Matsubara},
keywords = {Deep reinforcement learning, Robotic cloth manipulation, Dynamic policy programming},
abstract = {Deep Reinforcement Learning (DRL), which can learn complex policies with high-dimensional observations as inputs, e.g., images, has been successfully applied to various tasks. Therefore, it may be suitable to apply them for robots to learn and perform daily activities like washing and folding clothes, cooking, and cleaning since such tasks are difficult for non-DRL methods that often require either (1) direct access to state variables or (2) well-designed hand-engineered features extracted from sensory inputs. However, applying DRL to real robots remains very challenging because conventional DRL algorithms require a huge number of training samples for learning, which is arduous in real robots. To alleviate this dilemma, in this paper, we propose two sample efficient DRL algorithms: Deep P-Network (DPN) and Dueling Deep P-Network (DDPN). The core idea is to combine the nature of smooth policy update with the capability of automatic feature extraction in deep neural networks to enhance the sample efficiency and learning stability with fewer samples. The proposed methods were first investigated by a robot-arm reaching task in the simulation that compared previous DRL methods and applied to two real robotic cloth manipulation tasks: (1) flipping a handkerchief and (2) folding a t-shirt with a limited number of samples. All the results suggest that our method outperformed the previous DRL methods.}
}
@article{KATSUMATA2019149,
title = {Optimal exciting motion for fast robot identification. Application to contact painting tasks with estimated external forces},
journal = {Robotics and Autonomous Systems},
volume = {113},
pages = {149-159},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017307091},
author = {Takuma Katsumata and Benjamin Navarro and Vincent Bonnet and Philippe Fraisse and André Crosnier and Gentiane Venture},
keywords = {Dynamic identification, Force control, Painting task, Kuka LWR},
abstract = {Accurate geometric and inertial parameter estimates of a modern manipulator are of crucial importance to obtain good performances during a contact task or for obtaining more and more required realistic simulations. CAD data are often provided by the manufacturer, but these are inaccurate and do not take into account eventual end-effector modifications. Fortunately, they can be identified. However, in real industrial applications, dynamic identification is rarely performed because it supposedly requires a cumbersome and long procedure. There is a need of a practical but accurate method to identify dynamics parameters. Thus, this paper proposes a practical framework to identify a Kuka LWR robot in less than 10 s. An experimental comparison between several cost functions showed that log{det(⋅)} is the best trade-off for getting a good parameters accuracy within a minimal time. The procedure identifies very accurately the inertial parameters of the robot and of its end-effector and recognizes its geometric parameters from a look-up table. When using identified parameters, joint torques were estimated with an RMS difference lower than 1 N m when compared to measured ones. The identified model was then used to generate a contact painting trajectory. During this contact task, the external forces were estimated and controlled without the use of a force sensor. Experimentation showed that the external forces could be identified with an RMS difference lower than 3 N.}
}
@article{GRONDIN201963,
title = {Lightweight and optimized sound source localization and tracking methods for open and closed microphone array configurations},
journal = {Robotics and Autonomous Systems},
volume = {113},
pages = {63-80},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017309399},
author = {François Grondin and François Michaud},
keywords = {Sound source localization, Sound source tracking, Microphone array, Online processing, Embedded system, Mobile robot, Robot audition},
abstract = {Human–robot interaction in natural settings requires filtering out the different sources of sounds from the environment. Such ability usually involves the use of microphone arrays to localize, track and separate sound sources online. Multi-microphone signal processing techniques can improve robustness to noise but the processing cost increases with the number of microphones used, limiting response time and widespread use on different types of mobile robots. Since sound source localization methods are the most expensive in terms of computing resources as they involve scanning a large 3D space, minimizing the amount of computations required would facilitate their implementation and use on robots. The robot’s shape also brings constraints on the microphone array geometry and configurations. In addition, sound source localization methods usually return noisy features that need to be smoothed and filtered by tracking the sound sources. This paper presents a novel sound source localization method, called SRP-PHAT-HSDA, that scans space with coarse and fine resolution grids to reduce the number of memory lookups. A microphone directivity model is used to reduce the number of directions to scan and ignore non significant pairs of microphones. A configuration method is also introduced to automatically set parameters that are normally empirically tuned according to the shape of the microphone array. For sound source tracking, this paper presents a modified 3D Kalman (M3K) method capable of simultaneously tracking in 3D the directions of sound sources. Using a 16-microphone array and low cost hardware, results show that SRP-PHAT-HSDA and M3K perform at least as well as other sound source localization and tracking methods while using up to 4 and 30 times less computing resources respectively.}
}
@article{2020103680,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {134},
pages = {103680},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(20)30520-0},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020305200}
}
@article{LAN2019132,
title = {Autonomous robot photographer with KL divergence optimization of image composition and human facial direction},
journal = {Robotics and Autonomous Systems},
volume = {111},
pages = {132-144},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017308114},
author = {Kai Lan and Kosuke Sekiyama},
keywords = {Robot photographer, Variational Bayes, Composition evaluation, Viewpoint selection},
abstract = {In this paper, we propose a robot photography system that can autonomously search for the optimal target viewpoint. Two technical issues of scene composition evaluation and viewpoint selection are solved by this system. A composition evaluation method for photos is developed using well-known composition rules based on Kullback–Leibler (KL) divergence, considering the directional information of each target. To reduce the calculation cost of the composition evaluation in the case where the number of targets is large, automatic target grouping is conducted via variational Bayes. The optimal viewpoint with respect to the composition is selected from a number of candidate viewpoints around the targets based on KL divergence. Finally, the fact that better composed photos can be autonomously photographed by the proposed system is validated via experiments and human evaluations.}
}
@article{HA201981,
title = {Topology-guided path integral approach for stochastic optimal control in cluttered environment},
journal = {Robotics and Autonomous Systems},
volume = {113},
pages = {81-93},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017308874},
author = {Jung-Su Ha and Soon-Seo Park and Han-Lim Choi},
keywords = {Stochastic optimal control, Topological motion planning, Linearly-solvable optimal control, Multi-modality},
abstract = {This paper addresses planning and control of robot motion under uncertainty that is formulated as a continuous-time, continuous-space stochastic optimal control problem, by developing a topology-guided path integral control method. The path integral control framework, which forms the backbone of the proposed method, re-writes the Hamilton–Jacobi–Bellman equation as a statistical inference problem; the resulting inference problem is solved by a sampling procedure that computes the distribution of controlled trajectories around the trajectory by the passive dynamics. For motion control of robots in a highly cluttered environment, however, this sampling can easily be trapped in a local minimum unless the sample size is very large, since the global optimality of local minima depends on the degree of uncertainty. Thus, a homology-embedded sampling-based planner that identifies many (potentially) local-minimum trajectories in different homology classes is developed to aid the sampling process. In combination with a receding-horizon fashion of the optimal control the proposed method produces a dynamically feasible and collision-free motion plans without being trapped in a local minimum. Numerical examples on a synthetic toy problem and on quadrotor control in a complex obstacle field demonstrate the validity of the proposed method.}
}
@article{LEE2019178,
title = {Bird’s eye view localization of surrounding vehicles: Longitudinal and lateral distance estimation with partial appearance},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {178-189},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018301052},
author = {Elijah S. Lee and Wongun Choi and Dongsuk Kum},
keywords = {Distance estimation, Partial appearance, Vehicle detection, Grounded edge and reference point, Bird’s eye view localization},
abstract = {On-road vehicle detection is essential for perceiving driving settings, and localizing the detected vehicle helps drivers predict possible risks and avoid collisions. However, there are limited works on vehicle detection with partial appearance, and the method for partially visible vehicle localization has not been explored. In this paper, a novel framework for vehicle detection and localization with partial appearance is proposed using stereo vision and geometry. First, the original images from the stereo camera are processed to form a v-disparity map. After object detection using v-disparity, vehicle candidates are generated with prior knowledge of possible vehicle locations on the image. Deep learning-based verification completes vehicle detection. For each detected vehicle, partially visible vehicle tracking algorithm is newly introduced. To track partially visible vehicles, this algorithm detects the vehicle edge on the ground, defined as the grounded edge, and then selects a reference point for Kalman filter tracking. Finally, a rectangular box is drawn on the bird’s eye view to represent vehicle’s longitudinal and lateral location. The proposed system successfully performs partially visible vehicle detection and tracking. For testing the localization performance, the datasets in a highway and an urban setting are used and provide less than 1.5 m longitudinal error and 0.4 m lateral error in standard deviation.}
}
@article{2020103418,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {124},
pages = {103418},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(20)30015-4},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020300154}
}
@article{YOU20191,
title = {Advanced planning for autonomous vehicles using reinforcement learning and deep inverse reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {114},
pages = {1-18},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018302021},
author = {Changxi You and Jianbo Lu and Dimitar Filev and Panagiotis Tsiotras},
keywords = {Reinforcement learning, Inverse reinforcement learning, Deep neural-network, Maximum entropy, Path planning, Autonomous vehicle},
abstract = {Autonomous vehicles promise to improve traffic safety while, at the same time, increase fuel efficiency and reduce congestion. They represent the main trend in future intelligent transportation systems. This paper concentrates on the planning problem of autonomous vehicles in traffic. We model the interaction between the autonomous vehicle and the environment as a stochastic Markov decision process (MDP) and consider the driving style of an expert driver as the target to be learned. The road geometry is taken into consideration in the MDP model in order to incorporate more diverse driving styles. The desired, expert-like driving behavior of the autonomous vehicle is obtained as follows: First, we design the reward function of the corresponding MDP and determine the optimal driving strategy for the autonomous vehicle using reinforcement learning techniques. Second, we collect a number of demonstrations from an expert driver and learn the optimal driving strategy based on data using inverse reinforcement learning. The unknown reward function of the expert driver is approximated using a deep neural-network (DNN). We clarify and validate the application of the maximum entropy principle (MEP) to learn the DNN reward function, and provide the necessary derivations for using the maximum entropy principle to learn a parameterized feature (reward) function. Simulated results demonstrate the desired driving behaviors of an autonomous vehicle using both the reinforcement learning and inverse reinforcement learning techniques.}
}
@article{2019ii,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {ii},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(19)30005-3},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019300053}
}
@article{ANTUNES201956,
title = {Testing of a torque vectoring controller for a Formula Student prototype},
journal = {Robotics and Autonomous Systems},
volume = {113},
pages = {56-62},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018301970},
author = {João Antunes and André Antunes and Pedro Outeiro and Carlos Cardeira and Paulo Oliveira},
keywords = {Torque control, Control systems, Optimal control, System implementation},
abstract = {The torque vectoring controller is the electrical substitute of a mechanical differential, with the advantage of improving the stability and handling of the vehicle. This work tackles the design, implementation and testing of a torque vectoring algorithm to be implemented in a Formula Student prototype. First is presented a dynamic test model used for the design and tuning of the controllers, which is then validated with real data from a real vehicle. Secondly, a computation methodology to achieve a reference value is proposed. Two controllers are presented, a PID and a LQR controller, with both being designed and tested in simulation. In a final part, the two controllers are implemented in a Formula Student prototype. The results from the vehicle with and without the controllers are then compared and the performance improvement discussed.}
}
@article{OISHI201913,
title = {SeqSLAM++: View-based robot localization and navigation},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {13-21},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S092188901730684X},
author = {Shuji Oishi and Yohei Inoue and Jun Miura and Shota Tanaka},
keywords = {SeqSLAM, View-based localization, Navigation, Mobile robot},
abstract = {This paper presents a new approach to view-based localization and navigation in outdoor environments, which are indispensable functions for mobile robots. Several approaches have been proposed for autonomous navigation. GPS-based systems are widely used especially in the case of automobiles, however, they can be unreliable or non-operational near tall buildings. Localization with a precise 3D digital map of the target environment also enables mobile robots equipped with range sensors to estimate accurate poses, but maintaining a large-scale outdoor map is often costly. We have therefore developed a novel view-based localization method SeqSLAM++ by extending the conventional SeqSLAM in order not only to robustly estimate the robot position comparing image sequences but also to cope with changes in a robot’s heading and speed as well as view changes using wide-angle images and a Markov localization scheme. According to the direction to move provided by the SeqSLAM++, the local-level path planner navigates the robot by setting subgoals repeatedly considering the structure of the surrounding environment using a 3D LiDAR. The entire navigation system has been implemented in the ROS framework, and the effectiveness and accuracy of the proposed method was evaluated through off-line/on-line navigation experiments.}
}
@article{2020103544,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {128},
pages = {103544},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(20)30381-X},
url = {https://www.sciencedirect.com/science/article/pii/S092188902030381X}
}
@article{SANTOS2019103266,
title = {Editorial: Special issue on autonomous driving and driver assistance systems},
journal = {Robotics and Autonomous Systems},
volume = {121},
pages = {103266},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.103266},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019306682},
author = {Vitor Santos and Angel D. Sappa and Miguel Oliveira and Arturo {de la Escalera}}
}
@article{JEONG201910,
title = {A robust walking controller optimizing step position and step time that exploit advantages of footed robot},
journal = {Robotics and Autonomous Systems},
volume = {113},
pages = {10-22},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018305372},
author = {Hyobin Jeong and Inho Lee and Okkee Sim and KangKyu Lee and Jun-Ho Oh},
keywords = {Biped walking, Step position adjustment, Step time adjustment, Divergent component of motion},
abstract = {This study proposes a robust humanoid step control algorithm that optimizes ground reaction force, step position and step time. Our method is focused on the robot that has finite size of foot and designed to exploit its advantages. The foot allows for the range of ZMP presence and our algorithm use this ZMP range to absorb sensor noise, modeling error and certain amount of disturbances. Thanks to these effect, our step controller is able to produce new stepping time and the stepping position stably. From quadratic programming (QP) technique, we can consider maximum kinematical foot range and maximum foot velocity in the optimization process by setting inequality constraints. The CoM trajectory is re-planned in each control cycle with a short cycle preview controller. The linear inverted pendulum model (LIPM) simulation and full dynamics simulation shows that our proposing method precisely reflect the advantages of footed robot and significant improvement in walking robustness to strong perturbation.}
}
@article{2020103582,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {130},
pages = {103582},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(20)30422-X},
url = {https://www.sciencedirect.com/science/article/pii/S092188902030422X}
}
@article{2020103658,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {133},
pages = {103658},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(20)30498-X},
url = {https://www.sciencedirect.com/science/article/pii/S092188902030498X}
}
@article{LU201988,
title = {Development of a sEMG-based torque estimation control strategy for a soft elbow exoskeleton},
journal = {Robotics and Autonomous Systems},
volume = {111},
pages = {88-98},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018305128},
author = {Longhai Lu and Qingcong Wu and Xi Chen and Ziyan Shao and Bai Chen and Hongtao Wu},
keywords = {Torque estimation control strategy, Soft exoskeleton, sEMG, Motion intention, Power assistance},
abstract = {Motor dysfunction has become a serious threat to the health of older people and the patients with neuromuscular impairment. The application of exoskeleton to motion assistance has received increasing attention due to its promising prospects. The major contribution of this paper is to develop a joint torque estimation control strategy for a soft elbow exoskeleton to provide effective power assistance. The surface electromyography signal (sEMG) from biceps is utilized to estimate the motion intension of wearer and map into the real-time elbow joint torque. Moreover, the control strategy fusing the estimated joint torque, estimated joint angle from inertial measurement unit and encoder feedback signal is proposed to improve motion assistance performance. Finally, further experimental investigations are carried out to compare the control effectiveness of the proposed intention-based control strategy to that of the proportional control strategy. The experimental results indicate that the proposed control strategy provides better performance in elbow assistance with different loads, and the average efficiency of assistance with heavy load is about 42.66%.}
}
@article{LIANG201949,
title = {An improved scheme for eliminating the coupled motion of surgical instruments used in laparoscopic surgical robots},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {49-59},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018305888},
author = {Yunlei Liang and Zhijiang Du and Weidong Wang and Zhiyuan Yan and Lining Sun},
keywords = {Cable-driven mechanism, Coupled motion, Feedforward neural network, Prediction model},
abstract = {Considering the nonlinear characteristics such as backlash hysteresis and coupled motion commonly exist in cable-driven mechanism of laparoscopic surgical robot end-effector, it is a great challenge to control the motion of robotic end-effector precisely during the surgical procedure. Due to the effects of coupled motion, the surgical end-effector will not move accurately as surgeons expected. Previous studies mostly focused on the design of special compensation mechanisms and software compensation algorithms to solve coupled motion problem. However, these approaches are limited because the backlash hysteresis is ignored and the mechanism of end-effector is restricted. This paper shows an improved scheme to eliminate the coupled motion of end-effector and reduce the position tracking error. The proposed decoupling scheme is conducted in three stages. Firstly, the time and frequency domain information of the driving motor current and the motion information of surgical instrument are extracted in real-time. Thereafter, a feedforward neural network is designed to identify the movement stage of end-effector. Finally, a prediction model is designed to predict the coupling error, after that the coupling error can be eliminated by using feedforward compensation control. An experimental platform was set up to verify the effectiveness of the proposed control scheme, and the results of corresponding comparative experiments revealed that the proposed strategy can substantially improve the tracking accuracy.}
}
@article{NEMEC2019168,
title = {Precise localization of the mobile wheeled robot using sensor fusion of odometry, visual artificial landmarks and inertial sensors},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {168-177},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.019},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018300757},
author = {Dušan Nemec and Vojtech Šimák and Aleš Janota and Marián Hruboš and Emília Bubeníková},
keywords = {Localization, Sensor fusion, Odometry, Landmarks, Inertial sensors, Mean square error},
abstract = {This article proposes a method for sensor fusion between odometers, gyroscope, accelerometer, magnetometer and visual landmark localization system. The method is designed for estimation of all 6 degrees of freedom (both translation and attitude) of a wheeled robot moving in uneven terrain. The fusion method is based on continuous estimation of the mean square error of each estimated value and allows different sampling rates of each sensor. Due to the simple implementation, it is suitable for real-time processing in the low-cost hardware. In order to evaluate the precision of the estimated position, stochastical models of sensors (with parameters matching real hardware sensors) were used and random trajectories were simulated. The virtual experiments showed that the method is resistant to the failure of any sensor except the odometers; however, each sensor provides improvement in the resultant precision.}
}
@article{ZHANG201922,
title = {Improvement of human–machine compatibility of upper-limb rehabilitation exoskeleton using passive joints},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {22-31},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018305001},
author = {Leiyu Zhang and Jianfeng Li and Peng Su and Yanming Song and Mingjie Dong and Qiang Cao},
keywords = {Stroke, Upper-limb rehabilitation exoskeleton, Configuration synthesis, Human-machine compatibility, Passive joint, Glenohumeral joint},
abstract = {The upper-limb rehabilitation exoskeleton is a critical piece of equipment for stroke patients to compensate for deficiencies of manual rehabilitation and reduce physical therapists’ workloads. In this paper, configuration synthesis of an exoskeleton is completed using advanced mechanism theory. To adapt glenohumeral (GH) movements and improve exoskeletal compatibility, six passive joints were introduced into the connecting interfaces based on optimal configuration principles. The optimal configuration of the passive joints can effectively reduce the gravitational influences of the exoskeleton device and the upper extremities. A compatible exoskeleton (Co-Exos) with 11 degrees of freedom was developed while retaining a compact volume. A new approach is presented to compensate vertical GH movements. The theoretical displacements of translational joints were calculated by the kinematic model of the shoulder loop Θs. A comparison of the theoretical and measured results confirms that the passive joints exhibited good human–machine compatibility for GH movements. The hysteresis phenomenon of translational joints appeared in all experiments due to the elasticoplasticity of the upper arm and GH. In comparable experiments, the effective torque of the second active joint was reduced by an average of 41.3% when passive joints were released. The wearable comfort of Co-Exos was thus improved significantly.}
}
@article{BOLBOLI201919,
title = {Stiffness feasible workspace of cable-driven parallel robots with application to optimal design of a planar cable robot},
journal = {Robotics and Autonomous Systems},
volume = {114},
pages = {19-28},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018306572},
author = {Javad Bolboli and Mohammad A. Khosravi and Farzaneh Abdollahi},
keywords = {Cable-driven parallel robot, Stiffness, Internal forces, Stiffness-feasible workspace, Optimal design},
abstract = {In this paper stiffness of cable-driven parallel robots (CDPRs) is analyzed in detail and based on this analysis, the stiffness-feasible workspace is introduced. This workspace includes all stable poses which increasing the internal forces can modify the total stiffness of the robot. It has been shown that in the CDPRs, the concept of the internal forces can be applied for keeping cables in tension and increasing stiffness. However, it should be noted that increasing the internal forces may decrease the overall stiffness of the mechanism and it is only applicable in stabilizable poses of the workspace. Therefore, stiffness-feasible workspace determines the allowable internal forces range which can increase the stiffness and it will guarantee that in this range of the internal forces, the structure of the robot is stable. In this paper, by employing this criterion and evolutionary algorithms, a CDPR is optimally designed, and a set of answers is presented. In this design, in addition to the stiffness-feasible workspace, another criterion as stiffness number is presented which is useful for specifying the distribution of the stiffness and stiffness-feasibility of the robot.}
}
@article{CRAYE2019244,
title = {Exploring to learn visual saliency: The RL-IAC approach},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {244-259},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018304792},
author = {Céline Craye and Timothée Lesort and David Filliat and Jean-François Goudou},
keywords = {Visual saliency, Bounding box proposals, Intrinsic motivation, Intelligent adaptive curiosity, Autonomous mobile robots, Incremental learning, Deep learning},
abstract = {The problem of object localization and recognition on autonomous mobile robots is still an active topic. In this context, we tackle the problem of learning a model of visual saliency directly on a robot. This model, learned and improved on-the-fly during the robot’s exploration provides an efficient tool for localizing relevant objects within their environment. The proposed approach includes two intertwined components. On the one hand, we describe a method for learning and incrementally updating a model of visual saliency from a depth-based object detector. This model of saliency can also be exploited to produce bounding box proposals around objects of interest. On the other hand, we investigate an autonomous exploration technique to efficiently learn such a saliency model. The proposed exploration, called Reinforcement Learning-Intelligent Adaptive Curiosity (RL-IAC) is able to drive the robot’s exploration so that samples selected by the robot are likely to improve the current model of saliency. We then demonstrate that such a saliency model learned directly on a robot outperforms several state-of-the-art saliency techniques, and that RL-IAC can drastically decrease the required time for learning a reliable saliency model.}
}
@article{MANTELLI2019304,
title = {A novel measurement model based on abBRIEF for global localization of a UAV over satellite images},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {304-319},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S092188901830438X},
author = {Mathias Mantelli and Diego Pittol and Renata Neuland and Arthur Ribacki and Renan Maffei and Vitor Jorge and Edson Prestes and Mariana Kolberg},
keywords = {UAV, Localization, abBRIEF, Satellite images},
abstract = {This paper presents a method for global localization and tracking of an Unmanned Aerial Vehicle (UAV) over satellite images. We propose a new measurement model based on a novel version of BRIEF descriptor and apply it in a Monte Carlo Localization system that estimates the UAV pose in 4 degrees of freedom. The model is used to compare images obtained from the UAV downward looking camera against patches of satellite images such as the ones available on Google™ Earth. The proposed method was validated using real flights sequences and has yield good results with different maps of the same region spawning many years and covering large areas.}
}
@article{CORRELL2020103530,
title = {Robots and autonomous systems, SI DARS 2018},
journal = {Robotics and Autonomous Systems},
volume = {129},
pages = {103530},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103530},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020303250},
author = {Nikolaus Correll and Mac Schwager}
}
@article{2019ii,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {ii},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(19)30603-7},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019306037}
}
@article{TALAMINO201993,
title = {Anticipatory kinodynamic motion planner for computing the best path and velocity trajectory in autonomous driving},
journal = {Robotics and Autonomous Systems},
volume = {114},
pages = {93-105},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018301957},
author = {Jordi Pérez Talamino and Alberto Sanfeliu},
keywords = {Autonomous driving, ADAS, Urban, Anticipation, Kinodynamic motion planning, Path planning, -splines, Velocity profiles},
abstract = {This paper presents an approach, using an anticipatory kinodynamic motion planner, for obtaining the best trajectory and velocity profile for autonomous driving in dynamic complex environments, such as driving in urban scenarios. The planner discretizes the road search space and looks for the best vehicle path and velocity profile at each control period of time, assuming that the static and dynamic objects have been detected. The main contributions of the work are in the anticipatory kinodynamic motion planner, in a fast method for obtaining the G2-splines for path generation, and in a method to compute and select the best velocity profile at each candidate path that fulfills the vehicle kinodynamic constraints, taking into account the passenger comfort. The method has been developed and tested in MATLAB through a set of simulations in different representative scenarios, involving fixed obstacles and moving vehicles. The outcome of the simulations shows that the anticipatory kinodynamic planner performs correctly in diverse dynamic scenarios, maintaining smooth accelerations for passenger comfort.}
}
@article{DONG2019221,
title = {Kinematics-based incremental visual servo for robotic capture of non-cooperative target},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {221-228},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017307273},
author = {Gangqi Dong and Zheng H. Zhu},
keywords = {Visual servo, Robotic capture, Non-cooperative target, Adaptive extended Kalman filter, Kinematics-based incremental control},
abstract = {This paper presents the concept and experimental results of a kinematics-based incremental visual servo control approach for robotic manipulators with an eye-in-hand configuration to capture non-cooperative targets autonomously. The vision system is adopted to estimate the real time position and motion of the target by an integrated algorithm of photogrammetry and the adaptive extended Kalman filter. The unknown intercept point of trajectories of the target and the end-effector is dynamically predicted and updated based on the target estimates and is served as the desired position of the end-effector. An incremental control law is developed for the robotic manipulator to avoid multiple solutions of the robotic inverse kinematics. The end-effector is then controlled by the proposed control scheme to approach the dynamically estimated interception point directly. The proposed approach is validated experimentally by custom built robotic manipulator. To demonstrate the effectiveness of the proposed approach, uncertainties, such as, joint flexibility of the robotic manipulator, backlash of actuators, nonlinear target motion, camera mounting bias, etc., have not been considered in the control law. The experimental results show that the predicted minimum tracking time is reduced asymptotically as the end-effector approaches the target, which demonstrate the proposed control scheme is effective and reliable. The advantages of the proposed control approach are: it does not require a robotic dynamic model that most of the existing robotic control based on; it avoids the multiple solution problem in the inverse kinematics; it is insensitive to system uncertainties; and it is much easier for engineering implementation.}
}
@article{2021103768,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {139},
pages = {103768},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(21)00053-1},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000531}
}
@article{2021103740,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {137},
pages = {103740},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(21)00025-7},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000257}
}
@article{2019ii,
title = {Editorial Board},
journal = {Robotics and Autonomous Systems},
volume = {115},
pages = {ii},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(19)30170-8},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019301708}
}
@article{SHIOTA201920,
title = {Enhanced Kapandji test evaluation of a soft robotic thumb rehabilitation device by developing a fiber-reinforced elastomer-actuator based 5-digit assist system},
journal = {Robotics and Autonomous Systems},
volume = {111},
pages = {20-30},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017308084},
author = {Kouki Shiota and Shota Kokubu and Tapio V.J. Tarvainen and Masashi Sekine and Kahori Kita and Shao Ying Huang and Wenwei Yu},
keywords = {Fiber-reinforced Elastomer Actuators (FEA), Enhanced Kapandji Test, Hand rehabilitation, Thumb function, Soft actuators, Pneumatic artificial rubber muscle},
abstract = {The main function of human hands is to grasp and manipulate objects, to which the thumb contributes the most. Various robotic hand rehabilitation devices have been developed for providing efficient hand function training. However, there have been few studies on thumb rehabilitation devices. Previously, we proposed a soft thumb rehabilitation device which is based on a parallel-link mechanism, driven by two different types of soft actuators. In this study, the device was integrated into a 5-digit assist system, in which fiber-reinforced elastomer actuators with improved bending angles, forces, and degrees of freedom were assembled onto a forearm socket. The device was evaluated by an enhanced Kapandji-Test, which included also a pressing force measurement in addition to the reachable positions of the thumb on the opposing fingers. The results showed that with the proposed approach, thumb functions for hand rehabilitation could be realized, which paves the way towards a full hand rehabilitation package with the 5-digit soft robotic hand rehabilitation system.}
}