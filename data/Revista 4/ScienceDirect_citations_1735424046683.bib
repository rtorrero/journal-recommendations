@article{SONG2022103523,
title = {Decomposition and replacement: Spatial knowledge distillation for monocular depth estimation},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103523},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103523},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000669},
author = {Minsoo Song and Wonjun Kim},
keywords = {Monocular depth estimation, Knowledge distillation, Laplacian pyramid, ReplaceBlock},
abstract = {Knowledge distillation has become a key technique for making smart and light-weight networks through model compression and transfer learning. Unlike previous methods that applied knowledge distillation to the classification task, we propose to exploit the decomposition-and-replacement based distillation scheme for depth estimation from a single RGB color image. To do this, Laplacian pyramid-based knowledge distillation is firstly presented in this paper. The key idea of the proposed method is to transfer the rich knowledge of the scene depth, which is well encoded through the teacher network, to the student network in a structured way by decomposing it into the global context and local details. This is fairly desirable for the student network to restore the depth layout more accurately with limited resources. Moreover, we also propose a new guidance concept for knowledge distillation, so-called ReplaceBlock, which replaces blocks randomly selected in the decoded feature of the student network with those of the teacher network. Our ReplaceBlock gives a smoothing effect in learning the feature distribution of the teacher network by considering the spatial contiguity in the feature space. This process is also helpful to clearly restore the depth layout without the significant computational cost. Based on various experimental results on benchmark datasets, the effectiveness of our distillation scheme for monocular depth estimation is demonstrated in details. The code and model are publicly available at : https://github.com/tjqansthd/Lap_Rep_KD_Depth.}
}
@article{FANG2022103542,
title = {Fast coding unit partitioning algorithms for versatile video coding intra coding},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103542},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103542},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000827},
author = {Jiunn-Tsair Fang and Bang-Hao Liu and Pao-Chi Chang},
keywords = {Versatile video coding, Quadtree with nested multi-type tree, Coding unit, Intra coding, Convolutional neural network},
abstract = {Versatile video coding (VVC) is the newest video compression standard. It adopts quadtree with nested multi-type tree (QT-MTT) to encode square or rectangular coding units (CUs). The QT-MTT coding structure is more flexible for encoding video texture, but it is also accompanied by many time-consuming algorithms. So, this work proposes fast algorithms to determine horizontal or vertical split for binary or ternary partition of a 32 × 32 CU in the VVC intra coding to replace the rate-distortion optimization (RDO) process, which is time-consuming. The proposed fast algorithms are actually a two-step algorithm, including feature analysis method and deep learning method. The feature analysis method is based on variances of pixels, and the deep learning method applies the convolution neural networks (CNNs) for classification. Experimental results show that the proposed method can reduce encoding time by 28.94% on average but increase Bjontegaard delta bit rate (BDBR) by about 0.83%.}
}
@article{REN2022103574,
title = {ADPNet: Attention based dual path network for lane detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103574},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103574},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001080},
author = {Fenglei Ren and Haibo Zhou and Lu Yang and Fulong Liu and Xin He},
keywords = {Lane detection, Semantic segmentation, Attention mechanism, Lane fitting},
abstract = {Recently, the task of lane detection has been greatly improved with the rapid development of deep learning and autonomous driving. However, there exist limitations like the challenging complex scenarios and real-time efficiency. In this paper, we present a novel Attention Based Dual Path Network (ADPNet) to handle the task of lane detection. The ADPNet treat the process of lane detection as a task of binary semantic segmentation, where the Detail Path is designed to capture detailed low-level information and the Semantic Path with dual attention module is designed to capture contextual high-level information. We use the Feature Aggregation Module to fuse the information of the two paths, followed by the process of lane fitting to get a parametric description of lanes. The proposed ADPNet achieves good trade-off between the accuracy and real-time efficiency on TuSimple and CULane, which are two popular lane detection benchmark datasets. The results demonstrate that our architecture outperforms the current state-of-the-art methods.}
}
@article{YE2022103589,
title = {PRA-TPE: Perfectly Recoverable Approximate Thumbnail-Preserving Image Encryption},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103589},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103589},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001122},
author = {Xi Ye and Yushu Zhang and Ruoyu Zhao and Rushi Lan and Yong Xiang},
keywords = {Difference expansion, Privacy protection, Perfect recoverability, Approximate thumbnail-preserving encryption},
abstract = {With the popularity of cloud servers, an increasing number of people are willing to store their images in the cloud due to many conveniences such as online browsing and managing images. On the other hand, this inevitably causes users’ concerns about image privacy leakage. Many image encryption schemes are proposed to prevent privacy leakage, while most of them focus only on privacy protection and ignore the usability of encrypted images. For this purpose, Marohn et al. (2017) designed two approximate thumbnail-preserving encryption (TPE) schemes to balance image privacy and usability. However, the decrypted image in these two schemes are only perceptually close to the original one and the original image cannot be perfectly recovered. To this end, we design a perfectly recoverable approximate TPE scheme in this paper, which combines reversibledata hiding (RDH) with encryption schemes. The thumbnails of the original and processed images are similar to balance image privacy and usability well. Meanwhile, the reversibility of RDH and encryption schemes is utilized to ensure the perfect recoverability in the proposed scheme. Experiments show that the proposed approximate TPE scheme is no longer limited to balancing usability and privacy but attains perfect recovery.}
}
@article{BHALLA2022103485,
title = {A fuzzy convolutional neural network for enhancing multi-focus image fusion},
journal = {Journal of Visual Communication and Image Representation},
volume = {84},
pages = {103485},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103485},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000396},
author = {Kanika Bhalla and Deepika Koundal and Bhisham Sharma and Yu-Chen Hu and Atef Zaguia},
keywords = {Deep learning, Fusion, Fuzzy sets, Multi-focus images},
abstract = {The images captured by the cameras contain distortions, misclassified pixels, uncertainties and poor contrast. Therefore, the multi-focus image fusion (MFIF) integrates various input image features to produce a single fused image using all its objects in focus. However, it is computationally complex, which leads to inconsistency. Hence, the MFIF method is employed to generate the fused image by integrating the fuzzy sets (FS) and convolutional neural network (CNN) to detect focused and unfocused parts in both source images. It is also compared with other competing six MFIF methods like Neutrosophic set based stationary wavelet transform (NSWT), guided filters, CNN, ensemble CNN, image fusion-based CNN and deep regression pair learning (DRPL). Benchmark datasets validate the superiority of the proposed FCNN method in terms of four non-reference assessment measures having mutual information (1.1678), edge information (0.7281), structural similarity (0.9850) and human perception (0.8020) and two reference metrics such as Peak signal-to-noise ratio (57.23) and root mean square error (1.814).}
}
@article{SHANG2022103524,
title = {Cattle behavior recognition based on feature fusion under a dual attention mechanism},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103524},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103524},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000670},
author = {Cheng Shang and Feng Wu and MeiLi Wang and Qiang Gao},
keywords = {Dual attention mechanism, Feature fusion, Behavior recognition, Smart animal husbandry},
abstract = {In recent years, artificial intelligence has been widely used in such fields as agricultural informatization, precision agriculture and precision animal husbandry. Due to limited research on deep learning in real-time agricultural and pastoral situations, deep learning and computer vision have become very important topics in the agricultural field. Recent studies have shown that the fusion of features under different attention mechanisms will help advance the utilization of such features, and will thus influence the accuracy and generalization ability of the models used. In this paper, we propose a lightweight network structure based on feature fusion under a dual attention mechanism with the same activation and joint loss functions. More specifically, we propose an innovative method to improve the network structure of two different attention mechanisms, and achieve feature fusion by combining the two. At the same time, we keep the activation functions consistent with those of the original network structure, and we develop a joint loss function to expand the use of various features. We also take the novel approach of applying the trajectory behavior analysis method to walking and standing. Experiments using both a publicly available data set and a data set obtained from a farm show that our algorithm achieves state-of-the-art performance in terms of accuracy and generalization ability, as compared to other methods.}
}
@article{LIU2022103454,
title = {A novel dynamic gesture understanding algorithm fusing convolutional neural networks with hand-crafted features},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103454},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103454},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000153},
author = {Yanhong Liu and Shouan Song and Lei Yang and Guibin Bian and Hongnian Yu},
keywords = {Dynamic gesture understanding, Transfer learning, Feature fusion, Dempster–Shafer evidence theory, Support vector machine},
abstract = {Dynamic gestures have attracted much attention in recent years due to their user-friendly interactive characteristics. However, accurate and efficient dynamic gesture understanding remains a challenge due to complex scenarios and motion information. Conventional handcrafted features are computationally cheap but can only extract low-level image features. This leads to performance degradation when dealing with complex scenes. In contrast, deep learning-based methods have a stronger feature expression ability and hence can capture more abstract and high-level image features. However, they critically rely on a large amount of training data. To address the above issues, a novel dynamic gesture understanding algorithm based on feature fusion is proposed for accurate dynamic gesture prediction. It leverages the advantages of handcrafted features and transfer learning. Aimed at small-scale dynamic gesture data, transfer learning is introduced for capturing effective feature expression. To precisely model the critical temporal information associated with dynamic gestures, a novel feature descriptor, namely, AlexNet2, is proposed for effective feature expression of dynamic gestures from the spatial and temporal domain. On this basis, a decision-level feature fusion framework based on support vector machine (SVM) and Dempster–Shafer (DS) evidence theory is constructed to utilize handcrafted features and AlexNet2 to realize high-precision dynamic gesture understanding. To verify the effectiveness and robustness of the proposed recognition algorithm, analysis and comparison experiments are performed on the public Cambridge gesture dataset and Northwestern University hand gesture dataset. The proposed gesture recognition algorithm achieves prediction accuracies of 99.50% and 96.97% on these two datasets. Experimental results show that the proposed recognition framework exhibits a better recognition performance in comparison with related prediction algorithms.}
}
@article{XU2022103510,
title = {A reliable and unobtrusive approach to display area detection for imperceptible display camera communication},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103510},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103510},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000554},
author = {Jianshuang Xu and Johannes Klein and Jörn Jochims and Niklas Weissner and Rüdiger Kays},
keywords = {Display area detection, Display camera communication, Rectangle detection, Pattern recognition, Visible light communication},
abstract = {Object framework detection has been extensively studied in computer vision for applications such as document digitization and whiteboard scanning. Similarly, it is essential for display-camera communication systems, particularly when imperceptible data modulation is employed to enable simultaneous video playback and data transmission. Reliable and accurate localization of the encoded display area is critical for data demodulation and decoding. However, existing systems typically adapt established methods developed for other applications that do not meet the system requirements for high-rate data transmission. In this article, we propose a novel method for display area detection in the camera images by embedding a new localization marker into the display corners. While the localization marker is less obtrusive than conventional fiducial markers, our detection algorithm demonstrated excellent reliability regardless of the display content and background, according to simulation and experimental results. In addition, the detector achieved subpixel accuracy and real-time performance with modern smartphones.}
}
@article{DAI2022103439,
title = {Detecting moving object from dynamic background video sequences via simulating heat conduction},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103439},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103439},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000037},
author = {Yuan Dai and Long Yang},
keywords = {Moving object detection, Dynamic background, Heat conduction, Entropy, K-means clustering},
abstract = {Moving object detection is one of the essential tasks for surveillance video analysis. The dynamic background often composed by waving trees, rippling water or fountains, etc. in nature scene greatly interferes with the detection of moving objects in the form of noise. In this paper, a method simulating heat conduction is proposed to extract moving objects from dynamic background video sequences. Based on the visual background extractor (ViBe) with an adaptable distance threshold, we design a temperature field relying on the generated mask image to distinguish between the moving objects and the noise caused by dynamic background. In temperature field, a brighter pixel is associated with more energy. It will transfer a certain amount of energy to its neighboring darker pixels. Through multiple steps of energy transfer the noise regions loss more energy so that they become darker than the detected moving objects. After heat conduction, K-Means algorithm with the customized initial clustering centers is utilized to separate the moving objects from background. We test our method on many videos with dynamic background from public datasets. The results show that the proposed method is feasible and effective for moving object detection from dynamic background sequences.}
}
@article{JIN2022103564,
title = {Image blind restoration based on degradation representation network},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103564},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103564},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001006},
author = {Yan Jin and Zhiwei Jiang and Zhizhong Xue and Yibiao Hu},
keywords = {Deep learning, Contrastive learning, Image restoration, Convolution neural network},
abstract = {Most deep learning (DL)-based image restoration methods have exploited excellent performance by learning a non-linear mapping function from low quality images to high quality images. However, two major problems restrict the development of the image restoration methods. First, most existing methods based on fixed degradation suffer from significant performance drop when facing the unknown degradation, because of the huge gap between the fixed degradation and the unknown degradation. Second, the unknown-degradation estimation may lead to restoration task failure due to uncertain estimation errors. To handle the unknown degradation in the real application, we introduce a degradation representation network for single image blind restoration (DRN). Different from the methods of estimating pixel space, we use an encoder network to learn abstract representations for estimating different degradation kernels in the representation space. Furthermore, a degradation perception module with flexible adaptability to different degradation kernels is used to restore more structural details. In our experiments, we compare our DRN with several state-of-the-art methods for two image restoration tasks, including image super-resolution (SR) and image denoising. Quantitative results show that our degradation representation network is accurate and efficient for single image restoration.}
}
@article{SHAO2022103529,
title = {Multi-stream feature refinement network for human object interaction detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {86},
pages = {103529},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103529},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000712},
author = {Zhanpeng Shao and Zhongyan Hu and Jianyu Yang and Youfu Li},
keywords = {Human object interaction, Action recognition, Feature refinement and learning, Multi-stream neural network, Human pose, Object semantic information},
abstract = {Human–Object Interaction (HOI) detection is a crucial problem for comprehensive visual understanding, which aims to detect <human,action,object> triplets within an image. Many existing methods often exploit to integrate the human and object visual features , the spatial layout of human–object pairs, human poses, contextual information, and even object semantic information into a framework to infer the interactions, proving that all these components can contribute to improve the HOI detection. However, most methods simply concatenate these components that are not explicitly embedded in the feature learning for HOI detection. In this paper, we are trying to fuse these components explicitly using a multi-stream feature refinement network. The network extracts the visual features of humans, contexts, and objects, which receives the attentions from human poses, spatial configurations, and semantic prior knowledge of objects to refine these visual features, respectively. In addition, an additional graph neural network is employed here to learn the structural features of human–object pairs. We verify our method on V-COCO and HICO-DET datasets with extensive experiments. The experimental results demonstrate that our method is a simple yet effective for HOI detection, achieving superior performance to those state-of-the-art methods.}
}
@article{ZHANG2022103600,
title = {Imitative Collaboration: A mirror-neuron inspired mixed reality collaboration method with remote hands and local replicas},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103600},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103600},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001249},
author = {Zhenning Zhang and Zhigeng Pan and Weiqing Li and Zhiyong Su},
keywords = {Mixed reality, Remote collaboration, Mirror-neuron, Gestural interaction, Point cloud interaction},
abstract = {Mixed reality can overlay and display 3D digital content in the real world, convey abstract concepts to users, and promote the understanding of complex tasks. However, the abstract graphics overlaid on the physical space may cause a certain cognitive load for local users and reduce the efficiency of collaboration. To improve the efficiency of remote collaboration, we conducted an elicitation study on assembly tasks, explored the user needs for collaboration, and defined the design goals of our remote collaboration method. Inspired by the mirror-neuron mechanism, we present an imitative collaboration method that allows local users to imitate the interaction behavior of remote users to complete tasks. We also propose a series of interaction methods for remote users to select, copy, and interact with the local point clouds to facilitate the expression of collaboration intentions. Finally, the results of a user study evaluating our imitative collaboration method on assembly tasks are reported, confirming that our method improves collaboration efficiency while reducing the cognitive load of local users.}
}
@article{ZHANG2022103581,
title = {Fine-grained-based multi-feature fusion for occluded person re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103581},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103581},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001109},
author = {Guoqing Zhang and Chao Chen and Yuhao Chen and Hongwei Zhang and Yuhui Zheng},
keywords = {Occluded person re-identification, Multi-granularity feature, Feature fusion},
abstract = {Many previous occluded person re-identification(re-ID) methods try to use additional clues (pose estimation or semantic parsing models) to focus on non-occluded regions. However, these methods extremely rely on the performance of additional clues and often capture pedestrian features by designing complex modules. In this work, we propose a simple Fine-Grained Multi-Feature Fusion Network (FGMFN) to extract discriminative features, which is a dual-branch structure consisting of global feature branch and partial feature branch. Firstly, we utilize a chunking strategy to extract multi-granularity features to make the pedestrian information contained in it more comprehensive. Secondly, a spatial transformer network is introduced to localize the pedestrian’s upper body, and then introduce a relation-aware attention module to explore the fine-grained information. Finally, we fuse the features obtained from the two branches to obtain a more robust pedestrian representation. Extensive experiments verify the effectiveness of our method under the occlusion scenario.}
}
@article{GONG2022103541,
title = {Unified Chinese License Plate detection and recognition with high efficiency},
journal = {Journal of Visual Communication and Image Representation},
volume = {86},
pages = {103541},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103541},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000815},
author = {Yanxiang Gong and Linjie Deng and Shuai Tao and Xinchen Lu and Peicheng Wu and Zhiwei Xie and Zheng Ma and Mei Xie},
keywords = {Chinese license plate dataset, License plate detection and recognition, End-to-end, Real-time},
abstract = {Recently, deep learning-based methods have reached an excellent performance on License Plate (LP) detection and recognition tasks. However, it is still challenging to build a robust model for Chinese LPs since there are not enough large and representative datasets. In this work, we propose a new dataset named Chinese Road Plate Dataset (CRPD) that contains multi-objective Chinese LP images as a supplement to the existing public benchmarks. The images are mainly captured with electronic monitoring systems with detailed annotations. To our knowledge, CRPD is the largest public multi-objective Chinese LP dataset with annotations of vertices. With CRPD, a unified detection and recognition network with high efficiency is presented as the baseline. The network is end-to-end trainable with totally real-time inference efficiency (30 fps with 640 p). The experiments on several public benchmarks demonstrate that our method has reached competitive performance. The code and dataset will be publicly available at https://github.com/yxgong0/CRPD.}
}
@article{CHEN2022103424,
title = {Security measurement of a medical communication scheme based on chaos and DNA coding},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103424},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103424},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002868},
author = {Lei Chen and Chengqing Li and Chao Li},
keywords = {Cryptanalysis, Chaotic cryptography, Chosen-plaintext attack, DNA coding, DICOM image security, Privacy protection},
abstract = {To encrypt sensitive information existing in a color DICOM images, a medical privacy protection scheme (called as MPPS) based on chaos and DNA coding was proposed by using two coupled chaotic systems to produce cryptographic primitives. Relying on some empirical analyzes and experimental results, the designers of MPPS claimed that it can withstand a chosen-plaintext attack and some other classic attacking models. However, this statement is groundless. In this paper, we investigate the essential properties of MPPS and DNA coding, and we then propose an efficient chosen-plaintext attack to disclose its equivalent secret-key. The attack only needs ⌈log256(3⋅M⋅N)⌉+4 pair of chosen plain-images and the corresponding cipher-images, where M×N and “3” are the size of the RGB color image and the number of color channels, respectively. In addition, the other claimed superiorities are questioned from the perspective of modern cryptography. Both theoretical and experimental results are presented to support the efficiency of the proposed attack and the other reported security faults. The proposed cryptanalysis results will promote the proper application of DNA encoding to protect multimedia privacy data, especially that in a DICOM image.}
}
@article{DING2022103410,
title = {Graph-based relational reasoning in a latent space for skeleton-based action recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103410},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103410},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002741},
author = {Wenwen Ding and GuangHui Zhou and ChongYang Ding and Guang Li and Kai Liu},
keywords = {Deep learning, Graph neural networks, Graph convolutional network, Message passing, Spatio-temporal graph, Grassmannian geometry},
abstract = {Motivated by the powerful capability of deep neural networks in feature learning, a new graph-based neural network is proposed to learn local and global relational information on skeleton sequences represented as spatio-temporal graphs (STGs). The pipeline of our network architecture consists of three main stages. As the first stage, spatial–temporal sub-graphs (sub-STGs) are projected into a latent space in which every point is represented as a linear subspace. The second stage is based on message passing to acquire the localized correlated features of the nodes in the latent space. The third stage relies on graph convolutional networks (GCNs) to reason the long-range spatio-temporal dependencies through a graph representation of the latent space. Finally, the average pooling layer and the softmax classifier are then employed to predict the action categories based on the extracted local and global correlations. We validate our model in terms of action recognition using three challenging datasets: the NTU RGB+D, Kinetics Motion, and SBU Kinect Interaction datasets. The experimental results demonstrate the effectiveness of our approach and show that our proposed model outperforms the state-of-the-art methods.}
}
@article{WANG2022103556,
title = {On the multi-level embedding of crypto-image reversible data hiding},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103556},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103556},
url = {https://www.sciencedirect.com/science/article/pii/S104732032200092X},
author = {Xu Wang and Ching-Chun Chang and Chia-Chen Lin and Chin-Chen Chang},
keywords = {Reversible data hiding, Encrypted images, Multi-Level, Block mapping, Very high embedding capacity},
abstract = {Crypto-space reversible image steganography has attracted increasing attention, given its ability to embed authentication information without revealing the image content. This paper presents an efficient reversible data hiding scheme for crypto-images: a block predictor is applied to compute prediction errors, then an adaptive block mapping algorithm is utilized to compress them whose amplitudes are within a small threshold, finally, this strategy can be applied in a multi-level manner to achieve a higher embedding capacity. Due to the correlations among adjacent pixels in the block, images can be sufficiently compressed to reserve abundant space for additional data embedding. Different from the prior arts, the compression code of the image is fully encrypted. Experimental results verify that the embedded data and original image can be perfectly recovered, the security is higher compared with the state-of-the-arts, and a significant improvement in the average embedding rate is achieved on two large-scale image datasets.}
}
@article{LANG2022103592,
title = {Discriminative feature mining hashing for fine-grained image retrieval},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103592},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103592},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001195},
author = {Wenxi Lang and Han Sun and Can Xu and Ningzhong Liu and Huiyu Zhou},
keywords = {Fine-grained image retrieval, Attention drop, Attention re-sample, Deep hashing},
abstract = {With the development of multimedia technology, fine-grained image retrieval has gradually become a new hot topic in computer vision, while its accuracy and speed are limited due to the low discriminative high-dimensional real-valued embedding. To solve this problem, we propose an end-to-end framework named DFMH (Discriminative Feature Mining Hashing), which consists of the DFEM (Discriminative Feature Extracting Module) and SHCM (Semantic Hash Coding Module). Specifically, DFEM explores more discriminative local regions by attention drop and obtains finer local feature expression by attention re-sample. SHCM generates high-quality hash codes by combining the quantization loss and bit balance loss. Validated by extensive experiments and ablation studies, our method consistently outperforms both the state-of-the-art generic retrieval methods as well as fine-grained retrieval methods on three datasets, including CUB Birds, Stanford Dogs and Stanford Cars.}
}
@article{WANG2022103580,
title = {Self-supervised multi-scale pyramid fusion networks for realistic bokeh effect rendering},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103580},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103580},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001110},
author = {Zhifeng Wang and Aiwen Jiang and Chunjie Zhang and Hanxi Li and Bo Liu},
keywords = {Bokeh rendering, Circle of confusion, Self-supervised, Multi-scale fusion, Structure consistency},
abstract = {Images with visual pleasing bokeh effect are often unattainable for mobile cameras with compact optics and tiny sensors. To balance the aesthetic requirements on photo quality and expensive high-end SLR cameras, synthetic bokeh effect rendering has emerged as an attractive machine learning topic for engineering applications on imaging systems. However, most of bokeh rendering models either heavily relied on prior knowledge such as scene depth or were topic-irrelevant data-driven networks without task-specific knowledge, which restricted models’ training efficiency and testing accuracy. Since bokeh is closely related to a phenomenon called ”circle of confusion”, therefore, in this paper, following the principle of bokeh generation, a novel self-supervised multi-scale pyramid fusion network has been proposed for bokeh rendering. During the pyramid fusion process, structure consistencies are employed to emphasize the importance of respective bokeh components. Task-specific knowledge which mimics the ”circle of confusion” phenomenon through disk blur convolutions is utilized as self-supervised information for network training. The proposed network has been evaluated and compared with several state-of-the-art methods on a public large-scale bokeh dataset- the ”EBB!” Dataset. The experiment performance demonstrates that the proposed network has much better processing efficiency and can achieve better realistic bokeh effect with much less parameters size and running time. Related source codes and pre-trained models of the proposed model will be available soon on https://github.com/zfw-cv/MPFNet.}
}
@article{LIU2022103452,
title = {Multimodal face aging framework via learning disentangled representation},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103452},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103452},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000141},
author = {Lu Liu and Shenghui Wang and Lili Wan and Haibo Yu},
keywords = {Face aging, Disentangled representation, Variational auto-encoder, KL divergence, Generative adversarial network},
abstract = {Existing face aging (FA) approaches usually concentrate on a universal aging pattern, and produce restricted aging faces from one-to-one mapping. However, the diversity of living environments impact individuals differently in their oldness. To simulate various aging effects, we propose a multimodal FA framework based on face disentanglement technique of age-specific and age-irrelevant information. A Variational Autoencoder (VAE)-based encoder is designed to represent the distribution of the age-specific attributes. To capture the age-irrelevant features, a cycle-consistency loss of unpaired faces is utilized among various age spans. The extensive experimental results demonstrate that the sampled age-specific codes along with an age-irrelevant feature make the multimodal FA diverse and realistic.}
}
@article{YU2022103611,
title = {EPLL image restoration with a bounded asymmetrical Student’s-t mixture model},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103611},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103611},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001353},
author = {Qiqiong Yu and Guo Cao and Hao Shi and Youqiang Zhang and Peng Fu},
keywords = {EPLL image restoration, Finite mixture model, Bounded asymmetrical Student’s-t mixture model, Anisotropic nonlocal self-similarity, Regularization parameters},
abstract = {The expected patch log-likelihood (EPLL) model is a patch prior-based image restoration method which received extensive attention in image processing in recent years for its outstanding ability to preserve the detail and structure. However, due to using the Gaussian mixture model (GMM) with the noise sensitivity as the local prior, the EPLL model suffers from undesired artifact and poor robustness frequently. In this paper, to restrain the generation of artifact of EPLL model, we replace the GMM with a bounded asymmetrical Student’s-t mixture model (BASMM), which is sufficiently flexible to fit different shapes of image data, such as non-Gaussian, non-symmetric, and bounded support data. Then, the anisotropic nonlocal self-similarity (ANSS) based regularization parameters are designed to improve the robustness of the proposed model. Experimental results demonstrate the competitiveness of our proposed model compared with that of state-of-the-art methods in performance both visually and quantitatively.}
}
@article{GUO2022103484,
title = {An adaptive kernelized correlation filters with multiple features in the tracking application},
journal = {Journal of Visual Communication and Image Representation},
volume = {84},
pages = {103484},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103484},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000384},
author = {Dequan Guo and Gexiang Zhang and Ferrante Neri and Sheng Peng and Qiang Yang and Paul Liu},
keywords = {Visual properties, Target tracking, Multiple features, Kernelized correlation filter},
abstract = {Automatic target detection and tracking systems are used extensively in complex scenes. In long-term tracking, some visual attributes of objects are changing, such as illumination, size, profile, and so on. To address the issue, it is particularly important to describe the essential properties of the objects in tracking. An enhanced kernelized correlation filter tracking strategy fused multiple features with location prediction is proposed. To make the object appearance models more accuracy and robustness, based on the original histogram of oriented gradient features, we integrate the hue, saturation, value, and grayscale information to construct a new descriptor to represent the target appearance. Moreover, location prediction and bi-linear interpolation are employed to obtain the more accurate target position. Experiments show that the proposed strategy can obtain superior or competitive performance in challenging benchmark data sets. In practice, the algorithm is applied to track shuttle bus targets in the airport apron.}
}
@article{LI2022103573,
title = {Deep image compression based on multi-scale deformable convolution},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103573},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103573},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001079},
author = {Daowen Li and Yingming Li and Heming Sun and Lu Yu},
keywords = {Deep image compression, Multi-scale deformable convolution, Spatial attention},
abstract = {Deep image compression efficiency has been improved in the past years. However, to fully exploit context information for compressing image objects of different scales and shapes, more adaptive geometric structure of inputs should be considered. In this paper, we novelly introduce deformable convolution and its spatial attention extension into deep image compression task to fully exploit the context information. Specifically, a novel deep image compression network with Multi-Scale Deformable Convolution and Spatial Attention, named MS-DCSA, is proposed to better extract compact and efficient latent representation as well as reconstruct higher-quality images. First, multi-scale deformable convolution is presented to provide multi-scale receptive fields for learning spatial sampling offsets in deformable operations. Subsequently, multi-scale deformable spatial attention module is developed to generate attention masks to re-weight extracted features according to their importance. In addition, the multi-scale deformable convolution is applied to design delicate up/down sampling modules. Extensive experiments demonstrate that the proposed MS-DCSA network achieves improved performance on both PSNR and MS-SSIM quality metrics, compared to conventional as well as competing deep image compression methods.}
}
@article{MUKHERJEE2022103470,
title = {FuseKin: Weighted image fusion based kinship verification under unconstrained age group},
journal = {Journal of Visual Communication and Image Representation},
volume = {84},
pages = {103470},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103470},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000268},
author = {Moumita Mukherjee and Toshanlal Meenpal and Aarti Goyal},
keywords = {Kinship verification, Image fusion, Feature selection, Face frontalization},
abstract = {Kinship verification using facial images is mainly performed with a single face sample per person. To perform with a single sample, it is very difficult to specify an age group where kin pairs may have higher similarities. To address the above problem, we propose a novel weighted multi sample fusion (WMSF) method. The proposed WMSF method combines kin signals present in multiple samples per person (MSPP) to form a FuseKin image. To select the most discriminant features from the extracted feature vector, we propose a patch based discriminative analysis (PDA) method. Weights are calculated using the PDA method so as to reduce the discrimination between positive FuseKin pairs. Experiments were conducted on two different datasets which contain multiple face image samples per person, namely Family101 and Family in the Wild (FIW) to validate the performance of the proposed methods. Our method achieves competitive results as compared to other state-of-the-art methods.}
}
@article{FAN2022103508,
title = {Video anomaly detection using CycleGan based on skeleton features},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103508},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103508},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000542},
author = {Zheyi Fan and Shuhan Yi and Di Wu and Yu Song and Mengjie Cui and Zhiwen Liu},
keywords = {Anomaly detection, Cycle-consistent adversarial networks, Pose estimation, Dynamic skeleton feature, Reconstruction error},
abstract = {Anomaly detection is a challenging task in the field of intelligent video surveillance. It aims to identify anomalous events by monitoring the video captured by visual sensors. The main difficulty of this task is that the definition of anomalies is ambiguous. In recent years, most anomaly detection methods use a two-stage learning strategy, i.e., feature extraction and model building. In this paper, with the idea of refactoring, we propose an end-to-end anomaly detection framework using cyclic consistent adversarial networks (CycleGAN). Dynamic skeleton features are used as network constraints to alleviate the inaccuracy of feature extraction algorithms of a single generative adversarial network. In the training phase, only normal video frames and the corresponding skeleton features are used to train the generator and discriminator. In the testing phase, anomalous behaviors with high reconstruction errors can be filtered out by manually set thresholds. To the best of our knowledge, this is the first time CycleGAN has been used for video anomaly detection. Experimental results on challenging datasets show that our method can accurately detect anomalous behaviors in videos collected by video surveillance systems and is comparable to the current state-of-the-art methods.}
}
@article{WANG2022103450,
title = {BGGMM-HMT based locally optimum image watermark detector in high-order NSST difference domain},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103450},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103450},
url = {https://www.sciencedirect.com/science/article/pii/S104732032200013X},
author = {Xiang-yang Wang and Xin Shen and Pan-pan Niu and Hong-ying Yang},
keywords = {Image watermarking, Statistical watermark detector, High-order difference coefficients, Bounded generalized Gaussian mixture, Hidden Markov tree, Nonsubsampled Shearlet transform},
abstract = {Imperceptibility, robustness and data payload are three main requirements of any image watermarking systems to guarantee desired functionalities, but there is a tradeoff among them from the information-theoretic perspective. How to achieve this balance is a major challenge. In this paper, we propose a new statistical image watermarking scheme, which is based on the high-order difference coefficients in nonsubsampled Shearlet transform (NSST) domain and the bounded generalized Gaussian mixture model-based hidden Markov tree (BGGMM-HMT). In the watermark embedding process, we use a nonlinear embedding approach to hide the digital watermark into the robust high-order difference coefficients, which can achieve better imperceptibility. In the watermark detection process, high-order difference coefficients are accurately modeled by using BGGMM-HMT, where the distribution characteristics of high-order difference coefficients can be captured through BGGMM, and the scale dependencies of high-order difference coefficients can be captured through HMT. Statistical model parameters are then estimated by combining the approach of minimizing the higher bound on data negative log-likelihood function and upward–downward algorithm. Finally, an image watermark detector based on BGGMM-HMT is developed using the locally optimum (LO) decision rule. For the proposed detector, the receiver operating characteristic (ROC) expression is derived in detail. We evaluate the proposed scheme from different aspects and compare it with the state-of-the-art schemes. After a large number of experimental tests, the encouraging results obtained prove the effectiveness of our watermarking scheme.}
}
@article{ASLAM2022103598,
title = {A3N: Attention-based adversarial autoencoder network for detecting anomalies in video sequence},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103598},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103598},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001237},
author = {Nazia Aslam and Prateek Kumar Rai and Maheshkumar H. Kolekar},
keywords = {Anomaly detection, Attention mechanism, Adversarial autoencoder, Generative adversarial network},
abstract = {This paper presents a novel attention-based adversarial autoencoder network (A3N) that consists of a two-stream decoder to detect abnormal events in video sequences. The first stream of the decoder is a reconstructive model responsible for recreating the input frame sequence. However, the second stream is a future predictive model used to predict the future frame sequence through adversarial learning. A global attention mechanism is employed at the decoder side that helps to decode the encoded sequences effectively. The training of A3N is carried out on normal video data. The attention-based reconstructive model is used during the inference stage to compute the anomaly score. A3N delivers a considerable average speed of 0.0227 s (∼44 fps ) for detecting anomalies in the testing phase on used datasets. Several experiments and ablation analyses have been performed on UCSD Pedestrian, CUHK Avenue and ShanghaiTech datasets to validate the efficiency of the proposed model.}
}
@article{CHEN2022103489,
title = {Cheating in (halftone-secret) visual cryptography: Analysis of blind authentication schemes},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103489},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103489},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000426},
author = {Yu-Chi Chen and Gwoboa Horng},
keywords = {Visual cryptography, Human vision system, Binocular cheating, Cheating prevention, Security},
abstract = {In visual cryptography (VC), cheating is an important security concern where dishonest participants will fool honest ones and make them accept a fake secret by providing fake shares. Share and blind authentications are two categories of cheating prevention, and the last one relies on the inherent robust of shares against cheating attacks. In the previous studies, cheating in VC only focuses on operating a ‘pixel block’ instead of a region of adjacent pixels. However, the well-known advantage of VC is to decode the secret image by using the human vision system (HVS), so it leads to a natural issue to reconsider cheating a region. In this paper, we formally address the binocular cheating attack (BCA) for a region to augment effectiveness of original cheating for a block. Finally, we demonstrate how to realize BCA by presenting non-trivial techniques against some blind authentication schemes, and further obtain implausible results. The BCA can also be applied to halftone secret.}
}
@article{YANG2022103553,
title = {Blind quality assessment of tone-mapped images using multi-exposure sequences},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103553},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103553},
url = {https://www.sciencedirect.com/science/article/pii/S104732032200089X},
author = {Jiachen Yang and Yanshuang Zhou and Yang Zhao and Jiabao Wen},
keywords = {High dynamic range, Tone-mapped image, Image quality assessment (IQA)},
abstract = {The tone mapping operator (TMO) enables high dynamic range (HDR) images to be presented on low dynamic range (LDR) consumer electronic devices. However, the results obtained by this method are not always ideal due to the reduced number of bits. In comparison, the multi-exposure image fusion (MEF) bypasses the intermediate HDR image composition and directly produces an image presented on standard devices. Inspired by this, this paper proposes a quality assessment method for tone-mapped image (TMI) based on generating multi-exposure sequences. Specifically, the method uses a generative adversarial network (GAN) to generate a set of sequences with different exposure levels based on the TMIs. Then a two-branch convolutional neural network (CNN) is used to extract features from the tone-mapped images and the multi-exposure reference sequences, respectively. Finally, the transformer is used to mine the intrinsic connections between TMIs and multi-exposure sequences and learn the mapping relationships from feature space to quality space. We conducted extensive experiments on the ESPL-LIVE HDR database. The applicability and effectiveness of the proposed method are verified by comparing and analyzing relevant features and model configurations with existing mainstream evaluation algorithms.}
}
@article{TANG2022103527,
title = {Counting Objects by Diffused Index: Geometry-free and training-free approach},
journal = {Journal of Visual Communication and Image Representation},
volume = {86},
pages = {103527},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103527},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000700},
author = {Mengyi Tang and Maryam Yashtini and Sung Ha Kang},
keywords = {Object counting, Variational analysis, Alternating minimization, Fast methods, Clustering, Convergence analysis},
abstract = {Counting objects is a fundamental but challenging problem. In this paper, we propose diffusion-based, geometry-free, and learning-free methodologies to count the number of objects in images. The main idea is to represent each object by a unique index value regardless of its intensity or size, and to simply count the number of index values. First, we place different vectors, refer to as seed vectors, uniformly throughout the mask image. The mask image has boundary information of the objects to be counted. Secondly, the seeds are diffused using an edge-weighted harmonic variational optimization model within each object. We propose an efficient algorithm based on an operator splitting approach and alternating direction minimization method, and theoretical analysis of this algorithm is given. An optimal solution of the model is obtained when the distributed seeds are completely diffused such that there is a unique intensity within each object, which we refer to as an index. For computational efficiency, we stop the diffusion process before a full convergence, and propose to cluster these diffused index values. We refer to this approach as Counting Objects by Diffused Index (CODI). We explore scalar and multi-dimensional seed vectors. For Scalar seeds, we use Gaussian fitting in histogram to count, while for vector seeds, we exploit a high-dimensional clustering method for the final step of counting via clustering. The proposed method is flexible even if the boundary of the object is not clear nor fully enclosed. We present counting results in various applications such as biological cells, agriculture, concert crowd, and transportation. Some comparisons with existing methods are presented.}
}
@article{TANWAR2022103503,
title = {SecureDL: A privacy preserving deep learning model for image recognition over cloud},
journal = {Journal of Visual Communication and Image Representation},
volume = {86},
pages = {103503},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103503},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000529},
author = {Vishesh Kumar Tanwar and Balasubramanian Raman and Amitesh Singh Rajput and Rama Bhargava},
keywords = {Cloud computing, Image classification, Permutation ordered binary number system, Encrypted domain},
abstract = {The key benefits of cloud services such as low cost, access flexibility, and mobility have attracted worldwide users to utilize deep learning algorithms for computer vision. These cloud servers are maintained by third parties, where users are always concerned about sharing their confidential data with them. In this paper, we addressed these concerns for by developing SecureDL, a privacy-preserving image recognition model for encrypted data over cloud. The proposed block-based image encryption scheme is well designed to protect image’s visual information. The scheme constitutes an order-preserving permutation ordered binary number system and pseudo-random matrices. The proposed method is proved to be secure in a probabilistic viewpoint, and using various cryptographic attacks. Experiments are conducted over several image recognition datasets, and the trade-off analytics between the achieved recognition accuracy and data encryption is well described. SecureDL overcomes the storage and computational overheads that occur with fully-homomorphic and multi-party computation based secure recognition schemes.}
}
@article{ZHAOXIN2022103591,
title = {Crowd counting in complex scenes based on an attention aware CNN network},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103591},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103591},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001183},
author = {Li Zhaoxin and Lu Shuhua and Lan Lingqiang and Liu Qiyuan},
keywords = {Crowd counting, Density estimation, Attentive maps},
abstract = {Crowd counting with density estimation has been an active research community due to its significant applications in the fields of public security, video surveillance, traffic monitoring. However, Crowd counting for congested scenes often suffers from some obstacles including severe occlusions, large scale variations, noise interference, etc. In this paper, using the first ten layers of a modified VGG16 and dilated convolution layers as the framework, we have proposed a CNN based crowd counting and density estimation model improved by the attention aware modules with residual connections. To tackle the problem of noise interference, convolutional block attention modules have been introduced into the deep network to segment the foreground and background to focus on interest information, refining deeper features of the input image. To improve information transmission and reuse, residual connections are utilized to link 3 attention blocks. Meanwhile, dilated convolution layers keep larger reception fields and obtain high-resolution density maps. The proposed method has been evaluated on three public benchmarks, i.e. Shanghai Tech A & B, UCF-QNRF and MALL, achieving the mean absolute errors of 64.6 & 8.3, 113.8 and 1.68, respectively. The results outperform some existing excellent approaches. This indicates that the proposed model has high accuracy and better robustness, which is suitable for crowd counting and density estimation in various congested scenes.}
}
@article{GAO2022103481,
title = {High-capacity reversible data hiding in encrypted images based on adaptive block encoding},
journal = {Journal of Visual Communication and Image Representation},
volume = {84},
pages = {103481},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103481},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000372},
author = {Kai Gao and Ji-Hwei Horng and Chin-Chen Chang},
keywords = {Encrypted image, Data hiding, Adaptive block encoding, Embedding capacity},
abstract = {This paper proposes a new high-capacity reversible data hiding scheme in encrypted images. The content owner first divides the cover image into blocks. Then, the block permutation and the bitwise stream cipher processes are applied to encrypt the image. Upon receiving the encrypted image, the data hider analyzes the image blocks and adaptively decides an optimal block-type labeling strategy. Based on the adaptive block encoding, the image is compressed to vacate the spare room, and the secret data are encrypted and embedded into the spare space. According to the granted authority, the receiver can restore the cover image, extract the secret data, or do both. Experimental results show that the embedding capacity of the proposed scheme outperforms state-of-the-art schemes. In addition, security level and robustness of the proposed scheme are also investigated.}
}
@article{LIN2022103572,
title = {ACGAN: Attribute controllable person image synthesis GAN for pose transfer},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103572},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103572},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001067},
author = {ShaoYue Lin and YanJun Zhang},
keywords = {GAN, Person image synthesis, Artifact},
abstract = {At present, pose transfer and attribute control tasks are still the challenges for image synthesis network. At the same time, there are often artifacts in the images generated by the image synthesis network when the above two tasks are completed. The existence of artifacts causes the loss of the generated image details or introduces some wrong image information, which leads to the decline of the overall performance of the existing work. In this paper, a generative adversarial network (GAN) named ACGAN is proposed to accomplish the above two tasks and effectively eliminate artifacts in generated images. The proposed network was compared quantitatively and qualitatively with previous works on the DeepFashion dataset and better results are obtained. Moreover, the overall network has advantages over the previous works in speed and number of parameters.}
}
@article{AKBACAK2022103501,
title = {Deep multi-query video retrieval},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103501},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103501},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000530},
author = {Enver Akbacak and Cabir Vural},
keywords = {Video hashing, Pareto optimization, Multi-query video retrieval},
abstract = {Video retrieval methods have been developed for a single query. Multi-query video retrieval problem has not been investigated yet. In this study, an efficient and fast multi-query video retrieval framework is developed. Query videos are assumed to be related to more than one semantic. The framework supports an arbitrary number of video queries. The method is built upon using binary video hash codes. As a result, it is fast and requires a lower storage space. Database and query hash codes are generated by a deep hashing method that not only generates hash codes but also predicts query labels when they are chosen outside the database. The retrieval is based on the Pareto front multi-objective optimization method. Re-ranking performed on the retrieved videos by using non-binary deep features increases the retrieval accuracy considerably. Simulations carried out on two multi-label video databases show that the proposed method is efficient and fast in terms of retrieval accuracy and time.}
}
@article{JIN2022103436,
title = {Video frame deletion detection based on time–frequency analysis},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103436},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103436},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000013},
author = {Xiao Jin and Yuting Su and Peiguang Jing},
keywords = {Digital video forensics, Frame tampering detection, Nonlinear quantization, S transform},
abstract = {With the emergence of diverse multimedia editing software, a great number of edited or tampered video resources appear on the Internet, some of which can mix with the genuine ones. Digital video authenticity is an important step to make the best use of these video resources. As a common video forgery operation, frame tampering can change the video content and confuse viewers by removing or inserting some specific frames. In this paper, we explore the traces created by compression process and propose a new method to detect frame tampering based on the high-frequency features of reconstructed DCT coefficients in the tampered sequences. Experimental results demonstrate that our proposed method can effectively detect frame tampering operation, and accurately locate the breakpoint of frame tampering in the streams.}
}
@article{ZHOU2022103596,
title = {Shadow detection via multi-scale feature fusion and unsupervised domain adaptation},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103596},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103596},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001225},
author = {Kai Zhou and Wen Wu and Yan-Li Shao and Jing-Long Fang and Xing-Qi Wang and Dan Wei},
keywords = {Image processing, Shadow detection, Unsupervised domain adaptation, Multi-scale feature fusion},
abstract = {Shadow detection is significant for scene understanding. As a common scenario, soft shadows have more ambiguous boundaries than hard shadows. However, they are rarely present in the available benchmarks since annotating for them is time-consuming and needs expert help. This paper discusses how to transfer the shadow detection capability from available shadow data to soft shadow data and proposes a novel shadow detection framework (MUSD) based on multi-scale feature fusion and unsupervised domain adaptation. Firstly, we set the existing labeled shadow dataset (i.e., SBU) as the source domain and collect an unlabeled soft shadow dataset (SSD) as the target domain to formulate an unsupervised domain adaptation problem. Next, we design an efficient shadow detection network based on the double attention module and multi-scale feature fusion. Then, we use the global–local feature alignment strategy to align the task-related feature distributions between the source and target domains. This allows us to obtain a robust model and achieve domain adaptation effectively. Extensive experimental results show that our method can detect soft shadows more accurately than existing state-of-the-art methods.}
}
@article{LIANG2022103469,
title = {Multi-receptive Field Aggregation Network for single image deraining},
journal = {Journal of Visual Communication and Image Representation},
volume = {84},
pages = {103469},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103469},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000256},
author = {Songliang Liang and Xiaozhe Meng and Zhuo Su and Fan Zhou},
keywords = {Image deraining, Dilated convolution, Attention mechanism},
abstract = {Image deraining is a significant problem that ensures the visual quality of images to prompt computer vision systems. However, due to the insufficiency of captured rain streaks features and global information, current image deraining methods often face the issues of rain streaks remaining and image blurring. In this paper, we propose a Multi-receptive Field Aggregation Network (MRFAN) to restore a cleaner rain-free image. Specifically, we construct a Multi-receptive Field Feature Extraction Block (MFEB) to capture rain features with different receptive fields. In MFEB, we design a Self-supervised Block (SSB) and an Aggregation Block (AGB). SSB can make the network adaptively focus on the critical rain features and rain-covered areas. AGB effectively aggregates and redistributes the multi-scale features to help the network simulate rain streaks better. Experiments show that our method achieves better results on both synthetic datasets and real-world rainy images.}
}
@article{RAHMAN2022103521,
title = {Improving lung region segmentation accuracy in chest X-ray images using a two-model deep learning ensemble approach},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103521},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103521},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000657},
author = {Md Fashiar Rahman and Yan Zhuang and Tzu-Liang (Bill) Tseng and Michael Pokojovy and Peter McCaffrey and Eric Walser and Scott Moen and Alex Vo},
keywords = {Lung segmentation, Chest X-ray, Deep learning, UNet, CNNs, Ensemble},
abstract = {We propose a deep learning framework to improve segmentation accuracy of the lung region in Chest X-Ray (CXR) images. The proposed methodology implements a “divide and conquer” strategy where the original CXRs are subdivided into smaller image patches, segmented them individually, and then reassembled to achieve the complete segmentation. This approach ensembles two models, the first of which is a traditional Convolutional Neural Network (CNN) used to classify the image patches and subsequently merge them to obtain a pre-segmentation. The second model is a modified U-Net architecture to segment the patches and subsequently combines them to obtain another pre-segmented image. These two pre-segmented images are combined using a binary disjunction operation to get the initial segmentation, which is later post-processed to obtain the final segmentation. The post-processing steps consist of traditional image processing techniques such as erosion, dilation, connected component labeling, and region-filling algorithms. The robustness of the proposed methodology is demonstrated using two public (MC, JPCL) and one proprietary (The University of Texas Medical Branch - UTMB) datasets of CXR images. The proposed framework outperformed many state-of-the-arts competitions presented in the literature.}
}
@article{WENG2022103487,
title = {Adaptive reversible data hiding for JPEG images with multiple two-dimensional histograms},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103487},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103487},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000414},
author = {Shaowei Weng and Ye Zhou and Tiancong Zhang},
keywords = {Reversible data hiding, Multiple histograms, Two-dimensional histogram, Rate–distortion model},
abstract = {Joint photographic experts group (JPEG) can provide good quality with small file size but also eliminate extensively the redundancies of images. Therefore, hiding data into JPEG images in terms of maintaining high visual quality at small file sizes has been a great challenge for researchers. In this paper, an adaptive reversible data hiding method for JPEG images containing multiple two-dimensional (2D) histograms is proposed. Adaptability is mainly reflected in three aspects. The first one is to preferentially select sharper histograms for data embedding after K histograms are established by constructing the kth (k∈{1,2,…,K}) histogram using the kth non-zero alternating current (AC) coefficient of all the quantized discrete cosine transform blocks. On the other hand, to fully exploit the strong correlation between coefficients of one histogram, the smoothness of each coefficient is estimated by a block smoothness estimator so that a sharply-distributed 2D-histogram is constructed by combining two coefficients with similar smoothness into a pair. The pair corresponding to low complexity is selected priorly for data embedding, leading to high embedding performance while maintaining low file size. Besides, we design multiple embedding strategies to adaptively select the embedding strategy for each 2D histogram. Experimental results demonstrate that the proposed method can achieve higher rate–distortion performance which maintaining lower file storage space, compared with previous studies.}
}
@article{NR2022103500,
title = {Fragile watermarking scheme for tamper localization in images using logistic map and singular value decomposition},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103500},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103500},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000517},
author = {Neena Raj N.R. and Shreelekshmi R.},
keywords = {Image authentication, Fragile watermarking, Tamper localization, Logistic map, Singular value decomposition},
abstract = {This paper presents a fragile watermarking scheme for tamper localization using Singular Value Decomposition (SVD) and logistic map. The proposed scheme divides the image into blocks of size 2 × 2 pixels and generates an 8-bit watermark from each block. The watermark is computed by permuting the six Most Significant Bits (MSBs) of each pixel in the block using the logistic map, followed by SVD. To secure, the watermark thus generated is further encrypted using the logistic map. This encrypted watermark is embedded into 2 Least Significant Bits (LSBs) of each pixel to enable tamper detection and localization. The experimental results demonstrate that the proposed scheme can precisely locate tampered regions under copy-paste, content removal, text addition, noise addition, vector quantization, collage, content only, and constant feature attacks. Tamper localization accuracy is better or comparable to the state-of-the-art tamper localization algorithms.}
}
@article{ZHANG2022103555,
title = {A multi-stage spatio-temporal adaptive network for video super-resolution},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103555},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103555},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000918},
author = {Yuhang Zhang and Zhenzhong Chen and Shan Liu},
keywords = {Video super-resolution, ConvLSTM, Inter-frame correlation, Residual stacked bidirectional architecture},
abstract = {Video super-resolution aims at restoring the spatial resolution of the reference frame based on consecutive input low-resolution (LR) frames. Existing implicit alignment-based video super-resolution methods commonly utilize convolutional LSTM (ConvLSTM) to handle sequential input frames. However, vanilla ConvLSTM processes input features and hidden states independently in operations and has limited ability to handle the inter-frame temporal redundancy in low-resolution fields. In this paper, we propose a multi-stage spatio-temporal adaptive network (MS-STAN). A spatio-temporal adaptive ConvLSTM (STAC) module is proposed to handle input features in low-resolution fields. The proposed STAC module utilizes the correlation between input features and hidden states in the ConvLSTM unit and modulates the hidden states adaptively conditioned on fused spatio-temporal features. A residual stacked bidirectional (RSB) architecture is further proposed to fully exploit the processing ability of the STAC unit. The proposed STAC and RSB architecture promote the vanilla ConvLSTM’s ability to exploit the inter-frame correlations, thus improving the reconstruction quality. Furthermore, different from existing methods that only aggregate features from the temporal branch once at a specified stage of the network, the proposed network is organized in a multi-stage manner. The corresponding temporal correlation in features at different stages can be fully exploited. Experimental results on Vimeo-90K-T and UMD10 datasets show that the proposed method has comparable performance with current video super-resolution methods. The code is available at https://github.com/yhjoker/MS-STAN.}
}
@article{WANG2022103590,
title = {Progressive enhancement network with pseudo labels for weakly supervised temporal action localization},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103590},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103590},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001171},
author = {Qingyun Wang and Yan Song and Rong Zou and Xiangbo Shu},
keywords = {Temporal action localization, Weak supervision, Pseudo label, Video understanding},
abstract = {Weakly supervised temporal action localization (WSTAL) is crucial for real world applications, as it relieves the huge burden of frame-level annotations for fully supervised action detection. Most existing WSTAL methods focused on classifying video snippets, or detecting action boundaries. However, the predictions from these well-designed models have not been fully utilized. Accordingly, we propose a weakly-supervised framework called the progressive enhancement network (PEN), which takes full advantages of the predictions generated by the preceding models to enhance the subsequent models. Specifically, snippet-level pseudo labels are generated from the preceding predictions by considering the similarity and temporal distance between action snippets. Then subsequent models are progressively enhanced by using pseudo labels as a supervision, and utilizing their underlying semantics to make the feature representation more qualified for the temporal localization task. Extensive experiments which are carried out on two popular benchmarks, THUMOS’14 and ActivityNet v1.2, demonstrate the effectiveness of our method.}
}
@article{B2022103578,
title = {Deep chroma prediction of Wyner–Ziv frames in distributed video coding of wireless capsule endoscopy video},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103578},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103578},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001055},
author = {Sushma B. and Aparna P.},
keywords = {Distributed video coding, Chroma prediction, Convolutional neural networks, Video compression, Wireless capsule endoscopy},
abstract = {Compression of captured video frames is crucial for saving the power in wireless capsule endoscopy (WCE). A low complexity encoder is desired to limit the power consumption required for compressing the WCE video. Distributed video coding (DVC) technique is best suitable for designing a low complexity encoder. In this technique, frames captured in RGB colour space are converted into YCbCr colour space. Both Y and CbCr representing luma and chroma components of the Wyner–Ziv (WZ) frames are processed and encoded in existing DVC techniques proposed for WCE video compression. In the WCE video, consecutive frames exhibit more similarity in texture and colour properties. The proposed work uses these properties to present a method for processing and encoding only the luma component of a WZ frame. The chroma components of the WZ frame are predicted by an encoder–decoder based deep chroma prediction model at the decoder by matching luma and texture information of the keyframe and WZ frame. The proposed method reduces the computations required for encoding and transmitting of WZ chroma component. The results show that the proposed DVC with a deep chroma prediction model performs better when compared to motion JPEG and existing DVC systems for WCE at the reduced encoder complexity.}
}
@article{MALLIKA2022103483,
title = {Neural Style Transfer for image within images and conditional GANs for destylization},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103483},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103483},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000360},
author = { Mallika and Jagpal Singh Ubhi and Ashwani Kumar Aggarwal},
keywords = {Image security, Convolutional neural network, Information hiding, StegoExpose, Steganography},
abstract = {In this paper, the feature representation of an image by CNN is used to hide the secret image into the cover image. The style of the cover image hides the content of the secret image and produce a stego image using Neural Style Transfer (NST) algorithm, which resembles the cover image and also contains the semantic content of secret image. The main technical contributions are to hide the content of the secret image in the in-between hidden layered style features of the cover image, which is the first of its kind in the present state-of-art-technique. Also, to recover the secret image from the stego image, destylization is done with the help of conditional generative adversarial networks (GANs) using Residual in Residual Dense Blocks (RRDBs). Further, stego images from different layer combinations of content and style features are obtained and evaluated. Evaluation is based on the visual similarity and quality loss between the cover-stego pair and the secret-reconstructed secret pair of images. From the experiments, it has been observed that the proposed algorithm has 43.95 dB Peak Signal-to-Noise Ratio (PSNR)), .995 Structural Similarity Index (SSIM), and .993 Visual Information Fidelity (VIF) for the ImageNet dataset. The proposed algorithm is found to be more robust against StegExpose than the traditional methods.}
}
@article{TARIQ2022103594,
title = {HEVC’s intra mode process expedited using Histogram of Oriented Gradients},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103594},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103594},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001213},
author = {Junaid Tariq and Amir Ijaz and Ammar Armghan and Hameedur Rahman and Hashim Ali and Fayadh Alenezi},
keywords = {Histogram of Oriented Gradients, HEVC, Intra mode, Fast Encoding},
abstract = {The brute-force behavior of High Efficiency Video Coding (HEVC) is the biggest hurdle in the communication of multimedia content. Therefore, two novel methods will be presented here to expedite the intra mode decision process of HEVC. In the first algorithm, the feasibility of Histogram of Oriented Gradients (HOG) for early intra mode decision is presented by using statistical evidence. Then, HOG of the current block and 35 intra predictions are obtained. The intra-prediction that gives the least sum of absolute difference (SAD) with the HOG of the current block is selected as the termination point. In the second algorithm, the difference between the Hardmard-cost of intra modes is modeled to achieve fast intra mode decision. The proposed algorithms accelerated the encoding process of the HEVC by 5% and 35.57%, while their Bjontegaard Delta Bit Rate (BD-BR) is 1.09% and 1.61%, respectively.}
}
@article{FANG2022103464,
title = {Facial makeup transfer with GAN for different aging faces},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103464},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103464},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000244},
author = {Sen Fang and Mingxing Duan and Kenli Li and Keqin Li},
keywords = {Domain, Facial aging, GAN, Makeup},
abstract = {Facial aging is widely used in criminal tracking and the search for lost children. If the aging face is made up, it will greatly affect the discrimination of the tracking system. Therefore, the research on the makeup of different aging faces is extremely important. Existing studies have achieved a good transition from the non-makeup domain to the makeup domain in facial makeup transfer. But few studies involve the transfer of facial makeup at different ages. In addition, existing datasets rarely contain both age and makeup attributes, which make the transfer of facial makeup for different ages full of challenges. To solve the above problems, we propose a learning framework, called AM-Net, which can realize facial makeup transfer for different ages while protecting identity information. AM-Net is composed of two sub-network modules: Aging-Net and Makeup-Net. AM-Net first learns the aging mechanism of faces through Aging-Net, and then, it feeds the learned aging mode to Makeup-Net. After that, AM-Net trains Makeup-Net to realize the mapping relationship between the non-makeup domain to the makeup domain and transfer the makeup style to the face of the non-makeup. Throughout the network, multiple losses are applied to ensure AM-Net preserve information about the identity, background, etc. Extensive experiments are conducted on different datasets with different state-of-the-art methods, which prove the effectiveness of AM-Net.}
}
@article{LIU2022103586,
title = {Two-stream interactive network based on local and global information for No-Reference Stereoscopic Image Quality Assessment},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103586},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103586},
url = {https://www.sciencedirect.com/science/article/pii/S104732032200116X},
author = {Yun Liu and Baoqing Huang and Guanghui Yue and Jingkai Wu and Xiaoxu Wang and Zhi Zheng},
keywords = {Stereoscopic image quality evaluation, Binocular fusion, Asymmetric convolution kernel, CNN, Summation and difference channels},
abstract = {Nowadays, stereoscopic image quality assessment (SIQA) based on convolutional neural network (CNN) has become the mainstream model of image quality assessment (IQA). Compared with the two-dimensional quality evaluation model, stereoscopic image quality evaluation is more challenging due to the effects of depth and parallax information. In this paper, we propose a two-stream interactive network model to perform quality evaluation, which can well simulate the process of human stereo visual perception. Meanwhile, we enhance the extraction of local and global features of images by asymmetric convolution kernel and interactive sub-networks of inter-layers, respectively, which can further optimize our network model. Our proposed algorithm was evaluated on four public databases. The final experimental results show that our proposed algorithm exhibits good performance not only on the whole database but also on each single distortion type.}
}
@article{ZHENG2022103486,
title = {Colorful 3D reconstruction at high resolution using multi-view representation},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103486},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103486},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000402},
author = {Yanping Zheng and Guang Zeng and Haisheng Li and Qiang Cai and Junping Du},
keywords = {3D reconstruction, Colorful volumes, Super resolution, Multi-view representation},
abstract = {High-quality 3D models should contain accurate shapes, as well as other correct attributes, such as realistic surface color. However, current researches were mostly focused on the reconstruction of shapes. We present a method to reconstruct high-resolution colorful 3D models from single images. Shapes and colors are learned separately, using a coarse-to-fine strategy in which the 3D color is expressed as 3-channel volumes. Colorful volumes share the same spatial dimension with generated shape volumes. We propose orthographic colorful maps to retain and recover projected coordinates and corresponding color for 3D surface points. To achieve a fine granularity increase in the quality of maps from low-resolution to high-resolution, we introduce 2D super resolution during reconstructing 3D shapes and color volumes. Models are carved by utilizing predicted high-resolution silhouette, depth and color details. Experimental results in a subset of the ShapeNet dataset and the Colorful Human dataset show the effectiveness of our method.}
}
@article{2023103999,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103999},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(23)00249-3},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002493}
}
@article{CHEN2022103552,
title = {A penalty function semi-continuous thresholding methods for constraints of hashing problems},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103552},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103552},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000888},
author = {Qian Chen and Zhengwei Shen and Zhe Chen},
keywords = {Hash coding, Large-scale data retrieval, Orthogonality constraint, Quantization error reduction},
abstract = {The Hashing process is an effective tool for handling large-scale data (for example, images, videos, or multi-model data) retrieval problems. To get better retrieval accuracy, hashing models usually are imposed with three rigorous constraints, i.e., discrete binary constraint, uncorrelated condition, and the balanced constraint, which will lead to being ‘NP-hard’. In this study, we divide the whole constraints set into the uncorrelated (orthogonality) constraint and the binary discrete balance constraint and propose a fast and accurate penalty function semi-continuous thresholding (PFSCT) hash coding algorithm based on forward–backward algorithms. In addition, we theoretically analyze the equivalence between the relaxed model and the original problems. Extensive numerical experiments on diverse large-scale benchmark datasets demonstrate comparable performance and effectiveness of the proposed method.}
}
@article{XING2022103520,
title = {Information hiding in the sharing domain},
journal = {Journal of Visual Communication and Image Representation},
volume = {86},
pages = {103520},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103520},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000645},
author = {Fengyue Xing and Xuehu Yan and Long Yu and Yuyuan Sun},
keywords = {Secret image sharing, Random element utilization model, Information hiding},
abstract = {Secret image sharing (SIS) can divide a secret image into several shadow images for protection. Information hiding in the sharing domain (IHSD) fuses SIS and information hiding (IH) to simultaneously share any secret image and hide any information, and this technique can be applied in cloud computing, law enforcement and medical diagnoses. IHSD not only marks shadow images with information to prevent malicious tampering and for convenient management, search and identification but also enhances the robustness of IH. In this paper, we first introduce a formal definition of IHSD. Then, we describe a general IHSD model and algorithms with a concrete example in detail. In IHSD, we design the random element utilization model to control the random pixels generated from SIS. Then, we obtain shadow images with hidden information to realize SIS and IH simultaneously. The inputs of SIS with secret images, steganography and extra information in algorithms are without any limitations. Theoretical analyses, experiments and comparisons are presented to prove the effectiveness and feasibility of IHSD.}
}
@article{HE2022103449,
title = {TGP-PCQA: Texture and geometry projection based quality assessment for colored point clouds},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103449},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103449},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000128},
author = {Zhouyan He and Gangyi Jiang and Mei Yu and Zhidi Jiang and Zongju Peng and Fen Chen},
keywords = {Colored point cloud, Visual quality assessment, Texture and geometry projection, Objective quality assessment},
abstract = {Colored point cloud (PC) will inevitably encounter distortion during its acquisition, processing, coding and transmission, which may affect the visual quality of the colored PC. Therefore, it is necessary to design an effective tool for colored PC quality assessment (PCQA). In this paper, considering the mapping relationship of perception between the colored PC and its corresponding projection images, we propose a novel PCQA method based on texture and geometry projection (denoted as TGP-PCQA). The main idea of the proposed TGP-PCQA method is to obtain texture and geometry projection maps from different perspectives for evaluating the colored PC. Specifically, 4D tensor decomposition is used to obtain the combination and difference information between the reference and distorted texture projection maps for mainly characterizing texture distortion of colored PC. Meanwhile, the edge features of the geometry projection map are calculated to measure the global or local geometry distortion. All of the extracted features are combined to predict an overall quality of colored PC. In addition, this paper establishes a multi-distorted colored PC database named CPCD2.0 with compression distortions and Gaussian noise, which orients to the influence of both geometry and texture components in distortion. Experimental results on two open subjective evaluation databases (IRPC and SJTU-PCQA) and the self-built CPCD2.0 database show that the proposed TGP-PCQA method outperforms the state-of-the-art PCQA methods. We are also providing the self-built CPCD2.0 database free of charge at https://github.com/cherry0415/CPCD2.0.}
}
@article{SOLTANIMOHAMMADI2022103514,
title = {An efficient six-parameter perspective motion model for VVC},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103514},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103514},
url = {https://www.sciencedirect.com/science/article/pii/S104732032200061X},
author = {Iman {Soltani Mohammadi} and Mohammad Ghanbari and Mahmoud Reza Hashemi},
keywords = {Multimedia streaming, Video coding, Motion estimation, Spatial transform, BD-rate reduction},
abstract = {Tilt and pan camera movements are common in computer games or social media videos. These types of videos contain numerous perspective transforms while today’s video codecs rely on translational and affine motion models for motion compensation. The general perspective motion model with 8 parameters (8PMM) has unreasonably high processing time. In this paper, the eight-parameter perspective transform is simplified into a six-parameter transform to keep the time complexity within an acceptable range while modeling the most relevant transforms. Also, two motion prediction modes, Advanced Perspective Motion Vector Prediction (APMVP) and Perspective Model Merge (PMM), are proposed. The implementation results show an average of 7.0% BD-rate reduction over H.266/VVC Test Model with a maximum of 20% encoding time overhead. The results also show a 71% processing time reduction in comparison to 8PMM while experiencing an average of 5.6% increase in BD-Rate. Much better visual quality is measured through VMAF quality meter.}
}
@article{VERMA2022103554,
title = {A two stream convolutional neural network with bi-directional GRU model to classify dynamic hand gesture},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103554},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103554},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000906},
author = {Bindu Verma},
keywords = {Dynamic hand gesture, Gated recurrent unit, Convolutional neural network, Recurrent neural network},
abstract = {Dynamic hand gesture recognition is still an interesting topic for the computer vision community. A set of feature vectors can represent any hand gesture. A Recurrent Neural Network (RNN) can recognize these feature vectors as a hand gesture that analyzes the temporal and contextual information of the gesture sequence. Thus, we proposed a hybrid deep learning framework to recognize dynamic hand gestures. In the Hybrid model GoogleNet is pipelined with a Bidirectional GRU unit to recognize the dynamic hand gesture. Dynamic hand gestures consist of many frames, and features of each frame need to be extracted to get the temporal and dynamic information of the performed gesture. As RNN takes input as a sequence of feature vectors, we extract features from videos using pretrained GoogleNet. As Gated Recurrent Unit is one of the variants of RNN to classify the sequential data, we created a feature vector that corresponds to each video and passed it to the bidirectional GRU (BGRU) network to classify the gestures. We evaluate our model on four publicly available hand gesture datasets. The proposed method performs well and is comparable with the existing methods. For instance, we achieved 98.6% accuracy on Northwestern University Hand Gesture(NWUHG), 99.6% on SKIG, 99.4% on Cambridge Hand Gesture (CHG) datasets respectively. We performed our experiments on DHG14/28 dataset and achieved an accuracy of 97.8% with 14-gesture classes and 92.1% on 28-gesture classes. DHG14/28 dataset contains skeleton and depth data, and our proposed model used depth data and achieved comparable accuracy.}
}
@article{LEE2022103459,
title = {Single-image depth estimation using relative depths},
journal = {Journal of Visual Communication and Image Representation},
volume = {84},
pages = {103459},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103459},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000190},
author = {Jae-Han Lee and Chang-Su Kim},
keywords = {Monocular depth estimation, Relative depth, 3D analysis},
abstract = {Depth estimation from a single RGB image is a challenging task. It is ill-posed since a single 2D image may correspond to various 3D scenes at different scales. On the other hand, estimating the relative depth relationship between two objects in a scene is easier and may yield more reliable results. Thus, in this paper, we propose a novel algorithm for monocular depth estimation using relative depths. First, using a convolutional neural network, we estimate two types of depths at multiple spatial resolutions: ordinary depth maps and relative depth tensors. Second, we restore a relative depth map from each relative depth tensor. A relative depth map is equivalent to an ordinary depth map with global scale information removed. For the restoration, sparse pairwise comparison matrices are constructed from available relative depths, and missing entries are filled in using the alternative least square (ALS) algorithm. Third, we decompose the ordinary and relative depth maps into components and recombine them to yield a final depth map. To reduce the computational complexity, relative depths at fine spatial resolutions are directly used to refine the final depth map. Extensive experimental results on the NYUv2 dataset demonstrate that the proposed algorithm provides state-of-the-art performance.}
}
@article{WANG2022103512,
title = {Dual-path image pair joint discrimination for visible–infrared person re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103512},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103512},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000591},
author = {Zhongjie Wang and Li Liu and Huaxiang Zhang},
keywords = {Person re-identification, visible–infrared, Image pair generation, Dual-path discrimination},
abstract = {Because the imaging spectra of infrared images and visible light images are different, there is a huge modal difference between visible light images and infrared ones. Existing methods use image conversion to solve the problem of modal difference between two images, but these methods usually fail to focus on the complete information of images, which lead to the results of cross modal person re-identification are unstable. To solve this problem, we propose a new visible–infrared person re-identification method, called dual-path image pair joint discriminant model (DPJD), which simultaneously optimizes the distance within and between classes, and supervises the network learning to identify feature representations. We generate images with different modalities for the samples, and separately compose the same modality image pair and different modality image pair so as to overcome the inconsistent alignment issues. In addition, we also propose a discriminant module based on dual-path (DMDP) to improve the generation quality and discrimination accuracy of image pairs. Experiments on two benchmark datasets SYSU-MM01 and RegDB demonstrate its effectiveness.}
}
@article{PENG2022103570,
title = {Progressive Erasing Network with consistency loss for fine-grained visual classification},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103570},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103570},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001043},
author = {Jin Peng and Yongxiong Wang and Zeping Zhou},
keywords = {FGVC, PEN, Multi-grid erasure mechanism, Cross-layer incentive block, Consistency loss},
abstract = {Fine-grained Visual Categorization (FGVC) in computer vision aims to recognize images belonging to multiple subordinate categories of a super-category. The difficulty of FGVC lies in the close resemblance among inter-classes and large variations among intra-classes. Most existing networks only focus on a few discriminative regions, while ignoring many subtle complementary features. So we propose a Progressive Erasing Network (PEN). In PEN, a Multi-Grid Erasure mechanism augments data samples and assists in capturing the local discriminative features, where the overall structure of the image is destroyed indirectly through pixel-wise erasure. Cross-layer feature aggregation by extracting salient class features is of great significance in FGVC. However, the capability of cross-layer feature representation based on a simple aggregation strategy is still inefficient. To this end, the proposed Consistency loss explores the cross-layer semantic affinity, which guides the Cross-Layer Incentive (CLI) block to mine more efficient feature representations of different granularity. We also integrate Cross Entropy and Complementary Entropy to take the distribution of negative classes into account for better classification performance. Our method uses end-to-end training with only classification labels. Experimental results show that our model outperforms the state-of-the-art on three fine-grained benchmarks.}
}
@article{SORENG2022103466,
title = {Verifiable varying sized (m,n,n) multi-image secret sharing with combiner verification and cheater identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {84},
pages = {103466},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103466},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000232},
author = {Aswini Vinay Soreng and Shyamalendu Kandar},
keywords = {Multi-image secret sharing, Varying sized, Verifiable multi image secret sharing, Dealer authentication, Cheater identification, Combiner verification},
abstract = {In conventional multi-image secret sharing schemes (MISSS) images are shared by the trusted dealer and the shares are sent to the set of participants through a secure channel. During reconstruction, the participants submit shares to a trusted combiner. But the method will collapse if any of the actors perform cheating. This brings verifiable image secret sharing in the research arena. Verifying the trustworthiness of the dealer by the shareholders before accepting shares (dealer verification), examining the genuineness of the shares submission request received from a combiner (combiner verification), checking the authenticity of the shares received from participants by the combiner (cheating detection/ cheater identification) are techniques related to verifiable secret sharing. Medical images, images used at the military or diplomatic level; contain sensible information. Thus the authenticity of the reconstructed images should be checked beforehand. In the case of multi-image secret sharing, the researchers use bit padding if the plaintext images are of different sizes. This adds an extra level of burden during sharing and retrieval. A verifiable varying size (m,n,n) multi-image secret sharing is addressed in this article. Here m (wherem≤n) varying sized images are shared among n participants and during reconstruction all the shares are required. The major contribution of the addressed technique is that it has the capability of dealer authentication, combiner verification, and cheater identification. Another advancement is that most of the communication can be made through a public channel. The test results generate noise like images and statistical analysis, security analysis say in favor of the claims. Comparison with some state-of-the-art techniques gives it a stable platform in verifiable multi-image secret sharing.}
}
@article{ROSSI2022103595,
title = {Self-Balanced R-CNN for instance segmentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103595},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103595},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001201},
author = {Leonardo Rossi and Akbar Karimi and Andrea Prati},
keywords = {Object detection, Instance segmentation, Imbalance in R-CNN networks, Two-stage deep learning architectures},
abstract = {Current state-of-the-art two-stage models on instance segmentation task suffer from several types of imbalances. In this paper, we address the Intersection over the Union (IoU) distribution imbalance of positive input Regions of Interest (RoIs) during the training of the second stage. Our Self-Balanced R-CNN (SBR-CNN), an evolved version of the Hybrid Task Cascade (HTC) model, brings brand new loop mechanisms of bounding box and mask refinements. With an improved Generic RoI Extraction (GRoIE), we also address the feature-level imbalance at the Feature Pyramid Network (FPN) level, originated by a non-uniform integration between low- and high-level features from the backbone layers. In addition, the redesign of the architecture heads toward a fully convolutional approach with FCC further reduces the number of parameters and obtains more clues to the connection between the task to solve and the layers used. Moreover, our SBR-CNN model shows the same or even better improvements if adopted in conjunction with other state-of-the-art models. In fact, with a lightweight ResNet-50 as backbone, evaluated on COCO minival 2017 dataset, our model reaches 45.3% and 41.5% AP for object detection and instance segmentation, with 12 epochs and without extra tricks. The code is available at https://github.com/IMPLabUniPr/mmdetection/tree/sbr_cnn.}
}
@article{LIU2022103517,
title = {LRHW-AP: Using ranking-based metric as loss for Person Re-Identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103517},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103517},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000633},
author = {Yifei Liu and Yaling Liang and Ziheng Chen},
keywords = {Person Re-Identification, Image Retrieval, Average Precision, Pooling},
abstract = {Optimizing a ranking-based metric as the loss function, such as Average Precision (AP), has been found very effective in image retrieval tasks, but it has received less attention in Person Re-Identification (Re-ID). In this paper, Low Rank High Weight (LRHW) AP is proposed to apply the AP-optimizing method on the Re-ID task. LRHW-AP employs high weight on the low rank positive instances, which provides more information for model optimization than high rank positive instances and distribute in high gradient area. We propose a new pooling method called Power Activation Weighted Mean (PAWM) pooling which can unify a set of pooling methods because of a changeable activation function and a trainable parameter. Thus one can adjust and train PAWM to adapt to the target task to improve the model performance. Besides, we incorporate Warmup and Exponentially Decay Scheduler with a delay period, called Warmup Delay Exponentially Decay Scheduler, which brings further improvement. Through an extensive set of ablation studies, we verify that all methods mentioned above contribute to the performance boosts on Re-ID and the model achieves 95.3% rank-1 and 88.4% mAP on Market1501 with ResNet50.}
}
@article{HASSANIN2022103448,
title = {Learning discriminative representations for multi-label image recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103448},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103448},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000116},
author = {Mohammed Hassanin and Ibrahim Radwan and Salman Khan and Murat Tahtali},
keywords = {Multi-label recognition, Multi-label-contrastive learning, Contrastive representation, Deep learning},
abstract = {Multi-label recognition is a fundamental, and yet is a challenging task in computer vision. Recently, deep learning models have achieved great progress towards learning discriminative features from input images. However, conventional approaches are unable to model the inter-class discrepancies among features in multi-label images, since they are designed to work for image-level feature discrimination. In this paper, we propose a unified deep network to learn discriminative features for the multi-label task. Given a multi-label image, the proposed method first disentangles features corresponding to different classes. Then, it discriminates between these classes via increasing the inter-class distance while decreasing the intra-class differences in the output space. By regularizing the whole network with the proposed loss, the performance of applying the well-known ResNet-101 is improved significantly. Extensive experiments have been performed on COCO-2014, VOC2007 and VOC2012 datasets, which demonstrate that the proposed method outperforms state-of-the-art approaches by a significant margin of 3.5% on large-scale COCO dataset. Moreover, analysis of the discriminative feature learning approach shows that it can be plugged into various types of multi-label methods as a general module.}
}
@article{GAO2022103511,
title = {Visible–infrared person re-identification based on key-point feature extraction and optimization},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103511},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103511},
url = {https://www.sciencedirect.com/science/article/pii/S104732032200058X},
author = {Wenbo Gao and Li Liu and Lei Zhu and Huaxiang Zhang},
keywords = {Visible–infrared person re-identification, Feature extraction, Multi-hop, Self-attention},
abstract = {Feature extraction for visible–infrared person re-identification (VI-ReID) is challenging because of the cross-modality discrepancy in the images taken by different spectral cameras. Most of the existing VI-ReID methods often ignore the potential relationship between features. In this paper, we intend to transform low-order person features into high-order graph features, and make full use of the hidden information between person features. Therefore, we propose a multi-hop attention graph convolution network (MAGC) to extract robust person joint feature information using residual attention mechanism while reducing the impact of environmental noise. The transfer of higher order graph features within MAGC enables the network to learn the hidden relationship between features. We also introduce the self-attention semantic perception layer (SSPL) which can adaptively select more discriminant features to further promote the transmission of useful information. The experiments on VI-ReID datasets demonstrate its effectiveness.}
}
@article{HE2022103549,
title = {HEVC video information hiding scheme based on adaptive double-layer embedding strategy},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103549},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103549},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000876},
author = {Songhan He and Dawen Xu and Lin Yang and Yong Liu},
keywords = {Adaptive video information hiding, HEVC, PU partition mode, STC, Double-layer embedding},
abstract = {High Efficiency Video Coding (HEVC) is well-known as an internationally popular video coding standard, and HEVC-based steganography has received increasing attention. In this paper, a new adaptive HEVC video information hiding method based on Prediction Unit (PU) partition mode and double-layer embedding strategy is proposed. Double-layer embedding is a method to complete the first-layer embedding using the mapping rules of PU partition mode, and to perform the second-layer embedding after the first-layer embedding. The cost assignment function designed in this paper can accurately evaluate the second-layer data embedding distortion. The frame position, motion properties and block size of PU are taken into consideration for the second-layer data embedding, and the syndrome-trellis codes (STCs) are used to minimize the embedding distortion. Experimental results show that the proposed adaptive double-layer embedding algorithm has better embedding efficiency and less embedding distortion in most cases.}
}
@article{YANG2022103482,
title = {AMBTC-based secret image sharing by simple modular arithmetic},
journal = {Journal of Visual Communication and Image Representation},
volume = {84},
pages = {103482},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103482},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000359},
author = {Ching-Nung Yang and Xiaotian Wu and Min-Jung Chung and Xuliang Zhang},
keywords = {Secret image sharing, Secret sharing, Simple modular arithmetic, AMBTC, Distortion-free},
abstract = {A distortionless secret image sharing scheme using finite field GF(PG) (PG is the largest prime less than a given number) is investigated for sharing the absolute moment block truncation coding (AMBTC) images. Two adjusting operations are devised to modify the AMBTC trios (i.e., quantization pixels and bit maps) suitable for GF(PG) sharing. Polynomials under GF(PG) are constructed for encrypting the quantization pixels and bit maps. When sharing the trios under GF(PG), the AMBTC image is perfectly recovered. Moreover, the bit map sharing under GF(PG) can be extended to GF(PS) (PS is the smallest prime larger than a given number). Another scheme using GF(PS) is constituted by combining quantization pixel sharing under GF(PG) and bit map sharing under GF(PS). But the scheme using GF(PS) is not always lossless. Experimental results show that the proposed schemes are effective. Since GF(PG) and GF(PS) are simple modular arithmetic, improved computational efficiency is obtained.}
}
@article{PAN2022103461,
title = {Optimized convolutional pose machine for 2D hand pose estimation},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103461},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103461},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000219},
author = {Tianhong Pan and Zheng Wang and Yuan Fan},
keywords = {Convolutional block attention module (CBAM), Convolutional pose machine (CPM), 2D hand pose estimation, Resnet-18, Feature fusion},
abstract = {Hand pose estimation is a challenging task owing to the high flexibility and serious self-occlusion of the hand. Therefore, an optimized convolutional pose machine (OCPM) was proposed in this study to estimate the hand pose accurately. Traditional CPMs have two components, a feature extraction module and an information processing module. First, the backbone network of the feature extraction module was replaced by Resnet-18 to reduce the number of network parameters. Furthermore, an attention module called the convolutional block attention module (CBAM) is embedded into the feature extraction module to enhance the information extraction. Then, the structure of the information processing module was adjusted through a residual connection in each stage that consist of a series of continuous convolutional operations, and requires a dense fusion between the output from all previous stages and the feature extraction module. The experimental results on two public datasets showed that the OCPM network achieved excellent performance.}
}
@article{WANG2022103569,
title = {A zero-watermark algorithm for multiple images based on visual cryptography and image fusion},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103569},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103569},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001031},
author = {Baowei Wang and Weishen Wang and Peng Zhao},
keywords = {Zero-watermark, Multiple images, Copyright protection, Non-extended visual cryptography, Gray-weighted average image fusion},
abstract = {At present, it is difficult for the multiple images zero-watermark algorithm to protect all the images in the image set, and repeated operations will reduce the efficiency of the algorithm. To solve these issues, the proposed algorithm can design a reasonable copyright protection scheme according to the number of images in the image set to realize the protection of all images, and reduce the cost of time and storage. The gray-weighted average image fusion method is used to fuse multiple normalized standard images into one image. The LWT(Lifting the Wavelet Transform)-QR decomposition is applied to the effective area of the fusion image to obtain the robust feature image. Non-extended visual cryptography is used to enhance the security of the algorithm. A zero-watermark image is obtained by using the XOR manipulation for the feature image and the public shared image. Experimental results demonstrate that the proposed algorithm has good performance.}
}
@article{SHARMA2022103467,
title = {A novel unsupervised multiple feature hashing for image retrieval and indexing (MFHIRI)},
journal = {Journal of Visual Communication and Image Representation},
volume = {84},
pages = {103467},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103467},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000220},
author = {Saurabh Sharma and Vishal Gupta and Mamta Juneja},
keywords = {Computer vision, Image indexing, Multi-view hashing, Approximate nearest neighbor search, Feature fusion, Graph theory},
abstract = {Recently, techniques that can automatically figure out the incisive information from gigantic visual databases are urging popularity. The existing multi-feature hashing method has achieved good results by fusing multiple features, but in processing these multi-features, fusing multi-features into one feature will cause the feature dimension to be very high, increasing the amount of calculation. On the one hand, it is not easy to discover the internal ties between different features. This paper proposes a novel unsupervised multiple feature hashing for image retrieval and indexing (MFHIRI) method to learn multiple views in a composite manner. The proposed scheme learns the binary codes of various information sources in a composite manner, and our scheme relies on weighted multiple information sources and improved KNN concept. In particular, here we adopt an adaptive weighing scheme to preserve the similarity and consistency among binary codes. Precisely, we follow the graph modeling theory to construct improved KNN concept, which further helps preserve different statistical properties of individual sources. The important aspect of improved KNN scheme is that we can find the neighbors of a data point by searching its neighbors’ neighbors. During optimization, the sub-problems are solved in parallel which efficiently lowers down the computation cost. The proposed approach shows consistent performance over state-of-the-art (three single-view and eight multi-view approaches) on three broadly followed datasets viz. CIFAR-10, NUS-WIDE and Caltech-256.}
}
@article{HU2022103585,
title = {High dynamic range imaging with short- and long-exposures based on artificial remapping using multiscale exposure fusion},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103585},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103585},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001158},
author = {Junbao Hu and Lingfeng Wu and Na Li},
keywords = {High dynamic range imaging, Exposure fusion, Artificial remapping, Image processing},
abstract = {High dynamic range imaging (HDRI) is an excellent high-quality image acquisition technique, which can reflect real human visual characteristics from one (or several) captured low dynamic range (LDR) image. However, the input LDR image only provides partial information of the scene. Besides, in traditional HDRI methods that require multiple captured images as input, field of view errors can be induced, which will be difficult to apply it to the emerging image acquisition systems. Here, we propose a novel HDRI method that reconstructs an HDR image from only a pair of short- and long-exposure images based on artificial remapping using multi-scale exposure fusion. Firstly, we introduce a simulated exposure model called artificial remapping to synthesize a multi-exposure image sequence from the input LDR image pairs. Then, weighting maps of the sequence for fusion can be obtained according to the evaluation factors of contrast, saturation, as well as improved exposedness. Finally, we utilize the pyramid based multiscale exposure fusion framework to integrate them into an enhanced HDR image. Comparative experiments, fully implemented on some source images, have been demonstrated that better performance can be realized compared with some competing methods in qualitative and quantitative evaluation. Note that the operation of the proposed method is simple yet effective, which is easy to popularize. The method thus can be potentially applied to the emerging image acquisition systems where two images are captured simultaneously by two image sensors or by one image sensor with a pair of short- and long-exposure setting.}
}
@article{SUN2022103458,
title = {A dynamic constraint representation approach based on cross-domain dictionary learning for expression recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103458},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103458},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000189},
author = {Zhe Sun and Raymond Chiong and Zheng-ping Hu and Sandeep Dhakal},
keywords = {Facial expression recognition, Cross-domain dictionary learning, Dynamic constraint representation},
abstract = {Facial expression recognition (FER) is an active research area that has attracted much attention from both academics and practitioners of different fields. In this paper, we investigate an interesting and challenging issue in FER, where the training and testing samples are from a cross-domain dictionary. In this context, the data and feature distribution are inconsistent, and thus most of the existing recognition methods may not perform well. Given this, we propose an effective dynamic constraint representation approach based on cross-domain dictionary learning for expression recognition. The proposed approach aims to dynamically represent testing samples from source and target domains, thereby fully considering the feature elasticity in a cross-domain dictionary. We are therefore able to use the proposed approach to predict class information of unlabeled testing samples. Comprehensive experiments carried out using several public datasets confirm that the proposed approach is superior compared to some state-of-the-art methods.}
}
@article{MALLICK2022103548,
title = {Posture and sequence recognition for Bharatanatyam dance performances using machine learning approaches},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103548},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103548},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000864},
author = {Tanwi Mallick and Partha Pratim Das and Arun Kumar Majumdar},
keywords = {Posture recognition, Sequence recognition, Dance segmentation, Multi-modal dance modeling, Machine learning, Bharatanatyam Dance analysis},
abstract = {Understanding the underlying semantics of performing arts like dance is a challenging task. Analysis of dance is useful to preserve cultural heritage, make video recommendation systems, and build tutoring systems. To create such a dance analysis application, three aspects of dance analysis must be addressed: (1) segment the dance video to find representative action elements, (2) recognize the detected action elements, and (3) recognize sequences formed by combining action elements according to specific rules. This paper attempts to address the three fundamental problems of dance analysis raised above, with a focus on Indian Classical Dance, em Bharatanatyam. Since dance is driven by music, we use both musical and motion information to extract action elements. The action elements are then recognized using machine learning and deep learning techniques. Finally, the Hidden Markov Model (HMM) and Long Short-Term Memory (LSTM) are used to recognize the dance sequence.}
}
@article{MOHAMMADI2022103478,
title = {A general framework for reversible data hiding in encrypted images by reserving room before encryption},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103478},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103478},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000347},
author = {Ammar Mohammadi},
keywords = {Encrypted image, Local difference, Prediction-errors, Reversible data hiding},
abstract = {In this paper a general framework to adopt different predictors for reversible data hiding in the encrypted image is presented. Employing linear regression, we propose innovative predictors that contribute more significantly to accomplish more payload than conventional ones. Reserving room before encryption (RRBE) is designated in the proposed scheme making possible to attain high embedding capacity. In RRBE procedure, pre-processing is allowed before image encryption. In our scheme, pre-processing comprises of three main steps: computing prediction-errors, blocking and labeling of the errors. By blocking, we obviate the need for lossless compression to when a content owner is not enthusiastic. Lossless compression is employed in recent state of the art schemes to improve payload. We surpass the prior arts exploiting proper predictors, more efficient labeling procedure and blocking of the prediction-errors.}
}
@article{PENG2022103518,
title = {Presentation attack detection based on two-stream vision transformers with self-attention fusion},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103518},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103518},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000621},
author = {Fei Peng and Shao-hua Meng and Min Long},
keywords = {Presentation attack detection, Multi-scale retinex with color restoration, Vision transformer, Deep learning, Feature fusion},
abstract = {Aiming at the performance degradation of the existing presentation attack detection methods due to the illumination variation, a two-stream vision transformers framework (TSViT) based on transfer learning in two complementary spaces is proposed in this paper. The face images of RGB color space and multi-scale retinex with color restoration (MSRCR) space are fed to TSViT to learn the distinguishing features of presentation attack detection. To effectively fuse features from two sources (RGB color space images and MSRCR images), a feature fusion method based on self-attention is built, which can effectively capture the complementarity of two features. Experiments and analysis on Oulu-NPU, CASIA-MFSD, and Replay-Attack databases show that it outperforms most existing methods in intra-database testing and achieves good generalization performance in cross-database testing.}
}
@article{GAO2022103446,
title = {Robust object tracking via deformation samples generator},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103446},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103446},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000104},
author = {Xuesong Gao and Yuan Zhou and Shuwei Huo and Zizi Li and Keqiu Li},
keywords = {Object tracking, Convolutional neural network, Deformation samples generator, Alternating strategy},
abstract = {In object tracking applications, it is common for trackers to experience drift problems when the object of interest becomes deformed, which compromises the ability of the tracker to track the object. It is therefore desirable to develop a learning tracker classifier that is robust to deformations. The performance of existing trackers that employ deep classification networks degrades when the amount of training data is limited and does not cover all possible scenarios. While these limitations can be mitigated in part by using larger training datasets, these datasets may still not cover all situations and the positive samples are still monotonous. To overcome this problem, we propose a novel deformation samples generator that generates samples that would normally be difficult for the tracker to classify. In the proposed framework, both the classifier and deformation samples generator learn in a joint manner. Our experiments show that the proposed approach outperforms state-of-the-art methods in both quantitative and qualitative evaluations for the visual object tracking task.}
}
@article{FU2022103567,
title = {Distributed three-level QR codes based on visual cryptography scheme},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103567},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103567},
url = {https://www.sciencedirect.com/science/article/pii/S104732032200102X},
author = {Zhengxin Fu and Liguo Fang and Hangying Huang and Bin Yu},
keywords = {Three-level QR code, Visual cryptography scheme, Multiple formats},
abstract = {Owing to the large storage and fast machine recognition, QR codes have been widely utilized in many fields such as mobile payment, website navigation and user identity authentication. However, any QR code reader can access to the message contained in the QR code, the security becomes a major challenge to QR codes for privacy usage scenarios. Moreover, the management of QR codes for users are also inconvenient, since the human vision is hard to distinguish a QR code from the others. To solve the security and management problems, we propose the three-level QR codes for a group of participants. The first-level management information and the second-level public information are recognizable for the human vision and QR code reader device, respectively. The third-level privacy information is protected using visual cryptography scheme, and can be decoded using simple and non-cryptography computations. Furthermore, the shares can be stored or transferred in not only e-format but also print-format and photo-format, leading to the broad applicability. Experimental results and analysis demonstrate that the proposed scheme can encode three-level information into several distributed QR codes, and has more advantages compared with the previous schemes.}
}
@article{ZHANG2022103621,
title = {A GCN-based fast CU partition method of intra-mode VVC},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103621},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103621},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001419},
author = {Saiping Zhang and Shixuan Feng and Jingwu Chen and Chunjie Zhou and Fuzheng Yang},
keywords = {Versatile Video Coding, Intra partition mode prediction, Complexity reduction, Global convolutional network},
abstract = {In this paper, a global convolutional network (GCN)-based fast coding unit (CU) partition method of intra-mode VVC is proposed. By using the GCN module with large kernel size convolutions, the proposed method can capture global information in CUs, leading to an accurate partition mode prediction in the quad-tree plus multi-type tree (QTMT) structure. Ranked according to predicted probabilities, the partition modes with lower probabilities are discarded, which reduces the computational complexity of VVC. Additionally, tradeoffs between performance and complexity can be achieved with different strategies. Experimental results demonstrated that the proposed method can reduce encoding time by 51.06%∼61.15% while increasing Bjøntegaard delta bit-rate (BD-BR) by 0.84%∼1.52% when implemented in VTM 10.0, outperforming the state-of-the-art methods, and that the proposed method can be used to accelerate VVenC 1.0 at the preset slower, achieving higher performance and lower complexity compared with the original VVenC 1.0 at the presets slow and medium.}
}
@article{KHAIRE2022103531,
title = {Deep learning and RGB-D based human action, human–human and human–object interaction recognition: A survey},
journal = {Journal of Visual Communication and Image Representation},
volume = {86},
pages = {103531},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103531},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000724},
author = {Pushpajit Khaire and Praveen Kumar},
keywords = {Human action recognition, CNN, LSTM, Human–human interaction, Human–object interaction, Deep learning, RGB-D sensors, Multi-modality, Fusion, Skeleton, GCN},
abstract = {Human activity recognition is one of the most studied topics in the field of computer vision. In recent years, with the availability of RGB-D sensors and powerful deep learning techniques, research on human activity recognition has gained momentum. From simple human atomic actions, the research has advanced towards recognizing more complex human activities using RGB-D data. This paper presents a comprehensive survey of the advanced deep learning based recognition methods and categorizes them in human atomic action, human–human interaction, human–object interaction. The reviewed methods are further classified based on the individual modality used for recognition i.e. RGB based, depth based, skeleton based, and hybrid. We also review and categorize recent challenging RGB-D datasets for the same. In addition, the paper also briefly reviews RGB-D datasets and methods for online activity recognition. The paper concludes with a discussion on limitations, challenges, and recent trends for promising future directions.}
}
@article{YANG2022103465,
title = {ACSiam: Asymmetric convolution structures for visual tracking with Siamese network},
journal = {Journal of Visual Communication and Image Representation},
volume = {84},
pages = {103465},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103465},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000207},
author = {Zhen Yang and Chaohe Wen and Lingkun Luo and Hongping Gan and Tao Zhang},
keywords = {Visual tracking, Siamese network, Asymmetric convolution, FD, Occlusion target},
abstract = {Object trackers based on Siamese network usually transform the tracking task into a matching problem between the candidate samples and the target template. However, with the increasing depth and width of backbone networks, researches on Siamese trackers using backbone networks are not very advanced. Therefore, it is necessary for us to further investigate the characteristics of backbone network. As a fact, the ability of backbone network to extract features can directly determine the performance of object tracker. Given this, in this paper, we first propose an asymmetric convolutional network to improve the representational capability of backbone network. And then, the strip convolution is employed to enhance the operational capability of square kernel convolution in the backbone network. Besides, we also construct a novel module named Feature Dropblock (i.e., FD) to simulate the occlusion of hidden space, which goal is to improve the performance of backbone network in the target tracking under occlusion. To demonstrate the effectiveness of the proposed tracker, extensive ablation studies are conducted. Better results are obtained on the tracking benchmarks OTB100 and VOT2018, compared to other state-of-the-art trackers.}
}
@article{LIU2022103434,
title = {Attention mechanism enhancement algorithm based on cycle consistent generative adversarial networks for single image dehazing},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103434},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103434},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002935},
author = {Yan Liu and Hassan Al-Shehari and Hongying Zhang},
keywords = {Image dehazing, Attention mechanism, Deep learning, Generative adversarial networks},
abstract = {This paper proposes AMEA-GAN, an attention mechanism enhancement algorithm. It is cycle consistency-based generative adversarial networks for single image dehazing, which follows the mechanism of the human retina and to a great extent guarantees the color authenticity of enhanced images. To address the color distortion and fog artifacts in real-world images caused by most image dehazing methods, we refer to the human visual neurons and use the attention mechanism of similar Horizontal cell and Amazon cell in the retina to improve the structure of the generator adversarial networks. By introducing our proposed attention mechanism, the effect of haze removal becomes more natural without leaving any artifacts, especially in the dense fog area. We also use an improved symmetrical structure of FUNIE-GAN to improve the visual color perception or the color authenticity of the enhanced image and to produce a better visual effect. Experimental results show that our proposed model generates satisfactory results, that is, the output image of AMEA-GAN bears a strong sense of reality. Compared with state-of-the-art methods, AMEA-GAN not only dehazes images taken in daytime scenes but also can enhance images taken in nighttime scenes and even optical remote sensing imagery.}
}
@article{LI2022103513,
title = {XOR-based visual cryptography scheme with essential shadows},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103513},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103513},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000608},
author = {Peng Li and Liping Yin and Jianfeng Ma and Hongtao Wang},
keywords = {XOR operation, Essential shadows, Visual cryptography, Secret image sharing, Shadow image, Access structure partition},
abstract = {Visual cryptography scheme with essential shadows (EVCS) is of great significance since it provides different levels of the importance to shadows. In this paper, we propose a general construction method for (t, s, k, n)-VCS with essential shadows based on XOR operation ((t, s, k, n)-EXVCS), which originates from the partition of access structure. The secret image is encrypted into s essential shadows and n-s non-essential shadows. Any k shadows including at least t essentials can cooperate to decode the secret image and the decoding process is implemented by XOR operation on the involved shadows. Our scheme achieves perfectly reconstruction of secret image in the revealed image and the less size of shadows and revealed images. The experiments are conducted to testify the feasibility and practicability of the proposed scheme.}
}
@article{WANG2022103614,
title = {Spatial-frequency HEVC multiple description video coding with adaptive perceptual redundancy allocation},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103614},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103614},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001389},
author = {Feifeng Wang and Jing Chen and Huanqiang Zeng and Canhui Cai},
keywords = {Multiple description coding, High efficiency video coding (HEVC), Human visual system, Error resilience},
abstract = {Multiple description coding (MDC) approaches improve the error-resilient performance of video transmission by introducing redundancy. The existing multiple description video coding (MDVC) schemes are rarely designed for particular coding structure of HEVC, and the characteristics of the human visual system (HVS) are seldom considered. In this paper, a spatial-frequency multiple description video coding with adaptive perceptual redundancy allocation framework, named SF-PMDVC, is proposed for HEVC. For descriptions generation, after polyphase down-sampling in spatial domain, a transformation based on integer discrete cosine transform (DCT) is expended to adapt to the flexible coding unit partitioning process in HEVC, and the frequency coefficients are segmented and mapped to reduce the bitrate of each description. To further improve the performance of MDVC, an adaptive perceptual redundancy allocation strategy based on visual saliency is proposed, which improves the coding efficiency adapting to the visual perception. Experimental results show that the proposed scheme improves the error resiliency of HEVC by achieving superior objective and subjective reconstructed video quality as compared to the state-of-the-art MDVC methods for HEVC.}
}
@article{ZHOU2022103526,
title = {A brief survey on adaptive video streaming quality assessment},
journal = {Journal of Visual Communication and Image Representation},
volume = {86},
pages = {103526},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103526},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000694},
author = {Wei Zhou and Xiongkuo Min and Hong Li and Qiuping Jiang},
keywords = {Quality of experience, Video quality assessment, Adaptive streaming, Performance analysis, Deep convolutional neural network, Spatio-temporal characteristics},
abstract = {Quality of experience (QoE) assessment for adaptive video streaming plays a significant role in advanced network management systems. It is especially challenging in case of dynamic adaptive streaming schemes over HTTP (DASH) which has increasingly complex characteristics including additional playback issues. In this paper, we provide a brief overview of adaptive video streaming quality assessment. Upon our review of related works, we analyze and compare different variations of objective QoE assessment models with or without using machine learning techniques for adaptive video streaming. Through the performance analysis, we observe that hybrid models perform better than both quality-of-service (QoS) driven QoE approaches and signal fidelity measurement. Moreover, the machine learning-based model slightly outperforms the model without using machine learning for the same setting. In addition, we find that existing video streaming QoE assessment models still have limited performance, which makes it difficult to be applied in practical communication systems. Therefore, based on the success of deep learned feature representations for traditional video quality prediction, we also apply the off-the-shelf deep convolutional neural network (DCNN) to evaluate the perceptual quality of streaming videos, where the spatio-temporal properties of streaming videos are taken into consideration. Experiments demonstrate its superiority, which sheds light on the future development of specifically designed deep learning frameworks for adaptive video streaming quality assessment. We believe this survey can serve as a guideline for QoE assessment of adaptive video streaming.}
}
@article{HU2022103457,
title = {Shareability-Exclusivity Representation on Product Grassmann Manifolds for Multi-camera video clustering},
journal = {Journal of Visual Communication and Image Representation},
volume = {84},
pages = {103457},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103457},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000177},
author = {Yongli Hu and Cuicui Luo and Junbin Gao and Boyue Wang and Yanfeng Sun and Baocai Yin},
keywords = {Multi-camera video clustering, Grassmann manifolds, Product Grassmann manifolds},
abstract = {With the rapid popularity of multi-camera networks, one human action is usually captured by multiple cameras located at different angles simultaneously. Multi-camera videos contain the distinct perspectives of one action, therefore multiple views can overcome the impacts of illumination and occlusion. In this paper, we propose a novel multi-camera video clustering model, named Shareability-Exclusivity Representation on Product Grassmann Manifolds (PGM-SER), to address two key issues in traditional multi-view clustering methods (MVC): (1) Most MVC methods directly construct a shared similarity matrix by fusing multi-view data or their corresponding similarity matrices, which ignores the exclusive information in each view; (2) Most MVC methods are designed for multi-view vectorial data, which cannot handle the nonlinear manifold structure hidden in multi-camera videos. The proposed PGM-SER firstly adopts Product Grassmann Manifolds to represent multi-camera videos, then simultaneously learn their shared and exclusive information in global structures to achieve multi-camera video clustering. We provide an effective optimization algorithm to solve PGM-SER and present the corresponding convergence analysis. Finally, PGM-SER is tested on three multi-camera human action video datasets and obtain satisfied experimental results.}
}
@article{HE2022103622,
title = {SIM-MFR: Spatial interactions mechanisms based multi-feature representation for background modeling},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103622},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103622},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001420},
author = {Wei He and Jiexin Li and Qi Qi and Bing Tu and Xianfeng Ou and Longyuan Guo},
keywords = {Object detection, Dynamic backgrounds, K-means, Multi-feature, Complementary notion},
abstract = {Moving object detection is frequently used as a springboard for advanced computer vision analysis in complex scenes. Nevertheless, due to unstable changes in the background, most existing background model hardly maintain superior performance. To this concern, we propose a novel pixel-level background model that has three innovations. First, we introduce K-means to directly model the spatiotemporal dependencies between pixels. These dependencies are exploited to discover static core information in the high-frequency changing spatial domain, resulting in excellent property in dynamic backgrounds. Besides, the notion of complementarity is taken as a feature selection criterion. In multi-feature model, the ability to supervise each other between features is important in the ambiguity challenges, e.g., shadow. Finally, feature models recommend each other in the update mechanism, and the diffusion rate of effective information in each feature model can be maximized by finding the best candidate feature. By virtue of this mechanism, model can be updated efficiently when large background migration occurs, e.g., PTZ. Experimental results on some standard benchmarks show that SIM-MFR can achieve promising performance compared to some state-of-the-art approaches.}
}
@article{TAN2022103588,
title = {Unsupervised learning of multi-task deep variational model},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103588},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103588},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001146},
author = {Lu Tan and Ling Li and Wan-Quan Liu and Sen-Jian An and Kylie Munyard},
keywords = {Unsupervised learning, Integration approach, Deep neural networks, Variational general frameworks, Diverse applications},
abstract = {We propose a general deep variational model (reduced version, full version as well as the extension) via a comprehensive fusion approach in this paper. It is able to realize various image tasks in a completely unsupervised way without learning from samples. Technically, it can properly incorporate the CNN based deep image prior (DIP) architecture into the classic variational image processing models. The minimization problem solving strategy is transformed from iteratively minimizing the sub-problem for each variable to automatically minimizing the loss function by learning the generator network parameters. The proposed deep variational (DV) model contributes to the high order image edition and applications such as image restoration, inpainting, decomposition and texture segmentation. Experiments conducted have demonstrated significant advantages of the proposed deep variational model in comparison with several powerful techniques including variational methods and deep learning approaches.}
}
@article{QIAO2022103506,
title = {Classifying between computer generated and natural images: An empirical study from RAW to JPEG format},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103506},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103506},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000578},
author = {Tong Qiao and Xiangyang Luo and Hongwei Yao and Ran Shi},
keywords = {Image origin forensics, Imaging procedure, Distribution model, Likelihood ratio test},
abstract = {Computer generated (CG) images have been gradually overspread on the Internet, resulting in difficult discrimination from natural images (NIs) captured by an authentic imaging device. Although some discriminators can deal with NIs in JPEG format, the classification between uncompressed NIs (that are possibly generated in any imaging procedure before compression) and CG ones still remains unknown. Thus, this paper aims to establish multiple discriminators classifying between NIs and CG images. We first describe the main imaging procedure and its intrinsic property, which characterizes the discriminative features for classification. Then, the residual noise (representing intrinsic characteristic) is extracted. Its statistical distribution indeed helps us establish multiple discriminators, consisting of the generalized likelihood ratio test (GLRT) under the framework of hypothesis testing theory. Extensive experiments empirically verify our proposed multiple discriminators outperform many prior arts. Furthermore, the robustness of discriminators is validated with considering some post-processing attacks.}
}
@article{CHANG2022103547,
title = {Multi-task learning for video anomaly detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103547},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103547},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000852},
author = {Xingya Chang and Yuxin Zhang and Dingyu Xue and Dongyue Chen},
keywords = {Anomaly detection, Multi-task learning, Deep SVDD, Future frame prediction, Local probability estimation},
abstract = {We propose a multi-task learning framework for video anomaly detection based on a novel pipeline. Our model contains two crossing streams, one stream employs the backbone of Attention-R2U-net for future frame prediction, while the other is designed based on an encoder–decoder network to reconstruct the input optical flow maps. In addition, the latent layers of the two streams are merged together and assigned with a Deep SVDD-based loss at each location individually. Through the combination of these three tasks, the two-stream-crossing pipeline can be trained end-to-end to provide a comprehensive evaluation for the anomaly targets. Experimental results on several popular benchmark datasets show that our model outperforms the state-of-the-art competing models, which can be applied to different types of anomalous targets and meanwhile achieves remarkable precision.}
}
@article{ZHANG2022103425,
title = {Multi-domain residual encoder–decoder networks for generalized compression artifact reduction},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103425},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103425},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100287X},
author = {Yi Zhang and Damon M. Chandler and Xuanqin Mou},
keywords = {Artifact reduction, Residual network, Compression artifact, Quality factor estimation},
abstract = {A fundamental requirement for designing compression artifact reduction techniques is to restore the artifact free image from its compressed version regardless of the compression level. Most existing algorithms require the prior knowledge of JPEG encoding parameters to operate effectively. Although there are works that attempt to train universal models to deal with different compression levels, some JPEG quality factors (QF) are still missing. To overcome these potential limitations, in this paper, we present a generalized JPEG-compression artifact reduction framework that relies on improved QF estimator and rectified networks to take into account all possible QF values. Our method, called a generalized compression artifact reducer (G-CAR), first predicts QF by analyzing luminance patches with high activity. Then, based on the estimated QF, images are adaptively restored by the cascaded residual encoder–decoder networks learned in multiple domains. Results tested on six benchmark datasets demonstrate the effectiveness of our proposed model.}
}
@article{WANG2022103545,
title = {Underwater image super-resolution and enhancement via progressive frequency-interleaved network},
journal = {Journal of Visual Communication and Image Representation},
volume = {86},
pages = {103545},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103545},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000839},
author = {Li Wang and Lizhong Xu and Wei Tian and Yunfei Zhang and Hui Feng and Zhe Chen},
keywords = {Underwater image, Super-resolution, Enhancement, Deep learning, Frequency domain},
abstract = {Underwater images usually contain severely blurred details, color distortion, and low contrast, warranting efficient methods to obtain clean images. However, most convolutional neural network-based approaches involve high computational cost, numerous model parameters, and even poor performance. Besides, the mapping from input to output is learned using a single path, ignoring the frequency domain information. To solve these challenges, we propose a novel progressive frequency-interleaved network (PFIN) for underwater imagery super-resolution and enhancement. Specifically, progressive frequency-domain module (PFDM) and convolution-guided module (CGM) constitute PFIN for effective color deviation correction and detail enhancement. PFDM that possesses global spatial attention, multi-scale residual, and frequency information modulation blocks gradually learn frequency features and explicitly compensate for detail loss. Furthermore, CGM comprising a series of convolution blocks generates discriminative characteristics to modulate in PFDM for better accommodating degraded representations. Extensive experiments demonstrate the superiority of our PFIN regarding quantitative evaluations and visual quality.}
}
@article{HU2022103426,
title = {Collaborative Distribution Alignment for 2D image-based 3D shape retrieval},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103426},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103426},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002881},
author = {Nian Hu and Heyu Zhou and An-An Liu and Xiangdong Huang and Shenyuan Zhang and Guoqing Jin and Junbo Guo and Xuanya Li},
keywords = {3D shape retrieval, Cross-domain retrieval, Multi-view learning},
abstract = {Retrieving 3D shapes with 2D images has become a popular research area nowadays, and a great deal of work has been devoted to reducing the discrepancy between 3D shapes and 2D images to improve retrieval performance. However, most approaches ignore the semantic information and decision boundaries of the two domains, and cannot achieve both domain alignment and category alignment in one module. In this paper, a novel Collaborative Distribution Alignment (CDA) model is developed to address the above existing challenges. Specifically, we first adopt a dual-stream CNN, following a similarity guided constraint module, to generate discriminative embeddings for input 2D images and 3D shapes (described as multiple views). Subsequently, we explicitly introduce a joint domain-class alignment module to dynamically learn a class-discriminative and domain-agnostic feature space, which can narrow the distance between 2D image and 3D shape instances of the same underlying category, while pushing apart the instances from different categories. Furthermore, we apply a decision boundary refinement module to avoid generating class-ambiguity embeddings by dynamically adjusting inconsistencies between two discriminators. Extensive experiments and evaluations on two challenging benchmarks, MI3DOR and MI3DOR-2, demonstrate the superiority of the proposed CDA method for 2D image-based 3D shape retrieval task.}
}
@article{HE2022103579,
title = {Unsupervised blind image quality assessment based on joint structure and natural scene statistics features},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103579},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103579},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001092},
author = {Qinglin He and Chao Yang and Fanxi Yang and Ping An},
keywords = {Blind image quality assessment, Structure information, Natural scene statistics, Karhunen–Loéve transform},
abstract = {Compared with the widely used supervised blind image quality assessment (BIQA) models, unsupervised BIQA models require little prior knowledge for calculating the objective quality scores of distorted images. In this paper, we propose an unsupervised BIQA method that aims to achieve both good performance and generalization capability with low computational complexity. Carefully selected and extensive structure and natural scene statistics (NSS) features can better represent image quality. First, we employ phase congruency (PC) and finely selected gradient magnitude map and Laplacian of Gaussian response (GM-LOG) features to represent image structure information. Second, we calculate the local mean-subtracted and contrast-normalized (MSCN) coefficients and the Karhunen–Loéve transform (KLT) coefficients to represent the naturalness of the distorted images. Last, multivariate Gaussian (MVG) model with joint features extracted from both the pristine images and the distorted images is adopted to calculate the objective image quality. Extensive experiments conducted on nine IQA databases demonstrate that the proposed method achieves better performance than the state-of-the-art BIQA methods.}
}
@article{2023103941,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {96},
pages = {103941},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(23)00191-8},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001918}
}
@article{BELBEL2022103525,
title = {Improved inter-view correlations for low complexity MV-HEVC},
journal = {Journal of Visual Communication and Image Representation},
volume = {86},
pages = {103525},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103525},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000682},
author = {Amel Belbel and Amara Bekhouch and Noureddine Doghmane and Saliha Harize and Nasreddine Kouadria},
keywords = {Inter-view offset, Disparity estimation, Coding unit, Inter-view correlations, Prediction unit},
abstract = {The more advanced multi-view extension, MV-HEVC, effectively exploits visual similarities between multi-view videos and enables high compression efficiency. Each view in the multi-view sequence depends on the captured scene, the distance between cameras and recording angles. Increasing the distance between dependent viewpoints generates an inter-view disparity. This impacts the inter-view similarities, affects the disparity estimation and further increases the computational complexity of the MV-HEVC encoder. In this paper, an efficient earlier disparity estimation is proposed for low complexity MV-HEVC. This algorithm is based on reducing the complexity of disparity estimation by eliminating the inter-view offset. Moreover, the inter-view similarities are controlled by considering the reliability of each coding unit size in the search range. This reliability is estimated by reducing the number of searching points within a new limited window. For reliable motion estimation, we further proposed an earlier decision of coding units splitting in the dependent views according to those in the reference views. Experimental results show that the proposed algorithm can achieve an average encoding time saving of 20.37%–40,61% with marginal performance degradation.}
}
@article{LIU2022103456,
title = {Tracking by dynamic template: Dual update mechanism},
journal = {Journal of Visual Communication and Image Representation},
volume = {84},
pages = {103456},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103456},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000165},
author = {Jing Liu and Yating Wang and Xiangdong Huang and Yuting Su},
keywords = {Visual object tracking, Siamese trackers, Template update mechanism, Real-time tracking},
abstract = {Recently, Siamese trackers have received widespread attention for visual object tracking owing to their good balance between speed and performance. Those Siamese trackers heavily depend on target template while conventional practice fixes the template to initial frame. This strategy makes it unable to cope with variation of target appearance, which often leads to tracking failures and causes the gap in performance from other tracking methods. Despite the performance gain achieved by few template update methods with target templates generated by the tracked results, these tracked templates are easy to accumulate errors and cause tracking drift. In this paper, we propose two template update mechanisms to effectively adapt the target template during the tracking process which is dubbed as DTDU (Dynamic Template with Dual Update). Unlike predecessors that directly use the tracked template, we use initial template to perform similar transformation to the tracked template. Then the similar transformed template and the tracked template are combined linearly to capture the variation of target appearance. These updated templates are stored in a memory bank and retrieved to generate the final target template. In order to enhance quick update of memory bank to accommodate the target appearance, we use the retrieved template to further update the templates in memory bank for subsequent tracking. Extensive experiments on OTB-2015, VOT2016, VOT2018 and GOT-10k datasets have proved the effectiveness of these two update mechanisms and the proposed tracker achieves a real-time speed of 44 fps.}
}
@article{YUAN2022103428,
title = {Accurate bounding-box regression with distance-IoU loss for visual tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103428},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103428},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100290X},
author = {Di Yuan and Xiu Shu and Nana Fan and Xiaojun Chang and Qiao Liu and Zhenyu He},
keywords = {Visual tracking, Bounding-box regression, Distance-IoU loss},
abstract = {Most existing trackers are based on using a classifier and multi-scale estimation to estimate the target state. Consequently, and as expected, trackers have become more stable while tracking accuracy has stagnated. While trackers adopt a maximum overlap method based on an intersection-over-union (IoU) loss to mitigate this problem, there are defects in the IoU loss itself, that make it impossible to continue to optimize the objective function when a given bounding box is completely contained within/without another bounding box; this makes it very challenging to accurately estimate the target state. Accordingly, in this paper, we address the above-mentioned problem by proposing a novel tracking method based on a distance-IoU (DIoU) loss, such that the proposed tracker consists of target estimation and target classification. The target estimation part is trained to predict the DIoU score between the target ground-truth bounding-box and the estimated bounding-box. The DIoU loss can maintain the advantage provided by the IoU loss while minimizing the distance between the center points of two bounding boxes, thereby making the target estimation more accurate. Moreover, we introduce a classification part that is trained online and optimized with a Conjugate-Gradient-based strategy to guarantee real-time tracking speed. Comprehensive experimental results demonstrate that the proposed method achieves competitive tracking accuracy when compared to state-of-the-art trackers while with a real-time tracking speed.}
}
@article{SUN2022103587,
title = {Progressive multi-branch embedding fusion network for underwater image enhancement},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103587},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103587},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001134},
author = {Kaichuan Sun and Fei Meng and Yubo Tian},
keywords = {Underwater image enhancement, Marine snow removal, Multi-branch embedding fusion, Multi-stage framework},
abstract = {The underwater image enhancement techniques are essential for ocean research and engineering applications. In this paper, we propose a progressive multi-branch embedding fusion network (PMEFN) to improve image quality. Specifically, a multi-branch embedding fusion module (MEFM) is designed. The distorted images and its sharpened versions are used as the input, which are fused to learn the contextualized features based on a two-branch hybrid encoder–decoder module (HEDM2) combined with the triple attention module to focus on the noise region. Afterwards, we use the multi-stage refining framework to decompose the image enhancement or marine snow removal tasks into multiple stages and progressively learn the nonlinear functions from the distorted inputs. Additionally, the outputs generated at each stage are further refined and enhanced based on a three-branch hybrid encoder–decoder module (HEDM3). We perform experiments using real underwater datasets, including EUVP, UFO-120, UIEB, and synthetic dataset MSRB. The experimental results show that the proposed method has a superior performance as compared to other methods in terms of quantitative performance and visual quality. In addition, the effectiveness of each component is further validated by performing ablation experiments.}
}
@article{BARAHA2022103546,
title = {Restoration of speckle noise corrupted SAR images using regularization by denoising},
journal = {Journal of Visual Communication and Image Representation},
volume = {86},
pages = {103546},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103546},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000840},
author = {Satyakam Baraha and Ajit Kumar Sahoo},
keywords = {Denoiser, Despeckling, PnP priors, Rayleigh noise, RED, Synthetic aperture radar},
abstract = {Speckle noise removal is a well-established problem in synthetic aperture radar (SAR) image processing. Among different methods focused on the reconstruction of SAR images, variational models have achieved state-of-the-art performance. In this paper, a Rayleigh based speckle reduction algorithm is developed using the variational framework. The forward model is combined with recently proposed regularization by denoising (RED) prior. However, RED has been proposed in literature for the additive noise model. Multiplicative noise in SAR images prevents the direct application of RED to variational models. Hence, logarithm transformation is applied to change the multiplicative noise model to additive model, and the forward model from Rayleigh to Fisher–Tippett distribution. The resulting optimization problem is solved using the alternating direction method of multipliers. Further, the proof of the convergence analysis is carried out for the above framework. Simulations convey that the proposed method has better despeckling performance compared to that of state-of-the-art methods.}
}
@article{ZHANG2022103617,
title = {A no-reference perceptual image quality assessment database for learned image codecs},
journal = {Journal of Visual Communication and Image Representation},
volume = {88},
pages = {103617},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103617},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001377},
author = {Jiaqi Zhang and Zhigao Fang and Lu Yu},
keywords = {Image quality assessment, Learning-based image compression, Generated image compression},
abstract = {The drastic growth of research in image compression, especially deep learning-based image compression techniques, poses new challenges to objective image quality assessment (IQA). Typical artifacts encountered in the emerging image codecs are significantly different from that produced by traditional block-based codecs, leading to inapplicability of the existing objective IQA algorithms. Towards advancing the development of objective IQA algorithms for recent compression artifacts, we built a learning-based compressed image quality assessment (LCIQA) database involving traditional block-based image codecs, hybrid neural network based image codecs, convolutional neural network based and generative adversarial network (GAN) based end-to-end optimized image coding approaches. Our study confirms the statistical difference and human perception difference between reconstructions of learned compression and traditional block-based compression. We propose a two-step deep learning model for learning-based compressed image quality assessment. Extensive experiments on LCIQA database demonstrate that our proposed model performs better than other counterparts on learning-based compressed images, especially on GAN compressed images, and achieves competitive performance to the state-of-the-art IQA metrics on traditional compressed images.}
}
@article{LIU2022103505,
title = {An efficient real-time target tracking algorithm using adaptive feature fusion},
journal = {Journal of Visual Communication and Image Representation},
volume = {85},
pages = {103505},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103505},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000566},
author = {Yanyan Liu and Changcheng Pan and Minglin Bie and Jin Li},
keywords = {Feature fusion, Target tracking, Real time},
abstract = {Visual-based target tracking is easily influenced by multiple factors, such as background clutter, targets’ fast-moving, illumination variation, object shape change, occlusion, etc. These factors influence the tracking accuracy of a target tracking task. To address this issue, an efficient real-time target tracking method based on a low-dimension adaptive feature fusion is proposed to allow us the simultaneous implementation of the high-accuracy and real-time target tracking. First, the adaptive fusion of a histogram of oriented gradient (HOG) feature and color feature is utilized to improve the tracking accuracy. Second, a convolution dimension reduction method applies to the fusion between the HOG feature and color feature to reduce the over-fitting caused by their high-dimension fusions. Third, an average correlation energy estimation method is used to extract the relative confidence adaptive coefficients to ensure tracking accuracy. We experimentally confirm the proposed method on an OTB100 data set. Compared with nine popular target tracking algorithms, the proposed algorithm gains the highest tracking accuracy and success tracking rate. Compared with the traditional Sum of Template and Pixel-wise LEarners (STAPLE) algorithm, the proposed algorithm can obtain a higher success rate and accuracy, improving by 2.3% and 1.9%, respectively. The experimental results also demonstrate that the proposed algorithm can reach the real-time target tracking with 50+fps. The proposed method paves a more promising way for real-time target tracking tasks under a complex environment, such as appearance deformation, illumination change, motion blur, background, similarity, scale change, and occlusion.}
}
@article{LIN2022103566,
title = {Mutual information maximizing GAN inversion for real face with identity preservation},
journal = {Journal of Visual Communication and Image Representation},
volume = {87},
pages = {103566},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103566},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322001018},
author = {Chengde Lin and Shengwu Xiong and Yaxiong Chen},
keywords = {Generative adversarial network, GAN inversion, Mutual information maximizing, Face identity preservation, Face editing},
abstract = {Recent generative adversarial networks (GANs) have yielded remarkable performance in face image synthesis. GAN inversion embeds an image into the latent space of a pretrained generator, enabling it to be used for real face manipulation. However, current inversion approaches for real faces suffer the dilemma of initialization collapse and identity loss. In this paper, we propose a hierarchical GAN inversion for real faces with identity preservation based on mutual information maximization. We first use a facial domain guaranteed initialization to avoid the initialization collapse. Furthermore, we prove that maximizing the mutual information between inverted faces and their identities is equivalent to minimizing the distance between identity features from inverted and original faces. Optimization for real face inversion with identity preservation is implemented on this mutual information-maximizing constraint. Extensive experimental results show that our approach outperforms state-of-the-art solutions for inverting and editing real faces, particularly in terms of face identity preservation.}
}