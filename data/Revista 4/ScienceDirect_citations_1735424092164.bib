@article{ZHOU2020102860,
title = {Adaptive deep feature aggregation using Fourier transform and low-pass filtering for robust object retrieval},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102860},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102860},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301115},
author = {Ziyao Zhou and Xinsheng Wang and Chen Li and Ming Zeng and Zhongyu Li},
keywords = {Image retrieval, Convolutional neural networks, Feature aggregation, Fourier transform, Low-pass filtering},
abstract = {With the rapid development of deep learning techniques, convolutional neural networks (CNN) have been widely investigated for the feature representations in the image retrieval task. However, the key step in CNN-based retrieval, i.e., feature aggregation has not been solved in a robust and general manner when tackling different kinds of images. In this paper, we present a deep feature aggregation method for image retrieval using the Fourier transform and low-pass filtering, which can adaptively compute the weights for each feature map with discrimination. Specifically, the low-pass filtering can preserve the semantic information in each feature map by transforming images to the frequency domain. In addition, we develop three adaptive methods to further improve the robustness of feature aggregation, i.e., Region of Interests (ROI) selection, spatial weighting and channel weighting. Experimental results demonstrate the superiority of the proposed method in comparison with other state-of-the-art, in achieving robust and accurate object retrieval under five benchmark datasets.}
}
@article{TANG2020102875,
title = {Translating video into language by enhancing visual and language representations},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102875},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102875},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301231},
author = {Pengjie Tang and Yunlan Tan and Jinzhong Li and Bin Tan},
keywords = {Video description, Feature enhancing, CNN, LSTM, Semantic},
abstract = {It is a fundamental task of translating videos into natural language automatically by computer. At present, the models for video description based on deep learning have made a great breakthrough. However, the static information loss is serious during encoding stage for motion feature of videos, and the linguistic feature from LSTM network lack personalized expression, leading to inappropriate words and poor semantics in generation sentences. In this work, a model with enhanced features of visual and language is proposed to address the challenges. First, static features of video frames from the first LSTM layer are incorporated, then fed into another LSTM layer according by frame sequence. Second, the feature of word is combined with the output of LSTM network for predicted probability of candidate word on each time step. The experimental results demonstrate effectiveness of the proposed approach with competitive performance compared with other state-of-the-art methods on various metrics.}
}
@article{WANG2020102907,
title = {Automatic foreground extraction from imperfect backgrounds using multi-agent consensus equilibrium},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102907},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102907},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301474},
author = {Xiran Wang and Jason Juang and Stanley H. Chan},
keywords = {Foreground extraction1, Alpha matting, Background subtraction, Video segmentation, Saliency detection, Plug-and-play ADMM, Consensus equilibrium},
abstract = {Extracting accurate foreground objects from a scene is an essential step for many video applications. Traditional background subtraction algorithms can generate coarse estimates, but generating high quality masks requires professional softwares with significant human interventions, e.g., providing trimaps or labeling key frames. We propose an automatic foreground extraction method in applications where a static but imperfect background is available. Examples include filming and surveillance where the background can be captured before the objects enter the scene or after they leave the scene. Our proposed method is very robust and produces significantly better estimates than state-of-the-art background subtraction, video segmentation and alpha matting methods. The key innovation of our method is a novel information fusion technique. The fusion framework allows us to integrate the individual strengths of alpha matting, background subtraction and image denoising to produce an overall better estimate. Such integration is particularly important when handling complex scenes with imperfect background. We show how the framework is developed, and how the individual components are built. Extensive experiments and ablation studies are conducted to evaluate the proposed method.}
}
@article{YIN2020102816,
title = {Reversible data hiding in binary images by flipping pattern pair with opposite center pixel},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102816},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102816},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300663},
author = {Xiaolin Yin and Wei Lu and JunHong Zhang and Jianfei Chen and Wanteng Liu},
keywords = {Reversible data hiding, Binary images, Pattern pair with opposite center pixel, Visual quality, Embedding payload},
abstract = {In this paper, a novel RDH scheme by flipping pattern pairs with opposite center pixel (PPOCPs) in binary images is proposed, aiming at decreasing the distortion while increasing the embedding payload. First, 25 patterns in the 3×3 block are designed which construct the PPOCPs according to the distance level providing a guarantee for reversibility. Then, a balanced score is designed which considers both visual distortion and embedding payload to select the optimal PPOCP, and the secret messages are embedded in the optimal PPOCP. For the receiver, the secret messages can be extracted precisely and the original binary image can be recovered by scanning the optimal PPOCP. PPOCP is a novel RDH model which fully considers the visual distortion caused by flipping pixels. Experimental results demonstrate the feasibility of the proposed RDH method for binary images, and the visual quality is satisfactory under high embedding payload and smallest pure flipping rate.}
}
@article{WANG2020102764,
title = {Sparsity adaptive matching pursuit for face recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {67},
pages = {102764},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102764},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300146},
author = {Yuhong Wang and Yali Peng and Shigang Liu and Jun Li and Xili Wang},
keywords = {Face recognition, Sparse representation, Matching pursuit},
abstract = {Sparse representation methods have exhibited promising performance for pattern recognition. However, these methods largely rely on the data sparsity available in advance and are usually sensitive to noise in the training samples. To solve these problems, this paper presents sparsity adaptive matching pursuit based sparse representation for face recognition (SAMPSR). This method adaptively explores the valid training samples that exactly represent the test via iterative updating. Next, the test samples are reconstructed via the valid training samples, and classification is performed subsequently. The two-phase strategy helps to improve the discriminating power of class probability distribution, and thus alleviates effect of the noise from the training samples to some extent and correctly performs classification. In addition, the method solves the sparse coefficient by comparing the residual between the test sample and the reconstructed sample instead of using the sparsity. A large number of experiments show that our method achieves promising performance.}
}
@article{GUI2020102792,
title = {Adaptive single image dehazing method based on support vector machine},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102792},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102792},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300420},
author = {Bian Gui and Yuhua Zhu and Tong Zhen},
keywords = {SVM, Adaptive dehazing, Automatic binary classification, Quality evaluation index},
abstract = {A dehazing method often only shows good results when processing the image for a certain haze concentration. So an adaptive hazy image dehazing method based on SVM is proposed. The innovation points are as follows: Firstly, combining the characteristics of the degraded images of haze weather, the dark channel histogram and texture features of the input images are extracted to form the feature vectors. These are trained by supervised learning through SVM algorithm to realize automatic binary classification of images; Secondly, the defined dehazing methods are called to process the classified result as a hazy image and the same quality evaluation indexes are used to evaluate each image output by different dehazing methods. Then, it outputs the highest evaluation image after haze removal. Finally, the output image is classified again by SVM until the image reaches the clearest it can be. The experimental results show that the proposed algorithm exhibits good contrast, brightness and color saturation from the visual effect. Also the scene adaptability and robustness of the algorithm are improved.}
}
@article{KANG2020102804,
title = {Combining polar harmonic transforms and 2D compound chaotic map for distinguishable and robust color image zero-watermarking algorithm},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102804},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102804},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300547},
author = {Xiaobing Kang and Fan Zhao and Yajun Chen and Guangfeng Lin and Cuining Jing},
keywords = {Polar harmonic transforms, Zero-watermarking, Geometric attacks, Chaos encryption},
abstract = {Although zero-watermarking can provide an effective and distortion-free scheme for image copyright protection, its robustness and discriminability do not meet expectations in existing methods. Some cannot resist effectively geometric attacks, others do not consider the discriminability and equalization. For that reason, this paper proposes a robust and distinguishable color image zero-watermarking algorithm based on polar harmonic transforms (PHTs) and compound chaotic map. In the proposed algorithm, firstly three PHTs moments of an image are computed simultaneously and accurate moments are selected for the robustness. Then, content-based binary feature sequence is acquired by judging the relation between magnitudes of adjacent moments for the discriminability. Finally, compound chaotic map is employed to encrypt copyright logo for ensuring security and scramble binary feature sequence for improving the equalization. Experimental results show that the proposed zero-watermarking algorithm has good equalization and discriminability, and an advantage in robustness compared with other zero-watermarking and traditional watermarking.}
}
@article{FAN2020102843,
title = {BURSTS: A bottom-up approach for robust spotting of texts in scenes},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102843},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102843},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300948},
author = {Jiayuan Fan and Tao Chen and Feng Zhou},
keywords = {Text spotting, CNN, Extremal region, Clustering},
abstract = {In this paper, we present a bottom-up approach for robust spotting of texts in scenes. In the proposed technique, character candidates are first detected using our proposed character detector, which leverages on the strengths of an Extremal Region (ER) detector and an Aggregate Channel Feature (ACF) detector for high character detection recall. The real characters are then identified by using a novel convolutional neural network (CNN) filter for high character detection precision. A hierarchical clustering algorithm is designed which combines multiple visual and geometrical features to group characters into word proposal regions for word recognition. The proposed technique has been evaluated on several scene text spotting datasets and experiments show superior spotting performance.}
}
@article{MA2020102876,
title = {Looking ahead: Joint small group detection and tracking in crowd scenes},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102876},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102876},
url = {https://www.sciencedirect.com/science/article/pii/S104732032030122X},
author = {Qiulin Ma and Qi Zou and Nan Wang and Qingji Guan and Yanting Pei},
keywords = {Group tracking, Delay decision, Joint optimization, Multiple hypothesis tracking},
abstract = {Small group detection and tracking in crowd scenes are basis for high level crowd analysis tasks. However, it suffers from the ambiguities in generating proper groups and in handling dynamic changes of group configurations. In this paper, we propose a novel delay decision-making based method for addressing the above problems, motivated by the idea that these ambiguities can be solved using rich temporal context. Specifically, given individual detections, small group hypotheses are generated. Then candidate group hypotheses across consecutive frames and their potential associations are built in a tree. By seeking for the best non-conflicting subset from the hypothesis tree, small groups are determined and simultaneously their trajectories are got. So this framework is called joint detection and tracking. This joint framework reduces the ambiguities in small group decision and tracking by looking ahead for several frames. However, it results in the unmanageable solution space because the number of track hypotheses grows exponentially over time. To solve this problem, effective pruning strategies are developed, which can keep the solution space manageable and also improve the credibility of small groups. Experiments on public datasets demonstrate the effectiveness of our method. The method achieves the state-of-the-art performance even in noisy crowd scenes.}
}
@article{ELAYAPERUMAL2020102820,
title = {Visual object tracking using sparse context-aware spatio-temporal correlation filter},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102820},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102820},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300705},
author = {Dinesh Elayaperumal and Young Hoon Joo},
keywords = {Context, ADMM, Spatio-temporal,  regularization, Visual tracking, Correlation filter},
abstract = {This paper presents a novel sparse context-aware spatio-temporal correlation filter tracker (SCAST) method for robust visual object tracking. Different from the existing trackers, this paper introduce an l1 multi-scale regularization parameter-based correlation filter that reduces the boundary effect due to partial occlusions, illumination and scale variations. At each iteration, the l1 regularization parameter is updated through spatial knowledge of each correlation filter coefficient. Besides, the contextual information acquired from the target region can lead to determining the accurate localization of the target. Moreover, contextual information has combined with spatio-temporal factor to achieve the better performance. Further, an objective function is designed with system constraints to ensure the applicability of the model and the optimal solution is derived by utilizing the alternating direction method of multiplier, which leads to low computational cost. Finally, the feasibility and superiority of proposed tracker algorithm is evaluated through three benchmark dataset: OTB-2013, OTB-2015, and TempleColor-128.}
}
@article{ZHOU2020102927,
title = {Transformed denoising autoencoder prior for image restoration},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102927},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102927},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301590},
author = {Jinjie Zhou and Zhuonan He and Xiaodong Liu and Yuhao Wang and Shanshan Wang and Qiegen Liu},
keywords = {Image restoration, Denoising autoencoder, Pixel domain, Wavelet domain},
abstract = {Image restoration problem is generally ill-posed, which can be alleviated by learning image prior. Inspired by the considerable performance of utilizing priors in pixel domain and wavelet domain jointly, we propose a novel transformed denoising autoencoder as prior (TDAEP). The core idea behind TDAEP is to enhance the classical denoising autoencoder (DAE) via transform domain, which captures complementary information from multiple views. Specifically, 1-level nonorthogonal wavelet coefficients are used to form 4-channel feature images. Moreover, a 5-channel tensor is obtained by stacking the original image under the pixel domain and 4-channel feature images under the wavelet domain. Then we train the transformed DAE (TDAE) with the 5-channel tensor as the network input. The optimized image prior is obtained based on the trained autoencoder, and it is incorporated into an iterative restoration procedure with the aid of the auxiliary variable technique. The resulting model is affiliationed by proximal gradient descent technique. Numerous experiments demonstrated that the TDAEP outperforms a set of image restoration benchmark algorithms.}
}
@article{HUANG2020102925,
title = {Convolutional neural network with adaptive inferential framework for skeleton-based action recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102925},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102925},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301589},
author = {Hong’en Huang and Hang Su and Zhigang Chang and Mingyang Yu and Jialin Gao and Xinzhe Li and Shibao Zheng},
keywords = {Skeleton-based action recognition, Pseudo image, Adaptive inferential framework, Different prior information},
abstract = {In the task of skeleton-based action recognition, CNN-based methods represent the skeleton data as a pseudo image for processing. However, it still remains as a critical issue of how to construct the pseudo image to model the spatial dependencies of the skeletal data. To address this issue, we propose a novel convolutional neural network with adaptive inferential framework (AIF-CNN) to exploit the dependencies among the skeleton joints. We particularly investigate several initialization strategies to make the AIF effective with each strategy introducing the different prior knowledge. Extensive experiments on the dataset of NTU RGB+D and Kinetics-Skeleton demonstrate that the performance is improved significantly by integrating the different prior information. The source code is available at: https://github.com/hhe-distance/AIF-CNN.}
}
@article{LI2020102799,
title = {Histopathological image classification through discriminative feature learning and mutual information-based multi-channel joint sparse representation},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102799},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102799},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300493},
author = {Xiao. Li and Hongzhong. Tang and Dongbo. Zhang and Ting. Liu and Lizhen. Mao and Tianyu. Chen},
keywords = {Discriminative feature learning, Stack-based discriminative prediction sparse decomposition (SDPSD), Mutual information-based Multi-channel joint sparse model (MIMJSM), Histopathological image classification},
abstract = {Histopathological image classification is a very challenging task because of the biological heterogeneities and rich geometrical structures. In this paper, we propose a novel histopathological image classification framework, which includes the discriminative feature learning and the mutual information-based multi-channel joint sparse representation. We first propose a stack-based discriminative prediction sparse decomposition (SDPSD) model by incorporating the class labels information to predict deep discriminant features automatically. Subsequently, a mutual information-based multi-channel joint sparse model (MIMCJSM) is presented to jointly encode the common component and particular components of the discriminative features. Especially, the main advantage of the MIMCJSM is the construction of a joint dictionary using a mutual information criterion, which contains a common sub-dictionary and three particular sub-dictionaries. Based on the joint dictionary, the MIMCJSM captures the relationship of multi-channel features, which can improve discriminative ability of joint sparse representation coefficients. Finally, the joint sparse representation coefficients of different levels can be aggregated using the spatial pyramid matching (SPM) model, and the linear support vector machine (SVM) is used as the classifier. Experimental results on ADL and BreaKHis datasets demonstrate that our proposed framework consistently performs better than popular existing classification frameworks. Additionally, it can show promising strong-robustness performance for histopathological image classification.}
}
@article{JIA2020102908,
title = {Scalable Hash From Triplet Loss Feature Aggregation For Video De-duplication},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102908},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102908},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301462},
author = {Wei Jia and Li Li and Zhu Li and Shuai Zhao and Shan Liu},
keywords = {Binary hash, Binary tree, Fisher vector, Triplet loss, Video de-duplication},
abstract = {The producing, sharing and consuming life cycle of video content creates massive amount of duplicates in video segments due to variable bit rate representation and fragmentation in the playbacks. The inefficiency of this duplicates to storage and communication motivate researchers in both academia and industry to come up with computationally efficient video deduplication solutions for storage and CDN providers. Moreover, the increasing demands of high resolution and quality aggravate the status of heavy burden of cluster storage side and restricted bandwidth resources. Hence, video de-duplication in storage and transmission is becoming an important feature for video cloud storage and Content Delivery Network (CDN) service providers. Despite of the necessity of optimizing the multimedia data de-duplication approach, it is a challenging task because we should match as many as possible duplicated videos under not removing videos by mistake. The current video de-duplication schemes mostly relies on the URL based solution, which is not able to deal with non-cacheable content like video, which the same piece of content may have totally different URL identification and fragmentation and different quality representations further complicate the problem. In this paper, we propose a novel content based video segmentation identification scheme that is invariant to the underlying codec and operational bit rates, it computes robust features from a triplet loss deep learning network that captures the invariance of the same content under different coding tools and strategy, while a scalable hashing solution is developed based on Fisher Vector aggregation of the convolutional features from the Triplet loss network. Our simulation results demonstrate the great improvement in terms of large scale video repository de-duplication compared with state-of-the-art methods.}
}
@article{WANG2020102868,
title = {Blind quality assessment for multiply distorted stereoscopic images towards IoT-based 3D capture systems},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102868},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102868},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301188},
author = {Xuejin Wang and Meiling Qi and Feng Shao and Qiuping Jiang and Xiangchao Meng},
keywords = {Internet of things, Image quality assessment, Multiply-distorted stereoscopic images, High order statistics},
abstract = {Empowered by 5G mobile communication networks, multimedia processing has been considered as a very promising application of Internet-of-Things (IoT). Stereoscopic image quality assessment (SIQA), as an important part of 3D capture system, can be embedded in the cloud or fog servers to automatically monitor the perceptual quality of the collected stereoscopic images. In this paper, a novel blind image quality assessment method towards IoT-based 3D capture systems is developed for multiply-distorted stereoscopic images (MDSIs), in which five complementary channels, including left view, right view, cyclopean map, summation map and difference map, are jointly considered in dictionary learning for characterizing the monocular receptive field (MRF) and binocular receptive field (BRF) properties of the visual cortex in response to MDSIs. Additionally, the high order statistics scheme is adopted by utilizing the statistical differences between the codebook and images to ensure the stable and robust quality prediction performance for MDSIs. The proposed method shows competitive prediction performances on four benchmark databases compared with the existing SIQA metrics.}
}
@article{SU2020102815,
title = {Clustering adaptive canonical correlations for high-dimensional multi-modal data},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102815},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102815},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300651},
author = {Shuzhi Su and Xianjin Fang and Gaoming Yang and Bin Ge and Ping Zheng},
keywords = {Canonical correlation analysis, Joint dimension reduction, Clustering adaptive, High-dimensional data},
abstract = {Multi-modal canonical correlation analysis (MCCA) is an important joint dimension reduction method and has been widely applied to clustering tasks of multi-modal data. MCCA-based clustering is usually dimension reduction of high-dimensional data followed by clustering of low-dimensional data. However, the two-stage clustering is difficult to ensure the adaptability of dimension reduction and clustering, which will affect the final clustering performance. To solve the issue, we propose a novel clustering adaptive multi-modal canonical correlations (CAMCCs) method, which constructs a unified optimization model of multi-modal correlation learning and clustering. The method not only realizes discriminant learning of correlation projection directions under unsupervised cases, but also is able to directly obtain class labels of multi-modal data. Additionally, the method also realizes out-of-sample extension in class labels. Solutions of CAMCCs are optimized by an iterative way, and we analyze its convergence. Extensive experimental results on various datasets have demonstrated the effectiveness of the method.}
}
@article{LIU2020102763,
title = {☆ - Discriminative dictionary learning algorithm based on sample diversity and locality of atoms for face recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102763},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102763},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300134},
author = {Shigang Liu and Yuhong Wang and Xiaosheng Wu and Jun Li and Tao Lei},
keywords = {Dictionary learning, Face recognition, Locality constrained, Sample diversity},
abstract = {Dictionary learning is one of the most important algorithms for face recognition. However, many dictionary learning algorithms for face recognition have the problems of small sample and weak discriminability. In this paper, a novel discriminative dictionary learning algorithm based on sample diversity and locality of atoms is proposed to solve the problems. The rational sample diversity is implemented by alternative samples and new error model to alleviate the small sample size problem. Moreover, locality can leads to sparsity and strong discriminability. In this paper, to enhance the dictionary discrimination and to reduce the influence of noise, the graph Laplacian matrix of atoms is used to keep the local information of the data. At the same, the relational theory is presented. A large number of experiments prove that the proposed algorithm can achieve more high performance than some state-of-the-art algorithms.}
}
@article{HERAVI2020102803,
title = {Adult-child 3D backward face aging model (3D B-FAM)},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102803},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102803},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300535},
author = {Farnaz Majid Zadeh Heravi and Amine Nait-Ali},
keywords = {3D modeling, Face aging, Textured 3D mesh, Biometrics, Anthropometry, Depth perception, Forensics},
abstract = {Face aging has been widely considered in many studies regarding all the potential applications. However, the de-aging known as the rejuvenation or backward modeling has recently received more attention. Previous studies mainly focused on rejuvenating faces from aged adults into young adults using two-dimensional (2D) models. In this work, we propose an extension of a previous 2D adult-child B-FAM into 3D model. This model allows a digital face appearance rejuvenation within a range of [75–3] years old. To evaluate the performances of the proposed approach, first, we proposed two performance evaluation modes, namely: Generic Perception Based and Biometric Verification Mode. Then, the performances have been evaluated over our own 3D database, called Face Time-Machine database constructed using 75 females and 70 males, leading to 500 textured surface meshes. Finally, results show that they are perceptually satisfying and system performance increases by using the faces obtained from our model.}
}
@article{AMEUR2020102842,
title = {Chronological pattern indexing: An efficient feature extraction method for hand gesture recognition with Leap Motion},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102842},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102842},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300936},
author = {Safa Ameur and Anouar {Ben Khalifa} and Med Salim Bouhlel},
keywords = {Leap Motion, Hand gesture recognition, Feature extraction, Time series data, Chronological indexing},
abstract = {Recently, Hand-Gesture-Recognition (HGR) systems has appreciably change the way of interaction between humans and computers thanks to advanced sensor technologies like the Leap-Motion-Controller (LMC). Despite the success achieved by many state-of-the-art methods, they have not worked on the rich temporal information existing in the sequential hand gesture data and characterizing the discriminative representation of different hand gesture classes. In this paper, we suggest a novel Chronological-Pattern-Indexing (CPI) approach which encodes the temporal orders of patterns for hand gesture time series data acquired by the LMC sensor. We extract a set of temporal patterns from different optimized projections. Then, we compare their temporal order and we encode the whole sequence with the index of the first coming pattern. We repeat these steps until we generate an efficient feature vector modeling the chronological dynamics of the hand gesture. The experiments demonstrate the potential of the proposed CPI approach for HGR systems.}
}
@article{WEI2020102751,
title = {Exploiting the local temporal information for video captioning},
journal = {Journal of Visual Communication and Image Representation},
volume = {67},
pages = {102751},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102751},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300018},
author = {Ran Wei and Li Mi and Yaosi Hu and Zhenzhong Chen},
keywords = {Local temporal information, Video captioning, Sliding windows, Reinforcement learning},
abstract = {Typical video captioning methods are developed based on the encoder-decoder architecture. To better exploit the local temporal information, e.g., details about objects and their corresponding actions, we propose a reinforcement learning based method to predict the adaptive sliding window size sequentially for better event exploration. More specifically, we introduce the single Monte-Carlo sample to approximate the gradient of reward-based loss function. And the self-critical strategy is employed to estimate baseline reward to diminish the variance of gradients. Moreover, temporal attention is utilized to selectively focus on a subset of temporal frame representations while generating each word. In addition, to better initialize the decoder’s state, we utilize the motion features extracted by 3D CNNs with mean pooling to endow the decoder with the prior knowledge of the entire video. To evaluate the proposed method, experiments are performed on three public benchmark datasets: Microsoft Video Description Corpus (MSVD), MSR Video to Text challenge (MSR-VTT) and Charades. The experimental results demonstrate the effectiveness of our method by comparing with state-of-the-art methods.}
}
@article{SONG2020102791,
title = {An efficient tensor completion method via truncated nuclear norm},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102791},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102791},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300419},
author = {Yun Song and Jie Li and Xi Chen and Dengyong Zhang and Qiang Tang and Kun Yang},
keywords = {Tensor completion, Tensor singular value decomposition, Truncated tensor nuclear norm, Visual data restoration},
abstract = {Tensor completion aims to recover missing entries from partial observations for multi-dimensional data. Traditional tensor completion algorithms process the dimensional data by unfolding the tensor into matrices, which breaks the inherent correlation and dependencies in multiple channels and lead to critical information loss. In this paper, we propose a novel tensor completion model for visual multi-dimensional data completion under the tensor singular value decomposition (t-SVD) framework. In the proposed method, tensor is treated as a whole and a truncated nuclear norm regularization is employed to exploit the structural properties in a tensor and hidden information existing among the adjacent channels of a tensor. Besides, we introduce a weighted tensor to adjust the residual error of each frontal slices in consideration of their different recovery statistics. It does enhance the sparsity of all unfoldings of the tensor and accelerates the convergence of the proposed method. Experimental results on various visual datasets demonstrate the promising performance of the proposed method in comparison with the state-of-the-art tensor completion methods.}
}
@article{HARISHBABU2020102912,
title = {A survey on analysis and implementation of state-of-the-art haze removal techniques},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102912},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102912},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301504},
author = {G. {Harish Babu} and N. Venkatram},
keywords = {Opalescent, Image dehazing, Image restoration, Computational time, Machine learning, Deep learning, Hardware implementation},
abstract = {Haze is a poor-quality state described by the opalescent appearance of the atmosphere which reduces the visibility. It is caused by high concentrations of atmospheric air pollutants, such as dust, smoke and other particles that scatter and absorb sunlight. The poor visibility can result in the failure of multiple computer vision applications such as smart transport systems, image processing, object detection, surveillance etc. One of the major issues in the field of image processing is the restoration of images that are corrupted due to different degradations. Typically, the images or videos captured in the outside environment have low contrast, colour fade and restricted visibility due to suspended particles of the atmosphere that directly influence the image quality. This can cause difficulty in identifying the objects in the captured hazy images or frames. To address this problem, several image dehazing techniques have been developed in the literature, each of which has its own advantages and limitations, but effective image restoration remains a challenging task. In recent times, various learning (Machine learning & Deep learning) based methods greatly condensed the drawbacks of manual design of haze related features and reduces the difficulty in efficient restoration of images with less computational time and cost. The current state-of-the-art methods for haze free images, mainly from the last decade, are thoroughly examined in this survey. Moreover, this paper systematically summarizes the hardware implementations of various haze removal methods in real time. It is with the hope that this current survey acts as a reference for researchers in this scientific area and to provide a direction for future improvements based on current achievements.}
}
@article{QI2020102814,
title = {A novel haze image steganography method via cover-source switching},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102814},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102814},
url = {https://www.sciencedirect.com/science/article/pii/S104732032030064X},
author = {Baojun Qi and Chunfang Yang and Lei Tan and Xiangyang Luo and Fenlin Liu},
keywords = {Steganography, Haze image, Natural steganography, UNIWARD, Steganalysis},
abstract = {In realistic outdoor scenarios, image sensors tend to suffer from various weather conditions (e.g., haze, rain, etc.),which make the images of the same scene taken at different times may be different. Therefore, one should be able to securely embed secret messages into these images by making use of the variations of the weather effects. Inspired by some recent natural steganography algorithms, this paper presents a novel haze image steganography method, which embeds messages through adjusting the weather effects of an input haze image, making it resemble the same image captured under another weather condition. The proposed steganography method consists of three parts: (1) model parameter estimation of the input haze image, (2) haze effects adjustment according to the atmospheric scattering model, (3) message embedding using the floating-point adjusted haze image. 10,000 haze images captured under different haze conditions in various scenarios were used to test the proposed steganography algorithm. The experimental results show that the proposed steganography algorithm is more secure than S-UNIWARD and HILL for steganalyzers who only have raw haze images.}
}
@article{PARK2020102759,
title = {Color image enhancement with high saturation using piecewise linear gamut mapping},
journal = {Journal of Visual Communication and Image Representation},
volume = {67},
pages = {102759},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102759},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300092},
author = {Junhee Park and Byung-Uk Lee},
keywords = {Gamut mapping, Color enhancement, Hue preservation, Color saturation},
abstract = {Most of the color image enhancement algorithms are implemented in two stages: gray scale image enhancement, which finds the target intensity, and then gamut mapping of the original color coordinates to the target. Therefore, hue preserving gamut mapping is an essential and crucial step, which influences colorfulness. In conventional color mapping methods, color saturation is reduced after intensity modification, which deteriorates subjective image quality. In this paper, a new color enhancement algorithm resulting in high color saturation is proposed. The proposed method employs multiplicative and additive color mapping to improve color saturation without clipping of a color component for increased target intensity as well as decreased cases. This new scheme is fast and effective, therefore, it can be employed to real time applications such as video signal processing.}
}
@article{LIN2020102686,
title = {Intra mode prediction for H.266/FVC video coding based on convolutional neural network},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102686},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102686},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303074},
author = {Ting-Lan Lin and Kai-Wen Liang and Jing-Ya Huang and Yu-Liang Tu and Pao-Chi Chang},
keywords = {H.266/Future Video Coding (FVC), Convolutional Neural Network (CNN), Intra mode},
abstract = {The next-generation video compression standard H.266/Future Video Coding (FVC) provides high compression efficiency in terms of the cost of computing the optimal intra mode from 67 modes. We propose an intra mode prediction method based on a convolutional neural network (CNN). An input image set of 20 × 20 blocks is used to train the CNN; the CNN is used to predict the best classes of intra mode direction. The CNN architecture comprises two convolutional layers and a fully connected layer. Compared with the default fast search method in FVC, the proposed method can achieve a 0.033% decrease in Bjøntegaard delta bit rate (BDBR) with only a slight increase in time.}
}
@article{JIANG2020102775,
title = {Cross-level reinforced attention network for person re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {69},
pages = {102775},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102775},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300250},
author = {Min Jiang and Cong Li and Jun Kong and Zhende Teng and Danfeng Zhuang},
keywords = {Person re-identification, Features of different levels, Soft attention, Hard attention, Reinforced attention},
abstract = {Attention mechanism is a simple and effective method to enhance discriminative performance of person re-identification (Re-ID). Most of previous attention-based works have difficulty in eliminating the negative effects of meaningless information. In this paper, a universal module, named Cross-level Reinforced Attention (CLRA), is proposed to alleviate this issue. Firstly, we fuse features of different semantic levels using adaptive weights. The fused features, containing richer spatial and semantic information, can better guide the generation of subsequent attention module. Then, we combine hard and soft attention to improve the ability to extract important information in spatial and channel domains. Through the CLRA, the network can aggregate and propagate more discriminative semantic information. Finally, we integrate the CLRA with Harmonious Attention CNN (HA-CNN) and form a novel Cross-level Reinforced Attention CNN (CLRA-CNN) to optimize person Re-ID. Experiment results on several public benchmarks show that the proposed method achieves state-of-the-art performance.}
}
@article{LI2020102911,
title = {(t, k, n) XOR-based visual cryptography scheme with essential shadows},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102911},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102911},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301498},
author = {Peng Li and Jianfeng Ma and Quan Ma},
keywords = {XOR operation, Essential participants, Visual cryptography, Secret image sharing},
abstract = {Visual cryptography scheme (VCS) shares a binary secret image into multiple shadows, only qualified set of shadows can reveal the secret image by stacking operation. However, VCS suffers the problems of low visual quality of the revealed image and large shadow size. A (t, k, n) XOR-based visual cryptography scheme (XVCS) shares the secret image into n shadows including t essentials and n-t non-essentials. A qualified set of shadows contains any k shadows including t essentials. The revealing process is implemented by XOR operation on the involved shadows. In this paper, we propose a construction method for (t, k, n)-XVCS with essential shadows. The secret image can be revealed perfectly, and the shadow size is small compared with VCS. Theoretical analysis and experimental results show the security and effectiveness of the proposed scheme.}
}
@article{LI2020102845,
title = {Volume preserving image segmentation with entropy regularized optimal transport and its applications in deep learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102845},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102845},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300961},
author = {Haifeng Li and Jun Liu and Li Cui and Haiyang Huang and Xue-Cheng Tai},
keywords = {Image segmentation, DCNN, Volume preserving, Optimal transport, Entropic regularization, TV regularization},
abstract = {Image segmentation with a volume constraint is an important prior for many real applications. In this work, we present a novel volume preserving image segmentation algorithm, which is based on the entropy and Total Variation (TV) regularized optimal transport theory. The volume and classification constraints can be regarded as two measures preserving constraints in the optimal transport. By studying the dual problem, we develop a simple but efficient dual algorithm for our model. Moreover, to be different from many variational based image segmentation algorithms, the proposed algorithm can be directly unrolled to a new Volume Preserving and TV regularized softmax (VPTV-softmax) layer for semantic segmentation in the popular Deep Convolution Neural Network (DCNN). The experiment results show that our proposed model is very competitive and can improve the performance of many semantic segmentation networks such as the popular U-net and DeepLabv3+.}
}
@article{ZHANG2020102899,
title = {Semi-supervised cross-modal representation learning with GAN-based Asymmetric Transfer Network},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102899},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102899},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301413},
author = {Lei Zhang and Leiting Chen and Weihua Ou and Chuan Zhou},
keywords = {Cross-modal retrieval, Modality gap, Generative adversarial network},
abstract = {In this paper, we proposed a semi-supervised common representation learning method with GAN-based Asymmetric Transfer Network (GATN) for cross modality retrieval. GATN utilizes the asymmetric pipeline to guarantee the semantic consistency and adopt (Generative Adversarial Network) GAN to fit the distributions of different modalities. Specifically, the common representation learning across modalities includes two stages: (1) the first stage, GATN trains source mapping network to learn the semantic representation of text modality by supervised method; and (2) the second stage, GAN-based unsupervised modality transfer method is proposed to guide the training of target mapping network, which includes generative network (target mapping network) and discriminative network. Experimental results on three widely-used benchmarks show that GATN have achieved better performance comparing with several existing state-of-the-art methods.}
}
@article{LI2020102818,
title = {Candidate region correlation for video action detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102818},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102818},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300687},
author = {Yeguang Li and Mingyuan Zhang and Liang Hu and Jun Li and Deqing Wang},
keywords = {Deep learning, Action detection, Region correlation, Self-attention mechanism},
abstract = {The rapid development of deep learning has prompted the development of video action detection technology. However, the accuracy of current video action detection algorithms can be improved further. Previous work has improved feature extraction by optimizing the network structure. In addition, the features of the candidate regions have been optimized by changing the representation of the regions. Although these methods have achieved promising results, they fail to consider the correlation among different candidate regions, generating uninformative (even redundant) candidate regions, and thus usually decrease the detection performance in practice. To address this problem, in this paper we propose a self-attention mechanism for candidate regions, which can help pursue the most informative regions. We obtain the region correlation by simultaneously determining the spatial and temporal correlation among different candidate regions. In addition, we focus on how to apply the correlation to optimize the original candidate region features and improve video action detection accuracy. The experimental results show the promising improvement achieved by our method over the state-of-the-art solutions.}
}
@article{LU2020102794,
title = {Deep hierarchical encoding model for sentence semantic matching},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102794},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102794},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300444},
author = {Wenpeng Lu and Xu Zhang and Huimin Lu and Fangfang Li},
keywords = {Hierarchical encoding model, Hierarchical measure mechanism, Sentence similarity, Semantic equivalence identification, Text representation},
abstract = {Sentence semantic matching (SSM) always plays a critical role in natural language processing. Measuring the intrinsic semantic similarity among sentences is very challenging and has not been substantially addressed. The latest SSM research usually relies on a shallow text representation and interaction between sentence pairs, which might not be enough to capture the complex semantic features and lead to limited performance. To capture more semantic context features and interactions, we propose a hierarchical encoding model (HEM) for sentence representation, further enhanced by a hierarchical matching mechanism for sentence interaction. Given two sentences, HEM generates intermediate and final representations in encoding layer, which are further handled by a novel hierarchical matching mechanism to capture more multi-view interactions in matching layer. The comprehensive experiments demonstrate that our model is capable to capture more sentence semantic features and interactions, which significantly outperforms the existing state-of-the-art neural models on the public real-world dataset.}
}
@article{WANG2020102850,
title = {An improved DCT-based JND estimation model considering multiple masking effects},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102850},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102850},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301012},
author = {Hongkui Wang and Li Yu and Haibing Yin and Tiansong Li and Shengwei Wang},
keywords = {DCT-based JND estimation model, Contrast masking, Disorderly concealment effect},
abstract = {The just noticeable distortion (JND) in the contour and orderly regions is easy to be overestimated and that in the disorderly areas is usually underestimated. In order to estimate the JND threshold more accurately, this paper proposes an improved DCT-based JND estimation model considering multiple masking effects properly. The contributions of this paper are characterized by twofold. On the one hand, a mean absolute difference based (MAD-based) block classification method is developed at first to classify image blocks into plain, contour and texture types accurately and quickly. And the JND model for contrast masking effect (CM-JND) is constructed as a modulation factor based on the MAD of each block. On the other hand, we propose a distance-based disorder evaluation metric to measure the disorder intensity in block level. Then, the JND model for the disorderly concealment effect (DC-JND) is proposed based on our psychological experiment. Finally, the total JND estimation threshold is modeled by fusing the spatial contrast sensitivity function, the luminance adaptation effect, the CM and DC effects. Experimental results show that the proposed DCT-based JND estimation model outperforms existing models in performance and complexity. Specifically, the proposed model shows more tolerance for distortions, lower computational complexity with better perceptual quality than other JND models.}
}
@article{XIE2020102770,
title = {Diverse receptive field network with context aggregation for fast object detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102770},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102770},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300201},
author = {Shaorong Xie and Chang Liu and Jiantao Gao and Xiaomao Li and Jun Luo and Baojie Fan and Jiahong Chen and Huayan Pu and Yan Peng},
keywords = {Object detection, Convolutional neural network, Context aggregation, Multi-scale contextual representations},
abstract = {Current context-utilizing detectors are all based on two-stage approaches. However, their computational efficiency and context quality extremely depend on the accuracy of proposal-generating methods, which limits their performance and makes them hardly perform real-time detection. In this work, we present a context-exploited method that integrates features in different receptive fields to obtain contextual representation. Based on this idea, we put forward the multi-branch diverse receptive field modules (DRF modules) and their design principles to encode context. To further utilize contextual information for fast object detection, we propose a one-stage diverse receptive field network (DRFNet). In DRFNet, the DRF modules are first applied to capture rich context as the basis, then a parallel structure is constructed to exploit the context at different scales along with DRF modules. Comprehensive experiments indicate that the context introduced by our methods improves the detection performance and DRFNet achieves a good trade-off between speed and accuracy.}
}
@article{YUAN2020102882,
title = {TRBACF: Learning temporal regularized correlation filters for high performance online visual object tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102882},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102882},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301255},
author = {Di Yuan and Xiu Shu and Zhenyu He},
keywords = {Visual tracking, Correlation filters, BACF tracker, Temporal regularization, ADMM},
abstract = {Correlation filter-based trackers (CFTs) have recently shown remarkable performance in the field of visual object tracking. The advantage of these trackers originates from their ability to convert time-domain calculations into frequency domain calculations. However, a significant problem of these CFTs is that the model is insufficiently robust when the tracking scenarios are too complicated, meaning that the ideal tracking performance cannot be acquired. Recent work has attempted to resolve this problem by reducing the boundary effects from modeling the foreground and background of the object target effectively (e.g., CFLB, BACF, and CACF). Although these methods have demonstrated reasonable performance, they are often affected by occlusion, deformation, scale variation, and other challenging scenes. In this study, considering the relationship between the current frame and the previous frame of a moving object target in a time series, we propose a temporal regularization strategy to improve the BACF tracker (denoted as TRBACF), a typical representative of the aforementioned trackers. The TRBACF tracker can efficiently adjust the model to adapt the change of the tracking scenes, thereby enhancing its robustness and accuracy. Moreover, the objective function of our TRBACF tracker can be solved by an improved alternating direction method of multipliers, which can speed up the calculation in the Fourier domain. Extensive experimental results demonstrate that the proposed TRBACF tracker achieves competitive tracking performance compared with state-of-the-art trackers.}
}
@article{CHENG2020102844,
title = {Protein secondary structure prediction based on integration of CNN and LSTM model},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102844},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102844},
url = {https://www.sciencedirect.com/science/article/pii/S104732032030095X},
author = {Jinyong Cheng and Yihui Liu and Yuming Ma},
keywords = {Protein secondary structure prediction, Convolution neural networks, Long short-term memory, Softmax, Random forest},
abstract = {Protein structure prediction is an important issue in computational biology, and protein secondary structure prediction is the basis for protein three-dimensional structure prediction. A protein secondary structure prediction method based on convolutional neural networks (CNN) and Long Short-Term Memory (LSTM) is proposed in this paper. The architecture of CNN has two convolutional layers, one max-pooling layer and one ReLU activation layer. The feature maps extracted from second convolutional layer are used to feed to softmax classifier, and the first probability output is obtained. The LSTM model has a sequence layer and a last layer. The feature is extracted from last layer and input to random forest classifier to get the second probability output. The two probabilistic outputs are weighted and integrated to obtain the prediction model EN-CSLR in this paper. Based on the advantages of integration of the two models, cross-validation experiments are performed on the 25pdb dataset, and Q3 reaches 80.18%, which is higher than using only one model. The experimental results show that the features extracted from CNN and LSTM models can effectively improve the accuracy of protein secondary structure prediction.}
}
@article{SUN2020102862,
title = {Multi-scale active patches fusion based on spatiotemporal LBP-TOP for micro-expression recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102862},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102862},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301139},
author = {Zhe Sun and Zheng-ping Hu and Mengyao Zhao and Shufang Li},
keywords = {Micro-expression, Multi-scale active patches, Weighted sparse representation, LBP-TOP},
abstract = {Micro-expressions are spontaneous emotions appearing on a face that is hard to conceal and thus making them different from normal facial expressions both in duration and subtlety. This paper investigates a challenging issue in micro-expression, where not all facial regions contribute equally to effective representation. Consequently, we proposed a multi-scale active patches fusion-based spatiotemporal LBP-TOP descriptor that considers the active contributions for different region area in faces. For the feature procedure, we exploit the average value of all patches under each scale to obtain the threshold that selectively fuses the local and global features. On the other hand, an improved weighted sparse representation based dual augmented Lagrange multiplier is adopted for the classification to remit the problem of sparse coefficients obtained by the traditional sparse representation algorithm. We conduct comprehensive experiments on CASME II and SAMM datasets and the accuracies respectively reach 77.30% and 58.82% using LOSO cross-validation.}
}
@article{JOSHI2020102834,
title = {Taguchi-TOPSIS based HOG parameter selection for complex background sign language recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102834},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102834},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300845},
author = {Garima Joshi and Sukhwinder Singh and Renu Vig},
keywords = {Histogram of Oriented Gradients (HOG), Taguchi, TOPSIS, Sign Language Recognition System (SLRS), Complex background, Indian Sign Language (ISL)},
abstract = {This paper presents an approach to design Indian Sign Language (ISL) recognition system for complex background. In many applications, Histogram of Oriented Gradients (HOG) have been proved to be effective. However, it is observed that the choice of HOG parameters affects the feature vector size and its classification capability. The objective is to select the parameter values in order to have maximal accuracy at a minimal computational time and reduced feature vector size. A combined Taguchi and Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) based decision-making technique is applied to determine the values of these parameters. Results show that the combined TOPSIS-Taguchi based technique is effective in selecting the parameter combination to get high overall performance. For the acquired ISL complex background dataset, the selected values of parameters are further used to obtain multi-level HOG resulting in the overall accuracy of 92% for 280 features.}
}
@article{OU2020102895,
title = {Gray-level image denoising with an improved weighted sparse coding},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102895},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102895},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301401},
author = {Yang Ou and Jianqiao Luo and Bailin Li and M.N.S. Swamy},
keywords = {Image denoising, Nonlocal self-similarity, Weight matrix, Weighted sparse coding},
abstract = {The nonlocal self-similarity of images means that groups of similar patches have low-dimensional property. The property has been previously used for image denoising, with particularly notable success via sparse coding. However, only a few studies have focused on the varying statistics of noise in different similar patches during the iterative denoising process. This has motivated us to introduce an improved weighted sparse coding for gray-level image denoising in this paper. On the basis of traditional sparse coding, we introduce a weight matrix to account for the noise variation characteristics of different similar patches, while introduce another weight matrix to make full use of the sparsity priors of natural images. The Maximum A-Posterior estimation (MAP) is used to obtain the closed-form solution of the proposed method. Experimental results demonstrate the competitiveness of the proposed method compared with that of state-of-the-art methods in both the objective and perceptual quality.}
}
@article{CAI2020102861,
title = {No-reference image sharpness assessment based on discrepancy measures of structural degradation},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102861},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102861},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301127},
author = {Hao Cai and Mingjie Wang and Wendong Mao and Minglun Gong},
keywords = {Image sharpness assessment, Structural degradation, No-reference, Orientation selectivity mechanism},
abstract = {The discrepancy between an image and its “reblurred” version indicates the extent of blur in the image. This paper presents a novel no-reference image sharpness evaluator leveraging the discrepancy measures of structural degradation in both the spatial and wavelet domains. Specifically, local structural degradation of an input image is characterized by the discrepancy measures of orientation selectivity-based visual patterns and log-Gabor filter responses between the image and its corresponding reblurred version respectively. Considering the influence of viewing distance on image quality, the global sharpness discrepancy is measured through inter-resolution self-similarities. Finally, the computed discrepancies are utilized as sharpness-aware features and then a support vector regressor is employed to map the feature vectors into quality scores. The performance of the proposed method is evaluated on six public image quality databases, including two real blurred image databases. Experimental results demonstrate that our proposed method achieves state-of-the-art performances across all these databases.}
}
@article{SU2020102753,
title = {Spatio-temporal metric learning for individual recognition from locomotion},
journal = {Journal of Visual Communication and Image Representation},
volume = {67},
pages = {102753},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102753},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300031},
author = {Yong Su and Simin An and Zhiyong Feng and Meng Xing and Jianhai Zhang},
keywords = {Individual recognition, Riemannian motion features, Spatio-Temporal Large Margin Nearest Neighbor (ST-LMNN), Spatio-Temporal Multi-Metric Learning (STMM)},
abstract = {Individual recognition from locomotion is a challenging task owing to large intra-class and small inter-class variations. In this article, we present a novel metric learning method for individual recognition from skeleton sequences. Firstly, we propose to model articulated body on Riemannian manifold to describe the essence of human motion, which can reflect biometric signatures of the enrolled individuals. Then two spatia-temporal metric learning approaches are proposed, namely Spatio-Temporal Large Margin Nearest Neighbor (ST-LMNN) and Spatio-Temporal Multi-Metric Learning (STMM), to learn discriminant bilinear metrics which can encode the spatio-temporal structure of human motion. Specifically, the ST-LMNN algorithm extends the bilinear model into classical Large Margin Nearest Neighbor method, which learns a low-dimensional local linear embedding in the spatial and temporal domain, respectively. To further capture the unique motion pattern for each individual, the proposed STMM algorithm learns a set of individual-specific spatio-temporal metrics, which make the projected features of the same person closer to its class mean than that of different classes by a large margin. Beyond that, we present a new publicly available dataset for locomotion recognition to evaluate the influence of both internal and external covariant factors. According to the experimental results from the three public datasets, we believe that the proposed approaches are both able to achieve competitive results in individual recognition.}
}
@article{MANANDHAR2020102871,
title = {Semantic granularity metric learning for visual search},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102871},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102871},
url = {https://www.sciencedirect.com/science/article/pii/S104732032030119X},
author = {Dipu Manandhar and Muhammet Bastan and Kim-Hui Yap},
keywords = {Deep learnin, Metric learning, Metric loss functions, Semantic similarity, Visual search},
abstract = {Existing metric learning methods often do not consider different granularity in visual similarity. However, in many domains, images exhibit similarity at multiple granularities with visual semantic concepts, e.g.fashion demonstrates similarity ranging from clothing of the exact same instance to similar looks/design or common category. Therefore, training image triplets/pairs inherently possess different degree of information. Nevertheless, the existing methods often treat them with equal importance which hinder capturing underlying granularities in image similarity. In view of this, we propose a new semantic granularity metric learning (SGML) that develops a novel idea of detecting and leveraging attribute semantic space and integrating it into deep metric learning to capture multiple granularities of similarity. The proposed framework simultaneously learns image attributes and embeddings with multitask-CNN where the tasks are linked by semantic granularity similarity mapping to leverage correlations between the tasks. To this end, we propose a new soft-binomial deviance loss that effectively integrates informativeness of training samples into metric-learning on-the-fly during training. Compared to recent ensemble-based methods, SGML is conceptually elegant, computationally simple yet effective. Extensive experiments on benchmark datasets demonstrate its superiority e.g., 1–4.5%-Recall@1 improvement over the state-of-the-arts (Kim et al., 2018; Cakir et al., 2019) on DeepFashion-Inshop dataset.}
}
@article{CHEN2020102849,
title = {A novel fast intra mode decision for versatile video coding},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102849},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102849},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301000},
author = {Yamei Chen and Li Yu and Hongkui Wang and Tiansong Li and Shengwei Wang},
keywords = {Versatile video coding, Intra mode decision, Mode correlation, Sorted candidate list, Early termination},
abstract = {Abstract
The latest video coding standard Versatile Video Coding (VVC) obtains superior coding efficiency compared to the High Efficiency Video Coding (HEVC), which is achieved by incorporating more effective and complex new coding tools. In this paper, we propose a novel fast intra mode decision algorithm for VVC, including following two strategies: (1) the correlation between the optimal modes of the adjacent blocks and the modes selected in the rough modes decision (RMD) process is analyzed and applied to reduce the modes in the candidate list; (2) modes in the candidate list are sorted in ascending order according to the modes’ cost calculated in the RMD process. An early termination method is proposed for terminating the optimal prediction mode decision process based on this new order early. These two strategies are incorporated into intra coding to reduce the coding complexity. Since these two strategies do not add any additional computational complexity, the proposed fast algorithm can achieve more complexity reduction. The experimental results show that the complexity reduction of the proposed algorithm is up to 44.74% compared to VVC reference software VTM2.0, and averagely 30.59% encoding time saving with 0.86% BDBR increase.}
}
@article{RAZZAGHI2020102817,
title = {Learning spatial hierarchies of high-level features in deep neural network},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102817},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102817},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300675},
author = {Parvin Razzaghi and Karim Abbasi and Pegah Bayat},
keywords = {Perceptual grouping, Modified Guided Co-occurrence Block (mGCoB), Map-wise Fully Connected Block (MFC), Spatial hierarchies of high-level features},
abstract = {This paper addresses a new approach to learn perceptual grouping of the extracted features of the convolutional neural network (CNN) to represent the structure contained in the image. In CNN, the spatial hierarchies between the high-level features are not taken into account. To do so, the perceptual grouping of features is utilized. To consider the intra-relationship between feature maps, modified Guided Co-occurrence Block (mGCoB) is proposed. This block preserves the joint co-occurrence of two features in the spatial domain and it prevents the co-adaptation. Also, to preserve the interrelationship in each feature map, the principle of common region grouping is utilized which states that the features which are located in the same feature map tend to be grouped together. To consider it, an MFC block is proposed. To evaluate the proposed approach, it is applied to some known semantic segmentation and image classification datasets that achieve superior performance.}
}
@article{HU2020102765,
title = {Parallel spatial-temporal convolutional neural networks for anomaly detection and location in crowded scenes},
journal = {Journal of Visual Communication and Image Representation},
volume = {67},
pages = {102765},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102765},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300158},
author = {Zheng-ping Hu and Le Zhang and Shu-fang Li and De-gang Sun},
keywords = {Abnormal detection, Video surveillance, Parallel 3D convolution neural networks, Spatial-temporal interest cuboids},
abstract = {Anomaly detection and location in crowded scenes have attracted a lot of attention in computer vision research community recently due to the increased applications of intelligent surveillance improve security in public. We propose a novel parallel spatial-temporal convolution neural networks model to detect and localize the abnormal behavior in video surveillance. Our approach contains two main steps. Firstly, considering the typical position of camera and the large number of background information, we introduce a novel spatial-temporal cuboid of interest detection method with varied-size cell structure and optical flow algorithm. Then, we use the parallel 3D convolution neural networks to describe the same behavior in different temporal-lengths. That step ensures that the most of behavior information in cuboids could be captured, also insures the reduction of information unrelated to the major behavior. The evaluation results on benchmark datasets show the superiority of our method compared to the state-of-the-art methods.}
}
@article{WU2020102878,
title = {Light field all-in-focus image fusion based on spatially-guided angular information},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102878},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102878},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301243},
author = {Yingchun Wu and Yumei Wang and Jie Liang and Ivan V. Bajić and Anhong Wang},
keywords = {Light field camera, Micro-lens array, All-in-focus image fusion, Spatial information, Angular information, Guided filtering},
abstract = {Compared to traditional 2D images, light field images record both spatial and angular information of the scene, which can provide more data for image fusion. In this paper, a light field all-in-focus image fusion algorithm based on spatially-guided angular information is proposed. In the proposed method, the initial weight maps carrying the angular information are calculated by comparing the block variance of the 4D light field data. The initial weight maps are then guided by digital refocused images carrying the spatial information to obtain the refined weight maps. In the refocused image multi-scale decomposition, the micro-lens calibration error is considered and the additional edge layers are extracted to suppress the edge artifacts. Experiments demonstrate the effectiveness of the proposed algorithm. Quantitative evaluation results show that the proposed algorithm performs the best in the feature-based index and structural similarity-based index without sacrificing the information and perceptual sharpness of the fused image.}
}
@article{WU2020102793,
title = {Probabilistic color visual cryptography schemes for black and white secret images},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102793},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102793},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300432},
author = {Xiaotian Wu and Ching-Nung Yang},
keywords = {Visual cryptography, Probabilistic, Color, Black and white, Pixel expansion, Secret image sharing},
abstract = {Color-black-and-white visual cryptography scheme (CBW-VCS) is a methodology that utilizes colors to alleviate the pixel expansion problem. In a general (k,n) CBW-VCS, when k and n become larger, the pixel expansion increases dramatically. In this paper, two constructions for constituting a (k,n) threshold probabilistic CBW-VCS (PCBW-VCS) are introduced, where the generated color shares are non-expansible. The two proposed constructions are proven to be valid constructions which satisfy the security and contrast conditions. Theoretical analysis and sufficient experiments are demonstrated to shown the effectiveness and advantages of the proposed PCBW-VCSs.}
}
@article{YEDROUDJ2020102910,
title = {Steganography using a 3-player game},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102910},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102910},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301486},
author = {Mehdi Yedroudj and Frédéric Comby and Marc Chaumont},
keywords = {Steganalysis, Deep learning, CNN, GAN},
abstract = {Image steganography aims to securely embed secret information into cover images. Until now, adaptive embedding algorithms such as S-UNIWARD or Mi-POD, were among the most secure and most often used methods for image steganography. With the arrival of deep learning and more specifically, Generative Adversarial Networks (GAN), new steganography techniques have appeared. Among them is the 3-player game approach, where three networks compete against each other. In this paper, we propose three different architectures based on the 3-player game. The first architecture is proposed as a rigorous alternative to two recent publications. The second takes into account stego noise power. Finally, our third architecture enriches the second one with a better interaction between embedding and extracting networks. Our method achieves better results compared to existing works Hayes and Danezis (2017), Zhu et al. (2018), and paves the way for future research on this topic.}
}
@article{ZHANG2020102821,
title = {Reversible data hiding in JPEG bitstream using optimal VLC mapping},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102821},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102821},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300717},
author = {Cheng Zhang and Bo Ou and Huawei Tian and Zheng Qin},
keywords = {Reversible data hiding, JPEG bitstream, VLC mapping, File size preservation},
abstract = {The traditional RDH method for JPEG bitstream is conducted by building the mapping between the variable length codes (VLC). However, the capacity is limited, and the file size may not be well preserved as the capacity is increased. This is because that the trade-off between the capacity and the file size has not been deeply investigated, neither explicitly formulated nor appropriately optimized. In this paper, we propose to take the file size preservation into consideration and minimize the file size increase for a given capacity. We use the value transfer matrix to simulate a theoretical model and then design some optimization rules to reach the reversible solution. Consequently, a better reversible VLC mapping can be obtained in terms of both the capacity and the file size preservation. The experimental results show that the proposed method can increase the capacity with a relatively low cost of file size increase.}
}
@article{LIU2020102833,
title = {Relative view based holistic-separate representations for two-person interaction recognition using multiple graph convolutional networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102833},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102833},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300833},
author = {Xing Liu and Yanshan Li and Tianyu Guo and Rongjie Xia},
keywords = {Activity analysis, Human interaction recognition, Skeleton data, Graph Convolution, Relative view},
abstract = {In this paper, we focus on recognizing person-person interactions using skeletal data captured from depth sensors. First, we propose a novel and efficient view transformation scheme. The skeletal interaction sequence is re-observed under a new coordinate system, which is invariant to various setups and capturing views of depth cameras as well as the position or facing orientation exchange between two persons. Second, we propose concise and discriminative interaction representations simply composed of the joint locations from two persons. Proposed representations are efficient to describe both the holistic interactive scene and individual poses performed by each subject separately. Third, we introduce the graph convolutional networks(GCN) to directly learn proposed skeletal interaction representations. Moreover, we design a multiple GCN-based model to provide the final class score. Extensive experimental results on three skeletal action datasets NTU RGB+D 60, NTU RGB+D 120 and SBU consistently demonstrate the superiority of our interaction recognition method.}
}
@article{SHEN2020102789,
title = {Image understanding via learning weakly-supervised cross-modal semantic translation},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102789},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102789},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300390},
author = {Guorong Shen},
keywords = {Image understanding, Cross-modal semantic translation, Weakly-supervised learning},
abstract = {Fusing cross-modal features is significant for image understanding, which aims at describing objects inside an image by optimally combining multiple visual channels. In the literature, low-level based multimodal feature fusion have achieved impressive performance. However, the semantic gap is a big limitation, i.e., these methods cannot reflect the how humans perceive image semantic objects. Supervised learning-based methods require intolerably expensive manual labeling, which is not a good choice in practice. To alleviate these limitations, we present an image understanding method by learning weakly-supervised based cross-modal semantic translation. More specifically, we design a manifold embedding algorithm to automatically translate image-level text semantic labels into several pixel-level image regions. Subsequently, we leverage a three-level spatial pyramid model to extract both local and global features of objects from training images. Afterwards, these cross-modal features are seamlessly concatenated to form a multiple feature matrix. Afterwards, these cross-modal features are seamlessly concatenated to form a multiple feature matrix. The feature matrix can be employed to learn a kernel SVM and ranking SVM for image classification and retrieval respectively. Comprehensive experiments on image recognition, classification and retrieval have demonstrated the effectiveness of our method.}
}
@article{YAN2020102917,
title = {An adaptive spatio-temporal perception aware quantization algorithm for AVS2},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102917},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102917},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301553},
author = {Yunyao Yan and Guoqing Xiang and Yuan Li and Xiaodong Xie and Huizhu Jia},
keywords = {Adaptive quantization, Perceptual video coding (PVC), Just noticeable distortion (JND), Spatially perceptual complexity, Temporally perceptual complexity, Subjective quality},
abstract = {Adaptive quantization proves to be an effective tool to improve coding performance. In this paper, we propose an adaptive spatiotemporal perception aware quantization algorithm to increase subjective coding performance. To measure the spatiotemporally perceptual redundancy, the perceptual complexity models are firstly established with spatial and temporal characteristics respectively. With the help of the models, the adaptive spatial and temporal quantization parameter (QP) offsets are then calculated for each coding tree unit (CTU), respectively. Finally, the perceptually optimal Lagrange multiplier of each CTU is determined with the spatial–temporal QP offset. Experimental results show that the proposed algorithm reduces 8.6% and 8.4% Bjontegaard-Delta Rate (BD-Rate) with Structural Similarity Index Metric (SSIM) in average over the second generation of Audio Video Coding Standard (AVS2) reference software RD17.0 in Low-Delay-P (LDP) and Random-Access (RA) configurations, respectively. The subjective assessment proves that the proposed algorithm can reduce the bitrates with the same subjective quality significantly.}
}
@article{WANG2020102852,
title = {Multi-path connected network for medical image segmentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102852},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102852},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301036},
author = {Dan Wang and Guoqing Hu and Chengzhi Lyu},
keywords = {Medical image segmentation, Multi-path connections, Convolutional neural networks, Encoder-decoder structure},
abstract = {In recent years, deep learning has been successfully applied to medical image segmentation. However, as the network extends deeper, the consecutive downsampling operations will lead to more loss of spatial information. In addition, the limited data and diverse targets increase the difficulty for medical image segmentation. To address these issues, we propose a multi-path connected network (MCNet) for medical segmentation problems. It integrates multiple paths generated by pyramid pooling into the encoding phase to preserve semantic information and spatial details. We utilize multi-scale feature extractor block (MFE block) in the encoder to obtain large and multi-scale receptive fields. We evaluated MCNet on three medical datasets with different image modalities. The experimental results show that our method achieves better performance than the state-of-the-art approaches. Our model has strong feature learning ability and is robust to capture different scale targets. It can achieve satisfactory results while using only 0.98 million (M) parameters.}
}
@article{FENG2020102881,
title = {Learning discriminative update adaptive spatial-temporal regularized correlation filter for RGB-T tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102881},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102881},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301279},
author = {Mingzheng Feng and Kechen Song and Yanyan Wang and Jie Liu and Yunhui Yan},
keywords = {Object tracking, Correlation filters, Adaptive spatial-temporal regularization, ADMM, Model updating},
abstract = {The RGB-T trackers based on correlation filter framework have been extensively investigated for that they can track targets more accurately in most complex scenes. However, the performance of these trackers is limited when facing some specific challenging scenarios, such as occlusion and background clutter. For different tracking targets, most of these trackers utilize fixed regularization constraint to build the filter model, which is obviously unreasonable to effectively present the appearance changes and characteristics of a specific target. In addition, they adopt a simple model update mechanism based on linear interpolation, which can easily lead to model degradation in challenging scenarios, resulting in tracker drift. To solve the above problems, we propose a novel adaptive spatial-temporal regularized correlation filter model to learn an appropriate regularization for achieving robust tracking and a relative peak discriminative method for model updating to avoid the model degradation. Besides, to make better integrate the unique advantages of the two modes and adapt the changing appearance of the target, an adaptive weighting ensemble scheme and a multi-scale search mechanism are adopted, respectively. To optimize the proposed model, we designed an efficient ADMM algorithm, which greatly improved the efficiency. Extensive experiments have been carried out on two available datasets, RGBT234 and RGBT210, and the experimental results indicate that the tracker proposed by us performs favorably in both accuracy and robustness against the state-of-the-art RGB-T trackers.}
}
@article{WANG2020102897,
title = {Weakly supervised single image dehazing},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102897},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102897},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301395},
author = {Cong Wang and Wanshu Fan and Yutong Wu and Zhixun Su},
keywords = {Image dehazing, Weakly supervised, Convolutional neural network (CNN), Multi-level multi-scale block},
abstract = {Single image dehazing is a critical image pre-processing step for many practical vision systems. Most existing dehazing methods solve this problem utilizing various of hand-crafted priors or by supervised training on the synthetic hazy image information (such as haze-free image, transmission map and atmospheric light). However, the assumptions on the hand-crafted priors are easily violated and collecting realistic transmission map and atmospheric light are unpractical. In this paper, we propose a novel weakly supervised network based on the multi-level multi-scale block. The proposed network reduces the constraint on the training data and automatically estimates the transmission map and the atmospheric light as well as the intermediate haze-free image without using any realistic transmission map and atmospheric light as supervision. Moreover, the estimated intermediate haze-free image helps to generate accurate transmission map and atmospheric light by embedding the physical-model, which presents reliable restoration of the final haze-free image. In particular, our network also can be trained on the real-world dataset to fine-tune the model and the fine-tuning operation improves the dehazing performance on the real-world dataset. Quantitative and qualitative experimental results demonstrate the proposed method performs on par with the supervised methods.}
}
@article{ESCOBEDOCARDENAS2020102772,
title = {Multimodal hand gesture recognition combining temporal and pose information based on CNN descriptors and histogram of cumulative magnitudes},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102772},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102772},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300225},
author = {Edwin Jonathan {Escobedo Cardenas} and Guillermo Camara Chavez},
keywords = {Hand gesture recognition, Spherical coordinates, Keyframe extraction, Pose and motion information, Convolucional neuronal networks, Histogram of cumulative magnitudes, Fusion schemes},
abstract = {In this paper, we present a new approach for dynamic hand gesture recognition. Our goal is to integrate spatiotemporal features extracted from multimodal data captured by the Kinect sensor. In case the skeleton data is not provided, we apply a novel skeleton estimation method to compute temporal features. Furthermore, we introduce an effective method to extract a fixed number of keyframes to reduce the processing time. To extract pose features from RGB-D data, we take advantage of two different approaches: (1) Convolutional Neural Networks and (2) Histogram of Cumulative Magnitudes. We test different integration methods to fuse the extracted spatiotemporal features to boost recognition performance in a linear SVM classifier. Extensive experiments prove the effectiveness and feasibility of the proposed framework for hand gesture recognition.}
}
@article{LONG2020102796,
title = {Locality preserving projection based on Euler representation},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102796},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102796},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300468},
author = {Tianhang Long and Yanfeng Sun and Junbin Gao and Yongli Hu and Baocai Yin},
keywords = {Locality preserving projection, Euler representation, Dimensionality reduction},
abstract = {Locality preserving projection (LPP) is a widely used linear dimensionality reduction method, which preserves the locality structure of the original data. Motivated by the fact that kernel technique can capture nonlinear similarity of features and help to improve separability between nearby data points, this paper proposes locality preserving projection model based on Euler representation (named as ELPP). This model first projects the data into a complex space with Euler representation, then learns the dimensionality reduction projection with preserving locality structure in this complex space. We also extend ELPP to F-ELPP by replacing the squared F-norm with F-norm, which will weaken the exaggerated errors and be more robustness to outliers. The optimization algorithms of the two models are given, and the convergence of F-ELPP is proved. A large number of experiments on several public databases have demonstrated that the two proposed models have good robustness and feature extraction ability.}
}
@article{WANG2020102847,
title = {An image similarity descriptor for classification tasks},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102847},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102847},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300985},
author = {Liangliang Wang and Deepu Rajan},
keywords = {Image similarity, Similarity representation, Deep features selection, Correlational descriptor},
abstract = {We develop an image similarity descriptor for an image pair, based on deep features. The development consists of two parts - selecting the deep layer whose features are to be included in the descriptor, and a representation of the similarity between the images in the pair. The selection of the deep layer follows a sparse representation of the feature maps followed by multi-output support vector regression. The similarity representation is based on a novel correlation between the histograms of the feature maps of the two images. Experiments to demonstrate the effectiveness of the proposed descriptor are carried out on four applications that can be cast as classification tasks.}
}
@article{HU2020102812,
title = {Generating video animation from single still image in social media based on intelligent computing},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102812},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102812},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300626},
author = {Tao Hu and Chao Liang and Geyong Min and Keqin Li and Chunxia Xiao},
keywords = {Animation, Convolutional neural network, Image motion analysis, Shape context, Stochastic motion texture},
abstract = {Bringing a single still image into reality is a challenging topic in computer animation because the driven and structural information in single still image is inadequate. In this paper, we present an image animating method for enhancing single still image in social media with virtual realistic and animated motions without prior information. We imitate the interaction between the active objects in an image and their neighboring passive objects. The existing actions in the image and the virtual specified force are employed to animate the active objects. Observing that the change between two subsequent motions of the active objects derives a motion tendency, we can calculate a virtual driving force based on the motion tendency. By virtue of the virtual driving force, the stochastic motion texture is used to animate the passive objects. Finally, the convolutional neural network is employed to optimize the virtual motion animations. In this way, the proposed method produces visually natural results while guaranteeing motion harmony between active objects and passive objects. To demonstrate the applicability and rationality of virtual animation driving force, our method generates several animations from still images in Social Media.}
}
@article{FU2020102760,
title = {A novel multi-focus image fusion method based on distributed compressed sensing},
journal = {Journal of Visual Communication and Image Representation},
volume = {67},
pages = {102760},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102760},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300109},
author = {Guan-Peng Fu and Shao-Hua Hong and Fu-Lin Li and Lin Wang},
keywords = {Distributed compressed sensing, Decision map, Multi-focus image fusion, Joint-sparsity-model-1},
abstract = {Multi-focus image fusion aims to produce an all-in-focus image by merging multiple partially focused images of the same scene. The main work is identifying the focused region and then composing all the focused regions. In this paper, a novel efficient multi-focus image fusion method based on distributed compressed sensing (DCS) is proposed. Firstly, the low-frequency and high-frequency images are obtained by comparing the variance of the source images, which are further utilized to get the low-frequency and high-frequency dictionaries. Secondly, DCS using joint sparsity model-1 (JSM-1) is applied to reconstruct the precise high-frequency images. Thirdly, the decision map is obtained based on all the high-frequency images and then improved by the morphological processing. Finally, the focused pixels are chosen from the source images through the decision map. Experimental results indicate that the proposed DCS-based method can be competitive with or even outperform some state-of-the-art methods in terms of both visual and quantitative metric evaluations.}
}
@article{2022103443,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103443},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(22)00007-4},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000074}
}
@article{SARDAR2020102768,
title = {A new lossless secret color image sharing scheme with small shadow size},
journal = {Journal of Visual Communication and Image Representation},
volume = {68},
pages = {102768},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102768},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300183},
author = {Md Kutubuddin Sardar and Avishek Adhikari},
keywords = {Secret image sharing, -threshold scheme, Small shadow size, Lossless recovery, Color image},
abstract = {Though there have been many research works carried out on the grayscale secret image sharing schemes, there is not sufficient research work available on the field of color secret image sharing schemes, especially on the lossless secret image sharing schemes. In the current work, we propose a new lossless secret color image sharing scheme with small shadow size. We directly deal with the three components of the intensities of the RGB values of each color pixel of the secret image. Using a mathematical transformation, we first embed the RGB intensity values of each secret pixel as an element of a suitably chosen finite field F. Then the whole mathematical operations are carried over the finite field F to generate shares using k-1 degree polynomials in the polynomial ring F[x]. Unlike most of the image sharing schemes, we do not have the preprocessing stage in which the secret image is transferred into random image either by using Arnold Cat map or by using some chaotic maps to avoid residual image effect. From experimental simulations, it is evident that the measure of randomness of the shares generated using our algorithm is same as that of the shares generated using the preprocessing stage in which Arnold Cat map or chaotic map is used, resulting our scheme simpler and efficient. Moreover, our scheme produces shares having smaller size than the secret image. Finally the recovery of the secret image by the qualified set of participants is lossless.}
}
@article{LIU2020102880,
title = {Transparency-guided ensemble convolutional neural network for the stratification between pseudoprogression and true progression of glioblastoma multiform in MRI},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102880},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102880},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301267},
author = {Xiaoming Liu and Xiaobo Zhou and Xiaohua Qian},
keywords = {Pseudo progression, Glioblastoma multiforme, Diffusion tensor imaging (DTI), Convolutional networks understanding, Ensemble CNN},
abstract = {For patients with glioblastoma multiform (GBM), differentiating pseudoprogression (PsP) from true tumor progression (TTP) is a challenging and time-consuming task for radiologists. Although deep neural networks can automatically diagnose PsP and TTP, lacking of interpretability has always been its major drawback. To overcome these shortcomings and produce more reliable outcomes, we propose a transparency-guided ensemble convolutional neural network (CNN) to automatically discriminate PsP and TTP in magnetic resonance imaging (MRI). A total of 84 patients with GBM were enrolled in the study. First, three typical convolutional neutral networks, namely VGG, ResNet and DenseNet, were trained to distinguish PsP and TTP. Subsequently, we used class-specific gradient information from convolutional layers to highlight the important regions in MRI scans. And radiologists selected the most lesion-relevant layer for each CNN. Finally, the selected layers are utilized to guide the construction of a multi-scale ensemble CNN whose classification accuracy reached 90.20%, and whose specificity is promoted 20% than that of a single CNN. The results demonstrate the presented network can enhance the reliability and accuracy of CNNs.}
}
@article{JIANG2020102846,
title = {Spatial-temporal saliency action mask attention network for action recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102846},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102846},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300973},
author = {Min Jiang and Na Pan and Jun Kong},
keywords = {Action recognition, Two-stream, Saliency attention, Key-frame},
abstract = {Recently, video action recognition about two-stream network is still a popular research topic in computer vision. However, most of current two-stream-based methods have two redundancy issues, including: inter-frame redundancy and intra-frame redundancy. To solve the above problems, a Spatial-Temporal Saliency Action Mask Attention network (STSAMANet) is built for action recognition. First, this paper introduces a key-frame mechanism to eliminate inter-frame redundancy. This mechanism can compute key frames on each video sequence to get the greatest difference between frames. Then, Mask R-CNN detection technology is introduced to build a saliency attention layer to eliminate intra-frame redundancy. This layer is to focus on the saliency human body and objects for each action class. We experiment on two public video action datasets, i.e., the UCF101 dataset and Penn Action dataset to verify the effectiveness of our method in action recognition.}
}
@article{YAO2020102795,
title = {JPEG quantization step estimation with coefficient histogram and spectrum analyses},
journal = {Journal of Visual Communication and Image Representation},
volume = {69},
pages = {102795},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102795},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300456},
author = {Heng Yao and Hongbin Wei and Tong Qiao and Chuan Qin},
keywords = {Image forensics, JPEG compression, Quantization-step estimation, Periodicity analysis},
abstract = {This paper proposes a new method for estimating quantization steps (QSs) from an image that has been previously JPEG-compressed and stored in a lossless format. In this method, DCT coefficients of each frequency band of JPEG-compressed image are aggregated in the QS and its multiples. The entire estimation process can be grouped into two categories: alternating and direct current bands. Considering that DCT coefficients under different QSs show different periodicity, QS estimation for each band is then further divided into three steps, which involve identifying whether the QS is one, two, or another value. For each step, the periodicity of DCT coefficients can be well exploited with the analyses of the DCT-coefficient histogram and its corresponding frequency magnitude spectrum. Experimental results demonstrate the efficacy of the proposed method and the superiority in QS estimation for previously JPEG-compressed images, especially in the case that the actual QSs are higher than two.}
}
@article{AFONSO2020102823,
title = {Hierarchical learning using deep optimum-path forest},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102823},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102823},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300730},
author = {Luis C.S. Afonso and Clayton R. Pereira and Silke A.T. Weber and Christian Hook and Alexandre X. Falcão and João P. Papa},
keywords = {Parkinson’s disease, Optimum-path forest, Handwriting dynamics, Hierarchical representation},
abstract = {Bag-of-Visual Words (BoVW) and deep learning techniques have been widely used in several domains, which include computer-assisted medical diagnoses. In this work, we are interested in developing tools for the automatic identification of Parkinson’s disease using machine learning and the concept of BoVW. The proposed approach concerns a hierarchical-based learning technique to design visual dictionaries through the Deep Optimum-Path Forest classifier. The proposed method was evaluated in six datasets derived from data collected from individuals when performing handwriting exams. Experimental results showed the potential of the technique, with robust achievements.}
}
@article{MOREIRA2020102771,
title = {Video action recognition based on visual rhythm representation},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102771},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102771},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300213},
author = {Thierry Pinheiro Moreira and David Menotti and Helio Pedrini},
keywords = {Action recognition, Visual rhythm, Video sequences, Computer vision},
abstract = {Advances in video acquisition and storage technologies have promoted a great demand for automatic recognition of actions. The use of cameras for security and surveillance purposes has applications in several scenarios, such as airports, parks, banks, stations, roads, hospitals, supermarkets, industries, stadiums, schools. An inherent difficulty of the problem is the complexity of the scene under usual recording conditions, which may contain complex background and motion, multiple people on the scene, interactions with other actors or objects, and camera motion. Most recent databases are built primarily with shared recordings on YouTube and with snippets of movies, situations where these obstacles are not restricted. Another difficulty is the impact of the temporal dimension since it expands the size of the data, increasing computational cost and storage space. In this work, we present a methodology of volume description using the Visual Rhythm (VR) representation. This technique reshapes the original volume of the video into an image, where two-dimensional descriptors are computed. We investigated different strategies for constructing the representation by combining configurations in several image domains and traversing directions of the video frames. From this, we propose two feature extraction methods, Naïve Visual Rhythm (Naïve VR) and Visual Rhythm Trajectory Descriptor (VRTD). The first approach is the straightforward application of the technique in the original video volume, forming a holistic descriptor that considers action events as patterns and formats in the visual rhythm image. The second variation focuses on the analysis of small neighborhoods obtained from the process of dense trajectories, which allows the algorithm to capture details unnoticed by the global description. We tested our methods in eight public databases, one of hand gestures (SKIG), two in first person (DogCentric and JPL), and five in third person (Weizmann, KTH, MuHAVi, UCF11 and HMDB51). The results show that the developed techniques are able to extract motion elements along with format and appearance information, achieving competitive accuracy rates compared to state-of-the-art action recognition approaches.}
}
@article{TARIQ2020102766,
title = {Pure intra mode decision in HEVC using optimized firefly algorithm},
journal = {Journal of Visual Communication and Image Representation},
volume = {68},
pages = {102766},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102766},
url = {https://www.sciencedirect.com/science/article/pii/S104732032030016X},
author = {Junaid Tariq and Ammar Armghan and Amir Ijaz and Imran Ashraf},
keywords = {HEVC, Intra mode, Firefly algorithm, Video coding, Most probable mode},
abstract = {High Efficiency Video Coding (HEVC) is the latest encoder that increased the intra modes from 9 to 35 to efficiently handle the contents of the video. The HEVC’s test model (HM) selects the optimal intra mode using the brute force method which increases the complexity of HEVC. This work, firstly, investigates the feasibility of firefly algorithm (FFA) due to its exploration and exploitation characteristics to expedite the intra mode decision in HEVC. Secondly, a novel objective function is formulated for FFA to efficiently compute the brightness for intra modes in FFA. Thirdly, the parameters of FFA are made dynamic to adjust according to the contents of video sequences. Simulation results demonstrate that the nature inspired algorithm, FFA, pays off by saving a minimum of 27% of the total coding time on average and doesn’t sacrifice quality by limiting Bjontegaard delta bit rate (BD-BR) increase to only 0.98% on average.}
}
@article{GUO2020102851,
title = {NERNet: Noise estimation and removal network for image denoising},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102851},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102851},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301024},
author = {Bingyang Guo and Kechen Song and Hongwen Dong and Yunhui Yan and Zhibiao Tu and Liu Zhu},
keywords = {Image denoising, Convolutional neural networks, Attention mechanism, Dilated convolution, Dilation rate selecting},
abstract = {While some denoising methods based on deep learning achieve superior results on synthetic noise, they are far from dealing with photographs corrupted by realistic noise. Denoising on real-world noisy images faces more significant challenges due to the source of it is more complicated than synthetic noise. To address this issue, we propose a novel network including noise estimation module and removal module (NERNet). The noise estimation module automatically estimates the noise level map corresponding to the information extracted by symmetric dilated block and pyramid feature fusion block. The removal module focuses on removing the noise from the noisy input with the help of the estimated noise level map. Dilation selective block with attention mechanism in the removal module adaptively not only fuses features from convolution layers with different dilation rates, but also aggregates the global and local information, which is benefit to preserving more details and textures. Experiments on two datasets of synthetic noise and three datasets of realistic noise show that NERNet achieves competitive results in comparison with other state-of-the-art methods.}
}
@article{BAI2020102835,
title = {Cross-domain representation learning by domain-migration generative adversarial network for sketch based image retrieval},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102835},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102835},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300857},
author = {Cong Bai and Jian Chen and Qing Ma and Pengyi Hao and Shengyong Chen},
keywords = {Sketch based image retrieval, Cross-domain learning, Generative adversarial learning, Similarity learning},
abstract = {Sketch based image retrieval (SBIR), which uses free-hand sketches to search the images containing similar objects/scenes, is attracting more and more attentions as sketches could be got more easily with the development of touch devices. However, this task is difficult as the huge differences between sketches and images. In this paper, we propose a cross-domain representation learning framework to reduce these differences for SBIR. This framework aims to transfer sketches to images with the information learned both in the sketch domain and image domain by the proposed domain migration generative adversarial network (DMGAN). Furthermore, to reduce the representation gap between the generated images and natural images, a similarity learning network (SLN) is also proposed with the new designed loss function incorporating semantic information. Extensive experiments have been done from different aspects, including comparison with state-of-the-art methods. The results show that the proposed DMGAN and SLN really work for SBIR.}
}
@article{SU2020102811,
title = {Neural machine translation with Gumbel Tree-LSTM based encoder},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102811},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102811},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300614},
author = {Chao Su and Heyan Huang and Shumin Shi and Ping Jian and Xuewen Shi},
keywords = {Neural machine translation, Tree to sequence, Gumbel Tree-LSTM},
abstract = {Neural machine translation has improved the translation accuracy greatly and received great attention of the machine translation community. Tree-based translation models aim to model the syntactic or semantic relation among long-distance words or phrases in a sentence. However, it faces the difficulties of expensive manual annotation cost and poor automatic annotation accuracy. In this paper, we focus on how to encode a source sentence into a vector in a unsupervised-tree way and then decode it into a target sentence. Our model incorporates Gumbel Tree-LSTM, which can learn how to compose tree structures from plain text without any tree annotation. We evaluate the proposed model on both spoken and news corpora, and show that the performance of our proposed model outperforms the attentional seq2seq model and the Transformer base model.}
}
@article{AMARANAGESWARAO2020102819,
title = {Wavelet based medical image super resolution using cross connected residual-in-dense grouped convolutional neural network},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102819},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102819},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300699},
author = {Gadipudi Amaranageswarao and S. Deivalakshmi and Seok-Bum Ko},
keywords = {Grouped convolution, Low-dose X-ray CT, Residual-in-dense, Super resolution, Wavelet sub-bands},
abstract = {In clinical analysis and diagnosis, high resolution (HR) computed tomography (CT) images are required for proper treatment of a patient. Developing HR medical images by X-ray CT devices require extended radiation exposure with large radiative dosages, putting the patient at potential risk of inducing cancer. So, radiation exposure should be reduced. However, photon starvation and beam hardening in low-dose X-rays will cause severe artifacts. Thus, an accurate reconstruction of low-dose X-ray CT images is required. To this end, we propose a wavelet based multi-channel and multi-scale cross connected residual-in-dense grouped convolutional neural network (WCRDGCNN) for accurate super resolution (SR) of medical images. The adopted filter groups reduce the connection weights, thereby reducing the computational complexity. Gradient vanishing problem is tackled by using residual and dense skip connections. The extensive experimentation results on benchmark datasets show that our method outperforms the state-of-the-art SR methods.}
}
@article{LI2020102901,
title = {Real-time sepsis severity prediction on knowledge graph deep learning networks for the intensive care unit},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102901},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102901},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301425},
author = {Qing Li and Lili Li and Jiang Zhong and L. Frank Huang},
keywords = {Deep neural networks, Sepsis, Intensive care units, Clinical informatics, Illness severity prediction, Knowledge graph},
abstract = {Sepsis is the third-highest mortality disease in intensive care units (ICUs). In this paper, we proposed a deep learning model for predicting the severity of sepsis patients. Most existing models based on attention mechanisms do not fully utilize knowledge graph based information for different organ systems, such that might constitute crucial features for predicting the severity of sepsis patients. Therefore, we have employed a medical knowledge graph as a reliable and robust source of side information. End-to-end neural networks that incorporate analyses of various organ systems simultaneously and intuitively were developed in the proposed model to reflect upon the condition of patients in a timely fashion. We have developed a pre-training technique in the proposed model to combine it with labeled data by multi-task learning. Experimental results on real-world clinical datasets, MIMIC-III and eIR, demonstrate that our model outperforms state-of-the-art models in predicting the severity of sepsis patients.}
}
@article{XIAO2020102918,
title = {OSED: Object-specific edge detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102918},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102918},
url = {https://www.sciencedirect.com/science/article/pii/S104732032030153X},
author = {Ling Xiao and Bo Wu and Youmin Hu},
keywords = {Region proposal, Edge detection, Deep supervision, Convolutional neural network},
abstract = {Object-specific edge detection (OSED) aims to detect object edges in an image along with classify the edge into object or non-object. It prunes edges which are not belonging to the object class for following processing, such as, feature matching for object detection, localization and three-dimensional reconstruction. In this paper, an OSED method that combines region proposal detectors with deep supervision nets to identify object-specific edges is proposed. It minimizes errors of object proposal by learning from hidden layers. Additionally, it combines features from different scales to detect object edges. In order to evaluate the performance of the OSED, we present two datasets which are captured in real scenes. The OSED method demonstrates a high accuracy of 90% and a high speed of 0.5 s for an image whose size is 512 × 448 pixels on the proposed datasets.}
}
@article{YANG2020102822,
title = {An embedding strategy on fusing multiple image features for data hiding in multiple images},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102822},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102822},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300729},
author = {Junxue Yang and Xin Liao},
keywords = {Multiple images steganography, Embedding strategy, Multiple image features, Image complexity, Steganographic capacity},
abstract = {Data hiding in multiple images has been a significant research direction in information security. How to reasonably design the embedding strategy to spread the payload among multiple images is still an open issue. In this paper, we propose an embedding strategy on fusing multiple features. We utilize the typical characteristic parameters of gray level co-occurrence matrix, the image entropy and the shape parameter to describe image complexity. Furthermore, we combine with the number of cover images, the number of cover images assigned to steganographer and the size of cover image to estimate the steganographic capacity of each image. The strategy is implemented together with some state-of-the-art single image steganographic algorithms. Experimental results demonstrate that the security performance of the proposed strategy is higher than that of the state-of-the-art embedding strategy against the blind universal pooled steganalysis.}
}
@article{2022103493,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {84},
pages = {103493},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(22)00046-3},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322000463}
}
@article{CHENG2020102863,
title = {Low-resource automatic cartoon image creation from limited samples},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102863},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102863},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301140},
author = {Hsu-Yung Cheng and Chih-Chang Yu},
keywords = {Image creation, Clustering, Convolutional neural networks},
abstract = {In this work, a framework that can automatically create cartoon images with low computation resources and small training datasets is proposed. The proposed system performs region segmentation and learns a region relationship tree from each learning image. The segmented regions are clustered automatically with an enhanced clustering mechanism with no prior knowledge of number of clusters. According to the topology represented by region relationship tree and clustering results, the regions are reassembled to create new images. A swarm intelligence optimization procedure is designed to coordinate the regions to the optimized sizes and positions in the created image. Rigid deformation using moving least squares is performed on the regions to generate more variety for created images. Compared with methods based on Generative Adversarial Networks, the proposed framework can create better images with limited computation resources and a very small amount of training samples.}
}
@article{LIAO2020102896,
title = {Real-time long-term tracker with tracking–verification–detection–refinement},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102896},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102896},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301383},
author = {Jiawen Liao and Chun Qi and Jianzhong Cao and Long Ren and Gaopeng Zhang},
keywords = {Correlation filter, Occlusion, Long-term, Detector, Real-time},
abstract = {Long-term tracking is one of the most challenging problems in computer vision. In this paper, we make full use of the Discriminative Correlation Filter (DCF), and propose a real-time long-term tracker by exploiting a joint tracking–verification–detection–refinement framework. We utilize a DCF which is updated aggressively to estimate translation and scale variation of the target. Subsequently, a passively updated DCF checks the reliability of the tracking result. Once the result is not reliable, we evoke the proposed optimized candidate detector to generate a small number of relatively high quality candidates. Finally, one DCF with an adaptive online learning rate is adopted to refine the predictions that the sparse candidates inferred. In addition, we employ a selection mechanism for the correlation responses to maintain reliable samples effectively. Extensive experiments show that the proposed method performs favorably against lots of state-of-the-art methods while running more than 30 frames per second on single CPU.}
}
@article{LIU2020102767,
title = {A real time expert system for anomaly detection of aerators based on computer vision and surveillance cameras},
journal = {Journal of Visual Communication and Image Representation},
volume = {68},
pages = {102767},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102767},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300171},
author = {Yeqi Liu and Huihui Yu and Chuanyang Gong and Yingyi Chen},
keywords = {Computer vision, Surveillance camera, Anomaly detection, Optical flow, Object region detection, Application},
abstract = {Aerators are essential and crucial auxiliary devices in intensive culture, especially in industrial culture in China. In this paper, we propose a real-time expert system for anomaly detection of aerators based on computer vision technology and existing surveillance cameras. The expert system includes two modules, i.e., object region detection and working state detection. First, we present a small object region detection method based on the region proposal idea. Moreover, we propose a novel algorithm called reference frame Kanade-Lucas-Tomasi (RF-KLT) algorithm for motion feature extraction in fixed regions. Then, we describe a dimension reduction method of time series for establishing a feature dataset with obvious boundaries between classes. Finally, we use machine learning algorithms to build the feature classifier. The proposed expert system can realize real-time, robust and cost-free anomaly detection of aerators in both the actual video dataset and the augmented video dataset. Demo is available at https://youtu.be/xThHRwu_cnI.}
}
@article{SUN2020102762,
title = {Local relation network with multilevel attention for visual question answering},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102762},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102762},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300122},
author = {Bo Sun and Zeng Yao and Yinghui Zhang and Lejun Yu},
keywords = {Visual question answering, Relation network, Attention mechanism},
abstract = {With the tremendous success of the visual question answering (VQA) tasks, visual attention mechanisms have become an indispensable part of VQA models. However, these attention-based methods do not consider any relationship among regions, which is crucial for the thorough understanding of the image by the model. We propose local relation networks for generating context-aware image features for each image region, which contain information on the relationship among the other image regions. Furthermore, we propose a multilevel attention mechanism to combine semantic information from the LRNs and the original image regions, rendering the decision of the model more reasonable. With these two measures, we improve the region representation and achieve better attentive effect and VQA performance. We conduct numerous experiments on the COCO-QA dataset and the largest VQA v2.0 benchmark dataset. Our model achieves competitive results, proving the effectiveness of our proposed LRNs and multilevel attention mechanism through visual demonstrations.}
}
@article{CHANDRASEKAR2020102905,
title = {Multiple objects tracking by a highly decisive three-frame differencing-combined-background subtraction method with GMPFM-GMPHD filters and VGG16-LSTM classifier},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102905},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102905},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301450},
author = {K. Silpaja Chandrasekar and P. Geetha},
keywords = {Tracking, Vehicles, Pedestrians, Gaussian mixture particle filter model, VGG16 and LSTM},
abstract = {Tracking of moving vehicles and pedestrians is the most important application in traffic surveillance videos. This study develops a highly efficient and fast multi-object tracking method using three-frame differencing-combined-background subtraction (TFDCBS)-coupled-automatic and fast histogram-entropy-based thresholding (HEBT) method together with GMPFM-GMPHD filters and VGG16-LSTM classifier. Here TFDCBS-HEBT methods identify the targeted objects with enclosed 3D bounding boxes and extracts multiple features from the raw images. Maximum number of error-free extracted multiple features (key points, multiple local convolutions, corners, and descriptors) are processed subsequently for object tracking by GMPFM-GMPHD Filters and an upgraded VGG16- LSTM classifier. The proposed method has been validated on KITTI 3D bounding box-dataset and its performance compared with three state-of-the-art tracking methods. Highest values of several performance parameters and the lowest computation time clearly demonstrate the promising feature of our new method for its application towards a fast and effective multi-target tracking of moving objects.}
}
@article{LYU2020102797,
title = {No-reference quality metric for contrast-distorted image based on gradient domain and HSV space},
journal = {Journal of Visual Communication and Image Representation},
volume = {69},
pages = {102797},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102797},
url = {https://www.sciencedirect.com/science/article/pii/S104732032030047X},
author = {Wenjing Lyu and Wei Lu and Ming Ma},
keywords = {Digital image forensics, No-reference image quality assessment, Contrast distortion, Gradient domain, HSV color space},
abstract = {Image quality assessment (IQA) plays an important role in digital image forensics. Due to the occurrence of contrast distortion during image acquisition and manipulation, IQA for contrast is a major issue. And it is vital for benchmarking and optimizing the image tampering detection and contrast-enhancement algorithms. In this paper, a new no-reference/blind image quality assessment (IQA) metric is proposed for evaluating image contrast. This research seeks for the inter-relationship between contrast distortion and visual perception quality. The comprehensive quality metric is obtained by combining local binary pattern (LBP) descriptor on gradient domain with color moment on HSV color space. And a prediction model is trained with support vector regression (SVR). Extensive analysis and cross validation are performed on four contrast relevant image databases, which validates the superiority of our proposed blind technique over state-of-the-art no-reference IQA methods.}
}
@article{WANG2020102867,
title = {Structured feature sparsity training for convolutional neural network compression},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102867},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102867},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301176},
author = {Wei Wang and Liqiang Zhu},
keywords = {Convolutional neural network, CNN compression, Structured sparsity, Pruning criterion},
abstract = {Convolutional neural networks (CNNs) with large model size and computing operations are difficult to be deployed on embedded systems, such as smartphones or AI cameras. In this paper, we propose a novel structured pruning method, termed the structured feature sparsity training (SFST), to speed up the inference process and reduce the memory usage of CNNs. Unlike other existing pruning methods, which require multiple iterations of pruning and retraining to ensure stable performance, SFST only needs to fine-tune the pretrained model with additional regularization on the less important features and then prune them, no multiple pruning and retraining needed. SFST can be deployed to a variety of modern CNN architectures including VGGNet, ResNet and MobileNetv2. Experimental results on CIFAR, SVHN, ImageNet and MSTAR benchmark dataset demonstrate the effectiveness of our scheme, which achieves superior performance over the state-of-the-art methods.}
}
@article{LI2020102774,
title = {Detail retaining convolutional neural network for image denoising},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102774},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102774},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300249},
author = {Xiaoxia Li and Juan Xiao and Yingyue Zhou and Yuanzheng Ye and Nianzu Lv and Xueyuan Wang and Shunli Wang and ShaoBing Gao},
keywords = {Image denoising, Convolutional neural network, Detail retaining, Image restoration, Gaussian denoising},
abstract = {Compared with the traditional image denoising method, although the convolutional neural network (CNN) has better denoising performance, there is an important issue that has not been well resolved: the residual image obtained by learning the difference between noisy image and clean image pairs contains abundant image detail information, resulting in the serious loss of detail in the denoised image. In this paper, in order to relearn the lost image detail information, a mathematical model is deducted from a minimization problem and an end-to-end detail retaining CNN (DRCNN) is proposed. Unlike most denoising methods based on CNN, DRCNN is not only focus to image denoising, but also the integrity of high frequency image content. DRCNN needs less parameters and storage space, therefore it has better generalization ability. Moreover, DRCNN can also adapt to different image restoration tasks such as blind image denoising, single image superresolution (SISR), blind deburring and image inpainting. Extensive experiments show that DRCNN has a better effect than some classic and novel methods.}
}
@article{WU2020102802,
title = {Hand pose estimation in object-interaction based on deep learning for virtual reality applications},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102802},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102802},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300523},
author = {Min-Yu Wu and Pai-Wen Ting and Ya-Hui Tang and En-Te Chou and Li-Chen Fu},
keywords = {Hand pose estimation, Deep learning, Convolutional neural network, Spherical part model},
abstract = {Hand Pose Estimation aims to predict the position of joints on a hand from an image, and it has become popular because of the emergence of VR/AR/MR technology. Nevertheless, an issue surfaces when trying to achieve this goal, since a hand tends to cause self-occlusion or external occlusion easily as it interacts with external objects. As a result, there have been many projects dedicated to this field for a better solution of this problem. This paper develops a system that accurately estimates a hand pose in 3D space using depth images for VR applications. We propose a data-driven approach of training a deep learning model for hand pose estimation with object interaction. In the convolutional neural network (CNN) training procedure, we design a skeleton-difference loss function, which effectively can learn the physical constraints of a hand. Also, we propose an object-manipulating loss function, which considers knowledge of the hand-object interaction, to enhance performance. In the experiments we have conducted for hand pose estimation under different conditions, the results validate the robustness and the performance of our system and show that our method is able to predict the joints more accurately in challenging environmental settings. Such appealing results may be attributed to the consideration of the physical joint relationship as well as object information, which in turn can be applied to future VR/AR/MR systems for more natural experience.}
}
@article{LEI2020102872,
title = {Novel shrinking residual convolutional neural network for efficient accurate stereo matching},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102872},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102872},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301218},
author = {Junfeng Lei and Yuxuan Dong and Tao Zhao and Jinsheng xiao and Yunhua Chen and Bijun Li},
keywords = {Stereo matching, Matching cost, Residual convolutional neural network},
abstract = {For stereo matching based on patch comparing using convolutional neural networks (CNNs), the matching cost estimation is highly dependent on the network structure, and the patch comparing is time consuming for traditional CNNs. Accordingly, we propose a stereo matching method based on a novel shrinking residual CNN, which consists of convolutional layers and skip-connection layers, and the size of the fully connected layers decreases progressively. Firstly, a layer-by-layer shrinking size model is adopted for the full-connection layers to greatly increase the running speed. Secondly, the convolutional layer and the residual structure are fused to improve patch comparing. Finally, the Loss function is re-designed to give higher weights to hard-classified examples compared with the standard cross entropy loss. Experimental results on KITTI2012 and KITTI2015 demonstrate that the proposed method can improve the operation speed while maintaining high accuracy.}
}
@article{LI2020102790,
title = {Learning latent geometric consistency for 6D object pose estimation in heavily cluttered scenes},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102790},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102790},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300407},
author = {Qingnan Li and Ruimin Hu and Jing Xiao and Zhongyuan Wang and Yu Chen},
keywords = {Geometric consistency, Geometric reasoning, Pose estimation, Convolutional neural networks},
abstract = {6D object pose (3D rotation and translation) estimation from RGB-D image is an important and challenging task in computer vision and has been widely applied in a variety of applications such as robotic manipulation, autonomous driving, augmented reality etc. Prior works extract global feature or reason about local appearance from an individual frame, which neglect the spatial geometric relevance between two frames, limiting their performance for occluded or truncated objects in heavily cluttered scenes. In this paper, we present a dual-stream network for estimating 6D pose of a set of known objects from RGB-D images. Our novelty stands in contrast to prior work that learns latent geometric consistency in pairwise dense feature representations from multiple observations of the same objects in a self-supervised manner. We show in experiments that our method outperforms state-of-the-art approaches on 6D object pose estimation in two challenging datasets, YCB-Video and LineMOD.}
}
@article{TSAI2020102798,
title = {An effective hybrid pruning architecture of dynamic convolution for surveillance videos},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102798},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102798},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300481},
author = {Chun-Ya Tsai and De-Qin Gao and Shanq-Jang Ruan},
keywords = {Optimize CNN, Dynamic convolution, Pruning, Smart surveillance application},
abstract = {The large-scale surveillance videos analysis becomes important as the development of the intelligent city; however, the heavy computational resources necessary for the state-of-the-art deep learning model makes real-time processing hard to be implemented. As the characteristic of high scene similarity generally existing in surveillance videos, we propose an effective compression architecture called dynamic convolution, which can reuse the previous feature maps to reduce the calculation amount; and combine with filter pruning to further speed up the performance. In this paper, we tested the presented method on 45 surveillance videos with various scenes. The experimental results show that the hybrid pruning architecture can reduce up to 80.4% of FLOPs while preserving the precision within 1.34% mAP; furthermore, the method can improve the processing speed up to 2.8 times compared to the traditional Single Shot MultiBox Detection.}
}
@article{ZHAO2021102921,
title = {Gradient-based conditional generative adversarial network for non-uniform blind deblurring via DenseResNet},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102921},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102921},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301565},
author = {Hongtian Zhao and Di Wu and Hang Su and Shibao Zheng and Jie Chen},
keywords = {Image deblurring, Conditional GAN, DenseResNet, -based gradient loss},
abstract = {Blind image deblurring aims to recover the sharp image from a blurry image. The problem is seriously ill-conditioned and many existing algorithms based on kernel estimation require heuristic parameter adjustments and high computational cost, and cannot perform well on non-uniform motion blurs. To address this issue, image deblurring is viewed as an image translation problem in this paper. The authors solve it based on a conditional generative adversarial network (GAN), where the sharp image is restored by an end-to-end trainable neural network. Different from the generative network in basic conditional GAN, the proposed generator is based on dense blocks and residual network (DenseResNet), aiming to mitigate the problems of overfitting and vanishing gradient, and strengthen the blur feature propagation. To generate clear structure, the basic conditional GAN formulation is further revised by introducing joint VGG features and L1-based gradient loss. Extensive experimental results demonstrate the superior performance of the proposed method.}
}
@article{WU2020102761,
title = {Saliency detection using adversarial learning networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {67},
pages = {102761},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102761},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300110},
author = {Yong Wu and Zhi Liu and Xiaofei Zhou},
keywords = {Adversarial learning, Generator, discriminator, Saliency detection, Feedback information},
abstract = {This paper proposes a novel model for saliency detection using the adversarial learning networks, in which the generator is used to generate the saliency map and the discriminator is deployed to guide the training process of overall network. Concretely, the training procedure of our model consists of three steps including the training of generator, the training of discriminator, and the training throughout the overall network. The key point of training process lies in the discriminator, which is designed to provide the feedback information for the acceleration of the generator and the refinement of saliency map. Therefore, during the training stage of overall network, the output of the generator, i.e. the coarse saliency map, is fed into the discriminator, yielding the corresponding feedback information. Following this way, we can obtain the final generator with a higher performance. For testing, the obtained generator is employed to perform saliency detection. Extensive experiments on four challenging saliency detection datasets show that our model not only achieves the favorable performance against the state-of-the-art saliency models, but also possesses the faster convergence speed when training the proposed model.}
}
@article{2022103474,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103474},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(22)00030-X},
url = {https://www.sciencedirect.com/science/article/pii/S104732032200030X}
}
@article{VERMA2020102866,
title = {Three stage deep network for 3D human pose reconstruction by exploiting spatial and temporal data via its 2D pose},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102866},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102866},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301164},
author = {Pratishtha Verma and Rajeev Srivastava},
keywords = {Human Pose Estimation (HPE), Human Pose Reconstruction (HPR), Frame Specific Pose Estimation (FSPE), Multi-Stage Cascaded Feature Connection (MSCFC), Feature Residual Connection (FRC)},
abstract = {3D Human Pose Reconstruction (HPR) is a challenging task due to less availability of 3D ground truth data and projection ambiguity. To address these limitations, we propose a three-stage deep network having the workflow of 2D Human Pose Estimation (HPE) followed by 3D HPR; which utilizes the proposed Frame Specific Pose Estimation (FSPE), Multi-Stage Cascaded Feature Connection (MSCFC) and Feature Residual Connection (FRC) Sub-level Strategies. In the first stage, the FSPE concept with the MSCFC strategy has been used for 2D HPE. In the second stage, the basic deep learning concepts like convolution, batch normalization, ReLU, and dropout have been utilized with the FRC Strategy for spatial 3D reconstruction. In the last stage, LSTM deep architecture has been used for temporal refinement. The effectiveness of the technique has been demonstrated on MPII, Human3.6M, and HumanEva-I datasets. From the experiments, it has been observed that the proposed method gives competitive results to the recent state-of-the-art techniques.}
}
@article{LIE2020102801,
title = {Semi-automatic 2D-to-3D video conversion based on background sprite generation},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102801},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102801},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300511},
author = {Wen-Nung Lie and Shao-Ting Chiu and Yi-Kai Chen and Jui-Chiu Chiang},
keywords = {Stereo video conversion, Depth propagation, Graph cut, Background sprite model},
abstract = {This paper presents a technique for semi-automatic 2D-to-3D stereo video conversion, which is known to provide user intervention in assigning foreground/background depths for key frames and then get depth maps for non-key frames via automatic depth propagation. Our algorithm treats foreground and background separately. For foregrounds, kernel pixels are identified and then used as the seeds for graph-cut segmentation for each non-key frame independently, resulting in results not limited by objects’ motion activity. For backgrounds, all video frames, after foregrounds being removed, are integrated into a common background sprite model (BSM) based on a relay-frame-based image registration algorithm. Users can then draw background depths for BSM in an integrated manner, thus reducing human efforts significantly. Experimental results show that our method is capable of retaining more faithful foreground depth boundaries (by 1.6–2.7 dB) and smoother background depths than prior works. This advantage is helpful for 3D display and 3D perception.}
}
@article{WANG2020102929,
title = {Multiple depth-levels features fusion enhanced network for action recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102929},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102929},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301607},
author = {Shengquan Wang and Jun Kong and Min Jiang and Tianshan Liu},
keywords = {Action recognition, Two-stream, Multiple depth-levels features fusion, Group-wise spatial-channel enhance},
abstract = {As a challenging task of video classification, action recognition has become a significant topic of computer vision community. The most popular methods based on two-stream architecture up to now are still simply fusing the prediction scores of each stream. In that case, the complementary characteristics of two streams cannot be fully utilized and the effect of shallower features is often overlooked. In addition, the equal treatment to features may weaken the role of the feature contributing significantly to the classification. Accordingly, a novel network called Multiple Depth-levels Features Fusion Enhanced Network (MDFFEN) is proposed. It improves on two aspects of two-stream architecture. In terms of the two-stream interaction mechanism, multiple depth-levels features fusion (MDFF) is formed to aggregate spatial–temporal features extracted from several sub-modules of original two streams by spatial–temporal features fusion (STFF). And with respect to further refining the spatiotemporal features, we propose a group-wise spatial-channel enhance (GSCE) module to highlight the meaningful regions and expressive channels automatically by priority assignment. The competitive results are achieved after we validate MDFFEN on three public challenging action recognition datasets, HDMB51, UCF101 and ChaLearn LAP IsoGD.}
}
@article{SILVA2020102773,
title = {Real-time license plate detection and recognition using deep convolutional neural networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102773},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102773},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300237},
author = {Sergio Montazzolli Silva and Claudio Rosito Jung},
keywords = {Convolutional neural networks, License plate, Deep learning},
abstract = {Automatic License Plate Recognition (ALPR) is an important task with many applications in Intelligent Transportation and Surveillance systems. This work presents an end-to-end ALPR method based on a hierarchical Convolutional Neural Network (CNN). The core idea of the proposed method is to identify the vehicle and the license plate region using two passes on the same CNN, and then to recognize the characters using a second CNN. The recognition CNN massively explores the use of synthetic and augmented data to cope with limited training datasets, and our results show that the augmentation process significantly increases the recognition rate. In addition, we present a novel temporal coherence technique to better stabilize the OCR output in videos. Our method was tested with publicly available datasets containing Brazilian and European license plates, achieving accuracy rates better than competitive academic methods and a commercial system.}
}
@article{DAI2020102800,
title = {End-to-end DeepNCC framework for robust visual tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102800},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102800},
url = {https://www.sciencedirect.com/science/article/pii/S104732032030050X},
author = {Kaiheng Dai and Yuehuan Wang},
keywords = {Visual tracking, End-to-end framework, NCC, Template matching},
abstract = {In this paper, we propose an NCC-based object tracking deep framework, which can be well initialized with the limited target samples in the first frame. The proposed framework contains a pretrained model, online feature fine-tuning layers and tracking processes. The pretrained model provides rich feature representations while online feature fine-tuning layers select discriminative and generic features for the tracked object. We choose normalized cross-correlation as a template tracking layer to perform the tracking process. To enable the learned features representation closely coordinated to the tracked target, we jointly train the feature representation network and tracking processes. In online tracking, an adaptive template and a fixed template are fused to find the optimal tracking results. Scale estimation and a high-confidence model update scheme are perfectly integrated into the framework to adapt to the target appearance changes. The extensive experiments demonstrate that the proposed tracker achieves superior performance compared with other state-of-the-art trackers.}
}
@article{ZHANG2020102870,
title = {Rate-distortion-complexity optimization for x265},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102870},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102870},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301206},
author = {Saiping Zhang and Fuzheng Yang and Shuai Wan},
keywords = {Rate-distortion-complexity optimization (RDCO), X265, High Efficiency Video Coding (HEVC), Computational complexity allocation},
abstract = {Rate-distortion optimization (RDO) is conventionally based on the analysis of rate-distortion (R-D) curve to minimize the coding distortion under the coding bits constraint. However, it is necessary to consider the computational complexity in the RDO process. In this paper, we obtain the Confidence LEvel - Computational complexity (CLEC) curves which indicate the characteristics of coding tree units (CTUs). Based on the CLEC curves, a rate-distortion-complexity optimization (RDCO) algorithm is proposed to optimize R-D under given computational complexity and achieve the optimal coding performance for x265. Experimental results demonstrate that the proposed algorithm can achieve a wide range of encoding speed under a given quantization parameter (QP) whereas the original x265 can only achieve a few fixed encoding speeds, and the proposed algorithm can reduce the BD-rate and increase the BD-PSNR by 6.59% and 0.13 dB on average under the same requirements of encoding speeds as the original x265.}
}
@article{ZHENG2020102848,
title = {No-reference stereoscopic images quality assessment method based on monocular superpixel visual features and binocular visual features☆},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102848},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102848},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300997},
author = {Zhi Zheng and Yun Liu and Yun Liu and Baoqing Huang and Hongwei Yu},
keywords = {Image quality evaluation, Superpixel visual patches, Human visual system, Support vector regression},
abstract = {No-reference quality assessment of images has received considerable attention. However, the accuracy of such assessment remains questionable because of its weak biological basis. In this paper, we propose a novel quality assessment model based on the superpixel index and biological binocular mechanisms. The technical contributions of our model are the introduction of local monocular superpixel features and three global binocular visual features. We utilize monocular superpixel segmentation to extract two types of entropies as the local visual features for accurate quality-aware feature extraction. In addition, natural scene statistics features are extracted from the binocular visual information to complement the local monocular features and quantify the naturalness of the stereoscopic images. Finally, a regression model is learned to evaluate the quality of the stereoscopic images. Experimental results from three popular databases demonstrate that the proposed model has a more reliable performance than earlier models in terms of prediction accuracy and generalizability.}
}
@article{KIM2020102903,
title = {Deep gradual flash fusion for low-light enhancement},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102903},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102903},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301449},
author = {Jae-Woo Kim and Je-Ho Ryu and Jong-Ok Kim},
keywords = {Image fusion, Flash fusion, Pseudo multi-exposure, Auto-encoder, GAN, Low light enhancement},
abstract = {In this paper, we propose gradual flash fusion, a new imaging concept that enables acquisition of pseudo multi-exposure images in a passive manner. This means that our gradual flash capture does not require any user-side manipulation (taking multiple shots or varying camera settings). Continuous high-speed capture naturally contains different intensities of flash in a single shooting. The captured gradual flash images, containing different information of the same scene, are fused to generate higher-quality images, especially in a low light scenario. For gradual flash fusion, we use a Generative Adversarial Network (GAN) based approach, where the generator is a tailored convolutional Auto-Encoder for image fusion. For the training, we build a custom dataset comprising gradual flash images and corresponding ground truths. This enables supervised learning, unlike most conventional image fusion studies. Experimental results demonstrate that gradual flash fusion achieves artifact-free and noise-free results resembling ground truth, owing to supervised adversarial fusion.}
}
@article{PAL2020102813,
title = {Low overhead spatiotemporal video compression over smartphone based Delay Tolerant Network},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102813},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102813},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300638},
author = {Tamal Pal and Sipra Das Bit},
keywords = {Colour filter array, Sobel filter, Inter-frame compression, Intra-frame compression, Delay tolerant network, Wi-fi direct},
abstract = {In this paper we provide a lightweight video compression scheme, Low Overhead Spatio-Temporal Video compression (Lost-Vision) scheme which is done through inter-frame and intra-frame compression. In inter-frame compression redundant frames are removed by a proposed interpolation search-based method and a lightweight edge detection technique. Then intra-frame compression is done by a proposed adaptive column dropping technique modifying an existing technique namely ICCD. At the receiver end, we propose two reconstruction filters targeting to improve reconstruction quality. Performance of our scheme in terms of energy efficiency and reconstruction quality is evaluated both theoretically and practically. In practical implementation, the proposed video compression scheme is assessed in a real environment with different terrains using a smartphones/tablet-based DTN-like network. A Comparison of our scheme with three recent works on video compression shows our scheme's dominance over the competing works with 52%, 45.6% and 53% energy in saving yet maintaining acceptable reconstruction quality.}
}
@article{SHOKRI2020102769,
title = {Salient object detection in video using deep non-local neural networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {68},
pages = {102769},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102769},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300195},
author = {Mohammad Shokri and Ahad Harati and Kimya Taba},
keywords = {Video saliency detection, Deep learning, Non-local neural networks, Fully convolutional neural networks},
abstract = {Detection of salient objects in image and video is of great importance in many computer vision applications. In spite of the fact that the state of the art in saliency detection for still images has been changed substantially over the last few years, there have been few improvements in video saliency detection. This paper proposes a novel non-local fully convolutional network architecture for capturing global dependencies more efficiently and investigates the use of recently introduced non-local neural networks in video salient object detection. The effect of non-local operations is studied separately on static and dynamic saliency detection in order to exploit both appearance and motion features. A novel deep non-local fully convolutional network architecture is introduced for video salient object detection and tested on two well-known datasets DAVIS and FBMS. The experimental results show that the proposed algorithm outperforms state-of-the-art video saliency detection methods.}
}