@article{LI2021103255,
title = {Stereoscopic image quality assessment considering visual mechanism and multi-loss constraints},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103255},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103255},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001668},
author = {Sumei Li and Yueyang Li and Yongtian Han},
keywords = {Image quality assessment, Binocular information, Multi-loss, Proxy label},
abstract = {In this paper, a convolutional neural network (CNN) with multi-loss constraints is designed for stereoscopic image quality assessment (SIQA). A stereoscopic image not only contains monocular information, but also provides binocular information which is as identically crucial as the former. So we take the image patches of left-view images, right-view images and the difference images as the inputs of the network to utilize monocular information and binocular information. Moreover, we propose a method to obtain proxy label of each image patch. It preserves the quality difference between different regions and views. In addition, the multiple loss functions with adaptive loss weights are introduced in the network, which consider both local features and global features and constrain the feature learning from multiple perspectives. And the adaptive loss weights also make the multi-loss CNN more flexible. The experimental results on four public SIQA databases show that the proposed method is superior to other existing SIQA methods with state-of-the-art performance.}
}
@article{HAGAG2021103117,
title = {Dual link distributed source coding scheme for the transmission of satellite hyperspectral imagery},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103117},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103117},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000730},
author = {Ahmed Hagag and Ibrahim Omara and Souleyman Chaib and Guangzhi Ma and Fathi E. Abd El-Samie},
keywords = {Hyperspectral image compression, Temporal redundancy in hyperspectral images, Dual link, Distributed source coding (DSC), Coset values, Lossless compression},
abstract = {Traditional lossless compression methods for satellite hyperspectral imagery focus on exploiting spatial and/or spectral redundancy. Those methods do not consider the temporal redundancy between images of the same area that are captured at different times. To exploit the temporal redundancy between hyperspectral images and reduce the amount of information to be transmitted from the space-satellite to the ground station via the downlink, this paper introduces a dual link distributed source coding (DLDSC) scheme for hyperspectral space-satellite communication. The proposed scheme employs the space-satellite dual link (i.e., the downlink and the uplink). The satellite onboard uses some side information from the ground station to calculate the hyperspectral image band coset values, and then, without syndrome coding, transmits to the ground station via the downlink. Coset coding is a typical technique used in distributed source coding (DSC), and here the coset values represent the timely hyperspectral image details. Typically, the coset values have lower entropy than that of the original source values. To exploit the temporal redundancy, the side information is computed in the ground station using the image captured at the previous time for the same area and transmitted to the space-satellite via the uplink. Hyperspectral images from the Hyperion satellite are used for the validation of the proposed scheme. The experimental results indicate that the proposed DLDSC scheme can reduce the original signal entropy by approximately 3.2 bits per sample (bps) and can achieve up to 1.0 bps and 1.6 bps gains over the lossless JPEG2000 standard and the state-of-art predictive CCSDS-123 method, respectively.}
}
@article{WU2021103234,
title = {Beyond ITQ: Efficient binary multi-view subspace learning for instance retrieval},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103234},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103234},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100153X},
author = {Zhijian Wu and Jun Li and Jianhua Xu and Wankou Yang},
keywords = {Instance retrieval, Multi-view fusion, Hamming subspace, Unsupervised learning},
abstract = {The existing hashing methods mainly handle either the feature based nearest-neighbor search or the category-level image retrieval, whereas a few efforts are devoted to instance retrieval problem. In this paper, we propose a binary multi-view fusion framework for directly recovering a latent Hamming subspace from the multi-view features for instance retrieval. More specifically, the multi-view subspace reconstruction and the binary quantization are integrated in a unified framework so as to minimize the discrepancy between the original multi-view high-dimensional Euclidean space and the resulting compact Hamming subspace. Besides, our method is essentially an unsupervised learning scheme without any labeled data involved, and thus can be used in the cases when the supervised information is unavailable or insufficient. Experiments on public benchmark and large-scale datasets reveal that our method achieves competitive retrieval performance comparable to the state-of-the-arts and has excellent scalability in large-scale scenario.}
}
@article{TIAN2021103199,
title = {3D reconstruction with auto-selected keyframes based on depth completion correction and pose fusion},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103199},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103199},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001267},
author = {Fangzheng Tian and Yongbin Gao and Zhijun Fang and Jia Gu and Shuqun Yang},
keywords = {3D reconstruction, Depth completion and correction, Pose fusion estimation, Auto-selected keyframes},
abstract = {Dense 3D reconstruction is required for robots to safely navigate or perform advanced tasks. The accurate depth information of the image and its pose are the basis of 3D reconstruction. The resolution of depth maps obtained by LIDAR and RGB-D cameras is limited, and traditional pose calculation methods are not accurate enough. In addition, if each image is used for dense 3D reconstruction, the dense point clouds will increase the amount of calculation. To address these issues, we propose a 3D reconstruction system. Specifically, we propose a depth network of contour and gradient attention, which is used to complete and correct depth maps to obtain high-resolution and high-quality depth maps. Then, we propose a method of fusion of traditional algorithms and deep learning for pose estimation to obtain accurate localization results. Finally, we adopt the method of autonomous selection of keyframes to reduce the number of keyframes, the surfel-based geometric reconstruction is performed to reconstruct the dense 3D environment. On the TUM RGB-D, ICL-NIUM, and KITTI datasets, our method significantly improves the quality of the depth maps, the localization results, and the effect of 3D reconstruction. At the same time, we have also accelerated the speed of 3D reconstruction.}
}
@article{TAGHIPOUR2021103113,
title = {A bottom-up and top-down human visual attention approach for hyperspectral anomaly detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103113},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103113},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000699},
author = {Ashkan Taghipour and Hassan Ghassemian},
keywords = {Hyperspectral image, Visual attention, Anomaly detection, Bottom-up attention, Top-down attention},
abstract = {Hyperspectral anomaly detection (HAD) is a branch of target detection which tries to locate pixels that are spectrally or spatially different from their background. In this paper, a visual attention approach is developed to leverage HAD. Traditional HAD methods often try to locate anomalous pixels based on spectral information. However, the spatial features of hyperspectral datasets provide valuable information. Here, we aim to fuse spatial and spectral anomaly features based on bottom-up (BU) and top-down (TD) visual attention mechanisms. Owe to the BU attention, spatial features are extracted by mimicking the primary visual cortex neurons functionality. Also, spectral information is obtained throughout a deep neural network that imitating the TD visual attention. The BU and TD approaches’ results are then integrated to provide both spectral and spatial information. The key findings of our results demonstrate the proposed method outperforms the six state-of-the-art AD methods based on different evaluation metrics.}
}
@article{SASIBHOOSHAN2021103236,
title = {WavNet — Visual saliency detection using Discrete Wavelet Convolutional Neural Network},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103236},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103236},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001541},
author = {Reshmi Sasibhooshan and Suresh Kumaraswamy and Santhoshkumar Sasidharan},
keywords = {Visual saliency detection, Discrete wavelet convolutional neural network, Edge structural similarity loss},
abstract = {In the recent advancements in image and video analysis, the detection of salient regions in the image becomes the initial step. This plays a crucial role in deciding the performance of such algorithms. In this work, a Multi-Resolution Feature Extraction (MRFE) technique that makes use of Discrete Wavelet Convolutional Neural Network (DWCNN) for generating features is employed. An Enhanced Feature Extraction (EFE) module extracts additional features from the high level features of the DWCNN, which are used to frame both channel as well as spatial attention models for yielding contextual attention maps. A new hybrid loss function is also proposed, which is a combination of Balanced Cross Entropy (BCE) loss and Edge based Structural Similarity (ESSIM) loss that effectively identifies and segments the salient regions with clear boundaries. The method is tested exhaustively with five different benchmark datasets and is proved superior to the existing state-of-the-art methods with a minimum Mean Absolute error (MAE) of 0.03 and F-measure of 0.956.}
}
@article{M2021103161,
title = {Multi-view motion modelled deep attention networks (M2DA-Net) for video based sign language recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103161},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103161},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001024},
author = {Suneetha M. and Prasad M.V.D. and Kishore P.V.V.},
keywords = {Multi view, Sign language recognition, Deep learning, Attention models, Motion modelled},
abstract = {Currently, video-based Sign language recognition (SLR) has been extensively studied using deep learning models such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs). In addition, using multi view attention mechanism along with CNNs could be an appealing solution that can be considered in order to make the machine interpretation process immune to finger self-occlusions. The proposed multi stream CNN mixes spatial and motion modelled video sequences to create a low dimensional feature vector at multiple stages in the CNN pipeline. Hence, we solve the view invariance problem into a video classification problem using attention model CNNs. For superior network performance during training, the signs are learned through a motion attention network thus focusing on the parts that play a major role in generating a view based paired pooling using a trainable view pair pooling network (VPPN). The VPPN, pairs views to produce a maximally distributed discriminating features from all the views for an improved sign recognition. The results showed an increase in recognition accuracies on 2D video sign language datasets. Similar results were obtained on benchmark action datasets such as NTU RGB D, MuHAVi, WEIZMANN and NUMA as there is no multi view sign language dataset except ours.}
}
@article{CHANG2021103097,
title = {Hybrid prediction-based pixel-value-ordering method for reversible data hiding},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103097},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103097},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000572},
author = {Jie Chang and Feng Ding and Xiaolong Li and Guopu Zhu},
keywords = {Reversible data hiding (RDH), Pixel-value-ordering (PVO), Rhombus prediction, Embedding capacity},
abstract = {Pixel-value-ordering (PVO) is an effective and promising method of reversible data hiding (RDH) and has received much attention in recent years. To improve performance, a pixel-based PVO (PPVO) method was recently introduced to predict the pixels to be embedded in a pixel-wise manner instead of the block-wise manner used by PVO. However, for PPVO, the surrounding neighbors of the predicted pixels are underutilized; moreover, its embedding does not adapt to the local complexity of the image to be embedded. To overcome the shortcomings of PPVO, this paper proposes a novel PVO method based on hybrid prediction for RDH. First, the surrounding neighbors of the pixel to be predicted are fully utilized by a hybrid prediction method, which combines rhombus prediction and pixel-wise prediction. Second, a modified embedding scheme based on multiple histograms is presented for adaptive embedding. Experimental results show the superior performance of the proposed method by comparing it with state-of-the-art RDH methods.}
}
@article{PEREZDELGADO2021103180,
title = {Revisiting the Iterative Ant-tree for color quantization algorithm},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103180},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103180},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001140},
author = {María-Luisa Pérez-Delgado},
keywords = {Color quantization, Clustering, Artificial ants},
abstract = {The Iterative Ant-tree for color quantization algorithm has recently been proposed to reduce the colors of an image at a low computational cost. It is a clustering-based method that generates good images compared to several well-known color quantization methods. This article proposes the modification of two features of the original algorithm: the value assigned to the parameter associated with the algorithm and the order in which the pixels of the image are processed. As a result, the new variant of the algorithm generates better images than the original and the results are less sensitive to the value selected for the parameter.}
}
@article{2023103867,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103867},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(23)00117-7},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001177}
}
@article{ELHARROUSS2021103116,
title = {A review of video surveillance systems},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103116},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103116},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000729},
author = {Omar Elharrouss and Noor Almaadeed and Somaya Al-Maadeed},
keywords = {Video surveillance system, Video analysis, Video surveillance systems trends},
abstract = {Automated surveillance systems observe the environment utilizing cameras. The observed scenario is then analysed using motion detection, crowd behaviour, individual behaviour, interaction between individuals, crowds and their surrounding environment. These automatic systems accomplish multitude of tasks which include, detection, interpretation, understanding, recording and creating alarms based on the analysis. Till recent, studies have achieved enhanced monitoring performance along with avoiding possible human failures by manipulation of different features of these systems. This paper presents a comprehensive review of such video surveillance systems as well as the components used with them. The description of the architectures used is presented which follows the most required analyses in these systems. For the bigger picture and wholesome view of the system, existing surveillance systems were compared in terms of characteristics, advantages, and difficulties which are tabulated in this paper. Adding to this, future trends are discussed which charts a path into the upcoming research directions.}
}
@article{ZHAN2021103253,
title = {Instance search via instance level segmentation and feature representation},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103253},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103253},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001656},
author = {Yu Zhan and Wan-Lei Zhao},
keywords = {Instance search, Instance segmentation, CNN},
abstract = {Instance search is an interesting task as well as a challenging issue due to the lack of effective feature representation. In this paper, an instance level feature representation built upon fully convolutional instance-aware segmentation is proposed. The feature is ROI-pooled from the segmented instance region. So that instances in various sizes and layouts are represented by deep features in uniform length. This representation is further enhanced by the use of deformable ResNeXt blocks. Superior performance is observed in terms of its distinctiveness and scalability on a challenging evaluation dataset built by ourselves. In addition, the proposed enhancement on the network structure also shows superior performance on the instance segmentation task.}
}
@article{MENG2021103176,
title = {Siamese CNN-based rank learning for quality assessment of inpainted images},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103176},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103176},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001139},
author = {Xiangdong Meng and Wei Ma and Chunhu Li and Qing Mi},
keywords = {Image inpainting, Rank learning, Image quality assessment, Siamese network},
abstract = {Existing NR-IIQA (no reference-based inpainted image quality assessment) algorithms assess the quality of an inpainted image via artificially designed unnaturalness expression, which often fail to capture inpainted artifacts. This paper presents a new deep rank learning-based method for NR-IIQA. The model adopts a siamese deep architecture, which takes a pair of inpainted images as input and outputs their rank order. Each branch utilizes a CNN structure to capture the global structure coherence and a patch-wise coherence assessment module (PCAM) to depict the local color and texture consistency in an inpainted image. To train the deep model, we construct a new dataset, which contains thousands of pairs of inpainted images with ground-truth quality ranking labels. Rich ablation studies are conducted to verify the key modules of the proposed architecture. Comparative experimental results demonstrate that our method outperforms existing NR-IIQA metrics in evaluating both inpainted images and inpainting algorithms.}
}
@article{LI2021103109,
title = {Multi-task learning with deformable convolution},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103109},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103109},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000687},
author = {Jie Li and Lei Huang and Zhiqiang Wei and Wenfeng Zhang and Qibing Qin},
keywords = {Multi-task learning, Deformable convolution, Recognition},
abstract = {Multi-task learning aims to tackle various tasks with branched feature sharing architectures. Considering its diversity and complexity, discriminative feature representations need to be extracted for each individual task. Fixed geometric structures as a limitation of convolutional neural networks (CNNs) in building models, is also exists and poses a severe challenge in multi-task learning since the geometric variations will augment when we deal with multiple tasks. In this paper, we go beyond these limitations and propose a novel multi-task network by introducing the deformable convolution. Our design, the Deformable Multi-Task Network (DMTN), starts with a single shared network for constructing a shared feature pool. Then, we present task-specific deformable modules to extract discriminative features to be tailored for each task from the shared feature pool. The task-specific deformable modules utilize two new parts, deformable part and alignment part, to extract more discriminative task-specific features while greatly enhancing the transformation modeling capability. Experiments conducted on various multi-task learning types demonstrate the effectiveness of the proposed method. On multiple classification tasks, semantic segmentation and depth estimation tasks, our DMTN exceeds state-of-the-art approaches against strong baselines.}
}
@article{WANG2021103226,
title = {Deep image compression with multi-stage representation},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103226},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103226},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001498},
author = {Zixi Wang and Guiguang Ding and Jungong Han and Fan Li},
keywords = {Deep image compression, Multi-stage representation, Data-dependent probability model, Convolutional neural network},
abstract = {While deep learning-based image compression methods have shown impressive coding performance, most existing methods are still in the mire of two limitations: (1) unpredictable compression efficiency gain when adopting convolutional neural networks with different depths, and (2) lack of an accurate model to estimate the entropy during the training process. To address these two problems, in this paper, a deep multi-stage representation based image compression (MSRIC) method is proposed. Owing to this architecture, the detail information of shallow stages and the compact information of deep stages can be utilized for image reconstruction. Furthermore, a data-dependent channel-wised factorized probability model (DCFPM) is adopted to increase the accuracy of entropy estimation. Experimental results indicate that the proposed method guarantees better perceptual performance at a wide range of bit-rates. Also, ablation studies are carried out to validate the above mentioned technologies.}
}
@article{SUN2021103134,
title = {Contextual information enhanced convolutional neural networks for retinal vessel segmentation in color fundus images},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103134},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103134},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000845},
author = {Muyi Sun and Kaiqi Li and Xingqun Qi and Hao Dang and Guanhong Zhang},
keywords = {Retinal vessel segmentation, Color fundus image analysis, Semantic segmentation, Cascaded dilated module, Context fusion},
abstract = {Accurate retinal vessel segmentation is a challenging problem in color fundus image analysis. An automatic retinal vessel segmentation system can effectively facilitate clinical diagnosis and ophthalmological research. In general, this problem suffers from various degrees of vessel thickness, perception of details, and contextual feature fusion in technique. For addressing these challenges, a deep learning based method has been proposed and several customized modules have been integrated into the well-known U-net with encoder–decoder architecture, which is widely employed in medical image segmentation. In the network structure, cascaded dilated convolutional modules have been integrated into the intermediate layers, for obtaining larger receptive field and generating denser encoded feature maps. Also, the advantages of the pyramid module with spatial continuity have been taken for multi-thickness perception, detail refinement, and contextual feature fusion. Additionally, the effectiveness of different normalization approaches has been discussed on different datasets with specific properties. Finally, sufficient comparative experiments have been enforced on three retinal vessel segmentation datasets, DRIVE, CHASE_DB1, and the STARE dataset with unhealthy samples. As a result, the proposed method outperforms the work of predecessors and achieves state-of-the-art performance.}
}
@article{LIU2021103210,
title = {Tile caching for scalable VR video streaming over 5G mobile networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103210},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103210},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100136X},
author = {Kedong Liu and Yanwei Liu and Jinxia Liu and Antonios Argyriou},
keywords = {VR video, Caching, Video tile, 5G systems},
abstract = {Currently, VR video delivery over 5G systems is still a very complicated endeavor. One of the major challenges for VR video streaming is the expectations for low latency that current mobile networks can hardly meet. Network caching can reduce the content delivery latency efficiently. However, current caching schemes cannot obtain ideal results for VR video since it requests the viewport interactively. In this paper, we propose a tiled scalable VR video caching scheme over 5G networks. VR chunks are first encoded into multi-granularity quality layers, and are then partitioned into tiles to facilitate viewport data access. By accommodating the 5G network infrastructure, the tiles are cooperatively cached in a three-level hierarchal system to reduce delivery latency. Furthermore, a quality-adaptive request routing algorithm is designed to cater for the 5G bandwidth dynamics. Experimental results show that the proposed scheme can reduce the transmission latency over conventional constant bitrate video caching schemes.}
}
@article{LIANG2021103159,
title = {Learn from the past – sequentially one-to-one video deblurring network},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103159},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103159},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001012},
author = {Chih-Hung Liang and Hung-Ting Su and Winston H. Hsu},
keywords = {Video deblurring, Deblurring, Image quality enhancement},
abstract = {With the growing availability of hand-held cameras in recent years, more and more images and videos are taken at any time and any place. However, they usually suffer from undesirable blur due to camera shake or object motion in the scene. In recent years, a few modern video deblurring methods are proposed and achieve impressive performance. However, they are still not suitable for practical applications as high computational cost or using future information as input. To address the issues, we propose a sequentially one-to-one video deblurring network (SOON) which can deblur effectively without any future information. It transfers both spatial and temporal information to the next frame by utilizing the recurrent architecture. In addition, we design a novel Spatio-Temporal Attention module to nudge the network to focus on the meaningful and essential features in the past. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art deblurring methods, both quantitatively and qualitatively, on various challenging real-world deblurring datasets. Moreover, as our method deblurs in an online manner and is potentially real-time, it is more suitable for practical applications.}
}
@article{ZHOU2021103197,
title = {Wasserstein distance feature alignment learning for 2D image-based 3D model retrieval},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103197},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103197},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001255},
author = {Yaqian Zhou and Yu Liu and Heyu Zhou and Wenhui Li},
keywords = {3D model retrieval, Multi-view learning, Cross-domain retrieval},
abstract = {2D image-based 3D model retrieval has become a hotspot topic in recent years. However, the current existing methods are limited by two aspects. Firstly, they are mostly based on the supervised learning, which limits their application because of the high time and cost consuming of manual annotation. Secondly, the mainstream methods narrow the discrepancy between 2D and 3D domains mainly by the image-level alignment, which may bring the additional noise during the image transformation and influence cross-domain effect. Consequently, we propose a Wasserstein distance feature alignment learning (WDFAL) for this retrieval task. First of all, we describe 3D models through a series of virtual views and use CNNs to extract features. Secondly, we design a domain critic network based on the Wasserstein distance to narrow the discrepancy between two domains. Compared to the image-level alignment, we reduce the domain gap by the feature-level distribution alignment to avoid introducing additional noise. Finally, we extract the visual features from 2D and 3D domains, and calculate their similarity by utilizing Euclidean distance. The extensive experiments can validate the superiority of the WDFAL method.}
}
@article{KHATIB2021103095,
title = {Learned Greedy Method (LGM): A novel neural architecture for sparse coding and beyond},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103095},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103095},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000560},
author = {Rajaei Khatib and Dror Simon and Michael Elad},
keywords = {Sparse representation, Orthogonal Matching Pursuit, Unfolding pursuit algorithms, Interpretable image processing architectures, Denoising, Deraining},
abstract = {The fields of signal and image processing have been deeply influenced by the introduction of deep neural networks. Despite their impressive success, the architectures used in these solutions come with no clear justification, being “black box” machines that lack interpretability. A constructive remedy to this drawback is a systematic design of networks by unfolding well-understood iterative algorithms. A popular representative of this approach is LISTA, evaluating sparse representations of processed signals. In this paper, we revisit this task and propose an unfolded version of a greedy pursuit algorithm for the same goal. More specifically, we concentrate on the well-known OMP algorithm, and introduce its unfolded and learned version. Key features of our Learned Greedy Method (LGM) are the ability to accommodate a dynamic number of unfolded layers, and a stopping mechanism based on representation error. We develop several variants of the proposed LGM architecture and demonstrate their flexibility and efficiency.}
}
@article{YAO2021103169,
title = {Stacking learning with coalesced cost filtering for accurate stereo matching},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103169},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103169},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001097},
author = {Peng Yao and Jieqing Feng},
keywords = {Stereo matching, Stacking, Random Forest, One-view disparity refinement},
abstract = {Deep learning based stereo matching algorithms have produced impressive disparity estimation for recent years; and the success of them has once overshadowed the conventional ones. In this paper, we intend to reverse this inferiority, by leveraging Stacking Learning with Coalesced Cost Filtering to make the conventional algorithms achieve or even surpass the results of deep learning ones. Four classical and Discriminative Dictionary Learning (DDL) algorithms are adopted as base-models for Stacking. For the former ones, four classical stereo matching algorithms are employed and regarded as ‘Coalesced Cost Filtering Module’; for the latter supervised learning one, we utilize the Discriminative Dictionary Learning (DDL) stereo matching algorithm. Then three categories of features are extracted from the predictions of base-models to train the meta-model. For the meta-model (final classifier) of Stacking, the Random Forest (RF) classifier is selected. In addition, we also employ an advanced one-view disparity refinement strategy to compute the final refined results more efficiently. Performance evaluations on Middlebury v.2 and v.3 stereo data sets demonstrate that the proposed algorithm outperforms other four most challenging stereo matching algorithms. Besides, the submitted online results even show better results than deep learning ones.}
}
@article{NING2021103115,
title = {MRANet: Multi-atrous residual attention Network for stereo image super-resolution},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103115},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103115},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000717},
author = {Luyao Ning and Anhong Wang and Lijun Zhao and Weimin Xue and Donghan Bu},
keywords = {Stereo cameras, Stereo image super-resolution, Discriminative ability, Parallax extraction, Attention mechanism},
abstract = {In recent years, stereo cameras have been widely used in various fields. Due to the limited resolution of real equipments, stereo image super-resolution (SR) is a very important and hot topic. Recent studies have shown that deep network structures can directly affect feature expression and extraction and thus influence the final results. In this paper, we propose a multi-atrous residual attention stereo super-resolution network (MRANet) with parallax extraction and strong discriminative ability. Specifically, we propose a multi-scale atrous residual attention (MARA) block to obtain receptive fields of different scales through a multi-scale atrous convolution and then combine them with attention mechanisms to extract more diverse and meaningful information. Moreover, we propose a stereo feature fusion unit for stereo parallax extraction and single viewpoint feature refinement and integration. Experiments on benchmark datasets show that MRANet achieves state-of-the-art performance in terms of quantitative metrics and visual quality compared with several SR methods.}
}
@article{BAHRAMI2021103232,
title = {A comparative study between single and multi-frame anomaly detection and localization in recorded video streams},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103232},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103232},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001528},
author = {Maedeh Bahrami and Majid Pourahmadi and Abbas Vafaei and Mohammad Reza Shayesteh},
keywords = {Anomaly detection, Deep learning, Convolutional autoencoder, Image reconstruction},
abstract = {Video anomaly detection is usually studied by considering the spatial and temporal contexts. This paper focuses first on spatial context and shows that it can be a fast real-time solution. In the first part of this work there are two main contributions: employing a new deep network for reconstruction and introducing a new regularity scoring function. The new deep architecture is based on pyramid of input images and compared to UNet, the proposed architecture boosts AUC by 15% and the new regularity scoring function is based on SSIM. The second part employs a multiframe approach to distinguish temporal behavior anomalies. The second approach enhances the results by 7% compared to spatial anomaly detection. Comparing the two approaches, if computing power is limited and real time anomaly detection is looked for, single frame detection is preferred while multi frame analysis offers a much wider possibility of anomaly detection.}
}
@article{V2021103110,
title = {Facial Expression Recognition through person-wise regeneration of expressions using Auxiliary Classifier Generative Adversarial Network (AC-GAN) based model},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103110},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103110},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000675},
author = {Dharanya V. and Alex Noel {Joseph Raj} and Varun P. Gopi},
keywords = {Facial Expression Recognition (FER), Subject dependence, Conditional GAN(CGAN), Auxiliary Classifier GAN(ACGAN), U-Net, Capsule Network(capsuleNet)},
abstract = {Recently, Facial Expression Recognition (FER) has gained much attention in the research area for its various applications. In the facial expression recognition task, subject-dependent issue is predominant when a small-scale database is used for training the system. The proposed Auxiliary Classifier Generative Adversarial Network (AC-GAN) based model regenerates ten expressions (angry, contempt, disgust, embarrassment, fear, joy, neutral, pride, sad, surprise) from input face image and recognizes its expression. To alleviate the subject dependence issue, we train the model person-wise and generate all the above expressions for a person and allow the discriminator to classify the expressions. The generator of our model uses U-Net Architecture, and the discriminator uses Capsule Networks for improved feature extraction. The model has been evaluated on the ADFES-BIV dataset yielding an overall classification accuracy of 93.4%. We also compared our model with the existing methods by evaluating our model on commonly used datasets like CK+, KDEF.}
}
@article{SENA2021103091,
title = {A content-based late fusion approach applied to pedestrian detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103091},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103091},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000559},
author = {Jessica Sena and Artur Jordão and William Robson Schwartz},
keywords = {Edestrian detection, Content-based fusion, Spatial consensus, Multiple detectors, Late fusion},
abstract = {The diversity of pedestrians detectors proposed in recent years has encouraged some works to fuse them to achieve a more accurate detection. The intuition behind it is to combine the detectors based on its spatial consensus. The hypothesis is that a location pointed by multiple detectors has a high probability of actually belonging to a pedestrian, while false positive regions have little consensus among detectors (small support) which allows discarding the false positives in these regions. We proposed a novel method called Content-Based Spatial Consensus (CSBC), which, in addition to relying on spatial consensus, considers the content of the detection windows to learn a weighted-fusion of pedestrian detectors. The result is a reduction in false alarms and an enhancement in the detection. In this work, we also demonstrated that there is small influence of the feature used to learn the contents of the windows of each detector, which enables our method to be efficient even employing simple features. The CSBC overcomes state-of-the-art fusion methods in the ETH dataset and the Caltech dataset. Particularly, our method is also more efficient, since fewer detectors are necessary to achieve expressive results.}
}
@article{WANG2021103136,
title = {Underwater image super-resolution using multi-stage information distillation networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103136},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103136},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000833},
author = {Huan Wang and Hao Wu and Qian Hu and Jianning Chi and Xiaosheng Yu and Chengdong Wu},
keywords = {Underwater image, Super-resolution, Visually-guided underwater robots, Convolutional neural network},
abstract = {Recently, single image super-resolution (SISR) has been widely applied in the fields of underwater robot vision and obtained remarkable performance. However, most current methods generally suffered from the problem of a heavy burden on computational resources with large model sizes, which limited their real-world underwater robotic applications. In this paper, we introduce and tackle the super resolution (SR) problem for underwater robot vision and provide an efficient solution for near real-time applications. We present a novel lightweight multi-stage information distillation network, named MSIDN, for better balancing performance against applicability, which aggregates the local distilled features from different stages for more powerful feature representation. Moreover, a novel recursive residual feature distillation (RRFD) module is constructed to progressively extract useful features with a modest number of parameters in each stage. We also propose a channel interaction & distillation (CI&D) module that employs channel split operation on the preceding features to produce two-part features and utilizes the inter channel-wise interaction information between them to generate the distilled features, which can effectively extract the useful information of current stage without extra parameters. Besides, we present USR-2K dataset, a collection of over 1.6K samples for large-scale underwater image SR training, and a testset with an additional 400 samples for benchmark evaluation. Extensive experiments on several standard benchmark datasets show that the proposed MSIDN can provide state-of-the-art or even better performance in both quantitative and qualitative measurements.}
}
@article{WANG2021103124,
title = {Perceptual hash-based coarse-to-fine grained image tampering forensics method},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103124},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103124},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100078X},
author = {Xiaofeng Wang and Qian Zhang and Chuntao Jiang and Jianru Xue},
keywords = {Perceptual image hash, Simple linear iterative clustering, Image forging detection, Image tempering localization},
abstract = {As an active forensic technology, perceptual image hash has important application in image content authenticity detection and integrity authentication. In this paper, we propose a hybrid-feature-based perceptual image hash method that can be used for image tampering detection and tampering localization. In the proposed method, we use the color features of image as global features, use point-based features and block-based features as local features, and combine with the structural features to generate intermediate hash code. Then we encrypt and randomize to generate the final hash code. Using this hash code, we present a coarse-to-fine grained forensics method for image tampering detection. The proposed method can realize object-level tampering localization. Abundant experimental results show that the proposed method is sensitive to content changes caused by malicious attacks, and the tampering localization precision achieves pixel level, and it is robust to a wide range of geometric distortions and content-preserving manipulations. Compared with the state-of-the-art schemes, the proposed scheme yields superior performance.}
}
@article{TAN2021103250,
title = {Sequence-tracker: Multiple object tracking with sequence features in severe occlusion scene},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103250},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103250},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001644},
author = {Xu Tan and Zhengwei Li and Qiaokang Liang and Wei Sun and Yaonan Wang and Dan Zhang},
keywords = {Object tracking, Sequence features, Severe occlusion scene, Sequence-tracker},
abstract = {Multiple object tracking is one of the most fundamental tasks in computer vision, and it is still very challenging for real-world applications due to its severe occlusion and motion blur. Most of the existing methods solve these multiple object tracking issues by performing data association based on the deep features of the detections in consecutive frames, which only contain the spatial information of the detected objects. Therefore, the inaccuracy of data association would easily occur, especially in the severe occlusion scenes. In this paper, a novel multiple object tracking model named sequence-tracker (STracker) has been proposed, which combines both the temporal and spatial features to perform data association. We trained a sequence feature extraction network based on video pedestrian re-identification offline, fused the obtained sequence features with the depth features of the previous frame, and then implemented the Hungarian algorithm for data association. Experiments have been carried out to validate the effectiveness of the proposed algorithm and the corresponding results indicates that it can significantly improve the trajectory quality of our dataset in this paper. Remarkably, for the public detector results from MOT official website, the proposed algorithm can achieve up to 57.2% MOTA and 50.9% IDF1 on the MOT17 dataset.}
}
@article{AHMED2021103177,
title = {Classifier aided training for semantic segmentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103177},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103177},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001127},
author = {Ifham Abdul Latheef Ahmed and Mohamed Hisham Jaward},
keywords = {Scene understanding, Semantic segmentation, Computer vision, Deep learning},
abstract = {Semantic segmentation is a prominent problem in scene understanding expressed as a dense labeling task with deep learning models being one of the main methods to solve it. Traditional training algorithms for semantic segmentation models produce less than satisfactory results when not combined with post-processing techniques such as CRFs. In this paper, we propose a method to train segmentation models using an approach which utilizes classification information in the training process of the segmentation network. Our method employs the use of classification network that detects the presence of classes in the segmented output. These class scores are then used to train the segmentation model. This method is motivated by the fact that by conditioning the training of the segmentation model with these scores, higher order features can be captured. Our experiments show significantly improved performance of the segmentation model on the CamVid and CityScapes datasets with no additional post processing.}
}
@article{LI2021103187,
title = {EdgeGAN: One-way mapping generative adversarial network based on the edge information for unpaired training set},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103187},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103187},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100119X},
author = {Yijie Li and Qiaokang Liang and Zhengwei Li and Youcheng Lei and Wei Sun and Yaonan Wang and Dan Zhang},
keywords = {Lightweight generative adversarial network, Image conversion, Image-to-image translation, Unpaired image-to-image translation},
abstract = {Image conversion has attracted mounting attention due to its practical applications. This paper proposes a lightweight network structure that can implement unpaired training sets to complete one-way image mapping, based on the generative adversarial network (GAN) and a fixed-parameter edge detection convolution kernel. Compared with the cycle consistent adversarial network (CycleGAN), the proposed network features simpler structure, fewer parameters (only 37.48% of the parameters in CycleGAN), and less training cost (only 35.47% of the GPU memory usage and 17.67% of the single iteration time in CycleGAN). Remarkably, the cyclic consistency becomes not mandatory for ensuring the consistency of the content before and after image mapping. This network has achieved significant processing effects in some image translation tasks, and its effectiveness and validity have been well demonstrated through typical experiments. In the quantitative classification evaluation based on VGG-16, the algorithm proposed in this paper has achieved superior performance.}
}
@article{XIAO2021103270,
title = {Robust model adaption for colour-based particle filter tracking with contextual information},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103270},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103270},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001760},
author = {Jingjing Xiao and Mourad Oussalah},
keywords = {Object tracking, Video Analysis, Scale modification, Background learning},
abstract = {Color-based particle filters have emerged as an appealing method for targets tracking. As the target may undergo rapid and significant appearance changes, the template (i.e. scale of the target, color distribution histogram) also needs to be updated. Traditional updates without learning contextual information may imply a high risk of distorting the model and losing the target. In this paper, a new algorithm utilizing the environmental information to update both the scale of the tracker and the reference appearance model for the purpose of object tracking in video sequences has been put forward. The proposal makes use of the well-established color-based particle filter tracking while differentiating the foreground and background particles according to their matching score. A roaming phenomenon that yields the estimation to shrink and diverge is investigated. The proposed solution is tested using both simulated and publicly available benchmark datasets where a comparison with six state-of-the-art trackers has been carried out. The results demonstrate the feasibility of the proposal and lie down foundations for further research on tackling complex visual tracking problems.}
}
@article{XIE2021103195,
title = {Dynamic Dual-Peak Network: A real-time human detection network in crowded scenes},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103195},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103195},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001243},
author = {Yefan Xie and Jiangbin Zheng and Xuan Hou and Yue Xi and Fengming Tian},
keywords = {Anchor free, Crowded scenes, CNN, Human detection},
abstract = {Human detection in crowded scenes is challenging since the objects occlude and overlap each other. Compared to general pedestrian detection, there is also more variation in human posture. This paper proposes a real-time human detection network, Dynamic Dual-Peak Network (DDPNet), which specifically addresses human object detection in overlapping and crowded scenes. We design a deep cascade fusion module to enhance the feature extraction capability of the anchor-free model for small objects in crowded scenes. In the meantime, the head–body dual-peak activation module is used to improve the prediction score of the central region of the occluded individual through low occlusion components. By this improvement strategy, the network’s ability is enhanced to discriminate individuals in crowded scenes and alleviate the problem caused by individual posture variation. Ultimately, we propose a novel Exhale–Inhale method to adjust the feature mapping ranges for various scale objects dynamically. In the process of ground truth mapping, the overlapping of individual feature information is reduced. Our DDPNet achieves competitive performance on the CrowdHuman dataset and executes real-time inference of almost 3x∼7x faster than competitive methods.}
}
@article{LEE2021103223,
title = {Adaptive fractional motion and disparity estimation skipping in MV-HEVC},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103223},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103223},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001486},
author = {Jin Young Lee and Sang-hyo Park},
keywords = {Encoding complexity, Disparity estimation (DE), Motion estimation (ME), Interview coding, MV-HEVC},
abstract = {MV-HEVC can efficiently compress multiview video data captured from different viewpoints. To achieve high coding efficiency, it consists of not only inter coding but also interview coding. The inter coding includes a motion estimation (ME) process that reduces temporal redundancies between consecutive frames, and the interview coding performs a disparity estimation (DE) that reduces interview redundancies between neighboring views. As a result, MV-HEVC needs high encoding complexity to perform both ME and DE. In order to reduce the complexity, this paper proposes an adaptive fractional ME and DE skipping method in a partitioned inter prediction unit (PU) mode, based on a result of a 2 N × 2 N inter PU coding. Experimental results show that the proposed method efficiently reduces the encoding complexity with negligible coding loss, compared to conventional methods.}
}
@article{WEN2021103178,
title = {Cross-modal dynamic convolution for multi-modal emotion recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103178},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103178},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001085},
author = {Huanglu Wen and Shaodi You and Ying Fu},
keywords = {Artificial neural networks, Pattern recognition, Affective behavior, Multi-modal temporal sequences},
abstract = {Understanding human emotions requires information from different modalities like vocal, visual, and verbal. Since human emotion is time-varying, the related information is usually represented as temporal sequences and we need to identify both emotion-related clues and their cross-modal interactions inside. However, emotion-related clues are sparse and misaligned in temporally unaligned sequences, making it hard for previous multi-modal emotion recognition methods to catch helpful cross-modal interactions. To this end, we present cross-modal dynamic convolution. To deal with sparsity, cross-modal dynamic convolution models the temporal dimension locally to avoid being overwhelmed by unrelated information. Cross-modal dynamic convolution is easy to stack, enabling it to model long-range cross-modal temporal interactions. Besides, models with cross-modal dynamic convolution are more stable during training than with cross-modal attention, bringing more possibilities in multi-modal sequential model designing. Extensive experiments show that our method can achieve competitive performance compared to previous works while being more efficient.}
}
@article{MEHRA2021103137,
title = {TheiaNet: Towards fast and inexpensive CNN design choices for image dehazing},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103137},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103137},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000791},
author = {Aryan Mehra and Pratik Narang and Murari Mandal},
keywords = {Dehazing, Computation efficiency, Speed and memory tests, Design choices},
abstract = {This work examines inexpensive design choices for dehazing as an end-to-end image-to-image mapping problem without relying on the physical scattering model. The proposed TheiaNet is free from intermediate-computation of transmission map, enabling haze removal in a highly resource constrained environments. The simplicity of the network is augmented by a spatial cleaning bottleneck block, that adds faster feature extraction without adding to trainable parameters. We also analyze the effectiveness of multi-cue color space (RGB, HSV, LAB, YCbCr) over single cue color space (RGB) for end-to-end dehazing. A comprehensive set of experiments were conducted on HazeRD, D-Hazy and the more recent Reside datasets. The proposed TheiaNet significantly outperforms the existing CNN and GAN based state-of-the-art methods in terms of PSNR and SSIM on all these datasets. It also outperforms all existing methods in term of speed, compute and memory efficiency, making it more efficient. This work highlights how judicious application-specific components can augment simple CNNs to denoise faster, and more accurately than deeper heavier networks, which is supported by an ablation analysis as well.}
}
@article{GUO2021103152,
title = {Facial parts swapping with generative adversarial networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103152},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103152},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000948},
author = {Jingtao Guo and Yi Liu},
keywords = {Facial parts swapping, Generative adversarial network, Deep leaning},
abstract = {In this paper, we present a novel deep generative facial parts swapping method: parts-swapping generative adversarial network (PSGAN). PSGAN independently handles facial parts, such as eyes (left eye and right eye), nose, mouth and jaw, which achieves facial parts swapping by replacing the target facial parts with source facial parts and reconstructing the entire face image with these parts. By separately modeling the facial parts in the form of region inpainting, the proposed method can successfully achieve highly photorealistic face swapping results, enabling users to freely manipulate facial parts. In addition, the proposed method is able to perform jaw editing based on sketch guidance information. Experimental results on the CelebA dataset suggest that our method achieves superior performance for facial parts swapping and provides higher user control flexibility.}
}
@article{DESOUZABRITO2021103112,
title = {Weighted voting of multi-stream convolutional neural networks for video-based action recognition using optical flow rhythms},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103112},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103112},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000705},
author = {André {de Souza Brito} and Marcelo {Bernardes Vieira} and Saulo {Moraes Villela} and Hemerson Tacon and Hugo {de Lima Chaves} and Helena {de Almeida Maia} and Darwin {Ttito Concha} and Helio Pedrini},
keywords = {Convolutional neural networks, Action recognition, Optical flow rhythm},
abstract = {Two of the most important premises of an ensemble are the diversity of its components and how to combine their votes. In this paper, we propose a multi-stream architecture based on the weighted voting of convolutional neural networks to deal with the problem of recognizing human actions in videos. A major challenge is how to include temporal aspects into this kind of approach. A key step in this direction is the selection of features that characterize the complexity of human actions in time. In this context, we propose a new stream, Optical Flow Rhythm, besides using other streams for diversity. To combine the streams, a voting system based on a new weighted average fusion method is introduced. In this scheme, the weights of classifiers are defined by an optimization process led by a metaheuristic. Experiments conducted on the UCF101 and HMDB51 datasets demonstrate that our method is comparable to state-of-the-art approaches.}
}
@article{XU2021103133,
title = {Image deraining with Adversarial Residual Refinement Network},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103133},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103133},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100081X},
author = {Wei Xu and Song Qiu and Kunyao Huang and Wei Liu and Junzhe Zuo and Haoming Guo},
keywords = {Image deraining, Deep learning, GAN},
abstract = {Prior image deraining works mainly have two problems: (1) they do not generalize well to various datasets; (2) too much detail information is lost in the heavy rain area of the rain image. To overcome these two problems, we propose a new two-stage Adversarial Residual Refinement Network (ARRN) to deal with heavy rain images. Specifically, for the first problem, we first introduce a new implicit rain model to model a rain image as a composition of a background image and a residual image. Based on the proposed implicit model, we then propose the ARRN which consists of an image decomposition stage and an image refinement stage. For the second problem, a new attention Wasserstein Generative Adversarial Networks (WGAN) loss in the refinement stage is introduced to force the network to focus on refining heavily degraded areas. Comprehensive experiments demonstrate the effectiveness of the proposed approach.}
}
@article{JIANG2021103192,
title = {Illumination-based adaptive saliency detection network through fusion of multi-source features},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103192},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103192},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100122X},
author = {Chunxu Jiang and Yu Liu and Jinglin Sun and Jichang Guo and Wei Lu},
keywords = {Multi-source, Illumination discrimination, Salient object detection, Deep learning},
abstract = {Salient object detection (SOD) tasks aim to outline the most concerned part of human vision, which is widely used in computer vision fields. Due to possibility of the insufficient illumination in the application environment (such as night or dim indoor environment), RGB images from visible channels usually lose most of their performance, while thermal images can improve the detection performance. Therefore, it is in urgent need of a robust saliency detection method, which can handle complex illumination conditions and take use of features from multiple sources intelligently. Accordingly, we propose our ‘illumination based multi-source fused salient object detection network’ (IAN-MF-SOD network). Taking the illumination condition as a quantitative reference, we guide features from two sources to fuse adaptively and intelligently, so that our method can enhance both of their advantages. For different illumination conditions, we distribute different fusion weights for each RGB–thermal image pair. Well fused images are generated as inputs to a trained SOD network to obtain saliency maps. Due to the analysis of our proposed IAN-score, our method performs favorably against traditional RGB-based SOD networks.}
}
@article{KANG2021103106,
title = {Specular highlight region restoration using image clustering and inpainting},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103106},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103106},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000663},
author = {Hosun Kang and Dokyung Hwang and Jangmyung Lee},
keywords = {Specular highlight detection, Highlight identification, Pixel analysis, Pixel clustering, Image inpainting},
abstract = {A novel specular highlight restoration algorithm has been proposed to remove the specular highlight in a real-time vision system. In this paper, the specular highlight region has been detected and it has been restored by image inpainting method. Most of specular highlight detection algorithms are effective only for a single image. And the auto-threshold algorithm has been implemented in real time, but, it has still low reliability with a heavy computational cost. The proposed system detects pixels corresponding to the specular highlight region in the HSI color space through a newly defined classification table. So, Specular highlight can be detected quickly with these simplified classification table. In addition, it has versatility because it can be combined with various existing high-performance image inpainting method. The superiority of the proposed algorithm is compared with the conventional specular highlight removal algorithm by combining proposed algorithm with two high-performance image inpainting techniques.}
}
@article{SHAO2021103231,
title = {Generative image inpainting with salient prior and relative total variation},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103231},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103231},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001516},
author = {Hang Shao and Yongxiong Wang},
keywords = {Image inpainting, GAN, Corruption recognition, Salient prior, Relative total variation},
abstract = {Image inpainting is an important research direction of image processing. The generative adversarial network (GAN), which can reconstruct new reasonable content in the corrupted region, is the most interesting tool in current inpainting technologies. However, the previous deep methods generally need to be pre-added the binary mask representing the corruption location as the extra input. A novel inpainting algorithm which does not require additional external labels is proposed in this paper. The algorithm consists of two parts: corruption recognition module and content inpainting module, which can recognize and fill random corruption. In the recognizer, the salient object from the uncorrupted region is used as the prior for distinguishing corruption. In the inpainting module, a two-stage network is applied to reconstruct the image from coarse content to texture details. To avoid the misdetection in recognition which has a negative impact on the restoration in inpainting, we perform relative total variational filtering on the corrupted image, and use the salient map as the supervision of detail reconstruction. Qualitative and quantitative experiments on multiple datasets verify the effectiveness of our recognition module, the competitive advantage of our inpainting module, and the enlightening significance of our total algorithm in image inpainting.}
}
@article{INOUE2021103251,
title = {Amplitude based keyless optical encryption system using deep neural network},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103251},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103251},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001632},
author = {Kotaro Inoue and Myungjin Cho},
keywords = {Optical encryption, Deep neural network, Amplitude-based double random phase encryption},
abstract = {Double random phase encryption (DRPE) system is a simple and powerful encoding technique that consists of only two lenses and two random phase masks. However, there are many issues for applying to actual security systems such as phase acquisition, vulnerability to phase retrieval techniques, and data throughput. Although various extensions of DRPE have addressed each issue, there is no comprehensive solution. To tackle all the issues of DRPE, we propose a new amplitude-based DRPE (ADRPE) system using deep learning. The encoding is the same as the current ADRPE system, and the decoding is achieved by an inverse ADRPE system using convolution neural networks. Our system can achieve a real-time end-to-end encryption system without any additional optical devices and exposure of the keys. To demonstrate our method, we applied it to simulations with various datasets such as passwords, Quick-Response (QR) codes, and fingerprints.}
}
@article{KHAN2021103175,
title = {A ghostfree contrast enhancement method for multiview images without depth information},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103175},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103175},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001115},
author = {Rizwan Khan and You Yang and Qiong Liu and Zahid Hussain Qaisar},
keywords = {Multi-view low-light images, Feature matching, Exposure fusion},
abstract = {High dynamic range (HDR) images greatly improve visual content quality, but pose challenges in processing, acquisition, and display. Images captured in real-world scenarios with multiple nonlinear cameras, extremely short unknown exposure time, and a shared light source present the additional challenges of incremental baseline and angle deviation amongst the cameras. The disparity maps in such conditions are not reliable; therefore, we propose a method that relies on the accurate detection and matching of feature points across adjacent viewpoints. We determine the exposure gain among the matched feature points in the involved views and design an image restoration method to restore multiview low dynamic range (MVLDR) images for each viewpoint. Finally, the fusion of these restored MVLDR images produces high-quality images for each viewpoint without capturing a series of bracketed exposure. Extensive experiments are conducted in controlled and uncontrolled conditions, and results prove that the proposed method competes for the state-of-the-arts.}
}
@article{CHEN2021103128,
title = {A fast algorithm based on gray level co-occurrence matrix and Gabor feature for HEVC screen content coding},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103128},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103128},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000821},
author = {Jing Chen and Jianshan Ou and Huanqiang Zeng and Canhui Cai},
keywords = {Screen content video coding, High efficiency video coding (HEVC), Intra prediction, Partition decision},
abstract = {To reduce the computational complexity of screen content video coding (SCC), a fast algorithm based on gray level co-occurrence matrix and Gabor feature model for HEVC-SCC, denoted as GGM, is proposed in this paper. By studying the correlation of non-zero number in gray level co-occurrence matrix with different partitioning depth, the coding unit (CU) size of intra coding can be prejudged, which selectively skips the intra prediction process of CU in other depth. With Gabor filter, the edge information reflecting the features of screen content images to the human visual system (HVS) are extracted. According to Gabor feature, CUs are classified into natural content CUs (NCCUs), smooth screen content CUs (SSCUs) and complex screen content CUs (CSCUs), with which, the calculation and judgment of unnecessary intra prediction modes are skipped. Under all-intra (AI) configuration, experimental results show that the proposed algorithm GGM can achieve encoding time saving by 42.13% compared with SCM-8.3, and with only 1.85% bit-rate increasement.}
}
@article{TANG2021103209,
title = {Video hashing with secondary frames and invariant moments},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103209},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103209},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001358},
author = {Zhenjun Tang and Shaopeng Zhang and Xianquan Zhang and Zhixin Li and Zhenhai Chen and Chunqiang Yu},
keywords = {Video hashing, Hash function, Secondary frame, Discrete wavelet transform (DWT), Invariant moments},
abstract = {Video hashing is a useful technique of many multimedia systems, such as video copy detection, video authentication, tampering localization, video retrieval, and anti-privacy search. In this paper, we propose a novel video hashing with secondary frames and invariant moments. An important contribution is the secondary frame construction with 3D discrete wavelet transform, which can reach initial data compression and robustness against noise and compression. In addition, since invariant moments are robust and discriminative features, hash generation based on invariant moments extracted from secondary frames can ensure good classification of the proposed video hashing. Extensive experiments on 8300 videos are conducted to validate efficiency of the proposed video hashing. The results show that the proposed video hashing can resist many digital operations and has good discrimination. Performance comparisons with some state-of-the-art algorithms illustrate that the proposed video hashing outperforms the compared algorithms in classification in terms of receiver operating characteristic results.}
}
@article{SADEDDINE2021103193,
title = {Recognition of user-dependent and independent static hand gestures: Application to sign language},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103193},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103193},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001231},
author = {Khadidja Sadeddine and Fatma Zohra Chelali and Rachida Djeradi and Amar Djeradi and Sidahmed Benabderrahmane},
keywords = {Static hand gesture recognition, Sign language recognition, GLAC, Gabor wavelet, Curvelet transform, Combined classifiers},
abstract = {Static hand gesture (HG) recognition for both user-dependent and user-independent is a challenging problem, especially when there are changes in lighting, hand position, and background, the recognition becomes more complex. To solve this problem, this paper proposes a static hand gesture recognition based on a set of image descriptors: Gradient Local Auto-Correlation (GLAC), Gabor Wavelet Transform (GWT), and Fast Discrete Curve Transform (FDCT). Principal Component Analysis (PCA) was used to reduce dimensionality. Tests were performed on three sign language datasets and one hand posture dataset using neural network classifiers, K-Nearest Neighbor (KNN) classifiers, and combined classifiers. The results obtained were compared to the state of the art and show an accuracy of 100% for user-independent and 98.33% for user-dependent gestures, despite the difficult acquisition conditions of the datasets.}
}
@article{ZHANG2021103224,
title = {A regional distance regression network for monocular object distance estimation},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103224},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103224},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001474},
author = {Yufeng Zhang and Lianghui Ding and Yuxi Li and Weiyao Lin and Mingbi Zhao and Xiaoyuan Yu and Yunlong Zhan},
keywords = {Monocular distance estimation, Object detection, Deep neural network, Surveillance},
abstract = {Monocular pipelines are convenient and cost-effective solutions for object distance estimation in 3D vision. Current methods for monocular object distance estimation either perform inaccurately or require heavy work on data collection. In this paper, we propose a network with R-CNN based structure to implement object detection and distance estimation simultaneously. We append an efficient branch to integrate the information of camera extrinsic parameters with RGB data in our network. Further, optimized multi-scale feature is utilized to enrich the representation power of deep feature, hence to enhance the estimation accuracy. Finally, several regression methods are explored to improve distance estimation results. We train and validate our network on KITTI object dataset, and compare with other methods to show that our method is accurate and easy to train. To prove the generality of our method under other scenarios, we construct a dataset of surveillance scenes, and conduct similar experiments on this dataset.}
}
@article{LIAO2021103244,
title = {GIFMarking: The robust watermarking for animated GIF based deep learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103244},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103244},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001590},
author = {Xin Liao and Jing Peng and Yun Cao},
keywords = {Animated GIF images, Robust watermarking, 3D convolutional neural networks, Adversarial network},
abstract = {Animated GIF has become a key communication tool in contemporary social platforms thanks to highly compatible with affective performance, and it is gradually adopted in commercial applications. Therefore, the copyright protection of the animated GIF requires more attention. Digital watermarking is an effective method to embed invisible data into a digital medium that can identify the creator or authorized users. However, few works have been devoted to robust watermarking for the animated GIF. One of the main challenges is that the animated image also contains time frame dimension information compare with still images. This paper proposes a robust blind watermarking framework based 3D convolutional neural networks for the animated GIF image, which achieves watermark image embedding and extraction for the animated GIF. Also, noise simulation is developed in frame-level to ensure robustness for the attack of the temporal dimension in this framework. Furthermore, the invisibility of the watermarked animated image is optimized by adversarial learning. Experimental results provide the effectiveness of the proposed framework and show advantages over existing works.}
}
@article{PEIXOTO2021103174,
title = {Harnessing high-level concepts, visual, and auditory features for violence detection in videos},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103174},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103174},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001073},
author = {Bruno M. Peixoto and Bahram Lavi and Zanoni Dias and Anderson Rocha},
abstract = {In detecting sensitive media, violence is one of the hardest to define objectively, and thus, a significant challenge to detect automatically. While many studies were conducted in detecting aspects of violence, very few try to approach the general concept. We propose a method that aims to enable machines to understand a high-level concept of violence by first breaking it down into smaller, more objective ones, such as fights, explosions, blood, and gunshots, to combine them later, leading to a better understanding of the scene. For this, we leverage characteristics of each individual sub-concept of violence (relying upon custom-tailored convolutional neural networks) to guide how they should be described. A fight scene should incorporate temporal features that a scene with blood does not need to describe. A scene with explosions or gunshots should weigh more on its audio features. With this multimodal approach, we trained visual and auditory feature detectors and later combined them into a decision neural network to give us a violence detector that considers several different aspects of the problem. This robust and modular approach allows different cultures and users to adapt the detector to their specific needs.}
}
@article{BAISA2021102952,
title = {Robust online multi-target visual tracking using a HISP filter with discriminative deep appearance learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {102952},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102952},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301802},
author = {Nathanael L. Baisa},
keywords = {Multiple target filtering, HISP filter, Online tracking, Appearance learning, CNN, MOT challenge},
abstract = {We propose a novel online multi-target visual tracker based on the recently developed Hypothesized and Independent Stochastic Population (HISP) filter. The HISP filter combines advantages of traditional tracking approaches like MHT and point-process-based approaches like PHD filter, and it has linear complexity while maintaining track identities. We apply this filter for tracking multiple targets in video sequences acquired under varying environmental conditions and targets density using a tracking-by-detection approach. We also adopt deep CNN appearance representation by training a verification-identification network (VerIdNet) on large-scale person re-identification data sets. We construct an augmented likelihood in a principled manner using this deep CNN appearance features and spatio-temporal information. Furthermore, we solve the problem of two or more targets having identical label considering the weight propagated with each confirmed hypothesis. Extensive experiments on MOT16 and MOT17 benchmark data sets show that our tracker significantly outperforms several state-of-the-art trackers in terms of tracking accuracy.}
}
@article{WANG2021103151,
title = {Saliency detection via coarse-to-fine diffusion-based compactness with weighted learning affinity matrix},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103151},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103151},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000936},
author = {Fan Wang and Guohua Peng},
keywords = {Saliency detection, Diffusion-based compactness, Multi-view graphs, Weighted learning affinity matrix},
abstract = {Diffusion-based compactness is an effective method for foreground-based saliency detection, in which one key is the conventional graph construction. However, the conventional graph only displays the local structure but not preserves global relevance information. Therefore, diffusion-based compactness cannot highlight complete salient object which contains multiple areas with different features, and the extracted salient regions with weak homogeneous. Aiming to address these problems, we propose a saliency detection method via coarse-to-fine diffusion-based compactness with a weighted learning affinity matrix. Firstly, we construct multi-view conventional graphs to calculate the rough compactness cue. Secondly, we build a two-stage multi-view weighted graphs using a weighted learning affinity matrix and compute the coarse-to-fine compactness cue. Extensive experiments tested on three benchmark datasets, demonstrating the superior against several state-of-the-art methods.}
}
@article{ZAMANI2021103228,
title = {Atom specific multiple kernel dictionary based Sparse Representation Classifier for medium scale image classification},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103228},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103228},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001504},
author = {Fatemeh Zamani and Mansour Jamzad and Hamid R. Rabiee},
keywords = {Sparse Representation Classifier, Multiple Kernel Learning, Kernel local weighting, Dictionary learning, Image classification},
abstract = {Kernel based Sparse Representation Classifier (KSRC) can classify images with acceptable performance. In addition, Multiple Kernel Learning based SRC (MKL-SRC) computes the weighted sum of multiple kernels in order to construct a unified kernel while the weight of each kernel is calculated as a fixed value in the training phase. In this paper, an MKL-SRC with non-fixed kernel weights for dictionary atoms is proposed. Kernel weights are embedded as new variables to the main KSRC goal function and the resulted optimization problem is solved to find the sparse coefficients and kernel weights simultaneously. As a result, an atom specific multiple kernel dictionary is computed in the training phase which is used by SRC to classify test images. Also, it is proved that the resulting optimization problem is convex and is solvable via common algorithms. The experimental results demonstrate the effectiveness of the proposed approach.}
}
@article{WEN2021103145,
title = {Rethinking pre-training on medical imaging},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103145},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103145},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000894},
author = {Yang Wen and Leiting Chen and Yu Deng and Chuan Zhou},
keywords = {Transfer learning, Medical image analysis, Convolutional neural network, Survival prediction},
abstract = {Transfer learning from natural image datasets, such as ImageNet, is common for applying deep learning to medical imaging. However, the modalities of natural and medical images differ considerably, and the reason for the latest medical research preferring ImageNet to medical data is questionable. In this study, we investigated the properties of medical pre-training and its transfer effectiveness on various medical tasks. Through an intuitive convolution-based analysis, we determined the modality characteristics of images. Surprisingly, medical pre-training showed exceptional performance for a classification task but not for a segmentation task since medical data are visually homogeneous and lack morphological information. Using data with diverse modalities helped overcome such drawbacks, resulting in medical pre-training achieving performance comparable to pre-training with ImageNet with considerably fewer samples than ImageNet for both aforementioned tasks. Finally, a study of learned representations and realistic scenarios indicated that while ImageNet is the best choice for medical imaging, medical pre-training has significant potential.}
}
@article{LI2021103107,
title = {Siamese target estimation network with AIoU loss for real-time visual tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103107},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103107},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000651},
author = {Zhiyong Li and Chenming Hu and Ke Nai and Jin Yuan},
keywords = {Visual tracking, Siamese network, Target estimation network, Intersection over union (IoU) loss},
abstract = {The fully convolutional siamese network based trackers achieve great progress recently. Most of these methods focus on improving the capability of siamese network to represent the target. In this paper, we propose our model which focuses on estimating the state of the target with our proposed novel IoU (intersection over union) loss function which is named AIoU. Our model consists of a siamese subnetwork for feature extraction and a target estimation subnetwork for state representation. The target estimation subnetwork contains a classification head for classifying background and foreground and a regression head for estimating target. In order to regress better bounding boxes, we further study the loss function utilized in the regression head and propose a powerful IoU loss function. Our tracker achieves competitive performance on OTB2015, VOT2018, and VOT2019 benchmarks with a speed of 180 FPS, which proves the effectiveness of our method.}
}
@article{DEBNATH2021103165,
title = {A comprehensive survey on computer vision based concepts, methodologies, analysis and applications for automatic gun/knife detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103165},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103165},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100105X},
author = {Rajib Debnath and Mrinal Kanti Bhowmik},
keywords = {Weapon detection, Survey, Security and surveillance, Multiple object detection, Visual and concealed},
abstract = {The ability to detect gun and gun held in hand or other body parts is a typical human skill. The same problem presents an imperative task for computer vision system. Automatic observer independent detection of hand held gun or gun held in the other body part, whether it is visible or concealed, provides enhance security in vulnerable places and initiates appropriate action there. Compare to the automatic object detection systems, automatic detection of gun has very few successful attempts. In the present scope of this paper, we present an extensive survey on automatic detection of gun and define a taxonomy for this particular detection system. We also describe the inherent difficulties related with this problem. In this survey of published papers, we examine different approaches used in state-of-the-art attempts and compare performances of these approaches. Finally, this paper concludes pointing to the possible research gaps in related fields.}
}
@article{CHANG2021103248,
title = {Quality assessment of screen content images based on multi-stage dictionary learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103248},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103248},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001620},
author = {Yongli Chang and Sumei Li and Anqi Liu and Jie Jin},
keywords = {Image quality assessment, Screen content images, Sparse dictionary, Hierarchical feature extraction},
abstract = {In this paper, we propose an effective method for quality assessment of screen content images (SCIs) based on multi-stage dictionary learning. To simulate the brain’s layered processing of signals, we proposed a hierarchical feature extraction strategy, which is called multi-stage dictionary learning, to simulate the hierarchical information processing of brain. First, the standard deviation of normalized map obtained from training image is used to select the training data in a certain proportion, which can ensure the learning efficiency and reduce the training burden. Next, the reconstructed map is weighted as the input of the next-stage dictionary learning. Then using the trained dictionary, the sparse representation is applied to extract features. Meanwhile, considering that some important features may be ignored in the process of multi-stage dictionary learning, we use Log Gabor filter to extract feature maps, and then calculate the correlation between feature maps as another kind of compensation features. Final, for the two feature sets, we choose SVR and feature codebook to learn two objective scores, and then use the adaptive weighting strategy to get the final objective quality score. Experimental results show that the proposed method is superior to several mainstream SCIs metrics on two publicly available databases.}
}
@article{ZHANG2021103170,
title = {Stable self-attention adversarial learning for semi-supervised semantic image segmentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103170},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103170},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001103},
author = {Jia Zhang and Zhixin Li and Canlong Zhang and Huifang Ma},
keywords = {Self-Attention Mechanism, Adversarial Learning, Semi-Supervised Learning, Spectral Normalization, Semantic Image Segmentation},
abstract = {The application of adversarial learning for semi-supervised semantic image segmentation based on convolutional neural networks can effectively reduce the number of manually generated labels required in the training process. However, the convolution operator of the generator in the generative adversarial network (GAN) has a local receptive field, so that the long-range dependencies between different image regions can only be modeled after passing through multiple convolutional layers. The present work addresses this issue by introducing a self-attention mechanism in the generator of the GAN to effectively account for relationships between widely separated spatial regions of the input image with supervision based on pixel-level ground truth data. In addition, the adjustment of the discriminator has been demonstrated to affect the stability of GAN training performance. This is addressed by applying spectral normalization to the GAN discriminator during the training process. Our method has better performance than existing full/semi-supervised semantic image segmentation techniques.}
}
@article{WANG2021103123,
title = {Statistical image watermarking using local RHFMs magnitudes and beta exponential distribution},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103123},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103123},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000778},
author = {Xiang-yang Wang and Jing Tian and Jia-lin Tian and Pan-pan Niu and Hong-ying Yang},
keywords = {Digital watermarking, Local RHFMs magnitudes, Beta-exponential distribution, Modified maximum likelihood estimation, ML decision criterion},
abstract = {The imperceptibility, robustness and data payload are widely considered as the three main properties vital for any image watermarking systems. They are complimentary to each other and hence challenging to attain the right balance between them. The statistical model-based multiplicative watermarking is an effective way to achieve the tradeoff among imperceptibility, robustness and data payload. Radial harmonic Fourier moments (RHFMs) is a strong tool in image processing, which has many advantages, such as lower noise sensitivity, powerful image description ability and geometric invariance feature. In this paper, we propose a new statistical image watermarking scheme using local RHFMs magnitudes and Beta exponential distribution model. Our image watermarking scheme consists of two parts, namely, embedding and extraction. In the embedding process, we divide the host image into no-overlapping blocks and compute the local RHFMs of image blocks, then insert the watermark signal into the robust local RHFMs magnitudes through multiplicative approach. In the extraction phase, robust local RHFMs magnitudes are firstly modeled by employing the Beta exponential distribution, where the statistical properties of local RHFMs magnitudes are captured accurately. Then the modified maximum likelihood parameter estimation (MMLE) approach is introduced to estimate the statistical parameters of Beta exponential distribution model. And finally an image watermark decoder for multiplicative watermarking is developed using Beta exponential distribution and maximum likelihood decision criterion. Experimental results on some test images and comparison with well-known existing methods demonstrate the efficacy and superiority of the proposed statistical image watermarking.}
}
@article{GOLTS2021103208,
title = {Image compression optimized for 3D reconstruction by utilizing deep neural networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103208},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103208},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001346},
author = {Alex Golts and Yoav Y. Schechner},
keywords = {Image compression, 3D reconstruction, Deep learning, Recurrent neural networks},
abstract = {Computer vision tasks are often expected to be executed on compressed images. Classical image compression standards like JPEG 2000 are widely used. However, they do not account for the specific end-task at hand. Motivated by works on recurrent neural network (RNN)-based image compression and three-dimensional (3D) reconstruction, we propose unified network architectures to solve both tasks jointly. These joint models provide image compression tailored for the specific task of 3D reconstruction. Images compressed by our proposed models, yield 3D reconstruction performance superior as compared to using JPEG 2000 compression. Our models significantly extend the range of compression rates for which 3D reconstruction is possible. We also show that this can be done highly efficiently at almost no additional cost to obtain compression on top of the computation already required for performing the 3D reconstruction task.}
}
@article{NIEN2021103242,
title = {Region-level bit allocation for rate control of 360-degree videos using cubemap projection},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103242},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103242},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001589},
author = {Yu-Chieh Nien and Chih-Wei Tang},
keywords = {360-degree video coding, Cubemap projection (CMP), Rate control, Bit allocation, Machine learning, Detection of high HEVC coding cost regions},
abstract = {Featuring with more uniform sampling density in the sphere domain and less non-uniform geometric deformations in the planar domain, variants of cubemap projection (CMP) format enable the higher compression ratio in on-going 360-degree video coding standardization. Different from single-view videos, 360-degree CMP videos feature with content discontinuity combined with the abrupt change of motion vectors between some adjacent faces. However, there is few bit allocation scheme designed for rate control of video coding of CMP format. Thus, this paper proposes a region-level bit allocation scheme for rate control of interframe coding of CMP format. The proposed scheme consists of two parts. The first part is machine learning based high HEVC coding cost region detection for individual faces, where the feature descriptor of a CTU consists of the face based texture complexity, motion magnitude, motion density, and temporal coherence of motion vector. The second part is fitting function based region-level bit allocation. Different from previous work, bits are assigned to the high coding cost region and non-high coding cost region in individual faces of CMP format. Experimental results indicate that the proposed scheme achieves higher bitrate accuracy and larger BD-WS-PSNR compared with the original rate control scheme of the reference software of HEVC, HM16.16 with the 360Lib.}
}
@article{PILARCZYK2021103221,
title = {Are emotional objects visually salient? The Emotional Maps Database},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103221},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103221},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001462},
author = {Joanna Pilarczyk and Weronika Janeczko and Radosław Sterna and Michał Kuniecki},
keywords = {Meaning maps, Saliency, Emotion, Arousal, Natural scenes, Key objects},
abstract = {The visual system prioritizes emotional content in natural scenes, but it is unclear whether emotional objects are systematically more salient. We compare emotional maps - created by averaging multiple manual selections of the most meaningful regions in images of negative, positive, and neutral affective valence - with saliency maps generated by Graph-Based Visual Saliency, Proto-object, and SalGAN models. We found that similarity between emotional and saliency maps is modulated by the scenes’ arousal and valence ratings: the more negative and high-arousing content, the less it was salient. Simultaneously, the negative and high-arousing content was the easiest to identify by the participants, as shown by the highest inter-individual agreement in the selections. Our results support the “affective gap” hypothesis, i.e., decoupling of emotional meaning from image’s formal features. The Emotional Maps Database created for this study, proven useful in gaze fixation prediction, is available online for scientific use.}
}
@article{ZHANG2021103186,
title = {HP-VCS: A high-quality and printer-friendly visual cryptography scheme},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103186},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103186},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001188},
author = {Denghui Zhang and Hongbin Zhu and Shenglong Liu and Xu Wei},
keywords = {Visual cryptography scheme, Image security, Halftoning, Color VCS},
abstract = {Visual Cryptography Scheme (VCS) is a secret-sharing scheme which aims to encrypt a secret message into multiple shares and transmit them to participants over an untrusted communication channel. Although human vision can easily reveal the secret message by stacking a sufficient number of shares, this scheme reduces the visual quality of recovered images. This paper presents a novel high-quality and printer-friendly VCS. When providing high-quality recovery, this scheme keeps the size of the shares the same as the secret image. Experimental results show that, compared with previous work, the proposed scheme significantly improves the performance of recovered images.}
}
@article{LI2021103166,
title = {Reversible data hiding in encrypted color images using cross-channel correlations},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103166},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103166},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001061},
author = {Ming Li and Hua Ren and Yong Xiang and Yushu Zhang},
keywords = {Reversible data hiding, Image encryption, Cross-channel correlation, Homomorphism},
abstract = {In recent years, the increasing requirements in cloud storage and cloud computing have made it necessary to encrypt digital images for privacy protection. Meanwhile, many reversible data hiding (RDH) algorithms in the encrypted domain have been proposed. However, most of these algorithms are for gray-level images, and the intrinsic cross-channel correlations of color images cannot be utilized to improve the embedding capacity. In this paper, we propose a novel data hiding method for encrypted color images. In the encryption stage, the homomorphic property of encryption is achieved by basic modular addition. During the data hiding process, the cross-channel correlations between R, G and B channels are generated in encrypted domain, and data hiding is performed by the difference histogram shifting. Analysis and experiments demonstrate that the proposed method is secure and the RDH performance is superior.}
}
@article{LI2021103149,
title = {Single image deblurring with cross-layer feature fusion and consecutive attention},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103149},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103149},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000924},
author = {Yaowei Li and Ye Luo and Guokai Zhang and Jianwei Lu},
keywords = {Image deblurring, Cross-layer feature fusion, Consecutive attention},
abstract = {Single image deblurring aims to restore the single blurry image to its sharp counterpart and remains an active topic of enduring interest. Recently, deep Convolutional Neural Network (CNN) based methods have achieved promising performance. However, two primary limitations mainly exist on those CNNs-based image deblurring methods: most of them simply focus on increasing the complexity of the network, and rarely make full use of features extracted by encoder. Meanwhile, most of the methods perform the deblurred image reconstruction immediately after the decoder, and the roles of the decoded features are always underestimated. To address these issues, we propose a single image deblurring method, in which two modules to fuse multiple features learned in encoder (the Cross-layer Feature Fusion (CFF) module) and manipulate the features after decoder (the Consecutive Attention Module (CAM)) are specially designed, respectively. The CFF module is to concatenate different layers of features from encoder to enhance rich structural information to decoder, and the CAM module is able to generate more important and correlated textures to the reconstructed sharp image. Besides, the ranking content loss is employed to further restore more realistic details in the deblurred images. Comprehensive experiments demonstrate that our proposed method can generate less blur and more textures in deblurred image on both synthetic datasets and real-world image examples.}
}
@article{NAVEED2021103135,
title = {Driver activity recognition by learning spatiotemporal features of pose and human object interaction},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103135},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103135},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000808},
author = {Humza Naveed and Fareed Jafri and Kashif Javed and Haroon Atique Babri},
keywords = {Driver activity recognition, Feature extraction, Spatiotemporal features, Driver activity recognition dataset},
abstract = {Detecting hazardous activity during driving can be useful in curbing roadside accidents. Existing techniques utilizing image based features for encoding such activity can sometimes misclassify crucial scenarios. One particular work by Zhao et al. (2013 [1], 2013 [2], 2011 [3]) suggests an image based feature set that encodes the driver’s pose, which is categorized into one of four activities. We bring more clarity in understanding the activity by proposing a richer, video based feature set that adeptly exploits spatiotemporal information of the driver. Our feature set encodes the driver’s pose, crucial variations in pose and interactions with objects within the vehicle. The feature set is tested on our newly created dataset since the ones used in literature are not publicly available. Our proposed feature set captures a larger number of activities and using standard classifiers and benchmarks it has shown significant improvements over the existing ones.}
}
@article{LEE2021103246,
title = {SAF-Nets: Shape-Adaptive Filter Networks for 3D point cloud processing},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103246},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103246},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001619},
author = {Seon-Ho Lee and Chang-Su Kim},
keywords = {Point cloud processing, Shape-adaptive filter, Deep learning},
abstract = {A deep learning framework for 3D point cloud processing is proposed in this work. In a point cloud, local neighborhoods have various shapes, and the semantic meaning of each point is determined within the local shape context. Thus, we propose shape-adaptive filters (SAFs), which are dynamically generated from the distributions of local points. The proposed SAFs can extract robust features against noise or outliers, by employing local shape contexts to suppress them. Also, we develop the SAF-Nets for classification and segmentation using multiple SAF layers. Extensive experimental results demonstrate that the proposed SAF-Nets significantly outperform the state-of-the-art conventional algorithms on several benchmark datasets. Moreover, it is shown that SAFs can improve scene flow estimation performance as well.}
}
@article{WU2021103265,
title = {Component-based metric learning for fully automatic kinship verification},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103265},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103265},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001735},
author = {Huishan Wu and Jiawei Chen and Xiao Liu and Junlin Hu},
keywords = {Kinship verification, Metric learning, Component, Feature combination, Facial image},
abstract = {This paper introduces a fully automatic method for kinship verification from facial images. Recently, a number of methods have been proposed to verify kinship from facial images, however, most of these methods are needed to exactly align face images before feature extraction in a manual manner. Unlike these methods, our method does not depend on face alignment. Firstly, we localize several facial feature points by utilizing a facial feature detector to extract SIFT descriptor around each feature point of a face image. Lastly, two ways, feature combination and distance metric learning, are used to verify the kinship of a pair of face images. For feature combination, three simple strategies of feature combination and support vector machine classifier are used for kinship verification. For metric learning, we propose a component-based metric learning (CML) method to measure the distance of each face pair, which jointly learns multiple local distance metrics, and one specific distance metric for each facial feature point. Experimental results show the effectiveness of our proposed approach on two popular kinship datasets.}
}
@article{XU2021103189,
title = {Generative detect for occlusion object based on occlusion generation and feature completing},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103189},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103189},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001218},
author = {Can Xu and Wenxi Lang and Rui Xin and Kaichen Mao and Haiyan Jiang},
keywords = {Occlusion, Object detection, Feature completing, Generative adversarial networks},
abstract = {Detecting the object with external occlusion has always been a hot topic in computer version, while its accuracy is always limited due to the loss of original object information and increase of new occlusion noise. In this paper, we propose a occluded object detection algorithm named GC-FRCN (Generative feature completing Faster RCNN), which consists of the OSGM (Occlusion Sample Generation Module) and OSIM (Occlusion Sample Inpainting Module). Specifically, the OSGM mines and discards the feature points with high category response on the feature map to enhance the richness of occlusion scenes in the training data set. OSIM learns an implicit mapping relationship from occluded feature map to real feature map adversarially, which aims at improving feature quality by repair the noisy object feature. Extensive experiments and ablation studies have been conducted on four different datasets. All the experiments demonstrate the GC-FRCN can effectively detect objects with local external occlusion and has good robustness for occlusion at different scales.}
}
@article{JIANG2021103142,
title = {Combining Fields of Experts (FoE) and K-SVD methods in pursuing natural image priors},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103142},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103142},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000882},
author = {Feng Jiang and ZhiYuan Chen and Amril Nazir and WuZhen Shi and WeiXiang Lim and ShaoHui Liu and SeungMin Rho},
keywords = {FoE, K-SVD, Adaptive filters, Joint statistical prior, Nature image priors},
abstract = {Natural image prior is one of the most efficient ways to represent images for computer vision tasks. In the literature, filter response statistics prior and synthesis-based sparse representation are two dominant prior models, which have been investigated separately and our knowledge of the relation between these two methods remains limited. In this paper, we examine the inherent relationship between the Fields of Experts (FoE) and K-SVD methods in the pursuit of natural image priors. We theoretically analyze and show that these two prior models have a mutually complementary relationship in the pursuit of the structure of natural images space. Based on these findings, a novel joint statistical prior is proposed, in which adaptive filters are obtained by exploring clues from both priors and utilized to characterize the subtle structure of natural images subspace. Qualitative and quantitative experiments demonstrate that the proposed method achieves a more comprehensive and reliable estimation of natural image prior and is competitive to both alternative and state-of-the-art methods.}
}
@article{PENG2021103207,
title = {Generalized multiple sparse information fusion for vehicle re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103207},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103207},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001334},
author = {Jinjia Peng and Guangqi Jiang and Huibing Wang},
keywords = {Vehicle re-identification, Hierarchical attention network, Multi-views},
abstract = {Vehicle re-identification (reID) aims to search the target vehicle in a non-overlapping multi-camera network, which is important for the intelligent analysis in large scale of surveillance videos. Many existing methods have employed various techniques to achieve discriminative information. However, those methods always focus on the description of one view for the same vehicle images. Hence, a generated multiple sparse information fusion method is proposed for exploiting latent features from multi-views, which employs three different deep networks to extract multiple features from coarse to fine. And these features are regarded as multi-view features. Besides, to fuse these features reasonably, the paper transfers various features into a common space for better seeking distinctive features. Especially, besides ResNet, two feature learning networks are proposed to learn different features, respectively. One is designed to learn robust feature by dropping some features randomly when training the reID model. Another is to combine various salient features from different layers, which forms strong features for the reID task. Moreover, comprehensive experimental results have demonstrated that our proposed method can achieve competitive performances on benchmark datasets VehicleID and VeRi-776.}
}
@article{SINGH2021103241,
title = {Variational optimization based single image dehazing},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103241},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103241},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001577},
author = {Kavinder Singh and Anil Singh Parihar},
keywords = {Image dehazing, Transmission, Atmospheric light, Haze},
abstract = {In this paper, we present a new approach for single image dehazing based on the proposed variational optimization. A hazy image captures the information about haze in terms of the transmission map and object details present in it. We propose to estimate the initial transmission map by performing the structure-aware smoothing of the hazy image. Further, we formulated a variational optimization for the estimation of final transmission, which refines the initial transmission of a hazy image. Atmospheric light can be considered to be constant throughout the scene for practical purposes. The uniform atmospheric light is computed from the dark channel of a hazy image. The exhaustive experimentation shows that the performance of the proposed method is comparable or better.}
}
@article{KUMAR2021103122,
title = {An improved Gamma correction model for image dehazing in a multi-exposure fusion framework},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103122},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103122},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000766},
author = {Avishek Kumar and Rajib Kumar Jha and Naveen K. Nishchal},
keywords = {Image dehazing, Gamma correction model, Multi-exposure fusion},
abstract = {A usual problem encountered during bad weather conditions is the degraded image quality due to haze/fog. In basic Gamma correction method there is always an uncertainty regarding the choice of a particular exponential factor, which improves the quality of the input image because of the nonlinearity involved in the process. This issue has been solved in this study by proposing a modified Gamma correction method, in which the exponential correction factor is varied incrementally to generate images. We also propose the implementation of an automatic image selection criterion for fusion which helps chose images with varied and distinct features. The implementation of the multi-exposure fusion framework is done in the hue-saturation-value color space which has close resemblance with the human vision. The intensity channel of the selected images is fused in the gradient domain which captures minute details and takes an edge as compared to other conventional fusion based methods. The fused saturation channel is obtained by averaging fusion followed by enhancement using a non-linear sigmoid function. The hue channel of the input hazy image is left unprocessed to avoid color distortion. The experimental analysis demonstrates that the proposed method outperforms most of the single image dehazing methods.}
}
@article{LIU2021103220,
title = {S&CNet: A lightweight network for fast and accurate depth completion},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103220},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103220},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001450},
author = {Ziyang Liu and Lei Zhang and Weihai Chen and Xingming Wu and Zhengguo Li},
keywords = {Dense depth completion, Lightweight network, Coarse-to-fine, Attention module},
abstract = {Dense depth completion is essential for autonomous driving and robotic navigation. Existing methods focused on attaining higher accuracy of the estimated depth, which comes at the price of increasing complexity and cannot be well applied in a real-time system. In this paper, a coarse-to-fine and lightweight network (S&CNet) is proposed for dense depth completion to reduce the computational complexity with negligible sacrifice on accuracy. A dual-stream attention module (S&C enhancer) is proposed according to a new finding of deep neural network-based depth completion, which can capture both the spatial-wise and channel-wise global-range information of extracted features efficiently. Then it is plugged between the encoder and decoder of the coarse estimation network so as to improve the performance. The experiments on KITTI dataset demonstrate that the proposed approach achieves competitive result with respect to state-of-the-art works but via an almost four times faster speed. The S&C enhancer can also be easily plugged into other existing works to boost their performances significantly with negligible additional computations.}
}
@article{SHANG2021103185,
title = {A generic, cluster-centred lossless compression framework for joint auroral data},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103185},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103185},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001176},
author = {Kun Shang and Wanqiu Kong and Tan Qu and Zejun Hu and Jiaji Wu and Witold Pedrycz},
keywords = {Joint auroral observation, Lossless compression, Hierarchical clustering, Auroral spectral data, DMSP SSJ5, DMSP SSUSI},
abstract = {Studying the well-known phenomenon “aurora” plays a pivotal role in investigating the solar–terrestrial coupling mechanism. A special auroral spectrograph in Antarctic Zhongshan Station constitutes a auroral observation joint system with satellite-borne sensors of the Defense Meteorological Satellite Program. Multipoint observation by this system provides more essential information for relevant studies than single observation by each instrument, but also results in a multifold increased volume of data that are difficult to be either stored or transmitted. To address this difficulty, we develop a clustering-based, generic lossless data compression framework that combines the usage of various ultimate compressors with a hierarchical clustering algorithm to exert the strength of all the compressors in data reduction. This framework achieves an always-best compression performance for different-sized datasets with a reasonable time consumption, which promises the design of pipelines using it for real-time data transmission.}
}
@article{SALDANHA2021103202,
title = {Performance analysis of VVC intra coding},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103202},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103202},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001292},
author = {Mário Saldanha and Gustavo Sanchez and César Marcon and Luciano Agostini},
keywords = {Versatile video coding, VVC, Intra-frame coding, Performance analysis},
abstract = {This article presents a performance analysis of Versatile Video Coding (VVC) intra-frame prediction. VVC is the next generation of video coding standards, which has been developed to supply the demand of upcoming video applications. VVC brings several innovations and enhancements for the intra-frame prediction to improve the encoding efficiency. These improvements comprise larger block sizes, more flexible block partitioning, more angular intra-frame prediction modes, multiple transform selection, non-separable secondary transform, among others. This article provides a detailed description of these tools, discussing how they work together in the intra-frame coding flow to raise the compression performance. Moreover, this article presents encoding complexity, encoding usage distribution, and rate-distortion-complexity analyses of the intra-frame prediction tools over different quantization scenarios. Based on these analyses, this article provides support for future works focusing on VVC intra-frame coding, including complexity reduction, complexity control, and real-time hardware design.}
}
@article{PUTEAUX2021103085,
title = {A survey of reversible data hiding in encrypted images – The first 12 years},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103085},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103085},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100050X},
author = {Pauline Puteaux and SimYing Ong and KokSheik Wong and William Puech},
keywords = {Multimedia security, Image encryption, Data hiding, Signal processing in the encrypted domain},
abstract = {In the last few years, with the increasing popularity of cloud computing and the availability of mobile smart devices as well as ubiquitous network connections, more and more users are uploading their personal data to remote servers. However, this can lead to significant security breaches, where confidentiality, integrity and authentication are constantly threatened. To overcome these multiple problems, multimedia data must be secured, for example by means of encryption before transmission and storage. In this survey, we look into the issues involved in handling encrypted multimedia data, and more specifically we focus on reversible data hiding in encrypted images (RDHEI). The aim of this survey is to present the birth and evolution of RDHEI methods over the last 12 years. We first highlight different classes and characteristics of RDHEI, then describe representative RDHEI methods. A comparison table is presented to summarize the key features and achievements of each representative RDHEI method considered in this survey. Finally, we share the future outlook of emerging applications and open research topics relevant to RDHEI for the next 12 years and beyond.}
}
@article{GAO2021103148,
title = {Disocclusion filling for depth-based view synthesis with adaptive utilization of temporal correlations},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103148},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103148},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000912},
author = {Pan Gao and Tiantian Zhu and Manoranjan Paul},
keywords = {Depth-image-based-rendering, Gaussian mixture model, Expectation maximization, Foreground depth correlation, Adaptive hole-filling},
abstract = {The depth image-based rendering paves the path to success of 3-D video. However, one issue still remained in 3-D video is how to fill the disocclusion areas. To this end, Gaussian mixture model (GMM) is commonly employed to generate the background, and then to fill the holes. Nevertheless, GMM usually has poor performance for sequences with big foreground reciprocation. In this paper, we aim to enhance the synthesis performance. Firstly, we propose an expectation maximization based GMM background generation method, in which the pixel mixture distribution is derived. Secondly, we propose a refined foreground depth correlation approach, which recovers the background frame-by-frame based on depth information. Finally, we adaptively choose the background pixels from these two methods for filling. Experimental results show that the proposed method outperforms existing non-deep learning based hole filling methods by around 1.1 dB, and significantly surpasses deep learning based alternative in terms of subjective quality.}
}
@article{YANG2021103245,
title = {SWS-DAN: Subtler WS-DAN for fine-grained image classification},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103245},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103245},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001607},
author = {Zhen Yang and Zhipeng Wang and Lingkun Luo and Hongping Gan and Tao Zhang},
keywords = {Fine-grained, Classification, WS-DAN, SWS-DAN, Data augmentation, Loss function},
abstract = {Currently, weakly supervised data augmentation network (WS-DAN) has been proved to be one of the state-of-the-art methods for fine-grained image classification due to its effectiveness on attention-guided data augmentation and bilinear attention pooling. Taking WS-DAN as the backbone, in this paper, we further propose a subtler WS-DAN recognition network, namely, SWS-DAN. Specifically, we first construct a novel “salience-guided data augmentation” scheme composed of cutblock, part-aware cropping, and SCutMix operations, which can more effectively expand the number of training dataset and improve the weakness addressed in the data augmentation procedure of WS-DAN. Meanwhile, the novel data-augmentation manner reduces background noise and mines more discriminative regions simultaneously, thereby avoiding the overfitting. In caring about the key issue in fine-grained image classification task is how to distinguish the extremely similar subclasses (e.g., Artic Tern, Elegant Tern, and Forsters Tern), we then design a “Top-k” loss function that mainly focuses on the similar classes so as to find their extraordinary subtle differences. Extensive experiments carried out on common fine-grained image datasets demonstrate that SWS-DAN can surpass WS-DAN with a significant margin in the classification performance.}
}
@article{FANG2021103256,
title = {Robust multimodal discrete hashing for cross-modal similarity search},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103256},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103256},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100167X},
author = {Yuzhi Fang},
keywords = {Hashing, Robust, Cross-modal retrieval, Unsupervised learning},
abstract = {Hashing technology improves the search efficiency and reduces the storage space of data. However, building an effective modal with unsupervised cross modal retrieval and generating efficient binary code is still a challenging task, considering of some issues needed to be further discussed and researched for unsupervised multimodal hashing. Most of the existing methods ignore the discrete restriction, and manually or experientially determine the weights of each modality. These limitations may significantly reduce the retrieval accuracy of unsupervised cross-modal hashing methods. To solve these problems, we propose a robust hash modal that can efficiently learn binary code by employing a flexible and noise-resistant ℓ2,1-loss with nonlinear kernel embedding. In addition, we introduce an intermediate state mapping that facilitate later modal optimization to measure the loss between the hash codes and the intermediate states. Experiments on several public multimedia retrieval datasets validate the superiority of the proposed method from various aspects.}
}
@article{SANYAL2021103190,
title = {Who will receive the ball? Predicting pass recipient in soccer videos},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103190},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103190},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001206},
author = {Samriddha Sanyal},
keywords = {Pass prediction, Dependent models, Soccer dataset},
abstract = {The game of soccer involves an act of one team trying to score a goal against the other. During the game, defending players constantly try to predict the pass of the attacking player to prevent a goal. So, pass prediction is an important facet to anticipate the game strategy of participating teams. Here we present a probabilistic framework for pass prediction. Aberrating the state-of-the-art notion of mutually independent decision models, the proposed framework predicts pass recipients by integrating two dependent models, designed from the coordinates of the players in abstract top-view visualization. To evaluate the real time efficacy of the proposed pass prediction framework, a soccer data set has been introduced. The proposed pass prediction algorithm is compared against recent methods and the ground truth available in the soccer data set. The proposed method outperforms the existing approaches by a noticeable margin.}
}
@article{AMSAPRABHAA2021103218,
title = {A survey on spatio-temporal framework for kinematic gait analysis in RGB videos},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103218},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103218},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001449},
author = {M. Amsaprabhaa and Y. {Nancy Jane} and H. {Khanna Nehemiah}},
keywords = {Human gait recognition, Spatio-temporal features, Gait databases, Gait recognition representation, Kinematic joint points, Gait prediction},
abstract = {Human gait recognition from videos is one of the promising research topics for analyzing human walking behavior. Spatio-temporal features and kinematics interesting points (three dimensional skeleton points) are the two key metrics in the gait examination. In general, input to gait recognition methods is categorized into 3 groups namely; two dimensional video-based, depth image-based and three dimensional (3D) skeleton-based methods. This work aims to present a survey on spatio-temporal and kinematic gait characteristics based on visual and 3D skeletal traits in RGB videos. A detailed insight on the various benchmarked gait databases, gait recognition representations based on model-based, model-free approaches and classifiers are presented in this review. Also, this paper investigates the performance metrics, application areas and covariate factors that influence the gait recognition process. Finally, the paper outlines the future perspective of gait recognition system based on kinematic joint points.}
}
@article{LI2021103206,
title = {Semantic meaning modulates object importance in human fixation prediction},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103206},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103206},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001322},
author = {Aoqi Li and Zhenzhong Chen},
keywords = {Visual attention, Image saliency, Semantic attributes, Object importance},
abstract = {Humans tend to allocate attention to semantic entities. Objects are important in fixation selection, but not all the objects are equally attractive. In this paper, we introduce the concept of attribute bias to characterize the influence of semantic attributes compared with low-level saliency on fixation distribution. Two different ways are adopted to get two sets of semantic attributes. In both cases, most semantic attributes have a positive influence on drawing attention and contribute more than low-level saliency in object areas. We also find that attribute bias is robust to low-level saliency and can consistently reflect the relative attractiveness of objects with different semantic attributes. It is demonstrated that such bias helps make better fixation predictions by distinguishing the importance of objects, although low-level saliency models with better performance are less dramatically improved by attribute bias. These findings indicate the role of conceptual meaning as opposed to features in visual attention.}
}
@article{JI2021103238,
title = {Superpixel alpha-expansion and normal adjustment for stereo matching},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103238},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103238},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001565},
author = {Penglei Ji and Jie Li and Hanchao Li and Xinguo Liu},
keywords = {Stereo matching, Superpixel, Alpha-expansion, Graph cuts, Normal adjustment},
abstract = {This paper presents a continuous stereo disparity estimation method based on superpixel segmentation and graph-cuts. We re-parameterize the disparity with a 3D tangent plane, and propose two algorithms to optimize the Markov Random Field (MRF) energy. The first algorithm, called superpixel α-expansion, is built on superpixel segmentation to localize the label proposal and the expansion scope. Three levels of superpixels with increasing granularity are generated for acceleration. The second algorithm, called normal adjustment, optimizes the 3D planes for the regions with low texture and/or illumination changes. The normal adjustment is performed along a depth-first similarity path of superpixels. We evaluate our method on the Middlebury 3.0 evaluation benchmark and the Eth3d benchmark. Experimental results show that our method achieves high accuracy on both evaluation benchmarks. (Middlebury 3.0 evaluation benchmark: http://vision.middlebury.edu/stereo/eval3/. Eth3d benchmark: https://www.eth3d.net/low_res_two_view.)}
}
@article{DENG2021103121,
title = {Diverse Features Fusion Network for video-based action recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103121},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103121},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000754},
author = {Haoyang Deng and Jun Kong and Min Jiang and Tianshan Liu},
keywords = {Three-stream action recognition, Diverse features fusion, DIverse Compact Bilinear, Channel-spatial Attention},
abstract = {The two-stream convolutional network has been proved to be one milestone in the study of video-based action recognition. Lots of recent works modify internal structure of two-stream convolutional network directly and put top-level features into a 2D/3D convolution fusion module or a simpler one. However, these fusion methods cannot fully utilize features and the way fusing only top-level features lacks rich vital details. To tackle these issues, a novel network called Diverse Features Fusion Network (DFFN) is proposed. The fusion stream of DFFN contains two types of uniquely designed modules, the diverse compact bilinear fusion (DCBF) module and the channel-spatial attention (CSA) module, to distill and refine diverse compact spatiotemporal features. The DCBF modules use the diverse compact bilinear algorithm to fuse features extracted from multiple layers of the base network that are called diverse features in this paper. Further, the CSA module leverages channel attention and multi-size spatial attention to boost key information as well as restraining the noise of fusion features. We evaluate our three-stream network DFFN on three public challenging video action benchmarks: UCF101, HMDB51 and Something-Something V1. Experiment results indicate that our method achieves state-of-the-art performance.}
}
@article{DJERIDA2021103258,
title = {Development of scale and illumination invariant feature detector with application to UAV attitude estimation},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103258},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103258},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001681},
author = {Achraf Djerida and Zhonghua Zhao and Jiankang Zhao},
keywords = {Feature detection, Scale invariance, Illumination invariance, UAV attitude estimation},
abstract = {Feature detection has great importance in many applications such as vision navigation. Examining the developed detectors, it is found in many recent studies that most of the scale-invariant detectors are sensitive to illumination. In this work, we propose a novel detector that has good robustness to both scale and illumination. Motivated by the good robustness of Log-Gabor kernels toward light changes, we employ these kernels as a basis to construct the scale space. To detect potential features, we develop an effective interest points measure which is motivated by the concept of the autocorrelation and Hessian matrices. To confirm the good performance of our detector, we hold experiments on many datasets and with comparisons to common state-of-the-art methods. Furthermore, we evaluate the saliency of the detected features on a UAV attitude estimation task.}
}
@article{GUERMAZI2021103183,
title = {Facial micro-expression recognition based on accordion spatio-temporal representation and random forests},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103183},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103183},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001164},
author = {Radhouane Guermazi and Taoufik {Ben Abdallah} and Mohamed Hammami},
keywords = {Facial micro-expressions, Pyramid levels’ combination, Accordion representation, Low-dimensional feature space, Random Forests, Proximity measure},
abstract = {Micro-expressions are very brief involuntary facial expressions which appear on the face of humans when they unconsciously conceal an emotion. Creating a solution allowing an automatic recognition of the facial micro-expressions from video sequences has garnered increasing attention from experts across such different disciplines as computer science, security, and psychology. This paper offered a solution to facial micro-expressions recognition, based on accordion spatio-temporal representation and Random Forests. The proposed feature space, called “Uniform Local Binary Patterns on an Accordion 2D representation of sub-regions presented by a Pyramid of levels (LBPAccPu2)”, exploits the effectiveness of uniform LBP patterns applied on an accordion representation of sub-regions at different sizes. Random Forests were used to select the most discriminating features and reduce the classification ambiguity of similar micro-expressions through a new proximity measure. The main objective of our paper was to demonstrate that the use of few features could be more efficient to produce a strong micro-expression recognition classifier that outperforms the approaches that rely on high dimensional features space. The experimental results across six micro-expression datasets show the effectiveness of the proposed solution with an accuracy rate that can reach 81.38% on CasmeII dataset. Compared to some famous competitive state-of-the-art approaches, the proposed solution proved its performance thanks to its accuracy rate as well as the number of features it uses.}
}
@article{TAMBE2021103141,
title = {Deep multi-feature learning architecture for water body segmentation from satellite images},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103141},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103141},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000870},
author = {Rishikesh G. Tambe and Sanjay N. Talbar and Satishkumar S. Chavan},
keywords = {Convolutional neural network, Deep learning, Refinement modules, Satellite image analysis, Water body extraction},
abstract = {Automatic water body extraction from satellite images of various scenes is a classical and challenging task in remote sensing and image interpretation. Convolutional neural network (CNN) has become prominent option for performing image segmentation task in remote sensing applications. However, CNN-based networks have non-trivial issues for segmenting such as: (1) blurring boundary pixels; (2) large number of trainable parameters; and (3) huge number of training samples. In this paper, we propose an end-to-end multi-feature based CNN architecture, called as W-Net, to perform water body segmentation. W-Net consists of contracting/expanding networks and inception layers. W-Net takes advantage of contracting network to capture context information while localization is achieved with expanding network. With these networks, W-Net is able to train on less number of images and extract water pixels accurately. Use of inception layers reduces computational burden within the network by decreasing total number of trainable parameters. W-Net incorporated two refinement modules to enhance predicted results which mitigate blurring effect and to inspect continuity of boundary pixels. Dataset consisting 2671 images with manually annotated ground truths are built to validate performance and effectiveness of our proposed method. In addition, we evaluated our method on crack detection dataset where W-Net achieved competitive performance with Deepcrack. W-Net accomplished excellent performance on the water body dataset (I∕U=0.9434 and F−score=0.9509).}
}
@article{WANG2021103201,
title = {LRGAN: Visual anomaly detection using GAN with locality-preferred recoding},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103201},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103201},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001280},
author = {Jianzhu Wang and Wei Huang and Shengchun Wang and Peng Dai and Qingyong Li},
keywords = {Visual anomaly detection, GAN, Locality, Recoding},
abstract = {Deep neural networks, including deep auto-encoder (DAE) and generative adversarial networks (GAN), have been extensively applied for visual anomaly detection. These models generally assume that reconstruction errors should be lower for normal samples but higher for anomalies. However, it has been found that DAE based models can sometimes reconstruct anomalies very well and thus result in false alarms or misdetections. To address this problem, we propose a model using GAN with locality-preferred recoding, named LRGAN. LRGAN is inspired by the observation that both normal and abnormal samples are not completely scattered throughout the latent space but clustered separately at some local regions. Therefore, a locality-preferred recoding (LR) module is designed to compulsively represent the latent vectors of anomalies by normal ones. As a result, reconstructions of anomalies will approximate to normal samples and corresponding residuals can thus be enlarged. To partly avoid latent vectors of normal samples being recoded, we further present an improved model using GAN with an adaptive LR (ALR) module, named LRGAN+. ALR applies the clustering algorithm to generate a more compact codebook; more importantly, it helps LRGAN + automatically skip the LR module for possible normal samples with a threshold strategy. Our proposed method is evaluated on two public datasets (i.e., MNIST and CIFAR-10) and one real-world industrial dataset (i.e., Fasteners), considering both one-class and multi-class anomaly detection protocols. Experimental results demonstrate that LRGAN is comparable with state-of-the-art methods and LRGAN + outperforms these methods on all datasets.}
}
@article{WANG2021103260,
title = {An Interconnected Feature Pyramid Networks for object detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103260},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103260},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100170X},
author = {Qiang Wang and Lukuan Zhou and Yuncong Yao and Yong Wang and Jun Li and Wankou Yang},
keywords = {Attention mechanism, Feature Pyramid Networks, Object detection, Deep learning},
abstract = {Although deep learning makes major breakthroughs in object detection, object detection still faces several limitations listed as follows: (1) Many works underplay the feature selection, leading to the resulting key features are not prominent enough and prone to noise; (2) Many works pass back features in a layer-by-layer manner to achieve multi-scale features. However, as the distance of layers from each other increases, the semantics are diluted, and the transfer of information between layers becomes difficult. To overcome these problems, we propose a new Interconnected Feature Pyramid Networks (IFPN) for feature enhancement. It can simultaneously select attentive features through the attention mechanism and realize the free flow of information. On the basis of the improvements, we design a new IFPN Detector. Experiments on COCO dataset and Smart UVM dataset show that our method can bring a significant improvement.}
}
@article{ZHONG2021103138,
title = {Attention-guided image captioning with adaptive global and local feature fusion},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103138},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103138},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000869},
author = {Xian Zhong and Guozhang Nie and Wenxin Huang and Wenxuan Liu and Bo Ma and Chia-Wen Lin},
keywords = {Image captioning, Encoder-decoder, Spatial information, Adaptive attention},
abstract = {Although attention mechanisms are exploited widely in encoder-decoder neural network-based image captioning framework, the relation between the selection of salient image regions and the supervision of spatial information on local and global representation learning was overlooked, thereby degrading captioning performance. Consequently, we propose an image captioning scheme based on adaptive spatial information attention (ASIA), extracting a sequence of spatial information of salient objects in a local image region or an entire image. Specifically, in the encoding stage, we extract the object-level visual features of salient objects and their spatial bounding-box. We obtain the global feature maps of an entire image, which are fused with local features and the fused features are fed into the LSTM-based language decoder. In the decoding stage, our adaptive attention mechanism dynamically selects the corresponding image regions specified by an image description. Extensive experiments conducted on two datasets demonstrate the effectiveness of the proposed method.}
}
@article{XU2021103119,
title = {Detecting facial manipulated videos based on set convolutional neural networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103119},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103119},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000742},
author = {Zhaopeng Xu and Jiarui Liu and Wei Lu and Bozhi Xu and Xianfeng Zhao and Bin Li and Jiwu Huang},
keywords = {Digital video forensics, Deepfake, Set convolutional neural network, Set reduce},
abstract = {With the boom of artificial intelligence, facial manipulation technology is becoming more simple and more numerous. At the same time, the technology also has a large and profound negative impact on face forensics, such as Deepfakes. In this paper, in order to aggregate multiframe features to detect facial manipulation videos, we solve facial manipulated video detection from set perspective and propose a novel framework based on set, which is called set convolutional neural network (SCNN). Three instances of the proposed framework SCNN are implemented and evaluated on the Deepfake TIMIT dataset, FaceForensics++ dataset and DFDC Preview datset. The results show that the method outperforms previous methods and can achieve state-of-the-art performance on both datasets. As a perspective, the proposed method is a fusion promotion of single-frame digital video forensics network.}
}
@article{BONOMI2021103239,
title = {Dynamic texture analysis for detecting fake faces in video sequences},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103239},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103239},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001553},
author = {Mattia Bonomi and Cecilia Pasquini and Giulia Boato},
keywords = {Manipulated videos, Deepfakes, Video forensics, Local Derivative Patterns},
abstract = {The creation of manipulated multimedia content involving human characters has reached in the last years unprecedented realism, calling for automated techniques to expose synthetically generated faces in images and videos. This work explores the analysis of spatio-temporal texture dynamics of the video signal, with the goal of characterizing and distinguishing real and fake sequences. We propose to build a binary decision on the joint analysis of multiple temporal segments and, in contrast to previous approaches, to exploit the textural dynamics of both the spatial and temporal dimensions. This is achieved through the use of Local Derivative Patterns on Three Orthogonal Planes (LDP-TOP), a compact feature representation known to be an important asset for the detection of face spoofing attacks. Experimental analyses on state-of-the-art datasets of manipulated videos show the discriminative power of such descriptors in separating real and fake sequences, and also identifying the creation method used. Linear Support Vector Machines (SVMs) are used which, despite the lower complexity, yield comparable performance to previously proposed deep models for fake content detection.}
}
@article{LEE2021103144,
title = {Subpixel rendering for diamond-shaped PenTile displays using patch-based adaptive filters},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103144},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103144},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000900},
author = {Jae-Han Lee and Kyung-Rae Kim and Chang-Su Kim},
keywords = {Subpixel rendering, Diamond-shaped PenTile displays, Color distortion reduction},
abstract = {We propose a novel subpixel rendering algorithm for diamond-shaped PenTile displays, which reduces color distortions while improving apparent resolutions. We develop two types of subpixel rendering filters: main filter and color distortion reduction (CDR) filters. To derive the filters, we formulate a quadratic program to minimize the difference between an original input image and a virtual image that the human visual system perceives. By imposing two constraints for filter size and coefficients, we obtain the main filter, which has a suitable size and is normalized. Then, we design the CDR filters based on the analysis of various patch patterns for image areas. We define the patch patterns to classify local areas with possible color distortions. By imposing additional constraints according to the patch patterns, we derive the CDR filters. Lastly, by matching local areas in the input image into the pre-defined patch patterns, we render the image using the main filter and the CDR filters, which are applied adaptively to the local areas. Experimental results demonstrate that the proposed subpixel rendering algorithm improves apparent resolutions and suppresses color distortions effectively, thereby outperforming conventional algorithms.}
}
@article{DING2021103200,
title = {A CNN model for real time hand pose estimation},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103200},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103200},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001279},
author = {Lu Ding and Yong Wang and Robert Laganière and Dan Huang and Shan Fu},
keywords = {Hand pose estimation, CNN, Model-based, Fine-tuning},
abstract = {Recently convolutional neural networks (CNNs) have been employed to address the problem of hand pose estimation. In this work, we introduce an end-to-end deep architecture that can accurately estimate hand pose through the joint use of model-based and fine-tuning methods. In the model-based stage, we make use of the prior information in hand model geometry to ensure the geometric validity of the estimated poses. Next, we introduce a fine-tuning approach that learns to refine the errors between the model and observed hand. Our approach is validated on three challenging public datasets and achieves state-of-the-art performance.}
}
@article{OPPONGBEDIAKO2021103204,
title = {Joint model of gradient magnitude and Gabor features via Spatio-Temporal slice},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103204},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103204},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001310},
author = {Daniel {Oppong Bediako} and Xuanqin Mou},
keywords = {Full-reference (FR) video quality assessment, Gradient magnitude, Gabor filter},
abstract = {To form a high-performance video quality predictor, we developed a framework for full-reference (FR) video quality assessment that integrates Spatio-temporal slice analysis (STS) to create a high-performance predictor of video quality. However, both gradient and Gabor are spatial–temporal structural capturers used for the simultaneous extraction of both spatial and temporal features. In this paper, we proposed a novel VQA algorithm via a joint model of gradient magnitude and Gabor features (JMG) between the STS images of the reference videos and their distorted counterparts to assess the degradation of video quality effectively. Firstly, gradient magnitude and the Gabor filter were constructed to extract the spatiotemporal features of the video sequence. However, the two-feature model combined to predict the perceptual quality of frames. This new proposed VQA model is known as the horizontal and time STS (HT-JMG) model. To further investigate the influence of spatial dissimilarity, we combined the frame-by-frame spatial T-JMG(S) factor with the HT-JMG and propose another VQA model, called the time, horizontal, and vertical STS (THV-JMG) model. Finally, the results of the experiment showed that the proposed method has a strong correlation with subjective perception and is competitive with state-of-the-art full reference VQA models.}
}
@article{ZHAO2021103098,
title = {Image encryption using linear weighted fractional-order transform},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103098},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103098},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000584},
author = {Tieyu Zhao and Lin Yuan and Yingying Chi},
keywords = {Fractional-order Fourier transform, Weighted fractional-order transform, Image encryption},
abstract = {As the linear weighted fractional-order Fourier transform (LWFRFT), an extension of the Fourier transform, has been widely studied, many linear weighted fractional-order transforms (LWFRTs) have been proposed consequently. Our research shows that the LWFRT has limitations when applied to image encryption. For example, its application to image encryption leads to the security risks of key invalidation. In this paper, we propose a new reformulation of the LWFRT which establishes the relation between many fractional-order transforms. With the help of the new reformulation, we point out the limitations of the LWFRT and analyze the reasons for key invalidation in image encryption. Finally, numerical simulation verifies our perspective.}
}
@article{TSAI2021103065,
title = {Air-writing recognition using reverse time ordered stroke context},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103065},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103065},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000341},
author = {Tsung-Hsien Tsai and Jun-Wei Hsieh and Chuan-Wang Chang and Chin-Rong Lay and Kuo-Chin Fan},
keywords = {Air writing recognition, Backward time-order stroke representation, 3D-sensor, Gesture-based interaction},
abstract = {Air-writing is a new human and smart device communication approach, permits users to write inputs in a natural and relentless way. This touch-less way can prevent users fromvirus infection such as COVID-19. Compared with othermethods, air writing ismore challenging due to its unique characteristics such as redundant lifting strokes, multiplicity (different writing styles from various users), and confusion (different character types written in air are similar). Without the need of any starting trigger, a novel reverse time-ordered algorithm is proposed in this paper toefficiently filter out unnecessary lifting strokes, and thus simplifies the matching procedure. As to the second and third issues, a tiered arrangement structure is proposed by sampling the air-writing results with various sampling rates to solvethe multiplicity and confusion problems. Analyzed with other recently proposed air writing algorithms, the proposed approach reaches satisfactory recognition accuracy (above 94%) without any starting triggers.}
}
@article{ZHOU2021103182,
title = {A directional margin paradigm for noise suppression in face recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103182},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103182},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001152},
author = {Yang Zhou and Xun Gong and Peng Yang},
keywords = {Face recognition, Margin paradigm, Loss function, Noisy labels, Two-step learning},
abstract = {Convolutional neural networks (CNN) have achieved outstanding face recognition (FR) performance with increasing large-scale face datasets. With face dataset size grown, noisy data will inevitably increase, undoubtedly bringing difficulties to data cleaning. In this paper, the probability that the sample belongs to noise can be determined based on the cosine distance (cosθ) of normalized angle center and face feature vector in the margin-based loss functions. According to this finding, we propose a two-step learning method integrated into the loss function. The new proposed directional margin loss function combines the noise probability with the label as the supervision information. Experiments show that our method can tolerate noisy data and get high FR accuracy when the training datasets mix with more than 30% noise. Our approach can also achieve a great result of 79.33% in MegaFace challenge one using a noisy training dataset.}
}
@article{SUBRAMANIAN2021103132,
title = {A deep genetic algorithm for human activity recognition leveraging fog computing frameworks},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103132},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103132},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000857},
author = {R. Raja Subramanian and V. Vasudevan},
keywords = {Deep genetic algorithm, Human activity recognition, Fog computing, Ambulatory healthcare},
abstract = {With modern e-healthcare developments, ambulatory healthcare has become a prominent requirement for physical or mental ailed, elderly, childhood people. One of the major challenges in such applications is timing and precision. A potential solution to this problem is the fog-assisted cloud computing architecture. The activity recognition task is performed with the hybrid advantages of deep learning and genetic algorithms. The video frames captured from vision cameras are subjected to the genetic change detection algorithm, which detects changes in activities of subsequent frames. Consequently, the deep learning algorithm recognizes the activity of the changed frame. This hybrid algorithm is run on top of fog-assisted cloud framework, fogbus and the performance measures including latency, execution time, arbitration time and jitter are observed. Empirical evaluations of the proposed model against three activity data sets shows that the proposed deep genetic algorithm exhibits higher accuracy in inferring human activities as compared to the state-of-the-art algorithms.}
}
@article{WANG2021103203,
title = {Privacy-preserving reversible data hiding based on quad-tree block encoding and integer wavelet transform},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103203},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103203},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001309},
author = {Xu Wang and Ching-Chun Chang and Chia-Chen Lin and Chin-Chen Chang},
keywords = {Reversible data hiding, Encrypted images, Quad-tree partition, IWT},
abstract = {Applications on the cloud server have matured, and protecting the privacy of the content owner has attracted more attention. Privacy-Preserving Reversible data hiding (PP-RDH) is an efficient technique for embedding additional data into an encrypted image. In this paper, we propose a privacy-preserving reversible data hiding scheme using the quad-tree partition and Integer Wavelet Transform (IWT) techniques. Our scheme focuses on improving the embedding rate and quality of the recovered image when a 2 × 2-sized, block-based image encryption method is applied to ensure relative higher security. On this basis, the IWT technique transforms the encrypted image, and coefficients in three high frequency subbands are converted into 8-bit binary system. Then, the quad-tree partition technique encodes each 8 × 8-sized coefficient block, since there are many zeroes in the front bit planes. The experimental results indicated that our proposed scheme significantly improved the embedding rate, and guaranteed lossless image recovery and data extraction.}
}