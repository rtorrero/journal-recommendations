@article{HU2019292,
title = {A new method of Thangka image inpainting quality assessment},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {292-299},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.045},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303705},
author = {Wenjin Hu and Yuqi Ye and Fuliang Zeng and Jiahao Meng},
keywords = {Image inpainting quality assessment, Structural symmetry, Thangka image, Harris corner},
abstract = {In order to solve the problem of Thangka image inpainting quality assessment (IIQA) and existing quality evaluation methods are not suitable for inpainting Thangka image, this paper proposes a new non-reference quality evaluation method which can effectively solve this problem. Firstly, due to lack of original Thangka image, the proposed method using symmetry of Thangka images to predicted an undamaged image. Secondly, we extract harries corner in Thangka inpainting images to show the structural feature of total graph. Thirdly, demonstrate subjective evaluation score of inpainting Thangka image by caparison difference of structural feature between inpainting and predicted original area. Finally, in order to compensate the lack of Thangka images in existing database, we add Generative Adversarial Nets (GANs) to generate large number of available image. Experiments show that our proposed method generates a state-of-the-art index for Thangka image inpainting quality which correlated with human vision.}
}
@article{DOSSANTOS2019407,
title = {Generalization of feature embeddings transferred from different video anomaly detection domains},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {407-416},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.035},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300926},
author = {Fernando P. {dos Santos} and Leonardo S.F. Ribeiro and Moacir A. Ponti},
keywords = {Video, Transfer learning, Feature generalization, Anomaly detection},
abstract = {Detecting anomalous activity in video surveillance often suffers from limited availability of training data. Transfer learning may close this gap, allowing to use existing annotated data from some source domain. However, analyzing the source feature space in terms of its potential for transfer of learning to another context is still to be investigated. This paper reports a study on video anomaly detection, focusing on the analysis of feature embeddings of pre-trained CNNs with the use of novel cross-domain generalization measures that allow to study how source features generalize for different target video domains. This generalization analysis represents not only a theoretical approach, can be useful in practice as a path to understand which datasets allow better transfer of knowledge. Our results confirm this, achieving better anomaly detectors for video frames and allowing analysis of transfer learning’s positive and negative aspects.}
}
@article{TU2019253,
title = {Deep feature representation for anti-fraud system},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {253-256},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.031},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300409},
author = {Bing Tu and Danbing He and Yongheng Shang and Chengle Zhou and Wujing Li},
keywords = {Convolutional neural network, Anti-fraud, Distance metric},
abstract = {Online payment is becoming popular due to the development of e-commerce. So payment safety is more important. Since there is a fraudulent situation, an anti-fraud system is indispensable. GMM was leveraged in many anti-fraud applications, but it only takes positive sample into account. Convolutional neural network is a strong strategy for learning deep representation of samples. So in this paper, we propose a CNNs architecture to deal with this problem. And the distance metric method can effectively identify whether candidates are the same person. Experimental results show the effectiveness of our method.}
}
@article{DU2019347,
title = {An efficient privacy protection scheme for data security in video surveillance},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {347-362},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.027},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300367},
author = {Ling Du and Wei Zhang and Huazhu Fu and Wenqi Ren and Xinpeng Zhang},
keywords = {Privacy protection, Data security, Video surveillance},
abstract = {The advancement in video surveillance has raised significant concerns about privacy protection. The existing methods focus on identifying the sensitive region and preserving the behavior of the target, however, they ignore the recoverability of private content. In this paper, we propose a novel and efficient privacy protection scheme for data security in video surveillance, which jointly addresses several key challenges, including de-identification, behavior preservation, recoverability, and compressibility in one unified system. Our method constructs a public stream and a private residual error stream by blurring the private sensitive region. With our scheme, ordinary users could recognize the behaviors in the public identity-protected video stream for surveillance purpose, while authorized users are able to access the recovered private content (e.g., for law investigations). Moreover, the compressed privacy protected region and residual error could be able to save the costs associated with transmission and storage. The extensive experiments on two standard surveillance datasets and a user study demonstrate the effectiveness of our privacy protection system.}
}
@article{KONG2019537,
title = {Collaborative multimodal feature learning for RGB-D action recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {537-549},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930063X},
author = {Jun Kong and Tianshan Liu and Min Jiang},
keywords = {RGB-D action recognition, Multimodal data, Max-margin learning framework, Supervised matrix factorization},
abstract = {The emergence of cost-effective depth sensors opens up a new dimension for RGB-D based human action recognition. In this paper, we propose a collaborative multimodal feature learning (CMFL) model for human action recognition from RGB-D sequences. Specifically, we propose a robust spatio-temporal pyramid feature (RSTPF) to capture dynamic local patterns around each human joint. The proposed CMFL model fuses multimodal data (skeleton, depth and RGB), and learns action classifiers using the fused features. The original low-level feature matrices are factorized to learn shared features and modality-specific features under a supervised fashion. The shared features describe the common structures among the three modalities while the modality-specific features capture intrinsic information of each modality. We formulate shared-specific features mining and action classifiers learning in a unified max-margin framework, and solve the formulation using an iterative optimization algorithm. Experimental results on four action datasets demonstrate the efficacy of the proposed method.}
}
@article{LIU20191,
title = {Recognition and extraction of named entities in online medical diagnosis data based on a deep neural network},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {1-15},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300525},
author = {Xin Liu and Yanju Zhou and Zongrun Wang},
keywords = {Continuous bags of word clusters, Deep neural network, Online medical diagnosis data, Named entities},
abstract = {How to correctly recognize and extract named entities, such as disease names, medical measurements and therapies, from online medical diagnosis data remains challenging. For the one hand, conventional natural language processing methods cannot be directly applied in the field of online medical diagnosis (OMD). Although existing supervised or unsupervised learning algorithms have offered strategies for OMD on open websites, such methods might extensively rely on specific knowledge sources or manually designed features. For the other hand, due to the large size of the data and the sophistication of the data structure, it is difficult to establish a robust NLP model for recognition and extraction of clinical named entities in paragraphs. Therefore, in the paper, we try to establish a new deep neural network (DNN), combine the bi-directional long short-term memory (Bi-LSTM) and the conditional random field (CRF), and utilize online medical diagnosis data for recognition and extraction of clinical named entities. Different from existing artificial neural networks (ANN), the proposed neural network well functions even without manual rules or features. The word representations input into the DNN is connected by character-based representations and continuous bags of word clusters (CBOWC) in an embedding way. We test the new DNN on different online medical diagnosis datasets obtained from a scalable web crawler, and compare it with the long short-term memory (LSTM) neural network linguistic model, the convolutional neural network (CNN) model and the most advanced Bi-LSTM-CRF. The fundamentals for the method selection is that they have been suggested to generate acceptable results. According to the comparison analyses, the DNN is proved to be a reliable tool and improves every benchmark performance of OMD.}
}
@article{WANG201976,
title = {A K-anonymous clustering algorithm based on the analytic hierarchy process},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {76-83},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.052},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303791},
author = {Kun Wang and Wei Zhao and Junjie Cui and Yanpeng Cui and Jianwei Hu},
keywords = {Clustering algorithm, Analytic hierarchy process},
abstract = {To protect the privacy of users, tables generally must be anonymized before publication. All existing anonymous methods have deficiencies. They do not consider the differences in attributes, or the optimization of information loss and time efficiency. his paper proposes a new method called KACM to realize k-anonymity. This method is mainly used for hybrid tables. The calculation of the distance between records considers the connection between quasi-identifier attributes and sensitive attributes, their effect on the sensitive privacy, and the information loss during the anonymity process. In the clustering process, the records with the minimum distance are always selected to add, and the clustering is individually controlled according to k to realize the equalization division of the equivalence class and reduce the total amount of distance calculation. Finally, the validity and practicability of the method are proved using theory and experiment.}
}
@article{CAI2019141,
title = {Research on image processing of intelligent building environment based on pattern recognition technology},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {141-148},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301063},
author = {Wei Cai and Xiaodong Wen and Qiu Tu and Xiujuan Guo},
keywords = {Intelligent building, Image processing, Satellite imagery, Building contours, Edge detection},
abstract = {With the continuous development of urbanization, urban population, economy and other factors have a close impact on the geometry and distribution of urban buildings. Obtaining information of urban buildings from aerial images or satellite images quickly and accurately is not only conducive to updating geospatial data, but also of great significance for effective monitoring of new thematic information such as new buildings. Moreover, in recent years, the research and improvement of building recognition and contour extraction algorithms based on satellite images or aerial images are helpful to the recognition and classification of urban buildings. It is of great significance to the acquisition of GIS data, the understanding of images, large-scale mapping and many other applications of remote sensing data. With the development of artificial intelligence and computer technology, the image processing of intelligent building environment based on pattern recognition technology has become an important research direction in the field of intelligent building image recognition. Based on the concept, principle and technology analysis of pattern recognition technology, this paper studies the application of pattern recognition technology in the image processing of intelligent building environment. In this paper, based on image processing of intelligent building as the basic theoretical platform, with the pattern recognition technology as the basic research means, three problems of image processing, image extraction and image recognition in image processing of building intelligent environment are studied respectively, and corresponding reasonable solutions are put forward.}
}
@article{ZHANG2019425,
title = {Research on the size of mechanical parts based on image recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {425-432},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.035},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300410},
author = {Laigang Zhang and Qili Yang and Qun Sun and Deying Feng and Ying Zhao},
keywords = {Image recognition, Machine vision, Size measurement, Mechanical parts},
abstract = {Because the image measurement technology based on machine vision has the advantages of high accuracy, high efficiency and non-contact measurement, this kind of measurement technology has gradually become the focus of attention in industrial production measurement and detection. Based on the analysis of image measurement technology, this paper studies the measurement method of mechanical parts size based on image recognition and improves related algorithms. Specific research work is as follows: Design a measurement method of machine parts size based on image recognition, study and analyze the formation of noise, types and corresponding denoising technology, select a fast median filtering algorithm to achieve filtering. Polynomial interpolation is applied to the sub-pixel edge location method to extract the edges accurately. Some classical operators are studied and analyzed with the specific part image to be tested as the experimental object. Several classical operators are compared and analyzed through many experiments. Experiments show that the improved morphological gradient operator can effectively refine the image edge. The experimental scheme proposed in this paper can better realize the measurement of mechanical parts size, and the improved algorithm has significantly improved the accuracy than before.}
}
@article{YUAN2019527,
title = {A multi-image Joint Re-ranking framework with updateable Image Pool for person re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {527-536},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.041},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300495},
author = {Mingyue Yuan and Dong Yin and Jinwen Ding and Zhipeng Zhou and Chengfeng Zhu and Rui Zhang and An Wang},
keywords = {Person re-identification, Image pool, Multi-shot, Re-ranking},
abstract = {Real-world video surveillance has increasing demand for person re-identification. Existing multi-shot works usually aggregate single sample features by computing the average features or using time series model. The Multi-image Joint Re-ranking framework with updateable Image Pool that we are proposing will give a different approach. First, we defined the term ‘Image Pool’ to store image samples for each pedestrian. Next, the updating rules of Image Pool has been defined in order to optimize the representativeness of it. Second, we compute initial ranking lists of every sample in Image Pool, and propose the ‘Multiple-image Joint Re-ranking’ algorithm to aggregate initial ranking lists. We calculate the rank score of partial elements of initial ranking lists. In the end, we get final ranking list by ascending the order of the rank scores. We validated our re-ranking results on Market-1501, iLIDS-VID, PRID-2011 and our ITSD datasets, and the results outperform other methods.}
}
@article{FOUMANI2019195,
title = {A probabilistic topic model using deep visual word representation for simultaneous image classification and annotation},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {195-203},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300094},
author = {Seyed Navid Mohammadi Foumani and Ahmad Nickabadi},
keywords = {Image classification and annotation, Topic models, Probabilistic model, Deep learning, Convolutional neural network, LLC},
abstract = {Researches have shown that holistic examination of an image provides better understanding of the image compared to separate processes each devoted to a single task like annotation, classification or segmentation. During the past decades, there have been several efforts for simultaneous image classification and annotation using probabilistic or neural network based topic models. Despite their relative success, most of these models suffer from the poor visual word representation and the imbalance between the number of visual and annotation words in the training data. This paper proposes a novel model for simultaneous image classification and annotation model based on SupDocNADE, a neural network based topic model for image classification and annotation. The proposed model, named wSupDocNADE, addresses the above shortcomings by using a new coding and introducing a weighting mechanism for the SupDocNADE model. In the coding step of the model, several patches extracted from the input image are first fed to a deep convolutional neural network and the feature vectors obtained from this network are coded using the LLC coding. These vectors are then aggregated in a final descriptor through sum pooling. To overcome the imbalance between the visual and annotation words, a weighting factor is considered for each visual or annotation word. The weights of the visual words are set based on their frequencies obtained from the pooling method and the weights of the annotation words are learned from the training data. The experimental results on three benchmark datasets show the superiority of the proposed model in both image classification and annotation tasks over state-of-the-art models.}
}
@article{ZOU2019266,
title = {Research on image steganography analysis based on deep learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {266-275},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.034},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300914},
author = {Ying Zou and Ge Zhang and Leian Liu},
keywords = {Steganalysis, Steganography, Feature learning, Deep learning, Convolutional neural network, Transfer learning, Multitask learning},
abstract = {Although steganalysis has developed rapidly in recent years, it still faces many difficulties and challenges. Based on the theory of in-depth learning method and image-based general steganalysis, this paper makes a deep study of the hot and difficult problem of steganalysis feature expression, and tries to establish a new steganalysis paradigm from the idea of feature learning. The main contributions of this paper are as follows: 1. An innovative steganalysis paradigm based on in-depth learning is proposed. Based on the representative deep learning method CNN, the model is designed and adjusted according to the characteristics of steganalysis, which makes the proposed model more effective in capturing the statistical characteristics such as neighborhood correlation. 2. A steganalysis feature learning method based on global information constraints is proposed. Based on the previous research of steganalysis method based on CNN, this work focuses on the importance of global information in steganalysis feature expression. 3. A feature learning method for low embedding rate steganalysis is proposed. 4. A general steganalysis method for multi-class steganography is proposed. The ultimate goal of general steganalysis is to construct steganalysis detectors without distinguishing specific types of steganalysis algorithms.}
}
@article{ZHANG2019448,
title = {Quantitative similarity calculation method for trajectory-directed line using sketch retrieval},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {448-454},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.040},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930046X},
author = {Yue Zhang and Yaping Lin},
keywords = {Trajectory, Directed line, Topological relationship, Key point, Similarity},
abstract = {Moving objects may have various kinds of topological events when they move relative to a directed line, such as moving in/out, folding back and stopping. This research develops a trajectory-directed line query mode using sketch-based retrieval in order to realize the query of topological relationship between a trajectory and a directed line. The research also proposes a quantitative similarity calculation method for trajectory-directed line. The sketched stroke-directed line is used as input query condition and is described as a sequence of key points for semantic association. The trajectory-directed line to be queried is also described as a sequence of key points. The time sequence distance metric is used to calculate the similarity between them. For the trajectory-directed line with different number of key points, the similarity calculation is carried out using two kinds of distance matching methods. The results show that the method is simple and feasible, and can realize the quantitative query of trajectory data.}
}
@article{PASSOS2019475,
title = {Barrett’s esophagus analysis using infinity Restricted Boltzmann Machines},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {475-485},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.043},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300513},
author = {Leandro A. Passos and Luis A. {de Souza Jr.} and Robert Mendel and Alanna Ebigbo and Andreas Probst and Helmut Messmann and Christoph Palm and João Paulo Papa},
keywords = {Barrett’s esophagus, Infinity Restricted Boltzmann Machines, Meta-heuristics, Deep learning},
abstract = {The number of patients with Barret’s esophagus (BE) has increased in the last decades. Considering the dangerousness of the disease and its evolution to adenocarcinoma, an early diagnosis of BE may provide a high probability of cancer remission. However, limitations regarding traditional methods of detection and management of BE demand alternative solutions. As such, computer-aided tools have been recently used to assist in this problem, but the challenge still persists. To manage the problem, we introduce the infinity Restricted Boltzmann Machines (iRBMs) to the task of automatic identification of Barrett’s esophagus from endoscopic images of the lower esophagus. Moreover, since iRBM requires a proper selection of its meta-parameters, we also present a discriminative iRBM fine-tuning using six meta-heuristic optimization techniques. We showed that iRBMs are suitable for the context since it provides competitive results, as well as the meta-heuristic techniques showed to be appropriate for such task.}
}
@article{WU2019550,
title = {A (k,n) threshold partial reversible AMBTC-based visual cryptography using one reference image},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {550-562},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300598},
author = {Xiaotian Wu and Dong Chen and Ching-Nung Yang and Yi-Yun Yang},
keywords = {Visual cryptography, Block truncation coding, Partial reversible, Meaningful share, Contrast, Reference image},
abstract = {Recently, Yang et al. introduced a (k,n) approach called Reversible Absolute moment block truncation coding Visual Cryptography Scheme (RAVCS) (Yang et al., 2017) to conceal a secret image into n AMBTC shares. However, a large number of reference images is used in their method. To reduce the number of reference images, a (k,n) PRAVCS using one AMBTC reference image is introduced. In encoding phase, a binary secret image is shared into n AMBTC shadow images according to the base matrices generated by two proposed constructions. In decoding phase, the secret image is recovered by stacking sufficient bitmaps, and the AMBTC reference image is partially recovered as well. When n AMBTC shares are used, losslessly reconstruction for the reference image is obtained. Theoretical analysis and experiments by the proposed method are demonstrated to show the effectiveness. Moreover, Construction 1 has larger contrast, while Construction 2 achieves higher reversibility for more thresholds.}
}
@article{KHOSRAVI2019217,
title = {Image quality assessment using a novel region smoothness measure},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {217-228},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.019},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318302906},
author = {Mohammad Hossein Khosravi and Hamid Hassanpour},
keywords = {Image quality assessment, Full reference, Image region smoothness, Maximally stable extremal region, Percentile averaging},
abstract = {One of the most efficient descriptions of image structure, which has been widely used in image quality assessment (IQA) studies, is the three-components model. Based on this model, the major structural components of an image are edges, textures and flat regions. We found that this model is basically derived from the abstract concept of image region smoothness. Indeed, each of these three components, is a particular region with special smoothness characteristics. Inspired by this fact, we developed an efficient general-purpose full-reference IQA technique, in which the amount of region smoothness degradation is gauged using our efficient MSER (maximally stable extremal region)-based region smoothness measure. For this, we build a block-based smoothness similarity map, and extract the image quality score, using a percentile averaging scheme. Experimental results are provided on popular benchmark databases, which confirm that the proposed approach has a reasonable prediction performance compared to the state-of-the-art image quality metrics.}
}
@article{WANG201944,
title = {Research on automatic target detection and recognition based on deep learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {44-50},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300240},
author = {Jia Wang and Chen Liu and Tian Fu and Lili Zheng},
keywords = {Image processing, Target detection, Target recognition, In-depth learning},
abstract = {With the development of computer technology, the related achievements of image processing have been applied. Among them, the results of automatic target detection and recognition are widely used in the fields of reconnaissance, early warning and traffic control with the application of UAV. But now, the research of automatic target detection and tracking is becoming smaller and smaller. The original automatic target detection and recognition algorithm seems to be inadequate. The bottleneck of low-level feature design and optimization makes the accuracy and efficiency of automatic target detection inefficient. Therefore, based on in-depth learning, this paper establishes a method to automatically learn effective image features from images to achieve automatic target detection. Through the simulation of target detection in VEDAI database. The results show that the recognition rate of the proposed model is more than 95%. The results show that the proposed method can realize the automatic detection and recognition of targets very well.}
}
@article{HU2019407,
title = {No reference quality assessment for Thangka color image based on superpixel},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {407-414},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.039},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300483},
author = {Wenjin Hu and Yuqi Ye and Jiahao Meng and Fuliang Zeng},
keywords = {Image quality, No reference assessment, Superpixel, Information entropy, Thangka image},
abstract = {In view of the situation that a large number of Thangka images are missing part of color information because of time and environmental factors, the existing image evaluation methods are inconsistent with the result of subjective evaluation. This paper aims at evaluating the damaged Thangka color image, and proposes a new method of image quality evaluation based on superpixel and color entropy. In this algorithm, we use the uniformity of Thangka color image to extract color feature based on CIE 1976 L* a* b* (CIELAB) color space and superpixel. Therefore, the loss of color information in the complex area of Thangka images is well handled. The color entropy is used to quantify the color distribution and structure characteristics of each superpixel, and then we can get the preliminarily evaluation score. In the end, large amounts of data are obtained through some operations such as image deformation and rotating by the Generative Adversarial Nets (GANs), which makes the final evaluation score more reliable. Experimental results show that this method can obtain a good consistency with the subjective results, and Spearman rank order the correlation coefficient (SROCC) and Pearson linear correlation coefficient (PLCC) of the new method already exceed 0.9.}
}
@article{KUO2019346,
title = {Interpretable convolutional neural networks via feedforward design},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {346-359},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930104X},
author = {C.-C. Jay Kuo and Min Zhang and Siyang Li and Jiali Duan and Yueru Chen},
keywords = {Interpretable machine learning, Convolutional neural networks, Principal component analysis, Linear least-squared regression, Cross entropy, Dimension reduction},
abstract = {The model parameters of convolutional neural networks (CNNs) are determined by backpropagation (BP). In this work, we propose an interpretable feedforward (FF) design without any BP. The FF design adopts a data-centric approach. It derives network parameters of the current layer based on data statistics from the output of the previous layer in a one-pass manner. To construct convolutional layers, we develop a new signal transform, called the Saab (Subspace approximation with adjusted bias) transform. It is a variant of the principal component analysis with an added bias vector to annihilate activation’s nonlinearity. Multiple Saab transforms in cascade yield multiple convolutional layers. As to fully-connected layers, we construct them using a cascade of multi-stage linear least squared regressors. The classification and robustness performances of BP- and FF-designed CNNs applied to the MNIST and the CIFAR-10 datasets are compared. Finally, we comment on the relationship between BP and FF designs.}
}
@article{REHMAN2019574,
title = {Face liveness detection using convolutional-features fusion of real and deep network generated face images},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {574-582},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300641},
author = {Yasar Abbas Ur Rehman and Lai-Man Po and Mengyang Liu and Zijie Zou and Weifeng Ou and Yuzhi Zhao},
keywords = {Convolution neural networks, Face anti-spoofing, Face liveness detection, Adaptive fusion, Auto-encoder, DNG face images},
abstract = {Conventionally, classifiers designed for face liveness detection are trained on real-world images, where real-face images and corresponding face presentation attacks (PA) are very much overlapped. However, a little research has been carried out in utilization of the combination of real-world face images and face images generated by deep convolutional neural networks (CNN) for face liveness detection. In this paper, we evaluate the adaptive fusion of convolutional-features learned by convolutional layers from real-world face images and deep CNN generated face images for face liveness detection. Additionally, we propose an adaptive convolutional-features fusion layer that adaptively balance the fusion of convolutional-features of real-world face images and face images generated by deep CNN during training. Our extensive experiments on the state-of-the-art face anti-spoofing databases, i.e., CASIA, OULU and Replay-Attack face anti-spoofing databases with both intra-database and cross-database scenarios indicate promising performance of the proposed method on face liveness detection compared to state-of-the-art methods.}
}
@article{NGUYEN2019231,
title = {A client-based adaptation framework for 360-degree video streaming},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {231-243},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300124},
author = {D.V. Nguyen and Huyen T.T. Tran and Truong Cong Thang},
keywords = {Virtual reality, 360-degree videos, Framework, Adaptive streaming},
abstract = {360-degree video is one of the key components of Virtual Reality (VR) applications. 360-degree videos viewed on Head Mounted Displays can offer impressive viewing experiences to users. Yet, streaming of 360-degree videos over the Internet is a very challenging task since it requires extremely high bandwidth. To reduce the bandwidth requirement while still providing good experiences, viewport adaptive streaming has been introduced. In this paper, we propose a client-based adaptation framework for viewport adaptive streaming of 360 videos, which can support different application scenarios. The key components and important issues are presented with a general problem formulation for tile version selection. Especially, we introduce for the first time the use of bitrate and quality estimation in viewport adaptive streaming of 360 videos. Experiments show that the proposed framework can significantly improve video quality for users. In addition, the impacts of buffering delay and projection formats in VR context are investigated.}
}
@article{ZHONG201950,
title = {Predictor-corrector image interpolation},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {50-60},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301178},
author = {Baojiang Zhong and Kai-Kuang Ma and Zhifang Lu},
keywords = {Image interpolation, Predictor-corrector, Edge-guided, Contrast-guided, Directional filtering},
abstract = {A novel image interpolation methodology is proposed in this paper, called the predictor-corrector interpolation (PCI). Given a low-resolution (LR) image, our PCI scheme begins with the prediction stage, aiming to interpolate the LR-sized input image to a high-resolution (HR) image which is of the same size as the final interpolated image. In the subsequent correction stage, those salient pixels (e.g., edge pixels) of the predicted image are identified and then necessary corrections are made to them for further improving the image quality. To demonstrate the effectiveness of this PCI methodology, the sparse mixing estimator (SME) interpolation is selected as the predictor, and a modified version of the contrast-guided interpolation (CGI) is developed and exploited as the corrector. Hence, the proposed PCI algorithm is denoted as PCI(sme,hr-cgi), which shows a superior performance over a number of comparable state-of-the-art image interpolation algorithms in both objective and subjective image quality assessment.}
}
@article{ZHANG2019168,
title = {Application research of image compression and wireless network traffic video streaming},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {168-175},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.042},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303687},
author = {Ge Zhang and Jianlin Wang and Chaokun Yan and Sheng Wang},
keywords = {Image compression, DASH, Streaming media, Wavelet transform, Noise-signal ratio},
abstract = {With the increasingly busy urban traffic and the development of modern communication technologies, traffic conditions need to be transmitted from major intersections to command and dispatch centers for analysis and processing, which raises a large number of problems of storing and transmitting static images of traffic conditions. Research on image compression of traffic conditions has also become a hot issue that people pay more and more attention to. In the process of traffic image research, due to the lack of essential attributes of the image, especially, the selection and use of compression methods has greater blindness. However, an overall analysis of the image prior to traffic image processing is a difficult task. This article selects the road traffic data, public transit data and orbital data first to compress the image. Then the streaming Media transmission System of DASH is introduced. In the specific application, the code of traffic data flow in this paper is converted into Real Media format through SDK. With the help of Helix Server, all traffic data flow files can be integrated into the synchronous Media integration language, based on the Internet of TCP/IP which is released in a stream through Real System. The experimental results show that the traffic conditions such as vehicle queuing, congestion and signal lights are directly mastered, the signal timing is timely adjusted or other means are adopted to ease the traffic, the distribution of traffic flow is changed, and ordinary terminal users are enabled to master the distribution of traffic flow through wireless network and choose the travel path actively.}
}
@article{CHEN2019398,
title = {Three dimensional image of stress space geotechnical constitutive model},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {398-406},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301051},
author = {Fujiang Chen},
keywords = {Rock and soil, Elastoplasticity, Constitutive model, Numerical simulation},
abstract = {There are various discontinuities in geotechnical engineering problems, which can be roughly divided into two types: one is the discontinuities formed by various actions of rock and soil, such as joints, faults, fissures and shear failure surfaces in rock mass; The other is the contact surface between geotechnical structures such as various foundations, retaining structures, underground structures and geotechnical bodies. Therefore, we cannot ignore its existence in computation. At present, the finite element method and other numerical methods have been widely used to solve geotechnical engineering problems, but the solution of the above discontinuous deformation problems has not been well solved. In this paper, the constitutive model of rock and soil is established. Based on the theory of image processing technology, the common methods of dealing with discontinuous deformation problems in finite element method are pointed out, and the unreasonable points are pointed out. It is more reasonable to treat such problems as contact problems, because it can reflect the main characteristics of discontinuous deformation surface. Come on. The three-dimensional image of the constitutive model has three important functions: (1) it is helpful to enhance the theory of constitutive theory and lay a foundation for further mastery of the theory of constitutive theory; (2) Abstract constitutive theory is no longer just a bunch of boring formulas, which is helpful for beginners to improve their interest in learning the theory of constitutive theory. (3) Finally, a three-dimensional image constitutive model of rock and soil is established through experiments, and the program written by MATLAB is compared with the test results of conventional triaxial compression of rock and soil, which fully verifies the correctness and effectiveness of the constitutive model of rock and soil established in this paper.}
}
@article{BASAVARAJU2019117,
title = {Object Memorability Prediction using Deep Learning: Location and Size Bias},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {117-127},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300082},
author = {Sathisha Basavaraju and Sibaji Gaj and Arijit Sur},
keywords = {Object Memorability, Deep Learning, Transfer Learning},
abstract = {Object memorability prediction is a task of estimating the probability that a human recognises the recurrence of an object after a single view. Initial research on object memorability showed that it is possible to predict the object memorability scores from the intrinsic features of an object. Though the existing works proposed some of the features for object memorability prediction task, the influence of Spatial-location and Spatial-size of an object to its memorability have not been explored yet. In this work, the importance of these two characteristics in determining object memorability prediction is investigated and the same is demonstrated by building a baseline model. Further, a deep learning model is devised for automatic feature learning on these two object characteristics. Experimental results highlight that the Spatial-location and Spatial-size of an object play a significant role in object memorability prediction and the proposed models outperformed the existing methods.}
}
@article{XU2019192,
title = {Study on clues for gold prospecting in the Maizijing-Shulonggou area, Ningxia Hui autonomous region, China, using ALI, ASTER and WorldView-2 imagery},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {192-205},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300628},
author = {Yuanjin Xu and Pengyan Meng and Jianguo Chen},
keywords = {Remote sensing, Mineral prospecting, Fracture, Hydrothermal alteration, Gold mineralization, Maizijing-Shulonggou area},
abstract = {Several gold occurrences have been discovered in the Maizijing-Shulonggou area, Ningxia Hui autonomous region, China. The clues for gold prospecting in this area may relate to fractures and hydrothermal alterations, but it is very difficult to conduct a field investigation on the clues because of the alpine valleys. To explore the clues for gold prospecting, this study extracted geological information related to fracture zones and hydrothermal alterations using ALI, ASTER, and WorldView-2 data, and subsequently, explored potential relationships between extracted geological characteristics and gold mineralization. The WorldView-2 image in the study area was used for fracture interpretation. The azimuth and density of linear fractures were investigated, and the results showed that linear fractures with NE direction were most common, the gold occurrences except Au2 were all located in NE fracture zones, and all gold occurrences located in high fracture density areas (level 1–3 areas) where NE and NW fractures were the most common observations. This suggests that the high fracture density and NE and NW fractures are important indicators of potential gold mineralization. Hydrothermal alteration minerals in exposed bedrocks were mapped using ALI and ASTER imagery. Based on the spectral analysis, ASTER image in the study area were used to map quartz, illite and chlorite, while ALI image were used to map limonite. The spatial distribution of alteration minerals revealed regional specific alteration mineral in gold mineralization. In the northern Maizijing region, the composition of alteration mineral in the surrounding of Au1 was overwhelmed by quartz, while the alteration minerals in the surrounding of Au2-4 were dominated by limonite in the southern Shulonggou region, which showed different indicator minerals for gold prospecting in the two regions. As a result, the key areas for prospecting are the NE and NW fracture zones with quartz veins in the Maizijing region and the NE and NW fracture zones with limonitization in the Shulonggou region. In the Shulonggou region, pyrite in buried quartz-pyritization belt and illitization-pyritization belt is likely to be the precursor of limonite. Heavy mineral assemblages were analyzed for native gold content, and the results suggested high native gold content in the illitization-pyritization belt, followed by quartz-pyritization belt, which was consistent with the strong association of limonite and gold mineralization, thus the two belts shall be prioritized for gold prospecting in the Shulonggou region.}
}
@article{AFJAL2019514,
title = {Band reordering heuristics for lossless satellite image compression with 3D-CALIC and CCSDS},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {514-526},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.042},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300501},
author = {Masud Ibn Afjal and Md. Al Mamun and Md. Palash Uddin},
keywords = {Satellite image, Lossless compression, Band reordering, CCSDS, Correlation exploitation, 3D-CALIC},
abstract = {Remote sensing satellite images are used widely in space imaging applications as they collect significant information of ground objects through capturing the ground surface in immense wavelength bands. The size of these images is typically enormous in quantity due to the bulky number of capturing wavelengths. The images need to transmit to the ground from the sensors for a specific application. Thus, the efficient compression techniques are required to fit the available bandwidth for reducing the transmission time. The data in the images are usually redundant spatially, spectrally and temporally which give an ample opportunity to compress the images in various domains. Most importantly, the data features have a strong correlation in the separate spectral area. As a result, the similarity-based band reordering strategy is used to increase the compression performance in comparison to the image having natural band order. However, finding the optimal band reordering is still a computationally challenging problem. In this paper, three different methods namely Band Reordering based on Consecutive Continuity Breakdown Heuristics (BRCCBH), Band Reordering based on Weighted-Correlation Heuristic (BRWCH) and Segmented BRCCBH have been proposed for the compression of multispectral, hyperspectral and hyperspectral sounder data. The presented methods are different on the number and type of heuristics used for obtaining the optimal band reordering. The performances of the proposed band reordering methods are tested using CCSDS 123 lossless predictor and lossless 3D-CALIC. The experimental results show the significant improvement on compression performance by using the proposed band ordering techniques for different types of real multispectral data (3–5% using CCSDS and 2–3% using 3D-CALIC), hyperspectral data (0.2–0.7% using CCSDS and 0.8–1% using 3D-CALIC) and hyperspectral sounder data (5.5–7% using CCSDS and 4–5% using 3D-CALIC).}
}
@article{CHEN2019229,
title = {Hybrid image super-resolution using perceptual similarity from pre-trained network},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {229-235},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.022},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300744},
author = {Yanxiang Chen and Huadong Tan and Luming Zhang and Jie Zhou and Qiang Lu},
keywords = {Super-resolution, Hybrid method, Adaptive weight, Pre-trained VGG network},
abstract = {The goal of super-resolution (SR) is to recover a high-resolution (HR) image from its corresponding low-resolution (LR) image. It is an ill-posed problem. Most recent methods are based on external training data. They can reconstruct pleasing HR results, especially when the input patch has a similar counterpart within the training dataset. Other methods are driven by self-similarity and are called internal methods. They can produce visually plausible HR images when the input images contain abundant regular structures. In this paper, we propose a hybrid method for image SR that exploits the complementary advantages of external and internal SR methods. Each input LR patch is first super-resolved using convolutional neural network (CNN) for external SR and self-similarity for internal SR. Then, we calculate the perceptual similarity between the feature representations from the pre-trained VGG network to learn an adaptive weight. Finally, our algorithm automatically selects the optimal method on the basis of the calculated adaptive weight. The experimental results of our visual and quantitative evaluations verify the effectiveness of the proposed method, by comparing it with state-of-the-art methods.}
}
@article{LIU2019300,
title = {Analysis of Beijing Tianjin Hebei regional credit system from the perspective of big data credit reporting},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {300-308},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.018},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300227},
author = {Cheng-yong Liu and Ling-Jan Chiou and Cheng-chung Li and Xiu-Wen Ye},
keywords = {SVM, Information asymmetry, Convolution neural network, Decision tree, Credit investigation},
abstract = {With the wide application of big data technology and the continuous innovation of the financial industry, the development of Internet finance has become irreversible. The new financial model has improved the efficiency of financial services and formed new ways of payment and transaction. The traditional credit information system has been unable to meet the needs of the development of the financial industry. The research methods adopted in this paper mainly include literature research, case analysis and comparative analysis. The development process, current situation and characteristics of China's personal credit system, as well as the development process and current situation of China's personal credit system under Internet finance are analyzed. It also points out the main problems faced by China's personal credit reporting system under the Internet finance. Then, aiming at the specific problems faced by China's personal credit reporting system under Internet finance, this paper makes case studies of representative market-oriented credit reporting institutions such as Beijing, Tianjin and Hebei, and uses convolutional neural network to analyze the specific practices of these individual credit reporting institutions and achieves results. At the same time, it compares how different credit agencies make up for the shortcomings of personal credit reporting system. Finally, it draws the conclusion of perfecting the personal credit system of our country under the Internet finance, points out the new problems that may be brought by the superposition of various solutions, and puts forward the countermeasures and suggestions of perfecting the personal credit system of our country under the Internet finance.}
}
@article{MA2019426,
title = {Traffic surveillance video coding with libraries of vehicles and background},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {426-440},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301038},
author = {Changyue Ma and Dong Liu and Xiulian Peng and Li Li and Feng Wu},
keywords = {Background, High Efficiency Video Coding (HEVC), Library-based coding, Traffic surveillance video, Vehicle},
abstract = {This paper presents a video coding scheme tailored for traffic surveillance videos, which features a pre-built library that is utilized in both encoder and decoder to pursue higher compression efficiency. We are motivated by the observation that, in traffic surveillance videos, not only the background is steady for a long while, but also the foreground (e.g. vehicles) contains high redundancy. For example, in the video taken by a traffic monitoring camera, we can observe that the passing-through vehicles are usually similar. However, the redundancy in the vehicles and the background is not fully exploited in the current video coding schemes. To address this problem, we propose a library-based video coding scheme. Specifically, for each static monitoring camera, we can collect video in a period, and build a library of vehicles and background from that video. When encoding the following video of that camera, we utilize the pre-built library by searching similar vehicles and background from the library and using the retrieved vehicles and background to help encode. Accordingly, the decoder also refers to the same library to reconstruct the video. We design efficient algorithms for building the library, searching in the library, as well as utilizing the library for encoding/decoding, to fulfill the proposed scheme. Our scheme is implemented upon the state-of-the-art video coding system–High Efficiency Video Coding (HEVC), and is tested on our own collected traffic surveillance videos. Experimental results show that, compared to the HEVC anchor, our proposed scheme achieves as high as 47.0%, 37.1%, and 34.8% BD-rate reduction, under random-access, low-delay B, and low-delay P settings, respectively. Our scheme also achieves higher compression efficiency than the existing background-based methods for surveillance video coding.}
}
@article{ZHANG2019170,
title = {Frequency domain point cloud registration based on the Fourier transform},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {170-177},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300987},
author = {Shuang Zhang and Hua Wang and Jin-gang Gao and Chun-qi Xing},
keywords = {Fourier transform, Point cloud data, Frequency domain, Registration},
abstract = {Due to the limited measurement range and occlusion of single-line structured light, it is impossible to detect the side data of the whole part. It is proposed that point cloud registration method obtained from multiple rotations of parts in frequency domain by Fourier transform. In the process of point cloud registration, cross-section point cloud data are restored to the corresponding size matrix firstly. Secondly, Fourier transform is carried out to calculate the point cloud data. When calculating the rotation angle, the polar coordinate transformation is carried out at first, and then the cross power spectrum of the two matrices is obtained, so that the rotation and translation matrix of the point cloud can also be obtained. In this process, considering point cloud noise existence, the Sinc function is approximately replaced by the non-noise inverse Fourier transform of cross power spectrum, so that the noise has no influence on the determination of registration parameters in frequency domain registration. The registration accuracy of point cloud is checked by high precision rotation and multiple measurements of mobile platform. Finally, the rotation matrix and translation values are obtained.}
}
@article{ZHANG2019387,
title = {An interactive method for identifying the stay points of the trajectory of moving objects},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {387-392},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.038},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300471},
author = {Yue Zhang and Yaping Lin},
keywords = {Trajectory, Stay point, Space-time cube},
abstract = {The stay points of trajectory include important semantic information. Identifying the stay points is an indispensable step for mining and analyzing the trajectory data. Current methods for identifying stay points generally require pre-setting thresholds, which have a major impact on the identification results. Meanwhile, the setting of these thresholds is heavily reliant on empirical experience. There are many types of moving objects, such as vehicles, ships, airplanes, people and animals. These moving objects have a large variation in their trajectory data due to their different geometric/physical properties and navigation environments. Therefore, setting the appropriate thresholds for the identification process is by no means easy for non-professionals. To cope with this challenge, an interactive visualization method is suggested in this paper in order to help users to set the thresholds. In the proposed method, the Space-Time Cube (STC) is used to visualize the trajectory as the first step. In the next step, users interactively choose the typical stay points of the trajectory. The threshold is atomically determined using the geometric characteristics of the 3-dimensional bounding box of the space where the moving object stays on. Finally, the sliding window method is used to extract the stay points. Experimental results demonstrate that the proposed method is an efficient method for determining the thresholds in an intuitive and fast manner. As a result, it can significantly improve the efficiency of identifying the stay points of the trajectory for non-professionals.}
}
@article{WU2019116,
title = {Effect of subject's age and gender on face recognition results},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {116-122},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300197},
author = {Shifeng wu and Dahu Wang},
keywords = {Face recognition, Deep learning, Gender, Age},
abstract = {Nowadays, more and more places need authentication. Face recognition is a mature technology for identity verification research. Recognition accuracy is an important indicator for evaluating authentication algorithms. In order to improve the accuracy of identity verification, advanced face is used. Feature recognition algorithm is an effective way, but it is also an effective algorithm to study the factors affecting facial features. Therefore, many researchers study the recognition results based on the poses of the face, light and other factors. This paper is also a study on the factors affecting face recognition, mainly by studying the influence of the age and gender factors on the identity verification results, and using the deep learning method to classify facial features. The simulation results show that the average recognition rate reaches 83.73%. At the same time, this paper analyzes the effect of age and gender on the classification results. The results show that the recognition effect of middle-aged men in male subjects is lower than that of youth and the elderly. Women have little difference in recognition effect with age. Males have higher recognition rates than women.}
}
@article{XIA2019108,
title = {Trademark image retrieval via transformation-invariant deep hashing},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {108-116},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300112},
author = {Zhaoqiang Xia and Jie Lin and Xiaoyi Feng},
keywords = {Trademark image retrieval, Deep hashing, Transformation-invariant feature, Spatial transformer network, Recurrent convolutional network, Sample-weighted loss},
abstract = {Trademark images are usually used to distinguish goods due to their uniqueness, and the amount becomes too huge to search these images accurately and fast. Most existing methods utilize conventional dense features to search visually-similar images, however, the performance and search time are not satisfactory. In this paper, we propose a unified deep hashing framework to learn the binary codes for trademark images, resulting in good performance with less search time. The unified framework integrates two types of deep convolutional networks (i.e., spatial transformer network and recurrent convolutional network) for obtaining transformation-invariant features. These features are discriminative for describing trademark images and robust to different types of transformations. The two-stream networks are followed by the hashing layer. Network parameters are learned by minimizing a sample-weighted loss, which can leverage the hard-searched images. We conduct experiments on two benchmark image sets, i.e., NPU-TM and METU, and verify the effectiveness and efficiency of our proposed approach over state-of-the-art.}
}
@article{ZHANG2019272,
title = {3D human pose estimation from range images with depth difference and geodesic distance},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {272-282},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.028},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300355},
author = {Wenhui Zhang and Dehui Kong and Shaofan Wang and Zhiyong Wang},
keywords = {Human pose estimation, Superpixel, Geodesic distance, Random decision forest, Sparse representation},
abstract = {Depth difference, as a popularly used feature for characterizing pairwise pixels of range images, fails to precisely capture skeleton joints when human body possesses a wild and complicated articulation. As the geodesic distance of pairwise pixels is able to present a global connected property and adjacent pixels often belong to the same body component, we propose an effective and efficient framework for pose estimation from range images. Firstly, all the pixels of a range image are grouped into superpixels using an improved Simple Linear Iterative Clustering algorithm. Secondly, those superpixels are labelled as the components of a human body using the hybrid feature. Thirdly, componentwise cluster feature extraction is undertaken on skeleton joints of body components with K-means clustering algorithm. Finally, the feature points of each component are then stacked as a compact representation of human poses and mapped to the skeleton joints of a human body. Experimental results demonstrate that the proposed framework outperforms several state-of-the-art pose estimation methods.}
}
@article{LUO201961,
title = {Convolutional neural networks-based stereo image reversible data hiding method},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {61-73},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301166},
author = {Ting Luo and Gangyi Jiang and Mei Yu and Caiming Zhong and Haiyong Xu and Zhiyong Pan},
keywords = {Convolutional neural network (CNN), Predictor, Prediction error expansion (PEE), Reversible data hiding (RDH), Stereo image},
abstract = {For copyright and integrity protection of stereo images, a stereo image reversible data hiding (SIRDH) method based on the convolutional neural network (CNN) is presented. To increase the pixel prediction, a CNN-based predictor is designed. Firstly, pixels are divided into two types. For predicting one type of pixels in one view, the other type of pixels in the same view is considered as the low-resolution (LR) image. Secondly, a LR difference view between the LR image and the other view is computed, since the difference view consists of the texture and depth information. Then, CNN is trained to recover the high-precision difference view from the LR difference view, and the high-resolution image is computed to predict pixels accurately. Moreover, the prediction error expansion (PEE) is employed to embed data. Experimental results show that the proposed method is superior to existing SIRDH methods.}
}
@article{LIANG2019149,
title = {Huffman-code based retrieval for encrypted JPEG images},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {149-156},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301282},
author = {Haihua Liang and Xinpeng Zhang and Hang Cheng},
keywords = {Image retrieval, Encrypted JPEG image, Quantization table, Huffman code, Confidentiality, Integrity, Format compatibility},
abstract = {This paper proposes a retrieval scheme for encrypted JPEG images, based on Huffman code in the JPEG bitstream. Three parties are involved: content owner, cloud server, and authorized user. First, the content owner produces encrypted images by jointly using a stream cipher, permutation cipher, then uploads them to a cloud server. Especially, the conversion between encrypted quantization tables is still valid. With the same secret key, the authorized user submits the encrypted query image to the server. Second, the server extracts Huffman-code histogram from the encrypted image as a feature. Although the Huffman-code histogram is changed during encryption, encrypted images with similar content to the query image are returned to the user after feature comparison. Finally, through decryption and hash verification, the user can obtain authenticated plaintext images. Experimental results show that the proposed scheme ensures confidentiality, integrity and format compatibility, while image retrieval of different quality factors is still effective.}
}
@article{HUANG201928,
title = {A crowdsourced system for robust eye tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {28-32},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300070},
author = {Honghe Huang and Yi Xu and Xiao Hua and Weixiong Yan and Yanjie Huang},
keywords = {Eye tracking, Gaze shifting path, Large-scale dataset, CNN},
abstract = {Eye tracking is widely used in modern intelligent applications, such as HCI, somatosensory game and fatigue driving. Traditional eye tracking system based on Haar-like features or external hardware, which is loss of accuracy and complicated. It is obviously that human gaze point is related to head pose. However, the label of head pose in most dataset is ambiguous. So in this paper, we propose a crowdsourced system which can collect large-scale dataset for eye tracking. For better performance, we leverage head guidance point and random dot instead of fixed dot as the concern when capture frames from camera. And different illumination, poses and persons also considered for robust performance. And we propose a two-phase CNN training strategy for combining head pose and eye angles. The proposed CNN architecture can reduce the overfitting when we train eye tracking models with head pose directly. The experimental results show that our proposed method can perform well in eye tracking.}
}
@article{FANG2019140,
title = {Reduced-reference quality assessment of image super-resolution by energy change and texture variation},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {140-148},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.035},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303614},
author = {Yuming Fang and Jiaying Liu and Yabin Zhang and Weisi Lin and Zongming Guo},
keywords = {Image quality assessment (IQA), Image super-resolution, Reduced-reference (RR) quality assessment, Energy change, Texture variation},
abstract = {In this paper, we propose a novel reduced-reference quality assessment metric for image super-resolution (RRIQA-SR) based on the low-resolution (LR) image information. With the pixel correspondence, we predict the perceptual similarity between image patches of LR and SR images by two components: the energy change in low-frequency regions, which can be used to capture the global distortion in SR images, and texture variation in high-frequency regions, which can be used to capture the local distortion in SR images. The overall quality of SR images is estimated by perceptual similarity calculated by energy change and texture variation between local image patches of LR and HR images. Experimental results demonstrate that the proposed method can obtain better performance of quality prediction for SR images than other existing ones, even including some full-reference (FR) metrics.}
}
@article{NGUYEN2019206,
title = {You always look again: Learning to detect the unseen objects},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {206-216},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.020},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300732},
author = {Khanh-Duy Nguyen and Khang Nguyen and Duy-Dinh Le and Duc Anh Duong and Tam V. Nguyen},
keywords = {Deep learning, Dual-level deep networks, Object detection},
abstract = {Object detection has always attracted a lot of attention in computer vision due to its practical applications, i.e., robotics engineering, autonomous vehicles, and surveillance systems. Recently deep learning approaches have successfully improved the performance of object detection by a significant amount. However, there exist many challenging objects in the images that state-of-the-art approaches still fail to detect. In this paper, we propose an efficient approach that intentionally learns to detect the unseen (missing) objects. In particular, we utilize a dual-level of deep networks to efficiently detect difficult objects in images. The extensive experiments on three benchmarking datasets, PASCAL VOC, KITTI, and MS-COCO, show the superiority of our approach over the state-of-the-art methods.}
}
@article{ZHAO2019283,
title = {Research on video classification method of key pollution sources based on deep learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {283-291},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300215},
author = {Kunrong Zhao and Tingting He and Shuang Wu and Songling Wang and Bilan Dai and Qifan Yang and Yutao Lei},
keywords = {Pollution sources, Deep learning, Surveillance video classification, Convolution neural network},
abstract = {China's environmental problems are not only related to the fundamental interests of the broad masses of the people, but also to China's national security and international image. At present, China's environmental protection work is facing a complex situation. Pollution sources can be divided into natural pollution sources and man-made pollution sources. Natural sources of pollution refer to places where nature releases harmful substances or causes harmful effects to the environment, such as active volcanoes. Man-made pollution source refers to the pollution source formed by human activities, and is also the main object of environmental protection research and control. Among the man-made pollution sources, air pollution sources, water pollution sources and soil pollution sources can be classified according to the main objects of pollution. Among them, air pollution sources and water pollution sources have the greatest impact on human life. Therefore, it has become an important subject worthy of in-depth discussion to take automatic and electronic measures for potential environmental pollution incidents, discover environmental pollution problems in time, reduce the probability of environmental pollution incidents, and even put some major environmental pollution incidents in their infancy. In this paper, deep learning method is used to classify the existing key pollution source video. Water pollution experiments show that the accuracy of video counting reaches 93.1%, which is better than other video processing schemes. The operation time of the system reaches acceptable range, and a solution to meet the real-time requirement is put forward.}
}
@article{ZHANG201942,
title = {Application of artificial intelligence algorithms in image processing},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {42-49},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300975},
author = {Xin Zhang and Wang Dahu},
keywords = {Image processing, Artificial intelligence algorithm, Image segmentation, Ant colony algorithm},
abstract = {As the main media of human communication and understanding of the world, image is one of the important information sources of human intelligence activities. With the development of the times, the demand for image processing technology is increasing day by day. The rapid development of computer technology also provides a platform for the application of image processing. In order to achieve better image processing effect, this paper focuses on the application of artificial intelligence algorithm in image processing. Image segmentation is a technology that decomposes images into regions with different characteristics and extracts useful targets. It can be regarded as a combinatorial optimization problem. It is completely feasible to apply artificial intelligence algorithm to optimization problems. Firstly, this paper introduces the ant colony algorithm in artificial intelligence algorithm, elaborates the basic principle and mathematical model of the ant colony algorithm. Secondly, in order to improve the ability of global search of ant colony algorithm, this paper introduces the crowding degree function of fish into the ant colony algorithm. Finally, the improved ant colony algorithm is used in image segmentation to improve the effect of image segmentation. The simulation results show that it is feasible to use ant colony algorithm in image segmentation. And the optimization improvement of ant colony algorithm is effective. The improved ant colony algorithm applied in image segmentation can significantly improve the segmentation performance.}
}
@article{HUA2019258,
title = {Wind turbine bionic blade design and performance analysis},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {258-265},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.037},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300458},
author = {Xin Hua and Chunhua Zhang and Jinda Wei and Xingjun Hu and Hongliang Wei},
keywords = {Numerical simulation, Source of renewable energy, Wind turbine blade},
abstract = {With the growing shortage of energy, wind power is receiving an increasing amount of attention worldwide as a clean source of renewable energy. One of the most important components of the wind turbine is the bird-wing-like blade under the working principle. Three kinds of bionic blades are designed by using bionic wing type and configuration based on the good aerodynamic performance of seagull wings, combined with the theory of wind turbine blade design-the blade element theory. Standard blade and aerodynamic performances of bionic blade are evaluated by means of numerical simulation. Under different wind speeds, the results show that the blade torque of the bionic blade of total improved airfoil increases by 10.2%, the bionic blade of partially improved airfoil increases by 14%, and the configuration improved blade increases by 7%. To verify the correctness of the numerical simulation results, we run the bionic blade and establish a corresponding experimental device in an actual experiment. The results of the experiment are consistent with the trend of the numerical simulation.}
}
@article{ZHANG2019250,
title = {Research on key technologies of remote design of mechanical products based on artificial intelligence},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {250-257},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300616},
author = {Ya Zhang},
keywords = {Artificial intelligence, Machinery, Remote design, Image processing},
abstract = {With the development of information technology, mechanical product design has become an important part of the enterprise and design community. In the traditional mechanical design process, the product not only has performance requirements, but also has many economic requirements such as manufacturing cost and sales profit. Many conditions also have great uncertainty. In addition, in design and evaluation, in addition to the existing theoretical analysis, in most cases, it is mainly judged based on the designer’s experience. In view of the deficiencies in mechanical design, this paper proposes a key technology research of remote design of mechanical products based on artificial intelligence. The wireless communication technology is used to build a client/server wireless communication platform, which is convenient for designers to transmit remote information. The application of image processing and pattern recognition technology to target the components involved in mechanical products helps designers grasp many parameters of mechanical products. The research on the key technology of remote design of mechanical products based on artificial intelligence in this paper has certain reference value for the application of artificial intelligence technology in the field of mechanical product design with insufficient knowledge.}
}
@article{LI2019461,
title = {Edge guided compressive sensing for image reconstruction based on two-stage l0 minimization},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {461-474},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.025},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930032X},
author = {Dan Li and Zhaojun Wu and Qiang Wang},
keywords = {Compressive sensing, Image reconstruction,  minimization, Edge prior, Multiple sampling scheme},
abstract = {In compressive sensing framework, the results of image reconstruction are sometimes not accurate enough due to the downsampled measurements, especially when the sampling rate is relatively small. This paper proposes a novel edge guided compressive sensing (EGCS) algorithm for natural image reconstruction based on two-stage l0 minimization, aiming to improve the reconstruction performance. Firstly, wavelet transform is utilized to provide sparsity and multiple sampling scheme is employed to acquire the down-sampled measurements. Then, in the first stage, we design an edge-preserving smoothing method by l0 gradient minimization to extract the important edge prior accurately, which can not only contribute a lot to improve the reconstruction accuracy of image structures but also reduce the computational complexity remarkably. Also, the use of multiple sampling scheme is beneficial to enhancing the guidance accuracy of multiple edge prior. In the second stage, under the guidance of multiple edge prior, the intelligent searching strategies are designed by taking advantages of intelligent optimization algorithms in solving combinatorial optimization problems and utilizing the superior performance of greedy algorithm on reconstruction speed, which is conductive to solve the joint sparse reconstruction based on l0 minimization essentially. The better reconstruction performance can be achieved based on the fact that it is more likely to find the global optimal solution accurately by the designed intelligent searching strategies, especially when the sampling rate is relatively small. Experimental results on natural image reconstruction demonstrate that our proposed method EGCS is superior to the state-of-the-art reconstruction algorithms, and can well preserve the important image structures at the same time.}
}
@article{WEN2019157,
title = {A novel natural language steganographic framework based on image description neural network},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {157-169},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.016},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301154},
author = {Juan Wen and Xuejing Zhou and Mengdi Li and Ping Zhong and Yiming Xue},
keywords = {Natural language steganography, Image description, Neural network, Embedding capacity, BLEU, Perplexity},
abstract = {It is a challenge to conduct natural language steganography on Online interactive platforms such as photo-sharing websites since the stego texts should be consistent with the content of the images. In this paper, a novel natural language steganographic framework based on an end-to-end generative network is proposed. A Convolution Neural Network (CNN) combined with Long Short-Term Memory (LSTM) is trained to generate stego descriptions. Word by Word Hiding (WWH) and Sentence by Sentence Hiding (SSH) schemes are proposed to achieve various embedding capacity under the premise of sharing model between the sender and the receiver. Furthermore, a blind extraction scheme called Hash Hiding (HH) is proposed in case that the model is unavailable for data extraction. Comparative experiments show the superiority of the proposed framework. It is verified that the proposed framework is an effective carrier-less steganographic framework with competitive embedding capacity, considerable text quality, and good reversibility.}
}
@article{BAISA2019257,
title = {Development of a N-type GM-PHD filter for multiple target, multiple type visual tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {257-271},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.026},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300343},
author = {Nathanael L. Baisa and Andrew Wallace},
keywords = {Visual tracking, Random finite sets, FISST, Multiple target filtering, PHD filter, N-type GM-PHD filter, Gaussian mixture, OSPA metric},
abstract = {We propose a new framework that extends the standard Probability Hypothesis Density (PHD) filter for multiple targets having N⩾2 different types based on Random Finite Set theory, taking into account not only background clutter, but also confusions among detections of different target types, which are in general different in character from background clutter. Under Gaussianity and linearity assumptions, our framework extends the existing Gaussian mixture (GM) implementation of the standard PHD filter to create a N-type GM-PHD filter. The methodology is applied to real video sequences by integrating object detectors’ information into this filter for two scenarios. For both cases, Munkres’s variant of the Hungarian assignment algorithm is used to associate tracked target identities between frames. This approach is evaluated and compared to both raw detection and independent GM-PHD filters using the Optimal Sub-pattern Assignment metric and discrimination rate. This shows the improved performance of our strategy on real video sequences.}
}
@article{ZHANG2019501,
title = {IL-GAN: Illumination-invariant representation learning for single sample face recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {501-513},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300586},
author = {Yang Zhang and Changhui Hu and Xiaobo Lu},
keywords = {Single sample per person, Singular value decomposition, Generative adversarial network, Representation learning, Illumination-invariant face recognition},
abstract = {Single sample per person face recognition influenced by varying illumination is a tricky issue. Conventional techniques for illumination-invariant face recognition either realize illumination normalization on the whole face, or learn the illumination-invariant representation from the face image. This paper holds the opinion that deep learning method, which is more similar to the behavior of primate brain, can leverage the advantages of both the conventional techniques. Motivated by the success of generative adversarial network in image representation, this paper proposes IL-GAN model based on the basic structures of variational auto-encoder and generative adversarial network, generating the Controlled Illumination-level Face Image while preserves identity character as well performing a powerful latent representation from the face image, which encodes illumination-invariant signatures. Moreover, this model can be adopted in single sample per person face recognition. Meanwhile, this research proposes an novel illumination level estimation method based on singular value decomposition to generate the Controlled Illumination-level Face Image optionally. Finally, the performances of the proposed method and other state-of-the-art techniques are verified on the Extended Yale B, CMU PIE, IJB-A and our Self-built Driver Face databases. The experimental results indicate that the IL-GAN model outperforms previous approaches for single sample per person face recognition under varying illumination.}
}
@article{LI2019244,
title = {Seepage mechanism technical practice of hydraulic fracturing of coal seam and auxiliary image simulation technology},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {244-252},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930029X},
author = {Jiangtao Li and Baoshan Jia and Chunhua Zhang and Wei Wang},
keywords = {Gas extraction, Coal seam permeability enhancement, Sublevel hydraulic fracturing, Fracturing equipment, Auxiliary image technology},
abstract = {A new method of fracturing hydraulic fracturing is proposed for the characteristics of poor coal permeability and soft coal in coal mine area in China. This method can make the hydraulic pressure concentrated on the coal at one point, and the water pressure is reduced, Effective water pressure increases, the smaller the flow that can get a better fracture effect. Through the numerical simulation, the law of fracture distribution and permeability evolution during segmental hydraulic fracturing is analyzed. Complete sets of anti-reflection equipment for segmented hydraulic fracturing have been developed, which is mainly composed of mobile high-pressure pump station, high-pressure rubber hose, water injector, putters and corresponding connecting parts. It solves the problem of tightness of hydraulic fracturing device, and breaks through the problem of miniaturization of high pressure pumping station, so that the high pressure pumping station can achieve larger working pressure at smaller flow rate. The whole process is simulated and validated by assistant image simulation technology, and applied to Pansidong Coal Mine of Huainan Mining Group. The industrial test of hydraulic directional fracturing of Pan coal mine in Huainan mining group shows that the loose influence range of coal can reach 10 m, and the pressure of underground movable high pressure hydraulic pumping station reaches 25 MPa, and the flow rate is 180 L/min. Compared with non fracturing drilling, drilling, the average gas concentration after fracturing, the average daily average flow rate of mixed gas drainage quantity is improved significantly, the application effect is good, and the equipment is simple, easy to use, can be repeatedly used, and has better economic benefit and application prospect.}
}
@article{FAN201951,
title = {SphereReID: Deep hypersphere manifold embedding for person re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {51-58},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300100},
author = {Xing Fan and Wei Jiang and Hao Luo and Mengjuan Fei},
keywords = {Person re-identification, Classification, Feature embedding, CNN, Hypersphere},
abstract = {Many current successful Person Re-Identification (ReID) methods train a model with the softmax loss function to classify images of different persons and obtain the feature vectors at the same time. However, the underlying feature embedding space is ignored. In this paper, we use a modified softmax function, termed Sphere Softmax, to solve the classification problem and learn a hypersphere manifold embedding simultaneously. A balanced sampling strategy is also introduced. Finally, we propose a convolutional neural network called SphereReID adopting Sphere Softmax and training a single model end-to-end with a new warming-up learning rate schedule on four challenging datasets including Market-1501, DukeMTMC-reID, CHHK-03, and CUHK-SYSU. Experimental results demonstrate that this single model outperforms the state-of-the-art methods on all four datasets without fine-tuning or re-ranking. For example, it achieves 94.4% rank-1 accuracy on Market-1501 and 83.9% rank-1 accuracy on DukeMTMC-reID. The code and trained weights of our model will be released.}
}
@article{SHASTRI2019130,
title = {Dual image reversible data hiding using trinary assignment and centre folding strategy with low distortion},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {130-140},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301270},
author = {Shounak Shastri and V. Thanikaiselvan},
keywords = {Reversible data hiding, Dual images, Centre folding strategy, Trinary assignment, High PSNR, High security},
abstract = {Dual image Reversible Data Hiding (RDH) algorithms generate two stego images instead of one. These algorithms are considered more secure because the secret data can be recovered only if both stego images are available. This paper proposes a dual image RDH algorithm based on the Centre Folding Strategy (CFS) and the Shiftable Pixel Coordinate Selection Strategy. The proposed algorithm improves the visual quality of the two stego images while maintaining a high embedding rate. It uses a look-up table containing the shifts that the cover pixel will undergo after the data has been embedded. Some overhead information is required for accurate extraction of the hidden data and it is delivered to the users as a password after embedding. The results show that the proposed algorithm outperforms other methods in terms of visual quality with an average Peak-Signal-to-Noise-Ratio (PSNR) of 50.66 dB with an embedding rate of 1.56 bits per pixel.}
}
@article{ZHOU2019112,
title = {Fashion recommendations through cross-media information retrieval},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {112-120},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300963},
author = {Wei Zhou and P.Y. Mok and Yanghong Zhou and Yangping Zhou and Jialie Shen and Qiang Qu and K.P. Chau},
keywords = {Fashion recommendations, Image retrieval, Human parsing, Image features},
abstract = {Fashion recommendation has attracted much attention given its ready applications to e-commerce. Traditional methods usually recommend clothing products to users on the basis of their textual descriptions. Product images, although covering a large resource of information, are often ignored in the recommendation processes. In this study, we propose a novel fashion product recommendation method based on both text and image mining techniques. Our model facilitates two kinds of fashion recommendation, namely, similar product and mix-and-match, by leveraging text-based product attributes and image features. To suggest similar products, we construct a new similarity measure to compare the image colour and texture descriptors. For mix-and-match recommendation, we firstly adopt convolutional neural network (CNN) to classify fine-grained clothing categories and fine-grained clothing attributes from product images. Algorithm is developed to make mix-and-match recommendations by integrating the image extracted categories and attributes information are with text-based product attributes. Our comprehensive experimental work on a real-life online dataset has demonstrated the effectiveness of the proposed method.}
}
@article{YANG2019371,
title = {3D human pose estimation from a single image via exemplar augmentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {371-379},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.033},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300446},
author = {Jingjing Yang and Lili Wan and Wanru Xu and Shenghui Wang},
keywords = {Human pose estimation, Human pose recovery, Exemplar-based, Pose retrieval, Pose synthesis, Monocular},
abstract = {3D human pose estimation from a single image is a challenging problem due to occlusion, viewpoint variance, and the ill-posed nature of back projection. We follow a standard two-step pipeline which first detects 2D joint locations and uses them to infer 3D pose. For the first step, we use a recent deep learning-based detector. For the second step, we propose a novel exemplar-based algorithm to implicitly augment the exemplar set for 3D human pose estimation. The motivation of this algorithm is to well represent various poses in the real world with finite real exemplars. We achieve it by a strategy of synthesizing virtual candidate poses which ensures that the augmented exemplar set has much more variety. Moreover, we also present an effective approach to select the best exemplar from candidate set to well match the detected 2D pose. Experimental results show that our method achieves competitive performance on Human3.6M dataset.}
}
@article{ZALLUHOGLU2019170,
title = {Region based multi-stream convolutional neural networks for collective activity recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {170-179},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300689},
author = {Cemil Zalluhoglu and Nazli Ikizler-Cinbis},
keywords = {Collective activity recognition, Action recognition},
abstract = {Collective activity recognition, which analyses the behavior of groups of people in videos, is an important goal of video surveillance systems. In this paper, we focus on collective activity recognition problem and propose a new multi-stream convolutional neural network architecture that utilizes information extracted from multiple regions. The proposed method is the first work that uses a multi-stream network and multiple regions in this problem. Various strategies to fuse multiple spatial and temporal streams are explored. We evaluate the proposed method on two benchmark datasets, the Collective Activity Dataset and the Volleyball Dataset. Our experimental results show that the proposed method improves collective activity recognition performance when compared to the state-of-the-art approaches.}
}
@article{OLIVEIRA2019236,
title = {Relevance prediction in similarity-search systems using extreme value theory},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {236-249},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300720},
author = {Alberto Oliveira and Eric Oakley and Ricardo {da Silva Torres} and Anderson Rocha},
keywords = {Relevance prediction, Performance prediction, Extreme value theory, Weibull distribution, Information retrieval},
abstract = {Among the challenges present in the design of retrieval systems, how to accurately assess their performance is perhaps one of the most important. Many applications such as rank aggregation or relevance feedback can be significantly improved with online effectiveness estimation of queries. Thus, developing methodologies that can estimate performance with minimal supervision and at query time is of utmost importance for improving the results of existing retrieval systems. In this work, we explore score-based, post-retrieval approaches for relevance prediction of search systems. We first introduce two statistical methods based on the Extreme Value Theory to estimate which of the top-k objects retrieved for a query are relevant. Our prediction approach uses this estimation as a method to infer the overall performance of a query. The two relevance prediction methods were evaluated in image datasets covering several modalities and scoring approaches. We conducted experiments comparing the ground-truth relevances of several ranks with predictions generated by our proposed approach, measuring their effectiveness by way of normalized accuracy and Matthews Correlation Coefficient. Furthermore, we also evaluate the precision deducted from our approaches with the system’s expected performance. Those experiments show that the proposed approaches succeed in most relevance prediction scenarios of the top-ranked objects of a query, obtaining high accuracy.}
}
@article{XUE2019204,
title = {Research on image restoration algorithms based on BP neural network},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {204-209},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300203},
author = {Hongzhi Xue and Hongwei Cui},
keywords = {Image restoration, Image processing, Image denoising, BP neural network},
abstract = {With the development of information transmission technology and computer technology, information acquisition mode is mainly converted from character to image nowadays. However, in the process of acquiring and transmitting images, image damage and quality decrease due to various factors. Therefore, how to restore image has become a research hotspot in the field of image processing. This paper establishes an image restoration model based on BP neural network. The simulation results show that the proposed method has made a great improvement compared with the traditional image restoration method.}
}
@article{CHEN201910,
title = {An artificial intelligence based data-driven approach for design ideation},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {10-22},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300604},
author = {Liuqing Chen and Pan Wang and Hao Dong and Feng Shi and Ji Han and Yike Guo and Peter R.N. Childs and Jun Xiao and Chao Wu},
keywords = {Idea generation, Artificial intelligence in design, Data-driven design, Generative adversarial networks, Semantic network analysis, Network visualisation, Computational creativity},
abstract = {Ideation is a source of innovation and creativity, and is commonly used in early stages of engineering design processes. This paper proposes an integrated approach for enhancing design ideation by applying artificial intelligence and data mining techniques. This approach consists of two models, a semantic ideation network and a visual concepts combination model, which provide inspiration semantically and visually based on computational creativity theory. The semantic ideation network aims to provoke new ideas by mining potential knowledge connections across multiple knowledge domains, and this was achieved by applying “step-forward” and “path-track” algorithms which assist in exploring forward given a concept and in tracking back the paths going from a departure concept through a destination concept. In the visual concepts combination model, a generative adversarial networks model is proposed for generating images which synthesize two distinct concepts. An implementation of these two models was developed and tested in a design case study, which indicated that the proposed approach is able to not only generate a variety of cross-domain concept associations but also advance the ideation process quickly and easily in terms of quantity and novelty.}
}
@article{CHANG2019105,
title = {Research on sports video image based on fuzzy algorithms},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {105-111},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.033},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300847},
author = {Wu-Yeh Chang},
keywords = {Fuzzy clustering algorithm, Image segmentation, Membership, Motion video},
abstract = {With the rapid development of network and multimedia technology, a large number of sports and national fitness information are stored in various fitness guidance systems in the form of video and pictures. In order to better promote public fitness and facilitate learning and viewing, sports video has a variety of needs of editing, segmentation and integration. Aiming at the shortcomings of current sports video image segmentation methods, such as rough segmentation results and high spatial distortion rate, a sports video image segmentation method based on fuzzy clustering algorithm is proposed. This paper introduces the basic theory of fuzzy clustering algorithm, establishes second-order fuzzy attributes with normal distribution and gray value by means of time-domain difference images, assigns the fuzzy attribute S membership function, then performs fuzzy clustering on time-domain difference images, and obtains the segmentation results of moving video images by edge detection. The experimental results show that the method has high spatial accuracy, good noise iteration performance and low spatial distortion rate, and can accurately segment complex moving video images to obtain high-definition images.}
}
@article{BRIBIESCA201993,
title = {A chain code for representing high definition contour shapes},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {93-104},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301142},
author = {Ernesto Bribiesca and Fernanda Bribiesca-Contreras and Ángel Carrillo-Bermejo and Graciela Bribiesca-Correa and Nidiyare Hevia-Montiel},
keywords = {Slope chain code, Extended slope chain code, High definition contour shapes, Reconfigurable chain code, Bird wings},
abstract = {The biggest disadvantage of using chain code techniques is the generation of low definition contour shapes, in this paper we present the Extended Slope Chain Code (ESCC) which is an improvement on the Slope Chain Code (SCC). The ESCC is focused on the representation of high definition contour shapes. Generally speaking, most chain codes hold the length of the straight-line segments which represent the contour shape as a constant. In this case, the contour shapes represented by ESCC are composed of variable segments, which allow us to have a better description of the contour shape. Thus, the length of the segments are a function of the slope changes, i.e. the length of the next segment depends on the value of the slope change at that point. Therefore, the ESCC is continuously adjusting to the curvature requirements of contour shapes, in order to have a better description of contour shapes.}
}
@article{HUANG2019129,
title = {BALG: An alternative for fast and robust feature matching},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {129-139},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300690},
author = {Zhoudi Huang and Zhenzhong Wei and Guangjun Zhang},
keywords = {Feature matching, Binary descriptor, Affine invariant, Sequence order, Local geometric check},
abstract = {Robust feature matcher is usually inapplicable to real time computer vision applications as its high computational complexity, while fast feature matcher is short of robustness to geometric transformations. By studying the sampling pattern of binary descriptor and local geometric statistics of features, this paper proposes a fast and robust feature matching method with binary affine invariant descriptor and local geometric consistency check (BALG). The sampling pattern is adaptively adjusted according local affine transformation that can be effectively estimated from the local intensity moments. The affine sampling pattern improves the affine invariance of binary descriptor while enable fast processing. Furthermore, candidate matches are sorted into local groups along different orientations with sequence order constraint, and then followed by local geometric consistency check. False matches are efficiently filtered out with high recall. Extensive experiments on four publicly benchmark datasets prove the proposed method to be an alternative for time critical feature matcher.}
}
@article{KOSSYK2019121,
title = {Discriminative regularization of the latent manifold of variational auto-encoders},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {121-129},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301026},
author = {Ingo Kossyk and Zoltán-Csaba Márton},
keywords = {Variational auto-encoder, Regularization, Knowledge representation, Perceptual data compaction, Semi-supervised learning, Statistical performance analysis},
abstract = {We present an approach on training classifiers or regressors using the latent embedding of variational auto-encoders (VAE), an unsupervised deep learning method, as features. Usually VAEs are trained using unlabeled data and independently from the classifier, whereas we investigate and analyze the performance of a classifier or regressor that is trained jointly with the variational deep network. We found that models trained this way can improve the embedding s.t. to increase classification performance, and also can be used for semi-supervised learning, building up the information extracting latent representation in an incremental fashion. The model was tested on two widely known computer vision benchmarks, and its generalization power was evaluated on an independent dataset. Additionally, generally applicable statistical methods are presented for evaluating similarly performing classifiers, and used to quantify the performance increase. The general applicability and ease-of-use of deep learning approaches allows for a wide applicability of the method.}
}
@article{ZHENG2019563,
title = {Principal characteristic networks for few-shot learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {563-573},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300574},
author = {Yan Zheng and Ronggui Wang and Juan Yang and Lixia Xue and Min Hu},
keywords = {Few-shot learning, Principal characteristic, Mixture loss function, Embedding network, Fine-tuning},
abstract = {Few-shot learning aims to build a classifier that recognizes unseen new classes given only a few samples of them. Previous studies like prototypical networks utilized the mean of embedded support vectors to represent the prototype that is the representation of class and yield satisfactory results. However, the importance of these different embedded support vectors is not studied yet, which are valuable factors that could be used to push the limit of the few-shot learning. We propose a principal characteristic network that exploits the principal characteristic to better express prototype, computed by distributing weights based on embedded vectors’ different importance. The high-level abstract embedded vectors are extracted from our eResNet embedding network. In addition, we proposed a mixture loss function, which enlarges the inter-class distance in the embedding space for accurate classification. Extensive experimental results demonstrate that our network achieves state-of-the-art results on the Omniglot, miniImageNet and Cifar100 datasets.}
}
@article{XIE201962,
title = {Jointly social grouping and identification in visual dynamics with causality-induced hierarchical Bayesian model},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {62-75},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300057},
author = {Zhao Xie and Tianfu Wu and Xingming Yang and Luming Zhang and Kewei Wu},
keywords = {Group activity detection and recognition, Casual context, Granger casual topic model},
abstract = {We concentrate on modeling the person-person interactions for group activity recognition. In order to solve the complexity and ambiguity problems caused by a large number of human objects, we propose a causality-induced hierarchical Bayesian model to tackle the interaction activity video, referring to the “what” interaction activities happen, “where” interaction atomic occurs in spatial, and “when” group interaction happens in temporal. In particular, Granger Causality has been characterized with multiple features to encode the interacting relationships between each individual in the group. Furthermore, to detect and identify the concurrent interactive simultaneously, we investigate the Relative Entropy as a metric to measure the reasonable motion dependency between two arbitrary individuals. Filtered by the causality dependency, causality motion features have been cast as the multiplicative probabilistic ingredients in Bayesian factors to formulate the compact learned latent interaction patterns aggregately that enable the power of discrimination. Experiments demonstrate our model outperforms state-of-the-art models.}
}
@article{KONG2019215,
title = {Learning spatiotemporal representations for human fall detection in surveillance video},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {215-230},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.024},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300331},
author = {Yongqiang Kong and Jianhui Huang and Shanshan Huang and Zhengang Wei and Shengke Wang},
keywords = {Fall detection, Human silhouette, Motion history image, Dynamic image, Convolutional Neural Networks, High-quality representation},
abstract = {In this paper, a computer vision based framework is proposed that detects falls from surveillance videos. Firstly, we employ background subtraction and rank pooling to model spatial and temporal representations in videos, respectively. We then introduce a novel three-stream Convolutional Neural Networks as an event classifier. Silhouettes and their motion history images serve as input to the first two streams, while dynamic images whose temporal duration is equal to motion history images, are used as input to the third stream. Finally, we apply voting on the results of event classification to perform multi-camera fall detection. The main novelty of our method against the conventional ones is that high-quality spatiotemporal representations in different levels are learned to take full advantage of the appearance and motion information. Extensive experiments have been conducted on two widely used fall datasets. The results have shown to demonstrate the effectiveness of the proposed method.}
}
@article{ZHANG2019180,
title = {Infrared dim target detection method based on the fuzzy accurate updating symmetric adaptive resonance theory},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {180-191},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.018},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300719},
author = {Shuai Zhang and Fuyu Huang and Bingqi Liu and Hao Yu and Yichao Chen},
keywords = {Infrared dim target detection, Human vision system, Fuzzy accurate updating symmetric adaptive resonance theory, ROI model},
abstract = {Motivated by the human visual system, we propose an infrared dim target detection method that is based on a fuzzy accurate updating symmetric adaptive resonance theory network. From the bottom-up perspective, the regions of interest (ROIs) are extracted using a difference of Gaussians at multiple scales and our designed ROI model in a saliency map. From the top-down perspective, five feature categories are extracted using the ROI model, which are used to train the proposed Fuzzy AUSART network. The well-trained network realizes the true identification of all ROI candidates. The results of the receiver operating characteristic (ROC) curves verify that the proposed method can better adapt to different circumstances and targets in our experiment. The average detection accuracy of the Fuzzy AUSART is improved by 15.4%, and the average F1 index of the proposed method is higher than six typical comparison methods by more than a factor of 2.48.}
}
@article{ZHOU2019305,
title = {Deep blind quality evaluator for multiply distorted images based on monogenic binary coding},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {305-311},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930094X},
author = {Wujie Zhou and Lu Yu and Yaguan Qian and Weiwei Qiu and Yang Zhou and Ting Luo},
keywords = {Quality assessment, Monogenic binary coding, Local structural information, Blind prediction, Deep neural network},
abstract = {Perceptual quality evaluation of multiply distorted images has become a very challenging research topic. In this paper, we present a novel and efficient deep blind quality evaluator for multiply distorted images based on monogenic binary coding (MBC). Local complementary structural information and a deep learning method are employed to blindly evaluate the quality of multiply distorted images. First, a monogenic signal representation is utilized to decompose a multiply distorted image into three complementary components: orientation, phase, and magnitude. The quality-predictive features are then determined from the complementary components. Finally, the features are mapped to the human quality score of the multiply distorted image based on the deep neural network. The results on two newly established multiply distorted image subjective databases confirm that our metric has a better prediction performance than existing state-of-the-art full-reference and classical blind metrics.}
}
@article{LI2019276,
title = {Reinforcement learning based coding unit early termination algorithm for high efficiency video coding},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {276-286},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.021},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300677},
author = {Na Li and Yun Zhang and Linwei Zhu and Wenhan Luo and Sam Kwong},
keywords = {Coding tree unit, Early termination, High efficiency video coding, Markov decision processing, Actor-critic, Reinforcement learning},
abstract = {In this paper, we propose a Reinforcement Learning (RL) based Coding Unit (CU) early termination algorithm for High Efficiency Video Coding (HEVC). RL is utilized to learn a CU early termination classifier independent of depths for low complexity video coding. Firstly, we model the process of CU decision as a Markov Decision Process (MDP) according to the Markov property of CU decision. Secondly, based on the MDP, a CU early termination classifier independent of depths is learned from trajectories of CU decision across different depths with the end-to-end actor-critic RL algorithm. Finally, a CU decision early termination algorithm is introduced with the learned classifier, so as to reduce computational complexity of CU decision. We implement the proposed scheme with different neural network structures. Two different neural network structures are utilized in the implementation of RL based video encoder, which are evaluated to reduce video coding complexity by 34.34% and 43.33%. With regard to Bjøntegaard delta peak signal-to-noise ratio and Bjøntegaard delta bit rate, the results are −0.033 dB and 0.85%, −0.099 dB and 2.56% respectively on average under low delay B main configuration, when compared with the HEVC test model version 16.5.}
}
@article{PRASADARAO2019339,
title = {An integrated approach to emotion recognition and gender classification},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {339-345},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300951},
author = {K. {Prasada Rao} and M.V.P. {Chandra Sekhara Rao} and N. {Hemanth Chowdary}},
keywords = {Emotion recognition, MFCC, MSER, Speeded Up Robust Features (SURF), SVM, Viola Jones},
abstract = {The human-computer communication adds to worldwide access to training, information upgrade and the conveyance of value learning and instructions. Furthermore, the changing paradigm of machine perceiving the human emotions effectively is a technological advancement for the learning community. The automatic recognition of emotions reflected from speech and facial expressions for making the machine to understand the human verbal and non-verbal emotions is coined as “Emotion Recognition”. Emotion recognition systems, regardless, are not totally seen as subject independent dynamic features, so they are not sufficiently vigorous for constant acknowledgment assignments with subject assortment (human face), head movement, illumination change, speech, noise and so on when thought about exclusively. Hence, it is proposed to fuse the features of facial expressions and speech. The present framework utilizes the Speech (Mel Frequency Cepstral Coefficients) features and Facial (Maximally Stable Extremal Regions) features to predict the emotions of a person through a systematic and scientific study. Specifically, when combining MSER with MFCC, the recognition rates can be further improved by 2 to 3% on Indian Face Database and Berlin Speech Database.}
}
@article{ZHANG2019415,
title = {Multi-level and multi-scale deep saliency network for salient object detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {415-424},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.034},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300434},
author = {Qing Zhang and Jiajun Lin and JingJing Zhuge and Wenhao Yuan},
keywords = {Saliency detection, Salient object detection, Fully convolutional neural network, Multi-scale features},
abstract = {Traditional saliency model usually utilize handcrafted image features and various prior knowledge to pop out salient regions from complex surroundings. In this paper, we propose a novel FCN-like deep convolutional neural network for pixel-wise salient object detection. Our deep network automatically learns multi-level feature from different convolutional layers of a pre-trained convolutional neural network. Moreover, deeper side outputs are connected to the shallower ones, which provides a better feature representation and helps shallow side outputs to accurately locate salient regions. In addition, we adopt a weighted-fusion module to combine different side outputs for utilizing multi-scale and multi-level features. Finally, a fully connected CRF model can be optimally incorporated to improve spatial coherence and contour localization in the fused saliency map. Both qualitative and quantitative evaluations on four publicly available datasets demonstrate the robustness and efficiency of our proposed approach against 17 state-of-the-art methods.}
}
@article{CAI2019295,
title = {A real-time detection method of building energy efficiency based on image processing},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {295-304},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.032},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300835},
author = {Wei Cai and Xiaodong Wen and Saisai Wang and Lijuan Wang},
keywords = {Building energy efficiency, Image processing, Air tightness, Heat transfer coefficient},
abstract = {In recent years, with the rapid development of economy and the acceleration of urbanization process, building energy consumption has become an urgent problem to be solved. Its total energy consumption shows a trend of sustained growth, and the growth rate is faster and faster, so the requirements of building energy efficiency detection technology are higher and higher. Aiming at the problems of time-consuming and difficult detection of traditional detection methods, this paper introduces infrared image processing technology and proposes an energy-saving detection method for buildings based on image processing. The air tightness and heat transfer coefficient are studied respectively. Firstly, the feasibility of infrared method to detect air tightness is theoretically analyzed, and the corresponding technical scheme and detection process are given. Secondly, the infrared image is grayed and filtered to eliminate the influence of thermal defect area on the calculation of heat transfer coefficient after eliminating interference noise. Finally, based on the one-dimensional steady heat transfer theory, the principle of quantitative detection of heat transfer coefficient in infrared image is given by studying the heat transfer process of the wall, and the related correction method is designed. The feasibility and superiority of this method are illustrated by an example. Therefore, this method has good application prospects in building energy-saving detection.}
}
@article{XU2019159,
title = {Gait recognition based on capsule network},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {159-167},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300318},
author = {Zhaopeng Xu and Wei Lu and Qin Zhang and Yuileong Yeung and Xin Chen},
keywords = {Gait recognition, Capsule network, Deep learning},
abstract = {Gait as a biometric feature is widely used for human identification, and gait recognition has recently become a significant research problem. According to a small amount of labeled multi-view, multi-walking-condition and multi-clothes-condition human walking videos, we can find an effective model based on capsule network to capture more discriminative features and promote gait recognition performance. This paper works on gait recognition based on capsule network and we consider two different architectures, namely matching local features at the bottom layer based on capsule network and matching mid-level features at the middle layer based on capsule network, input images such as GEI, CGI, and resolution of input image. Empirical evaluations are conducted in the aspect of kinds scenarios, namely cross-walking-condition, cross-view and cross-clothes condition. The approaches are evaluated on the CASIA-B dataset and OU-ISIR Treadmill dataset B. These results show that the methods exceed the previous state-of-the-art outcomes.}
}
@article{WU201974,
title = {A combination of color-black-and-white visual cryptography and polynomial based secret image sharing},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {74-84},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.020},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301130},
author = {Xiaotian Wu and Ching-Nung Yang},
keywords = {Visual cryptography, Secret image sharing, Color, Embedding, Polynomial, Black and white},
abstract = {A new (k,n) threshold secret image sharing scheme with two decoding options is introduced in this paper. The proposed scheme combines color-black-and-white visual cryptography scheme (CBW-VCS) and polynomial based secret image sharing (PSIS) together, to offer stacking-to-see decryption and lossless image reconstruction. To construct the color shares, a general (k,n) threshold CBW-VCS is firstly given. A grayscale secret image is converted to a p-radix image and a binary image. The p-radix image is encrypted by the (k,n) PSIS under mod p operation, and n p-radix shadows are obtained. Then, a color share generation algorithm with data embedding is proposed to construct n color shares. Theoretical analysis and sufficient experiments are provided to show the effectiveness and advantages of the proposed scheme.}
}
@article{LI201923,
title = {A novel method of text line segmentation for historical document image of the uchen Tibetan},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {23-32},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.021},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300288},
author = {Zhenjiang Li and Weilan Wang and Yang Chen and Yusheng Hao},
keywords = {Tibetan historical document, Text line segmentation, Baseline, Upper edge, Connected region analysis, Dataset, Image processing},
abstract = {Text line segmentation is a key step in Tibetan historical document recognition. A novel method for text line segmentation was proposed based on the baseline in uchen Tibetan, and a new dataset was released, which was used to evaluate the results of text line segmentation of uchen Tibetan historical documents. In this paper, there were two steps for the proposed method: baseline detection and text line segmentation using the baseline. In baseline detection, the upper edges of all characters in the document were obtained by a horizontal gradient operator, then an edge connectivity definition was proposed by which the upper edge set was divided into disjoint subsets. Eligible sets were selected from these subsets, and the edges in these sets were joined in turn to obtain the baseline. In text line segmentation, the document image was truncated at the baseline position, then the adhesion regions were segmented again. Each connected region in the image was assigned to its nearest baseline. All connected regions belonging to the same baseline formed a text line. Experiments on the proposed dataset showed that the method could effectively avoid document distortion, the accuracy of text line segmentation was high, and the text line adhesion could be handled.}
}
@article{DONG2019380,
title = {Outlier detection and disparity refinement in stereo matching},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {380-390},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301014},
author = {Qicong Dong and Jieqing Feng},
keywords = {Outlier detection, Stereo matching, Match fixed point jumps, Normal-based plane fitting},
abstract = {Disparity estimation in ill-posed regions, such as occlusions, repetitive patterns and textureless regions, is a challenging problem in stereo matching. The initial disparities obtained in these regions tend to be regarded as outliers that must be detected and addressed. In this paper, two outlier detection methods are proposed, i.e., the efficient approach and the accurate approach. The efficient approach detects outliers by exploring the disparity map for the left image only and reduces runtime and memory costs. First, the match fixed point jumps (MFPJ) algorithm is proposed as an initial solution to detect outliers. Then, a high-probability outlier detection algorithm is proposed to accomplish denser outlier detection with less noise. The accurate approach first classifies outliers as occlusions or mismatches. Then, 3D label assignment is performed for occlusion outliers and normal-based plane fitting is conducted for mismatch outliers to refine the disparities of the outliers and to achieve an accurate stereo matching result. Evaluations of the Middlebury datasets demonstrate that the proposed methods effectively improve the stereo matching performance.}
}
@article{YOUS2019486,
title = {CNN-based method for blotches and scratches detection in archived videos},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {486-500},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300562},
author = {Hamza Yous and Amina Serir and Sofiane Yous},
keywords = {Digital archived video restoration, Defects detection, Convolutional neural network, Deep learning},
abstract = {In this work, we present a fully connected convolutional encoder-decoder for defects detection in archived video. The proposed method handles the detection of two of the most common archived video-related defects, namely blotches and scratches. It consists of two stages: (1) pixel-level classification and description of each video frame into defects pixels or not, by means of a novel CNN-based encoder-decoder architecture, and (2) spatio-temporal analysis to group and fine-tune the detections. For blotch detection, the learned features, extracted from an intermediate stage of the network, are used to evaluate the dissimilarity between the pre-selected regions in consecutive frames. For scratches detection, the morphology of scratches is used to eliminate false alarms. The experiments are performed on various video sequences suffering synthetic and real scratches and blotches. The results demonstrate the effectiveness of our approach and significant improvement against the most recent detectors.}
}
@article{SRINIVASARAO2019140,
title = {Hardware implementation of digital image skeletonization algorithm using FPGA for computer vision applications},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {140-149},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300045},
author = {Perumalla {Srinivasa Rao} and Kamatham Yedukondalu},
keywords = {FPGA, Skeleton, Gray scale images, Computer vision, 2-D image},
abstract = {Nowadays embedded multimedia devices are designed for computationally intensive applications such as image processing in various multimedia systems. Image processing algorithms should be implemented on hardware platforms for improving the performance. Reconfigurable hardware implementation using Field Programmable Gate Arrays (FPGAs) provides low latency with high performance in real time applications. FPGAs offer the reprogrammability of an application specific solution while retaining the performance advantage. In real time applications as image sizes increase rapidly, only hardware systems must be used with low complex software. In this paper, main perspective of developing and implementing skeletonization algorithm as a part of computer vision, pattern recognition application is focused and presented. A simple algorithm to skeletonize the 2-D image using MATLAB is developed. An architecture and implementation of this skeletonization algorithm for 2-D gray scale images is proposed. For analyzing pixel values 3 × 3 windowing operator is used. The proposed architecture is tested for an image size of 8 × 8, but the approach presented in this paper can be used for images of any size (M × N), if the FPGA memory is sufficiently large. The implementation was carried out on Xilinx Vertex 5 board.}
}
@article{ZHANG201969,
title = {Matrix permutation meets block compressed sensing},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {69-78},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.023},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300707},
author = {Bo Zhang and Yulin Liu and Jie Zhuang and Kai Wang and Yuqiang Cao},
keywords = {Matrix permutation, Image compression, Image coding, Block compressed sensing},
abstract = {Traditional block compressed sensing (CS) schemes encode nature images via a fixed sampling rate without taking the sparsity level differences among the blocks into consideration. In order to improve sampling efficiency, permutation-based block CS (BCS) schemes are proposed. In these schemes, the crux is to find a good permutation strategy which can make the nonzero entries evenly distributed among the blocks. In order to make the nonzero entries distributed among the blocks as evenly as possible, a novel matrix permutation strategy is proposed in this paper. Then, a BCS scheme with matrix permutation (BCS-MP) is proposed, which can be utilized to encode nature images effectively. Simulation results show that the proposed approach gets a significant gain of peak signal-to-noise ratio (PSNR) of the reconstructed-images compared with the state-of-the-art permutation-based ones and the traditional non-permutation one at the cost of slightly increasing the encoding time.}
}
@article{LIU201989,
title = {A weighted edge-based level set method based on multi-local statistical information for noisy image segmentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {89-107},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930001X},
author = {Cheng Liu and Weibin Liu and Weiwei Xing},
keywords = {Noisy image segmentation, Active contour model, Level set method, Weighted coefficients, Local statistical information, Edge stop function},
abstract = {Image segmentation plays a fundamental role in image processing. Active contour models have been widely used since they handle topological change easily and provide smooth contours. However, noise presents challenges for edge-based level set methods since it leads contours easily passing through objects or falling into local minima. In this paper, we propose a weighted edge-based level set method based on multi-local statistical information to better segment noisy images. Through analysing the deficiencies of constant length and regional coefficients and traditional edge stop function in noisy image segmentation, weighted length and regional coefficients and modified edge stop function are proposed to overcome their shortcomings, respectively. The weighted edge-based level set method is used to segment synthetic and real images that have added different types and levels of noise. The experiments indicate that our method provides higher segmentation accuracies and more accurate segmentation results, which demonstrate its effectiveness and robustness.}
}
@article{DU2019380,
title = {A low-cost, accurate strain measurement using multi-view amplification mechanism and visual polydimethylsiloxane lens},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {380-386},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.036},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300422},
author = {Qiu-Yue Du and Bin Tian},
keywords = {Strain measurement, Amplification mechanism, Visual PDMS lens, Smart-phone microscope},
abstract = {Strain sensors are widely applied in the industry requiring specific signal amplifiers and signal readers. However, the devices are unable to read data quickly and intuitively. The present study introduces a highly accurate, repeatable, mold-free strain sensor comprising of an amplification mechanism and a smart-phone microscope. Generation of the amplification mechanism using rapid prototyping especially 3D printing is summarized and evaluated. The sensitivity of the proposed amplifier is (39.47 ± 1.34) for nylon and (37.74 ± 2.41) for ABS. A visual polydimethylsiloxane (PDMS) lens with a focal length of 7.23 mm is attached to the iphone6 camera, performing as a microscope for image acquisition, which provides an equivalent focal length of 6.7 mm and a resolution of 0.691 µm.}
}
@article{LIU2019312,
title = {Correlation identification in multimodal weibo via back propagation neural network with genetic algorithm},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {312-318},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300665},
author = {Maofu Liu and Weili Guan and Jie Yan and Huijun Hu},
keywords = {Multimodal weibo, Correlation classification model, Back propagation, Genetic algorithm, GA-BP},
abstract = {The rapid development of social media services has spawned abundant user generated contents (UGC), such as Sina Weibo, which is one of the biggest Chinese microblogging platforms. In order to enhance the quality and popularity of the posted weibo (the microblog), Weibo users usually embed some social information and images or micro-videos, namely the multimodal weibo, and we assume that there is a close correlation among the multimodal weibo data, especially between the visual data (image/micro-video) and its corresponding text, for a multimodal weibo of high quality. Hence, we try to evaluate the quality of multimodal weibo via analyzing the correlation in the multimodal weibo. This paper constructs the classification model based on back propagation (BP) neural network with genetic algorithm (GA), to automatically identify the correlation within the multimodal weibo, and investigates three kinds of features from multimodal weibo to uncover their contributions to the correlation. The experimental results verify the superiority of the GA-BP based classification model over the traditional BP neural network.}
}
@article{CHEN2019149,
title = {Saliency prediction by Mahalanobis distance of topological feature on deep color components},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {149-157},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.026},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930077X},
author = {Jiazhong Chen and Qingqing Li and Ping Li and Yu Han and Lei Wu and Hefei Ling and Weimin Wu},
keywords = {Saliency, Deep color components, Topological feature, Covariance matrix, Mahalanobis distance},
abstract = {A new saliency prediction method via extracting topological feature and calculating Mahalanobis distance on deep color components is presented in this paper. Specifically, four selectable schemes of color components are considered and a deep convolutional network is used to learn the best scheme. Then the topological feature maps of an input image are extracted on the learned color components by the analysis of connectivity and adjacency. To achieve the final saliency map, a new fusion method is proposed by calculating the Mahalanobis distance between the feature maps and their means with their covariance matrices rather than summating the feature maps linearly. The numerical and visual evaluation shows that a competitive performance compared with fourteen state-of-the-art models is achieved by the proposed method.}
}
@article{YANG2019328,
title = {A content-based rate control algorithm for screen content video coding},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {328-338},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.031},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300823},
author = {Yang Yang and Liquan Shen and Hao Yang and Ping An},
keywords = {Screen content video, Rate control, Parameter update, Bit allocation},
abstract = {The popularity of 3D screen content videos like 3D animations calls for better coding performance of screen content videos. The exiting R-λ rate control model is designed to enable the coding performance of conventional nature videos, which can not accurately describe the Rate-Distortion (R-D) relationships of the screen content videos, especially for videos containing limited colors and sharper edges. This paper proposes a new rate control (RC) scheme which takes the divergent characteristics of text contents, screen images and nature images in screen content videos into consideration. In particular, the content-based rate control algorithm is developed at Coding Tree Unit (CTU) level, where CTUs are divided into three categories including Text-CTUs (T-CTUs), Screen Image-CTUs (SI-CTUs) and Nature Image-CTUs (NI-CTUs). Three R-λ models reflecting different R-D relationships of different CTUs are established, and the model parameters are updated on the basis of their own models. Furthermore, in view of the fact that continuity and mutability simultaneously exist in screen content videos, two bit allocation schemes for regions containing only one content are determined in this paper. Experimental results show that the proposed RC scheme achieves 0.749 dB BDPSNR increase under the Low Delay configuration and 0.637 dB BDPSNR increase under the Random Access configuration on average, compared to the default R-λ model in HEVC Screen Content Coding extension (HEVC-SCC) reference software.}
}
@article{FANG2019327,
title = {Iterative fusion convolutional neural networks for classification of optical coherence tomography images},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {327-333},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.022},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300306},
author = {Leyuan Fang and Yuxuan Jin and Laifeng Huang and Siyu Guo and Guangzhe Zhao and Xiangdong Chen},
keywords = {Classification, Convolutional neural network (CNN), Deep learning, Optical coherence tomography (OCT), Retinal},
abstract = {Optical coherence tomography (OCT) can achieve the high-resolution 3D tomography imaging of the retina, which is crucial for the diagnosis of retinal diseases. Currently, the classification of retinal OCT images is mainly conducted by ophthalmologists, which is time consuming and subjective. In this paper, we propose an iterative fusion convolutional neural network (IFCNN) method for the automatic classification of retinal OCT image. In the convolutional neural network (CNN), different convolutional layers contain feature information from different scales. Therefore, the proposed network adopts an iterative fusion strategy, which iteratively combines features in current convolutional layer with those of all previous layers in the CNN network, and thus can jointly utilize the features of different convolutional layers to achieve accurate classification of OCT images. Experimental results on a real retinal OCT dataset and a musculoskeletal radiographs dataset demonstrate the superiority of the proposed method over the traditional CNN and several well-known OCT classification methods.}
}
@article{TU201964,
title = {A multi-view object tracking using triplet model},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {64-68},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.032},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300392},
author = {Bing Tu and Wenlan Kuang and Yongheng Shang and Danbing He and Lin Zhao},
keywords = {Object tracking, BING feature, Triplet model},
abstract = {Object tracking is very important in intelligent systems, such as video surveillance, automatic drive and traffic security. Background subtraction algorithm is a mature method for foreground object extraction, but it may be affected by complicated background or the change of object shape. So in this paper, a novel object tracking method is proposed using a triplet model. First, BING feature is used to find some potential object proposals. Then, we construct a triplet model for each potential object. The triplet of the same object between two consecutive frames is considered similar. Finally, object tracking can be achieved by computing feature difference of triplets. Experimental results show that our method can achieve object tracking effectively and in real time.}
}
@article{AGUILAR2019360,
title = {Regularized uncertainty-based multi-task learning model for food analysis},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {360-370},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301002},
author = {Eduardo Aguilar and Marc Bolaños and Petia Radeva},
keywords = {Multi-task models, Uncertainty modeling, Convolutional neural networks, Food image analysis, Food recognition, Food group recognition, Ingredients recognition, Cuisine recognition},
abstract = {Food plays an important role in several aspects of our daily life. Several computer vision approaches have been proposed for tackling food analysis problems, but very little effort has been done in developing methodologies that could take profit of the existent correlation between tasks. In this paper, we propose a new multi-task model that is able to simultaneously predict different food-related tasks, e.g. dish, cuisine and food categories. Here, we extend the homoscedastic uncertainty modeling to allow single-label and multi-label classification and propose a regularization term, which jointly weighs the tasks as well as their correlations. Furthermore, we propose a new Multi-Attribute Food dataset and a new metric, Multi-Task Accuracy. We prove that using both our uncertainty-based loss and the class regularization term, we are able to improve the coherence of outputs between different tasks. Moreover, we outperform the use of task-specific models on classical measures like accuracy or F1.}
}
@article{CHAABOUNI201979,
title = {ChaboNet : Design of a deep CNN for prediction of visual saliency in natural video},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {79-93},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300550},
author = {Souad Chaabouni and Jenny Benois-Pineau and Chokri {Ben Amar}},
keywords = {Visual attention prediction in video, Deep convolutional neural networks, Saliency map, Residual motion, Dynamic content},
abstract = {Prediction of visual saliency in images and video is needed for video understanding, search and retrieval, coding, watermarking and other applications. The majority of prediction models are founded only on “bottom-up” features. Nevertheless, the “top-down” component of human visual attention becomes prevalent as human observers explore the visual scene. Visual saliency which is always a mix of bottom-up and top-down cues can be predicted on the basis of seen data. In this paper, a model of prediction of visual saliency in video on the basis of Deep convolutional neural networks (CNNs) is proposed. A Deep CNN architecture is designed. Various input channels for a CNN architecture are studied: using the known sensitivity of human visual system to residual motion, pixel colour values are completed with residual motion map. The latter is a normalized energy of residual motion in video frames with regard to the estimated global affine motion model. The experiments show that the choice of the input features for the Deep CNN depends on visual task: for highly dynamic content, the proposed model with residual motion is more efficient and gives decent results with relatively shallow Deep architecture.}
}
@article{ZHOU2019158,
title = {No-reference quality assessment for contrast-distorted images based on multifaceted statistical representation of structure},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {158-169},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.028},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300793},
author = {Yu Zhou and Leida Li and Hancheng Zhu and Hantao Liu and Shiqi Wang and Yao Zhao},
keywords = {Quality assessment, No-reference, Contrast-distorted images, Structure representation, Back propagation (BP)},
abstract = {In many real-world applications, images are prone to be degraded by contrast distortions during image acquisition. Quality assessment for contrast-distorted images is vital for benchmarking and optimizing the contrast-enhancement algorithms. To this end, this paper proposes a no-reference quality metric for contrast-distorted images based on Multifaceted Statistical representation of Structure (MSS). The “Multifaceted” has two meanings, namely (1) not only the luminance information, but also the chromatic information is used for structure representation. This is inspired by the fact that the chromatic information on the one hand affects the perception of image quality as well, and on the other hand it changes along with the contrast distortions. Therefore, the chromatic information should be integrated with the luminance information for quality assessment of contrast-distorted images, a fact most existing quality metrics overlook; (2) regarding structure representation, three aspects, i.e. spatial intensity, spatial distribution, and orientation of structures are calculated, which is enlightened by the fact that the human visual system (HVS) is sensitive to the three aspects of structures. Specifically, the image is first transformed from RGB to the S-CIELAB color space to obtain a representation that is more consistent with the characteristics of the HVS, as well as to separate the chromatic information from the luminance. Then the statistical structural features are extracted from both luminance and chromatic channels. Finally, the back propagation (BP) neural network is adopted to train a quality prediction model. Experimental results conducted on four public contrast-distorted image databases demonstrate the superiority of the proposed method to the relevant state-of-the-arts.}
}
@article{GE2019309,
title = {The application and design of neural computation in visual perception},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {309-315},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300276},
author = {Hengqing Ge and Haichun Yu},
keywords = {Neural computing, Visual perception, Contour detection, Face recognition},
abstract = {Visual perception is an important way for organisms to perceive the external world. Simulation of visual cognitive process can enhance the cognitive ability of machine vision. Therefore, how to simulate visual perception system and make computer have a high world understanding ability is a hotspot of current neurocomputing. Based on the information processing mechanism of visual perception system, this paper establishes a neural computing model based on visual perception mechanism. The simulation results of standard face database and natural landscape images show that the proposed method can recognize face samples better when other noise samples are added to the face image samples. In landscape contour fitting simulation, the results show that although this method has little advantage for large contour image recognition, but for small contour recognition, this method is obviously superior to other methods.}
}
@article{LIU2019150,
title = {Multimedia retrieval by deep hashing with multilevel similarity learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {150-158},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318302840},
author = {Qiuli Liu and Lu Jin and Zechao Li and Jinhui Tang},
keywords = {Multimedia retrieval, Deep neural networks, Hashing and multilevel similarity correlation measurement},
abstract = {Deep multimodal hashing has received increasing research attention in recent years due to its superior performance for large-scale multimedia retrieval. However, limited e orts have been made to explore the complex multilevel semantic structure for deep multimodal hashing. In this paper, we propose a novel deep multimodal hashing method, termed as Deep Hashing with Multilevel Similarity Learning (DHMSL), for learning compact and discriminative hash codes, which explores multilevel semantic similarity correlations of multimedia data. In DHMSL, multilevel similarity correlation is explored to learn the unified binary hash codes by exploiting the local structure and semantic label information simultaneously. Meanwhile, the bit balance and quantization constraints are taken into account to further make the unified hash codes compact. With the unified binary codes learned, two deep neural networks are jointly trained to simultaneously learn feature representations and two sets of nonlinear hash functions. Specifically, the well-designed loss functions are introduced to minimize the prediction errors of the feature representations as well as the errors between the unified binary codes and outputs of the networks. Extensive experiments on two widely-used multimodal datasets demonstrate that the proposed method can achieve the state-of-the-art performance for both image-query-text and text-query-image tasks.}
}
@article{XIAO2019287,
title = {Detection and segmentation of underwater CW-like signals in spectrum image under strong noise background},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {287-294},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.036},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300938},
author = {Zhaolin Xiao and Meng Zhang and Lisheng Chen and Haiyan Jin},
keywords = {CW-like signal, Signal spectrum analysis, Gaussian mixture model, Clustering analysis, Signal detection},
abstract = {Aiming at the detection and segmentation of underwater Continuous wave-like (CW-like) signals of under strong noise sea background, this paper introduces a Gaussian mixture model clustering method by analyzing the signal spectrum features. First, the time domain original signal is converted to its frequency domain correspondence by Windowed Fast Fourier Transform. Second, by studying on the feature of target signal, we introduce a spectrum filtering to alleviate the background noise of ocean environment, which is analyzed and calculated with both time and frequency information. Then, the target echo location signals is constructed using a Gaussian mixture mode based binary clustering algorithm. Finally, we use the EM algorithm and adaptive binarization for solving and optimizing the clustering results. Experimental results have shown the accuracy and efficiency of our detection, which can be also simply modified and applied for detecting similar and specific signal from complex background noise.}
}
@article{HUANG201933,
title = {Research on image screening model of ancient villages},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {33-41},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.029},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930080X},
author = {Ying Huang and Qingping Zhang},
keywords = {Ancient villages, Image screening, SIFT, Convolutional neural network},
abstract = {Ancient villages are the carrier of a nation's profound history and culture. They are the integration of history, culture, architecture and sculpture, and have many value attributes. With the development of society, the protection of ancient villages is very important. The establishment of digital archives is an important means to protect ancient villages. Because of the large number and wide distribution of ancient villages, crowd sourcing can quickly and widely access the digital resources of ancient villages. Because of the uneven quality and repetition of the images collected from ancient villages, it is necessary to screen the images of ancient villages. Therefore, this paper proposes a screening model of ancient villages based on SIFT and convolution neural network. Firstly, this paper chooses edge gray change rate and NIQE quality score to evaluate the quality of ancient village image; secondly, extracts SIFT features of image for matching, calculates matching similarity to determine whether the matched image is myopic repetition. Finally, the image is filtered or preserved by using convolution neural network with the edge gray change rate, NIQE quality score and some image attributes as features. Experiments show that the ancient village image screening model designed in this paper has higher accuracy and recall rate than other methods, and has better screening effect.}
}
@article{JI2019439,
title = {Modeling of image, video and text fusion quality data packet system for aerospace complex products based on business intelligence},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {439-447},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.053},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303778},
author = {CuiBin Ji and Guijiang Duan and HanYong Ma and Long Zhang and HuanYun Xu},
keywords = {Balanced scorecard, Business intelligence, Data warehouse, Quality data package, Polymorphic data, Complex product},
abstract = {For the construction of image, video and text fusion quality data packet system during the whole life of complex products, a business intelligence-based logic modeling method is proposed in this paper. As the amount of Polymorphic data from multiple distributed sources continues to grow exponentially, automation tools are becoming critical to decision makers. The balanced scorecard method is used as the basis for modeling, and the traditional dimensions are modified slightly to meet the requirements of quality data management in aerospace enterprises. A data warehouse with predefined fact and dimension tables is created, and a technical solution is provided to meet the requirements and scales of enterprises. In terms of model applications, any enterprise quality manager can extract value from data in the quality data package system for improvement. Online analytical processing (OLAP) cubes support major tasks, such as key quality feature source tracing and analysis, quality issue data mining and integrated quality data delivery.}
}
@article{LIU2019391,
title = {The optimization of sum-product network structure learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {391-397},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300653},
author = {Yang Liu and Tiejian Luo},
keywords = {Machine learning, Deep learning, Sum-product network, Structure learning},
abstract = {Sum-Product Network (SPN) are recently introduced deep tractable Probabilistic Graphical Models providing exact and tractable inference. SPN have been successfully employed as density estimators in some artificial intelligence fields, however, most of the proposed structure learning algorithms focus on improving the performance of a certain aspect of model, at the cost of reducing other performance. This is due to the fact that there is no effective balance between network width and depth during learning process. In this paper, we propose two clustering analysis algorithms to replace the clustering part of LearnSPN. We improve the structure quality of the generated model by deepening the network while adjusting the network width adaptively, trying to find a balance between the expressive power, representation ability, inference accuracy and simplicity. Experimental results prove that LearnSPN equipped by our clustering method has different degrees of improvement in various performances.}
}
@article{ZHOU2019393,
title = {Multi-camera transfer GAN for person re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {393-400},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.029},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300379},
author = {Shuren Zhou and Maolin Ke and Peng Luo},
keywords = {Generative adversarial networks, Deep learning, Computer vision, Person re-identification},
abstract = {Person re-identification is a cross-camera retrieval task. Person re-identification performance in a single dataset has been significantly improved, but person re-identification model trained in one dataset usually can't work well in another dataset. To solve this problem, this paper proposes a method of image-to-image translation, CTGAN (Multi-Camera Transfer GAN), which can be performed on multiple camera domains of pedestrian dataset by using one single model. The marked training images are transferred to each camera of the target dataset. At the same time, for the feature learning model, this paper adopts the MSCDA (Mixed Selective Convolution Descriptor Aggregation) method, which can locate the main pedestrian objects in the image, filter out the background noise, and keep the useful depth descriptor. In the paper, experiments show that the method is effective.}
}
@article{CHANG2019371,
title = {Research on de-motion blur image processing based on deep learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {371-379},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.030},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300811},
author = {Yanfen Chang},
keywords = {Deep learning, Image processing, Blurred image},
abstract = {In recent years, with the rapid development of computer technology and network technology, computer vision has been widely used in various scientific fields. Human motion recognition, as an important branch of computer vision, is essentially to classify human motion information in motion images correctly. It has great significance in intelligent monitoring and security, human-computer interaction, motion analysis and other fields. At present, there are still some problems in human motion recognition methods. Firstly, how to extract and characterize the motion information in images has been one of the difficulties in this field; secondly, with the appearance of kinect and other depth cameras, researchers have provided the depth information of human motion images, and how to effectively use these depth information to achieve human motion recognition and classification is also an important research issue; finally, when the amount of sample data is small, how to use the deep learning network model to achieve a higher human motion recognition rate? Based on UTD-MHAD database, this paper studies the human motion recognition of RGB image and depth image captured simultaneously by kinect, and carries out relevant discussion and analysis on the above problems, using micro-inertial sensors (MTi-G-700 developed by Xsens and Android mobile phones, tablets and other personal mobile devices come with MEMS gyroscopes and accelerometers) to correct the image to motion blur, build a new mathematical model, use the inertial data obtained by MIMU in a short time to estimate the position, attitude and speed of camera motion, correct the image pixel position, perform image de-motion blur processing, and then perform image processing such as denoising to solve the image motion blur problem. A new algorithm is developed and its science is verified by MATLAB simulation.}
}
@article{SHI2019455,
title = {An efficient 3D face recognition approach using Frenet feature of iso-geodesic curves},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {455-460},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300537},
author = {Biao Shi and Huaijuan Zang and Rongsheng Zheng and Shu Zhan},
keywords = {Facial curves, Frenet framework, 3D face recognition, Iso-geodesic, Pose invariant},
abstract = {Extracting efficient features from the large volume of 3D facial data directly is extremely difficult in 3D face recognition (3D-FR) with the latest methods, which mostly require heavy computations and manual processing steps. This paper presents a computationally efficient 3D-FR system based on a novel Frenet frame-based feature that is derived from the 3D facial iso-geodesic curves. In terms of the evaluation of the proposed method, we conducted a number of experiments on the CASIA 3D face database, and a superior recognition performance has been achieved. The performance evaluation suggests that the pose invariance attribute of the features relieves the need of an expensive 3D face registration in the face preprocessing procedure, where we take less time to process conversely. Our experiments further demonstrate that the proposed method not only achieves competitive recognition performance when compared with some existing techniques for 3D-FR, but also is computationally efficient.}
}
@article{AI2019417,
title = {Quantitative CT study of martial arts sports injuries based on image quality},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {417-425},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301075},
author = {Zexiu Ai},
keywords = {Image quality, Wushu, Joint damage, Quantitative CT},
abstract = {Wushu is an outstanding cultural heritage of the Chinese nation and one of the most extensive mass sports in China. As a traditional sports event in China, martial arts are undergoing rapid changes. However, martial arts are a systemic sport with high requirements for speed, explosiveness and coordination. In recent years, with the rapid development of martial arts, competitive competitions have become increasingly fierce. It is easy for athletes to suffer physical damage during the practice of difficult movements. This not only affects the normal exercise and physical health of martial arts enthusiasts, but also affects the improvement of sports level and teaching quality. Therefore, it studies the common parts of martial arts sports injuries. Distribution, looking for its causes, proposing preventive measures, rapid development of image processing technology, digital image has become an indispensable part of multimedia information technology. Digital image is an important carrier for people to obtain information and communicate. Under this background, the research of image quality evaluation has become a hot spot in the field of image processing. The purpose of this paper is to analyze the anatomical characteristics of knee joints of martial arts athletes, the mechanics of injury, the pathophysiological changes after injury, and establish a mathematical model by computer algorithm to accurately perceive the image quality of martial arts sports damage, and finally achieve the use of computer instead of human vision. The system goes to view and recognize images. In this paper, the application value of quantitative CT parameters of martial arts exercise in the evaluation of martial arts injury joints is based on image quality, in order to provide valuable reference for the treatment of martial arts injury selection and prognosis evaluation.}
}
@article{DUAN2019319,
title = {Feature level MRI fusion based on 3D dual tree compactly supported Shearlet transform},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {319-327},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.027},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300781},
author = {Chang Duan and Shuai Wang and Qihong Huang and Tao Wen and Ce Zhu and Yuanyuan Xu},
keywords = {Medical image fusion, Shearlet transform, Structure tensor},
abstract = {A medical fusion method can combine multiple data into one, which is very useful and convenient in the medical diagnosis. We investigated the fusion problem for 3D Magnetic Resonance Imaging (MRI) data and proposed a feature level MRI fusion method based on 3D Dual Tree Compactly Supported Shearlet Transform (CSST) and Structure Tensor. By employing a 3D version of the traditional CSST with dual tree (DT) structure, the 3D DT CSST is shift-invariant and has directional analysis capability for volume data. Utilizing the structural feature extraction capability of Structure Tensor, the proposed fusion rule can preserve the critical structure information of scanned organ in source data. Experimental results using 51 groups of MRI data show the effectiveness of the proposed fusion method.}
}
@article{DING20191,
title = {Depth-aware saliency detection using convolutional neural networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {1-9},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930118X},
author = {Yu Ding and Zhi Liu and Mengke Huang and Ran Shi and Xiangyang Wang},
keywords = {Saliency detection, Convolutional neural networks, Depth saliency network, Saliency fusion network, RGBD images, Stereoscopic images},
abstract = {This paper proposes a new end-to-end depth-aware saliency model using three convolutional neural networks including color saliency network, depth saliency network and saliency fusion network, for saliency detection in RGBD images and stereoscopic images. Firstly, the color image is fed to the color saliency network to generate the color saliency map. Then, by sharing the weights of some layers in the color saliency network, the depth saliency network exploits the weight initialization and multi-layer pyramid feature fusion to learn effective depth features from the three-channel depth image, which is converted from the original depth map, and generates the depth saliency map. Finally, the saliency fusion network integrates the color saliency map with the depth saliency map into the final saliency map, which can highlight salient object regions and suppress background regions more effectively. Experimental results on five public datasets demonstrate that our model achieves the better performance compared with the state-of-the-art depth-aware saliency models.}
}
@article{XU2019363,
title = {An improved multi-branch residual network based on random multiplier and adaptive cosine learning rate method},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {363-370},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.030},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300380},
author = {Yifeng Xu and Huigang Wang and Xing Liu and Weitao Sun's},
keywords = {Image classification, Residual network, Overfitting, Deep leaning, Batch size, Learning rate},
abstract = {Deep residual networks have emerged as a leading technique showing the accuracy and excellent convergence performance. However, because of overfitting and the vanishing gradient problems, we cannot achieve better results by increasing the model parameters. In this study, we propose a novel architecture that has a multi-branch residual network. The shortcut branch of residual networks can ease off the vanishing gradient. The random function and the structure of a multi-branch network improve the fitting ability. The dropout function can weaken overfitting. The proposed method also adopts the adaptive cosine learning rate method and variate batch size to improve the test accuracy. Some experimental investigations are set up to explore the impact of the following factors on the performance: the sequence of the layers, random numbers and different batch sizes, etc. We report that the results achieve 2.63% and 14.2% Top-1 error on CIFAR-10 and CIFAR-100.}
}