@article{FANG2019102609,
title = {Image classification toward breast cancer using deeply-learned quality features},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102609},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102609},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302305},
author = {Yan Fang and Jing Zhao and Lingzhi Hu and Xiaoping Ying and Yanfang Pan and Xiaoping Wang},
keywords = {Image classification, CNN, Quality score},
abstract = {Image classification plays an important role in computer vision and its applications, such as scene categorization, image retrieval. Convolutional neural network based methods have shown competitive performance in image classification, which aims to exploit deep feature of training images. In this paper, based on CNN methods and image quality assessment (IQA) algorithms, we propose a novel method for medical application, that is breast cancer classification. First, we leverage CNN architecture to calculate the number of pixels in the lesions, where maximum pooling layers are used. Then, large density of pixel regions will be assigned with large quality scores, which reflect more texture and grayscale features. Finally, we construct a multi-SVM based image kernel using obtained quality scores to achieve breast cancer classification. Experimental results show our proposed method outperforms single recognition based image classification methods such as pixel grayscale or gradient.}
}
@article{2021103390,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {81},
pages = {103390},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(21)00259-5},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002595}
}
@article{WANG2019129,
title = {Hierarchical deep transfer learning for fine-grained categorization on micro datasets},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {129-139},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301579},
author = {Ronggui Wang and Xuchen Yao and Juan Yang and Lixia Xue and Min Hu},
keywords = {Fine-grained categorization, Convolutional neural network, Transfer learning, Multi-task learning, Model compression},
abstract = {Fine-grained categorization is challenging due to its small inter-class and large intra-class variance. Moreover, requiring domain expertise makes fine-grained labelled data much more expensive to acquire. Existing models predominantly require extra information such as bounding box and part annotation in addition to the image category labels, which involves heavy manual labor. In this paper, we propose a novel hierarchical deep transfer learning model, based on a compression convolutional neural network. Our model transfers the learned image representation from large-scale labelled fine-grained datasets to micro fine-grained datasets, which avoids using expensive annotations and realizes visual categorization task effectively. Firstly, we introduce a cohesion domain to measure correlation degree between source domain and target domain. Secondly, the source-domain convolutional neural network is adjusted according to its metrical feedback, in order to select task-specific features that are suitable for transferring to the target domain. Finally, we make most of perspective-class labels, which are inherent attributes of fine-grained data for multi-task learning and learn all the attributes through joint learning to extract more discriminative representations. The proposed model not only economizes training time effectively and achieves high categorization accuracy, but also verifies that the inter-domain feature transition can accelerate learning and optimization.}
}
@article{ZHICHAO2019102571,
title = {Key pose recognition toward sports scene using deeply-learned model},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102571},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301853},
author = {Cao Zhi-chao and Lingling Zhang},
keywords = {Deep learning, Image processing, Motion model, Image quality},
abstract = {Key pose recognition (KPR) is widely used in sport analysis, which provides effective tools for coaches, athletes and other professionals to conduct game analysis and auxiliary training. KPR from video stream can be divided into individual-oriented and group-oriented. The former method is based on the segmentation and tracking of each target, and the characteristics of the individual are used to study the events in the group. The latter is to process and sample the global image, obtain the overall information, and then process the collected data to classify the abnormal situation and normal situation. In this paper, we propose key pose recognition method based on deep learning. Specifically, we first train an FCN network for foreground extraction with weightlifting video frame images to remove a large amount of background interference in the weightlifting video image. Further, by fine-tuning the CNN network, a network model suitable for the weight-of-weight video classification of the region of interest is obtained. Finally, according to the classification result, the classification result selection strategy is designed to extract the key pose. In addition, our algorithm can select high image quality key pose frame, which is important for sports training. The experimental results show that the proposed method is very competitive.}
}
@article{ANAGUN2019178,
title = {SRLibrary: Comparing different loss functions for super-resolution over various convolutional architectures},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {178-187},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.027},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301336},
author = {Yildiray Anagun and Sahin Isik and Erol Seke},
keywords = {Super-resolution, Convolutional neural networks, Loss functions},
abstract = {This study analyzes the effectiveness of various loss functions on performance improvement for Single Image Super-Resolution (SISR) using Convolutional Neural Network (CNN) models by surrogating the reconstructive map between Low Resolution (LR) and High Resolution (HR) images with convolutional filters. In total, eight loss functions are separately incorporated with Adam optimizer. Through experimental evaluations on different datasets, it is observed that some parametric and non-parametric robust loss functions promise impressive accuracies whereas remaining ones are sensitive to noise that misleads the learning process and consequently resulting in lower quality HR outcomes. Eventually, it turns out that the use of either Difference of Structural Similarity (DSSIM), Charbonnier or L1 loss functions within the optimization mechanism would be a proper choice, by considering their excellent reconstruction results. Among them, Charbonnier and L1 loss functions are fastest ones when the computational time cost is examined during training stage.}
}
@article{KONG2019174,
title = {Lossless compression codec of aurora spectral data using hybrid spatial-spectral decorrelation with outlier recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {174-181},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301610},
author = {Wanqiu Kong and Jiaji Wu and Zejun Hu and Gwanggil Jeon},
keywords = {Lossless compression, Aurora spectral data, Hyperspectral data, Prediction, Range coding, Outlier recognition},
abstract = {Aurora spectral data, the particular hyperspectral data of auroral spectra, have an irreplaceable research value in bridging the gap between solar activity and terrestrial evolution. Their requirement for high-volume storage and real-time transmission had been a great challenge until we proposed a CPU-paralleled and online-biprediction-based method. As it is no longer applicable because of the recent spectrograph reassembling, this paper presents a replacement strategy combines the unidirectional predictor with the entropy coder of the other dimension, as well as with using smoothing and outlier recognition. The hybrid encoders of Spat-SPCC and Spec-SPCC are developed distinguished by their respective prediction direction. Spat-SPCC tuned on one-day trial data is used for its better capability for compression, in the further comparison with various classical algorithms, it achieves the top-ranking compression performance and has the average processing time of 1 s per file so that its availability for the practical applications is validated.}
}
@article{WEN2019279,
title = {Deep fusion based video saliency detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {279-285},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301737},
author = {Hongfa Wen and Xiaofei Zhou and Yaoqi Sun and Jiyong Zhang and Chenggang Yan},
keywords = {Video saliency, Convolutional network, Feature integration, Boundary},
abstract = {This paper introduces a novel video saliency model for salient object detection in videos. Firstly, we generate multi-level deep features via a symmetrical convolutional neural network, in which the inputs are the current frame and the optical flow image. Then, the multi-level deep features are integrated in a hierarchical manner using a fusion network, which deploys attention module to make a selection for deep features. Lastly, the integrated deep feature is combined with the boundary information originated from shallow layer of the feature extraction networks, and the saliency map is generated by the saliency prediction step. The key advantages of our model lie on the attention module, the hierarchical integration and the boundary information, in which the former one acts as weight filter and is used to select the most salient regions in deep features, the middle one gives an effective integration manner for features from different layers and the last one provides well-defined boundaries for saliency map. Extensive experiments are performed on two challenging video dataset, and the experimental results show that our model consistently outperforms the state-of-the-art saliency models in a large margin.}
}
@article{ZHANG2019102606,
title = {Image quality guided biology application for genetic analysis},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102606},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102606},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302275},
author = {Yanjun Zhang and Shuai Gong and Mingjiu Luo},
keywords = {Image quality assessment, Low-level features, Deep learning, BP neural network},
abstract = {Image quality assessment (IQA) is widely applied in image processing, such as image retrieval, image aesthetic evaluation and image classification. To expand application of IQA, we propose a novel method toward biology application using IQA for genetic research. The proposed approach breaks through limitations of traditional biology research, which integrates image processing algorithms with biology applications. Specifically, we first conduct the dataset acquisition including frozen semen images and their according biology scores that reflect genetic attributes. Then, each obtained image will be assigned a quality score according to its grayscale features and texture features. Afterward, we leverage BP neural network for deep feature extraction with fusing quality score and biology score as tags. Finally, given a test image, we can predict its genetic attribute according to deep-learned model. Comprehensive experiments conducted on goat genetic research demonstrate satisfied performance of proposed method.}
}
@article{WANG2019102582,
title = {Reliable identification of redundant kernels for convolutional neural network compression},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102582},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102582},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302032},
author = {Wei Wang and Liqiang Zhu and Baoqing Guo},
keywords = {Network compression, Convolutional neural network, Pruning criterion, Channel-level pruning},
abstract = {To compress deep convolutional neural networks (CNNs) with large memory footprint and long inference time, this paper proposes a novel pruning criterion based on layer-wise Ln-norms of feature maps to identify unimportant convolutional kernels. We calculate the Ln-norm of the feature map outputted by each convolutional kernel to evaluate the importance of the kernel. Furthermore, we use different Ln-norms for different layers, e.g., L1-norm for the first convolutional layer, L2-norm for middle convolutional layers and L∞-norm for the last convolutional layer. With the ability of accurately identifying unimportant convolutional kernels in each layer, the proposed method achieves a good balance between model size and inference accuracy. Experimental results on CIFAR, SVHN and ImageNet datasets and an application example in a railwayintelligent surveillance system show that the proposed method outperforms existing kernel-norm-based methods and is generally applicable to any deep neural network with convolutional operations.}
}
@article{ZHANG2019166,
title = {An object counting network based on hierarchical context and feature fusion},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {166-173},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301580},
author = {Shihui Zhang and He Li and Weihang Kong and Lei Wang and Xiaofang Niu},
keywords = {Object counting, Density estimation, Feature fusion, Hierarchical context},
abstract = {Object counting is a challenging task in computer vision. In this paper, we propose an object counting network based on hierarchical context and feature fusion called HFNet. HFNet comprises a hierarchical context extraction module and an end-to-end convolution neural network. The hierarchical context extraction module extracts hierarchical features to the main network as context cues, aiming to provide more information to improve counting performance. The main network adds the relatively lower but naturally high-resolution feature maps into higher but semantic feature maps, whose benefits are: one is to reduce the risk of losing detailed information during multi-convolutions; the other is to against the scale variations in this task due to the fusion operation of the multi-scale feature maps. Experiments demonstrate HFNet achieves competitive results on crowd counting including UCF_CC_50 dataset and ShanghaiTech dataset and on vehicle counting including TRANCOS dataset. The contrast experiments also verify the structure rationality of HFNet.}
}
@article{KO2019102594,
title = {Quality-guided image classification toward information management applications},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102594},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102594},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302159},
author = {Kuo-Min Ko and Po-Chang Ko and Shih-Yang Lin and Zhen Hong},
keywords = {Deep learning, Convolutional neural network, Image retrieval, Image quality assessment},
abstract = {Image information management (IIM) is a key technique to improve the performance of large-scale image retrieval. However, IIM is still a big challenge due to the large sum of image datasets and traditional algorithms cannot cope with this problem. In order to solve these disadvantages, we propose a novel image classification algorithm based on image quality assessment (IQA) for image information management. Specifically, we first incorporate both low-level, high-level features as well as quality scores for image representation, where we leverage convolution neural network for deep feature extraction. Then, deep feature vector can be generated by column-wise stacking. Thus, each image can be represented by a feature vector. We leverage GMM to learn the distribution of obtained feature vectors. Similar image categories have similar probability distributions, we leverage the learned GMM model to calculate the posterior probability and image can be classified into corresponding category. Experimental results demonstrate the performance of our proposed method, and image information management is easier to implement.}
}
@article{SU2019102618,
title = {A square lattice oriented reversible information hiding scheme with reversibility and adaptivity for dual images},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102618},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102618},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302391},
author = {Guo-Dong Su and Yanjun Liu and Chin-Chen Chang},
keywords = {Dual-image, Reversible information hiding, Embedding rate, Histogram shifting, Real-time},
abstract = {This paper proposes an adaptive dual-image-based reversible information hiding scheme. First, for a pixel pair in the original image, an associated square lattice is selected to determine the maximum distortion that will be induced by hiding information. Then, an embedding rule, which combines the selected square lattice with integer rounding, is constructed. In addition, a novel histogram shifting mechanism to solve the problems of overflow and underflow is designed to decrease the distortions that are caused by the shifting of a large number of pixels. In our approach, both the original image and the secret messages can be recovered losslessly. Our experimental results show that the proposed scheme achieved performance that was superior to the state-of-the-art schemes, resulting in embedding rates up to 1.40 bpp and average PSNRs of approximately 47.60 dB. The proposed scheme also can be used in real-time systems due to the simplicity of its computations.}
}
@article{JIN2019193,
title = {Learning deep CNNs for impulse noise removal in images},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {193-205},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301609},
author = {Lianghai Jin and Wenhua Zhang and Guangzhi Ma and Enmin Song},
keywords = {Image, Impulse noise, Convolution neural network, Denoising},
abstract = {Deep learning has been widely applied in image processing and computer vision due to its powerful learning capability. Although some learning models have been proposed to suppress noise in images, most of them are developed for Gaussian noise and few are for impulse noise. This paper proposes an image recovery method based on deep convolutional neural networks for impulse noise removal. The proposed framework falls into two components: a classifier network which divides image pixels into noisy and noise-free, and a regression network which is trained for image reconstruction. In the regression network, the noise-free pixels identified by the classifier network together with the original noisy image are used for recovery of the noisy image. Furthermore, batch normalization is embedded to the network to improve denoising performance. Experimental results show that the proposed method can excellently remove impulse noise, providing clear performance improvements over other state-of-the-art denoising methods.}
}
@article{HAN2019381,
title = {Deformed landmark fitting for sequential faces},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {381-393},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930183X},
author = {Shoudong Han and Ziqing Yang and Qianqian Li and Yang Chen},
keywords = {Landmark fitting, Active shape model, Large displacement optical flow model, Global shape model},
abstract = {Fitting facial landmarks on unconstrained videos is a challenging task with broad applications. At present, many methods of one-shot landmark fitting have been proposed with varying degrees of success. However, most of them are heavily sensitive to initializations and usually rely on offline-trained static models, which limit their performance on sequential images with extensive variations. Therefore, they usually can’t align the deformed face very well. To address these limitations, we propose a method of deformed landmark fitting (DLF) for sequential faces, which is designed based on active shape model (ASM) and deformation tracking/correction. This method overcomes the loss of consecutive information between frames, and makes full use of the motion variation information of video sequences in time and space dimensions. Firstly, the optical flow values of several possible deformation points on the face are calculated by the large displacement optical flow (LDOF) model, and the tracking of these points in the current frame are performed through the optical flow motion vector. Secondly, the initial shape of face in each frame is established by the locations of these deformation points and the global shape model in ASM algorithm. Finally, on the basis of initial shape, according to the guidance of local texture model in ASM algorithm, different correction strategies are applied to different landmarks for local search, and then each landmark is reasonably suppressed to obtain the ultimate results. Our DLF observably improves the fitting accuracy for deformed faces, and takes full advantage of the continuity among video sequences. Compared with some state-of-the-art landmarkers, extensive experiments on landmark fitting for sequential faces show that our DLF performs outstandingly in terms of accuracy and robustness.}
}
@article{AFIFI201977,
title = {AFIF4: Deep gender classification based on AdaBoost-based fusion of isolated facial features and foggy faces},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {77-86},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301567},
author = {Mahmoud Afifi and Abdelrahman Abdelhamed},
keywords = {Gender classification, Deep convolutional neural networks, Face image dataset},
abstract = {Gender classification aims at recognizing a person’s gender. Despite the high accuracy achieved by state-of-the-art methods for this task, there is still room for improvement in generalized and unrestricted datasets. In this paper, we advocate a new strategy inspired by the behavior of humans in gender recognition. Instead of dealing with the face image as a sole feature, we rely on the combination of isolated facial components and a contextual feature which we call the foggy face. Then, we use these features to train deep convolutional neural networks followed by an AdaBoost-based score fusion to infer the final gender class. We evaluate our method on four challenging datasets to demonstrate its efficacy in achieving better or on-par accuracy with state-of-the-art methods. In addition, we present a new face dataset that intensifies the challenges of occluded faces and illumination changes, which we believe to be a much-needed resource for gender classification research.}
}
@article{YAN2019102603,
title = {An adaptive template matching-based single object tracking algorithm with parallel acceleration},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102603},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102603},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930224X},
author = {Baicheng Yan and Limin Xiao and Hang Zhang and Daliang Xu and Li Ruan and Zhaokai Wang and Yiyang Zhang},
keywords = {Visual object tracking, Adaptive template update, Parallel acceleration, Deep learning, Embedded platform},
abstract = {Existing template matching based visual object tracking algorithms usually require to manually update the template and have high execution cost on general embedded systems. To address these issues, an adaptive template matching-based single object tracking algorithm with parallel acceleration is proposed in this paper. In this algorithm, we propose an adaptive single object tracking algorithm framework to achieve template update online. Based on the Faster-RCNN model, we design a single object capture method to update the template. Meanwhile, we present a parallel strategy to accelerate the process of template matching. To evaluate the proposed algorithm, we use OTB benchmark to compare the performance with several state-of-the-art trackers on TX2 embedded platform. Experimental results show that the proposed method achieves a 5.9 times execution speed and 71.9% accuracy improvement over the comparison methods.}
}
@article{PENG2019102569,
title = {Quality assessment of stereoscopic video in free viewpoint video system},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102569},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301841},
author = {Zongju Peng and Shipei Wang and Fen Chen and Wenhui Zou and Gangyi Jiang and Mei Yu},
keywords = {Free viewpoint video system, Human visual characteristics, Video quality assessment, View synthesis},
abstract = {It is challenging to develop an effective quality assessment method for the stereoscopic video (SV) in the free viewpoint video (FVV) system because of asymmetric distortion and the interactively changing combinations. This paper proposes a quality assessment method that takes the characteristics of the SV in FVV systems into consideration. Specifically, considering the distortion introduced by the rendering process, the proposed method extracts a critical distortion area that can be perceivable by the human eye. The distortion degree of each critical distortion area is quantified and the overall results are pooled to obtain the initial quality score. The video pairs are classified into different combination types and the initial quality score is refined to obtain the final quality score. The experimental results show that the proposed method outperforms conventional objective methods.}
}
@article{ZIGHEM2019236,
title = {Two-stages based facial demographic attributes combination for age estimation},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {236-249},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.025},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301324},
author = {Mohammed-En-nadhir Zighem and Abdelkrim Ouafi and Athmane Zitouni and Yassine Ruichek and Abdelmalik Taleb-Ahmed},
keywords = {Age estimation, Demographic classification, Feature extraction, SVR, SVM},
abstract = {Automatic age estimation from face images is a topic of growing interest nowadays, because of its great value in various applications. The main challenge in automatic facial age estimation task comes from the large intra-class facial appearance variations due to both gender and race attributes. To this end, in this paper we propose a complete approach for age estimation based on demographic classification. The proposed approach consists of three main parts: (1) Automatic face detection and alignment to extract only the regions of interest. (2) Feature extraction from facial region images using Multi-level face representation. (3) Two-Stages age Estimation (TSE). The main idea of TSE is to classify the input face image into one of demographic classes, then estimate age within the identified demographic class. The experimental results demonstrate that our proposed approach can offer better performance for age estimation when compared to the state-of-the-art methods on MORPH-II, PAL and a subset of LFW databases.}
}
@article{WEI2019102581,
title = {Image quality assessment for intelligent emergency application based on deep neural network},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102581},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102581},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301968},
author = {Guanghui Wei and Zhou Sheng},
keywords = {Entropy theory, Big data, Wisdom emergency, Quality model, Neural network},
abstract = {Video surveillance is widely applied in modern intelligent systems, such as access control, pedestrian re-identification. However, with the rapid development of urbanization, urban traffic situation becomes more and more complex. It is a big challenge for video surveillance to cope with such massive data. In addition, existing emergency systems are far from modern requirement. So in this paper, we propose an image quality based framework to improve the performance of video surveillance and design a new urban intelligent emergency system. Specifically, we first analyze emergency evacuation of social group security incident for data acquisition including surveillance video and labels. Then, incorporating image quality assessment and convolution neural network, the dataset can be classified into several parts, and each part demonstrates a particular situation. Afterward, we introduce entropy theory to study the application of urban intelligence emergency. The results show that the research method proposed in this paper effectively obtains the evacuation parameters of evacuation personnel in the evacuation of sudden social group events, and improves the timeliness of information transmission in the evacuation process. The results show that the research method of this paper significantly improves the pertinence, effectiveness and perfection of the emergency plan in the application of urban emergency system.}
}
@article{SHANG2019259,
title = {Semantic consistency cross-modal dictionary learning with rank constraint},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {259-266},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301725},
author = {Fei Shang and Huaxiang Zhang and Jiande Sun and Li Liu},
keywords = {Cross-modal retrieval, Dictionary learning, Rank constraint},
abstract = {Cross-modal retrieval develops rapidly due to the growth and widespread applications of multimodal data. How to reduce the heterogeneous gap and impose effective constraints on different modalities are two basic problems. In this paper, we propose a novel Semantic Consistency cross-modal Dictionary learning algorithm with rank Constraint (SCDC) to solve these aforementioned problems. An orthogonal space learned by spectral regression is introduced, in which different modalities can be measured directly. Specifically, images and texts are encoded by their dictionaries to obtain corresponding reconstruction coefficients. A l21-norm term is imposed on these coefficients in order to select discriminative features and avoid over-fitting simultaneously. In the meantime, a rank constraint is imposed on the transformed features so as to improve the correlation of different modalities. Experimental results on three popular datasets demonstrate that SCDC is significantly superior to several state-of-the-art methods.}
}
@article{LI2019102605,
title = {Exclusive feature selection and multi-view learning for Alzheimer’s Disease},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102605},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102605},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302263},
author = {Jiaye Li and Lin Wu and Guoqiu Wen and Zhi Li},
keywords = {Alzheimer’s Disease, Multi-view, Exclusive lasso learning, Feature selection, Sparse learning},
abstract = {In Alzheimer’s Disease (AD) studies, high dimension and small sample size have been always an issue and it is common to apply a dimension reduction method to predict the early diagnosis of AD. In this paper, we propose a multi-view feature selection algorithm embedded with exclusive lasso learning and sparse learning. It extracts the feature subsets that best represent the symptoms of patients through feature selection, so as to reduce the dimension and achieve a better diagnosis rate. Firstly, in order to overcome the limitation of non-overlapping, the features under different view are clustered by fuzzy C-means clustering. Then, the exclusive group lasso learning is performed according to the clustering results and each view is sparsely learned through the l2,1-norm, resulting in better removal of redundant features. Finally, the results of each view are combined to obtain the final features subsets. This exclusive lasso learning combined through multiple views is novel in clinical practice and can effectively target AD. At the same time, the experimental results show that our method could achieve better results compared to its competing methods.}
}
@article{ANDALVIRREY2019209,
title = {Visual data of facial expressions for automatic pain detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {209-217},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.023},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301294},
author = {Reneiro {Andal Virrey} and Chandratilak {De Silva Liyanage} and Mohammad {Iskandar bin Pg Hj Petra} and Pg {Emeroylariffion Abas}},
keywords = {Facial expression recognition, Emotion database, Human pain detection, Feature learning},
abstract = {Facial expressions are complex, progress over time, and are challenging to interpret. The research subject of automated emotion recognition associated with the facial expressions has been a mainstream topic in computer vision focused on image processing and pattern recognition. Numerous databases of facial expressions are available to the research community, and are used as fundamental tools for the evaluation of a wide range of algorithms for face expression recognition. In this paper, we assessed the existing collections of facial expression datasets as a basis for building and evaluating the largely unmapped facet of pain expressions. To accentuate this topic, the study provides the summary of different characteristics of expressions that are relevant and justifiable indicators of pain. A preliminary platform is tested with accuracy rate of 85.66% using the collected FER datasets as testing inputs. Common challenges in face recognition were discussed, as well as the different methods used to address them. Different variants of feature learning techniques were also compared, and why some methods outperforms other existing methods.}
}
@article{FONG2019102593,
title = {Image quality assessment for advertising applications based on neural network},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102593},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102593},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302147},
author = {Cher-Min Fong and Hui-Wen Wang and Chien-Hung Kuo and Pei-Chun Hsieh},
keywords = {Deep learning, Image quality assessment, Image classification},
abstract = {Image quality assessment (IQA) is a key technique in computer vision, which is widely applied in image classification, image aesthetic prediction. IQA plays an important role in advertising assessment system, which can recommend higher quality advertising for users. However, traditional algorithms cannot effectively predict advertising quality. In this paper, we propose an advertising assessment system using IQA algorithm based on neural networks. Specifically, we first incorporate both low-level features and high-level sematic features for image representation, where manifold learning algorithm is leveraged for high-level feature learning. Then, we leverage CNN based method for deep representation learning, which will be concatenated into deep feature vector. Finally, we leverage HMM model for learning image quality of advertising based on learned feature vector. Comprehensive experiments show the effectiveness of our proposed method.}
}
@article{GAO2019206,
title = {Dim and small target detection based on feature mapping neural networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {206-216},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301695},
author = {Zhisheng Gao and Jiao Dai and Chunzhi Xie},
keywords = {Dim and small target detection, Feature mapping, Deep neural network, Constant false alarm rate, Background suppression},
abstract = {Dim and small target detection based on passive millimeter wave or infrared imaging is of great value in both security and military fields and has been studied extensively. The problems of weak distinction between small targets and backgrounds and of less extractable features of targets have always been a technical bottleneck for accurate detection of dim and small targets. For dim and small targets with few pixel-based features on complex and diverse backgrounds, we propose a high-precision detection algorithm based on feature mapping deep neural networks with a spindle network structure. Firstly, the features of low-dimension dim and small target blocks are mapped to a higher-dimensional space. An encoded neural network is then used to extract high-discriminant features to complete the background and target recognition. Background suppression and target enhancement is realized according to the intensity (the distinguished output of the network). Finally, a detection method based on the constant false alarm rate is used to detect dim and small targets. The experimental results show that, compared with several popular algorithms for millimeter-wave and infrared image detection in different scenarios, the proposed algorithm has a lower false alarm rate, higher detection accuracy and stronger robustness. Statistics for experiments on under various false alarm rates and signal-to-noise ratios show that the detection rate of the proposed method is about 15% higher than that of the compared algorithms. In experiments on real data, the detection rate of our algorithm is more than 25% higher than that of the suboptimal algorithm.}
}
@article{SILVA2019105,
title = {Radial feature descriptors for cell classification and recommendation},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {105-116},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301452},
author = {Romuere R.V. Silva and Flavio H.D. Araujo and Daniela M. Ushizima and Andrea G.C. Bianchi and Claudia M. Carneiro and Fatima N.S. Medeiros},
keywords = {Radial feature descriptors, Cell classification, Image retrieval, Convolutional neural networks},
abstract = {This paper introduces computational tools for cell classification into normal and abnormal, as well as content-based-image-retrieval (CBIR) for cell recommendation. It also proposes the radial feature descriptors (RFD), which define evenly interspaced segments around the nucleus, and proportional to the convexity of the nuclear boundary. Experiments consider Herlev and CRIC image databases as input to classification via Random Forest and bootstrap; we compare 14 different feature sets by means of False Negative Rate (FNR) and Kappa (κ), obtaining FNR =0.02 and κ=0.89 for Herlev, and FNR =0.14 and κ=0.78 for CRIC. Next, we sort and rank cell images using convolutional neural networks and evaluate performance with the Mean Average Precision (MAP), achieving MAP =0.84 and MAP =0.82 for Herlev and CRIC, respectively. Cell classification show encouraging results regarding RFD, including its sensitivity to intensity variation around the nuclear membrane as it bypasses cytoplasm segmentation.}
}
@article{ZHU2019368,
title = {Co-weighting semantic convolutional features for object retrieval},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {368-380},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301798},
author = {Jihua Zhu and Jiaxing Wang and Shanmin Pang and Weili Guan and Zhongyu Li and Yaochen Li and Xueming Qian},
keywords = {Object retrieval, Deep convolutional features, Aggregation},
abstract = {Deep feature aggregation, which refers to aggregating a set of local convolutional features into a global image-level vector, has attracted increasing attention in object instance retrieval. In this manuscript, we propose an unsupervised framework that aggregates feature maps by an adaptive selection and two weighting strategies. Particularly, the selection process finds the foreground contour by explaining the semantic structure implicated in the feature maps, while two weighting process including an adaptive Gaussian filter that highlights semantic features and an element-value sensitive channel vector that activates feature channels corresponding to sparse yet distinctive image patterns. Experimental results on benchmark image retrieval datasets validate that the selection and two weighting schemes are complementary in improving the discriminative power of image vectors. With the same experimental settings, the proposed approach outperforms state-of-the-art aggregation approaches by a considerable margin.}
}
@article{FAN2019140,
title = {Picture-level just noticeable difference for symmetrically and asymmetrically compressed stereoscopic images: Subjective quality assessment study and datasets},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {140-151},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301555},
author = {Chunling Fan and Yun Zhang and Huan Zhang and Raouf Hamzaoui and Qingshan Jiang},
keywords = {Picture-level JND, Subjective quality assessment test, Stereoscopic image},
abstract = {The Picture-level Just Noticeable Difference (PJND) for a given image and compression scheme reflects the smallest distortion level that can be perceived by an observer with respect to a reference image. Previous work has focused on the PJND of images and videos. In this paper, we study the PJND of symmetrically and asymmetrically compressed stereoscopic images for JPEG2000 and H.265 intra coding. We conduct interactive subjective quality assessment tests to determine the PJND point using both a pristine image and a distorted image as a reference. We find that the PJND points are highly dependent on the image content. In asymmetric compression, there exists a perceptual threshold in the quality difference between the left and right views due to the binocular masking effect. We generate two PJND-based stereo image datasets (one for symmetric compression and one for asymmetric compression) and make them accessible to the public.}
}
@article{CHEN2019102580,
title = {Quality assessment on remote sensing image based on neural networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102580},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102580},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301956},
author = {Guobin Chen and Maotong Zhai},
keywords = {Image quality assessment, Remote sensing image, Deep learning, Information entropy},
abstract = {Image quality assessment is of great significance for the designment and application of remote sensing systems. CNN based method is proposed for image quality assessment on remote sensing image in this paper. Specifically, we first introduce the convolutional neural network and deep learning method. Then a deep CNN architecture is constructed to automatically extract image features to evaluate image quality. Afterward, the information entropy threshold is used to remove the image blocks with less information content. Finally, a deep network model with two convolutional layers is used to achieve feature extraction and image quality scoring. The experimental results show that the quality score of this method has good subjective and objective consistency for multi-distortion remote sensing images and common multi-distortion images. Evaluation of distorted images does not depend on a specific database and has database independence. In addition, our proposed method is simple to achievement.}
}
@article{ZHANG201912,
title = {VST-Net: Variance-stabilizing transformation inspired network for Poisson denoising},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {12-22},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301439},
author = {Minghui Zhang and Fengqin Zhang and Qiegen Liu and Shanshan Wang},
keywords = {Poisson noise reduction, Deep learning, Variance-stabilizing transformation, Joint learning, Two-stage learning},
abstract = {Poisson noise occurs in various applications including medical imaging and night vision. There exist many traditional Poisson denoising algorithms, particularly, one class of impressive algorithm is based on variance-stabilizing transformation (VST). Inspired by the traditional VST scheme, in this paper we propose a novel Poisson denoising model based on convolutional neural network, called variance-stabilizing transform network (VST-Net). VST-Net inherits the structures and strengths of the traditional VST scheme via optimizing the nonlinear transformation by means of network design and supervised learning. The whole VST-Net network contains three sub-networks. The first and third sub-networks simulate the forward and inverse Anscombe transforms, respectively. Meanwhile, the second sub-network is devoted to playing the role of approximate Gaussian denoising. Joint learning strategy and two-stage progressive learning strategy are exploited to investigate the rationality and strength of the VST scheme. Experimental results verify the great potential of the VST-driven network. Code is available at: https://github.com/yqx7150/VST-Net.}
}
@article{SUN2019253,
title = {Image classification base on PCA of multi-view deep representation},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {253-258},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301713},
author = {Yaoqi Sun and Liang Li and Liang Zheng and Ji Hu and Wenchao Li and Yatong Jiang and Chenggang Yan},
keywords = {Image classification, Principal component analysis, Multi-view depth characters},
abstract = {In the age of information explosion, image classification is the key technology of dealing with and organizing a large number of image data. Currently, the classical image classification algorithms are mostly based on RGB images or grayscale images, and fail to make good use of the depth information about objects or scenes. The depth information in the images has a strong complementary effect, which can enhance the classification accuracy significantly. In this paper, we propose an image classification technology using principal component analysis based on multi-view depth characters. In detail, firstly, the depth image of the original image is estimated; secondly, depth characters are extracted from the RGB views and the depth view separately, and then the reducing dimension operation through the PCA is implemented. Eventually, the SVM is applied to image classification. The experimental results show that the method has good performance.}
}
@article{SHISHIDO201968,
title = {Smooth switching method for asynchronous multiple viewpoint videos using frame interpolation},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {68-76},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301440},
author = {Hidehiko Shishido and Aoi Harazaki and Yoshinari Kameda and Itaru Kitahara},
keywords = {Free-viewpoint video, Bullet-time, Frame interpolation, Morphing, Asynchronous multi-view videos},
abstract = {This research proposes a method that generates viewpoint smooth switching by reducing the flickering artefacts that are observed at bullet-times generated from asynchronous multi-view videos using frame interpolation processing. When we asynchronously capture multi-view videos of an object moving at a high velocity, deviations occur in the observed position at the bullet-times. We apply a frame interpolation technique to smooth this problem. By selecting suitable interpolated images that produce the smallest movement of the subject’s observed position, we smoothly generate a viewpoint-switched bullet-time video. In this paper, we examine the subjective evaluation of the video generated by the proposed method. And we also examine objective evaluation. Therefore, the effectiveness of the proposed method is shown. Furthermore, reproducibility was improved by considering the application conditions of the proposed method.}
}
@article{CHEN2019102604,
title = {Robust estimation for image noise based on eigenvalue distributions of large sample covariance matrices},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102604},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102604},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302251},
author = {Rui Chen and Fei Zhao and Changshui Yang and Yuan Li and Tiejun Huang},
keywords = {Noise level estimation, Eigenvalue distributions, Large sample covariance matrix, Random matrix theory},
abstract = {In this paper, we propose a novel algorithm to estimate Gaussian noise levels for captured natural images by rigorously analyzing the limiting distributions of the eigenvalue spectrum of a large covariance matrix with Gaussian samples. In order to select a relatively homogeneous region that best represents the noise, the corresponding image patches are first rearranged to construct a high-dimensional noise covariance matrix. And then, an optimal criterion for classifying homogeneous regions is derived based on the statistical relationship between the largest and the second largest eigenvalues of a sample covariance matrix. Moreover, we further explore the reasons for the bias of the maximum likelihood estimator of the noise variance both in high-dimensional settings and finite samples. According to random matrix theory, we clarify the asymptotic properties of the trace of a sample covariance matrix to measure the error bounds of estimation and then propose a new bias-corrected estimator. To this end, an effective estimation method for the noise level is devised based on the boundness and asymptotic behavior of pure noise eigenvalues of the selected patches. The estimation performance of our method has been guaranteed both theoretically and empirically. Experimental results have demonstrated that our approach can reliably infer true noise variance and is superior to the competing methods in terms of both estimation accuracy and robustness.}
}
@article{VIZCARRAMELGAR2019102592,
title = {A (2,2) XOR-based visual cryptography scheme without pixel expansion},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102592},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102592},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302135},
author = {Max E. {Vizcarra Melgar} and Mylène C.Q. Farias},
keywords = {Visual cryptography, Visual secret sharing, Color interference},
abstract = {Visual cryptography (VC) is a technique that encodes the content of a secret image into two or more images, which are called shares. These shares are printed on transparencies and superimposed to reveal the original secret image. Frequently, VC techniques require a pixel expansion and a good alignment, which reduces the final spatial resolution. In this paper, we propose a physical VC scheme that only requires two shares and do not demand a pixel expansion. The first share is a colored transparency printed on a Polyvinyl Chloride (PVC) surface of 3 mm, while the second share is a colored image displayed on a smartphone screen. The decoded pixels of the proposed scheme have defined colors and a good resolution. We perform a physical evaluation of the color interference properties of the two shares, to find the most adequate color space, and test the proposed method with practical examples.}
}
@article{DIMAURO2019234,
title = {Estimating the occupancy status of parking areas by counting cars and non-empty stalls},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {234-244},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301683},
author = {D. {Di Mauro} and A. Furnari and G. Patanè and S. Battiato and G.M. Farinella},
keywords = {Scene understanding, Deep learning, Smart cities, Counting, Vehicle detection, Semantic segmentation},
abstract = {This work presents and compares different vision-based approaches to estimate the occupancy status of parking areas by counting cars and non-empty parking stalls. Our investigation considers both the scenario in which parking stalls are marked on the ground and the more challenging one in which no assumption on the presence or position of stalls is assumed. We carry out an experimental analysis on a real-world dataset of videos collected in different parking areas. Specifically, this work compares solutions based on image classification, vehicle detection and semantic segmentation. Our analysis highlights that: (1) methods based on image classification can be effectively leveraged when the position of parking stalls is known in advance, (2) methods based on image segmentation should be preferred over methods based on object detection when the geometry of the scene is not known, (3) temporal smoothing can be effectively used to improve predictions over time.}
}
@article{HUANG2019102588,
title = {Research on image quality in decision management system and information system framework},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102588},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102588},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302093},
author = {Jui-Chan Huang and Hao-Chen Huang and Su-Hui Chu},
keywords = {Image quality, Quality model, Decision management, Information system, Decision support system},
abstract = {With the continuous development of society and the rapid advancement of science and technology, all walks of life are increasingly required to effectively manage their projects or affairs. Decision-making is the basis of project management, and the formation of decisions determines the formulation of corresponding programs, organizations and businesses. Decision-making is generated by decision makers based on the multi-faceted information they have, so decision management systems and information systems are inseparable in the management of projects and transactions. At present, research on decision management systems emphasizes management and decisions, and few researchers combine decision management and related information. In view of the current research deficiencies, and taking into account the needs of various industries for image applications, this paper introduces image quality evaluation methods, and systematically studies the decision management system and information system framework. The experiment part takes the intelligent image recognition and prevention system of citrus pests and diseases as an example, and the role of image quality in decision management system and information system from the perspective of practice. The results show that through the image quality evaluation system, in the identification and prevention of citrus pests and diseases, it can combine various aspects of information to make effective prevention and control decisions. The results show that the introduction of image quality evaluation can improve the comprehensiveness of information system. On this basis, decision-making system can help decision makers make correct decisions quickly and effectively.}
}
@article{NAN2019359,
title = {Research on fuzzy enhancement algorithms for infrared image recognition quality of power internet of things equipment based on membership function},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {359-367},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301828},
author = {Liu Dun Nan and Hou Rui and Li Qiang and Zhao Ning Ning and Ge Rui and Lu Yi},
keywords = {Power internet of things, Image recognition quality, Membership function, Image enhancement, Quality model},
abstract = {With the development of society, the demand for power is increasing, but the inefficiency of the original power grid has gradually become an obstacle to social development, so it is urgent to reform the power grid, and the power Internet of things is an important means of power reform. Effective infrared image recognition of power Internet of Things equipment can achieve real-time monitoring of equipment status. The quality of image recognition is not only related to recognition methods, but also to the quality of the image itself. In order to improve the quality of image recognition, an image enhancement algorithm based on membership function is proposed in this paper. Firstly, piecewise linear transformation is introduced, and on the basis of it, an improved piecewise linear transformation method is proposed. Secondly, an image enhancement algorithm based on membership function is proposed by improving the traditional fuzzy enhancement method. Finally, based on the image enhancement algorithm of membership function, an improved piecewise linear transformation method is introduced to form an image blur enhancement system. Through the simulation experiment, we can find that the two image enhancement algorithms are improved in this paper, which can enhance the image enhancement effect of the original two enhancement algorithms, and the image fuzzy enhancement system composed of two improved enhancement methods can enhance the image enhancement effect again, and effectively improve the infrared image recognition quality of power Internet of things equipment.}
}
@article{ZHAO2019102589,
title = {Learning a virtual codec based on deep convolutional neural network to compress image},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102589},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102589},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930210X},
author = {Lijun Zhao and Huihui Bai and Anhong Wang and Yao Zhao},
keywords = {Image representation, Image compression, Soft-projection, Virtual codec, Post-processing},
abstract = {In this paper, we propose a standard-compliant image compression framework based on image representation network (IRN) and post-processing neural network (PNN), which are trained by learning a virtual codec network (VCN). Firstly, we use a mixed-resolution image coding considering different types of distortions caused by image compression with different quality factors. Secondly, the VCN is introduced to learn a differentiable soft-projection from the represented image to the post-processed image to resolve the non-differentiable problem of hard quantization. Thirdly, the PNN is used to greatly enhance the quality of decoded images, since standard codecs always result in visually unpleasant blocking artifacts and ringing artifacts. Finally, our framework is trained in an end-to-end manner, whose convolutional kernels of the IRN, PNN and VCN are initialized by pre-training an auto-encoder network. Experimental results verify that our method has higher coding efficiency than the newest image representation-based compression method and many post-processing approaches.}
}
@article{XU2019182,
title = {Robust correlation filter tracking via context fusion and subspace constraint},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {182-192},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301701},
author = {Jiahong Xu and Cheng Cai and Jifeng Ning and Yunsong Li},
keywords = {Object tracking, Correlation filter, Context fusion, Subspace constraint, Model update},
abstract = {In the trackers based on correlation filter, the limitation of tracking samples and the efficiency of the solution affect the performance of the trackers. Based on the Discriminative Scale Space Tracking(DSST), we proposed a new framework to improve correlation filter tracking via context fusion and subspace constraint. First, the target and background are jointly modeled to enhance the ability to distinguish between target and background. Second, to achieve higher robustness, subspace constraint is applied to DSST, and it can be solved efficiently by subspace-based alternating direction method of multipliers(SADMM). Finally, through the model update strategy, high-quality samples are selected to update. The proposed algorithm is validated on the OTB2013, OTB2015 and TC128. Experimental results show that compared with fast DSST(fDSST), success rate and accuracy of the proposed method are increased by 4.2% and 6.8% respectively. Compared with the state-of-the-art trackers, the proposed method achieves a competitive result with real-time speed.}
}
@article{JI2019102602,
title = {Part-based visual tracking via structural support correlation filter},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102602},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102602},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302238},
author = {Zhangjian Ji and Kai Feng and Yuhua Qian},
keywords = {Object tracking, Support vector machines, Correlation filter, Structural learning, Temporal consistency, Scale estimation},
abstract = {To better deal with the partial occlusion issue and improve their efficiency of part-based and support vector machines (SVM) based trackers, we propose a novel part-based structural support correlation filter tracking method, which absorbs the strong discriminative ability from SVM and the excellent property of part-based tracking methods which is less sensitive to partial occlusion. Then, our proposed model can learn the support correlation filter of each part jointly by a star structure model, which preserves the spatial layout structure among parts and tolerates outliers of parts. In addition, our model introduces inter-frame consistencies of local parts to mitigate the drift problem. Finally, our model can accurately estimate the scale changes of object by the relative distance change among reliable parts. The extensive empirical evaluations on three benchmark datasets: OTB2015, TempleColor128 and VOT2015 demonstrate that the proposed method achieves comparable performance against several state-of-the-art trackers and runs in real time.}
}
@article{SRIVASTAVA2019330,
title = {Salient object detection using background subtraction, Gabor filters, objectness and minimum directional backgroundness},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {330-339},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301786},
author = {Gargi Srivastava and Rajeev Srivastava},
keywords = {Background subtraction, Gabor filters, Minimum directional backgroundness},
abstract = {Salient object detection is the process of identifying essential objects in an image. This paper solves this problem using background subtraction, Gabor filters, minimum directional backgroundness, and objectness. The first step is to calculate a backgroundness score for each region by calculating the difference between the feature vector of image boundary and image regions. This backgroundness map is then used for calculating the minimum directional background difference. The image is segmented using Gabor filters, and then the objectness criterion is used to choose the segment containing the salient object. The normalized foreground saliency map is then used to refine the selected segment. Further enhancement of this intermediate output is done using morphological operations, and boundary correction is done using the method of lazy snapping. The algorithm is tested on eight publicly available datasets and is compared against five algorithms. The performance is evaluated by PR-curve, F-Measure curve, and Mean Absolute Error.}
}
@article{YU2019102579,
title = {Pedestrian detection using multi-channel visual feature fusion by learning deep quality model},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102579},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102579},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301944},
author = {Peijia Yu and Yong Zhao and Jing Zhang and Xiaoyao Xie},
keywords = {Convolutional neural networks, Pedestrian detection, VGG-16 net, RA block, Faster R-CNN, Quality model},
abstract = {Object detection has been widely applied in modern intelligent systems, especially using convolutional neural networks (CNNs). Pedestrian detection is a key technique in video surveillance, which could automatically locate special pedestrian. However, conventional CNN based methods such as Fast/Faster R-CNN cannot handle pedestrian detection effectively due to the extremely similar of positives and hard negatives. In this paper, in order to solve hard negative problem in pedestrian detection, we incorporate classifier enhancement and representational ability of CNNs. More specifically, we first fuse multi-channel visual features (color, texture, semantic) for quality assessment. Then, we propose “Reduction-adjustment” (RA) block which can enhance feature extraction and can be flexibly embedded into CNNs. In our implementation, we embed RA blocks into a base model such as VGG 16. Afterwards, we apply Faster R-CNN as a detection system to classify and locate pedestrians. Extensive experiments on Caltech, ETH and CityPersons datasets demonstrate that our deep model is feasible and effective for pedestrian detection.}
}
@article{YE20191,
title = {Facial expression recognition via region-based convolutional fusion network},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {1-11},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301427},
author = {Yingsheng Ye and Xingming Zhang and Yubei Lin and Haoxiang Wang},
keywords = {Facial expression recognition, Emotion recognition, Convolution neural network},
abstract = {One of the key challenge issues of deep-learning-based facial expression recognition (FER) is learning effective and robust features from variant samples. In this paper, Region-based Convolutional Fusion Network (RCFN) is proposed to solve this issue via three aspects. Firstly, a muscle movement model is built to segment out crucial regions of frontal face, providing well-unified patches with benefits of removing unrepresentative regions and greatly reducing interference caused by facial organs with varied sizes and positions among individuals. Secondly, a fast and practical network is constructed to extract robust triple-level features from low level to semantic level in each crucial region and fuse them for FER. Thirdly, constrained punitive loss is introduced to leverage the network training for boosting up FER performance. The experiment results show that RCFN is effective in commonly used datasets like KDEF, CK+, and Oulu-CASIA, and can achieve comparable performance with other state-of-the-art FER methods.}
}
@article{FANFANI2019102586,
title = {FISH: Face intensity-shape histogram representation for automatic face splicing detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102586},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102586},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930207X},
author = {Marco Fanfani and Fabio Bellavia and Massimo Iuliani and Alessandro Piva and Carlo Colombo},
keywords = {Image forensics, Scene level analysis, Geometric constraints, Lighting environment, Face splicing detection},
abstract = {Tampered images spread nowadays over any visual media influencing our judgement in many aspects of our life. This is particularly critical for face splicing manipulations, where recognizable identities are put out of context. To contrast these activities on a large scale, automatic detectors are required. In this paper, we present a novel method for automatic face splicing detection, based on computer vision, that exploits inconsistencies in the lighting environment estimated from different faces in the scene. Differently from previous approaches, we do not rely on an ideal mathematical model of the lighting environment. Instead, our solution, built upon the concept of histogram-based features, is able to statistically represent the current interaction of faces with light, untied from the actual and unknown reflectance model. Results show the effectiveness of our solution, that outperforms existing approaches on real-world images, being more robust to face shape inaccuracies.}
}
@article{WANG2019309,
title = {Locally optimum image watermark decoder by modeling NSCT domain difference coefficients with vector based Cauchy distribution},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {309-329},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301671},
author = {Xiang-yang Wang and Si-yu Zhang and Li Wang and Hong-ying Yang and Pan-pan Niu},
keywords = {Image watermarking, NSCT difference coefficient, Vector based Cauchy distribution, Second-kind statistics, Locally most powerful test},
abstract = {Improving the ability of imperceptibility, watermark capacity, and robustness at the same time still remains a challenge within the digital image watermarking community. By modeling the robust nonsubsampled Contourlet transform (NSCT) difference coefficients with vector based Cauchy distribution and employing locally most powerful (LMP) test, we propose a locally optimum image watermark decoder in NSCT domain. We first compute the difference coefficients according to the inter-scale dependency between NSCT coefficients, and investigate the robustness of the NSCT difference coefficients by subjective visual error and objective mean squared error (MSE) terms. We then embed the digital watermark into the significant NSCT difference subband with highest energy by modifying the robust NSCT difference coefficients. At the receiver, by combining the vector based Cauchy probability distribution and LMP test, we propose a locally optimum blind watermark decoder in the NSCT domain. Here, robust NSCT difference coefficients are firstly modeled by employing the vector based Cauchy probability density function (PDF), where the Cauchy marginal statistics and various strong dependencies of NSCT coefficients are incorporated. Then the statistical model parameters of vector based Cauchy PDF are estimated using second-kind statistics approach. And finally a blind image watermark decoder is developed using vector based Cauchy PDF and LMP decision rule. We conduct extensive experiments to evaluate the performance of the proposed blind watermark decoder, in which encouraging results validate the effectiveness of the proposed technique, in comparison with the state-of-the-art approaches recently proposed in the literature.}
}
@article{WANG2019102627,
title = {A method of processing color image watermarking based on the Haar wavelet},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102627},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102627},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302482},
author = {Jianyu Wang and Zhiguo Du},
keywords = {Color image watermarking, Discrete Wavelet Transform (DWT)},
abstract = {Despite the fact that traditional digital watermarking technology is relatively mature, there are still some areas that have not been fully involved in. For example, image watermarking technology and the certification are still in its infancy at early times. The bottleneck problem of digital product safety protection all solved theoretically with the combination of computing theory as well as traditional digital watermarking technology and point out a new direction for the research of information security industry. Inspired by the traditional algorithm of image watermarking and based on the Haar wavelet function along with algorithm of 2-D discrete wavelet transform and selection, this article presents the techniques of watermark embedding and extraction of color images. The main appraisal criteria of the watermark include invisibility and robustness, and some other standards. Image watermarking is relatively simple in the spatial domain, where it cannot resist geometrical attacks. In the transform domain, this approach can resist both geometrical attacks and image processing attacks. Only when the carrier image suffers from severe damage with the image quality hugely compromised will the extracted watermark become unrecognizable. As a result, the algorithm presented in this article can well embed the color image in the carrier image, and has good resistance to attack operations such as loss compression and adding of noise.}
}
@article{MALAWSKI2019198,
title = {Improving multimodal action representation with joint motion history context},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {198-208},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.026},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301312},
author = {Filip Malawski and Bogdan Kwolek},
keywords = {Action recognition, Action descriptors, Depth maps, Feature selection, Multimodal representation, Decision-level fusion},
abstract = {Automatic recognition of actions can be addressed by employing data from multiple sensors, such as RGB cameras, depth sensors or inertial measurement units. Recent studies show that multimodal representations of actions are effective in providing rich information about motion patterns. In this work, we propose a novel action descriptor, called Joint Motion History Context, which is based on depth and skeleton data. It improves action representation when used with previously introduced descriptors that are based on depth, skeleton and inertial data. A feature selection method is proposed as well, which ranks features on the basis of their inter-class discriminative power, while minimizing redundancy in the selected feature subset. Decision-level fusion, based on Support Vector Machines and Multilayer Perceptron is employed to effectively combine motion pattern information from multiple feature sets. Experimental results on two publicly available datasets, FFD and UTD-MHAD, demonstrated that the proposed methods outperform state-of-the-art algorithms.}
}
@article{GALSHETWAR2019102615,
title = {Local energy oriented pattern for image indexing and retrieval},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102615},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102615},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302366},
author = {G.M. Galshetwar and L.M. Waghmare and A.B. Gonde and S. Murala},
keywords = {Local Binary Patterns (LBP), Local Mesh Patterns (LMeP), Local Directional Mask Maximum Edge Patterns (LDMaMEP)},
abstract = {A novel image indexing algorithm for Content Based Image Retrieval (CBIR) using Local Energy Oriented Patterns (LEOP) is proposed in this paper. LEOP encodes pixel level energy orientations to find minute spatial features of an image whereas existing methods use neighborhood relationship. LEOP maps four pixel progression orientations to find top two maximum energy changes for each reference pixel in the image i.e. for each reference 3×3 grid, two more 3×3 grids out of four pixel progression are extracted. Finally, LEOP encodes the relationship among pixels of three 3×3 local grids extracted. LEOP is applied on four different image databases named MESSIDOR, VIA/I-ELCAP, COREL and ImageNet Database using traditional CBIR framework. To test the robustness of proposed feature descriptor the experiment is extended to a learning based CBIR approach on COREL database. The LEOP outperformed state-of-the-art methods in both traditional as well as learning environments and hence it is a strong descriptor.}
}
@article{YANG2019102591,
title = {Cooperative media control parameter optimization of the integrated mixing and paving machine based on the fuzzy cuckoo search algorithm},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102591},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102591},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302123},
author = {Yikun Yang and Shengjie Jiao and Wenfa Wang},
keywords = {Integrated mixing and paving machine, Cooperative control, Cuckoo search, Fuzzy logic},
abstract = {First, this paper studies the MOH material mixing and paving equipment technology, and quantifies the correlations between each variable through the data fitting based on massive experimental data. Then, with the operation speed as the design variable and yielding the maximum mixing efficiency, minimum slip ratio and highest measuring accuracy as the three performance optimization objectives, an operation speed optimization model for the integrated mixer and paver has been built. Moreover, the principles and workflows of the cuckoo search are investigated, the population diversity and convergence rate of the cuckoo search are improved by using the fuzzy logic, and subsequently the operation speed optimization model is solved using the modified cuckoo search. At last, a simulation test based on MATLAB is carried out for validation. Research results show that the optimization results based on the modified cuckoo search algorithm excels those of frequently-used optimization algorithms such as the conventional cuckoo search and genetic algorithm. Also, the comparison between the cases of the optimized and standard operation speeds show that the mixing efficiency of the integrated mixing and paving machine can grow by 15.3%; the slip ratio drops by 54.2%; the measuring accuracy rises by 18.2%.}
}
@article{CAI2019250,
title = {Blind quality assessment of gamut-mapped images via local and global statistical analysis},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {250-259},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301397},
author = {Hao Cai and Leida Li and Zili Yi and Minglun Gong},
keywords = {Image quality assessment, Gamut mapping, Natural scene statistics},
abstract = {Gamut mapping is a key technology to achieve high-quality cross-media color reproduction. To optimize a gamut mapping algorithm, an important step is to conduct an accurate evaluation of its psycho-visual performance. This paper presents an objective blind image quality assessment (BIQA) metric for gamut-mapped images based on natural scene statistics. Considering both the local and global aspects of distortions in gamut-mapped images, two categories of statistics are analyzed. Specifically, the local statistical features are used to portray structural and color distortions and features extracted from global statistics are utilized to characterize the naturalness of image. The proposed metric does not need ground truth quality scores for training, thus it is ”completely” blind. Experimental results on three gamut mapping databases demonstrate that our method outperforms the state-of-the-art general-purpose BIQA models. To further validate its effectiveness, the proposed metric is applied for benchmarking GMAs as an application and achieves encouraging performance.}
}
@article{HE2019102587,
title = {Probabilistic guided polycystic ovary syndrome recognition using learned quality kernel},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102587},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102587},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302081},
author = {Dongyun He and Li Liu and Sheng Miao and Xiaoli Tong and Minjia Sheng},
keywords = {Probabilistic model, Image quality assessment, Image recognition},
abstract = {Image recognition aims to automatically search special objects in an image, such as human faces, vehicles, or buildings. In medical research, image recognition technique can also be applied for disease diagnosis and disease classification. Aiming at disadvantages of traditional methods in polycystic ovary syndrome (PCOS) recognition, we propose a probabilistic model for disease recognition using a deeply-learned image quality kernel. Specifically, we first segment training images into several equal-size grids for better cues discovery. Then, each grid within an image is quantitatively represented by a quality score according to grayscale and texture features. In this way, each image can be represented by a score matrix. Then, we leverage statistic based method to generate a long feature vector according to the score matrix. Afterward, we propose a probabilistic model to learn the distribution of obtained feature vector, which will be further fed into a SVM kernel for PCOS recognition. Experimental results show the effectiveness of our proposed method.}
}
@article{CHEN2019410,
title = {Research on regional energy efficiency based on GIS technology and image quality processing},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {410-417},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301816},
author = {Zhuolun Chen and Xiaowei Wu},
keywords = {Deep learning, GIS technology, Image quality analysis, Model, Quality},
abstract = {Economic development requires energy support, but the economy that relies too much on high energy consumption and high pollution is not sustainable. However, due to blind investment and low-level expansion of some new energy companies, some new energy industry has overcapacity and low production efficiency, slowing down the pace of new energy industry development, and limiting the development space of new energy companies. Thus, this paper is based on the GIS technology and image quality processing design improvement, around this idea, completed the new system simulation analysis, system development, experimental testing and so on. This paper focuses on the evaluation of the distribution efficiency of new energy industry based on image processing. Natural images have their own unique statistical properties, and interference can cause them to deviate from the original “natural statistical state.” According to the image itself and the characteristics of the human visual system, the expression of the image is adjusted, which is more conducive to the transmission, storage and understanding of information. Therefore, this paper proposes a new energy industry distribution efficiency evaluation method based on GIS technology and image processing from three aspects of the nature of natural images, the expression of image information and the characteristics of human visual system. The research shows that the method can objectively and effectively evaluate the distribution efficiency of the new energy industry, and has certain practicability, which can effectively alleviate the bottleneck problems faced above.}
}
@article{MA2019102578,
title = {Dimension reduction of image deep feature using PCA},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102578},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102578},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301932},
author = {Ji Ma and Yuyu Yuan},
keywords = {Deep learning, Feature extraction, Dimension reduction, PCA algorithm},
abstract = {Convolution neural networks based methods can derive deep features from training images. However, one challenge is that the dimension of the extracted image features increases dramatically with more network layers. To solve this problem, this paper focuses on the study of dimension reduction. After using deep learning to extract image features, the PCA algorithm is used to achieve dimension reduction. Specifically, we first leverage deep convolutional neural network to extract image features. Then, we introduce and leverage PCA algorithm to achieve dimension reduction. Aiming at the problem that it is difficult to process high-dimensional sparse big data based on PCA algorithm. This paper optimizes the PCA algorithm. After image preprocessing, the feasibility of PCA algorithm for dimension reduction of image feature extraction by deep learning is verified by simulation experiments. The efficiency of the proposed algorithm is proved by comparing the performance of PCA algorithm before and after optimization.}
}
@article{CHEN2019284,
title = {End-to-end single image enhancement based on a dual network cascade model},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {284-295},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301415},
author = {Yeyao Chen and Mei Yu and Gangyi Jiang and Zongju Peng and Fen Chen},
keywords = {Single image enhancement, Convolutional neural network, Dual network cascade model, Exposure prediction, Exposure fusion},
abstract = {A single-exposure image may lose details because of the imaging dynamic range limitations of single camera sensor. Multi-image fusion techniques are often used to improve the image quality, but if there are moving objects in the scene, the fused images may result in ghost artifacts. In order to avoid this problem and enhance single-exposure images, this paper proposes a dual network cascade model for single image enhancement, including exposure prediction network and exposure fusion network. First, the exposure prediction network generates two under-/over-exposure images that differ from the input normal-exposure image so as to recover the lost details of the under-exposed/over-exposed regions. Then, the exposure fusion network fuses the input image and the generated under-/over-exposure images to generate the final enhanced image. The loss function constructed by a structural dissimilarity index is used to alleviate chessboard artifacts in the generated image. Further, through three-phase training, the model robustly generates enhanced images without any post-processing. The experimental results demonstrate that the proposed method can effectively improve the image contrast and reconstruct details of under-exposed/over-exposed regions in the original image.}
}
@article{MEI2019102601,
title = {Learning local feature representation from matching, clustering and spatial transform},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102601},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102601},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302226},
author = {Jianhan Mei and Xudong Jiang and Jianfei Cai},
keywords = {Local image representation, Local feature learning, Convolutional Neural Network (CNN), Semi-supervised learning, Spatial transform},
abstract = {This paper focuses on learning the local image region representation via deep neural networks. Existing works mainly learn from matched corresponding image patches, with which the learned feature is too sensitive to the individual local patch matching result and cannot handle aggregation based tasks such as image level retrieval. Thus, we propose to use both the matched corresponding image patches and the clustering result as labels for the network training. To resolve the inconsistency between the matched correspondences and clustering results, we propose a semi-supervised iterative training scheme together with a dual margins loss. Moreover, a jointly learned spatial transform prediction network is utilized to obtain better spatial transform invariance of the learned local features. Using SIFT as the label initializer, experimental results show the comparable or even better performance than the hand-crafted feature, which sheds lights on learning local feature representation in an unsupervised or weakly supervised manner.}
}
@article{MICHAELREVINA201943,
title = {Face Expression Recognition with the Optimization based Multi-SVNN Classifier and the Modified LDP Features},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {43-55},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930152X},
author = {I. {Michael Revina} and W.R. {Sam Emmanuel}},
keywords = {Facial expression recognition, MultiSVNN classifier, Grasshopper optimization algorithm, Whale optimization algorithm, Local Directional Pattern},
abstract = {Facial expression recognition (FER) is the interesting research area that enables us to recognize the expression of the human face in the day-to-day life. Most of the traditional methods fail to recognize the expressions accurately as the expressions are based on the movements of the parts in the human face. The paper proposes the effective method of FER using the proposed Whale- Grasshopper Optimization algorithm based Multi-Support Vector Neural Network (W-GOA-based MultiSVNN). The features from the facial image is extracted using the Scale-Invariant Feature Transform (SIFT) and the proposed Scatter Local Directional Pattern (SLDP). The extracted features are classified using the proposed classifier to recognize the expression of the face. The proposed method of facial recognition enhances the recognition accuracy. The experimentation of the proposed algorithm is performed using the databases, such as Cohn-Kanade AU-Coded Expression Database and The Japanese Female Facial Expression (JAFFE) Database. The proposed algorithm outperforms the existing methods in terms of the accuracy, TPR, and FPR and the values are found to be 0.96, 0.96, and 0.009, respectively.}
}
@article{DUAN2019245,
title = {Multi-person pose estimation based on a deep convolutional neural network},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {245-252},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301658},
author = {Peng Duan and Tingwei Wang and Maowei Cui and Hongyan Sang and Qun Sun},
keywords = {Multi-person pose estimation, Improved faster R-CNN, Top-down structure, ResNet-152 model, Conditional random field},
abstract = {Human motion recognition based on computer vision plays an important role in many fields, such as video surveillance, virtual reality, and medical care. To solve the inaccurate multi-person pose estimation problem and improve the generalizability of the extracted features, this paper proposes a multi-person pose estimation method based on a deep convolutional neural network. This method mainly relies on a top-down structure which includes two stages. In the first stage, the bounding boxes that are likely to contain people are first detected by an improved faster R-CNN. Individuals in the complex scenario are then tailored by box cropping. In the second stage, we combine heatmap detection with coordinate regression to address the single person pose estimation problem. Specially, a deep convolutional ResNet is employed to produce heatmaps of human body. The precise location of each joint is achieved by the fully connected conditional random field. Experimental results demonstrate our method achieves comparable performance with the state-of-the-art ones.}
}
@article{SAADSHAKEEL2019102590,
title = {Learning sparse discriminant low-rank features for low-resolution face recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102590},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102590},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302111},
author = {M. {Saad Shakeel} and Kin-Man Lam and Shun-Cheung Lai},
keywords = {Face recognition, Feature fusion, Local features, Low rank approximation, Linear regression, Sparse coding},
abstract = {In this paper, we propose a novel approach for low-resolution face recognition, under uncontrolled settings. Our approach first decomposes a multiple of extracted local features into a set of representative basis (low-rank matrix) and sparse error matrix, and then learns a projection matrix based on our proposed sparse-coding-based algorithm, which preserves the sparse structure of the learned low-rank features, in a low-dimensional feature subspace. Then, a coefficient vector, based on linear regression, is computed to determine the similarity between the projected gallery and query image’s features. Furthermore, a new morphological pre-processing approach is proposed to improve the visual quality of images. Our experiments were conducted on five available face-recognition datasets, which contain images with variations in pose, facial expressions and illumination conditions. Experiment results show that our method outperforms other state-of–the-art low-resolution face recognition methods in terms of recognition accuracy.}
}
@article{HU2019225,
title = {On the use of joint sparse representation for image fusion quality evaluation and analysis},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {225-235},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301385},
author = {Yanxiang Hu and Qian Gao and Bo Zhang and Juntong Zhang},
keywords = {Image fusion, Quality evaluation, Sparse representation, Joint sparse representation, Atom remnant analysis},
abstract = {In this paper, a Spare Representation (SR) based fusion quality evaluation and analysis method is proposed. This method employs Joint Sparse Representation (JSR) to extract the source image remnants after fusion. These atom-level remnants indicate the fusion quality intuitively, and permit the analysis of fusion effect in learned feature space. Our analysis results indicate that high salient atoms always present poor expressions in fusion results. An improved fusion rule is designed to emphasis high salient atoms accordingly. In experiments, the effectiveness of our method was verified and the characteristics of atom JSR remnants were investigated in detail first. Then the new fusion rule was tested to demonstrate the value of JSR remnant analysis. The objective and subjective comparison results indicate that the proposed analytical evaluation metric can measure fusion quality and analysis atom fusion effect accurately. The new fusion rule provides a valuable alternative for SR fusion algorithm design.}
}
@article{STOSIC2019102598,
title = {Natural image segmentation with non-extensive mixture models},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102598},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102598},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302196},
author = {Dusan Stosic and Darko Stosic and Teresa Bernarda Ludermir and Tsang Ing Ren},
keywords = {Non-extensive statistics, Image segmentation, q-Gaussians, Finite mixture models},
abstract = {Finite mixture models have been widely used for image segmentation in many computer vision and pattern recognition problems. While images of natural scenes are difficult to model, we can employ emerging concepts from statistical physics to achieve better representations. This paper introduces a new class of finite mixture models for solving such problems. The proposed non-extensive mixture models have real-valued power-law exponents that characterize the degree of correlations. The exponents are used to capture rare or frequent occurring patterns in the image. They can describe complex features found with a hierarchy of sizes in natural images: from small objects with a few dozen pixels to large ones that occupy the entire image. We also present a method to determine the parameters based on maximum likelihood estimation. Our numerical experiments indicate more robust and accurate capabilities of non-extensive mixture models for natural image segmentation than conventional mixture models.}
}
@article{WU201987,
title = {A high-capacity reversible data hiding method for homomorphic encrypted images},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {87-96},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301543},
author = {Hao-Tian Wu and Yiu-ming Cheung and Zhiyuan Yang and Shaohua Tang},
keywords = {Homomorphic encryption, Privacy protection, Paillier cryptosystem, Image quality, Reversible data hiding},
abstract = {Recently, reversible data hiding in encrypted images has been developed to transmit useful data while the original images can be perfectly recovered when needed. In this paper, a new method is proposed for homomorphic encrypted images so that part of the hidden data can be extracted in encrypted domain and the rest are extractable after image decryption. Specifically, a plain-text image is preprocessed by reversibly embedding the bit values of some pixels into the image. The preprocessed image is encrypted in Paillier cryptosystem and two embedding algorithms are applied on the encrypted image in succession. Compared with the state-of-the-art schemes, higher embedding capacity can be achieved by applying the proposed method, respectively for data extraction before and after image decryption. Compared with the schemes with similar properties, better performances are achieved with the proposed method in terms of quality of directly decrypted image with respect to data hiding rate.}
}
@article{WANG2019260,
title = {Similarity-preserving hashing based on deep neural networks for large-scale image retrieval},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {260-271},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.03.024},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301300},
author = {Xiaofei Wang and Feifei Lee and Qiu Chen},
keywords = {Large-scale image retrieval, Similarity comparison, Deep learning, Multi-label learning, Quantization error},
abstract = {Similarity-preserving hashing has become the mainstream of approximate nearest neighbor (ANN) search for large-scale image retrieval. Recent research shows that deep neural networks can produce efficient feature representation. Most existing deep hashing schemes simply utilize the middle-layer features of the deep neural networks to measure the similarity between query images and database images. However, these visual features are suboptimal for discriminating the semantic information of images, especially for complex images that contain multiple objects. In this paper, a deep framework is employed to learn multi-level non-linear transformations to obtain advanced image features, and then we combine these intermediate features and top layer visual information to implement image retrieval. Three criterions are enforced on these compact codes: (1) minimal quantization loss; (2) evenly distributed binary; (3) independent bits. The experimental results on five public large-scale datasets demonstrate the superiority of our method compared with several other state-of-the-art methods.}
}
@article{XIAOHUA2019217,
title = {Two-level attention with two-stage multi-task learning for facial emotion recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {217-225},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301646},
author = {Wang Xiaohua and Peng Muzi and Pan Lijuan and Hu Min and Jin Chunhua and Ren Fuji},
keywords = {Facial emotion recognition, Attention mechanism, Multi-task learning, Valence-arousal dimension},
abstract = {Compared with facial emotion estimation on categorical model, dimensional emotion estimation can describe numerous emotions more accurately. Most prior works of dimensional emotion estimation only considered laboratory data and used video, speech or other multi-modal features. Compared with other modal data, static images has superiorities of accessibility, which is more conducive to the emotion estimation in real world. In this paper, a two-level attention with two-stage multi-task learning (2Att-2Mt) framework is proposed for facial emotion estimation on only static images. Firstly, the features of corresponding region (position level features) are extracted and enhanced automatically by first-level attention mechanism. Then, we utilize Bi-directional Recurrent Neural Network (Bi-RNN) with self-attention (second-level attention) to make full use of the relationship features of different layers (layer-level features) adaptively. And then, we propose a two-stage multi-task learning structure, which exploits categorical representations to ameliorate the dimensional representations and estimate valence and arousal simultaneously in view of the inherent complexity of dimensional representations and correlation of the two targets. The quantitative results conducted on AffectNet dataset show significant advancement on Concordance Correlation Coefficient(CCC) and Root Mean Square Error (RMSE), illustrating the superiority of the proposed framework. Besides, extensive comparative experiments have also fully demonstrated the effectiveness of different components (2Att and 2Mt) in our framework.}
}
@article{GE2019102577,
title = {Deep spatial attention hashing network for image retrieval},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102577},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102577},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301920},
author = {Lin-Wei Ge and Jun Zhang and Yi Xia and Peng Chen and Bing Wang and Chun-Hou Zheng},
keywords = {Hash function, Deep learning, Image retrieval, Convolutional neural network (CNN), Deep hashing},
abstract = {Hashing is one of the most popular image retrieval technique since its fast-computational speed and low storage cost. Recently, deep hashing methods have greatly improved the image retrieval performance in contrast to traditional hashing method. However, the binary hashing representation is only generated from the global image region, which may result in sub-optimal hashing code. Inspired by the latest advance in spatial attention mechanism, we propose an novel end-to-end deep hashing framework which composes of two sub-networks. One sub-network uses spatial attention model to determine the local features from more specific region of interest, another sub-network extracts the global features from original image. By combining the local and global features with learnable hash functions, the proposed deep hashing framework can optimize the deep hash function and high-quality binary code jointly. Numerous experiments on two large scale image benchmarks datasets have shown that the proposed method is superior to other existing methods for image retrieval.}
}
@article{SRIMAN201923,
title = {Multi-script text versus non-text classification of regions in scene images},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {23-42},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301403},
author = {Bowornrat Sriman and Lambert Schomaker},
keywords = {Text detection in scene images, Text/non-text classification, Color features, Color histogram autocorrelation},
abstract = {Text versus non-text region classification is an essential but difficult step in scene-image analysis due to the considerable shape complexity of text and background patterns. There exists a high probability of confusion between background elements and letter parts. This paper proposes a feature-based classification of image blocks using the color autocorrelation histogram (CAH) and the scale-invariant feature transform (SIFT) algorithm, yielding a combined scale and color-invariant feature suitable for scene-text classification. For the evaluation, features were extracted from different color spaces, applying color-histogram autocorrelation. The color features are adjoined with a SIFT descriptor. Parameter tuning is performed and evaluated. For the classification, a standard nearest-neighbor (1NN) and a support-vector machine (SVM) were compared. The proposed method appears to perform robustly and is especially suitable for Asian scripts such as Kannada and Thai, where urban scene-text fonts are characterized by a high curvature and salient color variations.}
}
@article{YANG2019188,
title = {Enhanced two-phase residual network for single image super-resolution},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {188-197},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S104732031930135X},
author = {Juan Yang and Wenjing Li and Ronggui Wang and Lixia Xue and Min Hu},
keywords = {Super-resolution, Convolutional neural network, Deep residual learning, Dilated convolution},
abstract = {Recent research on deep residual neural network has made breakthrough in single image super-resolution. However, these studies ignore persistent memory of information in the propagation, which fail to infer plausible high-frequency sufficiently. To solve this problem, we propose a novel enhanced two-phase residual network (ETRN). The proposed ETRN progressively generate high-resolution image in two phases so that we can recover accurate texture. We also present dense residual unit to capture hierarchical features in the local residual learning phase. With the help of these high-level representations, we use global residual learning to improve reconstruction quality in the second phase. In order to reduce cumbersome computation, between two phases we introduce dilated convolution which enlarges receptive field without increasing additional parameters. Comprehensive experiments show that ETRN has a significant improvement in both quantitation and visual perception.}
}
@article{CHEN2019402,
title = {Research on deep learning in the field of mechanical equipment fault diagnosis image quality},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {402-409},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301804},
author = {Xue Chen and Lanyong Zhang and Tong Liu and M.M. Kamruzzaman},
keywords = {Deep learning, Mechanical equipment, Equipment maintenance, Image quality},
abstract = {Image quality assessment (IQA) is an indispensable technique in computer vision, which is widely applied in image classification, image clustering. With the development of deep learning, deep neural network (DNN)-based methods have shown impressive performance. Thus, in this paper, we propose a novel method for mechanical equipment fault diagnosis based on IQA. More specifically, we first conduct data acquisition base on our practice. Afterwards, we leverage image processing method for removing noise. Subsequently, we leverage CNN-based method for image classification. Finally, different mechanical equipment images will be grouped into different categories and fault detection can be achieved. Extensive experiments demonstrate the effectiveness and robustness of our method.}
}
@article{KALJAHI2019102573,
title = {A new image size reduction model for an efficient visual sensor network},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102573},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102573},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301889},
author = {Maryam Asadzadeh Kaljahi and Palaiahnakote Shivakumara and Mohd Yamani Idna Idris and Mohammad Hossein Anisi and Michael Blumenstein},
keywords = {Visual sensor network, Image size reduction, Inter-redundancy, Intra-redundancy, Energy consumption, Quality of the image},
abstract = {Image size reduction for energy-efficient transmission without losing quality is critical in Visual Sensor Networks (VSNs). The proposed method finds overlapping regions using camera locations, which eliminate unfocussed regions from the input images. The sharpness for the overlapped regions is estimated to find the Dominant Overlapping Region (DOR). The proposed model partitions further the DOR into sub-DORs according to capacity of the cameras. To reduce noise effects from the sub-DOR, we propose to perform a Median operation, which results in a Compressed Significant Region (CSR). For non-DOR, we obtain Sobel edges, which reduces the size of the images down to ambinary form. The CSR and Sobel edges of the non-DORs are sent by a VSN. Experimental results and a comparative study with the state-of-the-art methods shows that the proposed model outperforms the existing methods in terms of quality, energy consumption and network lifetime.}
}
@article{YIN2019102613,
title = {A new charge structure based on computer modeling and simulation analysis},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102613},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102613},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302342},
author = {J.P. Yin and Y.Y. Han and X.F. Wang and B.H. Chang and F.D. Dong and Y.J. Xu},
keywords = {Charge structure, Computer modeling and simulation technology, PELE, Liner},
abstract = {This paper proposes a new charge structure, which can form a penetrator with enhanced lateral effect (PELE) or explosively formed projectile (EFP) by different initiation modes. The purpose of this paper is to study the dynamic response and aftereffect of the charge liner under explosive load in PELE mode. To this end, the computer modeling and simulation analysis method was used to simulate the forming and penetrating target progress of the PELE formed by charge liner. The results indicated that the ANSYS/LS-DYNA finite element software could accurately simulate the formation of PELE and the damage process to the target, and we could obtain exact forming and damage data by this method. The PELE damage element could be formed by the new charge structure liner under explosive load, its velocity was 2050 m/s, the head-to-tail velocity gradient was only 20 m/s, the length of damage element was 63.62 mm, and the length-diameter ratio was 1.79. And PELE damage element could effectively damage 15 mm target, the inlet opening hole diameter of the target was measured to be 68 mm, the outlet diameter was 72 mm. This finding could serve a technical basis for computer modeling and simulation technology of charge structure.}
}
@article{ZAKHAROV2019102574,
title = {Towards controllable image descriptions with semi-supervised VAE},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102574},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102574},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301890},
author = {Nikolai Zakharov and Hang Su and Jun Zhu and Jan Gläscher},
keywords = {VAE, Image caption, Generative models, Semi-supervised},
abstract = {Image captioning models successfully describe the visual contents of images using natural language. To generate more natural and diverse descriptions, a model must learn style-specific patterns and requires collecting style-specific datasets, which is time-consuming. To address this issue, we propose a semi-supervised deep generative model, Semi-supervised Conditional Variational Auto-Encoder (SCVAE). Our model is capable of leveraging more labelled and unlabelled data in the generative model schema. Extensive empirical results demonstrate that compared with the start-of-art models, our proposed method is able to generate more accurate image captions with more extensive styles.}
}
@article{LIU2019272,
title = {Learning based no-reference metric for assessing quality of experience of stereoscopic images},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {272-283},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301373},
author = {Tsung-Jung Liu and Kuan-Hsien Liu and Kuan-Hung Shen},
keywords = {Disparity-depth map, No reference, Quality of experience, Stereoscopic image, Visual discomfort},
abstract = {Human’s perception plays a very important role on image assessment, especially for stereoscopic images. In general, viewing stereoscopic 3D images will cause visual fatigue, eyestrain, dizziness or headache. Therefore, how to evaluate human’s perception of visual quality on 3D images becomes an emerging topic. In this paper, we propose a no-reference assessment metric for stereoscopic image quality of experience (QoE). First, the stereoscopic image pairs are used to calculate the disparity maps by optical flow estimation. Then the depth information are extracted from the disparity map, called as disparity-depth map. Next, we extract four types of features based on pixel value and distribution of disparity-depth map. Two regression models are used to predict visual discomfort scores. Also, we test the proposed method on EPFL 3D image database and IEEE-SA stereoscopic image database, respectively. The experiment results show that our proposed QoE assessment metric achieves excellent performance compared with state-of-the-art methods.}
}
@article{YUAN2019102629,
title = {IPTV video quality assessment model based on neural network},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102629},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102629},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302500},
author = {Ying Yuan and Cong Wang},
keywords = {BP neural network, IPTV video, Quality assessment, Clustering analysis},
abstract = {In recent years, with the continuous development of network science and technology and the continuous promotion of “three networks convergence”, IPTV (Interactive Network Television) has shown a rapid development trend. IPTV is different from the traditional one-way broadcasting mode of television, it can achieve interaction with the audience, and provide more personalized and diversified videos. With the rapid development of new media, massive video resources and a large number of video-related information are sweeping in. The evaluation of video can no longer be limited to the ratings of traditional platforms. Video playback on new media platforms, network impact, video content and other related data are also important indicators affecting video evaluation. Video quality is closely related to video content characteristics. Different videos have different sensitivity to the same packet loss rate. The IPTV video quality evaluation model based on content features considers the video content characteristics. Firstly, the QP, bit rate and motion vector (MV) information of the video is obtained by analyzing the video stream. Then, the time complexity of the video is calculated by using the obtained MV, and the spatial complexity of the video is calculated by quantization parameters and bit rate. Videos are classified by clustering analysis. On this basis, the BP neural network is used to establish the model and evaluate the video quality, so as to better reflect the visual perception of the human eye. The model has low computational complexity and is suitable for IPTV video quality assessment with certain computing power in network nodes.}
}
@article{SREEJA2019340,
title = {Towards genre-specific frameworks for video summarisation: A survey},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {340-358},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301774},
author = {M.U. Sreeja and Binsu C. Kovoor},
keywords = {Video summarisation, Video summary, Genre-specific, Skim, Keyframe},
abstract = {Video summarisation is characterised as the process of extracting meaningful frames or segments from a video that best represents the content of the whole video. The proposed framework surveys and categorizes the existing video summarisation models in the recent research works on the basis of genre. The most important phase of video summarisation is the detection of key frames or segments in the video. The strategy for identifying key frames or segments vary for each genre. The various genre analysed are user generated videos, movies and documentary, sports, surveillance, egocentric and informational talk videos with a total of more than 25 varying parameters significant to the respective genre. Comprehensive evaluations of the results obtained from the models are also included based on quantitative and qualitative parameters. The framework will help the user in deciding the technology to be adopted for video summarisation in a particular domain. The framework also aids in deciding the type of summary suitable for each genre and the available datasets in each genre for experimental analysis.}
}
@article{ARIGE201956,
title = {HEVC intra-frame drift cancellation matrix},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {56-67},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301531},
author = {A. Arige and M. Mitrea and I. Boujelbane},
keywords = {HEVC, Additive modification, Intra-drift free, Theoretical computation, Masking matrix},
abstract = {This paper investigates the possibility of avoiding the HEVC intra-frame drift effect induced by additive modifications of the luma DCT/DST coefficients. The main novelty consists in solving the intra-drift problem in its general form: starting from the equations defining the 35 intra-prediction modes and the DCT/DST computation, a mask multiplicative matrix cancelling the intra-frame drift effect is computed. The relevance of the advanced method with respect to the state of the art studies is illustrated, discussed and quantitatively assessed.}
}
@article{HUANG2019102585,
title = {Automatic building change image quality assessment in high resolution remote sensing based on deep learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102585},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102585},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302068},
author = {Fenghua Huang and Ying Yu and Tinghao Feng},
keywords = {Deep belief network, Building change detection, Morphological building index, Morphological shadow index, Extreme learning machine, Quality Assessment},
abstract = {The multi-temporal high-resolution remote sensing (HRRS) images are usually acquired at different imaging angles, with serious noise interferences and obvious building shadows, so that detecting the changes of urban buildings is a problem. In order to address this challenge, a deep learning-based algorithm called ABCDHIDL is proposed to automatically detect the building changes from multi-temporal HRRS images. Firstly, an automatic selection method of labeled samples of building changes based on morphology (ASLSBCM) is proposed. Secondly, a deep learning model (DBN-ELM) for building changes detection based on deep belief network (DBN) and extreme learning machine (ELM) is proposed. A convolution operation is employed to extract the spectral, texture and spatial features and generate a combined low-level features vector for each pixel in the multi-temporal HRRS images. The unlabeled samples are introduced to pre-train the DBN, and the parameters of DBN-ELM are globally optimized by jointly using the ELM classifier and the labeled samples are offered by ASLSBCM to further improve the detection accuracy. In order to evaluate the performance of ABCDHIDL, four groups of double-temporal WorldView2 HRRS images in four different experimental regions are selected respectively as the test datasets, and five other representative methods are used and compared with ABCDHIDL in the experiments of buildings change detection. The results show that ABCDHIDL has higher accuracy and automation level than the other five methods despite its relatively higher time consumption.}
}
@article{CLARO2019102597,
title = {An hybrid feature space from texture information and transfer learning for glaucoma classification},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102597},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102597},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302184},
author = {Maíla Claro and Rodrigo Veras and André Santana and Flávio Araújo and Romuere Silva and João Almeida and Daniel Leite},
keywords = {Glaucoma detection, Feature selection, Pre-trained CNNs, Transfer learning},
abstract = {Glaucoma is a progressive eye disease due to the increase in intraocular pressure. Accurate early detection may prevent vision loss. Most algorithms in the literature are not feasible for use in screening programs since they are not able to handle a wide diversity of images. We conducted an extensive study to determine the best set of features for image representation. Our feature extraction methodology included the following descriptors: LBP, GLCM, HOG, Tamura, GLRLM, morphology, and seven CNN architectures, that results in 30.682 features. Then, we used the gain ratio to order the features by importance and select the best set for glaucoma classification. Our tests were performed using 1675 images of DRISHTI, RIM-ONE, HRF, JSIEC, and ACRIMA databases. We concluded that a combination of the GLCM and pre-trained CNN’s has the potential to be used in a computer aid system for glaucoma detection. Our approach achieved an accuracy of 93.61%.}
}
@article{TIAN2019102576,
title = {Weighted correlation filters guidance with spatial-temporal attention for online multi-object tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102576},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102576},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301919},
author = {Sheng Tian and Lian Zou and Cian Fan and Liqiong Chen},
keywords = {Multi-object tracking, Weighted correlation filters, Tracking by detection, Spatial-temporal attention mechanism},
abstract = {In recent years, discriminative correlation filters based trackers have made remarkable achievements for single object tracking, while directly applying these trackers for multi-object tracking may encounter some problem in drifted results caused by occlusion and missing detection from the detector. Thus, we propose a weighted-correlation-filters framework with spatial-temporal attention mechanism for online multi-object tracking to solve the above problems. First, we use the weighted correlation filters with dynamic updating scheme to pre-track each object in the current frame, which helps to filter out the improper detection according to the position of pre-tack for each object and is capable of tracking objects of the false negative. Then, we introduce a spatial-temporal attention mechanism to produce a discriminative appearance model and calculate reliable similarity scores for data association. The proposed online algorithm achieves 48.4% in MOTA on challenging MOT17 benchmark dataset and better performance on MT and ML than some offline methods.}
}
@article{WANG2019102572,
title = {Content-sensitive superpixel segmentation via self-organization-map neural network},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102572},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102572},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301877},
author = {Murong Wang and Xiabi Liu and Nouman Q. Soomro and Guanhui Han and Weihua Liu},
keywords = {Superpixel segmentation, Content sensitive, Self-Organization Map (SOM), Clustering},
abstract = {Content-sensitive superpixel segmentation generates small superpixels in content-dense regions and large superpixels in content-sparse regions. It achieves higher segmentation accuracy than traditional superpixels. In this paper, we propose a content-sensitive superpixel segmentation algorithm based on Self-Organization-Map (SOM) neural network. First, we propose a novel metric to measure the content-sensitiveness of superpixels. Second, by using this metric, we develop a sampling algorithm to sample pixels from image according to their content-sensitiveness. Finally, a SOM neutral network is trained with the sampled pixels and used to segment the image into content-sensitive superpixels. The Berkeley Image Segmentation database and INRIA database are used to evaluate the proposed method. The experiment results show that the proposed approach outperforms state-of-the-art methods.}
}
@article{LI2019226,
title = {Fast combination filtering based on weighted fusion},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {226-233},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301634},
author = {Wujing Li and Wei He and Xianfeng Ou and Wenjing Hu and Jianhui Wu and Guoyun Zhang},
keywords = {Filter, Linear combination, Multi-scale, Edge preserving},
abstract = {This paper presents a fast filter based on a modified Lee filter. The smoothing operation in the proposed filter is a linear combination of the center pixel value and the average of pixel values in a window. Based on the new definitions of edge and noise, the local gradient mean is introduced to compute the coefficient of the linear combination. Besides, a multi-scale method is employed to smooth the pixel values near edges. The main advantage of the proposed filter is its computational cost. In addition to some point operations, only several mean filters are taken in this filter. No matter how large the window size is, the time complexity is O(N) (N is the pixel number). Experimental results have shown that the proposed filter can effectively smooth images while keeping edges well, thus can be widely used in computer vision and image processing.}
}
@article{YANG2019296,
title = {Exploring frame segmentation networks for temporal action localization},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {296-302},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300549},
author = {Ke Yang and Xiaolong Shen and Peng Qiao and Shijie Li and Dongsheng Li and Yong Dou},
keywords = {Action detection, Temporal action localization, Convolutional Neural Network},
abstract = {Temporal action localization is an important task of computer vision. Though many methods have been proposed, it still remains an open question how to predict the temporal location of action segments precisely. Most state-of-the-art works train action classifiers on video segments pre-determined by action proposal. However, recent work found that a desirable model should move beyond segment-level and make dense predictions at a fine granularity in time to determine precise temporal boundaries. In this paper, we propose a Frame Segmentation Network (FSN) that places a temporal CNN on top of the 2D spatial CNNs. Spatial CNNs are responsible for abstracting semantics in spatial dimension while temporal CNN is responsible for introducing temporal context information and performing dense predictions. The proposed FSN can make dense predictions at frame-level for a video clip using both spatial and temporal context information. FSN is trained in an end-to-end manner, so the model can be optimized in spatial and temporal domain jointly. We also adapt FSN to use it in weakly supervised scenario (WFSN), where only video level labels are provided when training. Experiment results on public dataset show that FSN achieves superior performance in both frame-level action localization and temporal action localization.}
}
@article{LI2019102611,
title = {A multiscale dilated dense convolutional network for saliency prediction with instance-level attention competition},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102611},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102611},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302329},
author = {Hao Li and Fei Qi and Guangming Shi and Chunhuan Lin},
keywords = {Saliency, Attention competition, Convolutional neural networks, Dense connections, Dilated convolution, Multiscale features},
abstract = {Data-driven saliency estimation attracts increasing interests in recent years because of the establishment of large-scale annotated datasets and the evolution of deep convolutional neural networks (CNN). Although CNN-based models perform much better than traditional ones in saliency prediction, there is still a gap between computational models and human behavior. One reason is that existing approaches fail assigning correct saliency to different objects in scenes with multiple objects. In this paper, we propose a multiscale dilated dense convolutional network to handle instance-level attention competition for better saliency prediction. In the proposed architecture, dense connections encode inter- and intra-class features for instance-level attention competition, dilated convolution collects contextual information to enrich feature representations of instances, and shortcut connections provide multiscale features for attention competition across scales. According to evaluations on three challenging datasets, CAT2000, SALICON, and MIT1003, the proposed model achieves the state-of-the-art performance.}
}
@article{JIAO2019102584,
title = {Weakly labeled fine-grained classification with hierarchy relationship of fine and coarse labels},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102584},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102584},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302056},
author = {Qihan Jiao and Zhi Liu and Linwei Ye and Yang Wang},
keywords = {Fine-grained classification, Weakly labeled images, Convolutional block attention, Feature fusion},
abstract = {The current work of fine-grained classification generally depends on a large number of fine labels of images. However, these fine labels are much more difficult to annotate than the coarse labels, which generalize fine labels based on the hierarchy of categories. In this paper, we propose to make fine labels prediction under a weakly supervised setting where a subset of training data is labeled with fine labels and the others only have coarse labels. We aim to explore the hierarchy relationship between coarse classes and fine classes to achieve a better performance on fine-grained classification and meanwhile reduce the heavy dependence on fine labels. To this end, we use convolutional block attention module and multi-scale convolution kernel based feature fusion to generate more effective features from multi-scale convolution kernels and multi-level features. Besides, an adaptive classification module exploits the hierarchy relationship of categories to learn the fine-grained classifier automatically according to the available labels of the training data. Comprehensive experiments on the CIFAR100 dataset, a subset of ImageNet and CUB-200-2011 dataset demonstrate the better fine-grained classification performance of our model.}
}
@article{ZHOU2019102612,
title = {Analysis of active faults based on natural earthquakes in Central north China},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102612},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102612},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302330},
author = {Junjie Zhou and Guowei Zhu and Qingchao Zhang and Zhenqiang Yang and Pengfei Sun and Jiahao Liu},
keywords = {Active fault zone, Earthquake distribution, Taihang mountain front fault, Velocity inversion},
abstract = {As an important part of the integration of Beijing-Tianjin-Hebei, it is very important to analyze the seismic activity of active structures in Central north China. There are two sets of active faults belt in the lot, and there have been devastating earthquakes, which need to grasp the level of seismic activity. Located at the boundary of the third-order tectonic unit, there are a series of faults in the area, such as the north to the east Taihang mountain front fault, the north to the east Xinhe fault and the north to the west Cixian-daming fault, which intersect and cut each other to form fault depression basin. There are different scales of NE, NNE, and NW faults, which are considered to be the birthplace of the earthquake. At the same time, more than 6 magnitude earthquake magnitude have happened in the Cixian and Xingtai. The seismogenic structure of the research shows that these earthquakes associated with deep fault activities, the source location in the deep crust velocity structure mutation. In order to determine and analyze the P-wave velocity structure characteristics and the hypocenter distribution, and the activity characteristics of the deep space of active fault belt, the natural seismic data monitored by seismic network are collected and organized, which are used to analyze the relationship between seismic wave velocity and hypocenter position. Due to the deep migration of the crustal material and the horizontal principal compressive the NEE direction stress in North China, the crustal thickness on the west side of the Taihang mountain front fault is greater than that of the east side, from 1 km to 7 km. Along the trend, the epicenter of the small earthquake is mainly distributed in the crustal thickening area on the west side of this active fault, and the epicenter of the eastern plain is less distributed. The depth of the small earthquake is concentrated in the range of 8–20 Km. Comprehensive analysis shows that the seismic p-wave velocity structure characteristics can be divided into the sedimentary cover, upper crust, the earth's crust and the lower crust structure, thickness of different location have change, the thickness of the sedimentary cover Taihang uplift zone thickness 0.1–3 km, to 5–7 km in Handan fault depression; The thickness of the crystalline basement in the Taihang mountain uplift is 3–5 km, and the Handan fault depression basin is thickened to 7–10 km. The thickness of the crust on the west side of Taihang mountain front fault is significantly greater than that on the east side. The thickness of the crust on the west side is decreased from 36–40 km on the west side to 30–35 km on the east side and about 7–10 km on the east side. Due to the near east-west tension, the zone has disengaging movement, forming the characteristics of shovel-type normal fault combination. In the earth's crust with high-speed and low-speed layer between configuration characteristics, seismic horizon of earthquake preparation 12–18 km deep in the earth's crust, characterized by low speed and high speed layer mutation position, concentrated distribution of small earthquakes, the seismogenic layer a concentration distribution in the crust velocity structure conversion section. Seismic activity is concentrated in the west end of the Cixian-daming fault and the west side of the Xinhe fault, with an average depth of 12–18 km.}
}
@article{ABDOLALI2019303,
title = {Robust subspace clustering for image data using clean dictionary estimation and group lasso based matrix completion},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {303-314},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301361},
author = {Maryam Abdolali and Mohammad Rahmati},
keywords = {Subspace estimation, Sparse representation, Sparse subspace clustering, Group lasso, Multi-scale estimation, Matrix completion},
abstract = {In this paper, we consider the problem of subspace clustering for image data under occlusion and gross spatially contiguous noise. The state of the art subspace clustering methods assume that the noise either follows independent Laplacian or Gaussian distributions. However, the realistic noise is much more complicated and exhibits different structures in different scales. To address this issue, we propose a multi-scale framework that extracts a clean self-expressive dictionary through an iterative approach and is capable of identifying probable corrupted elements in each sample. Using this information, not only we can estimate parameters of each subspace more accurately but also by optimizing a matrix completion problem based on group sparsity, we can recover corrupted regions more precisely and hence achieve higher clustering accuracy for corrupted samples. Numerical experiments on synthetic and real world data sets demonstrate the efficiency of our proposed framework in presence of occlusion and spatially contiguous noise.}
}
@article{RUI2019102600,
title = {Fault point detection of IOT using multi-spectral image fusion based on deep learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102600},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102600},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302214},
author = {Hou Rui and Zhao Yunhao and Tian Shiming and Yang Yang and Yang Wenhai},
keywords = {Convolution neural network, IoT fault point detection, Deep learning, Multi-spectral image fusion},
abstract = {Internet of Things (IoT) is widely applied in modern power systems, which could establish the intelligent power grid systems and obtain considerable social and economic benefits. IoT plays an important role in power grid safety production, user interaction, and information collection. However, existing methods cannot address problems of IoT devices accurately and quickly, such as fault detection. Aiming at the shortcomings of current power IoT equipment fault detection methods, this paper proposes a multi-spectral image fusion based on deep learning to detect fault points of power IoT equipment. The deep convolutional neural network is trained by simulating the image of the power device. The results show that the multi-spectral image descriptor based on deep learning presented in this paper shows very high accuracy in block matching, and the effect of image fusion is remarkable. This indicates that the proposed method can accurately integrate multi-spectral images of power equipment, helping to locate fault points quickly and accurately.}
}
@article{TIIROLA2019286,
title = {A learning based approach to additive, correlated noise removal},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {286-294},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301762},
author = {Juha Tiirola},
keywords = {Image denoising, Correlated noise, Convolutional neural networks, Deep learning},
abstract = {In this paper, removal of additive, signal-independent, correlated noise from images is considered. We consider the non-blind case, meaning that the stationary autocovariance function of the noise is assumed to be known. The denoising method is based on unrolled optimization where in the half-quadratic energy minimization each proximal step is replaced by a learnt convolutional neural network. The proximal steps take place in learnt transform domains. Functions producing the regularization parameters are also learnt. We assume that we have a distribution for autocovariance functions in order to be able to draw samples. For simplicity, we assume that the noise has low-pass spectral character and a typical autocovariance function has a relatively simple form. The experimental results demonstrate that in terms of PSNR values, the method performs better than two classical methods and a method based on a learnt patch prior.}
}
@article{WANG2019102608,
title = {Quality guided image recognition towards industrial materials diffusion},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102608},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102608},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302299},
author = {Hua Wang and Cong Li},
keywords = {Image recognition, Image quality},
abstract = {Ordered porous materials, especially mesoporous materials, molecular sieves and organometallic framework materials, are widely used in the fields of adsorption, membrane separation and catalytic reaction processes due to their unique properties. Molecular sieve porous material is a new type of engineering material with excellent performance, and its microstructure is one of the key factors affecting macroscopic physical properties and use effect. At present, in order to further broaden its application in the fields of separation, catalysis, sensors and micro-devices, at present, microscopic molecular level control of porous material composition and structure, macroscopically controlling its appearance and appearance has become a research of porous materials. An important development direction. However, the quantitative characterization of the molecular structure of molecular sieve porous materials and its influence on physical properties has always been the focus and difficulty in the field of materials science and engineering. At the same time, the microstructure of porous materials of molecular sieves is the key factor affecting its macroscopic physical properties, and the analysis of spatial structure characteristics. Has important research significance. Based on this paper, a method for predicting effective diffusion coefficient in porous materials by convolutional neural network is proposed. The training samples of porous material microstructure are generated by computer stochastic simulation, and the corresponding effective diffusion coefficients are calculated by finite element method. The image quality subjective evaluation model is used to control the microscopic picture precision of the molecular sieve porous material. The combination of the two methods can quickly and accurately calculate the effective diffusion coefficient.}
}
@article{CAETANO2019102596,
title = {Magnitude-Orientation Stream network and depth information applied to activity recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102596},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102596},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302172},
author = {Carlos Caetano and Victor H.C. {de Melo} and François Brémond and Jefersson A. {dos Santos} and William Robson Schwartz},
keywords = {Activity recognition, Convolutional neural networks (CNNs), Two-stream convolutional networks, Spatiotemporal information, Optical flow, Depth information},
abstract = {The temporal component of videos provides an important clue for activity recognition, as a number of activities can be reliably recognized based on the motion information. In view of that, this work proposes a novel temporal stream for two-stream convolutional networks based on images computed from the optical flow magnitude and orientation, named Magnitude-Orientation Stream (MOS), to learn the motion in a better and richer manner. Our method applies simple non-linear transformations on the vertical and horizontal components of the optical flow to generate input images for the temporal stream. Moreover, we also employ depth information to use as a weighting scheme on the magnitude information to compensate the distance of the subjects performing the activity to the camera. Experimental results, carried on two well-known datasets (UCF101 and NTU), demonstrate that using our proposed temporal stream as input to existing neural network architectures can improve their performance for activity recognition. Results demonstrate that our temporal stream provides complementary information able to improve the classical two-stream methods, indicating the suitability of our approach to be used as a temporal video representation.}
}
@article{HONG2019102575,
title = {Acceleration of RED via vector extrapolation},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102575},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102575},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301907},
author = {Tao Hong and Yaniv Romano and Michael Elad},
keywords = {Inverse problem, RED – REgularization by Denoising, Fixed-point, Vector extrapolation, Acceleration},
abstract = {Models play an important role in inverse problems, serving as the prior for representing the original signal to be recovered. REgularization by Denoising (RED) is a recently introduced general framework for constructing such priors using state-of-the-art denoising algorithms. Using RED, solving inverse problems is shown to amount to an iterated denoising process. However, as the complexity of denoising algorithms is generally high, this might lead to an overall slow algorithm. In this paper, we suggest an accelerated technique based on vector extrapolation (VE) to speed-up existing RED solvers. Numerical experiments validate the obtained gain by VE, leading to substantial savings in computations compared with the original fixed-point method.}
}
@article{2021103339,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {80},
pages = {103339},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(21)00224-8},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002248}
}
@article{LIU2019102610,
title = {Research on infrared image enhancement and segmentation of power equipment based on partial differential equation},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102610},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102610},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302317},
author = {Dun Nan Liu and Rui Hou and Wen Zhuo Wu and Jing Wen Hua and Xuan Yuan Wang and Bo Pang},
keywords = {Partial differential equation, Power equipment, Infrared image, Quality model},
abstract = {With the development of science and technology, image processing is applied more and more widely. In power system, a large number of power equipment always work in harsh environment, which easily leads to equipment damage, not only affects the normal operation of equipment, but also may lead to accidents. In order to ensure the normal operation of power equipment, it is necessary to monitor the operation status of power equipment in real time. The key of real-time monitoring is how to analyze the quality of infrared image. In order to improve the quality of infrared image of power equipment, this paper analyses the traditional image processing technology, mainly using partial differential equation to optimize and improve the image enhancement algorithm and segmentation method, so as to improve the quality of infrared image of power equipment. Firstly, in the aspect of infrared image enhancement, partial differential equation is used to improve the shortcomings of the traditional contrast enhancement method, enhance the texture details of the image while effectively improving the brightness visual effect of the image; then, in image segmentation, the stopping function of GAC model of classical partial differential equation is improved to improve the effect of infrared image segmentation of power equipment. Through the analysis of simulation experiments, this paper improves the traditional method by using partial differential equation in image enhancement and image segmentation, which can effectively improve the quality and segmentation effect of infrared image of power equipment.}
}
@article{LIU2019102570,
title = {Image classification toward lung cancer recognition by learning deep quality model},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102570},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301865},
author = {Ying Liu and Haodong Wang and Yue Gu and Xiaohong Lv},
keywords = {Image classification, Cancer recognition, Deep feature, CNN},
abstract = {Image classification aims to automatically group a set of images into several categorizations, which is widely applied in scene categorization, image clustering. Lung cancer recognition can be achieved by using image classification technique, since there are distinct differences between healthy lung and sick lung images. In this paper, we propose lung cancer recognition based on image quality assessment, which can distinguish sick lung images from healthy lung images. First, our dataset is acquired using low-dose CT scan combined with full-mode iterative recombination (IMR). Then, we incorporate both low-level and high-level features to extract deep representation from obtained dataset. Specifically, our designed low-level features include color moment and texture feature, and CNN based method is leveraged for deep feature extraction. For reducing artifacts and noise of images, we assign quality score for each training image. And quality score and deep feature are fused to generate deep representation. Afterward, we propose a probabilistic model to learn the distribution of deep representation. Finally, lung cancer recognition can be achieved using learned model. We conduct comprehensive experiments and our proposed method is verified effective.}
}
@article{WANG2019315,
title = {Action recognition using dynamic hierarchical trees},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {315-325},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301348},
author = {Tingwei Wang and Peng Duan and Bingxian Ma and Peng Wu and Weizhi Lu},
keywords = {Action recognition, Hierarchical modeling, Evolution, Tree kernel},
abstract = {Hierarchical models have shown their effectiveness for action recognition. However, most of the existing hierarchy construction methods fail to model the complex motion patterns in videos, and thus are vulnerable to the interference of the inevitable noise in action videos. Therefore, we propose a Dynamic Hierarchical Tree (DHT) model to characterize such complex motion for better recognition performance. First, a minimum maximum DTW (mmDTW) is developed to produce more stable atomic actions by limiting the minimum and maximum lengths of atomic actions. Then an aggregation method is utilized to construct a DHT for each video by merging atomic actions from bottom to top. Not only the similarity between frames but also the compatibility of dynamic evolution between frames and segments is exploited for the mmDTW and the aggregation process, making the DHTs more suitable for modeling actions in videos. Finally, a k-Nearest Neighbors Edge Pairs (kNNEP) kernel is proposed to compare two DHTs by using the mean similarity of k nearest neighbors edge pairs.}
}
@article{XU2019394,
title = {Personalized training through Kinect-based games for physical education},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {394-401},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301622},
author = {Mingliang Xu and Yafang Zhai and Yibo Guo and Pei Lv and Yafei Li and Meng Wang and Bing Zhou},
keywords = {Kinect, Educational games},
abstract = {In recent years, the Kinect-based systems that enable users to be trained without the participation of teachers have been widely used in the field of physical education. In this paper, we propose a novel technique that helps the Kinect-based training system to select the subsequential training material for the users according to their realtime performance. An algorithm based on the Hidden Markov Model is demonstrated to generate the customized training pathes(training curriculums) for each individual. We present an edutainment gaming system for children in order to illustrate the feasibility of the training method. A user study of 10 children participants is conducted and the results show that the proposed technique enhances the effect of physical training significantly.}
}
@article{ZHONG2019267,
title = {Discriminative representation learning for person re-identification via multi-loss training},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {267-278},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301749},
author = {Weilin Zhong and Tao Zhang and Linfeng Jiang and Jinsheng Ji and Zenghui Zhang and Huilin Xiong},
keywords = {Person re-identification, Multi-loss training, Inter-center loss},
abstract = {The identification model that employs softmax loss to minimize person identity classification errors has gradually gained popularity in person re-identification community due to its easy implementations. However, the softmax loss only encourages the separation of different identities. The intra-class differences caused by large view variations such as spatial misalignment and human pose change are not considered in the model training process. In this paper, we present a hybrid deep model that combines multiple loss functions to handle this problem. Specifically, the multi-loss function contains three terms, namely softmax loss, center loss, and a novel loss called inter-center loss. The center loss penalizes the distance between deep features and their center, aiming to reduce intra-class differences. The inter-center loss maximizes the distances between different class centers, aiming to further enlarge inter-class separation. Extensive experiments conducted on three public benchmark datasets including Market1501, CUHK03, and DukeMTMC-reID demonstrate the effectiveness of our method.}
}
@article{LIU2019102607,
title = {Representative discovery of structure cues for coronary heart disease recognition based on quality assessment},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102607},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102607},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302287},
author = {Miao Liu and Xiaoli Rong and Tiechao Jiang},
keywords = {Image recognition, Image quality assessment, Iterative reconstruction algorithm, Image match},
abstract = {Image recognition is an indispensable technique in computer vision, such as face recognition, vehicle recognition. It can be also widely used in medical research. Coronary heart disease recognition is still a big challenge due to artifacts or noise. In addition, recognizing coronary heart disease manually is a huge workload. So in this paper, we exploit representative structure cues for coronary heart disease recognition. More specifically, we first leverage Iterative Reconstruction (IR) algorithm to reduce the radiation dose during CT angiography acquisition. For structure cues discovery, we propose grid-based image segmentation algorithm, where each grid is represented by an according image quality score. High probability grid of disease cues corresponds to high quality score. Then, each training image is represented using quality score matrix. Afterward, we incorporate low-level features (grayscale and texture) with quality score matrix. Finally, image matching algorithm is proposed for structure cues discovery from test images. Experimental results demonstrate the effectiveness of our proposed method.}
}
@article{DONG2019102583,
title = {Dynamic gesture recognition by directional pulse coupled neural networks for human-robot interaction in real time},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102583},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102583},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302044},
author = {Jiaqi Dong and Zeyang Xia and Weiwu Yan and Qunfei Zhao},
keywords = {Human-robot interaction, Dynamic gesture recognition, Graph theory problem, Directional pulse coupled neuron network (DPCNN), Early recognition},
abstract = {Dynamic gesture recognition is a major topic for most real-time human-robot interaction applications. Recently, increasingly people have focused on dynamic gesture recognition based on 3D representation. In this paper, the gesture recognition is converted into the shortest path problem by transforming the feature matrix to an undirected graph and a novel dynamic gesture recognition algorithm of directional pulse coupled neuron network (DPCNN) is proposed for real time human-robot interactions. The DPCNN can select the firing direction by giving different excitations to neighbor neurons and reduce the effects of useless neurons. Furthermore, to reduce the recognition time, an early gesture recognition method based on the adaptive window is introduced to recognize the unfinished gestures. DPCNN reduces the computation time and achieves a high recognition rate on three public datasets compared with other algorithms which improves the efficiency of real-time dynamic gesture recognition and ensures a friendly experience for human-robot interactions.}
}
@article{SADEGHI2019152,
title = {Histogram distance metric learning for facial expression recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {152-165},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301592},
author = {Hamid Sadeghi and Abolghasem-A. Raie},
keywords = {Metric learning, Local metric learning, Chi-squared distance, Histogram classification, Facial expression recognition in the wild},
abstract = {Facial expression recognition is an interesting and challenging problem in computer vision. So far, much research has been performed in this area; however, facial expression recognition in uncontrolled conditions has remained an unresolved problem. The widely-used feature descriptors in computer vision are often histogram data. In this paper, a new metric learning method is presented for histogram data classification. In this method, chi-squared distance is appropriately modified for metric learning. Then, a convex cost function is proposed to use in metric learning optimization. Moreover, the proposed algorithm is redefined as Local Metric Learning for facial expression recognition problem. In this definition, the proposed metric learning method is applied locally on facial sub-regions. Experimental results on four histogram datasets (dslr, webcam, amazon, and caltech) as well as controlled and uncontrolled facial expression recognition datasets (CK+, SFEW, and RAF-DB) show that the proposed method has superior performance compared to the state-of-art methods.}
}
@article{HOU2019102599,
title = {Image anomaly detection for IoT equipment based on deep learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {64},
pages = {102599},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102599},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302202},
author = {Rui Hou and MingMing Pan and YunHao Zhao and Yang Yang},
keywords = {Operating environment monitoring, Image anomaly detection, Deep learning},
abstract = {Intelligent power grid systems is the trend of power development, since traditional methods of manually monitoring power equipment have been unable to meet the requirements of power systems. When an abnormal situation occurs in the operating environment, most monitoring devices cannot be quickly and accurately identified, which may have serious consequences. Aiming at the above problems, in this paper, we propose an anomaly detection algorithm for the monitoring environment of power IoT equipment operating environment based on deep learning from the perspective of personnel identification and fire smoke detection. The multi-stream CNN-based remote monitoring image personnel detection method and the deep convolutional neural network-based fire smoke detection method have achieved good results in personnel identification and fire smoke detection in the power equipment operating environment monitoring image, respectively. This provides a reference for monitoring image anomaly detection.}
}
@article{HAYAT2019295,
title = {Ghost-free multi exposure image fusion technique using dense SIFT descriptor and guided filter},
journal = {Journal of Visual Communication and Image Representation},
volume = {62},
pages = {295-308},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319301750},
author = {Naila Hayat and Muhammad Imran},
keywords = {Multi-exposure fusion, Dynamic range, Dense SIFT, Histogram, Quality measures, Pyramids},
abstract = {A ghost-free multi-exposure image fusion technique using the dense SIFT descriptor and the guided filter is proposed in this paper. The results suggest that the presented scheme produces high-quality images using ordinary cameras and that too without the ghosting artifact. To do so, the dense SIFT descriptor is used to extract the local contrast information from source images. Whereas, for the dynamic scenes, the histogram equalization and median filtering are used to calculate the color dissimilarity feature. Three weighting terms: local contrast, brightness, and color dissimilarity feature are used to estimate the initial weights. The estimated initial weights contain discontinuities. Therefore, the guided filter is used to remove the noise and discontinuity in initial weights. Finally, the fusion is performed using a pyramid decomposition method. Experimental results prove the superiority of the proposed technique over existing state-of-the-art methods in terms of both subjective and objective evaluation.}
}
@article{CHIN2019102595,
title = {Design of museum advertisement picture management system based on web},
journal = {Journal of Visual Communication and Image Representation},
volume = {63},
pages = {102595},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102595},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319302160},
author = {Shih-Hsien Chin and Cheng Chen and Po-Chang Ko and Shih-Yang Lin},
keywords = {The museum, Advertising pictures, C/S mode, The system design},
abstract = {With the rapid development of information technology and the popularity of computer and information technology, intelligent, digital has become a hot topic of discussion, industries in the daily work of the museum, there is a large amount of collection and collection information to process and maintenance staff to, so museum in various fields is also an urgent need to informationization, the digital revolution, allowing staff to liberate from the tedious work. One of the emphases of the daily work of museums is the management of the collections. Museum collection of computer management information system's purpose is to put as the basic object of information collection, through to the existing various CARDS, books and pictures, sounds, video, etc of traditional data processing and digital, set up a computer information management system, system including hardware and software of two parts. This paper starts from the background, elaborated the picture management system present situation, the existence superiority; Secondly, it is the system demand analysis, mainly introduced the system environment demand, performance demand and system role demand; Then is another part of the system outline design, including the overall system architecture design, use case design, database design; Finally is the system detailed design and the realization part, this part mainly is carries on the design to the system function module, and has carried on the programming realization.}
}