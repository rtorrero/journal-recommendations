@article{LIN2023103740,
title = {Action density based frame sampling for human action recognition in videos},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103740},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103740},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002607},
author = {Jie Lin and Zekun Mu and Tianqing Zhao and Hanlin Zhang and Xinyu Yang and Peng Zhao},
keywords = {Action recognition, Frame sampling, Action density, Neural network},
abstract = {In the action recognition, a proper frame sampling method can not only reduce redundant video information, but also improve the accuracy of action recognition. In this paper, an action density based non-isometric frame sampling method, namely NFS, is proposed to discard the redundant video information and sample the rational frames in videos for neural networks to achieve great accuracy on human action recognition, in which action density is introduced in our method to indicate the intensity of actions in videos. Particularly, the action density determination mechanism, focused-clips division mechanism, and reinforcement learning based frame sampling (RLFS) mechanism are proposed in NFS method. Via the evaluations with various neural networks and datasets, our results show that the proposed NFS method can achieve great effectiveness in frame sampling and can assist in achieving better accuracy on action recognition in comparison with existing methods.}
}
@article{XU2023103849,
title = {Person re-identification based on improved attention mechanism and global pooling method},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103849},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103849},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000998},
author = {Ruyu Xu and Yueyang Zheng and Xiaoming Wang and Dong Li},
keywords = {Person re-identification, Feature representation, Attention mechanism, Global pooling, Spatial transform},
abstract = {Deep network has become a new favorite for person re-identification (Re-ID), whose research focus is how to effectively extract the discriminative feature representation for pedestrians. In the paper, we propose a novel Re-ID network named as improved ReIDNet (iReIDNet), which can effectively extract the local and global multi-granular feature representations of pedestrians by a well-designed spatial feature transform and coordinate attention (SFTCA) mechanism together with improved global pooling (IGP) method. SFTCA utilizes channel adaptability and spatial location to infer a 2D attention map and can help iReIDNet to focus on the salient information contained in pedestrian images. IGP makes iReIDNet capture more effectively the global information of the whole human body. Besides, to boost the recognition accuracy, we develop a weighted joint loss to guide the training of iReIDNet. Comprehensive experiments demonstrate the availability and superiority of iReIDNet over other Re-ID methods. The code is available at https://github.com/XuRuyu66/ iReIDNet.}
}
@article{MALHOTRA2023103836,
title = {CB-D2RNet – An efficient context bridge network for glioma segmentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103836},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103836},
url = {https://www.sciencedirect.com/science/article/pii/S104732032300086X},
author = {Radhika Malhotra and Barjinder {Singh Saini} and Savita Gupta},
keywords = {MRI, Context bridge, Loss function, Dilated convolution},
abstract = {The recent automatic glioma segmentation and localization techniques obtained promising results, but there is much scope for improvement in execution complexity and segmentation efficiency. These methods often fail to pinpoint small and isolated target locations in necrotic and enhancing glioma sub-regions. Moreover, the computational complexity and number of model parameters utilized in these techniques are also high. To address such issues, a Context Bridge-Dense Dilated Residual Net (CB-D2RNet) is proposed in this paper which reflects the five novel contributions. Firstly, a Dense Dilated Convolutional (DDC) block is formed with four cascade branches to cope with large morphological differences in gliomas. Secondly, the skip connections in traditional UNet are redesigned to overcome the large contextual gap between encoder-decoder. Thirdly, a new loss function is proposed that handles unequal class distribution in gliomas and provides a regularization impact. Fourthly, the precise selection of dilation rates is made for each dilated convolutional block in the feature encoder to gather a more receptive view of complex and multiple tumor regions. Lastly, only a single convolutional operation is included in the feature encoder and decoder, unlike other state-of-the-art models. The experiments are conducted on BraTS 2018 and BraTS 2019 benchmarks, demonstrating that the proposed model performs competitively in all three glioma sub-regions. It achieves dice similarity coefficient for the whole tumor, tumor core, and enhancing tumor as 0.982, 0.987, and 0.976, respectively for the BraTS 2018 dataset, whereas 0.983, 0.989, and 0.977 respectively for the BraTS 2019 dataset. Besides this, the model uses only 6.7 million parameters, the lowest among other compared models.}
}
@article{NIU2023103757,
title = {Research on a face recognition algorithm based on 3D face data and 2D face image matching},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103757},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103757},
url = {https://www.sciencedirect.com/science/article/pii/S104732032300007X},
author = {Wenjie Niu and Yuankun Zhao and Zhiyan Yu and Yu Liu and Yu Gong},
keywords = {3D face recognition, Depth image, Deep learning, Data enhancement},
abstract = {Under the condition of weak light or no light, the recognition accuracy of the mature 2D face recognition technology decreases sharply. In this paper, a face recognition algorithm based on the matching of 3D face data and 2D face images is proposed. Firstly, 3D face data is reconstructed from the 2D face in the database based on the 3DMM algorithm, and the face depth image is obtained through orthogonal projection. Then, the average curvature map of the face depth image is used to enhance the data of the depth image. Finally, an improved residual neural network based on the depth image and curvature is designed to compare the scanned face with the face in the database. The method proposed in this paper is tested on the 3D face data in three public face datasets (Texas 3DFRD, FRGC v2.0, and Lock3DFace), and the recognition accuracy is 84.25%, 83.39%, and 78.24%, respectively.}
}
@article{CHEN2023103749,
title = {MTNet: Mutual tri-training network for unsupervised domain adaptation on person re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103749},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103749},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002693},
author = {Si Chen and Liuxiang Qiu and Zimin Tian and Yan Yan and Da-Han Wang and Shunzhi Zhu},
keywords = {Deep learning, Person re-identification, Domain adaptation, Mutual learning, Self-paced learning},
abstract = {The existing unsupervised domain adaptation (UDA) methods on person re-identification (re-ID) often employ clustering to assign pseudo labels for unlabeled target domain samples. However, it is difficult to give accurate pseudo labels to unlabeled samples in the clustering process. To solve this problem, we propose a novel mutual tri-training network, termed MTNet, for UDA person re-ID. The MTNet method can avoid noisy labels and enhance the complementarity of multiple branches by collaboratively training the three different branch networks. Specifically, the high-confidence pseudo labels are used to update each network branch according to the joint decisions of the other two branches. Moreover, inspired by self-paced learning, we employ a sample filtering scheme to feed unlabeled samples into the network from easy to hard, so as to avoid trapping in the local optimal solution. Extensive experiments show that the proposed method can achieve competitive performance compared with the state-of-the-art person re-ID methods.}
}
@article{JENY2023103737,
title = {Optimized video compression with residual split attention and swin-block artifact contraction},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103737},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103737},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002577},
author = {Afsana Ahsan Jeny and Md Baharul Islam},
keywords = {Video compression, Residual attention, Image reconstruction, Channel residual block, Artifact contraction},
abstract = {Research in video compression has seen significant advancement in the last several years. However, the existing deep learning-based algorithms continue to be plagued by erroneous motion compression and ineffective motion compensation architectures, resulting in compression errors with a lower rate–distortion trade-off. To overcome these challenges, we present an end-to-end purely deep learning-based video compression method through a set of primary operations (e.g., motion estimation, motion compression, motion compensation, residual compression, and artifact contraction) differently. A deep residual attention split (DRAS) block is introduced for motion compression networks to pay more attention to certain image regions to create more effective features for the decoder while boosting the rate–distortion optimization (RDO) efficiency. A channel residual block (CRB) is proposed in motion compensation to yield a more accurate predicted frame, potentially improving the residual frame. To mitigate the compression errors, an artifact contraction module (ACM) by residual swin convolution UNet block is included in this model to improve the reconstruction quality. To improve the final frame, a buffer is added to fine-tune the previous reference frames. These modules combine with a loss function by assessing the trade-off and enhancing the decoded video quality. A comprehensive ablation study demonstrates the effectiveness of the proposed blocks and modules for video compression. Experimental results show the competitive performance of the proposed method on four benchmark datasets.}
}
@article{WANG2023103758,
title = {Cross-view information interaction and feedback network for face hallucination},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103758},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103758},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000081},
author = {Huan Wang and Jianning Chi and Chengdong Wu and Xiaosheng Yu and Hao Wu},
keywords = {Image processing, Super-resolution, Face frontalization, Neural networks},
abstract = {Hallucinating a photo-realistic frontal face image from a low-resolution (LR) non-frontal face image is beneficial for a series of face-related applications. However, previous efforts either focus on super-resolving high-resolution (HR) face images from nearly frontal LR counterparts or frontalizing non-frontal HR faces. It is necessary to address all these challenges jointly for real-world face images in unconstrained environment. In this paper, we develop a novel Cross-view Information Interaction and Feedback Network (CVIFNet), which simultaneously handles the non-frontal LR face image super-resolution (SR) and frontalization in a unified framework and interacts them with each other to further improve their performance. Specifically, the CVIFNet is composed of two feedback sub-networks for frontal and profile face images. Considering the reliable correspondence between frontal and non-frontal face images can be crucial and contribute to face hallucination in a different manner, we design a cross-view information interaction module (CVIM) to aggregate HR representations of different views produced by the SR and frontalization processes to generate finer face hallucination results. Besides, since 3D rendered facial priors contain rich hierarchical features, such as low-level (e.g., sharp edge and illumination) and perception level (e.g., identity) information, we design an identity-preserving consistency loss based on 3D rendered facial priors, which can ensure that the high-frequency details of frontal face hallucination result are consistent with the profile. Extensive experiments demonstrate the effectiveness and advancement of CVIFNet.}
}
@article{JUNEJA2023103855,
title = {Aethra-net: Single image and video dehazing using autoencoder},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103855},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103855},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001050},
author = {Akshay Juneja and Vijay Kumar and Sunil Kumar Singla},
keywords = {Aethra-net, Image reconstruction, Transmission map, Vessel enhancement, Video dehazing},
abstract = {A fast and efficient video dehazing system with low computational complexity has a huge demand among drivers during hazy winter nights. There are only a few video dehazing models that exist in literature. Video dehazing requires the sequential extraction and processing of frames. The processed frames must be restored in the same sequence as the original video. However, the existing video dehazing algorithms suffer from color distortion due to the continuous processing of frames. They are not suitable for videos with dense haze. Furthermore, some dehazing systems require hardware, whereas the proposed model is completely software-based to reduce the computational costs. In this paper, an image and video dehazing system called Aethra-Net is developed. A gush enhancer-based autoencoder is modified to obtain the transmission map. The structure of gush enhancement module resembles the processing of light entering the human eye from different paths. The multiple blocks of Resnet-101 layers are employed to overcome vanishing gradient problem. The vessel enhancement filter is also incorporated to enhance the performance of the proposed system. The proposed model has a susceptibility to compute the dehazed images effectively. The proposed model is evaluated on various benchmark datasets and compared with the existing dehazing techniques. Experimental results reveal that the performance of Aethra-Net is found superior as compared to the existing dehazing models.}
}
@article{LYU2023103798,
title = {Reversible data hiding based on automatic contrast enhancement using histogram expansion},
journal = {Journal of Visual Communication and Image Representation},
volume = {92},
pages = {103798},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103798},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000482},
author = {WanLi Lyu and YaJie Yue and Zhaoxia Yin},
keywords = {Reversible data hiding, Histogram shifting, Automatic image enhancement, Reversible contrast enhancement, Histogram expansion},
abstract = {A lot of Reversible Data Hiding (RDH) methods aim to generate a stego image infinitely approaches the original image while the quality of the original image is leaved out of consideration. Juxtaposed with a plain image, a contrast enhanced version always improves the user experience significantly. Reversible Data Hiding with Contrast Enhancement (RDHCE) enhances the stego image contrast combined with its payloads and enables the cover image to be regained accurately after the payloads have been extracted. This study presents a novel RDHCE method using histogram expansion. First, a new local histogram selecting strategy is proposed to improve the contrast of the whole image. Meanwhile, the global average brightness is used as a reference to determine the shifting direction of the local histogram to prevent the image from being over-enhanced. Moreover, the contrast can be improved adaptively when a reasonable number of data is embedded at the selected embedding points. Experimental results show that, with a given payload, the proposed method achieves better contrast and maintains good visual quality compared with state-of-the-arts.}
}
@article{TONG2023103830,
title = {Rethinking PASCAL-VOC and MS-COCO dataset for small object detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103830},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103830},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000809},
author = {Kang Tong and Yiquan Wu},
keywords = {Data annotation, Small object detection, SDOD, Mini6K, Mini2022, Mini6KClean},
abstract = {The data and the algorithm are critical to deep learning-based small object detectors. In this paper, we rethink the PASCAL-VOC and MS-COCO dataset for small object detection. By visual analysis of the original annotations, we find that there are different labeling errors in these two datasets. To solve these problems, we build specific datasets, including SDOD, Mini6K, Mini2022 and Mini6KClean. The experimental results of several typical algorithms (e.g. SSD, YOLOv5, Faster RCNN and Deformable DETR) on the datasets show that data labeling errors (such as missing labels, category label errors, inappropriate labels) are another factor that affects the detection performance of small objects.}
}
@article{NASSERI2023103750,
title = {Online relational tracking with camera motion suppression},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103750},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103750},
url = {https://www.sciencedirect.com/science/article/pii/S104732032200270X},
author = {Mohammad Hossein Nasseri and Mohammadreza Babaee and Hadi Moradi and Reshad Hosseini},
keywords = {Multiple object tracking, Geometric interaction model, Camera motion suppression, Occlusion handling, Target re-identification, Cascade association},
abstract = {To overcome challenges in multiple-object tracking (MOT) tasks, recent algorithms use interaction cues alongside motion and appearance features. These algorithms use graph neural networks or transformers to extract interaction features that lead to high computation costs. In this paper, a novel interaction cue based on geometric features is presented aiming to detect occlusion and reidentify lost targets with low computational costs. Moreover, in the majority of algorithms, camera motion is considered negligible, which is a strong assumption that is not always true and can lead to identity (ID) switching or mismatching of targets. In this paper, a method for measuring camera motion is presented that efficiently reduces its effect on tracking. The proposed algorithm is evaluated on MOT17 and MOT20 datasets and achieves state-of-the-art performance on MOT17 with comparable results on MOT20. The code is also publicly available.11https://github.com/mhnasseri/for_tracking.}
}
@article{FA2023103826,
title = {Multi-scale spatial–temporal attention graph convolutional networks for driver fatigue detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103826},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103826},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000767},
author = {Shuxiang Fa and Xiaohui Yang and Shiyuan Han and Zhiquan Feng and Yuehui Chen},
keywords = {Driver fatigue detection, Driver behavior, Deep learning, Graph convolution networks},
abstract = {With the development of deep learning, fatigue detection technology for drivers has achieved remarkable achievements. Although the image-based approach achieves good accuracy, it inevitably leads to greater model complexity, which is unsuitable for mobile terminal devices. Luckily, human skeletal data significantly reduces the impact of noise and input data volume while retaining valid information, and it can better deal with real-world driving scenarios with the benefit of robustness in complex driving situations. This paper proposes a lightweight multi-scale spatio-temporal attention graph convolutional network (MS-STAGCN) to efficiently utilize skeleton data to identify driver states by aggregating locally and globally valid face information, which achieves good performance even for lightweight design. The experimental results show that the method achieves 92.4% accuracy on the NTHU-DDD dataset, which can be applied to fatigue detection tasks of the driver in real-world driving scenarios in the future.}
}
@article{KOMMANDURI2023103860,
title = {Bi-READ: Bi-Residual AutoEncoder based feature enhancement for video anomaly detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103860},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103860},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001104},
author = {Rangachary Kommanduri and Mrinmoy Ghorai},
keywords = {Anomaly, Residual connections, Optical flow, Unsupervised learning, Appearance consistency, Motion consistency},
abstract = {Video anomaly detection (VAD) refers to identifying abnormal events in the surveillance video. Typically, reconstruction based video anomaly detection techniques employ convolutional autoencoders with a limited number of layers, which extracts insufficient features leading to improper network training. To address this challenge, an end-to-end unsupervised feature enhancement network, namely Bi-Residual Convolutional AutoEncoder (Bi-ResCAE) has been proposed that can learn normal events with low reconstruction error and detect anomalies with high reconstruction error. The proposed Bi-ResCAE network incorporates long–short residual connections to enhance feature reusability and training stabilization. In addition, we propose to formulate a novel VAD model that can extract appearance and motion features by fusing both the Bi-ResCAE network and optical flow network in the objective function to recognize the anomalous object in the video. Extensive experiments on three benchmark datasets validate the effectiveness of the model. The proposed model achieves an AUC (Area Under the ROC Curve) of 84.7% on Ped1, 97.7% on Ped2, and 86.71% on the Avenue dataset. The results show that the Bi-READ performs better than state-of-the-art techniques.}
}
@article{SHARMA2023103774,
title = {Improved traffic sign recognition algorithm based on YOLOv4-tiny},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103774},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103774},
url = {https://www.sciencedirect.com/science/article/pii/S104732032300024X},
author = {Vipal Kumar Sharma and Pankaj Dhiman and Ranjeet Kumar Rout},
keywords = {Traffic sign, YOLO, Attention networks, Octave convolutions, Computer vision},
abstract = {This study offers an enhanced yolov4-tiny traffic sign identification method for easy deployment on mobile or embedded devices to address the difficulties of a high number of parameters, low recognition accuracy, and poor real-time performance of traffic sign recognition models in complex scenarios. The yolov4-tiny network serves as the model’s foundation. To begin, Octave Convolution is incorporated into the backbone network to eliminate low-frequency feature redundancy, lowering the number of parameters in the model and enhancing computational efficiency. Second, the convolutional block attention module is employed to improve the recognition accuracy of small and medium-sized targets by strengthening the weights of traffic sign regions and suppressing the weights of invalid features. Finally, in the feature fusion stage, the Feature Pyramid Networks structure is replaced with the Simplified Path Aggregation Network structure to improve the fusing of shallow feature information with deep semantic knowledge and lower the miss detection rate even more On the TT100K data set as well as on CCTSDB dataset, the experimental results suggest that our technique can achieve good recognition performance. With a 16MB model size, our solution improves the mean average precision by 3.5 percent and the Frame Per Second by 12.5 f/s when compared to the yolov4-tiny algorithm. Our method outperforms yolov4-tiny in terms of recognition accuracy and detection speed, and it can easily meet the real-time requirements for traffic sign recognition.}
}
@article{MEMON2023103748,
title = {AMSFF-Net: Attention-Based Multi-Stream Feature Fusion Network for Single Image Dehazing},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103748},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103748},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002681},
author = {Sanaullah Memon and Rafaqat Hussain Arain and Ghulam Ali Mallah},
keywords = {Image dehazing, Channel attention, Pixel attention, Mixed convolution attention, Residual dense block, Feature fusion},
abstract = {In this paper, an end-to-end convolutional neural network is proposed to recover haze-free image named as Attention-Based Multi-Stream Feature Fusion Network (AMSFF-Net). The encoder-decoder network structure is used to construct the network. An encoder generates features at three resolution levels. The multi-stream features are extracted using residual dense blocks and fused by feature fusion blocks. AMSFF-Net has ability to pay more attention to informative features at different resolution levels using pixel attention mechanism. A sharp image can be recovered by the good kernel estimation. Further, AMSFF-Net has ability to capture semantic and sharp textural details from the extracted features and retain high-quality image from coarse-to-fine using mixed-convolution attention mechanism at decoder. The skip connections decrease the loss of image details from the larger receptive fields. Moreover, deep semantic loss function emphasizes more semantic information in deep features. Experimental findings prove that the proposed method outperforms in synthetic and real-world images.}
}
@article{LIANG2023103831,
title = {Fast saliency prediction based on multi-channels activation optimization},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103831},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103831},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000810},
author = {Song Liang and Ruihang Liu and Jiansheng Qian},
keywords = {Saliency prediction, Convolutional neural networks, Human eye fixations, Deep learning},
abstract = {The saliency prediction precision has improved rapidly with the development of deep learning technology, but the inference speed is slow due to the continuous deepening of networks. Hence, this paper proposes a fast saliency prediction model. Concretely, the siamese network backbone based on tailored EfficientNetV2 accelerates the inference speed while maintaining high performance. The shared parameters strategy further curbs parameter growth. Furthermore, we add multi-channel activation maps to optimize the fine features considering different channels and low-level visual features, which improves the interpretability of the model. Extensive experiments show that the proposed model achieves competitive performance on the standard benchmark datasets, and prove the effectiveness of our method in striking a balance between prediction accuracy and inference speed. Moreover, the small model size allows our method to be applied in edge devices. The code is available at: https://github.com/lscumt/fast-fixation-prediction.}
}
@article{WANG2023103856,
title = {A Dual-Decoding branch U-shaped semantic segmentation network combining Transformer attention with Decoder: DBUNet},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103856},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103856},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001062},
author = {Yuefei Wang and Xi Yu and Xiaoyan Guo and Xilei Wang and Yuanhong Wei and Shijie Zeng},
keywords = {Semantic Segmentation, U-Shaped Network, Transformer ViT, Medical Image},
abstract = {Semantic Segmentation is an extremely important medical image auxiliary analysis method. However, existing networks have the following problems: 1) The amount of feature information of Encoder and Decoder is not equal under multi-branch architecture; 2) The direct processing of the original image by ViT Encoder is not sufficient; 3) Multi-channel features are too independent and lack of fusion. Combined with the ViT Encoder framework, this study proposes a 'Single Encoder – Double Decoder' structure: DBUNet. Firstly, ViT Encoder is employed as a part of the Decoder branches to enhance the shallow features. Then, a polarization amplification of channel weights is proposed and placed in front of the ViT Encoder module to achieve early image processing. Finally, a Bottleneck for feature fusion is proposed to solve the problem of channel independence. The comprehensive verification of 13 comparative networks in three aspects, combined with ablation experiments, jointly proves the superiority of DBUNet.}
}
@article{CHANG2023103736,
title = {SE-PSNet: Silhouette-based Enhancement Feature for Panoptic Segmentation Network},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103736},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103736},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002565},
author = {Shuo-En Chang and Yi Chen and Yi-Cheng Yang and En-Ting Lin and Pei-Yung Hsiao and Li-Chen Fu},
keywords = {Deep learning, Panoptic segmentation, Instance segmentation, Silhouette, confidence score},
abstract = {In this work, we propose a panoptic segmentation model that integrates bottom-up and top-down methods. Our framework is designed to guarantee both the performance and the inference speed. We also focus on improving the quality of semantic and instance masks. The proposed auxiliary task with the silhouette-based enhanced features can help the model improve the prediction quality of mask contours. Additionally, we introduce a new mask quality score intending to solve the occlusion problem. The model has less chance of ignoring small objects, which often have lower confidence scores than larger objects behind them. The results show that the proposed mask quality score can better distinguish the priority of objects when the occlusion occurs. We demonstrate the results of our work on two datasets: the COCO dataset and the CityScapes dataset. Via our approach, we obtained competitive results with fast inference time.}
}
@article{DASILVEIRA2023103775,
title = {Omnidirectional 2.5D representation for COVID-19 diagnosis using chest CTs},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103775},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103775},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000251},
author = {Thiago L.T. {da Silveira} and Paulo G.L. Pinto and Thiago S. Lermen and Cláudio R. Jung},
keywords = {2.5D representation, COVID-19 diagnosis, Ground-glass opacity, Omnidirectional imaging},
abstract = {The Coronavirus Disease 2019 (COVID-19) has drastically overwhelmed most countries in the last two years, and image-based approaches using computerized tomography (CT) have been used to identify pulmonary infections. Recent methods based on deep learning either require time-consuming per-slice annotations (2D) or are highly data- and hardware-demanding (3D). This work proposes a novel omnidirectional 2.5D representation of volumetric chest CTs that allows exploring efficient 2D deep learning architectures while requiring volume-level annotations only. Our learning approach uses a siamese feature extraction backbone applied to each lung. It combines these features into a classification head that explores a novel combination of Squeeze-and-Excite strategies with Class Activation Maps. We experimented with public and in-house datasets and compared our results with state-of-the-art techniques. Our analyses show that our method provides better or comparable prediction quality and accurately distinguishes COVID-19 infections from other kinds of pneumonia and healthy lungs.}
}
@article{ZHONG2023103747,
title = {Unsupervised self-attention lightweight photo-to-sketch synthesis with feature maps},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103747},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103747},
url = {https://www.sciencedirect.com/science/article/pii/S104732032200267X},
author = {Kunru Zhong and Zhenxue Chen and Chengyun Liu and Q. M. Jonathan Wu and Shuchao Duan},
keywords = {Photo-to-sketch synthesis, Unsupervised learning, Self-attention mechanism, Feature map},
abstract = {Face-sketch synthesis is important for gaining a clear portrait photo of suspects when solving crimes. Recent research has made a great process in self-attention generative adversarial networks. We propose a method of unsupervised learning in the synthesis of face sketch-to-photo using a new attention module. The method of processing on a small reference set of photo-sketch pairs adds to the attention module, a focus on the regions distinguishing photos from sketches on the basis of the feature maps obtained by the auxiliary classifier. Unlike previous attention-based methods, which cannot handle the geometric changes between domains, our model can translate images requiring holistic changes. At the same time, we reduce the layers of the discriminator according to different residual layers to optimize our network. With the proposed approach, we can train our networks using a small reference set of photo-sketch pairs together with a large number of face-photo datasets and more distinguishing facial-feature regions in the self-attention model. Experiments have shown the superiority of the proposed method to existing face sketch-to-photo synthesis models using fixed network architectures and hyper-parameters.}
}
@article{WANG2023103799,
title = {Virtual view synthesis using joint information from multi-view},
journal = {Journal of Visual Communication and Image Representation},
volume = {92},
pages = {103799},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103799},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000494},
author = {Yifan Wang and Fuzheng Yang and Ying Chen and Wei Zhang},
keywords = {Virtual view synthesis, Multi-view, Virtual reality(VR), Interpolation, Panoramic video},
abstract = {Immersive media has attracted widespread attention with the development of virtual reality. Three Degree of Freedom Plus media greatly enhances the user experience by allowing users’ head motion and viewpoint switching within a certain range. Due to the limitation of panoramic video acquisition and transmission, it is impossible to obtain videos from any viewpoint directly. Virtual view synthesis is the general solution to this problem. However, existing algorithms do not adequately consider the pixel correlation between multiple views. Thus, we propose a virtual view synthesis algorithm using joint information from multi-view panoramic videos to further explore the pixel correlation. Specifically, sub-pixels from different reference views in the virtual view are obtained by performing multi-view three-dimensional image warping. Dedicated area division and interpolation methods are then designed to improve the synthesized quality. Experimental results show that the proposed algorithm outperforms the state-of-the-art virtual view synthesis algorithms in performance and efficiency.}
}
@article{CAI2023103835,
title = {UAV image stitching by estimating orthograph with RGB cameras},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103835},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103835},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000858},
author = {Wenxiao Cai and Songlin Du and Wankou Yang},
keywords = {Natural image stitching, Image alignment, Orthographic projection, Parallax tolerant, Unified perspective},
abstract = {In the field of image stitching, cases with large camera optical center movement and large parallax have been the virgin territory of research. The goal of image stitching is to overcome the parallax and stitch a natural image. We look into this problem in the context of ultra-low altitude flight of a UAV. We model the 3D world in this scenario and quickly estimate orthographic projection by pairs of homography matrices. Our stitching method can achieve precise alignment since it takes parallax well into consideration. The stitching results are natural and the extra time consumed is short.}
}
@article{WANG2023103822,
title = {Uncertainty-guided joint attention and contextual relation network for person re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103822},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103822},
url = {https://www.sciencedirect.com/science/article/pii/S104732032300072X},
author = {Dengwen Wang and Yanbing Chen and Wangmeng Wang and Zhixin Tie and Xian Fang and Wei Ke},
keywords = {Person re-identification, Uncertainty-guided joint attention, Contextual relation network, Relation between features, Attention mechanism},
abstract = {Due to the influence of factors such as camera angle and pose changes, some salient local features are often suppressed in person re-identification tasks. Moreover, many existing person re-identification methods do not consider the relation between features. To address these issues, this paper proposes two novel approaches: (1) To solve the problem of being confused and misidentified when local features of different individuals have similar attributes, we design a contextual relation network that focuses on establishing the relationship between local features and contextual features, so that all local features of the same person both contain contextual information. (2) To fully and correctly express key local features, we propose an uncertainty-guided joint attention module. The module focuses on the joint representation of individual pixels and local spatial features to enhance the credibility of local features. Finally, our method achieves competitive performance on four widely recognized datasets compared with state-of-the-art methods.}
}
@article{CHEN2023103784,
title = {Image fusion based on discrete Chebyshev moments},
journal = {Journal of Visual Communication and Image Representation},
volume = {92},
pages = {103784},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103784},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000342},
author = {Xiaoxuan Chen and Shuwen Xu and Shaohai Hu and Xiaole Ma},
keywords = {Image fusion, Attention mechanism, Chebyshev moments, Average gradient},
abstract = {Though deep learning-based methods have demonstrated strong capabilities on image fusion, they usually improve the fusion performance by increasing the width and depth of the network, increasing the computational effort and being unsuitable for industrial applications. In this paper, an end-to-end network based on fixed convolution module of discrete Chebyshev moments is proposed, which does not need any pre- or post-processing. The proposed network is roughly composed of three parts: feature extraction module, fusion module and feature reconstruction module. In the feature extraction module, a novel fixed convolution module based on discrete Chebyshev moments is proposed to obtain different frequency components in a short time. To improve the image sharpness and fuse more details, a spatial attention mechanism based on average gradient is proposed in fusion module. Extensive results demonstrate that the proposed network can achieve remarkable fusion performance, high time efficiency and strong generalization ability.}
}
@article{HUANG2023103808,
title = {Robust reversible image watermarking scheme based on spread spectrum},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103808},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103808},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000585},
author = {Ziquan Huang and Bingwen Feng and Shijun Xiang},
keywords = {Robust reversible watermarking, Spread-spectrum code, Embedding amplitude, Linear correlation},
abstract = {Robust reversible watermarking can provide robustness against various attacks besides the ability to recover the cover image. However, robustness and reversibility are somewhat separate in many schemes. The original cover image cannot be recovered even if the watermarked image suffers from a tiny distortion. This paper presents a new robust reversible watermarking scheme by exploring the reversibility of spread-spectrum codes. Watermark bits are embedded by a suggested adaptive spread-spectrum code. The embedding amplitude used in the algorithms is determined by quantizing the source interference of the cover. The proposed scheme is robust to various attacks. Furthermore, since the embedding amplitude is available at the receiver, the original image can be recovered losslessly when there is no attack. Even in the presence of attacks, the original cover images can still be partially recovered. Experimental results demonstrate that the proposed scheme performs well on robustness and watermarked image quality, and provide extra reversibility that resists image distortions.}
}
@article{HUTAMAPUTRA2023103756,
title = {Eyes gaze detection based on multiprocess of ratio parameters for smart wheelchair menu selection in different screen size},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103756},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103756},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000068},
author = {William Hutamaputra and Fitri Utaminingrum and Agung Setia Budi and Kohichi Ogata},
keywords = {Eye gaze, Face detection, Facial landmark, Haar cascade, Midpoint, Threshold},
abstract = {Smart wheelchairs support paralysis disability people, especially paralysis of the hands and feet. Paralysis disability cannot operate menu selection with a touch screen. This research proposes a menu selection to select features in a smart wheelchair using the eye’s gaze and execute the menu by blinking the left eye. Multiprocess of ratio parameter is the methodology used and expanded in this research. The proposed method compared the ratio between the midpoint on the iris contour and registered midpoint on the selection step. This research is conducted under two conditions: the screen size being started in 14 and 17 inches. This research resulted in 91.48% and 90.68% accuracy in 14 and 17-inch screens for users without glasses. Meanwhile, 89.55% and 86.99% were obtained for wearing glasses users. The experiments were conducted where the position of a user is 30–39, 40–49, and 50–59 cm from the camera.}
}
@article{SAVNER2023103853,
title = {CrowdFormer: Weakly-supervised crowd counting with improved generalizability},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103853},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103853},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001037},
author = {Siddharth Singh Savner and Vivek Kanhangad},
keywords = {Crowd counting, Vision transformers, Weakly-supervised method, Generalizability},
abstract = {Convolutional neural networks (CNNs) have dominated the field of computer vision for nearly a decade. However, due to their limited receptive field, CNNs fail to model the global context. On the other hand, transformers, an attention-based architecture, can model the global context easily. Despite this, there are limited studies that investigate the effectiveness of transformers in crowd counting. In addition, the majority of the existing crowd-counting methods are based on the regression of density maps which requires point-level annotation of each person present in the scene. This annotation task is laborious and also error-prone. This has led to an increased focus on weakly-supervised crowd-counting methods, which require only count-level annotations. In this paper, we propose a weakly-supervised method for crowd counting using a pyramid vision transformer. We have conducted extensive evaluations to validate the effectiveness of the proposed method. Our method achieves state-of-the-art performance. More importantly, it shows remarkable generalizability.}
}
@article{BRAI2023103819,
title = {Low calculation cost of HEVC coding unit size based on spatial homogeneity detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103819},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103819},
url = {https://www.sciencedirect.com/science/article/pii/S104732032300069X},
author = {Radhia. Brai and Amara Bekhouch and Noureddine Doghmane and Saliha Harize and Nasreddine Kouadria},
keywords = {CU size decision, Complexity reduction, Spatial homogeneity},
abstract = {For the same video quality, HEVC gives 25% to 50% bitrate savings, compared to its predecessor the Advanced Video Coding H.264 and thus supports resolutions up to 8 K UHD. However, the reduction in bitrates provided by the HEVC leads to an increase in the computational cost of the encoding operation. This complexity can become a true handicap especially for real-time video streaming and also for VANET (Vehicular Ad-Hoc Network) applications such as traffic safety and Video surveillance. The improvement in the bitrates and also the increase in the calculation cost are due to the use of large and multi-sized coding, prediction and transform blocks. Indeed, the H264 coder is based on structure macroblocks with sizes 4 × 4, 8 × 8 and 16 × 16, while H.265 depends on Coding Tree Units (CTUs), CTUs select sizes 4 × 4, 8 × 8, 16 × 16, 32 × 32 and 64 × 64 blocks. This paper proposes a fast CU (Coding Unit) size decision method to reduce the HEVC calculation cost based on spatial homogeneity. Compared with the HM16.13 benchmark test model, the average coding time is reduced by around 40% for CIF / QCIF video sequences, 35% to 43% for class A, B and C test sequences. These important reductions in coding time are obtained with negligible loss of quality and an average increase in bitrates which does not exceed 0.89% for the three configuration modes (All intra, Random Access and Low Delay).}
}
@article{LI2023103800,
title = {TransCAM: Transformer attention-based CAM refinement for Weakly supervised semantic segmentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {92},
pages = {103800},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103800},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000500},
author = {Ruiwen Li and Zheda Mai and Zhibo Zhang and Jongseong Jang and Scott Sanner},
keywords = {Weakly supervised learning, Semantic segmentation, Vision transformer},
abstract = {Weakly supervised semantic segmentation (WSSS) with only image-level supervision is a challenging task. Most existing methods exploit Class Activation Maps (CAM) to generate pixel-level pseudo labels for supervised training. However, due to the local receptive field of Convolution Neural Networks (CNN), CAM applied to CNNs often suffers from partial activation — highlighting the most discriminative part instead of the entire object area. In order to capture both local features and global representations, the Conformer has been proposed to combine a visual transformer branch with a CNN branch. In this paper, we propose TransCAM, a Conformer-based solution to WSSS that explicitly leverages the attention weights from the transformer branch of the Conformer to refine the CAM generated from the CNN branch. TransCAM is motivated by our observation that attention weights from shallow transformer blocks are able to capture low-level spatial feature similarities while attention weights from deep transformer blocks capture high-level semantic context. Despite its simplicity, TransCAM achieves competitive performance of 69.3% and 69.6% on the respective PASCAL VOC 2012 validation and test sets, showing the effectiveness of transformer attention-based refinement of CAM for WSSS.}
}
@article{YE2023103824,
title = {Human object interaction detection based on feature optimization and key human-object enhancement},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103824},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103824},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000743},
author = {Qing Ye and Xikun Wang and Rui Li and Yongmei Zhang},
keywords = {Human object interaction detection, FOFR-CNN, Key human-object enhancement, Graph convolutional network},
abstract = {Aiming at the problem of unclear or missing human object interaction behavior objects in complex background, we propose a human object interaction detection algorithm based on feature optimization and key human-object enhancement. In order to solve the problem of missing human behavior objects, we propose Feature Optimized Faster Region Convolutional Neural Network (FOFR-CNN). FOFR-CNN is an object detection network optimized by multi-scale feature optimization algorithm, taking into account both image semantics and image structure. In order to reduce the interference of complex background, we propose a Key Human-Object Enhancement Network. The network uses an instance-based method to enhance the features of interactive objects. In order to enrich the interaction information, we use the graph convolutional network. Experimental results on HICO-DET, V-COCO and HOI-A datasets show that the proposed algorithm has significantly improved accuracy and multi-scale object detection ability compared with other human object interaction algorithms.}
}
@article{PANG2023103772,
title = {Feature generation based on relation learning and image partition for occluded person re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103772},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103772},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000226},
author = {Yunxiao Pang and Huaxiang Zhang and Lei Zhu and Dongmei Liu and Li Liu},
keywords = {Occluded person re-identification, Graph convolutional network, Partition},
abstract = {In order to solve the challenging tasks of person re-identification(Re-ID) in occluded scenarios, we propose a novel approach which divides local units by forming high-level semantic information of pedestrians and generates features of occluded parts. The approach uses CNN and pose estimation to extract the feature map and key points, and a graph convolutional network to learn the relation of key points. Specifically, we design a Generating Local Part (GLP) module to divide the feature map into different units. Based on different occluded conditions, the partition mode of GLP has high flexibility and variability. The features of the non-occluded parts are clustered into an intermediate node, and then the spatially correlated features of the occluded parts are generated according to the de-clustering operation. We conduct experiments on both the occluded and the holistic datasets to demonstrate its effectiveness.}
}
@article{YU2023103848,
title = {A hybrid indicator for realistic blurred image quality assessment},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103848},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103848},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000986},
author = {Shaode Yu and Jiayi Wang and Jiacheng Gu and Mingxue Jin and Yunling Ma and Lijuan Yang and Jianguang Li},
keywords = {Image quality assessment, Realistic blur, Feature selection, Machine learning},
abstract = {Blurriness is annoying yet common in digital images. Many sharpness assessment indicators using handcrafted features achieve impressive results on synthesized blurring images, while room exists for improvement on realistic datasets. This study presents a hybrid indicator in which no-reference indicators perform as mid-level feature extractors and their outputs are selected using a consensus-based method for discriminative ones. On realistic image datasets, 15 off-the-shelf indicators are explored, and experimental results reveal that the hybrid indicator obtains considerable improvement (≥ 21.5%, BID2011; ≥ 11.6%, CID2013; ≥ 7.1%, LIVE Challenge; and ≥ 11.6%, KonIQ-10k) compared to the baseline indicator. Meanwhile, the indicator requires more features for representation of diverse distortions (CID2013, LIVE Challenge and KonIQ-10k) than different blurriness (BID2011). Four regression models are investigated, and fitting neural network leads to overall better results. Realistic image quality assessment is challenging, fusion of existing indicators improves the performance, while to develop advanced indicators remains desirable.}
}
@article{LIU2023103796,
title = {Boosting semantic segmentation via feature enhancement},
journal = {Journal of Visual Communication and Image Representation},
volume = {92},
pages = {103796},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103796},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000469},
author = {Zhi Liu and Yi Zhang and Xiaojie Guo},
keywords = {Semantic segmentation, Feature enhancement, Deep learning},
abstract = {Semantic segmentation aims to map each pixel of an image into its corresponding semantic label. Most existing methods either mainly concentrate on high-level features or simple combination of low-level and high-level features from backbone convolutional networks, which may weaken or even ignore the compensation between different levels. To effectively take advantages from both shallow (textural) and deep (semantic) features, this paper proposes a novel plug-and-play module, namely feature enhancement module (FEM). The proposed FEM first uses an information extractor to extract the desired details or semantics from different stages, and then enhances target features by taking in the extracted message. Two types of FEM, i.e., detail FEM and semantic FEM, can be customized. Concretely, the former type strengthens textural information to protect key but tiny/low-contrast details from suppression/removal, while the other one highlights structural information to boost segmentation performance. By equipping a given backbone network with FEMs, there might contain two information flows, i.e., detail flow and semantic flow. Extensive experiments on the Cityscapes, ADE20K and PASCAL Context datasets are conducted to validate the effectiveness of our design. The code has been released at https://github.com/SuperZ-Liu/FENet.}
}
@article{JING2023103735,
title = {Edge-aware object pixel-level representation tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103735},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103735},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002553},
author = {Peiguang Jing and Zijian Huang and Jing Liu and Yating Wang and Jiexiao Yu},
keywords = {Object tracking, Edge-aware, Segmentation mask, Real-time},
abstract = {Recently, there has been a trend in tracking to use more refined segmentation mask instead of coarse bounding box to represent the target object. Some trackers proposed segmentation branches based on the tracking framework and maintain real-time speed. However, those trackers use a simple FCNs structure and lack of the edge information modeling. This makes performance quite unsatisfactory. In this paper, we propose an edge-aware segmentation network, which uses the complementarity between target information and edge information to provide a more refined representation of the target. Firstly, We use the high-level features of the tracking backbone network and the correlation features of the classification branch of the tracking framework to fuse, and use the target edge and target segmentation mask for simultaneous supervision to obtain an optimized high-level feature with rough edge information and target information. Secondly, we use the optimized high-level features to guide the low-level features of the tracking backbone network to generate more refined edge features. Finally, we use the refined edge features to fuse with the target features of each layer to generate the final mask. Our approach has achieved leading performance on recent pixel-wise object tracking benchmark VOT2020 and segmentation datasets DAVIS2016 and DAVIS2017 while running on 47 fps. Code is available at https://github.com/TJUMMG/EATtracker.}
}
@article{CAO2023103837,
title = {Screen-shooting resistant image watermarking based on lightweight neural network in frequency domain},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103837},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103837},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000871},
author = {Fang Cao and Tianjun Wang and Daidou Guo and Jian Li and Chuan Qin},
keywords = {Robust watermarking, Screen-shooting, Lightweight neural network, Frequency domain, Efficiency},
abstract = {Currently, digital mobile devices, especially smartphones, can be used to acquire information conveniently through photograph taking. To protect information security in this case, we propose an efficient screen-shooting resistant watermarking scheme via deep neural network (DNN) in the frequency domain to achieve additional information embedding and source tracing. Specifically, we enhance the imperceptibility of watermarked images and the robustness against various attacks in real scene by computing the residual watermark message and encoding it with the original image using a lightweight neural network in the DCT domain. In addition, a noise layer is designed to simulate the photometric and radiometric effects of screen-shooting transfer. During the training process, the enhancing network is used to highlight the coding features of distorted images and improve the accuracy of extracted watermark message. Experimental results demonstrate that our scheme not only effectively ensures the balance between the imperceptibility of watermark embedding and the robustness of watermark extraction, but also significantly improves computational efficiency compared with some state-of-the-art schemes.}
}
@article{SABAT2023103785,
title = {A computationally efficient moving object detection technique using tensor QR decomposition based TRPCA framework},
journal = {Journal of Visual Communication and Image Representation},
volume = {92},
pages = {103785},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103785},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000354},
author = {Neelesh Sabat and Subodh Raj M.S. and Sudhish N. George and Sunil Kumar T.K.},
keywords = {Moving object detection, Tensor QR decomposition, Robust principal component analysis, Half thresholding},
abstract = {Advancements in high-quality video cameras and the consequent capture of minute details of the scene have led the field of computer vision to remarkable heights. This paper develops a tensor QR decomposition-based approach for Moving Object Detection (MOD), which aims to reduce the computational complexity without disturbing the structural framework of the input video frames. The increased performance and efficiency of the proposed method lie in the usage of tensor QR decomposition along with l2,1 norm and l1/2 norm. It is designed on top of a tensor-based Robust Principal Component Analysis (TRPCA) framework. In addition, this work safeguards the variation along the spatio-temporal directions with the effective use of Tensor Total Variation (TTV) regularization. The results and the analysis prove that the proposed method improves the F-measure by 15%–45% and reduces the computational complexity by 75%–85% with respect to the counterparts.}
}
@article{MUHAMMED2023103854,
title = {A secure fingerprint template generation mechanism using visual secret sharing with inverse halftoning},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103854},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103854},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001049},
author = {Ajnas Muhammed and Alwyn Roshan Pais},
keywords = {Fingerprint template protection, Visual secret sharing, Minutiae, Security, Super-resolution, Inverse halftoning},
abstract = {Fingerprints are the most popular and widely practiced biometric trait for human recognition and authentication. Due to the wide approval, reliable fingerprint template generation and secure saving of the generated templates are highly vital. Since fingers are permanently connected to the human body, loss of fingerprint data is irreversible. Cancelable fingerprint templates are used to overcome this problem. This paper introduces a novel cancelable fingerprint template generation mechanism using Visual Secret Sharing (VSS), data embedding, inverse halftoning, and super-resolution. During the fingerprint template generation, VSS shares with some hidden information are formulated as the secure cancelable template. Before authentication, the secret fingerprint image is reconstructed back from the VSS shares. The experimental results show that the proposed cancelable templates are simple, secure, and fulfill all the properties of the ideal cancelable templates, such as security, accuracy, non-invertibility, diversity, and revocability. The experimental analysis shows that the reconstructed fingerprint images are similar to the original fingerprints in terms of visual parameters and matching error rates.}
}
@article{FADL2023103801,
title = {Automatic fake document identification and localization using DE-Net and color-based features of foreign inks},
journal = {Journal of Visual Communication and Image Representation},
volume = {92},
pages = {103801},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103801},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000512},
author = {Sondos Fadl and Khalid M. Hosny and Mohamed Hammad},
keywords = {Handwriting forgery detection, Addition, Alteration, Document examination, Forged document, CNN},
abstract = {Document examination is a vital mission for revealing illegal modifications that assist in the detection and resolution of criminal acts. Addition and alteration are more frequently used in handwritten documents. However, most of the documents have been modified with similar inks, and it is tough to detect or observe them with human eyes. As a result, there is a need for methods to automatically detect handwriting forgery to reach an accurate detection efficiently. In this paper, a novel and efficient method is proposed for automatically detecting altered handwritten documents and locating the fake part. Therefore, DE-Net is proposed to identify the forged document using a digitally scanned version of the document. Unlike the existing methods, a further localization schema is applied to locate the forged parts in the candidate forged document accurately. Where each forged document is segmented into objects. Color histograms of R, G, and B channels are used to generate a fused feature vector for each object. Then a structural similarity index (SSIM) is applied to detect the lower similarity parts as forged. The experimental results demonstrate that the proposed method can identify and localize foreign ink in handwritten documents with high performance.}
}
@article{SHAO2023103825,
title = {Real-time and robust visual tracking with scene-perceptual memory},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103825},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103825},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000755},
author = {Yanhua Shao and Xiao Zhang and Kuisheng Liao and Hongyu Chu},
keywords = {Visual tracking, Correlation filter, Scene-perceptual memory, Unmanned aerial vehicle, Aerial object tracking},
abstract = {Unmanned aerial vehicle (UAV) based aerial visual tracking is one of the research hotspots in computer vision. However, the mainstream trackers for UAV still have two shortcomings: (1) the accuracy of correlation filter tracker is greatly improved with more complex model, it impedes accuracy-speed trade-off. (2) object occlusion and camera motion in the aerial tracking scene also seriously restrict the application of aerial tracking. To address these problems, and inspired by AutoTrack tracker, we propose an aerial correlation filtering tracker based on scene-perceptual memory, Fast-AutoTrack. Firstly, to perceive and judge tracking anomalies, such as object occlusion and camera motion, inspired by the peak sidelobe ratio and AutoTrack, a confidence score is designed by perceiving and remembering the changing trend of the confidence and the local historical confidence. Secondly, after tracking anomaly occurring, several search regions are predicted based on the local object motion trend and the Spatio-temporal context information for object re-detection. Finally, to accelerate the model updating, the perceptual hashing algorithm (PHA) is used to obtain the similarity of the search regions between two adjacent frames. On typical aerial tracking datasets UAVDT, UAV123@10fps, and DTB70, Fast-AutoTrack run 71.4% faster than AutoTrack with almost equal accuracy and show favorable accuracy-speed trade-off.}
}
@article{SONG2023103773,
title = {Multi-scale Superpixel based Hierarchical Attention model for brain CT classification},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103773},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103773},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000238},
author = {Xiao Song and Xiaodan Zhang and Junzhong Ji and Ying Liu},
keywords = {Brian CT classification, Medical image processing, Multi-scale superpixel, Hierarchical attention},
abstract = {Brain CT image classification is critical for assisting brain disease diagnosis. The brain CT images contain much noisy information, and the lesions are unstable in shape and location, making the classification task more difficult when using conventional CNN models. In this paper, we propose a novel Multi-scale Superpixel based Hierarchical Attention (MSHA) model for brain CT classification by introducing the multi-scale superpixels to a hierarchical fusion structure to remove noise and help the model focus on the lesion areas. MSHA contains three modules: (1) a Semantic-level Information Extractor that extracts appearance and geometry information based on the superpixel of the image, (2) a Mixed Multi-head Attention module that obtains the mixed attention features from the semantic-level information, and (3) a Hierarchical Fusion Structure that fuses the multi-scale attention features from coarse to fine. Experiments on the brain CT dataset demonstrate the effectiveness of the proposed model.}
}
@article{QIAN2023103797,
title = {Deep interactive image segmentation based on region and Boundary-click guidance},
journal = {Journal of Visual Communication and Image Representation},
volume = {92},
pages = {103797},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103797},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000470},
author = {Yuxiang Qian and Yang Xue and Tao Wang},
keywords = {Interactive segmentation, Deep learning, Interaction strategy, Information diffusion},
abstract = {In interactive image segmentation, the target object of interest can be extracted based on the guidance of user interactions. One of the main goals in this task is to reduce the user interaction burden and ensure satisfactory segmentation with as few interactions as possible. Thanks to the development of deep learning technology, neural network-based interactive approaches have significantly improved the segmentation performance through powerful feature representation. Only limited point (click) interaction is required for the user to complete the segmentation. This paper mainly follows the deep learning-based interactive segmentation methods and explores more efficient interaction strategies and effective segmentation models. We further simplify user interaction to two clicks, where the first click is utilized to select the target region and the other aims to determine the target boundary. Based on the region and boundary clicks, an interactive two-stream network structure is naturally derived to learn the region and boundary features of interest. Furthermore, we also construct an information diffusion module to better propagate the region and boundary-click labels, which helps to enhance the similarity within the region and the discrimination between boundaries. Vast experiments on the popular GrabCut, Berkeley, DAVIS, MS COCO and SBD datasets verified the effectiveness of the proposed method.}
}
@article{HE2023103733,
title = {Chosen plaintext attack on JPEG image encryption with adaptive key and run consistency},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103733},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103733},
url = {https://www.sciencedirect.com/science/article/pii/S104732032200253X},
author = {Hongjie He and Yuan Yuan and Yuyun Ye and Heng-Ming Tai and Fan Chen},
keywords = {JPEG bitstream encryption, Chosen plaintext attack, Adaptive encryption key, ACCs encryption},
abstract = {A JPEG image encryption with the adaptive key and run consistency of MCUs is proposed. The chosen-plaintext attack (CPA) is given here on this encryption scheme. First, the adaptive key can be reproduced from the encrypted image, so that the plaintext images with the same adaptive key can be constructed. Second, the existence of run consistency of MCUs (RCM) between the original image and the encrypted image facilitates rapid estimation. In addition, the single swap for the runs of MCUs with RCM is designed for more accurate estimation. Detailed cryptanalytic results suggest that this encryption scheme can only be used to realize perceptual encryption but not to provide content protection for digital images. Furthermore, applications of the CPA to break other encryption schemes with RCM are presented.}
}
@article{HE2023103769,
title = {IPC-Net: Incomplete point cloud classification network based on data augmentation and similarity measurement},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103769},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103769},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000196},
author = {Yunqian He and Zhi Zhang and Zhe Wang and Yongkang Luo and Li Su and Wanyi Li and Peng Wang and Wen Zhang},
keywords = {Incomplete point clouds, Point cloud classification, Data augmentation, Similarity measurement},
abstract = {Existing point cloud classification researches are usually conducted on datasets with complete structure and clear semantics. However, in real point cloud scenes, the occlusion and truncation may destroy the completeness of objects affecting the classification performance. To solve this problem, we propose an incomplete point cloud classification network (IPC-Net) with data augmentation and similarity measurement. The proposed network learns the feature representation of incomplete point clouds and the semantic differences compared to the complete ones for classification. Specifically, IPC-Net adopts a random erasing-based data augmentation to deal with incomplete point clouds. IPC-Net also introduces an auxiliary loss function weighted by attention scores to measure the similarity between the incomplete and the complete point clouds. Extensive experiments verify that IPC-Net has the ability to classify incomplete point clouds and significantly improves the robustness of point cloud classification under different completeness.}
}
@article{HOU2023103746,
title = {Attention meets involution in visual tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103746},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103746},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002668},
author = {Yueen Hou and Zhijian Luo and Jiaming Deng and Yanzeng Gao and Kekun Huang and Weiguang Li},
keywords = {Visual tracking, Involution, Attention},
abstract = {In visual tracking, both convolution and attention are widely employed for feature enhancement and fusion. However, convolution does not adequately model global dependencies of samples due to its operation on local neighbors, while attention gives too much attention to global dependencies and too little to local dependencies. It is intrinsically infeasible to combine both methods to integrate global and local information. However, a recently-proposed model called involution uses kernels differing in spatial extent but sharing across channels, making it possible to take advantage of both convolution and attention. We propose an attention-involution (Att-Inv) model that uses an attention mechanism to generate involution kernels to take both global and local dependencies of samples into account. To improve the performance of our tracker, we develop and implement strategies of backbone network modification, template updates, and regression of bounding box distributions. We evaluate our tracker using benchmarks such as GOT10k, LaSOT, TrackingNet and OxUvA. Experimental results show that it is competitive with state-of-the-art trackers.}
}
@article{LI2023103833,
title = {DDFP:A data driven filter pruning method with pruning compensation},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103833},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103833},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000834},
author = {Guoqiang Li and Bowen Liu and Anbang Chen},
keywords = {Data driven, Model compression, Filter pruning, Pruning compensation},
abstract = {Neural network pruning techniques can be effective in accelerating neural network models, making it possible to deploy them on edge devices. In this paper, we propose to prune neural networks using data variance. Unlike other existing methods, this method is somewhat robust and does not invalidate our criteria depending on the number of data batches and the number of training sessions. We also propose a pruning compensation technique. This technique fuses the pruned convolutional information into the remaining convolutional kernel close to it. This fusion operation can effectively help retain the pruned information. We evaluate the proposed method on a number of standard datasets and compare it with several current state-of-the-art methods. Our method always achieves better performance. For example, on Tiny ImageNet, our method can prune 54.2% FLOPs of ResNet50 while obtaining a 0.22% accuracy improvement.}
}
@article{KHEZERLOU2023103781,
title = {A convolutional autoencoder model with weighted multi-scale attention modules for 3D skeleton-based action recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {92},
pages = {103781},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103781},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000317},
author = {F. Khezerlou and A. Baradarani and M.A. Balafar},
keywords = {Human action recognition, Motion trajectories, 3DPo-CDP descriptor, Change direction patterns, Pose features, Convolutional autoencoder, Weighted multi-scale attention, WMS block},
abstract = {The 3D skeleton sequences of action can be recognized based on series of meaningful movements including changes in the direction and geometry features of the body pose. In this paper, we introduce the 3DPo-CDP descriptor, which incorporates the change direction patterns of body joints and pose features in a unified deep structure to learn more adequate features for the action recognition problem. To this end, two types of features are extracted. First, Change Direction Patterns (CDPs) are extracted by following the important points of motion trajectories where a significant change of direction has occurred using two filtering phases. The CDPs capture the global features which are invariant to noise and insignificant temporal dynamics of joints. Second, Pose Features are employed to learn the intrinsic connectivity relationships of adjacent limbs and the variance distances of body joints from representative joints to concentrate on key-frames and informative joints. The complementary features of CDPs and 3D pose, which are transformed into images, are combined in a unified representation and fed into a new convolutional autoencoder. Unlike conventional convolutional autoencoders that focus on frames, high-level discriminative features of spatiotemporal relationships of whole body joints are extracted by introducing weighted multi-scale channel and spatial attention modules. In this paper, we show that adjacent and non-adjacent neighbors can be effectively used to compute different weights for extracting cross-interaction channels and multi-scale spatial relationships of the current pixel. The extracted features are combined with the wavelet representation of statistical body information and then classified with a multi-class SVM classifier. The experimental results demonstrate the effectiveness of the augmented 3DPo-CDP descriptor using an attentional convolution autoencoder structure on five challenging 3D action recognition datasets.}
}
@article{XU2023103821,
title = {Dual-branch deep image prior for image denoising},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103821},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103821},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000718},
author = {Shaoping Xu and Xiaohui Cheng and Jie Luo and Nan Xiao and Minghai Xiong and Changfei Zhou},
keywords = {Image denoising, Boosting performance, Dual-branch architecture, Two-stage denoising, Basic images, Unsupervised fusion},
abstract = {In this work, we propose a two-stage denoising approach, which includes generation and fusion stages. Specifically, in the generation stage, we first split the expanding path of the UNet backbone of the standard DIP (deep image prior) network into two branches, converting it into a Y-shaped network (YNet). Then we adopt the initial denoised images obtained with DAGL (dynamic attentive graph learning) and Restormer methods together with the given noisy image as the target images. Finally, we utilize the standard DIP on-line training routine to generate two complementary basic images, whose image quality is quite improved, with the help of a novel automatic iteration termination mechanism. In the fusion stage, we first split the contracting path of the standard UNet network into two branches for receiving the two basic images generated in the previous stage, and obtain a fused image as the final denoised image in a fully unsupervised manner. Extensive experimental results confirm that our method has a significant improvement over the standard DIP or other unsupervised methods, and outperforms recently proposed supervised denoising models. The noticeable performance improvement is attributed to the proposed hybrid strategy, i.e., we first adopt the supervised denoising methods to process the common content of images substantially, then utilize the unsupervised method to fine-tune the specific details. In other words, we take full advantage of the high performance of the supervised methods and the flexibility of the unsupervised methods.}
}
@article{LIU2023103806,
title = {Joint face completion and super-resolution using multi-scale feature relation learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103806},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103806},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000561},
author = {Zhilei Liu and Chenggong Zhang and Yunpeng Wu and Cuicui Zhang},
keywords = {Face restoration, Multi-scale feature, Graph convolution},
abstract = {Previous research on face restoration often focused on repairing specific types of low-quality facial images such as low-resolution (LR) or occluded facial images. However, in the real world, both the above-mentioned forms of image degradation often coexist. Therefore, it is important to design a model that can repair images that are LR and occluded simultaneously. This paper proposes a multi-scale feature graph generative adversarial network (MFG-GAN) to carry out face restoration in contexts in which both LR and occluded degradation modes coexist, and also to repair images with a single type of degradation. Based on the GAN, the MFG-GAN integrates the graph convolution and feature pyramid networks to restore occluded low-resolution face images to non-occluded high-resolution face images. The MFG-GAN uses a set of customized losses to ensure that high-quality images are generated. In addition, we designed the network in an end-to-end format. We conduct experiments on general face image restoration and facial expression restoration. Experimental results on the public-domain databases show that the proposed approach outperforms state-of-the-art methods in performing face super resolution (up to 4× or 8×) and face completion simultaneously and can recover better facial expression details.}
}
@article{FANG2023103754,
title = {Hierarchical context-agnostic network with contrastive feature diversity for one-shot semantic segmentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103754},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103754},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000044},
author = {Zhiyuan Fang and Guangyu Gao and Zekang Zhang and Anqi Zhang},
keywords = {Semantic segmentation, Few-shot learning, Unsupervised clustering, Hierarchical pyramid, Background exclusion},
abstract = {One-shot semantic segmentation aims at distinguishing pixels of an unseen category from the background, using merely one annotated image from the same category. However, most previous works neglect the feature diversity of foreground and the context information of background by using the masked average pooling. To solve these issues, we propose the Hierarchical Context-Agnostic Network (HCNet). It mainly includes two modules: (1) a Hierarchical Pyramid Supportive (HPS) module that generate the hierarchical supportive prototypes from coarse to fine to ensure feature diversity, and (2) a Background Exclusion Supportive (BES) module that explicitly introduces the contrastive information from the background for more precise category features. We conduct extensive experiments on Pascal-5i and COCO-20i to evaluate the performance of HCNet. HCNet achieves the mIoU score of 62.1% on Pascal-5i and 40.7% on COCO-20i and outperforms other works for the challenging one-shot segmentation, which has proved the efficiency of the whole network. Code is available at https://github.com/fangzy97/hcnet.}
}
@article{ZHANG2023103851,
title = {Multi-layer and Multi-scale feature aggregation for DIBR-Synthesized image quality assessment},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103851},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103851},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001013},
author = {Kexin Zhang and Xuejin Wang and Xiongli Chai and Feng Shao},
keywords = {Image quality assessment, DIBR-synthesized image, Distortion correction, BIQA},
abstract = {Depth-Image-Based Rendering (DIBR) is one of the main fundamental techniques for generating new viewpoints in 3D video applications such as multi-viewpoint video (MVV), free viewpoint video (FVV) and virtual reality (VR). Due to the imperfections of color images, depth maps or texture restoration techniques, several types of distortions occur in synthesized views. However, most of related works evaluated the quality of DIBR-synthesized views by only detecting a specific type of distortion, such as stretching, black holes, blurring, etc., which were unable to accurately evaluate the quality of DIBR-synthesized views. In this paper, a new no-reference image quality assessment method is proposed to evaluate the quality of DIBR-synthesized images by combining multi-layer and multi-scale features of images. To be specific, the distortions introduced by different stages of virtual viewpoint synthesis are first analyzed, and then multi-layer and multi-scale features are extracted to estimate the degree of texture and structure distortions. As a result, individual quality scores associated with two types of distortions (e.g., structural distortion and texture distortion) are aggregated to an overall image quality. Experimental results on two publicly available DIBR datasets show that the method has better performance than the state-of-the-art models. Index Terms: image quality assessment, DIBR-synthesized image, distortion correction, BIQA.}
}
@article{SUN2023103778,
title = {TsrNet: A two-stage unsupervised approach for clothing region-specific textures style transfer},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103778},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103778},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000287},
author = {Kexin Sun and Jie Zhang and Peng Zhang and Kexin Yuan and Geng Li},
keywords = {Object-level fashion texture transfer, TsrNet, Mask-GAN, Drf-module, Texture structure loss},
abstract = {Style transfer has been widely applied in the fashion industry for design assistance. This paper focuses on object-level image style transfer for specific texture regions on fashion images. The current methods mostly utilize extra conditional inputs to obtain regions of interest in fashion images, which take extensive time and cost to prepare these data. To address this issue, we propose a two-stage unsupervised approach named TsrNet for clothing texture style transfer between images. Firstly, we construct a network named Mask-GAN to unsupervisedly split out the clothing texture region. Secondly, we improve CycleGAN to interchange the texture on specific target regions of the two images. Specifically, considering the diversity of fashion texture, we construct a multiscale module (Drf-module) with dynamic perceptual fields and design an image gradient-based texture structure loss (GTS) to perform texture transfer. Qualitative and quantitative experimental results show the effectiveness of our algorithm for clothing-specific region style transfer.}
}
@article{LIU2023103770,
title = {Blind omnidirectional image quality assessment with representative features and viewport oriented statistical features},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103770},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103770},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000202},
author = {Yun Liu and Xiaohua Yin and Guanghui Yue and Zhi Zheng and Jinhe Jiang and Quangui He and Xinzhuang Li},
keywords = {Omnidirectional images, Quality assessment, Cross-channel color feature, Natural scene statistics},
abstract = {With the development of information technologies, various types of streaming images are generated, such as videos, graphics, Virtual Reality (VR)/omnidirectional images (OIs), etc. Among them, the OIs usually have a broader view and a higher resolution, which provides human an immersive visual experience in a head-mounted display. However, the current image quality assessment works cannot achieve good performance without considering representative human visual features and visual viewing characteristics of OIs, which limited OIs’ further development. Motivated by the above problem, this work proposes a blind omnidirectional image quality assessment (BOIQA) model based on representative features and viewport oriented statistical features. Specifically, we apply the local binary pattern operator to encoder the cross-channel color information, and apply the weighted LBP to extract the structural features. Then the local natural scene statistics (NSS) features are extracted by using the viewport sampling to boost the performance. Finally, we apply support vector regression to predict the OIs’ quality score, and experimental results on CVIQD2018 and OIQA2018 Databases prove that the proposed model achieves better performance than state-of-the-art OIQA models.}
}
@article{REN2023103846,
title = {Multi-loop graph convolutional network for multimodal conversational emotion recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103846},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103846},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000962},
author = {Minjie Ren and Xiangdong Huang and Wenhui Li and Jing Liu},
keywords = {Conversational emotion recognition, Multi-modal sentiment analysis, Graph convolutional network},
abstract = {Emotion recognition in conversations (ERC) has gained increasing research attention in recent years due to its wide applications in a surge of emerging tasks, such as social media analysis, dialog generation, and recommender systems. Since constituent utterances in a conversation are closely semantic-related, the constituent utterances’ emotional states are also closely related. In our consideration, this correlation could serve as a guide for the emotion recognition of constituent utterances. Accordingly, we propose a novel approach named Semantic-correlation Graph Convolutional Network (SC-GCN) to take advantage of this correlation for the ERC task in multimodal scenario. Specifically, we first introduce a hierarchical fusion module to model the dynamics among the textual, acoustic and visual features and fuse the multimodal information. Afterward, we construct a graph structure based on the speaker and temporal dependency of the dialog. We put forward a novel multi-loop architecture to explore the semantic correlations by the self-attention mechanism and enhance the correlation information via multiple loops. Through the graph convolution process, the proposed SC-GCN finally obtains a refined representation of each utterance, which is used for the final prediction. Extensive experiments are conducted on two benchmark datasets and the experimental results demonstrate the superiority of our SC-GCN.}
}
@article{HE2023103794,
title = {A bilateral attention based generative adversarial network for DIBR 3D image watermarking},
journal = {Journal of Visual Communication and Image Representation},
volume = {92},
pages = {103794},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103794},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000445},
author = {Zhouyan He and Lingqiang He and Haiyong Xu and Tong-Yuen Chai and Ting Luo},
keywords = {Watermarking, Depth-image-based rendering (DIBR) 3D image, Bilateral attention, Cross-modal feature fusion, Redundancy elimination},
abstract = {This paper presents a bilateral attention based generative adversarial network (BAGAN) for depth-image-based rendering (DIBR) 3D image watermarking to protect the image copyright. Convolutional block operations are employed to extract main image features for robust watermarking, but embedding watermark into some features will degrade image quality much. To relieve this kind of image distortion, the bilateral attention module (BAM) is utilized by mining correlations of the center view and the depth map to compute attention of the 3D image for guiding watermark to distribute over different image regions. Since a modality gap exists between the center view and the depth map, a cross-modal feature fusion module (CMFFM) is designed for BAM to bridge the cross-view gap. Because the depth map has lots of flat background information including many redundant features, to prune them, the depth redundancy elimination module (DREM) is used for cross-view feature fusion. In the decoder, two extractors with the same structure are built to recover watermark from the center view and the synthesized view, respectively. In addition, the discriminator is supposed to build a competitive relationship with the encoder to increase the image quality. The noise sub-network is used to train different image attacks for robustness. Extensive experimental results have demonstrated that the proposed BAGAN can obtain higher watermarking invisibility and robustness compared with existing DIBR 3D watermarking methods. Ablation experiments have also proven the effectiveness of DREM, CMFFM and BAM on BAGAN.}
}
@article{ZAGO2023103818,
title = {Benford’s law: What does it say on adversarial images?},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103818},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103818},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000688},
author = {João G. Zago and Eric A. Antonelo and Fabio L. Baldissera and Rodrigo T. Saad},
keywords = {Benford’s law, Adversarial attacks, Convolutional neural networks, Adversarial detection},
abstract = {Convolutional neural networks (CNNs) are fragile to small perturbations in the input images. These networks are thus prone to malicious attacks that perturb the inputs to force a misclassification. Such slightly manipulated images aimed at deceiving the classifier are known as adversarial images. In this work, we investigate statistical differences between natural images and adversarial ones. More precisely, we show that employing a proper image transformation for a class of adversarial attacks, the distribution of the leading digit of the pixels in adversarial images deviates from Benford’s law. The stronger the attack, the more distant the resulting distribution is from Benford’s law. Our analysis provides a detailed investigation of this new approach that can serve as a basis for alternative adversarial example detection methods that do not need to modify the original CNN classifier neither work on the high-dimensional pixel space for features to defend against attacks.}
}
@article{GHOSH2023103766,
title = {Image downscaling via co-occurrence learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103766},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103766},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000160},
author = {Sanjay Ghosh and Arpan Garai},
keywords = {Image downscaling, Kernel filtering, Co-occurrence similarity},
abstract = {Image downscaling is one of the widely used operations in image processing and computer graphics. It was recently demonstrated in the literature that kernel-based convolutional filters could be modified to develop efficient image downscaling algorithms. In this work, we present a new downscaling technique which is based on kernel-based image filtering concept. We propose to use pairwise co-occurrence similarity of the pixelpairs as the range kernel similarity in the filtering operation. The co-occurrence of the pixel-pair is learned directly from the input image. This co-occurrence learning is performed in a neighborhood based fashion all over the image. The proposed method can preserve the high-frequency structures, which were present in the input image, into the downscaled image. The idea is further extended to the case of fractions factor of downscaling. The resulting images retain visually-important details and do not suffer from edge-blurring artifact. We demonstrate the effectiveness of our proposed approach with extensive experiments on a large number of images downscaled with various downscaling factors.}
}
@article{WANG2023103859,
title = {Intermediate deep feature coding for human–machine vision collaboration},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103859},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103859},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001098},
author = {Weiqian Wang and Ping An and Xinpeng Huang and Kunqiang Huang and Chao Yang},
keywords = {Collaborative intelligence, Deep feature coding, Non-uniform quantization, Compact representation},
abstract = {Traditional image coding are mainly designed for human vision. While for collaborative intelligence, deep feature coding is specific for machine vision, which includes feature extraction and compression. Actually, deep features can build a bridge between human and machine vision. Therefore, we focus on generalized deep feature extraction and compression for multitask, which includes image reconstruction task for human vision and computer visual tasks for machine vision. After analyzing correlation among multitask, a reconstruction guided feature extraction strategy and feature fusion based network are proposed to get more generalized intermediate deep feature, which contains sufficient information friendly for human and machine vision. Besides, a non-uniform quantization method based on importance and a compact representation method for feature distribution information protection are proposed for high efficiency feature coding. Eventually, we come up with an entire intermediate deep feature coding framework including feature extraction and compression. Experimental results indicate the performance gains with our framework.}
}
@article{CHEN2023103745,
title = {Learning an attention-aware parallel sharing network for facial attribute recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103745},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103745},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002656},
author = {Si Chen and Xinyu Lai and Yan Yan and Da-Han Wang and Shunzhi Zhu},
keywords = {Facial attribute recognition, Multi-task learning, Attention mechanism, Parallel sharing network},
abstract = {Existing multi-task learning based facial attribute recognition (FAR) methods usually employ the serial sharing network, where the high-level global features are used for attribute prediction. However, the shared low-level features with valuable spatial information are not well exploited for multiple tasks. This paper proposes a novel Attention-aware Parallel Sharing network termed APS for effective FAR. To make full use of the shared low-level features, the task-specific sub-networks can adaptively extract important features from each block of the shared sub-network. Furthermore, an effective attention mechanism with multi-feature soft-alignment modules is employed to evaluate the compatibility of the local and global features from the different network levels for discriminating attributes. In addition, an adaptive Focal loss penalty scheme is developed to automatically assign weights to handle the problems of class imbalance and hard example mining for FAR. Experiments demonstrate that the proposed method achieves better performance than the state-of-the-art FAR methods.}
}
@article{WANG2023103805,
title = {LiDAR-only 3D object detection based on spatial context},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103805},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103805},
url = {https://www.sciencedirect.com/science/article/pii/S104732032300055X},
author = {Qiang Wang and Ziyu Li and Dejun Zhu and Wankou Yang},
keywords = {3D object detection, Convolutional neural network, LiDAR, Computer vision, Deep learning},
abstract = {LiDAR-based 3D Object detection is one of the popular topics in recent years, and it is widely used in the fields of autonomous driving and robot controlling. However, due to the scanning pattern of LiDAR, the point clouds of objects at far distance are sparse and more difficult to be detected. To solve this problem, we propose a two-stage network based on spatial context information, named SC-RCNN (Spatial Context RCNN), for object detection in 3D point cloud scenes. SC-RCNN first uses a backbone with sparse convolutions and submanifold sparse convolutions to extract the voxel features of point scenes and generate a series of candidate boxes. For the sparsity of far-distance point clouds, we design the local grid point pooling (LGP Pooling) to extract features and spatial context information around candidate regions for subsequent box refinement. In addition, we propose the pyramid candidate box augmentation (PCB Augmentation) to expand the candidate boxes with a multi-scale style, enriching the feature encoding. The experimental results show that SC-RCNN significantly outperforms previous methods on KITTI dataset and Waymo dataset, and is particularly robust to the sparsity of point clouds.}
}
@article{TANG2023103834,
title = {An efficient lightweight network for single image super-resolution},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103834},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103834},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000846},
author = {Yinggan Tang and Xiang Zhang and Xuguang Zhang},
keywords = {Super-resolution, Sparse, Self-attention, Efficiency, Lightweight},
abstract = {The outstanding performance of convolutional neural networks (CNNs) shown in single image super-resolution (SISR) strongly depends on network’s depth, which hampers its application in low-power computing devices. In this paper, a lightweight and efficient network (LESR) is proposed for SISR by constructing the shallow feature extraction block (SFBlock), the cascaded sparse mask blocks (SMBlocks) and the feature fusion block (FFBlock). The SFBlock efficiently extracts global informative features from the original low resolution image using sparse self-attention, SMBlock skips the redundant computation in extracted features, and more meaningful information is distilled for the sequential reconstruction block by the FFBlock. In addition, a recently proposed activation function called ACON-C is used to replace the ReLU function to ease the training difficulty. Extensive experiments show that our proposed network performs better than most advanced lightweight SISR algorithms with comparable parameters and less FLOPs on benchmark database for ×2/3/4 SISR.}
}
@article{BARAJASSOLANO2023103782,
title = {Compressive Spectral Video Sensing using the Convolutional Sparse Coding framework CSC4D},
journal = {Journal of Visual Communication and Image Representation},
volume = {92},
pages = {103782},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103782},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000329},
author = {Crisostomo Barajas-Solano and Juan-Marcos Ramirez and José Ignacio Martínez Torre and Henry Arguello},
keywords = {Compressive spectral video sensing, Convolutional sparse coding, Sparse representation, Spectral videos},
abstract = {Spectral Videos (SV) contain a scene’s spatial–spectral-time information. Just as with Spectral Images (SI), SVs require expensive sensing hardware, storage plus high frame ratios. Although Super Resolution techniques improve the quality of low-resolution SVs, Compressive Spectral Video Sensing (CSVS) senses high-quality SVs by extending the Compressive Sensing Image (CSI) techniques. CSI uses the universal Sparse Signal Representation (SSR) model for SVs and SIs despite the limited quality of the recovered signals. On the other hand, dictionaries synthesis models are used successfully for representing SIs, SVs, and in CSI. This work proposes the 4D convolutional sparse representation (CSC4D) for recovering full-resolution SV from CSVS measurements. It is based on a multidimensional formulation of the CSC model, profiting from its robustness without additional optical flow information. Extensive numerical simulations (two CSI architectures and noise models) show that the proposed CSC4D+CSVS improves the state-of-the-art in both quality and border sharpness by up to 1.5 dB.}
}
@article{ZHANG2023103783,
title = {A simple and effective static gesture recognition method based on attention mechanism},
journal = {Journal of Visual Communication and Image Representation},
volume = {92},
pages = {103783},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103783},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000330},
author = {Lizao Zhang and Qiuhong Tian and Qionglu Ruan and Zhixiang Shi},
keywords = {Sign language recognition, Deep learning, Attention mechanism},
abstract = {To solve the problem of low sign language recognition rate under the condition of small samples, a simple and effective static gesture recognition method based on an attention mechanism is proposed. The method proposed in this paper can enhance the features of both the details and the subject of the gesture image. The input of the proposed method depends on the intermediate feature map generated by the original network. Also, the proposed convolutional model is a lightweight general module, which can be seamlessly integrated into any CNN(Convolutional Neural Network) architecture and achieve significant performance gains with minimal overhead. Experiments on two different datasets show that the proposed method is effective and can improve the accuracy of sign language recognition of the benchmark model, making its performance better than the existing methods.}
}
@article{YUAN2023103807,
title = {Semantic-embedding Guided Graph Network for cross-modal retrieval},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103807},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103807},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000573},
author = {Mengru Yuan and Huaxiang Zhang and Dongmei Liu and Lin Wang and Li Liu},
keywords = {Cross-modal retrieval, Graph convolution network, Adversarial network, Graph aggregation network},
abstract = {Many methods focus on aligning image regions with the corresponding text fragments, and ignore that images contain fragments that cannot be expressed by texts. To fully express the information of images and prevent the performance degradation caused by fine-grained information deviating from the core meaning of images, we propose Semantic-embedding Guided Graph Network (SGGN) for cross-modal retrieval. It learns the detail representations of each modality, with an integrated semantic information, by guiding local fragments to capture the internal correlation of cross-modal data and effectively convey the information. To further bridge the semantic gap between different modalities, SGGN uses adversarial network to play a game, and uses graph aggregation network to absorb complementary information of neighbor samples. We evaluate our approach on two datasets. Our method (based on R@10) achieves 97.2% on Flickr30k dataset. On MS-COCO dataset, it reaches 99.2% using 1K test set and 92.0% using 5K test set.}
}
@article{HU2023103755,
title = {Hierarchical attention vision transformer for fine-grained visual classification},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103755},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103755},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000056},
author = {Xiaobin Hu and Shining Zhu and Taile Peng},
keywords = {Fine-grained visual classification, Vision transformer, Hierarchical attention selection, Attention-guided data augmentation},
abstract = {Recently, vision transformer has gained a breakthrough in image recognition. Its self-attention mechanism (MSA) can extract discriminative tokens information from different patches to improve image classification accuracy. However, the classification token in its deep layer ignore the local features between layers. In addition, the patch embedding layer feeds fixed-size patches into the network, which inevitably introduces additional image noise. Therefore, we propose a hierarchical attention vision transformer (HAVT) based on the transformer framework. We present a data augmentation method for attention cropping to crop and drop image noise and force the network to learn key features. Second, the hierarchical attention selection (HAS) module is proposed, which improves the network's ability to learn discriminative tokens between layers by filtering and fusing tokens between layers. Experimental results show that the proposed HAVT outperforms state-of-the-art approaches and significantly improves the accuracy to 91.8% and 91.0% on CUB-200–2011 and Stanford Dogs, respectively. We have released our source code on GitHub https://github.com/OhJackHu/HAVT.git.}
}
@article{LEI2023103852,
title = {DCAM: Disturbed class activation maps for weakly supervised semantic segmentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103852},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103852},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001025},
author = {Jie Lei and Guoyu Yang and Shuaiwei Wang and Zunlei Feng and Ronghua Liang},
keywords = {Weakly supervised semantic segmentation, Class activation map, Image-level class label, Disturbance injection},
abstract = {In the field of weakly supervised semantic segmentation (WSSS), Class Activation Maps (CAM) are typically adopted to generate pseudo masks. Yet, we find that the crux of the unsatisfactory pseudo masks is the incomplete CAM. Specifically, as convolutional neural networks tend to be dominated by the specific regions in the high-confidence channels of feature maps during prediction, the extracted CAM contains only parts of the object. To address this issue, we propose the Disturbed CAM (DCAM), a simple yet effective method for WSSS. Following CAM, we adopt a binary cross-entropy (BCE) loss to train a multi-label classification model. Then, we disturb the feature map with retraining to enhance the high-confidence channels. In addition, a softmax cross-entropy (SCE) loss branch is employed to increase the model attention to the target classes. Once converged, we extract DCAM in the same way as in CAM. The evaluation on both PASCAL VOC and MS COCO shows that DCAM not only generates high-quality masks (6.2% and 1.4% higher than the benchmark models), but also enables more accurate activation in object regions. The code is available at https://github.com/gyyang23/DCAM.}
}
@article{WANG2023103779,
title = {Image watermarking using DNST-PHFMs magnitude domain vector AGGM-HMT},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103779},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103779},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000299},
author = {Xiangyang Wang and Runtong Ma and Yixuan Shen and Panpan Niu},
keywords = {Statistical image watermarking, DNST-PHFMs magnitudes, Vector AGGM-HMT, LCGEM estimation, ML watermark decoder},
abstract = {Invisibility, robustness and payload are three indispensable and contradictory properties for any image watermarking systems. Therefore, in this paper a novel statistical image watermark decoder based on robust discrete nonseparable Shearlet transform (DNST)-polar harmonic Fourier moments (PHFMs) magnitude and effective vector anisotropic generalized Gaussian mixtures (AGGM)-hidden Markov tree (HMT). We begin with a detailed study on the robustness and statistical characteristics of local DNST- PHFMs magnitudes of natural images. This study reveals the excellent robustness, highly non-Gaussian marginal statistics and strong dependencies of local DNST-PHFMs magnitudes. We also find that conditioned on their generalized neighborhoods, the local DNST-PHFMs magnitudes can be approximately modeled as anisotropic generalized Gaussian variables. Based on these findings, we model local DNST-PHFMs magnitudes using a vector AGGM-HMT that can capture all interscale, interdirection, and interlocation dependencies. Meanwhile, model parameters can be estimated effectively by using localization clues guided expectation–maximization (LCGEM) approach. Finally, we develop a new statistical image watermark decoder using the vector AGGM-HMT and maximum likelihood (ML) decision rule. Extensive experimental results show the superiority of the proposed watermark decoder over several state-of-the-art statistical watermarking methods and some approaches based on convolutional neural networks.}
}
@article{PARK2023103863,
title = {Multiple transformation function estimation for image enhancement},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103863},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103863},
url = {https://www.sciencedirect.com/science/article/pii/S104732032300113X},
author = {Jaemin Park and An Gia Vien and Minhee Cha and Thuy Thi Pham and Hanul Kim and Chul Lee},
keywords = {Image enhancement, Multiple transformation functions, Color representation, Histogram representation},
abstract = {Most deep learning-based image enhancement algorithms have been developed based on the image-to-image translation approach, in which enhancement processes are difficult to interpret. In this paper, we propose a novel interpretable image enhancement algorithm that estimates multiple transformation functions to describe complex color mapping. First, we develop a histogram-based multiple transformation function estimation network (HMTF-Net) to estimate multiple transformation functions by exploiting both the spatial and statistical information of the input images. Second, we estimate pixel-wise weight maps, which indicate the contribution of each transformation function at each pixel, based on the local structures of the input image and the transformed images obtained by each transformation function. Finally, we obtain the enhanced image as the weighted sum of the transformed images using the estimated weight maps. Extensive experiments confirm the effectiveness of the proposed approach and demonstrate that the proposed algorithm outperforms state-of-the-art image enhancement algorithms for different image enhancement tasks.}
}
@article{DAI2023103823,
title = {Global-guided weakly-supervised learning for multi-label image classification},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103823},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103823},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000731},
author = {Yong Dai and Weiwei Song and Zhi Gao and Leyuan Fang},
keywords = {Global correlation, Feature disentanglement, Label-related regions, Weakly-supervised learning, Multi-label classification},
abstract = {Multi-label classification with region-free labels is attracting increasing attention compared to that with region-based labels due to the time-consuming manual region-labeling process. Existing methods usually employ attention-based technology to discover the conspicuous label-related regions in a weakly-supervised manner with only image-level region-free labels, while the region covering is not precise without exploring global clues of multi-level features. To address this issue, a novel Global-guided Weakly-Supervised Learning (GWSL) method for multi-label classification is proposed. The GWSL first extracts the multi-level features to estimate their global correlation map which is further utilized to guide feature disentanglement in the proposed Feature Disentanglement and Localization (FDL) networks. Specifically, the FDL networks then adaptively combine the different correlated features and localize the fine-grained features for identifying multiple labels. The proposed method is optimized in an end-to-end manner under weakly supervision with only image-level labels. Experimental results demonstrate that the proposed method outperforms the state-of-the-arts for multi-label learning problems on several publicly available image datasets. To facilitate similar researches in the future, the codes are directly available online at https://github.com/Yong-DAI/GWSL.}
}
@article{SHI2023103771,
title = {MT-Net: Fast video instance lane detection based on space time memory and template matching},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103771},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103771},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000214},
author = {Peicheng Shi and Chenghui Zhang and Shucai Xu and Heng Qi and Xinhe Chen},
keywords = {Lane detection, Jitter, Space–time memory, Template matching, Error propagation},
abstract = {Currently, only a few lane detection methods focus on the dynamic characteristics of a video. In continuous prediction, single-frame detection results produce different degrees of jitter, resulting in poor robustness. We propose a new fast video instance lane detection network, called MT-Net, based on space–time memory and template matching. Memory templates were used to establish feature associations between past and current frames from a local–global perspective to mitigate jitter from scene changes and other disturbances. Moreover, we also investigated the sources and spreading mechanism of memory errors. We designed new query frame and memory encoders to obtain higher-precision memory and query frame features. The experimental results showed that, compared with state-of-the-art models, the proposed model can reduce the number of parameters by 62.28% and the unnecessary jitter and unstable factors in muti-frame lane prediction results by 12.70%, and increases the muti-frame lane detection speed by 1.79. Our proposed methods has obvious advantages in maintaining multi-frame instance lane stability and reducing errors.}
}
@article{WU2023103847,
title = {BFANet: Effective segmentation network for low altitude high-resolution urban scene image},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103847},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103847},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000974},
author = {Letian Wu and Xian Zhang and Dejun Zhu and Wankou Yang},
keywords = {Urban scene segmentation, Real-time semantic segmentation, Bi-lateral network, Convolutional neural network},
abstract = {The semantic segmentation of low altitude high-resolution urban scene images taken by UAV plays an important role in city management. However, such images have the characteristics of inter-class homogeneity and intra-class heterogeneity. How to segment these images quickly and accurately is still challenging. In this paper, we propose a novel double-branch network. For the challenge of inter-class homogeneity, a boundary flow module is designed to enhance the flow of latent semantic information between two branches by imposing boundary constraints between classes. To alleviate intra-class heterogeneity, a context extraction module based on adaptive dynamic fusion is designed, which effectively captures the long-term relationship of features with very low parameters. Experiments on two typical datasets show that our approach achieves the best balance between accuracy and speed. Specifically, we achieve 65.8% mIoU and 74.1% mIoU on UAVid test set and UDD validation set respectively, and 60FPS on an NVIDIA TITAN Xp.}
}
@article{WANG2023103795,
title = {Global attention retinex network for low light image enhancement},
journal = {Journal of Visual Communication and Image Representation},
volume = {92},
pages = {103795},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103795},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000457},
author = {Yongnian Wang and Zhibin Zhang},
keywords = {Low light image enhancement, Retinex, Global attention, Channel attention},
abstract = {Most low-light image enhancement methods only adjust the brightness, contrast and noise reduction of low-light images, making it difficult to recover the lost information in darker areas of the image, and even cause color distortion and blurring. To solve the above problems, a global attention-based Retinex network (GARN) for low-light image enhancement is proposed in this paper. We propose a novel global attention module which computes multiple dimensional information in the channel attention module to help facilitate inference learning. Then the global attention module is embedded into different layers of the network to extract richer shallow texture features and deep semantic features. This means that the rich features are more conducive to learning the mapping relationship between low-light images to normal-light images, so that the detail recovery of dark regions is enhanced in low-light images. We also collected a low/normal light image dataset with multiple scenes, in which the images paired as training set can succeed to be applied to low-light image enhancement under different lighting conditions. Experimental results on publicly available datasets show that our method has better effectiveness and generality than the state-of-the-art methods in terms of evaluations metrics such as PSNR, SSIM, NIQE, Entropy.}
}
@article{DASILVA2023103744,
title = {Residual spatiotemporal convolutional networks for face anti-spoofing},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103744},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103744},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002644},
author = {Vitor Luiz {da Silva} and Josep Luis Lérida and Marta Sarret and Magda Valls and Francesc Giné},
keywords = {Face anti-spoofing, Residual networks, Channel-separated networks, Spoofing detection, Biometrics},
abstract = {Previous deep learning studies on Face Anti-Spoofing (FAS) systems have exploited many aspects of spatial data for face anti-spoofing detection, but few have used end-to-end spatiotemporal approaches to solving FAS problems. This paper aims to provide new perspectives for end-to-end spatiotemporal systems to deal with FAS problems, using five residual spatiotemporal convolutional models. This work analyzes and detects which network is the most appropriate for identifying spoofing on video-based identification systems. These five models were adapted to specific features of the FAS problem and its performance (accuracy and computational cost) were tested with OULU-NPU and SiW datasets. In addition, a cross-dataset validation was carried out. The experimentation shows the strengths and weaknesses of each model against the dependency on the temporal dimension, data initialization and different FAS environment conditions. According to experimentation, residual networks outperform the state-of-the-art, being the model based on decomposing spatial and temporal flow the best option.}
}
@article{XING2023103803,
title = {Learning dynamic relationship between joints for 3D hand pose estimation from single depth map},
journal = {Journal of Visual Communication and Image Representation},
volume = {92},
pages = {103803},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103803},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000536},
author = {Huiqin Xing and Jianyu Yang and Yang Xiao},
keywords = {Hand pose estimation, Dynamic anchor, Hand gesture, Depth map},
abstract = {3D hand pose estimation from a single depth map is an essential topic in computer vision. Most existing methods are devoted to designing a model to capture more spatial information or designing loss functions based on prior knowledge to constrain the estimated pose with prior spatial information. In this work, we focus on constraining the estimation process with spatial information adaptively by learning the mutual position relationship between joint pairs. Specifically, we propose a dynamic relationship network (DRN) with dynamic anchors. The preset fixed anchors are employed to estimate the position of each joint initially. Then, each joint is considered a dynamic anchor, which plays the role of a dynamic regressor to adjust the initially estimated position of each joint. The final estimation of each joint is the weighted sum of the results from all the dynamic anchors. Extensive experiments on benchmarks demonstrate that our method provides competitive results compared with state-of-the-arts.}
}
@article{WANG2023103751,
title = {Spatial-invariant convolutional neural network for photographic composition prediction and automatic correction},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103751},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103751},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000019},
author = {Yaoting Wang and Yongzhen Ke and Kai Wang and Jing Guo and Shuai Yang},
keywords = {Image composition classification, Aesthetic optimization, Space invariance, Deep learning},
abstract = {“Composition” determines the vividness of the image and its narrative power. Current research on image aesthetics implicitly considers simple composition rules, but no reliable composition classification and image optimization method explicitly considers composition rules. The existing composition classification models are not suitable for snapshots. We propose a composition classification model based on spatial-invariant convolutional neural networks (RSTN) with translation invariance and rotation invariance. It enhances the generalization of the model for snapshots or skewed images. Ultimately, the accuracy of the RSTN model improved by 3% over the Baseline to 90.8762%, and the rotation consistency improved by 16.015%. Furthermore, we classify images into three categories based on their sensitivity to editing: skew-sensitive, translation-sensitive, and non-space-sensitive. We design a set of composition optimization strategies for each composition that can effectively adjust the composition to beautify the image.}
}
@article{LIU2023103827,
title = {Multitone reconstruction visual cryptography based on phase periodicity},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103827},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103827},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000779},
author = {Zi-Nan Liu and Tao Liu and Bin Yan and Jeng-Shyang Pan and Hong-Mei Yang},
keywords = {Grayscale image, Visual cryptography, Phase periodicity, Light polarization},
abstract = {The current no-computation grayscale image visual cryptography (VC) can only achieve halftone reconstruction but cannot truly achieve multitone. To solve this problem, we propose the concept of phase periodicity of the λ/2 retarder film and calculate the optical axis angle set with phase periodicity. According to the phase periodicity, we propose a λ/2 retarder film phase periodicity visual cryptography (RPP-VC). In RPP-VC, the secret pixels are encoded as the optical axis angles of n λ/2 retarder films and distributed to n shares. The decoding process does not require computation. The reconstructed image has no pixel expansion and can reach up to 23 tones. The quality of the reconstructed images has been greatly improved and the evaluation indicators of perceived quality are nearly doubled compared with other grayscale image VC schemes. The experimental results verify the feasibility of RPP-VC.}
}
@article{WANG2023103739,
title = {Anomaly detection with dual-stream memory network},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103739},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103739},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002590},
author = {Zhongyue Wang and Ying Chen},
keywords = {Anomaly detection, Memory network, Optical flow, Memory sharing},
abstract = {Anomaly detection is an essential but challenging task. Existing DNN-based approaches tend to ignore the impact of network history state on extracting spatio-temporal correlations between video events. To address this problem, a Dual-Stream Memory Network (DSM-Net) has been proposed. It leverages historical information from the network to create a dual-stream memory module serving as complementary knowledge for the anomaly detection network. The memory module performs writing and reading in the form of a queue of data features. The writing records the historic information of video events through a moving average encoder, and the reading uses optical flow to uncover behavioral patterns in RGB images. Using a memory sharing strategy, the semantic information of the appearance branch and the motion branch can be integrated to reinforce the network. Results demonstrate that the proposed method on various standard datasets performs favorably when compared to existing methods.}
}
@article{JIN2023103816,
title = {Semantical video coding: Instill static-dynamic clues into structured bitstream for AI tasks},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103816},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103816},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000664},
author = {Xin Jin and Ruoyu Feng and Simeng Sun and Runsen Feng and Tianyu He and Zhibo Chen},
keywords = {Video coding, Semantically structured bitstream, Intelligent analytics},
abstract = {Traditional media coding schemes typically encode image or video into a semantic-unknown binary stream, which fails to directly support downstream intelligent tasks at the bitstream level. Semantically Structured Image Coding (SSIC) (Sun et al., 2020) makes the first attempt to enable partial-decoding image intelligent task analysis via a Semantically Structured Bitstream (SSB). However, the SSIC considers image coding and its generated SSB only contains the static object information. In this paper, we propose an advanced Semantically Structured Video Coding (SSVC). Video signals contain more rich dynamic motion information and redundancy. Thus, we present a reformulation of semantically structured bitstream (SSB) in SSVC which contains both static object characteristics and dynamic motion clues. Specifically, we introduce optical flow to encode continuous motion information and reduce cross-frame redundancy via a predictive coding architecture, then the optical flow and residual information are reorganized into SSB, which enables the proposed SSVC could better adaptively support video-based downstream intelligent applications. Extensive experiments on various vision tasks demonstrate that the proposed SSVC framework could directly support multiple intelligent tasks just depending on a partially decoded bitstream, saving bitrate consumption for intelligent analytics.}
}
@article{HONG2023103861,
title = {Fast Non-Local Attention network for light super-resolution},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103861},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103861},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001116},
author = {Jonghwan Hong and Bokyeung Lee and Kyungdeuk Ko and Hanseok Ko},
keywords = {Single Image Super-Resolution, Non-Local Attention, Light model},
abstract = {Although convolutional neural network-based methods have achieved significant performance improvement for Single Image Super-Resolution (SISR), their vast computational cost hinders real-world environment application. Thus, the interest in light networks for SISR is rising. Since existing SISR light models mainly focus on extracting fine local features using convolution operation, they have a limitation in that networks hardly capture global information. To capture the long-range dependency, Non-Local (NL) attention and Transformers have been explored in the SISR task. However, they are still suffering from a balancing problem between performance and computational cost. In this paper, we propose Fast Non-Local attention NETwork (FNLNET) for a super light SISR, which can capture the global representation. To acquire global information, we propose The Fast Non-Local Attention (FNLA) module that has low computational complexity while capturing global representation that reflects long-distance relationships between patches. Then, FNLA requires only 16 times lower computational cost than conventional NL networks while improving performance. In addition, we propose a powerful module called Global Self-Intension Mining (GSIM) that fuses the multi-information resources such as local, and global representation. Our FNLNET shows outstanding performance with fewer parameters and computational costs in the experiments on the benchmark datasets against state-of-the-art light SISR models.}
}
@article{OTHMAN2023103743,
title = {Classification networks for continuous automatic pain intensity monitoring in video using facial expression on the X-ITE Pain Database},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103743},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103743},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002632},
author = {Ehsan Othman and Philipp Werner and Frerk Saxen and Ayoub Al-Hamadi and Sascha Gruss and Steffen Walter},
keywords = {Continuous pain intensity recognition, Random Forest classifier, Facial expression, Long-Short Term Memory, Sample weighting},
abstract = {So far, the current methods in the clinical application do not facilitate continuous monitoring for pain and are unreliable, especially for vulnerable patients. In contrast, several automated methods have been proposed for this task by using facial features that were extracted independently from every frame of a given sequence. However, the obtained results were poor due to the failure to represent movement dynamics. To solve this problem, this work introduces three distinct methods regarding classification to monitor continuous pain intensity: (1) A Random Forest classifier (RFc) baseline method, (2) Long-Short Term Memory (LSTM) method, and (3) LSTM using sample weighting method (LSTM-SW). In this study, we conducted experiments with 11 datasets regarding classification, then compared results to regression results in Othman et al. (2021). Experimental results showed that the LSTM & LSTM-SW methods for continuous automatic pain intensity recognition performed better than guessing and RFc except with small datasets such as the reduced tonic datasets.}
}
@article{GILO2023103857,
title = {Unsupervised sub-domain adaptation using optimal transport},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103857},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103857},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001074},
author = {Obsa Gilo and Jimson Mathew and Samrat Mondal and Rakesh {Kumar Sanodiya}},
keywords = {Domain adaptation, Subdomain adaptation, Sliced wasserstein metric, Optimal transport},
abstract = {We focus on domain adaptation, a branch of transfer learning that concentrates on transferring knowledge from one domain to another when the data distributions differ. Specifically, we investigate unsupervised domain adaptation methods, which have abundant labeled examples from a source domain and unlabeled examples from a target domain available. We aim to minimize the distribution divergences between the domains using optimal transport with subdomain adaptation. Previous methods have mainly focused on reducing global distribution discrepancies between the domains, but these approaches cannot capture fine-grained information and do not consider the structure or geometry of the data. To handle these limitations, we propose Optimal Transport via Subdomain Adaptation (OTSA). Our method utilizes the sliced Wasserstein metric to reduce transportation costs while preserving geometrical data information and the Local Maximum Discrepancy (LMMD) to compute the local discrepancy in each domain category, which helps capture relevant features. Experiments were conducted on six standard domain adaptation datasets, and our method outperformed the majority of baselines. Our approach increased the average accuracy when compared with baselines on the OfficeHome (67.7% to 68.31%), Office-Caltech10 (91.8% to 96.33%), IMAGECLEF-DA (87.9% to 89.9%), VisDA-2017 (79.6% to 81.83%), Office31 (88.17% to 89.11%), and PACS (69.08% to 83.72%) datasets, respectively.}
}
@article{FU2023103731,
title = {TMSO-Net: Texture adaptive multi-scale observation for light field image depth estimation},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103731},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103731},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002516},
author = {Congrui Fu and Hui Yuan and Hongji Xu and Hao Zhang and Liquan Shen},
keywords = {Light field, Depth estimation, Epipolar plane image, Convolution neural network, Texture classification},
abstract = {Light field can record the four-dimensional information of light rays, i.e. the position and direction information in which depth information is implied. To improve the depth estimation accuracy, we propose a depth estimation algorithm based on convolutional neural network (CNN). First, a single image super resolution algorithm is adopted to spatially super resolve the sub-aperture images (SAIs). Second, to adapt the texture complexity, the SAIs are partitioned into two regions, i.e., simple texture region and complex texture region, based on the texture analysis of the central SAI. Third, the epipolar plane images (EPIs) in horizontal, vertical, 45 degree diagonal, and 135 degree diagonal directions for both complex and simple texture regions are extracted, and the corresponding EPIs for the simple and complex texture regions are fed into the specified network branches. Finally, a fusion module is designed to generate the depth map. Experimental results show that the quality of the estimated depth maps by the proposed method is better than the state-of-the-art methods in terms of both objective quality and subjective quality. Moreover, the proposed method is more robust to noise.}
}
@article{WANG2023103752,
title = {FE-YOLOv5: Feature enhancement network based on YOLOv5 for small object detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103752},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103752},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000020},
author = {Min Wang and Wenzhong Yang and Liejun Wang and Danny Chen and Fuyuan Wei and HaiLaTi KeZiErBieKe and Yuanyuan Liao},
keywords = {Small object detection, Feature enhancement, Spatial-aware},
abstract = {Due to their inherent characteristics, small objects have weaker feature representation after multiple down-sampling and are even annihilated in the background. FPN’s simple feature concatenation does not fully utilize multi-scale information and introduces irrelevant context into the information transfer, further reducing the detection performance of the small object. To address the above issues, we propose the simple but effective FE-YOLOv5. (1) We designed the feature enhancement module (FEM) to capture more discriminative features of the small object. Global attention and high-level global contextual information are used to guide shallow, high-resolution features. Global attention interacts with cross-dimensional feature interaction and reduces information loss. High-level context complements more detailed semantic information by modeling global relationships through non-local networks. (2) We design the spatially aware module (SAM) to filter spatial information and enhance the robustness of features. Deformable convolution performs sparse sampling and adaptive spatial learning to better focus on foreground objects. According to the experimental results, our proposed FE-YOLOv5 outperforms the other architectures in the VisDrone2019 dataset and Tsinghua-Tencent100K dataset. Compared to YOLOv5, the APS was improved by 2.8% and 2.9%, respectively.}
}
@article{ZHAO2023103828,
title = {HSP-MFL: A High-level Semantic Property driven Multi-task Feature Learning Network for unsupervised person Re-ID},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103828},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103828},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000780},
author = {Jing Zhao and Jie Liao and Jin Yuan},
keywords = {Unsupervised person re-identification, Multi-task learning, Feature fusion, Discriminative feature learning},
abstract = {Unsupervised person re-identification aims to distinguish different pedestrians from discriminative representations on the basis of unlabeled data. Currently, most unsupervised Re-ID approaches explore visual representations to generate pseudo-labels for model’s training, which may suffer from background noise and semantic loss. To tackle this problem, this paper proposes a High-level Semantic Property driven Multi-task Feature Learning Network (HSP-MFL) to firstly introduce three high-level semantic properties for unsupervised person Re-ID. Technically, we design a novel Multiple Feature Fusion Module (MFFM) to deeply explore the complex correlation among multiple semantic and visual features to capture the discriminative feature cues, as well as a multi-task training scheme to generate robust fusion features. The architecture is quite simple and does not consume extra labeling costs. Extensive experiments on three datasets demonstrate that both high-level semantic properties and multi-task learning are effective in performance improvement, yielding SOTA mAPs for unsupervised person Re-ID.}
}
@article{CHEN2023103776,
title = {FFTI: Image inpainting algorithm via features fusion and two-steps inpainting},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103776},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103776},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000263},
author = {Yuantao Chen and Runlong Xia and Ke Zou and Kai Yang},
keywords = {Image inpainting, Deep learning, Two-steps inpainting, Attention mechanism, Features fusion},
abstract = {In view of the faultiness that the existing image inpainting methods fail to make full use of the complete region to predict the missing region features when the object features are seriously missing, resulting in discontinuous features and fuzzy detail texture of the inpainting results, a fine inpainting method of incomplete image based on features fusion and two-steps inpainting (FFTI) is proposed in this paper. Firstly, the dynamic memory networks (DMN+) are used to fuse the external features and internal features of the incomplete image to generate the incomplete image optimization map. Secondly, a generation countermeasure generative network with gradient penalty constraints is constructed to guide the generator to rough repair the optimized incomplete image and obtain the rough repair map of the target to be repaired. Finally, the coarse repair graph is further optimized by the idea of coherence of relevant features to obtain the final fine repair graph. It is verified by simulation on three image data sets with different complexity, and compared with the existing dominant repair model for visual effect and objective data. The experimental results show that the results of the model repair in this paper are more reasonable in texture structure, better than other models in visual effect and objective data, and the Peak Signal-to-Noise Ratio of the proposed algorithm in the most challenging underwater targe dataset is 27.01, the highest Structural Similarity Index is 0.949.}
}
@article{PAPADOPOULOS2023103741,
title = {VICTOR: Visual incompatibility detection with transformers and fashion-specific contrastive pre-training},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103741},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103741},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002619},
author = {Stefanos-Iordanis Papadopoulos and Christos Koutlis and Symeon Papadopoulos and Ioannis Kompatsiaris},
keywords = {Recommendation system, Outfit matching, Visual compatibility, Computer vision, Deep learning},
abstract = {For fashion outfits to be considered aesthetically pleasing, the garments that constitute them need to be compatible in terms of visual aspects, such as style, category and color. Previous works have defined visual compatibility as a binary classification task with items in a garment being considered as fully compatible or fully incompatible. However, this is not applicable to Outfit Maker applications where users create their own outfits and need to know which specific items may be incompatible with the rest of the outfit. To address this, we propose the Visual InCompatibility TransfORmer (VICTOR) that is optimized for two tasks: 1) overall compatibility as regression and 2) the detection of mismatching items and utilize fashion-specific contrastive language-image pre-training for fine tuning computer vision neural networks on fashion imagery. We build upon the Polyvore outfit benchmark to generate partially mismatching outfits, creating a new dataset termed Polyvore-MISFITs, that is used to train VICTOR. A series of ablation and comparative analyses show that the proposed architecture can compete and even surpass the current state-of-the-art on Polyvore datasets while reducing the instance-wise floating operations by 88%, striking a balance between high performance and efficiency. We release our code at https://github.com/stevejpapad/Visual-InCompatibility-Transformer}
}
@article{DING2023103832,
title = {Accelerating QTMT-based CU partition and intra mode decision for versatile video coding},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103832},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103832},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000822},
author = {Gongchun Ding and Xiujun Lin and Junjie Wang and Dandan Ding},
keywords = {Versatile Video Coding, CU partition, Intra prediction, Intra mode, Complexity},
abstract = {The H.266/VVC achieves about 50% bitrate saving compared to its predecessor H.265/HEVC at the expense of exponentially increased computational complexity. The most efficient but complex technique for H.266/VVC intra frame coding is the QuadTree with a nested Multi-type Tree encoding structure (QTMT), which usually requires traversing the Rate-Distortion (R-D) cost of each partition and each mode for the best option. To alleviate such computational burden while preserving the coding efficiency as much as possible, this paper develops a multi-feature guided Fast CU Partition (FCP) and Laplacian guided Fast Mode Selection (FMS) to accelerate the intra QTMT decision together. For FCP, we regard the CU partition as a classification problem and adopt the Support Vector Machine (SVM) for its low-complexity implementation; after evaluating the contribution of a set of features, three representative features of video textures are selected and used to train the SVM model. Additionally, an advanced technique is applied by adopting a soft decision in SVM for a more flexible trade-off between the complexity and R-D performance. For FMS, we utilize the Laplace operator to determine the most probable directions of the current CU and skip half of the candidate modes for runtime saving. Experimental results demonstrate that the proposed FCP reduces the encoding time of H.266/VVC by 51.03% with 1.65% Bjøntegaard Delta Bit-Rate (BDBR) increase; the proposed FMS reduces the encoding time by 12.68% with 0.09% BDBR loss. Their direct combination and advanced combination finally lead to 54.84% encoding time reduction with 1.74% BDBR increase and 40.39% encoding time reduction with 1.33% BDBR increase, respectively, outperforming state-of-the-art approaches significantly.}
}
@article{SINGH2023103780,
title = {DSE-Net: Deep simultaneous estimation network for low-light image enhancement},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103780},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103780},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000305},
author = {Kavinder Singh and Anil Singh Parihar},
keywords = {Deep learning-based network, Simultaneous estimation, Illumination, Reflectance, Low-light (LOL) image enhancement (LLIE), Convolutional neural networks},
abstract = {This paper presents a novel approach for low-light image enhancement. We propose a deep simultaneous estimation network (DSE-Net), which simultaneously estimates the reflectance and illumination for low-light image enhancement. The proposed network contains three modules: image decomposition, illumination adjustment, and image refinement module. The DSE-Net uses a novel branched encoder–decoder based image decomposition module for simultaneous estimation. The proposed decomposition module uses a separate decoder to estimate illumination and reflectance. DSE-Net improves the estimated illumination using the illumination adjustment module and feeds it to the proposed refinement module. The image refinement module aims to produce sharp and natural-looking output. Extensive experiments conducted on a range of low-light images demonstrate the efficacy of the proposed model and show its supremacy over various state-of-the-art alternatives.}
}
@article{OU2023103804,
title = {3D Deformable Convolution Temporal Reasoning network for action recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103804},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103804},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000548},
author = {Yangjun Ou and Zhenzhong Chen},
keywords = {Action recognition, 3D deformable convolutional network, Reasoning},
abstract = {Modeling and reasoning of the interactions between multiple entities (actors and objects) are beneficial for the action recognition task. In this paper, we propose a 3D Deformable Convolution Temporal Reasoning (DCTR) network to model and reason about the latent relationship dependencies between different entities in videos. The proposed DCTR network consists of a spatial modeling module and a temporal reasoning module. The spatial modeling module uses 3D deformable convolution to capture relationship dependencies between different entities in the same frame, while the temporal reasoning module uses Conv-LSTM to reason about the changes of multiple entity relationship dependencies in the temporal dimension. Experiments on the Moments-in-Time dataset, UCF101 dataset and HMDB51 dataset demonstrate that the proposed method outperforms several state-of-the-art methods.}
}
@article{WAN2023103820,
title = {CANet: Context-aware Aggregation Network for Salient Object Detection of Surface Defects},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103820},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103820},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000706},
author = {Bin Wan and Xiaofei Zhou and Bin Zhu and Mang Xiao and Yaoqi Sun and Bolun Zheng and Jiyong Zhang and Chenggang Yan},
keywords = {Defect detection, Salient object detection, Weighted convolution pyramid, Cascaded fusion structure},
abstract = {Surface defect detection has become more and more important in the industrial manufacture and engineering application in recent years. However, due to the lack of overall perception and interaction among features layers, lots of computer vision-based detection methods cannot grab the complete details of defects when dealing with complex scenes, such as low contrast and irregular shape. Therefore, in this paper, we propose a Context-aware Aggregation Network (CANet) to accurately pop-out the defects, where we focus on the mining of context cues and the fusion of multiple context features. To be specific, embarking on the multi-level deep features extracted by encoder, we first deploy a sufficient exploration to dig the context information by deploying the weighted convolution pyramid (WCP) module, which extracts multi-scale context features, transfers the information flow between different resolution features, and fuses the features with same resolution. By this way, we can obtain the effective context pyramid features. Then, the decoder deploys the weighted context attention (WCA) module to filter the irrelevant information from context features and employs the cascaded fusion structure (CFS) to aggregate the multiple context cues in a hierarchical way. Following this way, the generated high-quality saliency maps can highlight the defects accurately and completely. Extensive experiments are performed on four public datasets, and the results firmly prove the effectiveness and superiority of the proposed CANet under different evaluation metrics.}
}
@article{PAN2023103844,
title = {A super-resolution-based license plate recognition method for remote surveillance},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103844},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103844},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000949},
author = {Sen Pan and Si-Bao Chen and Bin Luo},
keywords = {License plate recognition, Super-resolution, Remote surveillance, Neural network, Deep learning},
abstract = {With the continuous development of deep learning, neural networks have made great progress in license plate recognition (LPR). Nevertheless, there is still room to improve the performance of license plate recognition for low-resolution and relatively blurry images in remote surveillance scenarios. When it is difficult to enhance the recognition algorithm, we choose super-resolution (SR) to improve the quality of license plate images and thereby provide clearer input for the subsequent recognition stage. In this paper, we propose an automatic super-resolution license plate recognition (SRLPR) network which consists of four parts separately: license plate detection, character detection, single character super-resolution, and recognition. In the training stage, firstly, LP detection model needs to be trained alone and then its detection results will be used to successively train the three subsequent modules. During the test phase, for each input image, the network can get its LP number automatically. We also collect an applicable and challenging LPR dataset called SRLP, which is collected from real remote traffic surveillance. The experimental results demonstrate that our method achieves comprehensive quality of SR images and higher recognition accuracy compared with state-of-the-art methods. The SRLP dataset and the code for training and testing SRLPR network are available at https://pan.baidu.com/s/1vnhRa-c-dBj6jlfBZV5w4g.}
}
@article{LEE2023103850,
title = {Dual-branch vision transformer for blind image quality assessment},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103850},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103850},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001001},
author = {Se-Ho Lee and Seung-Wook Kim},
keywords = {Blind image quality assessment, No-reference image quality assessment, Vision transformer, Perceptual image processing},
abstract = {Blind image quality assessment (BIQA) has always been a challenging problem due to the absence of reference images. In this paper, we propose a novel dual-branch vision transformer for BIQA, which simultaneously considers both local distortions and global semantic information. It first extracts dual-scale features from the backbone network, and then each scale feature is fed into one of the transformer encoder branches as a local feature embedding to consider the scale-variant local distortions. Each transformer branch obtains the context of global image distortion as well as the local distortion by adopting content-aware embedding. Finally, the outputs of the dual-branch vision transformer are combined by using multiple feed-forward blocks to predict the image quality scores effectively. Experimental results demonstrate that the proposed BIQA method outperforms the conventional methods on the six public BIQA datasets.}
}
@article{GARCIALUCAS2023103829,
title = {A fast full partitioning algorithm for HEVC-to-VVC video transcoding using Bayesian classifiers},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103829},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103829},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000792},
author = {D. García-Lucas and G. Cebrián-Márquez and A.J. Díaz-Honrubia and T. Mallikarachchi and P. Cuenca},
keywords = {HEVC, VVC, Transcoding, MTT, Naïve-Bayes},
abstract = {The Versatile Video Coding (VVC) standard was released in 2020 to replace the High Efficiency Video Coding (HEVC) standard, making it necessary to convert HEVC encoded content to VCC to exploit its compression performance, which was achieved by using a larger block size of 128 × 128 pixels, among other new coding tools. However, 80.93% of the encoding time is spent on finding a suitable block partitioning. To reduce this time, this proposal presents an HEVC-to-VVC transcoding algorithm focused on accelerating the CTU partitioning decisions. The transcoder takes different information from the input bitstream of HEVC, and feeds it to two Bayes-based models. Experimental results show a time saving in the transcoding process of 45.40%, compared with the traditional cascade transcoder. This time gain has been obtained on average for all test sequences in the Random Access scenario, at the expense of only 1.50% BD-rate.}
}
@article{LI2023103777,
title = {Semantic prior-driven fused contextual transformation network for image inpainting},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103777},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103777},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000275},
author = {Haiyan Li and Yingqing Song and Haijiang Li and Zhengyu Wang},
keywords = {Image inpainting, Semantic prior generator, Fused contextual transformation, Aggregated semantic attention-aware, Discriminator},
abstract = {Recent advances in image inpainting have achieved impressive performance for generating plausible visual details on small regular image defects or simple backgrounds. However, current solution suffers from the lack of semantic priors for the image and the inability to deduce the image content from distant background, leading to distorted structures and artifacts in the results when inpainting large random irregular complicated images. To address these problems, a semantic prior-driven fused contextual transformation network for image inpainting is proposed as a promise solution. First, the semantic prior generator is put forward to map the semantic features of ground truth images and the low-level features of broken images to semantic priors. Subsequently, an image split-transform-aggregated strategy, named fusion context transformation block, is presented to infer rich multi-scale remote texture features and thus to improve the restored image finesse. Thereafter, an aggregated semantic attention-aware module, consisting of spatially adaptive normalization and enhanced spatial attention is designed to aggregate semantic priors and multi-scale texture features into the decoder to restore reasonable structure. Finally, the mask guided discriminator is developed to effectively discriminate between real and false pixels in the output image to improve the capability of the discriminator and hence to reduce the probability of artifacts containing in the output image. Comprehensive experimental results on CelebA-HQ, Paris Street View, and Places2 datasets demonstrate the superiority of the proposed network over the state-of-the-arts, whose PSNR, SSIM and MAE are improved about 20 %, 12.6 %, and 42 % gains, respectively.}
}
@article{PHAPHUANGWITTAYAKUL2023103845,
title = {Adaptive adversarial prototyping network for few-shot prototypical translation},
journal = {Journal of Visual Communication and Image Representation},
volume = {94},
pages = {103845},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103845},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000950},
author = {Aniwat Phaphuangwittayakul and Fangli Ying and Yi Guo and  Guohui and Surachai Santisookrat},
keywords = {Prototyping network, Few-shot image translation, Meta-learning, Generative adversarial network},
abstract = {Translating multiple real-world source images to a single prototypical image is a challenging problem. Notably, these source images belong to unseen categories that did not exist during model training. We address this problem by proposing an adaptive adversarial prototype network (AAPN) and enhancing existing one-shot classification techniques. To overcome the limitations that traditional works cannot extract samples from novel categories, our method tends to solve the image translation task of unseen categories through a meta-learner. We train the model in an adversarial learning manner and introduce a style encoder to guide the model with an initial target style. The encoded style latent code enhances the performance of the network with conditional target style images. The AAPN outperforms the state-of-the-art methods in one-shot classification of brand logo dataset and achieves the competitive accuracy in the traffic sign dataset. Additionally, our model improves the visual quality of the reconstructed prototypes in unseen categories. Based on the qualitative and quantitative analysis, the effectiveness of our model for few-shot classification and generation is demonstrated.}
}
@article{HAO2023103817,
title = {Rotational Voxels Statistics Histogram for both real-valued and binary feature representations of 3D local shape},
journal = {Journal of Visual Communication and Image Representation},
volume = {93},
pages = {103817},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103817},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000676},
author = {Linbo Hao and Xuefeng Yang and Ke Xu and Wentao Yi and Ying Shen and Huaming Wang},
keywords = {3D local feature descriptor, Point cloud, 3D multi-pose processing mechanism, Binary extension},
abstract = {3D Local feature description is an active and fundamental task in 3D computer vision. However, most of the existing descriptors fail to simultaneously achieve satisfactory performance among descriptiveness, robustness, efficiency, and compactness. To address these limitations, we first propose a real-valued descriptor named Rotational Voxels Statistics Histogram (RoVo), which exploits the novel 3D multi-pose processing mechanism proposed in this paper to calculate the 3D voxel density distribution in different 3D poses. Moreover, through well-designed binary encoding algorithms, we conduct the seamless extension of the real-valued RoVo descriptor to three binary representations that have different performance characteristics. Extensive evaluation experiments validate the superiority of the real-valued and three binary RoVo descriptors concerning descriptiveness, robustness, and efficiency. Furthermore, the three binary RoVo descriptors extend the performance of high compactness. Lastly, we perform the experiments of 3D scene registration and 3D object recognition to intuitively present the effectiveness of the four proposed RoVo descriptors.}
}
@article{ZHANG2023103862,
title = {Global guidance-based integration network for salient object detection in low-light images},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103862},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103862},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001128},
author = {Zenan Zhang and Jichang Guo and Huihui Yue and Yudong Wang},
keywords = {Low-light images, Salient object detection, Global information flow, Multi-level features cross integration, U-shaped attention refinement},
abstract = {Most of current salient object detection (SOD) methods focus on well-lit scenes, and their performance drops when generalized into low-light scenes due to limitations such as blurred boundaries and low contrast. To solve this problem, we propose a global guidance-based integration network (G2INet) customized for low-light SOD. First, we propose a Global Information Flow (GIF) to extract comprehensive global information, for guiding the fusion of multi-level features. To facilitate information integration, we design a Multi-level features Cross Integration (MCI) module, which progressively fuses low-level details, high-level semantics, and global information by interweaving. Furthermore, a U-shaped Attention Refinement (UAR) module is proposed to further refine edges and details for accurate saliency predictions. In terms of five metrics, extensive experimental results demonstrate that our method outperforms the existing twelve state-of-the-art models.}
}
@article{LIU2023103738,
title = {Exploring visual relationship for social media popularity prediction},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103738},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103738},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002589},
author = {An-An Liu and Hongwei Du and Ning Xu and Quan Zhang and Shenyuan Zhang and Yejun Tang and Xuanya Li},
keywords = {Social media popularity prediction, Visual relationship, Content-based filtering, Interpretability},
abstract = {Social media popularity prediction is an important channel to explore content sharing and communication on social networks. It aims to capture informative cues by analyzing multi-type data (such as images, user profiles, and text) to decide the popularity of a specified post. Intuitively, given an image, humans can volitionally focus on salient objects and relationships that are associated with their interests. For example, when we see the image including the relationship “elephant-attack-van”, it is more natural to increase our interest than the image with “elephant-near-van”. Therefore, exploiting such structural relationships is expected to help the prediction model search for evidence in support of the popularity of posts. However, most current works only focus on the global representation or the isolated objects, while ignoring the structure knowledge contained in images. To address this problem, we propose the relationship-aware social media popularity predictor. First, we extract inter-object relationships via a pre-trained scene graph generator. Then, we design a content-based filtering module to filter redundant relationships and capture the key 〈subject–predicate–object〉 information. Finally, we integrate relationship information with multi-type heterogeneous data and feed them into the CatBoost model for regression. Moreover, our predictor is capable of generating more intuitive interpretations by analyzing visual relationships in images to reasonably infer popularity scores. Extensive experiments conducted on the Social Media Prediction Dataset demonstrate that the proposed method can outperform other state-of-the-art models. Additional ablation studies and visualizations further validate the effectiveness and interpretability.}
}
@article{WU2023103742,
title = {A novel Siamese network object tracking algorithm based on tensor space mapping and memory-learning mechanism},
journal = {Journal of Visual Communication and Image Representation},
volume = {91},
pages = {103742},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2022.103742},
url = {https://www.sciencedirect.com/science/article/pii/S1047320322002620},
author = {Yongqiang Wu and Baohua Zhang and Xiaoqi Lu and Yu Gu and Yueming Wang and Xin Liu and Yan Ren and Jianjun Li},
keywords = {Object tracking, Siamese network, Tensor space, Memory-learning},
abstract = {The tracker is a core component of the tracking algorithm, but it is difficult to identify the object, which is a challenge to improve the tracking accuracy. This paper proposes a Siamese network-based tracking algorithm based on tensor space mapping and memory-learning mechanisms. Firstly, the source image is mapped to the tensor space to serialize the feature distributions. Then the gating mechanism is used to extract the association information about the adjacent state, which guides the update of the subsequent state, and the interactive information on the objects is used to locate the object. On this basis, a memory-learning module is built to traverse and extract the fine-grained features, which can filter the semantic information of the object learned by the tracker. As a result, the tracking accuracy is enhanced. The experiments show that the proposed algorithm has better performance than that of the comparison methods in the OTB100 data set and the VOT data set.}
}
@article{FANG2023103858,
title = {Dual cross knowledge distillation for image super-resolution},
journal = {Journal of Visual Communication and Image Representation},
volume = {95},
pages = {103858},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103858},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001086},
author = {Hangxiang Fang and Yongwen Long and Xinyi Hu and Yangtao Ou and Yuanjia Huang and Haoji Hu},
keywords = {Super resolution, Knowledge distillation, Convolutional neural networks},
abstract = {The huge computational requirements and memory footprint limit the practical deployment of super resolution (SR) models. Knowledge distillation (KD) allows student networks to obtain performance improvement by learning from over-parameterized teacher networks. Previous work has attempted to solve SR distillation problem by using feature-based distillation, which ignores the supervisory role of the teacher module itself. In this paper, we introduce a cross knowledge distillation framework to compress and accelerate SR models. Specifically, we propose to obtain supervision by cascading the student into the teacher network for directly utilizing teacher’s well-trained parameters. This not only reduces the difficulty of optimization for students but also avoids designing alignment with obscure feature textures between two networks. To the best of our knowledge, we are the first work to explore the cross distillation paradigm on the SR tasks. Experiments on typical SR networks have shown the superiority of our method in generated images, PSNR and SSIM.}
}
@article{WANG2023103753,
title = {Depth estimation of supervised monocular images based on semantic segmentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {90},
pages = {103753},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103753},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323000032},
author = {Qi Wang and Yan Piao},
keywords = {Monocular depth estimation, Semantic segmentation, Shared parameters, Multi-scale feature fusion},
abstract = {In recent years, the research method of depth estimation of target images using Convolutional Neural Networks (CNN) has been widely recognized in the fields of artificial intelligence, scene understanding and three-dimensional (3D) reconstruction. The fusion of semantic segmentation information and depth estimation will further improve the quality of acquired depth images. However, how to deeply combine image semantic information with image depth information and use image edge information more accurately to improve the accuracy of depth image is still an urgent problem to be solved. For this purpose, we propose a novel depth estimation model based on semantic segmentation to estimate the depth of monocular images in this paper. Firstly, a shared parameter model of semantic segmentation information and depth estimation information is built, and the semantic segmentation information is used to guide depth acquisition in an auxiliary way. Then, through the multi-scale feature fusion module, the feature information contained in the neural network on different layers is fused, and the local feature information and global feature information are effectively used to generate high-resolution feature maps, so as to achieve the goal of improving the quality of depth image by optimizing the semantic segmentation model. The experimental results show that the model can fully extract and combine the image feature information, which improves the quality of monocular depth vision estimation. Compared with other advanced models, our model has certain advantages.}
}