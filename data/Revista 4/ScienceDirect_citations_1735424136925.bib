@article{ZHU201938,
title = {Multimodal activity recognition with local block CNN and attention-based spatial weighted CNN},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {38-43},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.026},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303547},
author = {Suguo Zhu and Zhenying Fang and Yi Wang and Jun Yu and Junping Du},
keywords = {Activity recognition, Multimodal, Visual attention},
abstract = {Deep learning based human activity recognition approach combines spatial and temporal information to complete the recognition task. The temporal information is extracted by optical flow, which is always compensated by the warping method in order to achieve better performance. However, these methods usually take the global feature as the starting point, only consider global information of video frames, and ignore local information that reflects the changes of human behavior, causing the algorithm to be sensitive to the external environment such as occlusion, illumination change. In view of the above problems, this paper fuses the local spatial features of video frames, global spatial features and temporal features to recognize different actions, and further extracts the visual attention weight to make constraint on the global spatial features. Experiments show that the algorithm proposed in this paper has better accuracy compared with the existing methods.}
}
@article{2020102977,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {73},
pages = {102977},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(20)30199-1},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301991}
}
@article{2021103023,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {103023},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(21)00005-5},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000055}
}
@article{HALSTEAD2019439,
title = {Multimodal clothing recognition for semantic search in unconstrained surveillance imagery},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {439-452},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303274},
author = {Michael A. Halstead and Simon Denman and Sridha Sridharan and YingLi Tian and Clinton Fookes},
keywords = {Soft biometrics, Dempster-Shafer theory, Surveillance, Semantic person search},
abstract = {To date, surveillance based person search has focused on locating a person of interest from an image query, distinct from the law enforcement task of locating a person from a description. In this paper, we introduce a novel probabilistic framework that combines multiple traits whilst incorporating their uncertainty to tackle the emerging challenge: locating a person from a semantic query. In addressing this, we improve clothing texture recognition by leveraging Dempster-Shafer theory against an ensemble of support vector machines; achieving state-of-the-art performance for high and low resolution clothing textures. Our proposed person search framework combines information from clothing texture and colour in the torso and leg regions to produce a probabilistic match between unknown subjects and the designated target query. Results are presented on a newly created 520 subject surveillance dataset which is made available to researchers. This multi-modal person search technique achieves promising results for locating target subjects, without the requirement of pre-search target enrollment.}
}
@article{DONG2019187,
title = {Weighted locality collaborative representation based on sparse subspace},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {187-194},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.030},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303031},
author = {Xiao Dong and Huaxiang Zhang and Lei Zhu and Wenbo Wan and Zhenhua Wang and Qiang Wang and Peilian Guo and Hui Ji and Jiande Sun},
keywords = {Collaborative representation, Sparse subspace, Linear representation, Face recognition},
abstract = {This paper takes into account both unlabeled data and their local neighbors to learn their sparse representations, and proposes a face recognition approach named Weighted Locality Collaborative Representation Classifier based on sparse subspace (WLCRC). WLCRC firstly learns a subset of the original training data to build a much correlated dictionary, and then combines linear regression techniques together with weighted collaborative representation techniques to optimize the linear reconstruction of unlabeled data. It uses the newly built dictionary to learn the reconstruction coefficients for each unlabeled datum while considering the influence of its local neighbors. Classifications are performed according to the reconstruction residuals, and experimental results on benchmark datasets demonstrate that WLCRC is effective.}
}
@article{ZHANG2019334,
title = {Copyright protection method for 3D model of geological body based on digital watermarking technology},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {334-346},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303390},
author = {Xialin Zhang and Zaiquan Shen},
keywords = {Digital watermarking, Geological body, Point cloud model, Copyright protection},
abstract = {In the continuous development of Internet technology and geological information technology, problems such as illegal possession, copying, modification and dissemination of unauthorized digital products have occurred in the field of geological information technology. In the field of geological information technology, not only traditional digital products such as text, image, audio and video are produced, but also 3D model digital products unique to geological information technology, which are geological bodies, geological phenomena, geological structures, geological processes and geology. Regular 3D visualization analysis and the foundation and platform for comprehensive decision making of 3D visualization, the 3D model of geological body plays an increasingly important role in the field of geological information technology. Of course, it also faces the problems faced by traditional digital products and copyright protection. In this paper, the advantages and disadvantages of digital watermarking technology in copyright protection and the characteristics of geological body 3D model itself, and the current mature 3D model digital watermarking algorithm are introduced. The research idea of 3D geological body digital watermarking algorithm based on point cloud model is proposed. According to the spatial characteristics of the point cloud model of geological body, the spatial characteristic variables of the point cloud model are obtained by using the spatial characteristic analysis algorithm. The spatial domain information represented by the spatial characteristic variables is transformed into the frequency domain information, and then the frequency domain information is analyzed by means of mathematical statistics to extract the digital watermarking information of the whole geological body point cloud model. The feasibility of the algorithm is analyzed and verified by the experimental analysis. The digital watermark information before and after the attack is extracted by geometric attacks such as affine transformation and shearing. The extracted digital watermark information is correlated with the coefficient analysis and the robustness of the algorithm is obtained. A better robustness effect can effectively protect the copyright of the owner of the 3D model of the geological body.}
}
@article{HUANG201916,
title = {Automatic extraction of urban impervious surfaces based on deep learning and multi-source remote sensing data},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {16-27},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.051},
url = {https://www.sciencedirect.com/science/article/pii/S104732031830378X},
author = {Fenghua Huang and Ying Yu and Tinghao Feng},
keywords = {Multi-source remote sensing data, Deep learning, Extraction of urban impervious surface, ELM classifier, Fuzzy C means clustering},
abstract = {The conventional methods of urban impervious surfaces extraction mainly use the shallow-layer machine learning algorithms based on the medium- or low-resolution remote sensing images, and always provide low accuracy and poor automation level because the potential of multi-source remote sensing data are not fully utilized and the low-level features are not effectively organized. In order to address this problem, a novel method (AEIDLMRS) is proposed to automatically extract impervious surfaces based on deep learning and multi-source remote sensing data. First, the multi-source remote sensing data consisting of LIDAR points cloud data, Landsat8 images and Pléiades-1A images are pre-processed, re-sampled and registered, and then the combined features of spectral, elevation and intensity from the multi-source data are denoised using the minimum noise fraction (MNF) method to generate some representative MNF features. A small number of reliable labelled samples are automatically extracted using the fuzzy C-means (FCM) clustering method based on the MNF features. Secondly, the convolutional neural network (CNN) is used to extract the representative features of the neighborhood windows of each pixel in the fused Pléiades-1A image through multi-layer convolution and pooling operations. Finally, the combined features of MNF features and CNN features are pre-learned via the deep belief network (DBN). The DBN parameters are globally optimized jointly using the Extreme Learning Machine (ELM) classifier on the top level and the small set of labelled samples extracted via FCM, and the urban impervious surfaces are distinguished from others based on the trained ELM classifier and morphological operations. Experiments are performed to compare the proposed method with other three related methods in three different experimental regions respectively. Experimental results demonstrate that AEIDLMRS has better accuracy and automation level than the others under relatively good efficiency, and it is more suitable for the extraction of complex urban impervious surfaces.}
}
@article{DU2019166,
title = {Discriminant locality preserving projections based on L2,p-norm for image feature extraction and recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {166-177},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.037},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303158},
author = {Haishun Du and Guodong Li and Sheng Wang and Fan Zhang},
keywords = {Discriminant locality preserving projections, -norm, Subspace learning, Feature extraction, Image recognition},
abstract = {Conventional discriminant locality preserving projections (DLPP) is sensitive to outliers because the formulation of its objective function is based on L2-norm. Not only that, the learned features by DLPP are linear combinations of all the original features. So, it is hard to know which features play an important role in feature extraction, and the learned features often contain irrelevant information. In this paper, we propose a robust version of DLPP based on L2,p-norm with 0<p<1, termed DLPP-L2,p, for image feature extraction and recognition. DLPP-L2,p learns an optimal projection matrix by maximizing the L2,p-norm-based locality preserving between-class dispersion and minimizing the L2,p-norm-based locality preserving within-class dispersion simultaneously. Furthermore, by imposing an L2,p-norm penalty on the projection matrix to achieve row-sparsity, DLPP-L2,p can discard irrelevant features and transform relevant features simultaneously. Experimental results on several image datasets demonstrate the effectiveness and robustness of DLPP-L2,p.}
}
@article{2020102856,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102856},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(20)30107-3},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301073}
}
@article{LI2019701,
title = {Zero shot learning by partial transfer from source domain with L2,1 norm constraint},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {701-711},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.041},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303675},
author = {Xiao Li and Min Fang and Dazheng Feng and Haikun Li and Jinqiao Wu},
keywords = {Zero shot learning, Partial transfer, Visual similarity, Semantic similarity,  norm},
abstract = {Current zero shot learning methods mostly focus on applying the knowledge learnt by seen images to the unseen images. However, there is a big distribution difference between seen and unseen data, also called source and target domain. Thus, there are many irrelevant seen samples for unseen samples. We want to partially transfer the seen samples to target domain by selecting relevant seen samples. In this paper, we propose a method, zero shot learning by partial transfer from source domain with L2,1 norm constraint, called ZSLPT which embeds visual similarity and semantic similarity to transfer partial source samples. The relevant source samples are selected, while the irrelevant are eliminated. What’s more, we train source classification model used for transferring to target domain with the selected source samples, making the transferred target model more accurate. We have experimented on the state-of-the-art zero shot learning datasets, demonstrating that ZSLPT has good performance.}
}
@article{LU2019160,
title = {A method of visibility forecast based on hierarchical sparse representation},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {160-165},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.029},
url = {https://www.sciencedirect.com/science/article/pii/S104732031830302X},
author = {Zhenyu Lu and Bingjian Lu and Hengde Zhang and You Fu and Yunan Qiu and Tianming Zhan},
keywords = {FCM, Sparse representation, Predict, Visibility},
abstract = {This paper proposes a visibility forecast method based on hierarchical sparse representations. Firstly, it selects meteorological factors from the data of 138 ground stations located in Beijing, Tianjin and Hebei during the months (Oct.-to-Dec. and January) of years 2002–2016. Then, it uses fuzzy C means algorithm (FCM) to construct historical databases containing 5000 samples. Finally, it takes the meteorological factors corresponding to visibility as the sample of historical databases, and uses a hierarchical sparse representation to predict the visibility of new inputs. Experiment, conducted with the data of European Centre for Medium-Range Weather Forecasts (ECMWF), indicates a better performance of the hierarchical sparse representation in contrast to a sparse representation. And the visibility forecast based on hierarchical sparse representation is better than Beijing Regional Environmental Meteorology Prediction System (BREMPS) and BP neural network. The hierarchical sparse representation is simple and easy to expand, which improves the accuracy and reduce the absolute error, which is convenience for other meteorological analysis.}
}
@article{TIAN2019123,
title = {Research on image recognition method of bank financing bill based on binary tree decision},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {123-128},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.016},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303432},
author = {Man-Wen Tian and Shu-Rong Yan and Xiao-Xiao Tian and Jing-Ai Liu},
keywords = {Finance bill, Mode identification, Image processing, Binary decision tree},
abstract = {Financial paper is a note without reason debt or consideration acceptance, issued for obtaining money financing. Financial paper identification system is a hot issue of the current file analysis and identification system, it covers paper classification, image processing, character segmentation and identification, file image compression and other series of processes. A research on multiple aspects of financial paper identification system is made in this paper. On which basis, a financial paper identification system with applied value is established. Through substantive experimental test and practical application, the method has better classification performance and higher processing efficiency, and has been used in bank bill identification processing system in a large scale.}
}
@article{WANG2019130,
title = {A novel MEDLINE topic indexing method using image presentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {130-137},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318302980},
author = {Ye Wang and Lan Huang and Shuyu Guo and Leiguang Gong and Tian Bai},
keywords = {MEDLINE, Data visualization, Customized retrieval, Image presentation},
abstract = {MEDLINE is one of the largest databases of biomedical literatures. The search results from MEDLINE for medical terms are in the form of lists of articles with PubMed IDs. To further explore and select articles that may help identify potentially interesting interactions between terms, users need to navigate through the lists of URLs to retrieve and read actual articles to find relevancies among these terms. Such work becomes extremely time consuming and unbearably tedious when each query returns tens of thousands results with an uncertain recall rate. To overcome this problem, we develop a topic-specific image indexing and presentation method for discovering interactions or relatedness of medical terms from MEDLINE, based on which a prototype tool is implemented to help discover interactions between terms of types of diseases. The merits of the method is illustrated by search examples using the tool and MEDLINE abstract dataset.}
}
@article{YAO2020102838,
title = {Retraction notice to “Moving object surveillance using object proposals and background prior prediction” [J. Vis. Commun. Image Represent. 61 (2019) 85–92]},
journal = {Journal of Visual Communication and Image Representation},
volume = {69},
pages = {102838},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102838},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300894},
author = {Yiyang Yao and Peizhen Liu and Xiaowei Sun and Luming Zhang}
}
@article{YUAN201933,
title = {Research on image compression technology based on Huffman coding},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {33-38},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.043},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303717},
author = {Shuyun Yuan and Jianbo Hu},
keywords = {Image compression, Wavelet transform, Huffman coding, JPEG picture},
abstract = {With the development of information technology, image has become the mainstream of information transmission. Compared with character, image contains more information, but because image and character need more storage capacity, it will occupy more bandwidth in network transmission. In order to transmit image information more quickly, image compression is a good choice. This paper is based on an eye of image compression. The method of image compression in this paper is that firstly, the image is filtered by wavelet transform to remove the redundant information in the image, and then the Huffman method is used to encode the image. The simulation results of JPEG format image show that the size of the image can be reduced in the same image effect.}
}
@article{WU2019353,
title = {Blind image quality assessment with hierarchy: Degradation from local structure to deep semantics},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {353-362},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303316},
author = {Jinjian Wu and Jichen Zeng and Weisheng Dong and Guangming Shi and Weisi Lin},
keywords = {Blind image quality assessment, Hierarchical feature degradation, Local structure, Deep semantics},
abstract = {Though blind image quality assessment (BIQA) is highly desired in perceptual-oriented image processing systems, it is extremely difficult to design a reliable BIQA method. With the help of the prior knowledge, the human visual system (HVS) hierarchically perceives the quality degradation during the visual recognition. Inspired by this, we suggest different levels of distortion generate individual degradations on hierarchical features, and propose to consider the degradations on both low and high level features for quality prediction. By mimicking the orientation selectivity (OS) mechanism in the primary visual cortex, an OS based local structure is designed for low-level visual information representation. At the meantime, the deep residual network, which possesses multiple levels for feature integration, is employed to extract the deep semantics for high-level visual content representation. By fusing the local structure and the deep semantics, a hierarchical feature set is acquired. Next, the correlations between the degradations of image qualities and their corresponding hierarchical feature sets are analyzed, and a novel hierarchical feature degradation (HFD) based BIQA (HFD-BIQA) method is built. Experimental results on the legacy and wild image quality assessment databases demonstrate the prediction accuracy of the proposed HFD-BIQA method, and verify that the HFD-BIQA performs highly consistent with the subjective perception.}
}
@article{WANG2019101,
title = {Compass aided visual-inertial odometry},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {101-115},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.029},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303559},
author = {Yandong Wang and Tao Zhang and Yuanchao Wang and Jingwei Ma and Yanhui Li and Jingzhuang Han},
keywords = {Visual-inertial odometry (VIO), Compass, Sliding window estimator, Inconsistency, Pre-integration, Minimum cost function},
abstract = {With the development of vision and optimization techniques, visual-inertial odometry (VIO) has shown the capability of motion estimating in the GNSS-denied condition. The VIO can provide absolute pitch and roll angles estimating value, but no the absolute azimuth. In the paper, we proposed a VIO aided by compass, which can obtain the azimuth with respect to the north direction in the geographic frame. Moreover, aided by compass, the yaw angle estimating error was reduced to a greater degree, due to the measurement of azimuth. Furthermore, the consistency of the VIO backend estimator is improved as well, while the accuracy of the estimated pose states was also wholly improved. The aiding approach is a tightly-couple information fusion system of camera, IMU and magnetoresistive sensors. The optimization method is based on the pre-integration and bundle adjustment. In the paper, we derived the compass residual model based on the pre-integration model, and then its Jacobian and covariance formation were deduced to solve the nonlinear equations. The compass aided VIO software was implemented based on the Nvidia Jetson Tx2. The system was fully tested based on hardware-in-the-loop simulation and vehicle test in the real physical environment. The pose errors of VIOs with and without compass aiding were compared in the above tests. The simulation results showed that the position was and yaw errors were improved obviously; the compass aided VIO was still consistent, but the pure VIO was consistent not. The consistency character is evaluated by average NEES by Monte-Carlo in simulation. The vehicle test showed that the position error was reduced by 23%; the yaw error was reduced by 21%. As a result, the compass aided VIO not only improved the pose estimated accuracy, especially position and yaw, but also improved the consistency of VIO system.}
}
@article{2021103215,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {78},
pages = {103215},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(21)00141-3},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001413}
}
@article{XIONG2019606,
title = {Cancellation of motion artifacts in ambulatory ECG signals using TD-LMS adaptive filtering techniques},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {606-618},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.030},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303560},
author = {Fan Xiong and Dongyi Chen and Zhenghao Chen and Shumei Dai},
keywords = {Adaptive cancellation algorithm, Motion artifacts, Auxiliary dry electrode, Cosine transform},
abstract = {Wearable electrocardiogram (ECG) measurement systems have been widely used in patients with CVD (Cardiovascular Disease) which can be worn in daily lives. However, currently the main problem is motion artifact interference, and reducing motion artifacts (MA) is one of the most challenging problems encountered in the filtering and processing of physiological signals. In this paper, by analyzing the spectral energy changes during the input process of motion artifacts, a cosine transform LMS adaptive cancellation algorithm (DCT-LMS) implementation is proposed aiming to remove the motion artifacts from the ECG. In order to study the performance of the algorithm and effectively remove the motion artifacts in the ECG signal, this thesis collects ECG signals of people's daily activities from fabric-based chest straps with dry electrodes. It verifies the classic LMS adaptive elimination algorithm and the normalized one. Besides, two LMS adaptive cancellation algorithms based on sine and cosine transform are compared. The simulation and experimental results show that the cosine-based adaptive algorithm is superior to the classical LMS algorithm in eliminating high-amplitude motion artifact noise of ECG.}
}
@article{XU201967,
title = {Detection of alteration zones using hyperspectral remote sensing data from Dapingliang skarn copper deposit and its surrounding area, Shanshan County, Xinjiang Uygur autonomous region, China},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {67-78},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.032},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303043},
author = {Yuanjin Xu and Jianguo Chen and Pengyan Meng},
keywords = {Hyperspectral remote sensing, Detection of alteration zones, Dapingliang skarn copper deposit, Spectral matching},
abstract = {In application of hyperspectral remote sensing, alteration zones are primarily detected by identifying alteration mineral assemblages, but the interpretation of alteration mineral maps is often complicated by surface materials and by minerals not directly associated with alteration. This study was conducted in the Dapingliang skarn copper deposit and its surrounding area, the Shanshan County of the Xinjiang Uygur autonomous region, China. In order to successfully detect alteration zones associated with skarns, this study identified skarns rather than alteration minerals using field spectra of skarn outcrops. In this study, skarn in pixels was identified from spectral overall shape and spectral shapes of absorption-bands; SAM (spectral angle mapper) was applied in the identification. When SAM scores of spectral overall shape were less than 0.022 rad, the identified skarns were mainly distributed in the contact zones of intrusive rocks and carbonates; in particular, the three identified skarns areas (R1, R2 and R3) were consistent with the skarn areas in the geological map of the study area. The field inspection of skarns showed that the identified objects of the three marked targets (A, B and C) were almost consistent with the ground objects. These obtained results demonstrated that using the field spectra, it was possible to identify skarns in the hyperspectral image. To evaluate the identified skarn zones for use in mineral exploration, the end-member spectra extracted from the image were analysed, and alteration zones were detected using these end-member spectra. Compared with these alteration zones, the identified skarn zones were more reliable for mineral exploration of the study area.}
}
@article{THAI2019589,
title = {Contrast enhancement and details preservation of tone mapped high dynamic range images},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {589-599},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.024},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303444},
author = {Ba Chien Thai and Anissa Mokraoui and Basarab Matei},
keywords = {High dynamic range, Separable multiresolution, Lifting scheme, Optimized prediction, Cell-average interpolation, Tone mapping operators},
abstract = {This paper proposes a Tone Mapping (TM) approach converting a High Dynamic Range (HDR) image into a Low Dynamic Range (LDR) image while preserving as much information of the HDR image as possible to ensure a good LDR image visual quality. This approach is based on a separable near optimal lifting scheme using an adaptive powerful prediction step. The latter relies on a linear weighted combination depending on the neighboring coefficients extracting then the relevant finest details in the HDR image at each resolution level. Moreover the approximation and detail coefficients are modified according to the entropy of each subband. The pixel’s distribution of the coarse reconstructed LDR image is then adjusted according to a perceptual quantizer with respect to the human visual system using a piecewise linear function. Simulation results provide good results, both in terms of visual quality and TMQI metric, compared to existing competitive TM approaches.}
}
@article{ZHANG201963,
title = {The literature review of action recognition in traffic context},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {63-66},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.10.022},
url = {https://www.sciencedirect.com/science/article/pii/S104732031830258X},
author = {Haibo Zhang},
keywords = {Traffic, Action recognition, iDT, Deep learning, C3D},
abstract = {With the development of science and technology and the progress of computing level, the research field based on video is getting more and more attention. Video understanding is a hot and challenging topic in computer vision area. Human action recognition means to automatically analyze the ongoing actions from an unknown video or image sequence and classify them correctly. Action recognition technology is widely used in many areas, including patient monitoring system, human-computer interaction, intelligent video surveillance, virtual reality, intelligent security and other aspects. In addition, video retrieval based on content and intelligent image compression have broad application prospects, among which many methods of action recognition are used. With the increasing demand for technology, there are still some challenges of action recognition in traffic context. In this paper, we briefly review the development process of action recognition technology, and classify some relevant methods, then we summarize the overall development trend of this technology. Finally, we also discussed the practical significance of it in the future in traffic context.}
}
@article{RAY2019662,
title = {Object detection by spatio-temporal analysis and tracking of the detected objects in a video with variable background},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {662-674},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303286},
author = {Kumar S. Ray and Soma Chakraborty},
keywords = {Variable background, Object detection, Gabor filter, Spatio-temporal analysis, Minimum Spanning Tree (MST), Object tracking, Linear Assignment Problem (LAP), Kalman filter, Occlusion},
abstract = {In this paper we propose a novel approach for detecting and tracking objects in videos captured by moving cameras without any additional sensor. In such a video both the background and foreground change in each frame of the image sequence; making the separation of actual moving object from the background a challenging task. In this work, moving objects are detected as clusters of spatio-temporal blobs generated by spatio-temporal analysis of the image sequence using a three-dimensional Gabor filter and merged using Minimum Spanning Tree. Problem of data association during tracking is solved by Linear Assignment Problem and occlusion is handled by the application of Kalman filter. The major advantage of the proposed method is that, it does not require initialization or training on sample data to perform. Our algorithm demonstrated very satisfactory state-of-the-art result on benchmark videos. The performance of the algorithm is equivalent or superior to some benchmark algorithms.}
}
@article{HUANG2019453,
title = {Automatic extraction of impervious surfaces from high resolution remote sensing images based on deep learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {453-461},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.041},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303201},
author = {Fenghua Huang and Ying Yu and Tinghao Feng},
keywords = {High-resolution remote sensing images, Impervious surfaces extraction, Bilateral filtering, Convolutional neural network, Improved watershed algorithm},
abstract = {Due to the complexity of urban surface, the differences in impervious surface materials, the mutual interference between the spectra of ground objects and the huge impact of ground object shadows in high-resolution remote sensing (HRRS) images, it is improper to directly use shallow machine learning algorithms and conventional object-oriented segmentation methods to extract urban impervious surfaces from HRRS images. Therefore, a method for automatic extraction of impervious surfaces from HRRS images based on deep learning (AEISHIDL) is proposed to address this problem. Firstly, the original HRRS images are pre-processed and the Gram-Schmidt algorithm is employed for the fusion of panchromatic and multi-spectral bands in HRRS images. In addition, an enhanced bilateral filtering method considering edge characteristics (EBFCEC) is designed and adopted to remove noises and enhance edges of man-made objects in original HRRS images. Secondly, the EBFCEC filtered images are partitioned into multi-layer object sets by using improved marker watershed based on LAB color space (IMWLCS), and the related objects in different sets are re-segmented to have the same edges through edge integration, after which we extract spectral feature averages and shape feature values of all objects while the convolutional neural network (CNN) is used to calculate the CNN feature averages of all pixel neighborhoods in each object. Finally, the fuzzy c-means clustering (FCM) algorithm is employed jointly considering the spectrum, shape and CNN features of the segmented objects in HRRS images to judge whether the objects belong to impervious surfaces, thereby effectively increasing the accuracy of automatically extracting impervious surfaces. Two different experimental regions are selected from two different types of HRRS images (WorldView 2 and Pléiades-1A) respectively (4 experimental regions in all). The experimental results show that AEISHIDL has higher accuracy and automation level compared with other four representative methods in urban impervious surfaces extraction from HRRS images.}
}
@article{TIANLIN2019220,
title = {Application of entropy-based multi-attribute decision-making method to structured selection of settlement},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {220-232},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.026},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318302992},
author = {Duo Tianlin and Guo Jianzhong and Wu Fang and Zhai Renjian},
keywords = {Settlement, Cartographic generalization, Structured selection, Entropy method, Multi-attribute decision-making},
abstract = {Structured selection of settlements is a decision-making problem. The application of an entropy-based multi-attribute decision-making method to structured selection of settlements is actually the integration of the attribute method with the geometric method, which internalizes the consideration of geometric factors into attributes. Through avoiding the inaccuracy of subjective weighting via objective weighting and quantifying the importance degree of each settlement, it can solve the difficulties in structured selection of settlement to some extent.}
}
@article{2021103286,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103286},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(21)00188-7},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001887}
}
@article{FANG2019400,
title = {Stereoscopic image quality assessment by deep convolutional neural network},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {400-406},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303328},
author = {Yuming Fang and Jiebin Yan and Xuelin Liu and Jiheng Wang},
keywords = {Image quality assessment, Stereoscopic images, No reference, Convolutional neural network},
abstract = {In this paper, we propose a no-reference (NR) quality assessment method for stereoscopic images by deep convolutional neural network (DCNN). Inspired by the internal generative mechanism (IGM) in the human brain, which shows that the brain first analyzes the perceptual information and then extract effective visual information. Meanwhile, in order to simulate the inner interaction process in the human visual system (HVS) when perceiving the visual quality of stereoscopic images, we construct a two-channel DCNN to evaluate the visual quality of stereoscopic images. First, we design a Siamese Network to extract high-level semantic features of left- and right-view images for simulating the process of information extraction in the brain. Second, to imitate the information interaction process in the HVS, we combine the high-level features of left- and right-view images by convolutional operations. Finally, the information after interactive processing is used to estimate the visual quality of stereoscopic image. Experimental results show that the proposed method can estimate the visual quality of stereoscopic images accurately, which also demonstrate the effectiveness of the proposed two-channel convolutional neural network in simulating the perception mechanism in the HVS.}
}
@article{WANG2019102,
title = {Detection and tracking based tubelet generation for video object detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {102-111},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318302876},
author = {Bin Wang and Sheng Tang and Jun-Bin Xiao and Quan-Feng Yan and Yong-Dong Zhang},
keywords = {Object detection, Tubelet generation, Tubelet fusion},
abstract = {Video object detection (VID) is a more challenging task compared with still-image object detection, which not only needs to detect objects accurately per frame but also needs to track objects for a long period of time. In order to detect objects from videos, we propose a Detection And Tracking (DAT) based tubelet generation framework. Under this framework, we first propose a detection-based tubelet generation method which can generate tubelets with more accurate bounding boxes compared with traditional tracking-based methods. On the other hand, the latter can produce a higher recall of bounding boxes than the former in general. To take advantage of their complementary attributes, we further propose a novel tubelet fusion method to combine these multi-modal information (appearance information in independent images and contextual information in videos). Our extensive experiments on the well-known ILSVRC 2016 VID dataset show that our proposed method can achieve state-of-the-art performances.}
}
@article{XUE20191,
title = {Social multi-modal event analysis via knowledge-based weighted topic model},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {1-8},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.033},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303602},
author = {Feng Xue and Jian Sun and Xueliang Liu and Tianpeng Liu and Qiang Lu},
keywords = {Social media, Multi-modal, Topic mining, Knowledge-based},
abstract = {Along with the development of mobile Internet and social network, people’s lifestyles are also changing, and many social websites (e.g. Facebook, YouTube, and WeChat) have sprung up, which leads to the emergence of a large number of multimedia data (e.g. text, image, and video) of various events. The goal of this paper is to mine event topics efficiently from massive and unordered social media data, which is beneficial to search, browse and monitor significant social events for users or governments. In order to achieve this goal, this paper proposes a novel Knowledge-Based Multi-Modal Weighted Topic Model (KBMMWTM) for social event analysis. The proposed KBMMWTM has following advantages: (1) The proposed KBMMWTM can effectively take advantage of the multi-modality of social events jointly. (2) The proposed KBMMWTM exploits word correlation in dataset as prior knowledge to improve the performance of event topic mining. We evaluate our KBMMWTM model on a real dataset and full experiments show that our model outperforms state-of-the-art models.}
}
@article{JI2019195,
title = {Blind image quality assessment with semantic information},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {195-204},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.038},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303171},
author = {Weiping Ji and Jinjian Wu and Guangming Shi and Wenfei Wan and Xuemei Xie},
keywords = {No-reference image quality assessment, Human perception, Semantic network, Structural semantics, Spatial semantics},
abstract = {No-reference (NR) image quality assessment (IQA) aims to evaluate the quality of an image without reference image, which is greatly desired in the automatic visual signal processing system. Distortions degrade the visual contents and affect the semantics acquisition during the process of human perception. Although the existing methods evaluate the quality of images based on the structure, texture, or statistical characteristics, and deliver high quality prediction accuracy, they do not take the spatial semantics into account. From the perspective of human perception, distortions decrease the structural semantics that represent the structural information, and disturb the spatial semantics that describe the contents of images. Therefore, we attempt to measure the image quality by its degradation of semantics in an image. To extract the semantics of an image, a semantic network is proposed. The network contains convolutional neural networks (CNN) and Long Short-Term Memory (LSTM) that correspond to structural semantics and spatial semantics, respectively. CNN can be regarded as a coarse imitation of human visual mechanism to obtain the structural information, and LSTM can express the contents of an image. Then, by measuring the degradations of different semantics on images, a novel NR IQA is introduced. The proposed approach is evaluated on the databases of LIVE, CSIQ, TID2013, and LIVE multiply distorted database as well as LIVE in the wild image quality challenge database, and the results show superior performance to other state-of-the-art NR IQA methods. Furthermore, we explore the generalization capability of the proposed approach, and the experimental results indicate the proposed approach has a high robustness.}
}
@article{CHEN2019401,
title = {The experimental study about laser-induced dizziness effect of medium-wave infrared seeker which based on image processing},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {401-406},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303699},
author = {Zhaobing Chen and Kui Shi and Ning Chen and Long Shi and Xinyu Zhuang and Jiaqi Zhou and Yushuai Zhang and Hongqi Wang and Xingyang Liu and Guannan Li},
keywords = {Image processing, Laser dizziness interference, Medium wave infrared detector, Photoelectric interference},
abstract = {In view of the problem of laser dizzy interference of medium-wave infrared seeker detector in the field of photoelectric confrontation, there is a lack of equidistant distance test to verify the problem. In this paper, a medium-wave infrared seeker for short-range precision guided missile CCD detector was used as the subject, and the 3.8 μm wavelength medium wave laser was used as the laser source to study the calibration of the laser. In the experiment, two locations with a viewing distance of 14.5 km were selected to place the laser and missile seeker detectors respectively. The atmospheric transmittance was estimated using the MODTRAN software based on the meteorological parameters provided by the simple meteorological monitoring tool. Under the different target power density conditions, the simulation of the missile seeker head of the interference of the dizzy effect. By analyzing the interference images in the tracking state, the corresponding qualitative and quantitative dizziness interference results are obtained. After the calculation and analysis, it is considered that the laser energy output from the mid-wave infrared laser is 0.08 mW/m2 when the power density of the target surface of the detector is 0.08 mW/m2, respectively, under medium meteorological conditions with a visibility of 0.625. Box cannot lock the original target, there cannot effectively extract the target dizzy effect. The experimental results show that the anti-jamming ability of the anti-jamming capability of the medium-wave infrared precision guidance probe is verified by the photoelectric countermeasure, and it is considered that the dodge power density is smaller when the interference laser wavelength matches the wavelength of the seeker detector. The experimental data can provide theoretical guidance for the inversion and determination of parameters in the study of the medium wave infrared photoelectric countermeasure.}
}
@article{HUANG201914,
title = {Dynamic embedding strategy of VQ-based information hiding approach},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {14-32},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303456},
author = {Cheng-Ta Huang and Li-Chiun Lin and Cheng-Hsing Yang and Shiuh-Jeng Wang},
keywords = {Vector quantization, Dynamic embedding algorithm, Steganography},
abstract = {Information security is one of the most challenging issues. Cryptography and Steganography techniques are two popular methods for protecting data privacy. In this study, an information hiding method with dynamic embedding capacity based on vector quantization is proposed for protecting confidential data. To improve embedding capacity and image quality at the same time, dynamic-length secret bits are embedded into each pixel. Compared with previous approaches, the proposed method preforms better regarding the embedding capacity and image quality.}
}
@article{HEYDARI2019245,
title = {Cross-modal motion regeneration using Multimodal Deep Belief Network},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {245-260},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.042},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303213},
author = {Muhamad Javad Heydari and Saeed {Shiry Ghidary}},
keywords = {Multimodal deep learning, Multimodal Deep Belief Network, 2D/3D recovery, Cross-modal motion regeneration, Visual correspondence},
abstract = {In this paper, we propose a Multimodal Deep Belief Network (MDBN) for learning a generative model of 2D and 3D skeletal data. The MDBM learns the cross-modal relationship between these data in the form of a joint probability distribution over the space of multimodal inputs. It can regenerate any missing modality by sampling from the conditional distribution over the given data modality. The skeletal sequences are converted into the motion images which help us utilize the impressive power of the generative deep networks in the image processing. Furthermore, we use the variation of information (VI) as the training criterion, instead of the conventional maximum likelihood. It is proven that VI is efficient in cross-modal learning where some data modalities are missing. Our experimental results have shown that the model has an outstanding performance on the over-complete MHAD and CMU Mocap datasets in data-driven motion regeneration on a full-body 2D and 3D skeleton structures.}
}
@article{WALI201939,
title = {A new adaptive boosting total generalized variation (TGV) technique for image denoising and inpainting},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {39-51},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.047},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303730},
author = {Samad Wali and Huayan Zhang and Huibin Chang and Chunlin Wu},
keywords = {Total generalized variation, Boosting technique, Image denoising, Image inpainting, Primal-dual method},
abstract = {In this paper we present a new adaptive boosting technique for total generalized variation (TGV) based image denoising and inpainting. Instead of the strengthening and substracting steps in existing boosting techniques, the proposed technique is iteratively operated by two steps: the first step is to take average of restored image with observed image, and updated parameter; the second step is to operate the TGV restoration algorithm with the average and dynamic parameter. For each iteration, as the input contains more correct information, the restoration algorithm can produce signals with more details. We have solved our boosting TGV model by primal-dual method, and applied the boosting TGV technique for gray/color image denoising and inpainting. Our algorithms have been discussed about influence of parameters, computational cost and compared with several typical existing methods. Plenty of experimental results show that our method can produce images with more structures and prevent staircase artifacts effectively.}
}
@article{PRATES2019304,
title = {Kernel cross-view collaborative representation based classification for person re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {304-315},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303298},
author = {Raphael Prates and William Robson Schwartz},
keywords = {Person re-identification, Kernel collaborative representation based classification},
abstract = {Person re-identification aims at the maintenance of a global identity as a person moves among non-overlapping surveillance cameras. It is a hard task due to different illumination conditions, viewpoints and the small number of annotated individuals from each pair of cameras (small-sample-size problem). Common subspace learning methods have been proposed to handle the camera transition problems. However, after learning the low-dimensional representation, these methods usually compute distances using a simple cosine or Mahalanobis distance. Therefore, an still open question is how to better match probe and gallery images in the learned common subspace considering reduced number of training samples and the nonlinear behavior of the data. Collaborative Representation based Classification (CRC) has been employed successfully to address the small-sample-size problem in computer vision. However, the original CRC formulation is not well-suited for person re-identification since it does not consider that probe and gallery samples are from different cameras. Furthermore, it is a linear model, while appearance changes caused by different camera conditions indicate a strong nonlinear transition between cameras. To overcome such limitations, we propose the Kernel Cross-View Collaborative Representation based Classification (Kernel X-CRC), method that represents probe and gallery images by balancing representativeness and similarity nonlinearly. According to experimental results, we achieve state-of-the-art for rank-1 matching rates in three person re-identification datasets (CUHK03, PRID450S and GRID) and the second best results on VIPeR and CUHK01 datasets. Furthermore, we present outperforming results on Market-1501 dataset demonstrating that the Kernel X-CRC is suitable to a large-scale and multiple cameras scenario.}
}
@article{DING2019119,
title = {The passenger flow status identification based on image and WiFi detection for urban rail transit stations},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {119-129},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.033},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303055},
author = {Xiaobing Ding and Zhigang Liu and Haibo Xu},
keywords = {Rail transit, Safety of stations, Passenger flow identification, Passengers’ limiter of station, Emergency warning},
abstract = {During the peak hours, the concentration of passenger flow is relatively high for some busy subway lines, if the measures can’t be taken in time, more serious accidents may happen, which will influence the social image of the subway. At present, the passenger flow of the key stations is judged mainly by the experience of the staffs, and then the corresponding measures are taken, the errors may be large, and the relevant technical research is urgently needed. First, a data collection device called “the elf of passenger flow-collecting”, which integrates high definition camera image acquisition equipment and WIFI probe technology was set up. It can be used to collect the original passenger flow data of congestion points of subway stations. Second, a convolution neural network passenger flow identification algorithm based on deep learning is designed, which is used to estimate the P0 of stations. Third, because of the error in the video image recognition algorithm, the WIFI probe data acquisition scheme is designed, and the SQL preprocessing assembly for WIFI data processing is established. The noise of WIFI probe is preprocessed, and the flow rate of P5 based on WIFI probe is obtained. The difference between P0 and P5 is defined, and the degree of the difference between P0 and P5 is calculated, so the final passenger flow P6 can be obtained. Finally, the Songjiang University Hall Station of Shanghai Metro line 9 was taken as an experimental analysis object, the high definition camera and WIFI probe are set up on the spot, the passenger flow video data and the WIFI data are collected synchronously, so the real-time passenger flow in the station's internal position is estimated, and the accuracy is corrected, meanwhile the passenger flow early warning of the station position is obtained. An emergency response plan based on passenger flow early warning level is proposed, and the flow chart of passenger flow density inside Songjiang University hall station is drawn. The construction of the equipment platform and the identification and correction methods of passenger flow are of good practical guiding significance for the Metro to run safely.}
}
@article{LI2019565,
title = {Water level changes of Hulun Lake in Inner Mongolia derived from Jason satellite data},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {565-575},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303572},
author = {Shi Li and Jianping Chen and Jie Xiang and Yun Pan and Zhiyong Huang and Yongliang Wu},
keywords = {Jason satellite, Hulun Lake, Water level, Landsat, GRACE},
abstract = {Water levels in lakes can reflect changes in such bodies. Therefore, there is value in identifying the variations in water levels using observations from altimetry satellites and analyzing the possible causes. In this work, the water-level changes of Hulun Lake in Inner Mongolia during the period from 2002 to 2015 are monitored by the use of Jason satellite data, the results of which are compared with historical data. Landsat TM/ETM/OLI_TRIS remote sensing images are analyzed, and the surface area of the lake extracted from them and converted to the corresponding water level to verify the values obtained from the Jason observations. The results show a downward trend after 2000 (−0.98 mm/year) and a sharp increase after 2012 (3.07 mm/year). The root mean square error (RMSE) between the two methods was 0.2369 m, and the correlation coefficient was 0.986. By analyzing the various influencing factors, we draw the conclusion that the water level of Hulun Lake is affected by both natural factors (e.g., rainfall, runoff, evapotranspiration etc.) and anthropogenic influences (e.g., water consumption in coal mining, overgrazing, etc.). These are the main causes of the decrease in the area of Hulun Lake and other lakes in the Inner Mongolia Autonomous Region. By comparing the lake storage anomalies of Hulun Lake with the terrestrial Total Water Storage anomalies (TWSA) inverted from GRACE satellite data and the Surface Water Storage anomalies (SWSA) from WaterGAP Global Hydrology Model (WGHM) within the Hulun Basin, we find that not only do Hulun Lake and basin interact with each other, but also that Hulun Lake has an important function with regards to the changes within the basin as a whole. This work therefore provides a method for monitoring the dynamic changes of lake water levels, while analyzing the influencing factors based on multi-scale data. Such a method shows potential for being applied to efforts to ensure environmental protection.}
}
@article{YANG2019178,
title = {Online multi-object tracking combining optical flow and compressive tracking in Markov decision process},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {178-186},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.034},
url = {https://www.sciencedirect.com/science/article/pii/S104732031830316X},
author = {Tao Yang and Cindy Cappelle and Yassine Ruichek and Mohammed {El Bagdouri}},
keywords = {Multi-object tracking, Markov decision process, Tracking-learning-detection, Compressive sensing features},
abstract = {Effective features are important for visual tracking, and efficiency also needs to be considered especially for multi-object tracking. Thanks to the simplicity, we think compressive sensing features are suitable for this task. In this paper, we use compressive sensing features to improve the Markov decision process (MDP) multi-object tracking framework. First, we design a single object tracker which uses the compressive tracking to correct the optical flow tracking and apply this tracker into the MDP tracking framework. The appearance model constructed during compressive tracking also helps for data association. In order to validate our method, we firstly test the designed single object tracker with a common dataset. Then, we test our multi-object tracking method for vehicle tracking. Finally, we analyze and test our approach in the multi-object tracking (MOT) benchmark for pedestrian tracking. The results show our approach performs superiorly against several state-of-the-art online multi-object trackers.}
}
@article{2021103076,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {75},
pages = {103076},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(21)00042-0},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000420}
}
@article{RANA2019205,
title = {Boosting content based image retrieval performance through integration of parametric & nonparametric approaches},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {205-219},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318302888},
author = {Soumya Prakash Rana and Maitreyee Dey and Patrick Siarry},
keywords = {CBIR, Color moments, Ranklet transform, Nonparametric statistics, Moment invariants, Hypothesis test},
abstract = {The collection of digital images is growing at ever-increasing rate which rises the interest of mining the embedded information. The appropriate representation of an image is inconceivable by a single feature. Thus, the research addresses that point for content based image retrieval (CBIR) by fusing parametric color and shape features with nonparametric texture feature. The color moments, and moment invariants which are parametric methods and applied to describe color distribution and shapes of an image. The nonparametric ranklet transformation is performed to narrate the texture features. Experimentally these parametric and nonparametric features are integrated to propose a robust and effective algorithm. The proposed work is compared with seven existing techniques by determining statistical metrics across five image databases. Finally, a hypothesis test is carried out to establish the significance of the proposed work which, infers evaluated precision and recall values are true and accepted for the all image database.}
}
@article{CAI2019433,
title = {Evaluating hedge fund downside risk using a multi-objective neural network},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {433-438},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S104732031830275X},
author = {Zhaoquan Cai and Guangcai Chen and Lining Xing and Jinghui Yang and Xu Tan},
keywords = {Downside risk evaluation, Big data hedge fund, Multi-objective neural network},
abstract = {Predicting the downside risk of a hedge fund is the foundation of risk measurement. These predictions also provide conditions that can be used for designing and implementing risk prevention measures. Hence, this paper proposes a big data hedge fund downside risk evaluation model based on a multi-objective neural network. First, two evaluation indexes are defined. Then, local search is applied to merge parent and descendant populations. Only those individuals from the Pareto front are optimized. Experimental results suggest that this model and method is feasible and valid. Specifically, the VaR model is unable to estimate the possible extreme risk of a hedge fund. In contrast, the CVaR model can accurately measure the risks under extreme market conditions. However, a combination of VaR and CVaR can help a fund manager avoid extreme risks to a hedge fund.}
}
@article{CHEN2019416,
title = {ImmerTai: Immersive Motion Learning in VR Environments},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {416-427},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.039},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303183},
author = {Xiaoming Chen and Zhibo Chen and Ye Li and Tianyu He and Junhui Hou and Sen Liu and Ying He},
keywords = {Immersive education, Motion training, VR education},
abstract = {Immersive learning in Virtual Reality (VR) environments is the developing trend for future education systems including remote physical training. This paper presents “ImmerTai”, a system that is designed for effective remote motion training, particularly for Chinese Taichi, in an immersive way. With ImmerTai, the Taichi expert’s motion is captured and delivered to remote students in CAVE, HMD and PC environments for learning. The students’ motions are also captured for motion quality assessment and a group of students can form a virtual collaborative learning scenario. We built up a Taichi motion dataset with ground truth of motion quality, and based on this, we developed and evaluated several motion quality assessment methods. Then, user tests were designed and carried out to measure and compare the learning outcomes (learning time, quality and overall efficiency) of students in Cave Automatic Virtual Environment (CAVE), Head Mounted Display (HMD) and Personal Computer (PC) environments. Meanwhile, the connections between students’ learning outcomes and their VR experience were investigated and discussed too. Our results show that ImmerTai can accelerate the learning process of students noticeably (up to 17%) compared to non-immersive learning with the conventional PC setup. However, we observed a substantial difference in the quality of the learnt motion between CAVE (26% gain) and HMD (23% drop) compared to PC (baseline). While strong VR presence can enhance the learning experience of students, their learning outcomes are not fully consistent to their experience. Overall, ImmerTai with CAVE demonstrated a significantly higher learning efficiency than other tested environments.}
}
@article{ZHU2019128,
title = {Object detection and localization in 3D environment by fusing raw fisheye image and attitude data},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {128-139},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300069},
author = {Jun Zhu and Jiangcheng Zhu and Xudong Wan and Chao Wu and Chao Xu},
keywords = {Object detection, Deep learning, Data fusion, Fisheye camera, Micro aerial vehicle, Localization},
abstract = {In robotic systems, the fisheye camera can provide a large field of view (FOV). Usually, the traditional restoring algorithms are needed, which are computational heavy and will introduce noise into original data, since the fisheye images are distorted. In this paper, we propose a framework to detect objects from the raw fisheye images without restoration, then locate objects in the real world coordinate by fusing attitude information. A deep neural network architecture based on the MobileNet and feature pyramid structure is designed to detect targets directly on the fisheye raw images. Then, the target can be located based on the fisheye visual model and the attitude of the camera. Compared to traditional approaches, this approach has advantages in computational efficiency and accuracy. This approach is validated by experiments with a fisheye camera and an onboard computer on a micro-aerial vehicle (MAV).}
}
@article{LU2019269,
title = {Neutrosophic C-means clustering with local information and noise distance-based kernel metric image segmentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {269-276},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.045},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303225},
author = {Zhenyu Lu and Yunan Qiu and Tianming Zhan},
keywords = {Image segmentation, Noise clustering, Fuzzy clustering, Nutrosophic clustering},
abstract = {The traditional FCM algorithm is developed on the basis of classical fuzzy theory, though the classical fuzzy theory has its own limitations. The lack of expressive ability of uncertain information makes it hard for FCM algorithm to handle clustered boundary pixels and outliers. This paper proposes a Neutrosophic C-means Clustering with local information and noise distance-based kernel metric for image segmentation (NKWNLICM). At first, noisy distance and fuzzy spatial information are introduced to NCM model to improve the robustness of noise image segmentation. Then, the kernel function is used to measure the distance between pixels. By mapping low-dimensional data into high-dimensional data, the classification performance is further improved. At last, the fuzzy factor is redefined based on the distance between the center pixel and its neighborhood. The new fuzzy factor can excellently reflect the influence of neighborhood pixels on central pixels and improve the classification accuracy much better. The experimental results on Berkeley Segmentation Database demonstrates the excellent performance of the proposed method for noisy image segmentation.}
}
@article{HUANG2019233,
title = {Hyperspectral remote sensing image change detection based on tensor and deep learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {233-244},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318302773},
author = {Fenghua Huang and Ying Yu and Tinghao Feng},
keywords = {Tensor model, Deep learning, Support tensor machine, Hyperspectral remote sensing images, Change detection},
abstract = {Considering the bottleneck in improving the performance of the existing multi-temporal hyperspectral remote sensing (HSRS) image change detection methods, a HSRS image change detection solution based on tensor and deep learning is proposed in this study. At first, a tensor-based information model (TFS-Cube) of underlying features change in HSRS images is established. The wavelet texture feature change, spectral feature change and spatio-temporal autocorrelation coefficient of different-temporal related pixels are combined with three-order tensor, so as to make full use of the underlying features change information of HSRS images, optimize the organization mode and maintain the integrity of constraints between different underlying features. Secondly, a restricted Boltzmann Machine based on three-order tensor (Tensor3-RBM) is designed. The input, output and unsupervised learning of TFS-Cube tensor data are realized by multi-linear operations in Tensor3-RBMs. A large number of unlabeled samples are trained layer by layer through multilayer Tensor3-RBMs. Finally, the traditional BP neural network on the top layer of deep belief network (DBN) is replaced with support tensor machine (STM), and a deep belief network with multi-layer Tensor3-RBM and STM (TRS-DBN) is constructed. A small number of labeled samples are used for supervised learning and TRS-DBN global parameters optimization to improve the accuracy of TRS-DBN change detection. Two types of HSRS images from different sensors, AVIRIS and EO-1 Hyperion, are used as the data sources (double-temporal). Four representative experimental regions are randomly selected from the two areas covered by AVIRIS and EO-1 Hyperion HSRS images respectively (two regions in each area) to detect the land use changes. Experimental results demonstrate that TRS-DBN has higher change detection accuracy than similar methods and a good automation level.}
}
@article{XIAO201952,
title = {Defocus blur detection based on multiscale SVD fusion in gradient domain},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {52-61},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.048},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303742},
author = {Huimei Xiao and Wei Lu and Ruipeng Li and Nan Zhong and Yuileong Yeung and Junjia Chen and Fei Xue and Wei Sun},
keywords = {Defocus blur detection, Multiscale singular value decomposition, Sub-bands, Meanshift},
abstract = {Recently, defocus blur detection has been an extensive study, but it is still full of challenges in the blur estimation without having any prior knowledge of test image such as blur kernel, degree, or camera parameters. Inspired by the observation that the degree of defocus blur depth could be distinguished by different frequencies, a novel blur metric based on Multiscale SVD fusion (M-SVD) is proposed. The blur metric fuses different sub-bands of the selected singular values (SVs) in multiscale image windows, which could drastically reduce the chances of false positives for blur detection and overcome the difficulty that the sharp region is misjudged for a blur region because of its smooth texture. Finally, a blur map is applied on the test image combined with post-processing operation meanshift cluster to segment the blur region. Experimental results demonstrate that the proposed method can detect the defocus blur regions of test images with a satisfactory performance and outperforms the state-of-the-art methods.}
}
@article{YE2019515,
title = {Discovering spatio-temporal action tubes},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {515-524},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303468},
author = {Yuancheng Ye and Xiaodong Yang and YingLi Tian},
keywords = {Spatio-temporal action detection, Deep neural networks},
abstract = {In this paper, we address the challenging problem of spatial and temporal action detection in videos. We first develop an effective approach to localize frame-level action regions through integrating static and kinematic information by the early- and late-fusion detection scheme. With the intention of exploring important temporal connections among the detected action regions, we propose a tracking-by-point-matching algorithm to stitch the discrete action regions into a continuous spatio-temporal action tube. Recurrent 3D convolutional neural network is used to predict action categories and determine temporal boundaries of the generated tubes. We then introduce an action footprint map to refine the candidate tubes based on the action-specific spatial characteristics preserved in the convolutional layers of R3DCNN. In the extensive experiments, our method achieves superior detection results on the three public benchmark datasets: UCFSports, J-HMDB and UCF101.}
}
@article{YAN201989,
title = {Generalized general access structure in secret image sharing},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {89-101},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.031},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303067},
author = {Xuehu Yan and Yuliang Lu},
keywords = {Secret image sharing, General access structure, Chinese remainder theorem, Lossless recovery, Generalized general access structure},
abstract = {Generally speaking, the probability of every qualified set is the same and fixed in conventional secret image sharing (SIS) for general access structure (GAS). In this paper, first we introduce generalized GAS (GGAS), which allows the user to assign probability to every qualified set. Then we design a SIS scheme for GGAS by Chinese remainder theorem (CRT). On one hand, for any qualified set, we can decode the secret image at pre-assigned probability. When we collect all the shares, we can losslessly decode the secret image. On the other hand, for any forbidden set, we will decode nothing of the secret image. We only employ modular operation to decode the secret image, which may be suitable for real-time and green computing scenarios. Experimental results and analyses are realized to examine the effectiveness of our scheme.}
}
@article{MANDELJC2019503,
title = {AGs: Local descriptors derived from the dependent effects model},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {503-514},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303341},
author = {Rok Mandeljc and Jasna Maver},
keywords = {Local image descriptor, Dependent effects model, Image matching, Keypoint descriptor evaluation},
abstract = {We present a novel local descriptor based on the dependent effects model. Different types of effects are computed in a local region and properly normalized to form a descriptor, which is designed to be robust to rotation, scale changes, and skew. Two specific instances of descriptor are presented and evaluated. The first, named AG, is real-valued, and uses 126 floating-point values to represent 126 effects. The second version, named AGS, is binarized, and uses 60 bytes to represent 240 effects, each with two bits. The first bit represents the sign of an effect value, while the second denotes whether the value is near or far from zero. We experimentally evaluate the proposed descriptors in combination with popular keypoint detectors on standard feature matching datasets. The extensive evaluation shows that AG and AGS achieve high score in several performance measures, and as such, they represent an attractive alternative to popular local descriptors.}
}
@article{LIU2019576,
title = {Affective image classification by jointly using interpretable art features and semantic annotations},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {576-588},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.032},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303584},
author = {Xuan Liu and Na Li and Yong Xia},
keywords = {Affective image classification, Discrete emotion space, Deep convolutional neural network (DCNN), Feature extraction, Support vector machine (SVM)},
abstract = {Affective image classification, which aims to classify images according to their affective characteristics of inducing human emotions, has drawn increasing research attentions in the multimedia community. Although many features have been attempted, the semantic gap between low-level visual features and high-level emotional semantics, however, remains a major challenge. In this paper, we propose an affective image classification algorithm by jointly using the visual features extracted under the guidance of the art theory and semantic image annotations, such as the categories of objects and scenes, generated by a pre-trained deep convolutional neural network. This algorithm has been evaluated against three state-of-the-art approaches on three benchmark image datasets. Our results indicate that combining interpretable aesthetic features and semantic annotations can better characterize the emotional semantics and the proposed algorithm is able to produce more accurate affective image classification than the other three approaches.}
}
@article{CHEN2019138,
title = {Hybrid incremental learning of new data and new classes for hand-held object recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {138-148},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318302815},
author = {Chengpeng Chen and Weiqing Min and Xue Li and Shuqiang Jiang},
keywords = {Incremental learning, Object recognition, SVM, Human-machine interaction},
abstract = {Intelligence technology is an important research area. As a very special yet important case of object recognition, hand-held object recognition plays an important role in intelligence technology for its many applications such as visual question-answering and reasoning. In real-world scenarios, the datasets are open-ended and dynamic: new object samples and new object classes increase continuously. This requires the intelligence technology to enable hybrid incremental learning, which supports both data-incremental and class-incremental learning to efficiently learn the new information. However, existing work mainly focuses on one side of incremental learning, either data-incremental or class-incremental learning while do not handle two sides of incremental learning in a unified framework. To solve the problem, we present a Hybrid Incremental Learning (HIL) method based on Support Vector Machine (SVM), which can incrementally improve its recognition ability by learning new object samples and new object concepts during the interaction with humans. In order to integrate data-incremental and class-incremental learning into one unified framework, HIL adds the new classification-planes and adjusts existing classification-planes under the setting of SVM. As a result, our system can simultaneously improve the recognition quality of known concepts by minimizing the prediction error and transfer the previous model to recognize unknown objects. We apply the proposed method into hand-held object recognition and the experimental results demonstrated its advantage of HIL. In addition, we conducted extensive experiments on the subset of ImageNet and the experimental results further validated the effectiveness of the proposed method.}
}
@article{ZHAO2019651,
title = {Single image super-resolution based on adaptive convolutional sparse coding and convolutional neural networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {651-661},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.036},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303626},
author = {Jianwei Zhao and Chen Chen and Zhenghua Zhou and Feilong Cao},
keywords = {Super-resolution, Convolutional sparse coding, Convolutional neural network, Adaptive},
abstract = {The convolutional sparse coding-based super-resolution (CSC-SR) method has shown its good performance in single image super-resolution. It divides the low-resolution (LR) image into low-frequency part and the high-frequency part, and reconstructs their corresponding high-resolution (HR) image with bicubic interpolation and convolutional sparse coding (CSC) method, respectively. This paper is devoted to improve the performance of CSC-SR method. As convolutional neural network (CNN) can reveal the mapping relation between the LR image and the HR image for the low-frequency part better, we replace the bicubic interpolation with CNN to reconstruct the HR image for the low-frequency part. In addition, we propose an adaptive CSC method to reconstruct the HR image for the high-frequency part. We name our proposed super-resolution method as hybrid adaptive convolutional sparse coding-based super-resolution (HACSC-SR) method. Many comparison experiments illustrate that our proposed HACSC-SR method is superior to CSC-SR, CNN as well as several existing super-resolution methods.}
}
@article{NASIRI2019323,
title = {Using Expectation-Maximization for exposing image forgeries by revealing inconsistencies in shadow geometry},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {323-333},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S104732031830333X},
author = {Morteza Nasiri and Alireza Behrad},
keywords = {Image forensics, Forgery detection, Shadow geometry, EM algorithm, Image tampering},
abstract = {In this study, a new approach and mathematical framework are proposed for exposing image forgeries by detecting inconsistencies in the geometry of cast shadows. The main difficulty in detecting shadow inconsistencies is the precise establishment of correspondences between object points and their corresponding shadow points. To counter the problem, a mathematical framework is proposed to formulate the geometric transformation between the object points and their corresponding shadow points. We assume a rough correspondence between the object and shadow points and use Expectation-Maximization (EM) algorithm to simultaneously calculate the transformation parameters and categorize rough correspondences as inliers or outliers. To enhance the efficiency of the proposed algorithm, we extend the proposed algorithm to handle the ambiguity in initial correspondence by using the one-to-many correspondence strategy. Experimental results on the provided database comprising forged and authentic images showed the accuracy of 84% and 98% for one-to-one and one-to-many correspondence strategies, respectively.}
}
@article{ZHU2019532,
title = {A novel framework for semantic segmentation with generative adversarial network},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {532-543},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.020},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318302931},
author = {Xiaobin Zhu and Xinming Zhang and Xiao-Yu Zhang and Ziyu Xue and Lei Wang},
keywords = {Semantic segmentation, Generative adversarial network (GAN), Wasserstein distance, Auxiliary higher-order potential loss},
abstract = {Semantic segmentation plays an important role in a series of high-level computer vision applications. In the state-of-the-art semantic segmentation methods based on fully convolutional neural networks, all label variables are predicted independently from each other, and the restricted field-of-views of the convolutional filters are difficult to capture the long-range information. In this paper, a novel post-processing method based on GAN (Generative Adversarial Network) is explored to reinforce spatial contiguity in the output label maps. With the help of fully connected layers in the discriminator, the GAN can capture the long-range information, and provide an auxiliary higher-order potential loss to the segmentation model, thus the segmentation model has the ability of correcting higher order inconsistencies. Furthermore, the optimization scheme in Wasserstein GAN (WGAN) is adopted to the training process of our model to get better performance and stability. Extensive experiments on public benchmarking database demonstrate the effectiveness of the proposed method.}
}
@article{2020102886,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {71},
pages = {102886},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(20)30131-0},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301310}
}
@article{LI201912,
title = {Joint image encryption and compression schemes based on 16 × 16 DCT},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {12-24},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.018},
url = {https://www.sciencedirect.com/science/article/pii/S104732031830292X},
author = {Peiya Li and Kwok-Tung Lo},
keywords = {Image encryption, JPEG compression, 16  16 DCT, Statistical attack},
abstract = {Joint image encryption and compression schemes have shown their great potential values in protecting compressed images. To achieve the protection, a trade-off between encryption power and compression ability needs to be considered. In this paper, we propose two new joint encryption and compression schemes, where one scheme emphasizes compression performance, another highlights protection performance. For a given plain-image, we first raster scan it into non-overlapping 16 × 16 blocks, then apply various encryption techniques to it. In the first scheme, encryption operations are conducted at the transformation stage and quantization stage of JPEG. As for the second scheme, we add the run/size and value (RSV) pairs’ shuffling operation at JPEG’s entropy coding stage after first scheme’s encryption operations. Performance evaluations using various criteria are conducted to show that the first scheme has better compression efficiency, while the second scheme has better defense ability against the statistical attack.}
}
@article{ZHANG2019277,
title = {Super-resolution of single multi-color image with guided filter},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {277-284},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.040},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303195},
author = {Qinglin Zhang and Bingling Chen and Xuan Lu and Qiaoqiao Xia},
keywords = {Super-resolution, Multi-color image, Guided filter, Chromatic channel},
abstract = {Super-resolution reconstruction is a method that can transcend the limitations of optical imaging systems through the use of image processing algorithms. Recent techniques of super-resolution for single monochrome images develop rapidly, but for single multi-color images, to efficiently apply the monochrome super-resolution algorithms to all channels is still under exploration. In most of the recent research, the chromatic channels are simply upscaled by interpolation, which leads to the quality of the chromatic channels downgraded. This application may not be noticed by the visual systems of humans, but can affect other algorithms when super-resolution plays roles at image pre-processing. In this paper, we present a novel approach for multi-color super-resolution reconstruction. Using the super-resolution reconstructed luminance channel as the guide image, we adopt guided filters to manage the interpolated chromatic channels. Guided filters retain the sharp edges and fine details from the guided image and carry them to the output images. Meanwhile the whole process is quite computationally economic. Extensive experiments on natural images show that our method achieves better results than the method that is used in most of the algorithm in the literature in both statistic and visual aspects.}
}
@article{WANG2019210,
title = {A survey of recent work on fine-grained image classification techniques},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {210-214},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.049},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303754},
author = {Yafei Wang and Zepeng Wang},
keywords = {Image classification, Deep learning, Convolutional neural networks},
abstract = {Image classification is a hot topic in image processing. Image classification aims to automatically classify large numbers of images. Many methods have been proposed for solving this task. Traditional methods usually leverage low-level features. Clustering is the most commonly used method of image classification. In recent years, convolutional neural networks (CNNs) is widely used in extracting deep features. Many network architectures are proposed for image classification, such as ResNeXt, Cifar10. These deep learning methods aims at fusing features of texture, color and segmentation. In this paper, we discuss the different methods and techniques of image classification, and made a detailed summary of their performance. We believe that our work plays an important role in the field of image classification.}
}
@article{GUAN2019675,
title = {Graph-based supervised discrete image hashing},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {675-687},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.025},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303511},
author = {Jian Guan and Yifan Li and Jianguo Sun and Xuan Wang and Hainan Zhao and Jiajia Zhang and Zechao Liu and Shuhan Qi},
keywords = {Discrete optimization, Graph regularization, Image retrieval, Supervised discrete hashing},
abstract = {Learning based hashing have been widely adopted to the approximate nearest neighbour search in large-scale image retrieval. However, how to preserve the semantic information in hashing embedding is still a challenge problem. Moreover, most of the existing methods employ the relaxation strategy to solve discrete constraint problem, which may accumulate binary quantization error as the coding length increases. In this paper, we propose a graph-based supervised hashing framework to address these problems, where the semantic information is preserved from two aspects. On one hand, we employ a supervised learning model to keep the semantic consistency. On the other hand, the intrinsic manifold structure is captured by a graph-based model. In addition, to reduce the quantization error, we adopt a discrete optimization strategy to replace the relaxation one. Experiments conducted on three benchmark datasets to demonstrate the effectiveness of the proposed method.}
}
@article{SHEN2019712,
title = {Multi-task multimodal feature refinement for emotional speech animation},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {712-716},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.043},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303237},
author = {Jinqing Shen and Yunzhong Yu and Chongbiao Zhang and Yongming Xu and Feiwei Li and Yiyang Yao},
keywords = {Multimodal feature refinement, Emotional speech animation},
abstract = {Emotional human facial animation has become an indispensable technique in a series of multimedia systems. The technique first generates the phoneme and emotion sequences. Then, the viseme/expression sequences are calculated accordingly, which are further converted into a coherent facial animation video. In this work, a completely automatic system is designed by selecting acoustic features discriminative to both emotion and phoneme tags. More specifically, acoustic features highly representative to both emotion and phoneme tags are selected under a multi-task learning framework. Based on this, speech phoneme and emotion sequences are effectively calculated. Then, an active learning algorithm is developed to discover the key facial frames representative to both the phoneme and emotion tags. Finally, we associate each phoneme + emotion tuple with a key facial frame. And a popular morphing algorithm is employed to fit them into a coherent animation video. Experimental results have demonstrated that our generated facial animation video is natural, coherent, and highly synchronized with the input speech.}
}
@article{WU2019554,
title = {A deep generative directed network for scene depth ordering},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {554-564},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.034},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303596},
author = {Kewei Wu and Yang Gao and Hailong Ma and Yongxuan Sun and Tingting Yao and Zhao Xie},
keywords = {Deep generative directed-network, Depth ordering, Hidden Markov field, DenseNet},
abstract = {In this paper, we present a Deep Generative Directed-Network (DGDN), which estimates the occlusion relationship of boundaries. Specially, we use a low-level segmentater to partition the image into regions, then estimate their occlusion relationship by two perceptual depth cues. We decompose our DGDN model into three sub-modules to extract regional appearance cue, edgel orientation cue and to further infer global occlusion relationship with these cues, respectively. Firstly, we predict regional scene depth by a upsampling deep dense network (DenseNet). Secondly, we simultaneously estimate edgel occlusion with logistic regression. However, the occlusion relationship always suffers from unexpected conflicts due to noisy regional and edgel cues. Therefore, we finally infer occlusion relationship in a Hidden Markov Field (HMF), which tackles conflicts with bi-direction inference and the HMF parameters are exploited by iterative EM-like procedure. Ablation experiments on NYUv2 and Make3D database prove that our DGDN model outperforms state-of-the-art methods.}
}
@article{KUMAR2019345,
title = {EVS-DK: Event video skimming using deep keyframe},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {345-352},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303353},
author = {Krishan Kumar},
keywords = {Clustering, Deep learning, Event summarization, Highly connected subgraph, Key-frames, Video, Graph},
abstract = {In this automation era, video surveillance becomes an essential component and omnipresent at ATMs, public places, airports, railways, roadways, etc. There are many challenges to store and access such massive data generated by video surveillance. Therefore, a novel technique is required to manage the comprehensive view of the content. In this work, we propose an event summarization technique using Deep learning framework for monocular videos. A spatiotemporal similarity function is developed to construct a similarity matrix based on the visual features. Video frames are represented by the sparse matrix as graph vertices based on an objective function, where Highly Connected Subgraphs (HCS) are constructed as clusters. Finally, events are obtained from such clusters assuming that the centroid of the cluster is a key-frame of the event. Consequently, this approach does not require assumption to determine the number of clusters. Due to this advantage, users can select the number of keyframes without incurring an extra computational cost. Experimental results on two benchmark datasets show that the proposed model outperforms the state-of-the-art models on Precision and F-measure and also cover the major contents of the original video.}
}
@article{ZHENG2019380,
title = {A survey on image tampering and its detection in real-world photos},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {380-399},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.022},
url = {https://www.sciencedirect.com/science/article/pii/S104732031830350X},
author = {Lilei Zheng and Ying Zhang and Vrizlynn L.L. Thing},
keywords = {Image tampering detection, Image forgery detection, Image forensics, Image copy-move detection, Image splicing detection},
abstract = {Editing a real-world photo through computer software or mobile applications is one of the easiest things one can do today before sharing the doctored image on one’s social networking sites. Although most people do it for fun, it is suspectable if one concealed an object or changed someone’s face within the image. Before questioning the intention behind the editing operations, we need to first identify how and which part of the image has been manipulated. It therefore demands automatic tools for identifying the intrinsic difference between authentic images and tampered images. This survey provides an overview on typical image tampering types, released image tampering datasets and recent tampering detection approaches. It presents a distinct perspective to rethink various assumptions of tampering clues behind different detection approaches. And this further encourages the research community to develop general tampering localization methods in the future instead of adhering to single-type tampering detection.}
}
@article{LI2019374,
title = {Adversarial learning for viewpoints invariant 3D human pose estimation},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {374-379},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318302943},
author = {Yimeng Li and Jun Xiao and Di Xie and Jian Shao and Jinlong Wang},
keywords = {3D pose estimation, Adversarial learning},
abstract = {2D pose estimation have achieve remarkable performance with deep convolutional neural networks. However 3D pose estimation is current constrained by the limited datasets of 3D annotations. Meanwhile most annotated images are captured using Motion Capture system in lab or certain studio, which has large variations with large-scale monocular 2D pose datasets. We propose an adversarial learning framework, which can learn invariant human pose latent from 3D annotated datasets to optimize the estimation of monocular images with only 2D annotations. However there is large difference in observation coordinates between 2D datasets and 3D datasets, and this viewpoints issue should be separated from invariant pose latent. We add a viewpoints invariant module to automatically regulate observation viewpoints for generated 3D pose, which transforming the generated pose to more suitable observation in the 3D datasets. Our method achieve competitive results on both 2D and 3D benchmarks.}
}
@article{MUN2019688,
title = {Edge-enhancing bi-histogram equalisation using guided image filter},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {688-700},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.037},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303638},
author = {Junwon Mun and Yuneseok Jang and Yoojun Nam and Jaeseok Kim},
keywords = {Histogram equalisation, Contrast enhancement, Brightness preservation, Edge enhancement, Guided image filter},
abstract = {Histogram equalisation (HE) is a simple and effective contrast enhancement method. However, it has certain drawbacks, namely, brightness inconsistency, over-enhancement, and noise amplification. In addition, there is structure information loss while processing HE. To overcome those drawbacks simultaneously, we propose a novel edge enhancing bi-histogram equalisation method using guided image filter. In the proposed algorithm, a new adaptive plateau limit and a new edge-enhancing transformation function are proposed. The adaptive plateau limit makes the method robust to various histogram distributions, and the edge-enhancing transformation enhances edges while suppressing noise amplification in the flat region. The performance of the various HE algorithms are evaluated both quantitatively and qualitatively. The qualitative assessment shows that the proposed algorithm avoids over-enhancement and noise amplification, effectively. In addition, the quantitative metrics show that the proposed algorithm outperforms the existing HE algorithms in terms of local contrast, discrete entropy, and perceptual sharpening index.}
}
@article{YU201925,
title = {Adaptive perceptual quantizer for high dynamic range video compression},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {25-36},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S104732031830289X},
author = {Shengtao Yu and Cheolkon Jung},
keywords = {HEVC, Adaptive transfer function, High dynamic range, Perceptual quantizer, Perceptual uniformity},
abstract = {Although high dynamic range (HDR) videos are a very fascinating way to represent real-world scenes, they need a huge amount of memory to store and transmit due to the high bit-depth. Thus, it is a major challenge for HDR video coding to efficiently compress them without sacrificing perceptual quality. Perceptual Quantizer (PQ) transfer function provides a solution to this problem, which is adopted as the HEVC Main 10 Profile-based Anchor. However, PQ is not adaptive to HDR contents, thus reducing the coding efficiency. In this paper, we propose adaptive transfer function based on PQ for HDR video compression, called adaptive PQ. Different from PQ which uses a fixed mapping curve from luminance to luma, the proposed transfer function adaptively maps luminance to luma according to HDR contents. Thus, adaptive PQ is able to efficiently utilize possible luma values. Moreover, adaptive PQ achieves better perceptual uniformity in the luminance range than PQ. Experimental results demonstrate that adaptive PQ achieves a significant performance improvement in HDR video coding over PQ in terms of visual quality and bitrate.}
}
@article{XU2021103083,
title = {Retraction notice to “An iterative propagation based co-saliency framework for RGBD images” [J. Vis. Commun. Image Represent. 59 (2019) 186–194]},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103083},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103083},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000481},
author = {Haibo Xu and Ge Zhang and Qingming Zhang}
}
@article{YEH2019462,
title = {Fast prediction for quality scalability of High Efficiency Video Coding Scalable Extension},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {462-476},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303481},
author = {Chih-Hsuan Yeh and Jie-Ru Lin and Mei-Juan Chen and Chia-Hung Yeh and Cheng-An Lee and Kuang-Han Tai},
keywords = {High Efficiency Video Coding Scalable Extension, SHVC, Quality scalability, Fast decision, Inter-layer prediction},
abstract = {In response to the increased demand for high-resolution video, the new generation of video standards, High Efficiency Video Coding (HEVC) and its scalable extension (SHVC) have been finalized. The compression of HEVC/SHVC is efficiently improved and supports ultra-high resolution (UHD). Therefore, the coding complexity of HEVC/SHVC is much higher than those of previous standards. The framework of SHVC is based on HEVC and is divided into several types of scalable video. SHVC can be decoded into various video resolutions, frame rates and qualities, and only needs to be encoded once, but with higher complexity than HEVC. Thus, how to reduce the coding complexity of SHVC is the purpose of this paper. Our proposed algorithm accelerates the enhancement layer (EL) prediction by utilizing encoded Coding Unit (CU) sizes, prediction modes, motion vectors and Rate-Distortion Costs (RD-Costs) of the base layer (BL) and encoded CU sizes of the enhancement layer for quality scalability of SHVC. Experimental results show that the proposed algorithm can save lots of time while maintaining good video quality, and the performance is better than those of previous works.}
}
@article{VIJAYAKUMAR2019619,
title = {T2FCS filter: Type 2 fuzzy and cuckoo search-based filter design for image restoration},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {619-641},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S104732031830347X},
author = {Sagenela {Vijaya Kumar} and C. Nagaraju},
keywords = {Cuckoo search optimization algorithm, Type II fuzzy system, Image de-noising and restoration, Second derivative like measure of enhancement},
abstract = {A novel filter design for the restoration of the corrupted digital image is proposed in this paper. The filter design incorporates type II fuzzy system and cuckoo search optimization algorithm (T2FCS) based filter design for the restoration of the noise in the images. The noisy pixels in the images are detected using the proposed circular based searching scheme and the detected corrupt pixels are removed using the cuckoo search algorithm. The enhanced pixels in place of the corrupt pixels are acquired using the proposed type II fuzzy system. The proposed filter adapts to various noisy conditions such as random noise, salt and pepper noise and scratch noise. The experimentation of the proposed filter design is carried out over two images. The performance of the proposed T2FCS filter design is compared over the existing image restoration algorithms using metrics; Peak Signal to Noise ratio (PSNR), Structural Similarity Index (SSIM), Second Derivative Like Measure of Enhancement (SDME). The result obtained favours the performance of the proposed filter in the restoration of the noisy images.}
}
@article{LIU2019316,
title = {Multi-vision tracking and collaboration based on spatial particle filter},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {316-326},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.050},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303766},
author = {Long Liu and Zhaohui Xi and Qiang Sun},
keywords = {Visual tracking, Particle filter, Collaboration, Epipolar line, Homography},
abstract = {In existing multi-vision tracking methods, a distributed collaborative tracking mode based on homography constraints is often adopted, yet there are significant shortcomings to this approach. For example, visual information complementation is not used to improve the robustness of tracking, and collaborative tracking is limited by homography constraints. In this study, a three-dimensional spatial particle filter tracking method was proposed, and multi-vision joint tracking and collaboration were effectively achieved. This method was based on the existing particle filter framework. A two-dimensional plane particle was taken as the projection of a three-dimensional spatial particle on the imaging plane, and the formula for calculating a spatial particle’s weight was derived based on Bayesian posterior probability recursion. In addition, an approximation method to determine spatial particle weight was given. The resampling of spatial particles was performed by using an epipolar line resampling method, and a collaborative tracking mechanism was established based on the concept of resolution. The results showed that the proposed method had higher tracking precision and anti-occlusion performance than other existing methods. In this method, the robustness of tracking was effectively improved, and unlimited optimization cooperation between visual sensing was achieved.}
}
@article{CHEN2019261,
title = {3D object retrieval with graph-based collaborative feature learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {261-268},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.046},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303249},
author = {Feng Chen and Bo Li and Liang Li},
keywords = {3D Object retrieval, Collaborative feature learning, Hypergraph learning, Bipartite graph matching},
abstract = {3D object retrieval has attractive extensive research focus in recent years. Among various schemes, view based 3D object retrieval is regarded as a promising direction. In this paper, we present a novel view-based 3D object retrieval framework, which is deployed over a graph-based collaborative learning scheme to intelligently fuse multiple features. In particular, we introduce a hypergraph based collaborative feature learning scheme to fuse complement descriptors from both the contour and the interior region of 3D object effectively. Then, the view-based 3D object retrieval is done via a greedy bipartite graph matching algorithm, which achieves highly accurate and efficient 3D object matching. With the above bipartite graph matching and feature concatenation, significant performance improvement is achieved in the 3D object retrieval task, on either widely-used benchmark datasets or open competitions like SHREC15 challenge. In both evaluations, the proposed graph-based collaborative feature learning scheme has beaten a serial of existing approaches and state-of-the-art schemes.}
}
@article{TIAN2019544,
title = {Heterogeneous multimedia cooperative annotation based on multimodal correlation learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {544-553},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.028},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303523},
author = {Feng Tian and Quge Wang and Xin Li and Ning Sun},
keywords = {Multimedia annotation, Cooperative annotation, Multimodal correlation learning},
abstract = {Rich multimedia contents are dominating the current Web. In popular social media platforms such as FaceBook, Twitter, and Instagram, there are over millions of multimedia contents being created by users. In the meantime, multimedia data consists of data in multiple modalities, such as text, images, videos, audio, time series sequences, and so on. Many research efforts have been devoted to multimedia annotation to further improve the performance. However, the prevailing methods are designed for single-media annotation task. In fact, heterogeneous media content describes given labels from respective modality and is complementary to each other, and it becomes critical to explore advanced techniques for heterogeneous data analysis and multimedia annotation. Inspired by this idea, this paper presents a new multimodal correlation learning method for heterogeneous multimedia cooperative annotation, named unified space learning, which projects heterogeneous media data into one unified space. We formulate the multimedia annotation task into a semi-supervised learning framework, in which we learn different projection matrices for different media type. By doing so, different media content is aligned cooperatively, and jointly provides a more complementary profile of given semantic labels. Experimental results on data set with images, audio clips, videos and 3D models show that the proposed approach is more effective.}
}
@article{CHEN2019112,
title = {A multiscale Galerkin method for second-order boundary value problems of Fredholm integro-differential equation II: Efficient algorithm for the discrete linear system},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {112-118},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.027},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303006},
author = {Jian Chen and Minfan He and Taishan Zeng},
keywords = {Multiscale Galerkin method (MGM), Multilevel augmentation method (MAM), Boundary value problems, Fredholm integro-differential equation},
abstract = {A multiscale Galerkin method (MGM) was proposed recently by the same authors in order to solve second-order boundary value problems of Fredholm integro-differential equation. Although, the numerical solution of MGM is always stable because of the multiscale bases properties, obligatory of considerable computational cost and huge memory for achieving great approximation accuracy, are the main draw backs. To overcome MGM problems, in this paper, a new multilevel augmentation method (MAM) in order to solve discrete linear system is proposed. Applying the special matrix splitting techniques, approximate solution is obtained by (1) solving a linear system only at an initial lower level; (2) compensating the error by directly computing the product of matrices and vectors at the higher level without any iterations. Theoretical and experimental results approve that MAM and MGM have similar and optimum convergence orders, though MAM is more efficient than MGM.}
}
@article{HUANG2019428,
title = {Sharing hand gesture and sketch cues in remote collaboration},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {428-438},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303365},
author = {Weidong Huang and Seungwon Kim and Mark Billinghurst and Leila Alem},
keywords = {Hand gestures, Multimodal communication, Remote collaboration, Physical task, Sketches},
abstract = {Many systems have been developed to support remote guidance, where a local worker manipulates objects under guidance of a remote expert helper. These systems typically use speech and visual cues between the local worker and the remote helper, where the visual cues could be pointers, hand gestures, or sketches. However, the effects of combining visual cues together in remote collaboration has not been fully explored. We conducted a user study comparing remote collaboration with an interface that combined hand gestures and sketching (the HandsInTouch interface) to one that only used hand gestures, when solving two tasks; Lego assembly and repairing a laptop. In the user study, we found that (1) adding sketch cues improved the task completion time, only with the repairing task as this had complex object manipulation but (2) using gesture and sketching together created a higher task load for the user.}
}
@article{WANG201984,
title = {Facial expression recognition from image based on hybrid features understanding},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {84-88},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318302839},
author = {Fengyuan Wang and Jianhua Lv and Guode Ying and Shenghui Chen and Chi Zhang},
keywords = {Facial expression recognition, Convolutional neural networks, Scale-invariant feature transform, Deep-learning feature, Support vector machines},
abstract = {Facial expression recognition (FER) plays an important role in the applications of human computer interaction. Given the wide use of convolutional neural networks (CNNs) in automatic video and image classification systems, higher-level features can be automatically learned from hierarchical neural networks with big data. However, learning CNNs require large amount of training data for adequate generalization, while the Scale-invariant feature transform (SIFT) does not need large training samples to generate useful feature. In this paper, we propose a new hybrid feature representation for the recognition of facial expressions from a single image frame that uses a combination of SIFT and deep-learning feature of different level extracted from the CNN model, then adopt the combined features and classify the expression by support vector machines (SVM). The performance of the proposed method has been validated on public CK+ databases. To evaluate the generalization ability of our method, we also performed an experiment on a cross-database environment. Experimental results show that the proposed approach can achieve better classification rates compared with state-of-art CNN methods, which indicate the considerable potential of combining shallow feature with deep-learning feature.}
}
@article{SHAN2019642,
title = {Influence of CT scanning parameters on rock and soil images},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {642-650},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.014},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303407},
author = {Pengfei Shan and Xingping Lai},
keywords = {Digital image processing, Relative standard deviation, Parameters, Geotechnical CT image},
abstract = {Geotechnical mechanical testing machine is an important means to study the characteristics of rock and soil rupture, which is of great significance in shale gas exploitation, nuclear waste disposal and earthquake prediction. For the convenience of research, the complex structure of rock and soil is often neglected, and the geotechnical material is regarded as a macro continuum. On this basis, a new method is used, X-ray CT scale cracks, crack size is larger than the micro-scale cracks, the number of cracks is less, but geotechnical CT images can still show the crack initiation location, propagation path, through the process, cracks and the relationship between aggregate mortar. When CT-scale microcracks can be found, the length of microcracks is equal to the magnitude of aggregate-scale, and can be compared with numerical simulation results. In this paper, four different kinds of soil samples are selected to design relevant tests. The specific effects of CT scanning parameters on CT images of rock and soil samples are studied by direct and indirect methods combined with CT number curves under different scanning conditions. The results show that the scanning voltage and filtering function have great influence on CT images and CT numbers of rock and soil samples. The enhancement or inhibition of the filtering function to the geotechnical CT image depends on the property of the selected filtering function, but has nothing to do with the soil quality of the sample. Finally, the selection principle of the CT scanning parameters is given. With the help of reasonable CT scanning parameters, the quality of the geotechnical CT image can be improved and the relatively accurate geotechnical CT value can be obtained.}
}
@article{LIU201946,
title = {Multi-cue fusion: Discriminative enhancing for person re-identification},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {46-52},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.023},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318302955},
author = {Yongge Liu and Nan Song and Yahong Han},
keywords = {Deep learning, Fusion strategy, Re-identification},
abstract = {Person re-identification is an emerging research field in computer vision. Our paper aims to study how to improve the discrimination of person features. We find that some peculiarities of people have not been better attention in the semantic features of deep learning. However, some features obtained by traditional methods can better express the color, and these features are an important clue for re-identification. Therefore, in this paper, we combine traditional Gaussian features with deep semantic features to enhance the discrimination of overall features. At last, we have achieved good performance on two public datasets (Market1501 and VIPeR) in three main distance method learning (DML). In addition, we applied this model to the task of vehicle re-identification. Experiments show that our method has a great improvement on the VeRi vehicle dataset. We compare the results with the current high level results, which indicates the effectiveness of our model.}
}
@article{CHEN2019486,
title = {Person re-identification based on re-ranking with expanded k-reciprocal nearest neighbors},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {486-494},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.044},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303250},
author = {Ying Chen and Jin Yuan and Zhiyong Li and Yiqiang Wu and Mourad Nouioua and Guoqi Xie},
keywords = {Person re-identification, Re-ranking, Expanded k-reciprocal neighbors, Rank list similarity},
abstract = {In this paper, a robust re-ranking method based on expanded k-reciprocal neighbors is proposed. Our method assumes that if a gallery image is the probe image of the expanded k-reciprocal nearest neighbors, these images are more likely to be of the same person. Specifically, given a probe image, we replace the probe with its expanded reciprocal nearest neighbor and the final distance is computed by the mean value of the corresponding neighbor set. The proposed method is unsupervised, automatic and applicable to other person re-identification problems. Moreover, our method can perform well even with a simple direct rank list where the Euclidean distance was used to compute the distances between the images. Experiments on many public datasets demonstrate the effectiveness and robustness of our re-ranking method. The proposed method achieves 4.9% improvement in Rank-1 on the CUHK03 dataset and a significant improvement of 18.6% in mAP on the Duke dataset.}
}
@article{CHEN2019334,
title = {High-capacity reversible data hiding in encrypted images based on extended run-length coding and block-based MSB plane rearrangement},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {334-344},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.023},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303493},
author = {Kaimeng Chen and Chin-Chen Chang},
keywords = {Reversible data hiding, Run-length coding, Image encryption},
abstract = {In this paper, we propose a novel reversible data hiding method in encrypted images. The proposed method takes full advantage of the spatial correlation in the original images to vacate room for embedding data before image encryption. By jointly using an extended run-length coding and a block-based most significant bit (MSB) plane rearrangement mechanism, the MSB planes of images can be compressed efficiently to generate room for high-capacity embedding. The receiver can extract data directly from encrypted images with only the data hiding key, and the original image or the high-quality plain image that contains secret data can be recovered with only the encryption key. The experimental results prove that the proposed method can reach a high embedding rate and a high PSNR.}
}
@article{FANG201933,
title = {Adaptively feature learning for effective power defense},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {33-37},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300033},
author = {Jinghui Fang and Weijie Qian and Zhijun Zhao and Yiyang Yao and Zhen Wen},
keywords = {Intelligent video surveillance, Active defense technology, Key frames},
abstract = {Active defense technology is very important in intelligent systems and video surveillance. In some important fields, active defense system can effectively find intruders. Many intelligent video surveillance systems were proposed in recent years. They achieved good performance to some extent. Since power station is an important field, it is important to develop an intelligent video surveillance. Considering that detecting the whole surveillance video is time consumption and computation. So in this paper, we propose an active defense system to find intruders automatically. First, a key frame selection algorithm based on adaptive features is presented to select key frames. These key frames can cover the main content of videos and detecting these key frames can also reduce time consumption and computation. Then, a probabilistic model is proposed to learn the training data distribution. Finally, our system can achieve active defense based on probabilistic model. Experimental results show that our active defense system can achieve finding intruders effectively.}
}
@article{XU2019477,
title = {Scene graph captioner: Image captioning based on structural visual representation},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {477-485},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.027},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303535},
author = {Ning Xu and An-An Liu and Jing Liu and Weizhi Nie and Yuting Su},
keywords = {Image captioning, Scene graph, Structural representation, Attention},
abstract = {While deep neural networks have recently achieved promising results on the image captioning task, they do not explicitly use the structural visual and textual knowledge within an image. In this work, we propose the Scene Graph Captioner (SGC) framework for the image captioning task, which captures the comprehensive structural semantic of visual scene by explicitly modeling objects, attributes of objects, and relationships between objects. Firstly, we develop an approach to generate the scene graph by learning individual modules on the large object, attribute and relationship datasets. Then, SGC incorporates high-level graph information and visual attention information into a deep captioning framework. Specifically, we propose a novel framework to embed a scene graph into the structural representation, which captures the semantic concepts and the graph topology. Further, we develop the scene-graph-driven method to generate the attention graph by exploiting high internal homogeneity and external inhomogeneity among the nodes in the scene graph. Finally, a LSTM-based framework translates these information into text. We evaluate the proposed framework on a held-out MSCOCO dataset.}
}
@article{ZHENG201953,
title = {Joint residual pyramid for joint image super-resolution},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {53-62},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.028},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303018},
author = {Yan Zheng and Xiang Cao and Yi Xiao and Xianyi Zhu and Jin Yuan},
keywords = {Deep learning, Neural convolutional pyramid, Joint super-resolution, Residual block},
abstract = {Joint image super-resolution refers to methods to enhance the resolution of an image with the guidance of a higher resolution image. It is similar to image completion, which is shown to benefit from larger receptive fields in recent deep neural network based methods. However, larger receptive fields increase the depths and parameters of the network, which may cause degradation and large memory consumption. To this end, we propose a joint residual pyramid model by introducing residual blocks and linear interpolation layers into the convolutional neural pyramid (CNP), and adopting the CNP in the joint super-resolution framework. Our model consists of three sub-networks, two for feature extraction concatenated by another for image reconstruction. Experimental results show that our model outperforms existing state-of-the-art algorithms not only on data pairs of RGB/depth images, but also on data pairs like color/saliency and color-scribbles/colorized images, without significantly sacrificing computation efficiency and memory space.}
}
@article{2021103102,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {76},
pages = {103102},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(21)00062-6},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000626}
}
@article{ZHANG2019600,
title = {Binary image steganography based on joint distortion measurement},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {600-605},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.038},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303651},
author = {Junhong Zhang and Wei Lu and Xiaolin Yin and Wanteng Liu and Yuileong Yeung},
keywords = {Binary image steganography, Distortion measurement, Local texture pattern},
abstract = {Most state-of-the-art binary image steganography methods depend on the content of the image to determine where to embed secret messages, which is capacity-limited and indicates that their distortion measurement may be not precise enough. In this paper, we propose a kind of distortion measurement that is not only based on the discrimination effects after flipping the pixels but also depends on the visual effects of flipping corresponding pixels, which is called joint distortion measurement. Instead of selecting suitable position to embed secret messages, we then employ the syndrome-trellis code to minimize the embedding distortion and get messages embedded. And experimental results have demonstrated that the proposed distortion measurement has higher performance and the steganography scheme can achieve stronger statistical security with high capacity and image quality.}
}
@article{HU201937,
title = {Track circuit fault prediction method based on grey theory and expert system},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {37-45},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.10.024},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318302682},
author = {Li-Qiang Hu and Chao-Feng He and Zhao-Quan Cai and Long Wen and Teng Ren},
keywords = {Track circuit, Fault prediction, Grey theory, Expert system},
abstract = {Due to the lack of accurate state judgment and health analysis of equipment operation, track circuit implements the repair and maintenance strategy of fault repair or planned repair. For this reason, a novel track circuit fault prediction method is proposed based on grey theory and expert system. In the proposed method, the feature of grey prediction model is to establish dynamic differential equation and then predict its own development according to its own data. The dynamic prediction model with equal dimension is applied to improve original grey model. Based on the gray models, the expert system is used to simulate human experts to solve the problems in a professional field. It contains man-computer interface, inference engine, knowledge library, knowledge management system, interpretation module and dynamic database. The measurement data show this system can effectively predict several typical faults of HVAP track circuit, and prove the proposed system structure is effective. Such condition-based fault prognostic maintenance mechanism provides an effective solution to improve equipment maintenance efficiency, reduce maintenance cost and reduce equipment fault rate.}
}
@article{SUI201994,
title = {Image processing analysis and research based on game animation design},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {94-100},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303377},
author = {Kun Sui and Won-Hyung Lee},
keywords = {Image processing, Game animation, Visual expression, Human-machine interface},
abstract = {With the development of the game industry, and the new electronic consumption chain to promote the development of the game industry model continues to strengthen. Therefore, it is more necessary to make good use of good technology to do a good job of games. From the momentum of the development of mobile terminal games in recent years, the game industry has become a new economic growth point in China's economic transformation. With the development of computer and Internet, graphics and image processing technology has entered another unprecedented stage. In recent years, different image processing techniques have been used to process and analyze the game interface. Appropriate image processing methods include image enhancement, image binarization, image edge detection and image feature extraction. In this paper, by changing the mapping function from priority to probability and comparing the single mapping function of the previous algorithm, the mapping function of playback learning with higher probability of the important priority playback unit is found. In the experiment, the intuitive model strategy analysis of the improved algorithm is carried out firstly. Then the choice of CNN network layer structure, cost function analysis, efficiency analysis and game score comparison of each algorithm are carried out. Finally, the test results show that the new algorithm in this paper can make more effective decisions in video games and achieve the goal of winning higher scores and spending less time.}
}
@article{LI2019149,
title = {Learning target-aware correlation filters for visual tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {149-159},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.036},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303134},
author = {Dongdong Li and Gongjian Wen and Yangliu Kuai and Jingjing Xiao and Fatih Porikli},
keywords = {Correlation filter, Target likelihood map, Visual tracking},
abstract = {Discriminative Correlation Filters (DCF) have achieved enormous popularity in the tracking community. Generally, DCF based trackers assume that the target can be well shaped by an axis-aligned bounding box. Therefore, in terms of irregularly shaped objects, the learned correlation filter is unavoidably deteriorated by the background pixels inside the bounding box. To tackle this problem, we propose Target-Aware Correlation Filters (TACF) for visual tracking. A target likelihood map is introduced to impose discriminative weight on filter values according to the probability of this location belonging to the foreground target. According to the TACF formulation, we further propose an optimization strategy based on the Preconditioned Conjugate Gradient method for efficient filter learning. With hand-crafted features (HOG), our approach achieves state-of-the-art performance (62.8% AUC) on OTB100 while running in real-time (24 fps) on a single CPU. With shallow convolutional features, our approach achieves 66.7% AUC on OTB100 and the top rank in EAO on the VOT2016 challenge.}
}
@article{WANG2019525,
title = {Research on feature extraction algorithm for plantar pressure image and gait analysis in stroke patients},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {525-531},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303419},
author = {Mo Wang and Xin'an Wang and Zhuochen Fan and Fei Chen and Sixu Zhang and Chen Peng},
keywords = {Plantar pressure, Feature extraction, Image denoising, Clustering analysis, Gait analysis},
abstract = {The plantar pressure image is an important tool for gait analysis. It has important applications in evaluating the recovery of stroke patients after operation and formulating the rehabilitation training program. It is one of the key technologies of gait analysis to extract foot feature parameters from static/dynamic plantar pressure images. This article deals with the noise in the original image through the piecewise linear grayscale transformation, the time domain mean filter and the maximum value filter, then determine the position of the feet in the image by the foot localization algorithm based on the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) and the K-means clustering method. Finally, the plantar pressure feature parameters were extracted according to the positioned images. Based on the above feature parameter extraction algorithm, the plantar pressure feature parameters of 20 healthy subjects and 20 S patients with relative recovery period (2–6 months after the onset) were compared, showing a statistically significant difference (P < 0.001). Based on the above data, gait characteristics of stroke patients were further analyzed.}
}
@article{HU2019176,
title = {Video facial emotion recognition based on local enhanced motion history image and CNN-CTSLSTM networks},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {176-185},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.039},
url = {https://www.sciencedirect.com/science/article/pii/S104732031830364X},
author = {Min Hu and Haowen Wang and Xiaohua Wang and Juan Yang and Ronggui Wang},
keywords = {Video emotion recognition, Motion history image, LSTM, Facial landmarks},
abstract = {This paper focuses on the issue of recognition of facial emotion expressions in video sequences and proposes an integrated framework of two networks: a local network, and a global network, which are based on local enhanced motion history image (LEMHI) and CNN-LSTM cascaded networks respectively. In the local network, frames from unrecognized video are aggregated into a single frame by a novel method, LEMHI. This approach improves MHI by using detected human facial landmarks as attention areas to boost local value in difference image calculation, so that the action of crucial facial unit can be captured effectively. Then this single frame will be fed into a CNN network for prediction. On the other hand, an improved CNN-LSTM model is used as a global feature extractor and classifier for video facial emotion recognition in the global network. Finally, a random search weighted summation strategy is conducted as late-fusion fashion to final predication. Our work also offers an insight into networks and visible feature maps from each layer of CNN to decipher which portions of the face influence the networks’ predictions. Experiments on the AFEW, CK+ and MMI datasets using subject-independent validation scheme demonstrate that the integrated framework of two networks achieves a better performance than using individual network separately. Compared with state-of-the-arts methods, the proposed framework demonstrates a superior performance.}
}
@article{2020102938,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {72},
pages = {102938},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(20)30168-1},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301681}
}
@article{ZHANG20191,
title = {Supervised graph regularization based cross media retrieval with intra and inter-class correlation},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {1-11},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318302967},
author = {Meijia Zhang and Huaxiang Zhang and Junzheng Li and Li Wang and Yixian Fang and Jiande Sun},
keywords = {Cross media retrieval, Subspace learning, Supervised graph regularization},
abstract = {With the rapid development of internet technology, mining and retrieving the information from internet accurately is an urgent problem, among which, cross media retrieval becomes a hot spot of current research. This paper proposes a cross media retrieval approach, which learns two couples of projections based on different retrieval tasks. We first learn a common subspace to project heterogeneous media data to the isomorphic subspace, to measure the similarity of the heterogeneous media data in the isomorphic subspace. Second, we build isomorphic and heterogeneous adjacent graphs to preserve the correlations of the cross media data. Then we combine the two processes together to learn a common subspace. We also consider intra-class and inter-class similarity of images or texts in the unified framework. Third, the L2 norm is used to perform feature selection for different media data. Experimental results on three datasets demonstrate the effectiveness of the proposed approach.}
}
@article{2021103156,
title = {Editorial Board},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103156},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/S1047-3203(21)00098-5},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000985}
}
@article{YIN201959,
title = {Perceptually learning multi-view sparse representation for scene categorization},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {59-63},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300021},
author = {Weibin Yin and Dongsheng Xu and Zheng Wang and Zhijun Zhao and Chao Chen and Yiyang Yao},
keywords = {Scene categorization},
abstract = {Utilizing multi-channel visual features to characterize scenery images is standard for state-of-the-art scene recognition systems. However, how to encode human visual perception for scenery image modeling and how to optimally combine visual features from multiple views remains a tough challenge. In this paper, we propose a perceptual multi-view sparse learning (PMSL) framework to distinguish sceneries from different categories. Specifically, we first project regions from each scenery into the so-called perceptual space, which is established by combining human gaze behavior, color and texture. Afterward, a novel PMSL is developed which fuzes the above three visual cues into a sparse representation. PMSL can support absent channel visual features, which is frequently occurred in practical circumstances. Finally, the sparse representation from each scenery image is incorporated into an image kernel, which is further fed into a kernel SVM for scene categorization. Comprehensive experimental results on popular data sets have demonstrated the superiority of our method over well-known shallow/deep recognition models.}
}
@article{CHANG2019316,
title = {A mix-pooling CNN architecture with FCRF for brain tumor segmentation},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {316-322},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.047},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303262},
author = {Jie Chang and Luming Zhang and Naijie Gu and Xiaoci Zhang and Minquan Ye and Rongzhang Yin and Qianqian Meng},
keywords = {MR image segmentation, Convolutional Neural Network, Fully CRF},
abstract = {MR technique is prevalent for doctor to diagnose and assess glioblastomas which are the most lethal form of brain tumors. Although Convolutional Neural Networks (CNN) has been applied in automatic brain tumor segmentation and is proved useful and efficient, traditional one-pathway CNN architecture with convolutional layers and max pooling layers has limited receptive fields representing the local context information. Such mindset in traditional CNN may dismiss useful global context information. In this paper, we design a two-pathway model with average and max pooling layers in different paths. Besides, 1 × 1 kernels are followed input layers to add the non-linearity dimensions of input data. Finally, we combine the CNN architecture with fully connected CRF(FCRF) as a mixture model to introduce the global context information to optimize prediction results. Our experiments proved that the mixture model improved segmentation and labeling accuracy.}
}
@article{ZHI2019495,
title = {Face recognition based on genetic algorithm},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {495-502},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303389},
author = {Hui Zhi and Sanyang Liu},
keywords = {Face recognition, Genetic algorithm, Principal component analysis, Support vector machine},
abstract = {The development of computer technology has led to the development of face recognition technology. Nowadays, face recognition technology has been successfully applied in many fields with the help of computer technology and network technology. This paper establishes an effective face recognition model based on principal component analysis, genetic algorithm and support vector machine, in which principal component analysis is used to reduce feature dimension, genetic algorithm is used to optimize search strategy, and support vector machine is used to realize classification. Through the simulation experiment on the face database of the Institute of Technology of Chinese Academy of Sciences in 2003, the results show that the model can achieve face recognition with high efficiency, and the highest accuracy rate is 99%.}
}
@article{CHIANG2019363,
title = {A touchless interaction interface for observing medical imaging},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {363-373},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303304},
author = {Pei-Ying Chiang and Chun-Chi Chen and Chih-Hsien Hsia},
keywords = {Touchless, Volume rendering, Focus and context, Visualization, Medical imaging},
abstract = {Using volume rendering to generate 3D models is associated with the problem of missing features on areas of interest, which are possibly concealed by other information. This article presents a novel focus-and-context medical imaging observation system using gesture-based technique to build a touchless interactive environment. The system offers two types of medical imaging observation tool, namely, 3D section cutting tool and 3-axes cross-section synchronization tool, enabling users to quickly and easily observe tissue sections. Feature classification was achieved using region growing and size-based transfer approaches. Combined with view penetration function (cylinder and cone view penetration functions), the system allows for direct observation of hidden features. The analytical experimental results verified that the proposed system is easy to operate in a touchless environment and creates positive user experience regarding observation and interaction.}
}
@article{MU201979,
title = {A spatial-frequency-temporal domain based saliency model for low contrast video sequences},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {79-88},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318302852},
author = {Nan Mu and Xin Xu and Xiaolong Zhang},
keywords = {Salient object detection, Low contrast videos, Multimodal fusion, Region covariance, Incremental learning},
abstract = {The last few decades have witnessed rapid development of visual saliency detection, as it can detect object-of-interest from clutter environments to substantially facilitate a wide range of applications. However, traditional visual saliency detection models primarily rely on image features, which may face great challenges in low contrast video stream captured from low lighting scenarios. This paper proposes a dynamic multimodal fusion based visual saliency detection model towards low contrast videos, which combines saliency information from spatial, frequency, and temporal domains. In spatial domain, superpixel covariance is utilized to compute the region dissimilarity under low lighting scenarios; in frequency domain, the amplitude spectrum tuned method is used to suppress the background noise; in temporal domain, the incremental learning is employed to efficiently update background model from high dimensional video streams. Extensive experiments have been conducted to validate the effectiveness of the proposed model.}
}
@article{TANG20199,
title = {Multi-view non-negative matrix factorization for scene recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {9-13},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.040},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303663},
author = {Jinjiang Tang and Weijie Qian and Zhijun Zhao and Weiliang Liu and Ping He},
keywords = {Non-negative matrix factorization, Scene recognition, Multi-view},
abstract = {Accurately discriminating complicated sceneries from different categories is a useful technique in multimedia and computer vision. In this work, we propose a novel multi-view non-negative matrix factorization to detect human gaze behavior, which is subsequently integrated into an image kernel machine for scene categorization. More specifically, we first project regions from each scenery into the so-called perceptual space, which is established by combining color, texture, and semantic features. Then, a novel non-negative matrix factorization (NMF) algorithm is developed which decomposes the regions’ feature matrix into the product of the basis matrix and the sparse codes. The sparse codes indicate the saliency level of different regions which is used to constructed gaze shifting path. Thereby, the path from each scenery is derived and further incorporated into an image kernel for scene categorization. Comprehensive experiments on six scenery data sets have demonstrated the superiority of our method over a series of recognition models.}
}
@article{AFIQ2019285,
title = {A review on classifying abnormal behavior in crowd scene},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {285-303},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.035},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303146},
author = {A.A. Afiq and M.A. Zakariya and M.N. Saad and A.A. Nurfarzana and M.H.M. Khir and A.F. Fadzil and A. Jale and W. Gunawan and Z.A.A. Izuddin and M. Faizari},
keywords = {Crowd analysis, Abnormal detection, Gaussian Mixture Model (GMM), Hidden Markov Model (HMM), Optical Flow (OF), Spatio-Temporal Technique (STT)},
abstract = {Crowd behavior analysis has become one of the new areas of interest in the computer vision community due to the increasing demands from surveillance and security industries. It is important to meticulously understand crowd behavior to prevent any disaster and unwanted incidents such as thief, stampede and riots. For this purpose, crowd features such as density, motion and trajectory are analyzed to detect any abnormality in the crowd. Thus, this review is aimed to provide insight on several detection methods including Gaussian Mixture Model (GMM), Hidden Markov Model (HMM), Optical Flow method and Spatio-Temporal Technique (STT). Providing the latest development, the review presented the studies that are published in journals and conferences over the past 5 years.}
}
@article{SHAN2019407,
title = {Mesoscopic structure PFC∼2D model of soil rock mixture based on digital image},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {407-415},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303420},
author = {Pengfei Shan and Xingping Lai},
keywords = {Soil rock mixture, PFC∼2D model, Particle flow simulation, Meso mechanical properties},
abstract = {Soil-rock mixture (S/RM) is a very complex discontinuous medium material, which is a multiphase system consisting of high strength rock blocks (Rocks), relatively soft filling components (Soils) and corresponding pores. Because the mechanical properties of various components of soil-rock mixtures under external loads are very different, and there are extremely complex interactions between them. Therefore, the mechanical properties of this geotechnical material (such as stress transfer, failure mode, crack propagation, bearing capacity, etc.) are quite different from those of homogeneous geotechnical materials, and largely depend on the internal structure characteristics of soil-rock mixtures (such as particle size composition, particle shape, particle distribution and arrangement). Due to the complexity of the model, the simulation of its meso-mechanical properties is mostly confined to the random simulation of regular blocks. In this paper, an automatic generation method of PFC∼2D numerical model of soil-rock mixture microstructure based on digital image processing is proposed, and the experimental simulation is carried out with matlab. Thus, the rapid, real and automatic modeling of heterogeneous material microstructure by PFC∼2D software is realized. The PFC∼2D numerical calculation model of soil-rock mixtures is established. The results show that when the stone content is 80%, the analysis should be caused by the large amount of rock, which leads to the large internal voids, and the sudden unloading between the rock and the rock during compaction and then the structural reorganization.}
}