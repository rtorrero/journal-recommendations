@article{2025111258,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {160},
pages = {111258},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(24)01009-4},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010094}
}
@article{TIAN2024110390,
title = {Test-time adaptation for 6D pose tracking},
journal = {Pattern Recognition},
volume = {152},
pages = {110390},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110390},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001419},
author = {Long Tian and Changjae Oh and Andrea Cavallaro},
keywords = {6D pose tracking, Keypoints detection, Self-supervised learning, Transformer},
abstract = {We propose a test-time adaptation for 6D object pose tracking that learns to adapt a pre-trained model to track the 6D pose of novel objects. We consider the problem of 6D object pose tracking as a 3D keypoint detection and matching task and present a model that extracts 3D keypoints. Given an RGB-D image and the mask of a target object for each frame, the proposed model consists of the self- and cross-attention modules to produce the features that aggregate the information within and across frames, respectively. By using the keypoints detected from the features for each frame, we estimate the pose changes between two frames, which enables 6D pose tracking when the 6D pose of a target object in the initial frame is given. Our model is first trained in a source domain, a category-level tracking dataset where the ground truth 6D pose of the object is available. To deploy this pre-trained model to track novel objects, we present a test-time adaptation strategy that trains the model to adapt to the target novel object by self-supervised learning. Given an RGB-D video sequence of the novel object, the proposed self-supervised losses encourage the model to estimate the 6D pose changes that can keep the photometric and geometric consistency of the object. We validate our method on the NOCS-REAL275 dataset and our collected dataset, and the results show the advantages of tracking novel objects. The collected dataset and visualisation of tracking results are available: https://qm-ipalab.github.io/TA-6DT/}
}
@article{VIELHABEN2024110309,
title = {Explainable AI for time series via Virtual Inspection Layers},
journal = {Pattern Recognition},
volume = {150},
pages = {110309},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110309},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000608},
author = {Johanna Vielhaben and Sebastian Lapuschkin and Grégoire Montavon and Wojciech Samek},
keywords = {Interpretability, Explainable Artificial Intelligence, Time series, Discrete Fourier Transform, Invertible transformations, Audio classification},
abstract = {The field of eXplainable Artificial Intelligence (XAI) has witnessed significant advancements in recent years. However, the majority of progress has been concentrated in the domains of computer vision and natural language processing. For time series data, where the input itself is often not interpretable, dedicated XAI research is scarce. In this work, we put forward a virtual inspection layer for transforming the time series to an interpretable representation and allows to propagate relevance attributions to this representation via local XAI methods. In this way, we extend the applicability of XAI methods to domains (e.g. speech) where the input is only interpretable after a transformation. In this work, we focus on the Fourier Transform which, is prominently applied in the preprocessing of time series, with Layer-wise Relevance Propagation (LRP) and refer to our method as DFT-LRP. We demonstrate the usefulness of DFT-LRP in various time series classification settings like audio and medical data. We showcase how DFT-LRP reveals differences in the classification strategies of models trained in different domains (e.g., time vs. frequency domain) or helps to discover how models act on spurious correlations in the data.}
}
@article{LI2024110322,
title = {Sample diversity selection strategy based on label distribution morphology for active label distribution learning},
journal = {Pattern Recognition},
volume = {150},
pages = {110322},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110322},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000736},
author = {Weiwei Li and Wei Qian and Lei Chen and Xiuyi Jia},
keywords = {Label distribution learning, Active learning, Representativeness, Diversity, Label distribution morphology},
abstract = {Labeling a sample in label distribution learning is highly expensive because it involves several labels at the same time and also requires assigning an exact value as the significance of each label. Therefore, active learning, which lowers the labeling cost by actively querying the labels of the most useful data, becomes especially critical for label distribution learning. Most of the known active query algorithms are for multi-label learning, and applying them directly to label distribution learning will lose some key supervisory information and thus fail to yield satisfactory experimental results. In this paper, we propose a sample diversity selection strategy based on the label distribution morphology, which can select diverse samples with different distribution morphologies from a large number of unlabeled samples for active querying. First, we use the feature space to construct a dissimilarity matrix that describes the pairwise dissimilarity among the unlabeled samples in order to pick a subset of samples that are representative of the unlabeled dataset. Second, using the information about the label distribution morphologies provided by the predicted labels of the unlabeled samples, we design a diversity loss score for each unlabeled sample. This score reflects the degree of difference between the sample and the labeled training sample. Finally, we use a convex optimization method to select valuable samples that are diverse from the labeled samples and represent the distribution of the unlabeled samples. The results of the comparison experiments demonstrate the effectiveness of our approach.}
}
@article{WANG2024110387,
title = {AG-Meta: Adaptive graph meta-learning via representation consistency over local subgraphs},
journal = {Pattern Recognition},
volume = {151},
pages = {110387},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110387},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001389},
author = {Yi Wang and Changqin Huang and Ming Li and Qionghao Huang and Xuemei Wu and Jia Wu},
keywords = {Embedding representation, Local subgraphs, Few-shot graph learning, Meta-learning},
abstract = {Graph meta-learning has recently received significantly increased attention by virtue of its potential to extract common and transferable knowledge from learning different tasks on a graph. Existing methods for graph meta-learning usually leverage local subgraphs to transfer subgraph-specific information. However, they inherently face the challenge of imbalanced subgraphs due to inconsistent node density and different label distributions over local subgraphs. This paper proposes an adaptive graph meta-learning framework (AG-Meta) for learning the consistent and transferable representation of a graph in a way that can adapt to imbalanced subgraphs. Specifically, AG-Meta first learns the structural representation of subgraphs with various degrees using an Adaptive Graph Cascade Diffusion Network (AGCDN). AG-Meta then employs a prototype-consistency classifier to produce more accurate transferable inductive representations (also called prototypes) under few-shot settings with different label distributions of a subgraph. In the context of optimizing a model-agnostic meta-learner, a novel metric loss is finally introduced to achieve structural representation and prototype consistency. Extensive experiments are conducted to compare AG-Meta against baselines on five real-world networks, which validates that AG-Meta outperforms the state-of-the-art approaches.}
}
@article{WU2024110334,
title = {Transferable graph auto-encoders for cross-network node classification},
journal = {Pattern Recognition},
volume = {150},
pages = {110334},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110334},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000852},
author = {Hanrui Wu and Lei Tian and Yanxin Wu and Jia Zhang and Michael K. Ng and Jinyi Long},
keywords = {Cross-network node classification, Transfer learning, Domain adaptation, Graph auto-encoder, Graph convolutional network},
abstract = {Node classification is a popular and challenging task in graph neural networks, and existing approaches are mainly developed for a single network. With the advances in domain adaptation, researchers tend to leverage knowledge extracted from a fully-labeled source network to further improve the node classification performance in an unlabeled target network. This learning paradigm refers to cross-network node classification, which is the topic we studied in this paper. Specifically, we propose a novel model named Transferable Graph Auto-Encoders (TGAE), which first encodes the initial network data into latent representations and then decodes the learned features to preserve graph information. In the encoding phase, TGAE adopts the attentional mechanism to fuse the local and global information of nodes to discover latent node representations. To obtain transferable features between the source and target networks, TGAE aligns their distributions based on the learned representations by reducing marginal and conditional distribution differences. In the decoding phase, the latent representations are subjected to pairwise and reconstruction constraints, thus preserving structural proximity and graph topology information to learn discriminative features. Besides, a node classifier is trained to enhance the discriminant of the node representations further. Experimental results on several real-world datasets demonstrate that the proposed model achieves state-of-the-art performance in cross-network node classification tasks compared with existing methods.}
}
@article{GREGGIO2024110306,
title = {Unsupervised incremental estimation of Gaussian mixture models with 1D split moves},
journal = {Pattern Recognition},
volume = {150},
pages = {110306},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110306},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000578},
author = {Nicola Greggio and Alexandre Bernardino},
keywords = {Unsupervised learning, Gaussian mixture models, Model selection, Split and merge methods},
abstract = {In this paper, we propose a new type of split rule for incremental estimation of Gaussian Mixture Models with model selection. Split-based methods typically start with a mixture composed of a single component representing all data, and successively split and optimize components for a given model selection criterion. These algorithms are typically faster than alternatives, but depend critically on the component splitting method, since a good split rule promotes a faster convergence of the mixture optimization phase. We propose a new efficient and robust split rule that projects mixture components onto a 1D subspace and fits a two-component model to the projected data with the Expectation Maximization algorithm. The proposed approach is fast and robust to parameter tuning, being the ideal choice for applications that favor speed while still maintaining an acceptable accuracy. We illustrate the validity of the method through a series of experiments on synthetic and real datasets comparing the proposed method to alternatives of the state-of-the-art in terms of efficiency, accuracy, and sensitivity to parameter tuning.}
}
@article{KANG2024110391,
title = {Bibimbap : Pre-trained models ensemble for Domain Generalization},
journal = {Pattern Recognition},
volume = {151},
pages = {110391},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110391},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001420},
author = {Jinho Kang and Taero Kim and Yewon Kim and Changdae Oh and Jiyoung Jung and Rakwoo Chang and Kyungwoo Song},
keywords = {Transfer learning, Molecular classification, Domain generalization, Weight averaging, Ensemble learning, Chemical dataset},
abstract = {This paper addresses a machine learning problem often challenged by differences in the distributions of training and real-world data. We propose a framework that addresses the problem of underfitting in the ensembling method using pre-trained models and improves the performance and robustness of deep learning models through ensemble diversity. For the naive weight ensembling framework, we discovered that the ensembled models could not lie in the same loss basin under extreme domain shift conditions, suggesting that a loss barrier may exist. We used a fine-tuning step after the weighted ensemble to address the underfitting problem caused by the loss barrier and stabilize the batch normalization running parameters. We also inferred through qualitative analysis that the diversity of ensemble models affects domain generalization. We validate our method on a large-scale image dataset (ImageNet-1K) and chemical molecule data, which is suitable for testing with domain shift problems due to its data-splitting method.}
}
@article{LAN2024110318,
title = {RCsearcher: Reaction center identification in retrosynthesis via deep Q-learning},
journal = {Pattern Recognition},
volume = {150},
pages = {110318},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110318},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000694},
author = {Zixun Lan and Zuo Zeng and Binjie Hong and Zhenfu Liu and Fei Ma},
keywords = {Reaction center identification, Retrosynthesis, Deep Q-learning},
abstract = {The reaction center consists of atoms in the product with local properties that differ from those in the reactants. Previous studies focused on identifying the reaction center using semi-templated retrosynthesis methods, which are limited to single reaction center identification. In reality, however, many reaction centers involve multiple bonds or atoms, referred to as multiple reaction centers. This paper introduces RCsearcher, a unified framework that exploits the strengths of graph neural networks and deep reinforcement learning for identifying both single and multiple reaction centers. The key insight of the framework is that the single or multiple reaction center must be a node-induced subgraph of the molecular product graph. At each step, RCsearcher selects a node in the molecular product graph and adds it to the explored node-induced subgraph as an action. Comprehensive experiments demonstrate that RCsearcher consistently outperforms other baselines, and is able to identify reaction center patterns not present in the training set. Ablation experiments confirm the effectiveness of individual components of RCsearcher, including the beam search and the one-hop constraint of the action space.}
}
@article{HUANG2024110375,
title = {Medical image segmentation based on dynamic positioning and region-aware attention},
journal = {Pattern Recognition},
volume = {151},
pages = {110375},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110375},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001262},
author = {Zhongmiao Huang and Shuli Cheng and Liejun Wang},
keywords = {Medical image segmentation, Transformer, Dynamic Positioning Attention, Bi-Level Routing Attention},
abstract = {Transformer has already proven its ability to model long-distance dependencies. However, medical images have strong local structures. Directly using Transformer to extract features would not only contain redundant information increasing the computational effort, but also be detrimental to extracting local details. Given these issues, we propose a network based on dynamic positioning and region-aware attention, which adopts a two-stage feature extraction strategy. In the shallow layer, we design Dynamic Positioning Attention (DPA). It will localize to the key feature information and construct a variable window for it, then perform attention calculation. DPA improves the learning ability of local details, reduces the amount of computation. At the deep level, Bi-Level Routing Attention (BRA) is used to discard irrelevant key–value pairs, achieve content-aware sparse attention for the deep dispersed semantic information, and improve computational efficiency. After several experiments, the results show that our method achieves advanced performance on different types of datasets.}
}
@article{DU2024110358,
title = {Semi-supervised imbalanced multi-label classification with label propagation},
journal = {Pattern Recognition},
volume = {150},
pages = {110358},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110358},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001092},
author = {Guodong Du and Jia Zhang and Ning Zhang and Hanrui Wu and Peiliang Wu and Shaozi Li},
keywords = {Label propagation, Class-imbalance learning, Semi-supervised multi-label learning, Collaboration technique},
abstract = {Multi-label learning tasks usually encounter the problem of the class-imbalance, where samples and their corresponding labels are non-uniformly distributed over multi-label data space. It has attracted increasing attention during the past decade, however, there is a lack of methods capable of handling the imbalanced problem in a semi-supervised setting. This study proposes a label propagation technique to settle the semi-supervised imbalanced multi-label issue. Specially, we first utilize a collaborative manner to exploit the correlations from labels and instances, and learn a label regularization matrix to overcome the imbalanced problem in the labeled instance. After that, we extend to semi-supervised learning and explore to represent the similarity of instances with weighted graphs on labeled and unlabeled data. Then, the data distribution information and label correlations are fully utilized to design the loss function under the consistency assumption manner. At last, we present an iterative scheme to settle the optimization issue, thereby achieving label propagation to address the imbalanced challenge. Experiments on a variety of multi-label data sets show the favorable performance of the proposed method against related comparing approaches. Notably, the proposed method is also validated to be robust with a limited number of training instances.}
}
@article{NAM2024110332,
title = {FSDA: Frequency re-scaling in data augmentation for corruption-robust image classification},
journal = {Pattern Recognition},
volume = {150},
pages = {110332},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110332},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000839},
author = {Ju-Hyeon Nam and Sang-Chul Lee},
keywords = {Deep learning, Image classification, Convolutional neural networks, Data augmentation, Frequency domain},
abstract = {Modern convolutional neural networks (CNNs) are used in various applications, including computer vision, speech recognition, and robotics. However, practical usage in various applications requires large-scale datasets, and real-world data contains various corruptions that degrade the model’s performance owing to the inconsistencies in the training and testing distributions. In this study, we propose Frequency re-Scaling Data Augmentation (FSDA) to improve the classification performance, robustness against corruption, and localizability of classifiers trained on various image classification datasets. Our method consists of two processes: mask generation process (MGP) and pattern re-scaling process (PSP). MGP clusters the frequency domain spectra to produce similar frequency patterns, and then PSP scales frequency by learning rescaling parameters from frequency patterns. Because the CNN classifies images by focusing on their structural features highlighted with FSDA, CNN trained with the proposed method has more robustness against corruption than that with the other data augmentations (DAs). Our technique outperforms the existing DAs on four public image classification datasets, including the CIFAR-10/100, STL-10, and ImageNet. Particularly, our strategy increases the robustness of the classifier against the different corruption errors by an average of 5.04% over the baseline.}
}
@article{LI2024110321,
title = {Learning with incomplete labels of multisource datasets for ECG classification},
journal = {Pattern Recognition},
volume = {150},
pages = {110321},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110321},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000724},
author = {Qince Li and Yang Liu and Ze Zhang and Jun Liu and Yongfeng Yuan and Kuanquan Wang and Runnan He},
keywords = {Electrocardiogram, Multilabel classification, Incomplete labels, Multisource data mining},
abstract = {The shortage of annotated ECG data presents a significant impediment, hampering the overall generalization capabilities of machine learning models tailored for automated ECG classification. The collective integration of multisource datasets presents a potential remedy for this challenge. However, it is crucial to underscore that the mere addition of supplementary data does not automatically guarantee performance enhancement, given the unresolved challenges associated with multisource data. In this research, we address one such challenge, namely, the issue of incomplete labels arising from the diversity of annotations within multi-source ECG datasets. First, we identified three distinct types of label missing: dataset-related label missing, supertype missing, and subtype missing. To address the supertype missing effectively, we introduce a novel approach known as offline category mapping which leverages the hierarchical relationships inherent within the categories to recover the missing supertype labels. Additionally, two complementary strategies, referred to as prediction masking and online category mapping, are proposed to mitigating the adverse effects of subtype and dataset-related label missing on model optimization. These strategies enhance the model's ability to identify missing subtypes under conditions of weak supervision. These pioneering methodologies are integrated into a deep learning-based framework designed for multilabel ECG classification. The performance of our proposed framework is rigorously evaluated using realistic multi-source datasets obtained from the PhysioNet/CinC challenge 2020/2021. The proposed learning framework exhibits a notable improvement in macro-average precision, surpassing the corresponding baseline model by more than 25 % on the test datasets. As a result, this research study makes a substantial contribution to the field of ECG classification by addressing the critical issue of incomplete labels in multisource datasets, ultimately enhancing the generalization capabilities of machine learning models in this domain.}
}
@article{HE2024110413,
title = {LATFormer: Locality-Aware Point-View Fusion Transformer for 3D shape recognition},
journal = {Pattern Recognition},
volume = {151},
pages = {110413},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110413},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400164X},
author = {Xinwei He and Silin Cheng and Dingkang Liang and Song Bai and Xi Wang and Yingying Zhu},
keywords = {3D shape retrieval and classification, Point cloud, Multi-view, Multimodal fusion, Transformer},
abstract = {Recently, 3D shape understanding has achieved significant progress due to the advances of deep learning models on various data formats like images, voxels, and point clouds. Among them, point clouds and multi-view images are two complementary modalities of 3D objects, and learning representations by fusing both of them has been proven to be fairly effective. While prior works typically focus on exploiting global features of the two modalities, herein we argue that more discriminative features can be derived by modeling “where to fuse”. To investigate this, we propose a novel Locality-Aware Point-View Fusion Transformer (LATFormer) for 3D shape retrieval and classification. The core component of LATFormer is a module named Locality-Aware Fusion (LAF) which integrates the local features of correlated regions across the two modalities based on the co-occurrence scores. We further propose to filter out scores with low values to obtain salient local co-occurring regions, which reduces redundancy for the fusion process. In our LATFormer, we utilize the LAF module to fuse the multi-scale features of the two modalities both bidirectionally and hierarchically to obtain more informative features. Comprehensive experiments on four popular 3D shape benchmarks covering 3D object retrieval and classification validate its effectiveness.}
}
@article{GUO2024110333,
title = {Improving self-supervised action recognition from extremely augmented skeleton sequences},
journal = {Pattern Recognition},
volume = {150},
pages = {110333},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110333},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000840},
author = {Tianyu Guo and Mengyuan Liu and Hong Liu and Guoquan Wang and Wenhao Li},
keywords = {Self-supervised skeleton-based action recognition, Contrastive learning},
abstract = {Self-supervised contrastive learning has been widely applied to skeleton-based action recognition due to its ability to learn discriminative features. However, directly applying the existing contrastive learning framework for 3D skeleton learning is limited by the well-designed augmentations and the simple multi-stream decision-level fusion. To deal with these drawbacks, we propose a three-stream contrastive learning framework utilizing abundant information mining for self-supervised action representation (3s-AimCLR++). For single-stream contrastive learning, extreme augmentation is first proposed to generate more movement patterns, which can introduce more movement patterns to improve the universality of the learned representations. Since directly using extreme augmentation can barely boost the performance due to the drastic changes in original identity, the Distributional Divergence Minimization (DDM) loss is proposed to utilize the extreme augmentation more gently. Moreover, the Single-Stream Nearest Neighbors Mining (SNNM) is proposed to expand positive samples to make the learning process more reasonable. For multi-stream, existing methods simply ensemble the results. Yet, considering the complementarity of information between different streams, we propose Multi-Stream Aggregation and Interaction (MSAI) strategy to better fuse multi-stream information. Extensive experiments on NTU-60, NTU-120, and PKU-MMD datasets have verified that our 3s-AimCLR++ can significantly perform favorably against state-of-the-art methods under a variety of evaluation protocols. The code and models are available at https://github.com/Levigty/AimCLR-v2.}
}
@article{FENG2024110350,
title = {Adaptive weighted dictionary representation using anchor graph for subspace clustering},
journal = {Pattern Recognition},
volume = {151},
pages = {110350},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110350},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001018},
author = {Wenyi Feng and Zhe Wang and Ting Xiao and Mengping Yang},
keywords = {Dictionary representation, Subspace clustering, Anchor graph, Projection learning},
abstract = {Samples are commonly represented as sparse vectors in many dictionary representation algorithms. However, this method may result in loss of discriminatory information. Moreover, a redundant dictionary can increase the computational complexity of the algorithm. To tackle these challenges, we propose a novel method named Adaptive Weighted Dictionary Representation using Anchor Graph for Subspace Clustering (AWDR). First, AWDR constructs an anchor graph that encodes the classification information and establishes accurate connectivity components between anchors and clusters, thereby fully utilizing the discriminative information of the original samples. In addition, AWDR learns a complete-dictionary in the subspace to eliminate the noise and out-of-sample effects of the original sample space, while also improving computational efficiency. Finally, AWDR computes the coefficients for the samples in an adaptively weighted manner to find discriminative representation of the samples from the dictionary. Extensive experiments on real-world datasets demonstrate that our method is effective and efficient compared to the state-of-the-art methods.}
}
@article{GU2024110317,
title = {Orientation-aware leg movement learning for action-driven human motion prediction},
journal = {Pattern Recognition},
volume = {150},
pages = {110317},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110317},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000682},
author = {Chunzhi Gu and Chao Zhang and Shigeru Kuriyama},
keywords = {Stochastic human motion prediction, Motion transition learning, Deep generative model},
abstract = {The task of action-driven human motion prediction aims to forecast future human motion based on the observed sequence while respecting the given action label. It requires modeling not only the stochasticity within human motion but the smooth yet realistic transition between multiple action labels. However, the fact that most datasets do not contain such transition data complicates this task. Existing work tackles this issue by learning a smoothness prior to simply promote smooth transitions, yet doing so can result in unnatural transitions especially when the history and predicted motions differ significantly in orientations. In this paper, we argue that valid human motion transitions should incorporate realistic leg movements to handle orientation changes, and cast it as an action-conditioned in-betweening (ACB) learning task to encourage transition naturalness. Because modeling all possible transitions is virtually unreasonable, our ACB is only performed on very few selected action classes with active gait motions, such as “Walk” or “Run”. Specifically, we follow a two-stage forecasting strategy by first employing the motion diffusion model to generate the target motion with a specified future action, and then producing the in-betweening to smoothly connect the observation and prediction to eventually address motion prediction. Our method is completely free from the labeled motion transition data during training. To show the robustness of our approach, we generalize our trained in-betweening learning model on one dataset to two unseen large-scale motion datasets to produce natural transitions. Extensive experimental evaluations on three benchmark datasets demonstrate that our method yields the state-of-the-art performance in terms of visual quality, prediction accuracy, and action faithfulness.}
}
@article{LI2024110362,
title = {Multi-granularity Cross Transformer Network for person re-identification},
journal = {Pattern Recognition},
volume = {150},
pages = {110362},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110362},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001134},
author = {Yanping Li and Duoqian Miao and Hongyun Zhang and Jie Zhou and Cairong Zhao},
keywords = {Person re-identification, Cross transformer, Multi-granularity},
abstract = {Person re-identification (Re-ID) aims to retrieve the same person in the gallery. Transformers have been introduced to the Re-ID task due to their excellent ability to model long-range dependency. However, due to the properties of the global attention mechanism, they are less effective in capturing the discriminative local semantics of pedestrians compared to convolutional operations. To address this issue, we present a Multi-granularity Cross Transformer Network (MCTN) that progressively learns salient features of different local structures in a global context. Specifically, we first utilize a Multi-granularity Convolutional Layer (MCL) to investigate salient pedestrian features at various granularities. On this basis, we propose a Pyramidal Cross Transformer learning layer (PCT), which contains a pyramidal division of pedestrian image feature maps, differentiated feature extraction of different parts of pedestrians, and cross attention to exploring the local–global relationship of the feature map. It allows effective mining of local information in the global structure from a coarse-to-fine perspective. Furthermore, to enhance the interaction between low-level detailed features and high-level semantic features, a Hierarchical Aggregation Strategy (HAS) is introduced to fuse features learned by cross attention learning at different stages. Pedestrian features learned in shallow layers will serve as global priors for semantics learning in deep layers. We evaluate our method on four large-scale Re-ID datasets, and the experimental results reveal that the proposed method outperforms the state-of-the-art methods.}
}
@article{GUPTA2024110386,
title = {Efficient high-resolution template matching with vector quantized nearest neighbour fields},
journal = {Pattern Recognition},
volume = {151},
pages = {110386},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110386},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001377},
author = {Ankit Gupta and Ida-Maria Sintorn},
keywords = {Template matching, Vector quantized nearest neighbour field (VQ-NNF), Object detection, High-resolution template matching},
abstract = {Template matching is a fundamental problem in computer vision with applications in fields including object detection, image registration, and object tracking. Current methods rely on nearest-neighbour (NN) matching, where the query feature space is converted to NN space by representing each query pixel with its NN in the template. NN-based methods have been shown to perform better in occlusions, appearance changes, and non-rigid transformations; however, they scale poorly with high-resolution data and high feature dimensions. We present an NN-based method that efficiently reduces the NN computations and introduces filtering in the NN fields (NNFs). A vector quantization step is introduced before the NN calculation to represent the template with k features, and the filter response over the NNFs is used to compare the template and query distributions over the features. We show that state-of-the-art performance is achieved in low-resolution data, and our method outperforms previous methods at higher resolution.}
}
@article{BILGE2024110374,
title = {Cross-lingual few-shot sign language recognition},
journal = {Pattern Recognition},
volume = {151},
pages = {110374},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110374},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001250},
author = {Yunus Can Bilge and Nazli Ikizler-Cinbis and Ramazan Gokberk Cinbis},
keywords = {Sign language recognition, Few-shot learning, Cross-lingual sign language recognition},
abstract = {There are over 150 sign languages worldwide, each with numerous local variants and thousands of signs. However, collecting annotated data for each sign language to train a model is a laborious and expert-dependent task. To address this issue, this paper introduces the problem of few-shot sign language recognition (FSSLR) in a cross-lingual setting. The central motivation is to be able to recognize a novel sign, even if it belongs to a sign language unseen during training, based on a small set of examples. To tackle this problem, we propose a novel embedding-based framework that first extracts a spatio-temporal visual representation based on video and hand features, as well as hand landmark estimates. To establish a comprehensive test bed, we propose three meta-learning FSSLR benchmarks that span multiple languages, and extensively evaluate the proposed framework. The experimental results demonstrate the effectiveness and superiority of the proposed approach for few-shot sign language recognition in both monolingual and cross-lingual settings.}
}
@article{JI2024110398,
title = {Multi-task hierarchical convolutional network for visual-semantic cross-modal retrieval},
journal = {Pattern Recognition},
volume = {151},
pages = {110398},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110398},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001493},
author = {Zhong Ji and Zhigang Lin and Haoran Wang and Yanwei Pang and Xuelong Li},
keywords = {Vision and language, Cross-modal retrieval, Multi-task learning, Metric learning},
abstract = {Bridging visual and textual representations plays a central role in delving into multimedia data understanding. The main challenge arises from that images and texts exist in heterogeneous spaces, leading to the difficulty to preserve the semantic consistency between both modalities. To narrow the modality gap, most recent methods resort to extra object detectors or parsers to obtain the hierarchical representations. In this work, we address this problem by introducing our Multi-Task Hierarchical Convolutional Neural Network (MT-HCN). It is characterized by mining the hierarchical semantic information without the aid of any extra supervisions. Firstly, from the perspective of representing architecture, we leverage the intrinsic hierarchical structure of Convolutional Neural Networks (CNNs) to decompose the representations of both modalities into two semantically complementary levels, i.e., exterior representations and concept representations. The former focuses on discovering the fine-grained low-level associations between both modalities, meanwhile the latter underlines capturing more high-level abstract semantics. Specifically, we present a Self-Supervised Clustering (SSC) loss to preserve more fine-grained semantic clues in exterior representations. It is constituted on the basis of viewing multiple image/text pairs with similar exterior as a category. In addition, a novel harmonious bidirectional triplet ranking (HBTR) loss is proposed, which mitigate the adverse effects brought about by the biased and noisy negative samples. Besides hardest negatives, it also imposes the constraints on the distance between the positive pairs and the centroid of negative pairs. Extensive experimental results on two popular cross-modal retrieval benchmarks demonstrate our proposed MT-HCN can achieve the competitive results compared with the state-of-the-art methods.}
}
@article{CHEN2024110339,
title = {Integrating foreground–background feature distillation and contrastive feature learning for ultra-fine-grained visual classification},
journal = {Pattern Recognition},
volume = {150},
pages = {110339},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110339},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000906},
author = {Qiupu Chen and Lin Jiao and Fenmei Wang and Jianming Du and Haiyun Liu and Xue Wang and Rujing Wang},
keywords = {Ultra-fine-grained visual classification, Leaf cultivar identification, Self-supervised learning, Deep learning, Vision transformer},
abstract = {In pattern recognition, ultra-fine-grained visual classification (ultra-FGVC) has emerged as a paramount challenge, focusing on sub-category distinction within fine-grained objects. The near-indistinguishable similarities among such objects, combined with the dearth of sample data, intensify this challenge. In response, our FDCL-DA method is introduced, which integrates Foreground–background feature Distillation (FD) and Contrastive feature Learning (CL) with Dual Augmentation (DA). This method uses two different data augmentation techniques, standard and auxiliary augmentation, to enhance model performance and generalization ability. The FD module reduces superfluous features and augments the contrast between the principal entity and its backdrop, while the CL focuses on creating unique data imprints by reducing intra-class resemblances and enhancing inter-class disparities. Integrating this method with different architectures, such as ResNet-50, Vision Transformer, and Swin-Transformer (Swin-T), significantly improves these backbone networks, especially when used with Swin-T, leading to promising results on eight popular datasets for ultra-FGVC tasks.11The code is available at https://github.com/qpuchen/FDCL-DA.}
}
@article{CAI2024110420,
title = {Multi-view clustering via pseudo-label guide learning and latent graph structure recovery},
journal = {Pattern Recognition},
volume = {151},
pages = {110420},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110420},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001717},
author = {Ronggang Cai and Hongmei Chen and Yong Mi and Chuan Luo and Shi-Jinn Horng and Tianrui Li},
keywords = {Multi-view clustering, Latent space, Pseudo-label, Latent graph structure recovery, Enhanced label fusion},
abstract = {Multi-view clustering (MvC) accomplishes sample classification tasks by exploring information from different views. Currently, researchers have paid greater attention to graph-based MvC methods. However, most existing methods only consider the original graph structure and pay relatively little attention to the graph structure in the latent space. In addition, most methods need to pay more attention to the consistency of information on different labels. Otherwise, this may lead to the loss of label information. This paper presents a new multi-view clustering framework to address the above issues. The proposed method considers both the information in the latent space and the original data space, which firstly obtains the pseudo-label by latent representation learning and then lets the pseudo-label guide the learning of the complementary information between the raw data views. To ensure the integrity of the data structure, a latent graph structure recovery strategy is designed in the latent space. Finally, an enhanced label fusion strategy is designed to fusion the different types of labels, yielding an information-rich label matrix for clustering. Experimental results demonstrate the proposed method’s effectiveness compared to other advanced approaches.}
}
@article{YI2024110323,
title = {IDC-Net: Breast cancer classification network based on BI-RADS 4},
journal = {Pattern Recognition},
volume = {150},
pages = {110323},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110323},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000748},
author = {Sanli Yi and Ziyan Chen and Furong She and Tianwei Wang and Xuelian Yang and Dong Chen and Xiaomao Luo},
keywords = {Breast imaging reporting and data system(BI-RADS), Subcategories 4a-4c, Breast ultrasound images, CNN, CapsNet, IDC-Net},
abstract = {In the diagnosis of breast cancer, the 3 sub-categories 4a-4c of BI-RADS 4 are of great significance to doctors. However, low resolution of ultrasound image and high similarity between different category images pose great challenges to this task, which requires the network to be more capable of extracting image features. Therefore, in response to the efficient classification of BI-RADS 4a-4c in breast ultrasound images, we developed a lightweight classification network IDCNet, a neural network model combining the advantages of convolutional neural network(CNN) and CapsNet. In this model: Firstly, we proposed ID-Net based on CNN architecture and mainly constructed by ID block and DD block, which ensure the ID-Net deep and wide enough to extract sufficient local semantic information of image, and at the same time being lightweight. Secondly, we use the CapsNet to learn the position and posture information between the global features of the image, which makes up for the defects of CNN. Finally, two parallel paths of IDCNet and CapsNet are fused to enhance IDCNet's capability of feature extraction. To verify our method, experiments have been conducted on the breast ultrasound dataset of Yunnan cancer hospital and two public datasets. The classification results of our method have been compared with those obtained by five existing approaches. The experimental results show that the proposed method IDCNet has the highest Accuracy (98.54 %), Precision (98.54 %) and F1 score (98.54 %).}
}
@article{SAMPAIO2024110310,
title = {Regularization and optimization in model-based clustering},
journal = {Pattern Recognition},
volume = {150},
pages = {110310},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110310},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400061X},
author = {Raphael Araujo Sampaio and Joaquim {Dias Garcia} and Marcus Poggi and Thibaut Vidal},
keywords = {Clustering, Gaussian Mixture Models, Regularization, Optimization, Hybrid Genetic Algorithm},
abstract = {Due to their conceptual simplicity, k-means algorithm variants have been extensively used for unsupervised cluster analysis. However, one main shortcoming of these algorithms is that they essentially fit a mixture of identical spherical Gaussians to data that vastly deviates from such a distribution. In comparison, general Gaussian Mixture Models (GMMs) can fit richer structures but require estimating a quadratic number of parameters per cluster to represent the covariance matrices. This poses two main issues: (i) the underlying optimization problems are challenging due to their larger number of local minima, and (ii) their solutions can overfit the data. In this work, we design search strategies that circumvent both issues. We develop more effective optimization algorithms for general GMMs, and we combine these algorithms with regularization strategies that avoid overfitting. Through extensive computational analyses, we observe that optimization or regularization in isolation does not substantially improve cluster recovery. However, combining these techniques permits a completely new level of performance previously unachieved by k-means algorithm variants, unraveling vastly different cluster structures. These results shed new light on the current status quo between GMM and k-means methods and suggest the more frequent use of general GMMs for data exploration. To facilitate such applications, we provide open-source code as well as Julia packages (UnsupervisedClustering.jl and RegularizedCovarianceMatrices.jl) implementing the proposed techniques.}
}
@article{CAO2024110336,
title = {RASNet: Renal automatic segmentation using an improved U-Net with multi-scale perception and attention unit},
journal = {Pattern Recognition},
volume = {150},
pages = {110336},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110336},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000876},
author = {Gaoyu Cao and Zhanquan Sun and Chaoli Wang and Hongquan Geng and Hongliang Fu and Zhong Yin and Minlan Pan},
keywords = {Renal automatic segmentation, Multi-scale spatial perception, Attention mechanism, Image segmentation, Deep learning},
abstract = {For the treatment of renal disease, the application of radioactive equipment has become one of the important methods. Accurate segmentation of renal contour plays an important role in clinical diagnosis. However, manual renal contour drawing is not only inefficient but also prone to inaccurate outlining results due to different manual proficiency and fatigue caused by long-term work. There is little research on automatic renal segmentation with renal dynamic imaging. To address this issue, an improved model based on a deep neural network called Renal Automatic Segmentation Network (RASNet) is proposed, to aid in the automatic segmentation of renal contours. Besides, a multi-scale spatial perception module and a decoding module with attention connection are introduced to enrich the semantic information and further improve the accuracy of network segmentation. Extensive experiments were conducted on a renal dynamic medical image database established in this paper. Analysis results show the superiority of the proposed RASNet to several existing segmentation frameworks.}
}
@article{CHENG2024110409,
title = {Deep Joint Semantic Adaptation Network for Multi-source Unsupervised Domain Adaptation},
journal = {Pattern Recognition},
volume = {151},
pages = {110409},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110409},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001602},
author = {Zhiming Cheng and Shuai Wang and Defu Yang and Jie Qi and Mang Xiao and Chenggang Yan},
keywords = {Multi-source unsupervised domain adaptation, Maximum mean discrepancy, Domain shift, Data augmentation, Distribution alignment},
abstract = {Multi-source Unsupervised Domain Adaptation (MUDA) transfers knowledge learned from multiple labeled source domains to an unlabeled target domain by minimizing the domain shift between multiple source domains and the target domain. Recent studies on MUDA have focused on aligning the distribution of each pair of source and target domains in separate feature spaces to reduce their domain shift. However, these approaches suffer from two main shortcomings. First, they usually focus on the global domain shift which lacks consideration of the joint distribution of category-corresponded subdomains. Second, out-of-distribution samples far from the sample center are hard to align by the global domain alignment. Therefore, we propose a novel Deep Joint Semantic Adaptive Network (DJSAN) for MUDA. Specifically, a new maximum mean discrepancy-based metric, Joint Semantic Maximum Mean Discrepancy (JSMMD), is proposed, which can uniformly optimize the cross-domain joint distribution of category-corresponded subdomains on multiple task-specific layers. Moreover, to deal with the out-of-distribution hard samples, we propose an across-domain data augmentation method called Source-Target Domain Mixing (STDMix) to enhance the robustness of the model, which synthesizes the source domain and target domain into a new domain at a fixed ratio and utilizes information entropy to provide reliable pseudo-labels for samples in the target domain. Experimental results on three public datasets, i.e., Office-31, Digits-five, and Office-Home, show that our proposed method achieves improvements of 0.3%, 1.8%, and 2.7% in average accuracy, respectively.}
}
@article{MASKOVA2024110381,
title = {Deep anomaly detection on set data: Survey and comparison},
journal = {Pattern Recognition},
volume = {151},
pages = {110381},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110381},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001328},
author = {Michaela Mašková and Matěj Zorek and Tomáš Pevný and Václav Šmídl},
keywords = {Set data, Anomaly detection, Generative models, One-class classification, Set transformers},
abstract = {Detecting anomalous samples in set data is a problem attracting increased interest due to novel modalities, such as point-cloud data produced by lidars. Novel methods including those based on deep neural networks are often tuned for a single purpose prohibiting intuition of how relevant they are for another purpose or application domains. The aim of this survey is to: (i) review elementary concepts of anomaly detection of set data, (ii) identify the building blocks of deep anomaly detectors, and (iii) analyze the impact of these blocks on performance. The impact is studied in a large experimental comparison on a variety of benchmark datasets. The results reveal that the main factor determining the performance is the type of anomalies in the dataset. While deep methods embedding the whole set to a single fixed vector perform well on point cloud data, the methods embedding each feature vector independently are better for datasets from multi-instance learning. Moreover, sophisticated methods utilizing transformer blocks are frequently inferior to simple models with properly optimized hyperparameters. An independent factor in performance is the cardinality of sets, the proper treatment of which remains an open problem, as the existing analytical solution was found to be inaccurate.}
}
@article{SHIKKENAWIS2024110393,
title = {Noise level estimation using locality preserving natural image statistics},
journal = {Pattern Recognition},
volume = {151},
pages = {110393},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110393},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001444},
author = {Gitam Shikkenawis and Suman K. Mitra and Ashutosh Saxena},
keywords = {Additive White Gaussian Noise, Noise level estimation, 2D Orthogonal Locality Preserving Discriminant Projection, Natural image statistics},
abstract = {Natural images are known to have certain regular statistical properties. These properties get modified under any artificial change or distortion in natural images. Most common form of image degradation occurs in the form of noise. The amount of degradation in noisy images is measured by estimating the noise level. Many image processing applications such as denoising, restoration, segmentation, compression etc. use noise level information as a prior; inaccurate estimate of which may impact their performance. In this article, we explore natural image statistics in locality preserving transform domain. This property groups structurally similar images/image patches when projected in the transform domain. Image patches corrupted with similar noise level get projected close by in the locality preserving domain and show consistent coefficient behaviour. In particular, we use Two Dimensional Orthogonal Locality Preserving Projection (2DOLPP) as the domain transformation technique. 2DOLPP basis, representing natural images, are learnt in advance from a set of clean images, thereby reducing the computational time significantly. Features based on natural image statistics are extracted from 2DOLPP domain representation of input image patches. Mapping from feature space to noise level is carried out using support vector regression. The proposed noise estimation approach is at par with or surpasses the state-of-the-art techniques with much less computational time. Performance of this approach is stable across a wide range of noise levels and independent of the image structure.}
}
@article{SUN2024110377,
title = {Decoupled representation for multi-view learning},
journal = {Pattern Recognition},
volume = {151},
pages = {110377},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110377},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001286},
author = {Shiding Sun and Bo Wang and Yingjie Tian},
keywords = {Multi-view learning, Representation learning, Information bottleneck, Contrastive learning},
abstract = {Learning multi-view data is a central topic for advanced deep model applications. Existing efforts mainly focus on exploring shared information to maximize the consensus among all the views. However, after reasonably discarding superfluous task-irrelevant noise, the view-specific information is equally essential to downstream tasks. In this paper, we propose to decouple the multi-view representation learning into the shared and specific information extractions with parallel branches, and seamlessly adopt feature fusion in end-to-end models. The common feature is obtained based on the view-agnostic contrastive learning and view-discriminative training to minimize the discrepancy within the views. Simultaneously, the specific feature is learned with orthogonality constraints to minimize the view-level correlation. Besides, the semantic information in the features is reserved with supervised training. After disentangling the representations, we fuse the mutually complementary common and specific features for downstream tasks. Particularly, we provide a theoretical explanation for our method from an information bottleneck perspective. Compared with state-of-the-art multi-view models on benchmark datasets, we empirically demonstrate the advantage of our method in several downstream tasks, such as ordinary classification and few-shot learning. In addition, extensive experiments validate the robustness and transferability of our approach, when applying the learned representation on the source dataset to several target datasets.}
}
@article{ZHENG2024110369,
title = {Motion-guided and occlusion-aware multi-object tracking with hierarchical matching},
journal = {Pattern Recognition},
volume = {151},
pages = {110369},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110369},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001201},
author = {Yujin Zheng and Hang Qi and Lei Li and Shan Li and Yan Huang and Chu He and Dingwen Wang},
keywords = {Spatio-temporal, Occlusion, Spatial perception, Multiple object tracking},
abstract = {In the field of multi-target tracking, the widely embraced tracking-by-detection paradigm has rapidly progressed with the refinement of detectors and matching techniques. However, the paradigm of joint detection and tracking is relatively limited, and it is difficult to model complex scenes, such as the complexities introduced by camera motion and occlusion. In this work, a hierarchical joint detection and tracking framework is proposed, namely MSPNet. From a temporal concern, a motion-guided feature aggregation module is proposed to address the complexities of multi-frame variations. From a spatial concern, an occlusion-aware head and hierarchical spatial association are proposed to handle the challenges of occlusion. Extensive experiments on MOT challenging benchmarks demonstrate that the MSPNet can effectively reduce false negatives and improve the accuracy of tracking while outperforming a wide range of existing methods.}
}
@article{CHAN2024110353,
title = {Dog identification based on textural features and spatial relation of noseprint},
journal = {Pattern Recognition},
volume = {151},
pages = {110353},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110353},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001043},
author = {Yung-Kuan Chan and Chuen-Horng Lin and Ching-Lin Wang and Keng-Chang Tu and Shu-Chun Yang and Meng-Hsiun Tsai and Shyr-Shen Yu},
keywords = {Dog identification, Dog noseprint, Detection, Segmentation, Scaly block},
abstract = {This study proposes dog identification technology based on dog noseprints, which are equivalent to human fingerprints and possess unique characteristics. The aim is to utilize this technology for identifying and managing stray animals. The study presents three processing stages. In the first stage, YOLOv3 detects the dog's nose and nostril regions. The second stage involves enhancing the image's contrast and the contour of the scaly block using the multi-scale line detector. Finally, in the third stage, the shape and spatial features of the scaly block are extracted and utilized for dog identification. The study included a dataset of 276 dogs from multiple animal families and public shelters in Taiwan. The dataset was randomly divided into three groups to determine the optimal parameters for matching the dog noseprint via experimentation. Each dog identification group achieved an accuracy (ACC) exceeding 97.83 %, demonstrating that the proposed parameter-matching method offers high stability. Furthermore, in an additional experimental dataset consisting of dog noseprint images used for dog identification, the proposed method achieved an ACC exceeding 90.22 % in the Top 1 and 94.57 % in the Top 3. The ACC results across different groups consistently demonstrate the proposed method's ability to achieve high accuracy in dog identification. The source code and trained models are publicly available at: https://github.com/Chuen-HorngLin/Dog-Identification-Noseprint.}
}
@article{JIANG2024110388,
title = {A randomized algorithm for clustering discrete sequences},
journal = {Pattern Recognition},
volume = {151},
pages = {110388},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110388},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001390},
author = {Mudi Jiang and Lianyu Hu and Xin Han and Yong Zhou and Zengyou He},
keywords = {Sequence clustering, Sequential data analysis, Cluster analysis, Randomized algorithm},
abstract = {Cluster analysis is one of the most important research issues in data mining and machine learning. To date, numerous clustering algorithms have been proposed to tackle the fixed-length vector data. In many real applications, we need to detect clusters from a set of discrete sequences in which each sequence is an ordered list of items. Due to the sequential and discrete nature, the discrete sequence clustering problem is more challenging and most of existing vector data clustering algorithms cannot be directly employed. In this paper, we present a stochastic algorithm for clustering discrete sequences. Our method first quickly generates a set of random partitions over the sequential data set and then merges these random clustering results via weighted graph construction and partition. We perform extensive empirical comparisons on real data sets to show that our method is comparable to those state-of-the-art clustering algorithms with respect to both accuracy and efficiency.}
}
@article{WU2024110380,
title = {Query-centric distance modulator for few-shot classification},
journal = {Pattern Recognition},
volume = {151},
pages = {110380},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110380},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001316},
author = {Wenxiao Wu and Yuanjie Shao and Changxin Gao and Jing-Hao Xue and Nong Sang},
keywords = {Few-shot classification, Distance metric learning-based, Channel-weighting, Query-centric distance modulator},
abstract = {Few-shot classification (FSC) is a highly challenging task, as only a small number of labeled samples are available when identifying new categories. Distance metric learning-based methods have emerged as a prominent approach to FSC, which typically use a distance function to measure the difference between query and support samples for identifying the class membership of the query sample. However, these methods simply treat each channel difference between query and support features equally when computing the class scores. Since different channels in the learned feature seek for different patterns, these distance metrics fail to consider that different channels are of different importance to FSC, and thus cannot accurately measure the similarity between samples. To address this issue, we propose a Query-Centric Distance Modulator (QCDM) to generate query-related weights for each channel difference adaptively. Specifically, since the distribution of difference between a query sample and all support samples in a particular channel can reflect the importance of this channel to the classification of the query sample, we take this difference vector as input and generate a query-specific channel weight through a meta-network. QCDM can guide FSC models to focus on discriminative channel differences and achieve better generalization. QCDM is a plug-and-play module that can be seamlessly integrated with existing distance metric learning-based FSC methods. Extensive experimental results indicate that our method can effectively improve the performance of distance metric learning-based FSC methods. The source code is available in https://github.com/Wu-Wenxiao/QCDM.}
}
@article{WU2024110364,
title = {Graph Convolutional Network with elastic topology},
journal = {Pattern Recognition},
volume = {151},
pages = {110364},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110364},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001158},
author = {Zhihao Wu and Zhaoliang Chen and Shide Du and Sujia Huang and Shiping Wang},
keywords = {Graph convolutional networks, Semi-supervised classification, Learnable topology, Orthogonal constraint},
abstract = {Graph Convolutional Network (GCN) has drawn widespread attention in data mining on graphs due to its outstanding performance and rigor theoretical guarantee. However, some recent studies have revealed that GCN-based methods may mine latent information insufficiently owing to the underutilization of the feature space. Besides, the unlearnable topology also significantly imperils the performance of GCN-based methods. In this paper, we conduct experiments to investigate these issues, finding that GCN does not fully consider the potential structure in the feature space, and a fixed topology deteriorates the robustness of GCN. Thus, it is desired to distill node features and establish a learnable graph. Motivated by this goal, we propose a framework dubbed Graph Convolutional Network with elastic topology (GCNet11The source code is available at https://github.com/ZhihaoWu99/GCNet.). With the analysis of the optimization for the proposed flexible Laplacian embedding, GCNet is naturally constructed by alternative graph convolutional layers and adaptive topology learning layers. GCNet aims to deeply explore the feature space and employ the mined information to construct a learnable topology, which leads to a more robust graph representation. In addition, a set-level orthogonal loss is utilized to meet the orthogonal constraint required by the flexible Laplacian embedding and promote better class separability. Moreover, comprehensive experiments indicate that GCNet achieves remarkable performance and generalization on several real-world datasets.}
}
@article{SHARMA2024110351,
title = {Integrated convolutional neural networks for joint super-resolution and classification of radar images},
journal = {Pattern Recognition},
volume = {150},
pages = {110351},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110351},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400102X},
author = {Rahul Sharma and Bhabesh Deka and Vincent Fusco and Okan Yurduseven},
keywords = {Deep learning, Classification, Super-resolution, Computational imaging, Coded aperture},
abstract = {Deep learning techniques have been widely used for two-dimensional (2D) and three-dimensional (3D) computer vision problems, such as object detection, super-resolution (SR) and classification to name a few. Radar images suffer from poor resolution as compared to optical images, hence developing a high-accuracy model to solve computer vision problems, such as a classifier, is a challenge. This is because of the lack of high-frequency details in the input images which makes it difficult for the classifier model to generate accurate predictions. Ways of addressing this challenge include training the learning model with a large dataset or using a more complicated model, such as deeper layer architecture. However, employing such approaches might result in the overfitting of the model, where the model might not generalize well on previously unseen data. Also, generating a large dataset for training the model is a challenging task, especially in the case of radar images. An alternate solution for achieving high accuracy in radar classification problems is provided in this paper wherein a CNN-enabled super-resolution (SR) model is integrated with the classifier model. The SR model is trained to generate high-resolution (HR) millimeter-wave (mmW) images from any input low-resolution (LR) mmW images. These resolved images from the SR model will be used by the classifier model to classify the input images into appropriate classes, consisting of threat and non-threat objects. The training data for the dual CNN layers are generated using a numerical model of a near-field coded-aperture computational imaging (CI) system. This trained dual CNN model is tested with simulated data generated from the CI numerical model wherein a high classification accuracy of 95% and a fast inference time of 0.193 s are obtained, making it suitable for real-time automated threat classification applications. For fair comparison, the developed CNN model is also validated with experimentally generated reconstruction data, in which case, a classification accuracy of 94% is obtained.}
}
@article{PENG2024110335,
title = {OLCH: Online Label Consistent Hashing for streaming cross-modal retrieval},
journal = {Pattern Recognition},
volume = {150},
pages = {110335},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110335},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000864},
author = {Shu-Juan Peng and Jinhan Yi and Xin Liu and Yiu-ming Cheung and Zhen Cui and Taihao Li},
keywords = {Cross-modal hashing, Online label consistent hashing, Mini-batch online gradient descent, Forward–backward splitting},
abstract = {Cross-modal hashing has received growing interest to facilitating efficient retrieval across large-scale multi-modal data, and existing methods still face three challenges: 1) Most offline learning works are unsuitable for processing and training with streaming multi-modal data. 2) Current online learning methods rarely consider the potential interdependency between the label categories. 3) Existing supervised methods often utilize pairwise label similarities or adopt relaxation scheme to learn hash codes, which, respectively, require much computation time or accumulate large quantization loss during the learning process. To alleviate these challenges, this paper presents an efficient Online Label Consistent Hashing (OLCH) approach for streaming cross-modal retrieval. The proposed approach first exploits the relative similarity of semantic labels and utilizes the multi-class classification to derive the common semantic vector. Then, an online semantic representation learning framework is adaptively designed to preserve the semantic similarity across different modalities, and a mini-batch online gradient descent approach associated with forward–backward splitting is developed to discriminatively optimize the hash functions. Accordingly, the hash codes are incrementally learned with high discriminative capability, while avoiding high computation complexity to process the streaming data. Extensive experiments highlight the superiority of the proposed approach and show its very competitive performance in comparison with the state-of-the-arts.}
}
@article{YIN2024110392,
title = {Embrace sustainable AI: Dynamic data subset selection for image classification},
journal = {Pattern Recognition},
volume = {151},
pages = {110392},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110392},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001432},
author = {Zimo Yin and Jian Pu and Ru Wan and Xiangyang Xue},
keywords = {Data selection, Dynamic subset selection, Weighted sampling, Class distribution, Training efficiency},
abstract = {Data selection is commonly used to reduce costs and energy usage by training on a subset of available data. However, determining the appropriate subset size requires extensive dataset knowledge and experimentation, limiting transferability. Varying the validation set also produces unstable results and wastes computational resources. In this paper, we propose a data selection method for dynamically determining subset ratios based on model performance using only a training set. The data search space is narrowed through weighted sampling, leveraging statistical selection patterns. Parallel analysis of class distributions identifies the most representative samples with high selection potential. Extensive experiments validate our approach and demonstrate improved training efficiency. Our method speeds up various subset ratios by up to 2.2x on CIFAR-10, 1.9x on CIFAR-100, 2.0x on TinyImageNet, and 2.3x on ImageNet with negligible accuracy drops.}
}
@article{WANG2024110352,
title = {Prior based Pyramid Residual Clique Network for human body image super-resolution},
journal = {Pattern Recognition},
volume = {150},
pages = {110352},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110352},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001031},
author = {Simiao Wang and Yu Sang and Yunan Liu and Chunpeng Wang and Mingyu Lu and Jinguang Sun},
keywords = {Human body super-resolution, Residual clique block, Pyramid residual clique network, Uniform discrete curvelet transform},
abstract = {Recent research in the analysis of human images, such as human parsing and pose estimation, usually requires input images to have a sufficiently high resolution. However, small images of people are commonly encountered in our daily lives, particularly in surveillance applications. This paper aims to ultra-resolve a tiny person image to its high-resolution counterpart by learning effective feature representations and exploiting useful human body prior knowledge. First, we propose the Residual Clique Block (RCB) to fully exploit compact feature representations for image Super-Resolution (SR). Second, a series of RCBs are cascaded in a coarse-to-fine manner to construct the Pyramid Residual Clique Network (PRCN), which simultaneously reconstructs multiple SR results (e.g. 2×, 4×, and 8×) in one feed-forward pass. Third, we utilize the human parsing map as the shape prior, and the high-frequency sub-bands of Uniform Discrete Curvelet Transform (UDCT) as the texture prior to enhance the details of reconstructed human body image. Experimental results demonstrate that our proposed method achieves state-of-the-art performance with superior visual quality and PSNR/SSIM scores. Moreover, we show that our results can considerably enhance the performance of human parsing and pose estimation tasks.}
}
@article{ZHAO2024110376,
title = {Global and local multi-modal feature mutual learning for retinal vessel segmentation},
journal = {Pattern Recognition},
volume = {151},
pages = {110376},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110376},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001274},
author = {Xin Zhao and Jing Zhang and Qiaozhe Li and Tengfei Zhao and Yi Li and Zifeng Wu},
keywords = {Mutual learning, Multi-modal learning, OCTA images, Retinal vessel segmentation},
abstract = {Research on optical coherence tomography angiography (OCTA) images has received extensive attention in recent years since it provides more detailed information about retinal structures. The automatic segmentation of retinal vessel (RV) has become one of the key issues in the quantification of retinal indicators. To this end, there are various methods proposed with cutting-edge designs and techniques in the literature. However, most of them only learn features from single-modal data, despite the potential relation between data from different modalities. Clinically, 2D projection maps are more convenient for doctors to observe. Nevertheless, 3D volumes preserve the intrinsic retinal structure. We thus propose a novel multi-modal feature mutual learning framework that contains local mutual learning and global mutual learning capturing 2D and 3D information. In the framework, the 3D model and 2D model learn collaboratively and teach each other throughout the training process. Experimental results show that our method outperforms previous deep-learning methods in RV segmentation. The generalization experiments on the ROSE dataset demonstrate the portability and scalability of the proposed framework.}
}
@article{GUO2024110329,
title = {DSCA: A Dual Semantic Correlation Alignment Method for domain adaptation object detection},
journal = {Pattern Recognition},
volume = {150},
pages = {110329},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110329},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000803},
author = {Yinsai Guo and Hang Yu and Shaorong Xie and Liyan Ma and Xinzhi Cao and Xiangfeng Luo},
keywords = {Object detection, Domain Adaptive Object Detection, Domain adaptation, Semantic information, Semantic Correlation Alignment},
abstract = {In self-driving cars, adverse weather (e.g., fog, rain, snow, and cloud) or occlusion scenarios result in domain shift being unavoidable in object detection. Researchers have recently proposed Domain Adaptive Object Detection (DAOD), i.e., aligning the source and target domains at the image and instance levels distribution by utilizing the Unsupervised Domain Adaptation (UDA) method. However, the semantic correlation information is ignored leading to the effect of aligning not good, and low detection accuracy of objects in adverse weather or occlusion scenarios. Here, we propose a Dual Semantic Correlation Alignment (DSCA) method for DAOD to address the problem. The core idea behind DSCA is to make full use of semantic correlation information including context correlation semantic information and class correlation semantic information to align object semantic information in source and target domains, which supplement and enhance the missing information for target domains. It consists of a two-level semantic alignment: (1) context correlation semantic alignment is developed to obtain the context correlation semantic information of the object to align context semantic information at the image level; (2) class correlation semantic alignment is proposed to obtain the class correlation semantic information of the object to align class semantic information at the instance level. The two-level semantic alignment can effectively decrease negative transfer and complete object information to improve the detection accuracy of objects in different domains. Experiments on four challenging benchmarks show that our proposed DSCA method outperforms state-of-the-art DAOD methods.}
}
@article{RAJ2024110301,
title = {Leveraging spatio-temporal features using graph neural networks for human activity recognition},
journal = {Pattern Recognition},
volume = {150},
pages = {110301},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110301},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000529},
author = {M.S. Subodh Raj and Sudhish N. George and Kiran Raja},
keywords = {Covariance descriptor, Graph neural network, Human activity, Subspace clustering},
abstract = {Unsupervised human activity recognition (HAR) algorithms working on motion capture (mocap) data often use spatial information and neglect the activity-specific information contained in the temporal sequences. In this work, we propose a new unsupervised algorithm for HAR from mocap data to leverage both spatial and temporal information embedded in activity sequences. For this, we employ a shallow graph neural network (GNN) comprising a graph convolutional network and a gated recurrent unit to aggregate the spatial and temporal features of the mocap sequences, respectively. Moreover, we encode the transformations of the human body through log-regularized kernel covariance descriptors linked to the trajectory movement maps of mocap frames. These descriptors are then fused with the GNN features for downstream activity recognition tasks. Finally, HAR is performed by a new unsupervised algorithm using a neighborhood Laplacian regularizer and a normalized dictionary learning approach. The generalizability of the proposed model is validated by training the GNN on a public dataset and testing on the other datasets. The performance of the proposed model is evaluated using six publicly available human mocap datasets. Compared to existing approaches, the proposed model improves activity recognition consistently by 12%–30% across different datasets.}
}
@article{WANG2024110382,
title = {Adaptive and fuzzy locality discriminant analysis for dimensionality reduction},
journal = {Pattern Recognition},
volume = {151},
pages = {110382},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110382},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400133X},
author = {Jingyu Wang and Hengheng Yin and Feiping Nie and Xuelong Li},
keywords = {Adaptive and fuzzy k-means, Discrete fuzzy membership, Subblock partition, Locality discriminant analysis},
abstract = {Linear discriminant analysis (LDA) uses labeled samples for acquiring a discriminant projection direction, by which data of different categories are separated into distinct groups in a lower-dimensional subspace. In response to the issue that LDA lacks robustness to non-Gaussian data with rich local information, improvements on LDA explore the subspace manifold structure by building a fully connected similarity graph. However, these methods are vulnerable to the interference of noisy and redundant information. In this paper, we propose a new local LDA method, named adaptive and fuzzy locality discriminant analysis (AFLDA), which aims at extracting precise and compact features. Firstly, an adaptive and fuzzy k-means strategy is adopted, where the membership between data and corresponding subclass centers for each class is learned to establish a hybrid bipartite graph and capture local information. Secondly, we design a discrete and probability constraint imposed on the membership matrix to explore the intricate structure of multimodal data. Moreover, the subblock partition for each class makes data accommodate to multimodal subclasses. Thirdly, the maximum total scatter regularization term is introduced, which amply disperses the data to enhance the recognition of local structure and avoid trivial solutions. Finally, to eliminate the interference of noisy and redundant information, AFLDA learns an optimized subspace, where cluster centers and the membership matrix are updated alternately. Promising results in experiments illustrate the efficacy of the model.}
}
@article{GARCIAPEDRAJAS2024110342,
title = {A thorough experimental comparison of multilabel methods for classification performance},
journal = {Pattern Recognition},
volume = {151},
pages = {110342},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110342},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000931},
author = {Nicolás E. García-Pedrajas and José M. Cuevas-Muñoz and Gonzalo Cerruela-García and Aida {de Haro-García}},
keywords = {Multilabel learning, Performance comparison, Study of models},
abstract = {Multilabel classification as a data mining task has recently attracted increasing interest from researchers. Many current data mining applications address problems with instances that belong to more than one class. These problems require the development of new, efficient methods. Advantageously using the correlation among different labels can provide better performance than methods that manage each label separately. In recent decades, many methods have been developed to deal with multilabel datasets, which makes it difficult to decide which method is the most appropriate for a given task. In this paper, we present the most comprehensive comparison carried out so far. We compare a total of 62 different methods and several configurations of each one for a total of 197 trained models. We also use a large set of problems comprising 65 datasets. In addition, we studied the efficiency of the methods considering six different classification performance metrics. Our results show that, although there are methods that repeatedly appear among the top-performing models, the best methods are closely related to the metric used for evaluating the performance. We also analyzed different aspects of the behavior of the methods.}
}
@article{QIAO2024110314,
title = {Reading order detection in visually-rich documents with multi-modal layout-aware relation prediction},
journal = {Pattern Recognition},
volume = {150},
pages = {110314},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110314},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000657},
author = {Liang Qiao and Can Li and Zhanzhan Cheng and Yunlu Xu and Yi Niu and Xi Li},
keywords = {Document understanding, Reading order detection, Layout-aware positional embedding},
abstract = {Reading order detection aims to arrange the text logically, which is essential in understanding visual documents. Current methods mostly model the problem as a sequence generation task, which use insufficient modalities information ignore the various reading habits under different document layouts, leading to the lack of robustness for some complex scenarios. To address these challenges, we present a novel approach with the Multi-Modal Layout-Aware Relation Prediction. It employs a straightforward yet highly effective task formulation for predicting the order relation between text instances. Our model leverages visual, semantic, and positional features, with the positional features being adaptively generated through a layout-aware position embedding module. Then, different modality features are enhanced via a two-staged position-guided multi-modal fusion module. Additionally, we introduce two novel loss functions, Degree Loss and Cycle Loss, to effectively impose network constraints at multiple levels. Our experimental results, conducted on three real-world datasets, demonstrate that our proposed method achieves a new state-of-the-art level of performance.}
}
@article{CHEN2024110359,
title = {Camera-aware cluster-instance joint online learning for unsupervised person re-identification},
journal = {Pattern Recognition},
volume = {151},
pages = {110359},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110359},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001109},
author = {Zhaoru Chen and Zheyi Fan and Yiyu Chen and Yixuan Zhu},
keywords = {Unsupervised person re-identification, Camera variation, Online pseudo-label, Contrastive learning},
abstract = {Unsupervised person re-identification (re-ID) aims at learning discriminative feature representations for person retrieval without any annotations. Pseudo-label-based methods that iteratively perform pseudo-label generation and model training are currently the most popular approach to achieve this goal. However, distribution variations among cameras inevitably introduce noise in the generated pseudo-labels. Moreover, they are often assigned offline using relatively simple clustering criteria, which further accumulates the noise and limits the potential improvement in model performance. To address these issues, we propose a novel camera-aware cluster-instance joint online learning (CCIOL) framework that leverages the online inter-camera K-reciprocal nearest neighbors (OICKRNs) mined for each sample at every iteration to soften the traditional hard pseudo-labels at the cluster-level and generate multi-labels at the instance-level. Additionally, contrastive learning losses at two levels are employed to rectify the erroneous closeness between samples and promote intra-class aggregation and inter-class separation. Extensive experimental results on Market1501 and MSMT17 demonstrate the competitiveness of the proposed method compared to state-of-the-art unsupervised re-ID approaches.}
}
@article{ZHANG2024110411,
title = {Multi-label feature selection via latent representation learning and dynamic graph constraints},
journal = {Pattern Recognition},
volume = {151},
pages = {110411},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110411},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001626},
author = {Yao Zhang and Wei Huo and Jun Tang},
keywords = {Multi-label learning, Feature selection, Latent representation learning, Dynamic graph, Manifold learning},
abstract = {As an effective method to deal with the curse of dimensionality, multi-label feature selection aims to select the most representative subset of features by eliminating unfavorable features. Although great progress has been made in this field, how to mine adequate supervisory information from multi-label data remains a key challenge. Compared to the latent information of instances, the latent information of instance relevance contains both the basic information of instances and the latent relevance between instances. Base on this knowledge, we propose a novel multi-label feature selection method named LRDG that explores latent representation learning and dynamic graph constraints. Specifically, we introduce the latent representation of instance relevance as supervisory information for pseudo-label learning, and minimize information loss during pseudo-label learning by means of the label manifold, the non-negative constraints, and the minimization of the Frobenius norm between pseudo-labels and ground-truth labels. In addition, considering the shortcomings brought by traditional graph regularization, we propose to use the dynamic graph constructed from low-dimensional pseudo-labels to constrain feature weights. Extensive experiments on various multi-label datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/yunbao520/LRDG.}
}
@article{VILAR2024110326,
title = {A categorical interpretation of state merging algorithms for DFA inference},
journal = {Pattern Recognition},
volume = {150},
pages = {110326},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110326},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000773},
author = {Juan Miguel Vilar},
keywords = {Grammatical inference, DFA, Category theory, RPNI, EDSM},
abstract = {We use Category Theory to interpret the family of algorithms for inference of DFAs that work by merging states. This interpretation allows us to characterize the structure of the search space and to define criteria for the convergence of these algorithms to the correct DFA. We also prove that the well-known EDSM algorithm does not identify DFAs in the limit.}
}
@article{CHEN2024110298,
title = {Fast and explainable clustering based on sorting},
journal = {Pattern Recognition},
volume = {150},
pages = {110298},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110298},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000499},
author = {Xinye Chen and Stefan Güttel},
keywords = {Clustering, Fast aggregation, Sorting, Explainability},
abstract = {We introduce a fast and explainable clustering method called CLASSIX. It consists of two phases, namely a greedy aggregation phase of the sorted data into groups of nearby data points, followed by the merging of groups into clusters. The algorithm is controlled by two scalar parameters, namely a distance parameter for the aggregation and another parameter controlling the minimal cluster size. Extensive experiments are conducted to give a comprehensive evaluation of the clustering performance on synthetic and real-world datasets, with various cluster shapes and low to high feature dimensionality. Our experiments demonstrate that CLASSIX competes with state-of-the-art clustering algorithms. The algorithm has linear space complexity and achieves near linear time complexity on a wide range of problems. Its inherent simplicity allows for the generation of intuitive explanations of the computed clusters.}
}
@article{GALKA2024110395,
title = {Deterministic attribute selection for isolation forest},
journal = {Pattern Recognition},
volume = {151},
pages = {110395},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110395},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001468},
author = {Łukasz Gałka and Paweł Karczmarek},
keywords = {Anomaly detection, Isolation forest, Outlier detection, Deterministic model},
abstract = {Modern data mining techniques have been gained importance in recent years. In particular, anomaly detection algorithms, applied in key sectors of information technology, have been growing in popularity. One of the efficient and fast algorithms is Isolation Forest. The method consists of two separated stages: Forest formation and evaluation of elements. The first stage relies on forming a forest of isolation trees. Each tree is built in the same manner according to drawn samples and random divisions of data attributes. In this study, an innovative deterministic attribute selection method is proposed, maintaining its random value. New ideas based on imbalance, clustering, and a dispersion of values through non-linear transformation of elements are introduced and thoroughly analyzed. These novel anomaly detection approaches are applied to 25 real datasets, as well as our own artificially generated databases. The Area Under the ROC Curve and the Area Under the PR Curve are used as a measure of the outliers classification quality. The results of the numerical experiment have proven high efficiency and competitive evaluation speed of the proposals in comparison to other Isolation Forest-based approaches, as well as several other popular techniques.}
}
@article{HE2024110383,
title = {Introspective GAN: Learning to grow a GAN for incremental generation and classification},
journal = {Pattern Recognition},
volume = {151},
pages = {110383},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110383},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001341},
author = {Chen He and Ruiping Wang and Shiguang Shan and Xilin Chen},
keywords = {Incremental learning, Catastrophic forgetting, Generative Adversarial Networks},
abstract = {Lifelong learning, the ability to continually learn new concepts throughout our life, is a hallmark of human intelligence. Generally, humans learn a new concept by knowing what it looks like and what makes it different from the others, which are correlated. Those two ways can be characterized by generation and classification in machine learning respectively. In this paper, we carefully design a dynamically growing GAN called Introspective GAN (IntroGAN) that can perform incremental generation and classification simultaneously with the guidance of prototypes, inspired by their roles of efficient information organization in human visual learning and excellent performance in other fields like zero-shot/few-shot/incremental learning. Specifically, we incorporate prototype-based classification which is robust to feature change in incremental learning and GAN as a generative memory to alleviate forgetting into a unified end-to-end framework. A comprehensive benchmark on the joint incremental generation and classification task is proposed and our method demonstrates promising results. Additionally, we conduct comprehensive analyses over the properties of IntroGAN and verify that generation and classification can be mutually beneficial in incremental scenarios, which is an inspiring area to be further exploited. The code is available at https://github.com/TonyPod/IntroGAN.}
}
@article{HU2024110355,
title = {Robust multi-view learning via M-estimator joint sparse representation},
journal = {Pattern Recognition},
volume = {151},
pages = {110355},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110355},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001067},
author = {Yutao Hu and Yulong Wang and Han Li and Hong Chen},
keywords = {Robust multi-view learning, Joint sparse representation, M-estimator},
abstract = {Recently, multi-view learning has achieved extraordinary success in many research areas such as pattern recognition and data mining. Most existing multi-view methods mainly focus on exploring the correlation information between different views and their performance may severely degrade in the presence of heavy noises and outliers. In this paper, we put forward a robust multi-view joint sparse representation (RMJSR) method for multi-view learning. Firstly, we design a novel multi-view Cauchy estimator based loss function originating from robust statistics to address complex noises and outliers in reality. Based on this, we leverage the ℓ1,q norm to enhance our model by encouraging the learned representation of multiple views to share the same sparsity pattern. Secondly, to explore the optimal solution for the RMJSR model, we devise an effective optimization algorithm based on the half-quadratic (HQ) theory and the alternating direction method of multipliers (ADMM) framework. Thirdly, we provide the theoretical guarantee for revealing the theoretical condition for the success of the proposed method. Further, we have also provided extensive analysis of the proposed method, including the optimality condition, convergence analysis, and complexity analysis. Extensive experimental results validate the effectiveness and robustness of the proposed method in comparison with state-of-the-art competitors. The source code is available at https://github.com/Huyutao7/RMJSRC.}
}
@article{WANG2024110302,
title = {Cross-frame feature-saliency mutual reinforcing for weakly supervised video salient object detection},
journal = {Pattern Recognition},
volume = {150},
pages = {110302},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110302},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000530},
author = {Jian Wang and Siyue Yu and Bingfeng Zhang and Xinqiao Zhao and Ángel F. García-Fernández and Eng Gee Lim and Jimin Xiao},
keywords = {Video salient object detection, Scribble annotations, Cross-frame feature consistency, Cross-frame saliency consistency},
abstract = {Scribble annotations have recently become popular in video salient object detection. Previous methods only focus on utilizing shallow feature consistency for more integral predictions. However, there is potential for consistency between cross-frame deep features to be used to help regularize saliency predictions better. Besides, we have observed that leveraging saliency predictions as pseudo-supervision signals yields notable improvements in extracting both intra-frame and cross-frame deep features. This, in turn, leads to more precise and detailed object structural information. Thus, we propose a cross-frame feature-saliency mutual reinforcing training process to assist scribble annotations for integral video saliency predictions. Specifically, we design a cross-frame feature regularization head, which leverages intra-frame and cross-frame deep feature consistency to regularize saliency predictions as auxiliary supervision. Then, to help obtain more accurate feature consistency, we design a cross-frame saliency regularization head, where predicted saliency values are used as pseudo-supervision signals to acquire better feature consistency. In this way, our cross-frame feature and saliency regularization heads can benefit from each other to help the network learn more accurately. Extensive experiments show that our method can achieve better performances than the previous best methods. The project is available at https://github.com/muchengxue0911/CFMR.}
}
@article{HUANG2024110367,
title = {Relation fusion propagation network for transductive few-shot learning},
journal = {Pattern Recognition},
volume = {151},
pages = {110367},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110367},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001183},
author = {Yixiang Huang and Hongyu Hao and Weichao Ge and Yang Cao and Ming Wu and Chuang Zhang and Jun Guo},
keywords = {Few-shot learning, Image classification, Graph neural network},
abstract = {Previous graph-based meta-learning approaches have explored pairwise feature similarity to learn instance-level relations of samples, however, the gap between the sample relations in feature and label spaces is often ignored. It is empirically observed that instances with different labels may display considerable similarity in visual characteristics, making it challenging to distinguish between them in the feature space. To this end, we propose a dual-branch Relation Fusion Propagation Network (RFPN) for transductive few-shot learning, which explicitly models both feature and label relations across support-query pairs. Specifically, we design a Relation Fusion Block (RFB) to fuse instance-level and class-level relations, thus obtaining more robust fusion-level relations to guide feature propagation. In addition to the feature propagation branch, we encode the label relations and construct a label shortcut branch for label propagation. To alleviate the error accumulation during propagation, which is caused by uncertain pseudo-labels for query samples, we propose a Label Shortcut Mechanism (LSM) to progressively update the sample relations with the initial labels. Our full method is plug-and-play and can be easily applied in existing graph-based approaches for transductive few-shot learning. Extensive experiments demonstrate that our proposed RFPN yields significant improvements over the baselines and achieves promising performance on four popular few-shot classification benchmarks.}
}
@article{DING2024110366,
title = {Survey of spectral clustering based on graph theory},
journal = {Pattern Recognition},
volume = {151},
pages = {110366},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110366},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001171},
author = {Ling Ding and Chao Li and Di Jin and Shifei Ding},
keywords = {Spectral clustering, Similarity graph, Graph cut, Laplacian matrix, Eigenvector},
abstract = {Spectral clustering converts the data clustering problem to the graph cut problem. It is based on graph theory. Due to the reliable theoretical basis and good clustering performance, spectral clustering has been successfully applied in many fields. Although spectral clustering has many advantages, it faces the challenges of high time and space complexity when dealing with large scale complex data. Firstly, this paper introduces the basic concept of graph theory, reviews the properties of Laplacian matrix and the traditional graph cuts method. Then, it focuses on four aspects of the realization process of spectral clustering, including the construction of similarity matrix, the establishment of Laplacian matrix, the selection of eigenvectors and the determination of the number of clusters. In addition, some successful applications of spectral clustering are summarized. In each aspect, the shortcomings of spectral clustering and some representative improved algorithms are emphatically analyzed. Finally, the paper comprehensively analyzes some research on spectral clustering that has not yet been in-depth, and gives prospects on some valuable research directions.}
}
@article{GAO2024110340,
title = {Global feature-based multimodal semantic segmentation},
journal = {Pattern Recognition},
volume = {151},
pages = {110340},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110340},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000918},
author = {Suining Gao and Xiubin Yang and Li Jiang and Zongqiang Fu and Jiamin Du},
keywords = {RGB-P, Multimodal fusion, Cross-filed attention, Semantic segmentation},
abstract = {Incorporating complementary modality into RGB branch can significantly improve the effectiveness of semantic segmentation. However, fusion between the two modalities faces huge challenge due to the difference of their optical dimensions. Existed fusion methods can't keep a balance between performance and efficiency in aggregating detailed features. To address this problem, we propose a global feature-based network (GFBN) for semantic segmentation that establishes mapping function and extraction relationship among the multi-modalities. The GFBN contains three important modules, which are used for feature correction, fusion and edge enhancement. Firstly, the cross-attention rectification module (CARM) adaptively extracts mapping relationships and rectifies the RGB and complementary features. Secondly, the cross-field fusion module (CFM) integrates long-range rectified features of two branches to obtain an optimal fusion feature. Finally, the boundary guidance module (BGM) sharpens the boundary information of the fused features to effectively improve the segmentation accuracy of object boundaries. We make the experiments of GFBN on the challenging MCubeS and ZJU-RGB-Ps datasets. The results show that GFBN outperforms state-of-the-art methods by at least 0.64 % and 0.7 % on mean intersection over union (mIoU), respectively. It demonstrates the performance and efficiency of our proposed method. The code corresponding to our method can be found at the following link: https://github.com/Sci-Epiphany/GFBNext.}
}
@article{LIU2024110300,
title = {Local kernels based graph learning for multiple kernel clustering},
journal = {Pattern Recognition},
volume = {150},
pages = {110300},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110300},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000517},
author = {Zheng Liu and Shiluo Huang and Wei Jin and Ying Mu},
keywords = {Kernel, Graph, Multiple kernel learning, Clustering},
abstract = {Multiple kernel clustering (MKC) has been extensively studied in recent years. The focus of MKC is how to explore the information of base kernels. Although existing methods have promising leaning abilities, they ignore the intrinsic local structure contained in base kernels, which may negatively affect their performances. To address the above problem, a novel method, termed as consensus graph learning based on local kernels (CGLLK), is introduced. CGLLK is based on the partitions extracted by kernel k-means. Specifically, we first design a simple yet effective scheme to construct the local kernels of base kernels and then a consensus graph is applied to capture the complementary information contained in the extracted partitions of local kernels. CGLLK also considers the prior knowledge existing in base kernels. Since the partitions of local kernels and the learning stage of the consensus graph contain useful information for each other, the two processes are optimized jointly. Extensive experiments on some popular datasets are carried out to verify the effectiveness of the proposed method. Experimental results illustrate that CGLLK is much more competitive than the state-of-the-art algorithms.}
}
@article{MONDAL2024110421,
title = {Discriminative Regularized Input Manifold for multilayer perceptron},
journal = {Pattern Recognition},
volume = {151},
pages = {110421},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110421},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001729},
author = {Rahul Mondal and Tandra Pal and Prasenjit Dey},
keywords = {Discriminative regularization, Discriminative Regularized Input Manifold (DRIM), Generalization, Input manifold regularizer (IMR), Kullback–Leibler divergence (KLD), Multilayer perceptron (MLP)},
abstract = {Multilayer perceptron (MLP) fails to discriminate the ambiguous inputs belonging to the overlapping regions of multiple classes, resulting in misclassification. To classify the input samples accurately according to their classes, removing the ambiguity that occurred due to the sharing of common input space is important. In this article, a novel neural network framework, called Discriminative Regularized Input Manifold MLP (DRIM-MLP) is proposed to reduce this ambiguity and improve the classification accuracy. The proposed framework consists of two different feed forward networks that are trained simultaneously: (i) DRIM and (ii) MLP. The proposed DRIM learns the input distribution during its training and the learnt information is incorporated during the training of MLP to reduce the ambiguity. Simultaneously, the class information learnt by MLP is incorporated into the DRIM to learn discriminative information of the input distribution. The hidden layer output of DRIM estimates (i) the input of DRIM using the weights of hidden and output layers of DRIM and (ii) the class output of MLP using the weights of hidden and output layers of MLP. Here, the estimated class output, learnt by the DRIM that contains information of the input distribution, is used as a regularizer for the MLP to minimize the difference between the estimated class output of DRIM and the estimated class output of MLP itself. The hidden layer of MLP also estimates: (i) the class output of MLP using the weights between hidden and output layers of MLP and (ii) the input of DRIM using the weights between hidden and output layers of DRIM. Here, the estimated input learnt by MLP that contains class information is used as a regularizer for DRIM to minimize the difference between the estimated input of DRIM itself and the estimated input of MLP. These two regularizers are respectively called the Input manifold Regularizer (ImR) and the Discriminative Regularizer (DiscR). The experimental results based on ten standard data sets strongly support the effectiveness of the proposed DRIM-MLP compared to conventional MLP, auto encoder based MLP (AE-MLP), denoising auto encoder based MLP (DAE-MLP), AE-MLP with KLD, DAE-MLP with KLD along with two recent works of state-of-the-art.}
}
@article{QI2024110410,
title = {UGNet: Uncertainty aware geometry enhanced networks for stereo matching},
journal = {Pattern Recognition},
volume = {151},
pages = {110410},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110410},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001614},
author = {Zhengkai Qi and Junkang Zhang and Faming Fang and Tingting Wang and Guixu Zhang},
keywords = {Stereo matching, Disparity regression, Uncertainty guidance, Geometry-enhanced},
abstract = {Stereo matching is a fundamental research area in the field of computer vision. In recent years, iterative methods based on Gated Recurrent Units (GRUs) have showcased remarkable achievements in this domain. Despite their high accuracy, these methods suffer from notable limitations such as a reliance on a large number of iterations and a tendency to lose high-frequency details. To address these issues, we propose a novel uncertainty-aware framework that combines 3D convolution and GRU-based iterations, aiming to improve efficiency and accuracy. Specifically, we first introduce a probabilistic method to jointly train the disparity map and its corresponding uncertainty map using 3D convolutions. Next, leveraging the uncertainty map as a guide, we develop a novel uncertainty reweighting iterative module to assist in identifying errors in the coarse disparity and cost volume, thereby refining the disparity estimation process and significantly improving the iteration efficiency. Moreover, we introduce a high-resolution refinement module that utilizes Pixel Difference Convolution (PDC) to incorporate additional gradient information. This module can fine-tune the disparity estimation to enhance accuracy. Finally, our network is evaluated on multiple widely-used benchmark datasets. The results demonstrate its proficiency in predicting precise boundaries and effectively reduce iterations. Our model achieves comparable performance to other state-of-the-art methods, ranking 1st on KITTI 2015, and 2nd on KITTI 2012. These results validate its strong performance and generalizability.}
}
@article{CHEN2024110313,
title = {HDR light field imaging of dynamic scenes: A learning-based method and a benchmark dataset},
journal = {Pattern Recognition},
volume = {150},
pages = {110313},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110313},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000645},
author = {Yeyao Chen and Gangyi Jiang and Mei Yu and Chongchong Jin and Haiyong Xu and Yo-Sung Ho},
keywords = {Light field imaging, High dynamic range, Dynamic scene, Ghost-free, Benchmark},
abstract = {Light field (LF) imaging is an effective way to enable immersive applications. However, limited by the potential well capacity of the image sensor, the acquired LF images suffer from low dynamic range and are thus prone to under-exposure or over-exposure. High dynamic range (HDR) LF imaging is an efficacious avenue to improve the LF imaging's dynamic range. Unfortunately, for dynamic scenes, existing methods are inclined to produce ghosting artifacts and lose details in the saturated regions, while potentially damaging the parallax structure of generated HDR LF images. To address the above challenges, in this paper, we propose a new ghost-free HDR LF imaging method based on a deformable aggregation and angular embedding network. Specifically, considering the four-dimensional geometric structure of the LF image, a deformable alignment module is first designed to handle dynamic regions in the spatial domain, and then the aligned spatial features are fully fused through an aggregation operation. Subsequently, an angular embedding module is constructed to explore angular information to enhance the aggregated spatial features. Based on this, the above two modules are cascaded in a multi-scale manner to achieve multi-level feature extraction and enhance the feature representation ability. Finally, a decoder is leveraged to recover the ghost-free HDR LF image from the enhanced multi-scale features. For performance evaluation, this paper establishes a large-scale benchmark dataset with multi-exposure inputs and ground truth images. Extensive experimental results show that the proposed method generates visually pleasing HDR LF images while preserving accurate angular consistency. Moreover, the proposed method surpasses the state-of-the-art methods in both quantitative and qualitative comparisons. The code and dataset will be available at https://github.com/YeyaoChen/HDRLFI.}
}
@article{CIAMARRA2024110337,
title = {FLODCAST: Flow and depth forecasting via multimodal recurrent architectures},
journal = {Pattern Recognition},
volume = {150},
pages = {110337},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110337},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000888},
author = {Andrea Ciamarra and Federico Becattini and Lorenzo Seidenari and Alberto {Del Bimbo}},
keywords = {Depth forecasting, Optical flow forecasting, Segmentation},
abstract = {Forecasting motion and spatial positions of objects is of fundamental importance, especially in safety-critical settings such as autonomous driving. In this work, we address the issue by forecasting two different modalities that carry complementary information, namely optical flow and depth. To this end we propose FLODCAST a flow and depth forecasting model that leverages a multitask recurrent architecture, trained to jointly forecast both modalities at once. We stress the importance of training using flows and depth maps together, demonstrating that both tasks improve when the model is informed of the other modality. We train the proposed model to also perform predictions for several timesteps in the future. This provides better supervision and leads to more precise predictions, retaining the capability of the model to yield outputs autoregressively for any future time horizon. We test our model on the challenging Cityscapes dataset, obtaining state of the art results for both flow and depth forecasting. Thanks to the high quality of the generated flows, we also report benefits on the downstream task of segmentation forecasting, injecting our predictions in a flow-based mask-warping framework.}
}
@article{WANG2024110324,
title = {Self-supervised local rotation-stable descriptors for 3D ultrasound registration using translation equivariant FCN},
journal = {Pattern Recognition},
volume = {150},
pages = {110324},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110324},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400075X},
author = {Yifan Wang and Tianyu Fu and Xinyu Chen and Jingfan Fan and Deqiang Xiao and Hong Song and Ping Liang and Jian Yang},
keywords = {Rotation-stable descriptor, Translation equivariance, Feature matching, Three-dimensions ultrasound},
abstract = {Rotation-stable descriptors are crucial for feature matching in medical image registration. Most existing descriptors rely on hand-crafted models to achieve rotation stability, which are susceptible to complex noise and fail to efficiently extract batches of three-dimensional features, particularly for ultrasound volume. In this study, a translation equivariant design was performed based on the fully convolutional network to extract descriptors at different positions in batches by removing position bias errors, thereby improving the descriptor extraction efficiency. Descriptor rotation consistency is used for self-supervised training to avoid the need for data annotation. Before matching, the image ROI is restructured to adjust the input size of the network, further improving the descriptor extraction efficiency. Then, the multi-consistencies filter based on the correlation among descriptors, spatial positions, and texture features is designed to preserve stable matched pairs for accurate and robust registration results. Classification experimental results based on rotation stability show that the descriptors extracted by the proposed method have high classification accuracy, particularly under interference, such as noise, blur, and artifacts. Experimental results of clinical ultrasound image registration show that the proposed method has a lower registration error of 3.59 ± 1.15 mm compared with other methods. In addition, the descriptor extraction network proposed in this study has low training costs and high processing speed, further revealing the potential of the proposed method in clinical applications.}
}
@article{ALI2024110370,
title = {Wavelet-based Auto-Encoder for simultaneous haze and rain removal from images},
journal = {Pattern Recognition},
volume = {150},
pages = {110370},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110370},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001213},
author = {Asfak Ali and Ram Sarkar and Sheli Sinha Chaudhuri},
keywords = {Dehaze, Derain, Auto encoder, U-Net, Wavelet, Noise removal},
abstract = {Noise introduced due to weather can reduce the efficiency of computer vision applications as the visibility of the objects in images is greatly affected. Haze and rain are the most common weather conditions seen in nature. However, most of the algorithms found in the literature apply rain and haze removal approaches separately. To this end, in this paper, we propose a novel Wavelet-based deep Auto-encoder, called WAE, for simultaneously removing the haze and rain effects from images. The proposed network uses wavelet transformation and inverse wavelet transformation as an alternative to down-sampling and up-sampling operations, respectively, in order to add sparsity to the network. By training the model on both spatial and frequency domains, it learns non-stationary features that are found to be useful to remove haze and rain effects from images. The proposed model is tested on several rain and haze-affected image datasets, and it performs well in terms of standard evaluation metrics like structural similarity index measure and peak signal-to-noise ratio. The code can be found at : https://github.com/asfakali/WAE.git.}
}
@article{LI2024110422,
title = {Dual teachers for self-knowledge distillation},
journal = {Pattern Recognition},
volume = {151},
pages = {110422},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110422},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001730},
author = {Zheng Li and Xiang Li and Lingfeng Yang and Renjie Song and Jian Yang and Zhigeng Pan},
keywords = {Model compression, Image classification, Self-knowledge distillation, Dual teachers},
abstract = {We introduce an efficient self-knowledge distillation framework, Dual Teachers for Self-Knowledge Distillation (DTSKD), where the student receives self-supervisions by dual teachers from two substantially different fields, i.e., the past learning history and the current network structure. Specifically, DTSKD trains a considerably lightweight multi-branch network and acquires predictions from each, which are simultaneously supervised by a historical teacher from the previous epoch and a structural teacher under the current iteration. To our best knowledge, it is the first attempt to jointly conduct historical and structural self-knowledge distillation in a unified framework where they demonstrate complementary and mutual benefits. The Mixed Fusion Module (MFM) is further developed to bridge the semantic gap between deep stages and shallow branches by iteratively fusing multi-stage features based on the top-down topology. Extensive experiments prove the effectiveness of our proposed method, showing superior performance over related state-of-the-art self-distillation works on three datasets: CIFAR-100, ImageNet-2012, and PASCAL VOC.}
}
@article{CHEN2024110394,
title = {Data filtering for efficient adversarial training},
journal = {Pattern Recognition},
volume = {151},
pages = {110394},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110394},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001456},
author = {Erh-Chung Chen and Che-Rung Lee},
keywords = {Adversarial training, Data pruning, Multiple objective optimization},
abstract = {Adversarial training has been considered to be one of the most effective strategies to defend against adversarial attacks. Most existing adversarial training methods have shown a trade-off between training cost and robustness. This paper explores a new optimization direction from training data to reduce the computational cost of adversarial training without scarifying robustness. First, we show that some adversarial examples are less important, meaning that removing them does not hurt the robustness. Second, we propose a method to identify insignificant adversarial examples at a minimal cost. Third, we demonstrate that our approach can be integrated with other adversarial training frameworks with few modifications. The experimental results show that combined with previous works, our approach not only reduces about 20% of computational cost on the CIFAR10 and CIFAR100 datasets but also improves about 1.5% natural accuracy. With less computational cost, it achieves 58.22%, 30.68%, and 41.92% robust accuracy on CIFAR10, CIFAR100, and ImageNet datasets respectively, which are higher than those of the original methods.}
}
@article{OLIVASPADILLA2024110418,
title = {Explainable AI in human motion: A comprehensive approach to analysis, modeling, and generation},
journal = {Pattern Recognition},
volume = {151},
pages = {110418},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110418},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001699},
author = {Brenda Elizabeth Olivas-Padilla and Sotiris Manitsaris and Alina Glushkova},
keywords = {Human movement analysis, Explainable models, State-space modeling, Wearable sensors, Body dexterity analysis, Human pose generation, Gesture recognition},
abstract = {Extensive research has been conducted on analyzing human movements, driven by its diverse practical applications such as human–robot interaction, human learning, and clinical diagnosis. However, the current state-of-the-art still encounters scientific challenges when it comes to modeling human movements. There are two key aspects that need to be addressed. Firstly, new models should consider the stochastic nature of human movement and the physical structure of the human body to accurately predict the patterns in full-body motion descriptors over time. Secondly, while deep learning algorithms have been utilized, they lack explainability in terms of predicting body posture sequences, making it essential to improve their comprehensible representation of human movement. This paper aims to tackle these challenges by presenting three innovative methods for creating explainable representations of human movement. The study formulates human body movement as a state-space model based on the Gesture Operational Model (GOM). Model parameters are estimated through either one-shot training employing Kalman Filters or data-intensive training utilizing artificial neural networks. The trained models are utilized for analyzing the dexterity of expert professionals in full-body movements, enabling the identification of dynamic associations between body joints and gesture recognition. Additionally, these models are employed to generate artificial professional movements.}
}
@article{ZHAO2024110354,
title = {FApSH: An effective and robust local feature descriptor for 3D registration and object recognition},
journal = {Pattern Recognition},
volume = {151},
pages = {110354},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110354},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001055},
author = {Bao Zhao and Zihan Wang and Xiaobo Chen and Xianyong Fang and Zhaohong Jia},
keywords = {3D feature description, Point cloud, 3D registration, Local reference axis},
abstract = {Three-dimensional (3D) local feature descriptor plays an important role in 3D computer vision because it is widely used to build point-to-point correspondences in many 3D vision applications. However, existing descriptors are difficult to have both high descriptiveness and strong robustness to various nuisances (e.g., noise and occlusion). To address this problem, a descriptor named Fully Attribute-pairs Statistical Histogram (FApSH) is proposed. FApSH is constructed on a local reference axis (LRA), and fully encodes the relevancy information of five attributes at each neighbor point by ten attribute-pair statistics. In this process, a priori distribution-based partition strategy is proposed for evenly distributing the attribute values of all neighbor points, and a radial distance-based histogram assignment method is proposed to improve the robustness to noise and outliers. The proposed methods are rigorously evaluated on six benchmark datasets with different application scenarios and nuisances. The results show that FApSH has high descriptiveness and strong robustness. It obviously outperforms the existing handcrafted descriptors, and is comparable to some superior learning-based descriptors. The results also show that the proposed priori distribution-based partition strategy significantly reduces the length and also improves the descriptiveness of FApSH, and the proposed radial distance-based histogram assignment method improves the robustness of FApSH on various datasets.}
}
@article{CZERKAWSKI2024110378,
title = {Neural Knitworks: Patched neural implicit representation networks},
journal = {Pattern Recognition},
volume = {151},
pages = {110378},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110378},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001298},
author = {Mikolaj Czerkawski and Javier Cardona and Robert Atkinson and Craig Michie and Ivan Andonovic and Carmine Clemente and Christos Tachtatzis},
keywords = {Generative models, Image denoising, Image inpainting, Image super-resolution, Image synthesis, Internal learning, Zero-shot learning},
abstract = {Optimizing images as output of a neural network has been shown to introduce a powerful prior for image inverse tasks, capable of producing solutions of reasonable quality in a fully internal learning context, where no external datasets are involved. Two potential technical approaches involve fitting a coordinate-based Multilayer Perceptron (MLP), or a Convolutional Neural Network to produce the result image as output. The aim of this work is to evaluate the two counterparts, as well as a new framework proposed here, named Neural Knitwork, which maps pixel coordinates to local texture patches rather than singular pixel values. The utility of the proposed technique is demonstrated on the tasks of image inpainting, super-resolution, and denoising. It is shown that the Neural Knitwork can outperform the standard coordinate-based MLP baseline for the tasks of inpainting and denoising, and perform comparably for the super-resolution task.}
}
@article{WANG2024110365,
title = {Learning spatial-spectral dual adaptive graph embedding for multispectral and hyperspectral image fusion},
journal = {Pattern Recognition},
volume = {151},
pages = {110365},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110365},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400116X},
author = {Xuquan Wang and Feng Zhang and Kai Zhang and Weijie Wang and Xiong Dun and Jiande Sun},
keywords = {Dual graph embedding, Adaptive graph learning, Graph convolutional network, Image fusion, Hyperspectral image},
abstract = {Fusion of high spatial resolution multispectral (HR MS) and low spatial resolution hyperspectral (LR HS) images has become a significant way to produce high spatial resolution hyperspectral (HR HS) images. Though many methods have exploited the spatial nonlocal similarity (SNS) and spectral band correlation (SBC) in the HR HS image, it is difficult to model the priors explicitly because the HR HS image is unavailable in real scenes. As the low-dimensional degradation versions, HR MS and LR HS images inherit the SNS and SBC in the HR HS image, respectively. But these methods seldom consider the inheritance of SNS and SBC between the two source images and the HR HS image. In this paper, we propose a spatial–spectral dual adaptive graph embedding model (SDAGE) to exploit the SNS and SBC in HR MS and LR HS images for the regularization of their fusion. Specifically, spatial and spectral graphs are constructed adaptively to describe the SNS in the HR MS image and the SBC in the LR HS image. Then, the two graphs are embedded into the features for the reconstruction of the HR HS image. In this way, it is explicit to ensure the consistency of SNS and SBC between the source images and the HR HS image. Experiments on three benchmark datasets demonstrate the effectiveness of our SDAGE method. The code can be downloaded from https://github.com/RSMagneto/SDAGE.}
}
@article{LI2024110325,
title = {Certainty weighted voting-based noise correction for crowdsourcing},
journal = {Pattern Recognition},
volume = {150},
pages = {110325},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110325},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000761},
author = {Huiru Li and Liangxiao Jiang and Chaoqun Li},
keywords = {Crowdsourcing, Noise correction, Certainty, Class-dependent, Instance-dependent, Weighted voting},
abstract = {In crowdsourcing scenarios, we can obtain each instance’s multiple noisy label set from different workers and then use a ground truth inference algorithm to infer its integrated label. Despite the effectiveness of ground truth inference algorithms, there is still a certain level of noise in integrated labels. To reduce the impact of noise, many noise correction algorithms have been proposed in recent years. To the best of our knowledge, almost all these algorithms assume that workers have the same labeling certainty on different classes and instances. However, it is rarely true in reality due to the differences in workers’ individual preferences and cognitive abilities. In this paper, we argue that the labeling certainty of a worker should be class-dependent and instance-dependent. Based on this premise, we propose a certainty weighted voting-based noise correction (CWVNC) algorithm. At first, we use the consistency between worker-labeled labels and integrated labels on different classes to estimate the class-dependent certainty. Then, we train a probability-based classifier on the instances labeled by each worker separately and use it to estimate the instance-dependent certainty. Finally, we correct the integrated label of each instance by weighted voting based on class-dependent certainty and instance-dependent certainty. When the proposed algorithm CWVNC is examined, the average noise ratio of CWVNC on 34 simulated datasets is equal to 15.08%, and on two real-world datasets “Income” and “Music_genre” the noise ratio is equal to 25.77% and 26.94%, respectively. The results show that CWVNC significantly outperforms all other state-of-the-art noise correction algorithms used for comparison.}
}
@article{LI2024110357,
title = {Efficient image analysis with triple attention vision transformer},
journal = {Pattern Recognition},
volume = {150},
pages = {110357},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110357},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001080},
author = {Gehui Li and Tongtong Zhao},
keywords = {Categorization, Detection, Vision transformer, Colorization},
abstract = {This paper introduces TrpViT, a novel triple attention vision transformer that efficiently captures both local and global features. The proposed architecture tackles global information acquisition by employing three complementary attention mechanisms in a unique attention block: Window, Dilated, and Channel attention. This attention block extracts spatially local features while expanding the receptive field to capture richer global context. By integrating this attention block with convolution, a new C-C-T-T architecture is formed. We rigorously evaluate TrpViT, demonstrating state-of-the-art performance on various computer vision tasks, including image classification, 2D and 3D object detection, instance segmentation, and low-level image colorization. Notably, TrpViT achieves strong accuracy across all parameter scales, highlighting its computational efficiency and effectiveness.}
}
@article{2025111237,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {159},
pages = {111237},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(24)00988-9},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009889}
}
@article{HU2024110399,
title = {Prompting large language model with context and pre-answer for knowledge-based VQA},
journal = {Pattern Recognition},
volume = {151},
pages = {110399},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110399},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400150X},
author = {Zhongjian Hu and Peng Yang and Yuanshuang Jiang and Zijian Bai},
keywords = {Visual question answering, Large language model, Knowledge-based VQA, Fine-tuning, In-context learning},
abstract = {Existing studies apply Large Language Model (LLM) to knowledge-based Visual Question Answering (VQA) with encouraging results. Due to the insufficient input information, the previous methods still have shortcomings in constructing the prompt for LLM, and cannot fully activate the capacity of LLM. In addition, previous works adopt GPT-3 for inference, which has expensive costs. In this paper, we propose PCPA: a framework that Prompts LLM with Context and Pre-Answer for VQA. Specifically, we adopt a vanilla VQA model to generate in-context examples and candidate answers, and add a pre-answer selection layer to generate pre-answers. We integrate in-context examples and pre-answers into the prompt to inspire the LLM. In addition, we choose LLaMA instead of GPT-3, which is an open and free model. We build a small dataset to fine-tune the LLM. Compared to existing baselines, the PCPA improves accuracy by more than 2.1 and 1.5 on OK-VQA and A-OKVQA, respectively.}
}
@article{REN2024110331,
title = {Brain-driven facial image reconstruction via StyleGAN inversion with improved identity consistency},
journal = {Pattern Recognition},
volume = {150},
pages = {110331},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110331},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000827},
author = {Ziqi Ren and Jie Li and Lukun Wu and Xuetong Xue and Xin Li and Fan Yang and Zhicheng Jiao and Xinbo Gao},
keywords = {Brain-driven image reconstruction, Cross-modal, Neural decoding, StyleGAN inversion},
abstract = {The reconstruction of visual stimuli from fMRI data represents a major technological and scientific challenge at the forefront of contemporary neuroscience research. Deep learning techniques have played a critical role in advancing decoding models for visual stimulus reconstruction from fMRI data. Particularly, the use of advanced GANs has resulted in significant improvements in the quality of image generation, providing a powerful tool for addressing the challenges of this complex task. However, none of these studies have taken into account the inherent characteristics of the stimulus contents themselves; This, in turn, leads to unsatisfactory outcomes, as demonstrated by the inconsistent identity between reconstructed faces and ground truth in the decoding of facial images. In order to tackle this challenge, we introduce a new framework aimed at enhancing the accuracy of reconstructing facial images from fMRI data. Our key innovation involves extracting and disentangling multi-level visual information from brain signals in the latent space and optimizing high-level features for facial identity control using identity loss. Specifically, our framework uses StyleGAN inversion to extract hierarchical latent codes from images, which are then bridged to fMRI data through transformation blocks. Additionally, we introduce a multi-stage refinement method to enhance the accuracy of reconstructed faces, which involves progressively updating fMRI latent codes with custom loss functions designed for both feature- and image-wise optimization. Our experimental results demonstrate that our proposed framework effectively achieves two critical objectives: (1) accurate facial image reconstruction from fMRI data and (2) preservation of identity characteristics with a high level of consistency.}
}
@article{GAO2024110320,
title = {Improving generalized zero-shot learning via cluster-based semantic disentangling representation},
journal = {Pattern Recognition},
volume = {150},
pages = {110320},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110320},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000712},
author = {Yi Gao and Wentao Feng and Rong Xiao and Lihuo He and Zhenan He and Jiancheng Lv and Chenwei Tang},
keywords = {Generalized zero-shot learning, Domain shift, Semantic gap, Cluster, Semantic disentangling representation},
abstract = {Generalized Zero-Shot Learning (GZSL) aims to recognize both seen and unseen classes by training only the seen classes, in which the instances of unseen classes tend to be biased towards the seen class. In this paper, we propose a Cluster-based Semantic Disentangling Representation (CSDR) method to improve GZSL by alleviating the problems of domain shift and semantic gap. First, we cluster the seen data into multiple clusters, where the samples in each cluster belong to several original seen categories, so as to facilitate fine-grained semantic disentangling of visual feature vectors. Then, we introduce representation random swapping and contrastive learning based on the clustering results to realize the disentangling semantic representations of semantic-unspecific, class-shared, and class-unique. The fine-grained semantic disentangling representations show high intra-class similarity and inter-class discriminability, which improve the performance of GZSL by alleviating the problem of domain shift. Finally, we construct the visual-semantic embedding space by the variational auto-encoder and alignment module, which can bridge the semantic gap by generating strongly discriminative unseen class samples. Extensive experimental results on four public data sets prove that our method significantly outperforms state-of-the-art methods in generalized and conventional settings.}
}
@article{XIE2024110389,
title = {Parameter-free ensemble clustering with dynamic weighting mechanism},
journal = {Pattern Recognition},
volume = {151},
pages = {110389},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110389},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001407},
author = {Fangyuan Xie and Feiping Nie and Weizhong Yu and Xuelong Li},
keywords = {Parameter-free, Weighted ensemble clustering, Dynamic weighting, Self-weighted clustering},
abstract = {Ensemble clustering (EC) gains more and more attention because it can improve the effectiveness and robustness of single clustering methods. A popular ensemble approach is to construct a co-association matrix which represents the possibility that the sample pair is divided into different clusters by base clusterings. Then, some single clustering methods could be performed on it. However, this approach separates the construction of the co-association matrix from the generation of clustering results. Besides, the importance of base clusterings and clusters are often regarded as the same but their performance or quality is different actually. Although some weighted ensemble clustering methods have been proposed, typically, the weights are calculated based on certain criteria and then fixed. To cope with these issues, we propose a Parameter-Free Ensemble Clustering (PFEC) with dynamic weighting mechanism, which is capable of dynamically adjusting the weights of base clusterings and clusters. Firstly, the self-weighted framework is applied in our method to weight base clusterings automatically. Furthermore, an additional weight is assigned to each cluster in the base clustering, which can also be dynamically adjusted. This can help alleviate the issue of imbalanced clusters as well. Finally, a structured affinity graph is learned from the results of base clusterings through rank constraint and no post-processing is required. The experimental results on synthetic and real datasets illustrate the effectiveness of our proposed method.}
}
@article{GAO2024110396,
title = {Spectral clustering with linear embedding: A discrete clustering method for large-scale data},
journal = {Pattern Recognition},
volume = {151},
pages = {110396},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110396},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400147X},
author = {Chenhui Gao and Wenzhi Chen and Feiping Nie and Weizhong Yu and Zonghui Wang},
keywords = {Spectral clustering, Graph embedding, Unsupervised learning},
abstract = {In recent decades, spectral clustering has found widespread applications in various real-world scenarios, showcasing its effectiveness. Traditional spectral clustering typically follows a two-step procedure to address the optimization problem. However, this approach may result in substantial information loss and performance decline. Furthermore, the eigenvalue decomposition, a key step in spectral clustering, entails cubic computational complexity. This paper incorporates linear embedding into the objective function of spectral clustering and proposes a direct method to solve the indicator matrix. Moreover, our method achieves a linear time complexity with respect to the input data size. Our method, referred to as Spectral Clustering with Linear Embedding (SCLE), achieves a direct and efficient solution and naturally handles out-of-sample data. SCLE initiates the process with balanced and hierarchical K-means, effectively partitioning the input data into balanced clusters. After generating anchors, we compute a similarity matrix based on the distances between the input data points and the generated anchors. In contrast to the conventional two-step spectral clustering approach, we directly solve the cluster indicator matrix at a linear time complexity. Extensive experiments across multiple datasets underscore the efficiency and effectiveness of our proposed SCLE method.}
}
@article{NIU2024110304,
title = {Bidirectional feature learning network for RGB-D salient object detection},
journal = {Pattern Recognition},
volume = {150},
pages = {110304},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110304},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000554},
author = {Ye Niu and Sanping Zhou and Yonghao Dong and Le Wang and Jinjun Wang and Nanning Zheng},
keywords = {RGB-D salient object detection, Bidirectional feature fusion, Dual consistency loss},
abstract = {RGB-D salient object detection aims to perform the pixel-wise localization of salient objects from both RGB and depth images, whose challenge mainly comes from how to learn complementary features from each modality. Existing works often use increasingly large models for performance enhancement, which need large memory and time consumption in practice. In this paper, we propose a simple yet effective Bidirectional Feature Learning Network (BFLNet) for RGB-D salient object detection under limited memory and time conditions. To achieve accurate performance with lightweight backbone networks, an effective Bidirectional Feature Fusion (BFF) module is designed to merge features from both RGB and depth streams, in which the cross-modal fusions and cross-scale fusions are jointly conducted to fuse the immediate features in multiple scales and multiple modals. What is more, a simple Dual Consistency Loss (DCL) function is designed to prompt cross-modal fusion by keeping the consistency between cross-modal target predictions. Extensive experiments on four benchmark datasets demonstrate that our method has achieved the state-of-the-art performance with high efficiency in RGB-D salient object detection. Code will be available at https://github.com/nightsky-nostar/BFLNet.}
}
@article{DENG2024110425,
title = {r-FACE: Reference guided face component editing},
journal = {Pattern Recognition},
volume = {152},
pages = {110425},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110425},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001766},
author = {Qiyao Deng and Jie Cao and Yunfan Liu and Qi Li and Zhenan Sun},
keywords = {Face portrait editing, Face completion, Reference-guided},
abstract = {Although recent studies have made significant processes in face portrait editing, simple and accurate face component editing remains a challenge. Face components, such as eyes, nose, and mouth, have a shape style that is difficult to transfer. Existing methods either (1) manipulate pre-defined binary attribute labels, which is difficult to edit the shape of face components with observable changes, or (2) control the shape change by manually editing the intermediate representation (e.g., precise masks or sketches) of face components, which is time-consuming and requires painting skills. To break these limitations, we propose a simple and effective framework for diverse and controllable face component editing with geometric changes, which utilizes an inpainting model to learn the shape of face components from reference images without any manual annotations. In order to guide generated images to learn the shape style of reference face components, an example-guided attention module is designed to help the network focus on target face component regions. Moreover, a novel domain verification discriminator is introduced to pull the realism of the generated facial component close to the source face. Experimental results demonstrate that the proposed method outperforms conventional methods in image quality, editing accuracy, and diversity of results (see Video Demo).}
}
@article{ZHU2024110328,
title = {Separate first, then segment: An integrity segmentation network for salient object detection},
journal = {Pattern Recognition},
volume = {150},
pages = {110328},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110328},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000797},
author = {Ge Zhu and Jinbao Li and Yahong Guo},
keywords = {Deep learning, Computer vision, Salient object detection, Integrity segmentation network},
abstract = {Current methods aggregate multi-level features or introduce auxiliary information to get more refined saliency maps. However, little attention is paid to how to obtain complete salient objects in cluttered background. To address this problem, we propose an integrity segmentation network (ISNet) with a novel detection paradigm that first separates the targets completely and then segment them finely. Specifically, the ISNet architecture consists of a target separation (TS) branch and an object segmentation (OS) branch, trained using a hierarchical difference-aware (HDA) loss. The TS branch equipped with a fractal structure is utilized to produce saliency features with expanded boundary (SF w/ EB), which can enlarge the difference of border details to separate the target from background completely. Compared with the edge and skeleton information, the SF w/ EB contains a more complete structure, which can supplement the defect of salient objects. The OS branch is leveraged to generate complementary features, which gradually integrates the SF w/ EB and aggregated features to segment complete saliency maps. Moreover, we propose the HDA loss to further improve the structural integrity of prediction, which hierarchically assigns weight to pixels according to their differences. Hard pixels will be given more attention to discriminate the similar parts between foreground and background. Comprehensive experimental results on five datasets show that the proposed ISNet outperforms the state-of-the-art methods both quantitatively and qualitatively. Concretely, compared with three typical models, the average gain percentage reaches 2.6% in terms of Fβ, Sm and MAE on two large complex datasets. The improvements demonstrate that the proposed ISNet are beneficial for improving the integrity of prediction. Besides, the ISNet is efficient and runs at a real-time speed of 39.5 FPS when processing an image with size of 320 × 320. Furthermore, the proposed model has better generalization, which can also be applied to other vision tasks to handle complex scenes. Codes are available at https://github.com/lesonly/ISNet.}
}
@article{YU2024110373,
title = {An approach for handwritten Chinese text recognition unifying character segmentation and recognition},
journal = {Pattern Recognition},
volume = {151},
pages = {110373},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110373},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001249},
author = {Ming-Ming Yu and Heng Zhang and Fei Yin and Cheng-Lin Liu},
keywords = {Handwritten Chinese text recognition, Connectionist temporal classification, Character segmentation, Weakly supervised learning},
abstract = {Text line recognition methods can be categorized into explicit segmentation based and implicit segmentation based ones. Explicit segmentation based methods require character-level annotation during training, while implicit segmentation based methods, trained on line-level annotated data, face alignment drift challenges. Though some methods have been proposed to address these challenges using weakly supervised object detection, they often rely on cumbersome pseudo-box generation processes and complex decoding. In this paper, we propose a unified framework to overcome these challenges, achieving high accuracy in text recognition and character segmentation. To eliminate the need of character-level annotated real text line data in training, we introduce a novel training paradigm that utilizes character-level annotated synthetic data and line-level annotated real data jointly. For synthetic data, candidate characters are explicitly aligned with labeled characters to generate hard labels for supervising model training. For real data, implicit alignment is produced by Connectionist Temporal Classification (CTC) mapping to provide soft labels for weakly-supervised model training. And for inference, we propose two decoding strategies leveraging the advantages of Non-Maximum Suppression (NMS) and CTC decoding. Extensive experiments on benchmark datasets demonstrate the superior performance of our method in text recognition and character localization, even with minimal amounts of character-level annotated line data.}
}
@article{DU2024110316,
title = {Kinematics-aware spatial-temporal feature transform for 3D human pose estimation},
journal = {Pattern Recognition},
volume = {150},
pages = {110316},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110316},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000670},
author = {Songlin Du and Zhiwei Yuan and Takeshi Ikenaga},
keywords = {3D human pose estimation, Hybrid-kinematical feature encoder, Spatial–temporal feature transform, Kinematics awareness},
abstract = {3D human pose estimation plays an important role in various human-machine interactive applications, but how to effectively extract and represent the kinematical features of human body structure in video has always been a challenge. This paper presents some inspiring observations on the human body properties that hold heuristic patterns of human poses: 1) There is distinct temporal coherence in any kind of human pose; 2) there exist evident spatial and temporal correlations among local joints even though the human is doing complex actions. According to the observed patterns, a locally structured feature encoder and a spatial–temporal feature transform are proposed for kinematics-aware feature extraction and enhancement. Unlike existing works directly projecting every bone joint to pose features without distinction, the proposed locally-structured feature encoder maps the local connection property of human body structure to kinematical features which are neural embeddings extracted from both local and global groups of human bone joints. Since the local and global bone-joint groups are pre-defined according to human body kinematics, the kinematical features are able to represent body kinematics. The kinematical features are then transformed by the proposed spatial–temporal feature transform to enhance the spatial and temporal correlations among human bone joints. The overall framework well promotes the representation of human body kinematics for 3D pose estimation. Extensive experimental results on commonly used datasets show that the mean per joint position error (MPJPE) is significantly reduced when compared with state-of-the-art methods under the same experimental condition. The improvement is expected to promote machines to better understand human poses for building superior human-centered automation systems.}
}
@article{HAN2024110361,
title = {Enhancing identification for person search with multi-scale multi-grained representation learning},
journal = {Pattern Recognition},
volume = {150},
pages = {110361},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110361},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001122},
author = {Zhixiong Han and Bingpeng Ma},
keywords = {Person search, Transformer, Multi-scale, Multi-granularity},
abstract = {Person Search aims to simultaneously address Person Detection and Person Re-ID. There are various challenges in person search such as significant scale variations, occlusions, and partial instances. In this paper, we propose a Multi-Scale Multi-Grained (MSMG) sequential network for end-to-end person search, intended to alleviate these issues. To generate re-id representations robust to scale changes, MSMG leverages multi-scale RoI features and aggregates them with a proposed Multi-Scale feature Aggregation Encoder (MSAE). In this way, the aggregated multi-scale re-id features are enriched with more semantic information and detailed information, thereby being more discriminative for identification. Moreover, to produce re-id representations more robust to occlusions and partial instances, MSMG introduces a Multi-Grained feature Learning Decoder (MGLD) focused on multi-grained feature learning. MGLD adaptively decodes multi-grained re-id representations with more accurate semantic information through a regional deformable cross-attention module. Finally, the multi-scale multi-grained re-id representation substantially improves the identification accuracy under challenging cases. Through comprehensive experiments, we demonstrate that our method achieves state-of-the-art performance on two benchmark datasets. On the challenging PRW benchmark, MSMG obtains the best-reported results with a mean average precision (mAP) score of 61.3%.}
}
@article{HAN2024110385,
title = {BALQUE: Batch active learning by querying unstable examples with calibrated confidence},
journal = {Pattern Recognition},
volume = {151},
pages = {110385},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110385},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001365},
author = {Yincheng Han and Dajiang Liu and Jiaxing Shang and Linjiang Zheng and Jiang Zhong and Weiwei Cao and Hong Sun and Wu Xie},
keywords = {Active learning, Confidence calibration, Deep neural networks, Machine learning, Image classification},
abstract = {Active learning alleviates labeling costs by selecting and labeling the most informative examples from an unlabeled pool. However, most existing active learning approaches estimate informativeness with uncalibrated confidence, resulting in unreliable informativeness estimation. These approaches generally ignored two significant issues caused by uncalibrated confidence methods. Firstly, the average uncalibrated confidence generated by modern neural networks is usually higher than the accuracy. Secondly, examples located near the decision boundaries are unstable during prediction when the target model updates parameters in the last several epochs, even throughout the training process. This phenomenon, caused by the forgetting characteristic of neural networks, has a significant impact on some specific models that estimate the informativeness by predicted probability vectors or pseudo labels. To address these issues, in this paper, we propose a novel active learning approach to reliably estimate informativeness with calibrated confidence. Specifically, we integrate the intermediate predictions for each unlabeled example, generated by the target model during the training process, to generate calibrated confidence. The calibrated confidence can capture a tendentious label from an indecisive subset of the class space. We show that the calibrated confidence with tendentiousness can maintain the ability of correct predictions. The empirical results demonstrate that our approach outperforms the state-of-the-art active learning methods on image classification tasks.}
}
@article{WANG2024110397,
title = {Dual-path dehazing network with spatial-frequency feature fusion},
journal = {Pattern Recognition},
volume = {151},
pages = {110397},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110397},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001481},
author = {Li Wang and Hang Dong and Ruyu Li and Chao Zhu and Huibin Tao and Yu Guo and Fei Wang},
keywords = {Image dehazing, Deep learning, Frequency, Convolutional neural network},
abstract = {With rapid improvement of deep learning, significant progress has been made in image dehazing, leading to favorable outcomes in many methods. However, a common challenge arises as most of these methods struggle to restore intricate details with vibrant colors in complex haze. In response to this challenge, we present a novel dual-path dehazing network with spatial-frequency feature fusion (DDN-SFF) to remove heterogeneous haze. The proposed dual-path network consists of a spatial-domain vanilla path and a frequency-domain frequency-guided path, effectively harnessing spatial-frequency knowledge. To maximize the versatility of the learned features, we introduce a relaxation dense feature fusion (RDFF) module in the vanilla path. This module can skillfully re-exploit features from non-adjacent levels and concurrently generate new features. In the frequency-guided path, we integrate the discrete wavelet transform (DWT) and introduce a frequency attention (FA) mechanism for the flexible handling of specific channels. More precisely, we deploy a channel attention (CA) and a dense feature fusion (DFF) module for low-frequency channels, whereas a pixel attention (PA) and a residual dense block (RDB) module are implemented for high-frequency channels. In summary, the deep dual-path network fuses sub-bands with specific spatial-frequency features, effectively eliminating the haze and restoring intricate details along with rich textures. Extensive experimental results demonstrate the superior performance of the proposed DDN-SFF over state-of-the-art dehazing algorithms.}
}
@article{WANG2024110341,
title = {A patch distribution-based active learning method for multiple instance Alzheimer's disease diagnosis},
journal = {Pattern Recognition},
volume = {150},
pages = {110341},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110341},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400092X},
author = {Tianxiang Wang and Qun Dai},
keywords = {Multi-instance learning, Active learning, Alzheimer's disease, Attention mechanism},
abstract = {Medical data, particularly the complex brain imaging structures, acquisition presents significant difficulties and high diagnostic expenses, resulting in a scarcity of the trainable samples in the real-world scenarios. To overcome this limitation, we present an active learning-based sampling strategy that selects the most informative samples from the unlabeled candidate sample pool for expert annotation, leading to high classification performance with a reduced number of training samples. This study adopts a patch-level perspective and introduces a multi-instance learning framework for Alzheimer's Disease diagnosis. Initially, a patch pre-selection module is designed to identify pathology-prone regions while excluding background areas and irrelevant information. Subsequently, an inner-patch local attention mechanism block and an outer-patch global attention mechanism block are developed to enhance the extraction of discriminative local and global information by the network model. Finally, an active learning sampling strategy is devised to minimize the costs associated with data acquisition and expert annotation in medical domain. The effectiveness of the proposed network framework and active learning strategy was validated through four sets of control experiments on the ADNI dataset.}
}
@article{YI2024110330,
title = {GPONet: A two-stream gated progressive optimization network for salient object detection},
journal = {Pattern Recognition},
volume = {150},
pages = {110330},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110330},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000815},
author = {Yugen Yi and Ningyi Zhang and Wei Zhou and Yanjiao Shi and Gengsheng Xie and Jianzhong Wang},
keywords = {Salient object detection, Two-stream network, Multi-level feature fusion, Gating mechanism, Edge detection},
abstract = {The salient object detection task is to locate and detect salient regions in images, which is widely applied in various fields. In this paper, we propose a gated progressive optimization network (GPONet) for salient object detection. Firstly, to extract salient regions more accurately, we design a multi-level feature fusion module with a gating mechanism named gate fusion network (GFN). GFN focuses on the semantic information of high-level features as well as the detailed information of low-level features, enabling purposeful delivery of high-level features to low-level features. The gate fusion unit (GFU) is also able to maintain valid information and suppress redundant information in the fusion process. Secondly, while some existing methods have shown that the additional edge supervision can facilitate salient object detection, edge pixels are often much less common than non-edge pixels, leading to the challenge of class imbalance. To overcome this issue, we introduce detail labels that provide additional internal details as a supplementary supervisory signal. Combining these labels with proposed detail perception loss (DPL) enables our network to learn edge information of salient objects more effectively. To complement each other and guide information exchange between the two branches, we propose a cross guide module (CGM) to control the information flow transfer between them. Finally, we develop a simple and efficient attention fusion strategy to merge the prediction maps of the two branches to generate the final salient prediction map. Extensive experimental results validate that our method reaches optimal or comparable performance on several mainstream datasets. The code of GPONet is available from https://github.com/antonie-z/GPONet.}
}
@article{WANG2024110338,
title = {Discovering causally invariant features for out-of-distribution generalization},
journal = {Pattern Recognition},
volume = {150},
pages = {110338},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110338},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400089X},
author = {Yujie Wang and Kui Yu and Guodu Xiang and Fuyuan Cao and Jiye Liang},
keywords = {Out-of-distribution generalization, Local causal structure learning, Causal effect estimation},
abstract = {Out-of-distribution (OOD) generalization aims to generalize a model trained on source domains to unseen target domains. Recently, causality-based generalization methods have focused on learning invariant causal relationships around the label variable, as causal mechanisms are robust across different domains. However, these methods would yield an inaccurate causal variable set due to the lack of heterogeneous domain data or a prior causal structure, which severely weakens their generalization capacity. To address this problem, we propose a Causally Invariant Features Discovery (CIFD) framework, which combines causal structure discovery and causal effect estimation for selecting a high-quality causal variable set and realizing better OOD generalization. Specifically, CIFD first identifies all potential causal variables by learning a double-layer-based local causal structure around the label variable. Secondly, CIFD uses a double-layer causal effect estimator for estimating the causality of potential causal variables and obtaining true causal variables. The comprehensive experiments on both regression and classification tasks clearly demonstrate the superiority of our framework over the state-of-art methods.}
}
@article{YANG2024110319,
title = {Multi-task meta label correction for time series prediction},
journal = {Pattern Recognition},
volume = {150},
pages = {110319},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110319},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000700},
author = {Luxuan Yang and Ting Gao and Wei Wei and Min Dai and Cheng Fang and Jinqiao Duan},
keywords = {Data visualization, Bi-level optimization, Meta-learning, Multi-task learning},
abstract = {Time series classification faces two unavoidable problems. One is partial feature information and the other is poor label quality, which may affect model performance. To address the above issues, we create a label correction method to time series data with meta-learning under a multi-task framework. There are three main contributions. First, we train the label correction model with a two-branch neural network in the outer loop. While in the model-agnostic inner loop, we use pre-existing classification models in a multi-task way and jointly update the meta-knowledge so as to help us achieve adaptive labeling on complex time series. Second, we devise new data visualization methods for both image patterns of the historical data and data in the prediction horizon. Finally, we test our method with various financial datasets, including XOM, S&P500, and SZ50. Results show that our method is more effective and accurate than some existing label correction techniques.}
}
@article{LI2024110412,
title = {ISP-IRLNet: Joint optimization of interpretable sampler and implicit regularization learning network for accerlerated MRI},
journal = {Pattern Recognition},
volume = {151},
pages = {110412},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110412},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001638},
author = {Xing Li and Yan Yang and Hairong Zheng and Zongben Xu},
keywords = {Compressed sensing magnetic resonance imaging, Deep learning, Interpretable sampler learning, Implicit regularization learning},
abstract = {Compressed Sensing Magnetic Resonance Imaging (CS-MRI) was proposed to accelerate data acquisition and reconstruct MR images from under-sampled data in k-space. However, the traditional approaches design the sampling patterns separately from the reconstruction process, which often leads to suboptimal reconstruction performance. To address this issue, we propose a joint optimization model dubbed as ISP-IRLNet to yield optimal recovery including an interpretable sampler and a reconstructor using the implicit regularization learning network. In particular, we introduce a probabilistic distribution model of the sampling incorporated into the prior of k-space locations, which provides a robust CS-MRI sample pattern with an explicit expression. To estimate the approximating gradient in backward propagation for more stable training, we employ a differentiable binarization strategy. In addition, we unroll the ADMM optimization for solving the regularized reconstruction model to be a deep network (i.e., IRLNet) with a learnable implicit regularization sub-network to learn a regularizer or prior term implicitly. The experiments on our collected brain and public knee datasets demonstrate that our method provides an optimal sample pattern and achieves superior image reconstruction performance compared with existing methods, especially at high acceleration rates.}
}
@article{CAO2024110315,
title = {MHSAN: Multi-view hierarchical self-attention network for 3D shape recognition},
journal = {Pattern Recognition},
volume = {150},
pages = {110315},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110315},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000669},
author = {Jiangzhong Cao and Lianggeng Yu and Bingo Wing-Kuen Ling and Zijie Yao and Qingyun Dai},
keywords = {3D shape recognition, Self-attention, Multi-view learning, View aggregation},
abstract = {Multi-view learning has demonstrated promising performance for 3D shape recognition. However, existing multi-view methods usually focus on fusing multiple views and ignore the structural and discriminative information carried by 2D views. In this paper, we propose a multi-view hierarchical self-attention network (MHSAN) to explore the geometric and discriminative information from complex 2D views. Specifically, MHSAN consists of two self-attention networks. First, a global self-attention network is adopted to exploit the structure information by embedding position information of views. Then, the discriminative self-attention network learns discriminative information from the views with high classification scores. Through the proposed MHSAN, the geometric and discriminative information is condensed as the novel representation of 3D shapes. To validate the effectiveness of our proposed method, extensive experiments have been conducted on three 3D shape benchmarks. Experimental results demonstrate that our method is generally superior to the state-of-the-art methods in 3D shape classification and retrieval tasks.}
}
@article{SUN2024110400,
title = {Overall positive prototype for few-shot open-set recognition},
journal = {Pattern Recognition},
volume = {151},
pages = {110400},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110400},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001511},
author = {Liang-Yu Sun and Wei-Ta Chu},
keywords = {Few-shot learning, Open-set recognition, Prototype},
abstract = {Few-shot open-set recognition (FSOR) is the task of recognizing samples in known classes with a limited number of annotated instances while also detecting samples that do not belong to any known class. This is a challenging problem because the models must learn to generalize from a small number of labeled samples and distinguish them from an unlimited number of potential negative examples. In this paper, we propose a novel approach called overall positive prototype to effectively improve performance. Conceptually, negative samples would distribute throughout the feature space and are hard to be described. From the opposite viewpoint, we propose to construct an overall positive prototype that acts as a cohesive representation for positive samples that distribute in a relatively smaller neighborhood. By measuring the distance between a query sample and the overall positive prototype, we can effectively classify it as either positive or negative. We show that this simple yet innovative approach provides the state-of-the-art FSOR performance in terms of accuracy and AUROC.}
}
@article{YU2024110327,
title = {Self-supervised multi-task learning for medical image analysis},
journal = {Pattern Recognition},
volume = {150},
pages = {110327},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110327},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000785},
author = {Huihui Yu and Qun Dai},
keywords = {Medical image analysis, Self-supervised multi-task learning, Uniformity regularization, Chest X-ray image},
abstract = {Deep learning is crucial for preliminary screening and diagnostic assistance based on medical image analysis. However, limited annotated data and complex anatomical structures challenge existing models as they struggle to capture anatomical context information effectively. In response, we propose a novel self-supervised multi-task learning framework (SSMT), which integrates two key modules: a discriminative-based module and a generative-based module. These modules collaborate through multiple proxy tasks, encouraging models to learn global discriminative representations and local fine-grained representations. Additionally, we introduce an efficient uniformity regularization to further enhance the learned representations. To demonstrate the effectiveness of SSMT, we conduct extensive experiments on six public Chest X-ray image datasets. Our results highlight that SSMT not only outperforms existing state-of-the-art methods but also achieves comparable performance to the supervised model in challenging downstream tasks. The ablation study demonstrates collaboration between the key components of SSMT, showcasing its potential for advancing medical image analysis.}
}
@article{DU2024110372,
title = {Flexible image denoising model with multi-layer conditional feature modulation},
journal = {Pattern Recognition},
volume = {152},
pages = {110372},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110372},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001237},
author = {Jiazhi Du and Xin Qiao and Zifei Yan and Hongzhi Zhang and Wangmeng Zuo},
keywords = {Image denoising, Convolutional neural network, Additive white Gaussian noise, Feature modulation},
abstract = {For flexible non-blind image denoising, existing deep networks usually concatenate noisy image and noise level map as the input for handling various noise levels with a single model. However, in this kind of solution, the noise variance (i.e., noise level) is only deployed to modulate the first layer of convolution feature with channel-wise shifting, which is limited in balancing noise removal and detail preservation. In this paper, we present a novel flexible image denoising network (CFMNet) by equipping a U-Net backbone with multi-layer conditional feature modulation (CFM) modules. In comparison to channel-wise shifting only in the first layer, CFMNet can make better use of noise level information by deploying multiple layers of CFM. Moreover, each CFM module takes convolutional features from both noisy image and noise level map as input for better trade-off between noise removal and detail preservation. Experimental results show that our CFMNet is effective in exploiting noise level information for flexible non-blind denoising, and performs favorably against the existing deep image denoising methods in terms of both quantitative metrics and visual quality.}
}
@article{GUAN2024110424,
title = {Federated learning for medical image analysis: A survey},
journal = {Pattern Recognition},
volume = {151},
pages = {110424},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110424},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001754},
author = {Hao Guan and Pew-Thian Yap and Andrea Bozoki and Mingxia Liu},
keywords = {Federated learning, Machine learning, Medical image analysis, Data privacy},
abstract = {Machine learning in medical imaging often faces a fundamental dilemma, namely, the small sample size problem. Many recent studies suggest using multi-domain data pooled from different acquisition sites/centers to improve statistical power. However, medical images from different sites cannot be easily shared to build large datasets for model training due to privacy protection reasons. As a promising solution, federated learning, which enables collaborative training of machine learning models based on data from different sites without cross-site data sharing, has attracted considerable attention recently. In this paper, we conduct a comprehensive survey of the recent development of federated learning methods in medical image analysis. We have systematically gathered research papers on federated learning and its applications in medical image analysis published between 2017 and 2023. Our search and compilation were conducted using databases from IEEE Xplore, ACM Digital Library, Science Direct, Springer Link, Web of Science, Google Scholar, and PubMed. In this survey, we first introduce the background of federated learning for dealing with privacy protection and collaborative learning issues. We then present a comprehensive review of recent advances in federated learning methods for medical image analysis. Specifically, existing methods are categorized based on three critical aspects of a federated learning system, including client end, server end, and communication techniques. In each category, we summarize the existing federated learning methods according to specific research problems in medical image analysis and also provide insights into the motivations of different approaches. In addition, we provide a review of existing benchmark medical imaging datasets and software platforms for current federated learning research. We also conduct an experimental study to empirically evaluate typical federated learning methods for medical image analysis. This survey can help to better understand the current research status, challenges, and potential research opportunities in this promising research field.}
}
@article{PENG2024110360,
title = {Adapt only once: Fast unsupervised person re-identification via relevance-aware guidance},
journal = {Pattern Recognition},
volume = {150},
pages = {110360},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110360},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001110},
author = {Jinjia Peng and Jiazuo Yu and Chengjun Wang and Huibing Wang and Xianping Fu},
keywords = {Prototype-guided label learning, Label-flexible training, Fast person re-identification},
abstract = {Unsupervised domain adaptive person re-identification (UDA person reID) defines a task where labels in target domains are totally unknown while source domains are fully labeled. Assigning reliable labels quickly is a critical issue for UDA person reID that could be applied in the real-world scenarios. Recent studies focus on obtaining pseudo labels by clustering algorithms and then training the reID model with these labels. However, the main limitation of these methods is the high time complexity, which is caused by the calculation of all pair-wise similarities and multiple iterations in the clustering algorithm to obtain reliable results. When the data is very large or the feature dimensions are very high, the memory and time cost requirements of the clustering algorithm can increase rapidly. In this paper, we provide a fast unsupervised domain adaptive person reID framework (FUReID), which calculates the relevance between unlabeled samples only once to adapt to the new scenarios without any iterations in the stage of label generation. Especially, instead of pursuing accurate labels, FUReID considers constructing a lightweight paradigm to generate coarse labels and then refine these labels during the training stage. Therefore, FUReID designs a prototype-guided labeling method that only relies on calculating the relevance between the prototype vectors and the samples, and assigning coarse labels with noise. Then, to alleviate the issue of noise, FUReID designs a label-flexible training network with an adaptive selection strategy to refine those coarse labels progressively. For several widely-used person reID datasets, our method achieves 81.7%, 26.2%, and 87.7% in mAP on Market1501, MSMT17 and PersonX, respectively. Code is available at https://github.com/AILab90/FUReID.}
}
@article{HU2024110384,
title = {Uncertainty-driven active developmental learning},
journal = {Pattern Recognition},
volume = {151},
pages = {110384},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110384},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001353},
author = {Qinghua Hu and Luona Ji and Yu Wang and Shuai Zhao and Zhibin Lin},
keywords = {Object detection, Active developmental learning, Uncertainty estimation},
abstract = {Existing machine learning models can well handle common classes but struggle to detect unfamiliar or unknown classes due to environmental variations. To address this challenge, we propose a new task called active developmental learning (ADL), which empowers models to actively determine what to learn in the open world, thereby progressively enhancing the capability of detecting unfamiliar and unknown classes. Considering the uncertain essence of the task, we design an uncertainty-driven method for ADL (UADL) that measures and utilizes uncertainty to evaluate unfamiliar known classes and unknown classes separately, which consists of two stages: (1) unfamiliar detection of known classes and (2) unknown detection of novel classes. In the first stage, UADL identifies unfamiliar samples of known classes via known-class uncertainty calculated by GMMs on detectors’ heads. In the second stage, UADL identifies samples containing unknown classes via unknown-class uncertainty computed by class-specific GMMs in feature space. In both stages, uncertainty is used to select a minimal number of unlabeled samples for manual labeling, facilitating the model’s active self-development. Experiments on multiple object detection benchmark datasets demonstrate the feasibility and significant performance of UADL and show its effectiveness against the ADL task compared to other state-of-the-art approaches.}
}
@article{LYU2024110408,
title = {Rigid pairwise 3D point cloud registration: A survey},
journal = {Pattern Recognition},
volume = {151},
pages = {110408},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110408},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001596},
author = {Mengjin Lyu and Jie Yang and Zhiquan Qi and Ruijie Xu and Jiabin Liu},
keywords = {3D point cloud, Registration, Review, Deep learning},
abstract = {Over the past years, 3D point cloud registration has attracted unprecedented attention. Researchers develop various approaches to tackle the challenging task, such as optimization-based and deep learning-based methods. To systematically sort out the relevant literature and follow the state-of-the-art solutions, this paper conducts a thorough survey. We propose a novel taxonomy dubbed Intermediates Based Taxon (IBTaxon) which effectively categorizes multifarious registration approaches by the introduced intermediate variables or the leveraged intermediate modules. We further delve into each of the categories and present a comprehensive technique review with a focus on the distinct insight behind each of the methods. Besides, the relevant datasets and evaluation metrics are also combed and reorganized. We conclude our paper by discussing the possible open research problems and presenting our visions for future research in the field of 3D point cloud registration.}
}
@article{LI2024110356,
title = {Revisiting single-step adversarial training for robustness and generalization},
journal = {Pattern Recognition},
volume = {151},
pages = {110356},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110356},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001079},
author = {Zhuorong Li and Daiwei Yu and Minghui Wu and Sixian Chan and Hongchuan Yu and Zhike Han},
keywords = {Adversarial robustness, Robust overfitting, Adversarial sample generation, Adversarial training, Pattern recognition},
abstract = {Recently, single-step adversarial training has received high attention because it shows robustness and efficiency. However, a phenomenon referred to as “catastrophic overfitting” has been observed, which is prevalent in single-step defenses and may frustrate attempts to use FGSM adversarial training. To address this issue, we propose a novel method, Stable and Efficient Adversarial Training (SEAT). SEAT mitigates catastrophic overfitting by harnessing on local properties that differentiate a robust model from one prone to catastrophic overfitting. The proposed SEAT is underpinned by robust theoretical justifications, in that minimizing the SEAT loss is demonstrated to promote a smoother empirical risk, consequently enhancing robustness. Experimental results demonstrate that the proposed method successfully mitigates catastrophic overfitting, yielding superior performance amongst efficient defenses. Our single-step method can reach 51% robust accuracy for CIFAR-10 with l∞ perturbations of radius 8/255 under a strong PGD-50 attack, matching the performance of a 10-step iterative method at merely 3% computational cost.}
}
@article{WANG2024110368,
title = {Coordinating explicit and implicit knowledge for knowledge-based VQA},
journal = {Pattern Recognition},
volume = {151},
pages = {110368},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110368},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001195},
author = {Qunbo Wang and Jing Liu and Wenjun Wu},
keywords = {Pre-trained model, Knowledge-based VQA, Knowledge retrieval},
abstract = {Pre-trained models often generate plausible looking statements that are factually incorrect because of the inaccurate implicit knowledge contained in the model’s parameters. Related methods retrieve explicit knowledge from the external knowledge source to help improve the prediction performance and reliability. However, these methods often use weak training signals for the retriever, and require the model to make each prediction based on the retrieved knowledge, even when the retrieved knowledge is not reliable or the model can produce better prediction only using its implicit knowledge. Therefore, it is necessary to enable the pre-trained model to actively select more beneficial knowledge for producing better prediction. This work proposes a novel method to help the model to Coordinate Explicit and Implicit Knowledge (CEIK) for the knowledge-based visual question answering (VQA) task, which is an important direction of pre-trained models. Furthermore, a better training signal is proposed for the retriever according to whether the retrieved knowledge can correct the prediction. Experimental results demonstrate the effectiveness of our method.}
}