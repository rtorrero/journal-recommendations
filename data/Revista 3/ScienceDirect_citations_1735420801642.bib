@article{YE2024110508,
title = {SaliencyCut: Augmenting plausible anomalies for anomaly detection},
journal = {Pattern Recognition},
volume = {153},
pages = {110508},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110508},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002590},
author = {Jianan Ye and Yijie Hu and Xi Yang and Qiu-Feng Wang and Chao Huang and Kaizhu Huang},
keywords = {Anomaly detection, Data augmentation, Saliency},
abstract = {Anomaly detection under the open-set scenario is a challenging task that requires learning discriminative features to detect anomalies that were even unseen during training. As a cheap yet effective approach, data augmentation has been widely used to create pseudo anomalies for better training of such models. Recent wisdom of augmentation methods focuses on generating random pseudo instances that may lead to a mixture of augmented instances with seen anomalies, or out of the typical range of anomalies. To address this issue, we propose a novel saliency-guided data augmentation method, SaliencyCut, to produce pseudo but more common anomalies that tend to stay in the plausible range of anomalies. Furthermore, we deploy a two-head learning strategy consisting of normal and anomaly learning heads to learn the anomaly score of each sample. Theoretical analyses show that this mechanism offers a more tractable and tighter lower bound of the data log-likelihood. We then design a novel patch-wise residual module in the anomaly learning head to extract and assess anomaly features from each sample, facilitating the learning of discriminative representations of anomaly instances. Extensive experiments conducted on six real-world anomaly detection datasets demonstrate the superiority of our method to competing methods under various settings. Codes are available at: https://github.com/yjnanan/SaliencyCut.}
}
@article{ZHANG2024110456,
title = {Orthogonal subspace exploration for matrix completion},
journal = {Pattern Recognition},
volume = {153},
pages = {110456},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110456},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002073},
author = {Hongyuan Zhang and Ziheng Jiao and Xuelong Li},
keywords = {Low-rank matrix completion, Non-convex optimization, Orthogonal subspace exploration, Stiefel manifold},
abstract = {Matrix completion, aiming at restoring a low-rank matrix from observed entries, indicates the connection with the subspace clustering due to the low-rank property. However, it is expensive to incorporate subspace learning into the pervasive surrogate of matrix completion, the nuclear norm. In this paper, we design an orthogonal subspace exploration model for matrix completion, which can be easily integrated due to the succinct formulation. Then, we propose a non-convex surrogate with tractable solutions for low-rank matrix completion, so that the subspace exploration can be performed simultaneously. Compared with the existing surrogates (e.g., nuclear norm, Schatten-p norm, max norm, etc.), the proposed surrogate is differential such that the optimization is still simple even after the subspace exploration is incorporated. Although the surrogate is non-convex, a parameter-free algorithm that is proved to converge into the global optimum is developed. The optimization consists of closed-form solutions so that the orthogonal subspace exploration will not distinctly bring additional costs and the algorithm empirically converges within dozens of iterations. Experiments illustrate the efficiency and superiority of our model.}
}
@article{PARK2024110539,
title = {Robust pedestrian detection via constructing versatile pedestrian knowledge bank},
journal = {Pattern Recognition},
volume = {153},
pages = {110539},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110539},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002905},
author = {Sungjune Park and Hyunjun Kim and Yong Man Ro},
keywords = {Versatile pedestrian knowledge bank, Pedestrian detection},
abstract = {Pedestrian detection is a crucial field of computer vision research which can be adopted in various real-world applications (e.g., self-driving systems). However, despite noticeable evolution of pedestrian detection, pedestrian representations learned within a detection framework are usually limited to particular scene data in which they were trained. Therefore, in this paper, we propose a novel approach to construct versatile pedestrian knowledge bank containing representative pedestrian knowledge which can be applicable to various detection frameworks and adopted in diverse scenes. We extract generalized pedestrian knowledge from a large-scale pretrained model, and we curate them by quantizing most representative features and guiding them to be distinguishable from background scenes. Finally, we construct versatile pedestrian knowledge bank which is composed of such representations, and then we leverage it to complement and enhance pedestrian features within a pedestrian detection framework. Through comprehensive experiments, we validate the effectiveness of our method, demonstrating its versatility and outperforming state-of-the-art detection performances.}
}
@article{HASSANPOUR2024110442,
title = {E2F-Net: Eyes-to-face inpainting via StyleGAN latent space},
journal = {Pattern Recognition},
volume = {152},
pages = {110442},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110442},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001936},
author = {Ahmad Hassanpour and Fatemeh Jamalbafrani and Bian Yang and Kiran Raja and Raymond Veldhuis and Julian Fierrez},
keywords = {Eyes-to-face, Face inpainting, Face reconstruction, GAN latent space, StyleGAN},
abstract = {Face inpainting, the technique of restoring missing or damaged regions in facial images, is pivotal for applications like face recognition in occluded scenarios and image analysis with poor-quality captures. This process not only needs to produce realistic visuals but also preserve individual identity characteristics. The aim of this paper is to inpaint a face given periocular region (eyes-to-face) through a proposed new Generative Adversarial Network (GAN)-based model called Eyes-to-Face Network (E2F-Net). The proposed approach extracts identity and non-identity features from the periocular region using two dedicated encoders have been used. The extracted features are then mapped to the latent space of a pre-trained StyleGAN generator to benefit from its state-of-the-art performance and its rich, diverse and expressive latent space without any additional training. We further improve the StyleGAN's output to find the optimal code in the latent space using a new optimization for GAN inversion technique. Our E2F-Net requires a minimum training process reducing the computational complexity as a secondary benefit. Through extensive experiments, we show that our method successfully reconstructs the whole face with high quality, surpassing current techniques, despite significantly less training and supervision efforts. We have generated seven eyes-to-face datasets based on well-known public face datasets for training and verifying our proposed methods. The code and datasets are publicly available.11https://github.com/fatemejamalii/E2F-Net}
}
@article{SUN2024110500,
title = {Dual GroupGAN: An unsupervised four-competitor (2V2) approach for video anomaly detection},
journal = {Pattern Recognition},
volume = {153},
pages = {110500},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110500},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002516},
author = {Zhe Sun and Panpan Wang and Wang Zheng and Meng Zhang},
keywords = {Video anomaly detection, Dual GroupGAN, SE-U-Net, SE-VAE, weighting strategy},
abstract = {Abstracts
In response to the issues of overgeneralization in reconstruction-based methods and noise sensitivity in prediction-based methods for video anomaly detection, this paper proposes a novel unsupervised video anomaly detection approach using dual GroupGAN, refers to a four-competitor (2V2), based on channel attention mechanism. Our appraoch incorporates a channel attention mechanism into two generators, namely the SE-U-Net and SE-VAE, which respectively serve as the prediction and reconstruction networks. The SE-U-Net captures essential spatio-temporal features and automatically calibrates the channel dimension, while the SE-VAE learns global features from associated video frames. A weighting strategy is used to fuse the anomaly scores of the two networks and balance their emphasis on spatio-temporal feature representation. To wrap up, the proposed prediction network (SE-U-Net) is resistant to overgeneralization and improves quality of the reconstruction network (SE-VAE) when using the prediction frame as the input of SE-VAE. Also, the SE-VAE enhances predicted future frames from normal events, thereby increasing the robustness of the SE-U-Net. Experimental results from UCSD Ped2, CUHK Avenue, and ShanghaiTech datasets demonstrate the effectiveness of the proposed approach both qualitatively and quantitatively.}
}
@article{SONG2024110484,
title = {GANN: Graph Alignment Neural Network for semi-supervised learning},
journal = {Pattern Recognition},
volume = {154},
pages = {110484},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110484},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002358},
author = {Linxuan Song and Wenxuan Tu and Sihang Zhou and En Zhu},
keywords = {Attributes mining, High-order neighborhood exploration, Semi-supervised node classification, Graph neural networks},
abstract = {Graph neural networks (GNNs) have been widely investigated in the field of semi-supervised graph machine learning. Most methods fail to exploit adequate graph information when labeled data is limited, leading to the problem of oversmoothing. To overcome this issue, we propose the Graph Alignment Neural Network (GANN), a simple and effective graph neural architecture. A unique learning algorithm with three alignment rules is proposed to thoroughly explore hidden information for insufficient labels. Firstly, to better investigate attribute specifics, we suggest the feature alignment rule to align the inner product of both the attribute and embedding matrices. Secondly, to properly utilize the higher-order neighbor information, we propose the cluster center alignment rule, which involves aligning the inner product of the cluster center matrix with the unit matrix. Finally, to get reliable prediction results with few labels, we establish the minimum entropy alignment rule by lining up the prediction probability matrix with its sharpened result. Extensive studies on graph benchmark datasets demonstrate that GANN can achieve considerable benefits in semi-supervised node classification and outperform state-of-the-art competitors.}
}
@article{LIU2024110512,
title = {Towards robust and sparse linear discriminant analysis for image classification},
journal = {Pattern Recognition},
volume = {153},
pages = {110512},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110512},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002632},
author = {Jingjing Liu and Manlong Feng and Xianchao Xiu and Wanquan Liu},
keywords = {Image classification, Linear discriminant analysis (LDA), Sparse representation, ℓ-norm},
abstract = {Linear discriminant analysis (LDA) is a popular dimensionality reduction technique that has been widely used in pattern recognition. However, there exist a large number of redundant features and corrupted noise in real-world applications, which makes the performance of existing LDA methods degrade and thus leads to a decrease in classification accuracy. To address the above issues, we propose a novel robust and sparse LDA formulation dubbed RSLDA+. The key idea is introducing the mixed sparse regularization, i.e., ℓ0-norm plus ℓ2,0-norm, for feature representation and enforce ℓ0-norm for noise reduction. Furthermore, an optimization algorithm based on the alternating direction method of multipliers (ADMM) is developed in combination with hard thresholding operators. Extensive experiments on six common image datasets verify that the proposed RSLDA+ outperforms state-of-the-art LDA variants in classification accuracy. In addition, the ablation, robustness, convergence, stability, and sparsity are analyzed in detail. The results suggest that the proposed RSLDA+ provides an effective and robust method for image classification.}
}
@article{OLIVEIRA2024110471,
title = {Meta-learners for few-shot weakly-supervised medical image segmentation},
journal = {Pattern Recognition},
volume = {153},
pages = {110471},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110471},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400222X},
author = {Hugo Oliveira and Pedro H.T. Gama and Isabelle Bloch and Roberto Marcondes Cesar},
keywords = {Meta-learning, Weakly supervised segmentation, Few-shot learning, Medical images, Domain generalization},
abstract = {Most uses of Meta-Learning in visual recognition are very often applied to image classification, with a relative lack of work in other tasks such as segmentation and detection. We propose a new generic Meta-Learning framework for few-shot weakly supervised segmentation in medical imaging domains. The proposed approach includes a meta-training phase that uses a meta-dataset. It is deployed on an out-of-distribution few-shot target task, where a single highly generalizable model, trained via a selective supervised loss function, is used as a predictor. The model can be trained in several distinct ways, such as second-order optimization, metric learning, and late fusion. Some relevant improvements of existing methods that are part of the proposed approach are presented. We conduct a comparative analysis of meta-learners from distinct paradigms adapted to few-shot image segmentation in different sparsely annotated radiological tasks. The imaging modalities include 2D chest, mammographic, and dental X-rays, as well as 2D slices of volumetric tomography and resonance images. Our experiments consider in total 9 meta-learners, 4 backbones, and multiple target organ segmentation tasks. We explore small-data scenarios in radiology with varying weak annotation styles and densities. Our analysis shows that metric-based meta-learning approaches achieve better segmentation results in tasks with smaller domain shifts compared to the meta-training datasets, while some gradient- and fusion-based meta-learners are more generalizable to larger domain shifts. Guidelines learned from the comparative performance assessment of the analyzed methods are summarized to support those readers interested in the field.}
}
@article{CHEN2024110431,
title = {Frozen is better than learning: A new design of prototype-based classifier for semantic segmentation},
journal = {Pattern Recognition},
volume = {152},
pages = {110431},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110431},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001821},
author = {Jialei Chen and Daisuke Deguchi and Chenkai Zhang and Xu Zheng and Hiroshi Murase},
keywords = {Frozen prototype, Contrastive learning, Representation learning, Semantic segmentation},
abstract = {Semantic segmentation models comprise an encoder to extract features and a classifier for prediction. However, the learning of the classifier suffers from the ambiguity which is caused by two factors: (1) the weights of a classifier for similar categories may have positive similarities lowing the performance for similar categories, named correlation ambiguity, and (2) the classifier is prone to predict the category with a larger ℓ2 norm and vice versa, termed prior ambiguity. To comedy the issues, we propose Category-Basis Prototype (CBP), frozen and mutually orthogonalized prototypes with equalℓ2 norm. Orthogonalization prevents the prototypes from being similar to each other and the equality decouples the prediction from the ℓ2 norm. To better shape the feature space, we propose Online Centroid Contrastive Loss (OCCL) equipped with centroid and category-level losses. Experiments show that our method yields compelling results over two widely applied benchmarks indicating the effectiveness of our methods.}
}
@article{CHEN2024110496,
title = {Boosting sharpness-aware training with dynamic neighborhood},
journal = {Pattern Recognition},
volume = {153},
pages = {110496},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110496},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002474},
author = {Junhong Chen and Hong Li and C.L. Philip Chen},
keywords = {Flat minima, Generalization, Optimization, Sharpness-aware minimization},
abstract = {Learning algorithms motivated by minimizing the sharpness of loss surface is a hot research topic in improving generalization. The existing methods usually solve a constrained min–max problem to minimize sharpness and find flat minima. However, most constraints (i.e., the neighborhood of the sharpness) are inappropriate, leading to sub-optimal results. This paper theoretically explores the optimal neighborhood from the view of Probably Approximately Correct-Bayesian (PAC-Bayesian) framework. A closed form of the optimal neighborhood is provided. This neighborhood is determined by the Hessian matrix and the scales of parameters. Then a generalization bound is derived that serves as a guiding principle in the design of the sharpness minimization algorithm. The Dynamic neighborhood-based Sharpness-Aware Minimization algorithm is proposed, which can adaptively adjust the neighborhood during the training process to gain better performance. Also, the algorithm is proved can convergent at the rate O(logT/T). Experimental results demonstrate that the proposed algorithm outperforms the other methods (e.g., accuracy ＋2.86% over baseline on CIFAR-100 for VGG-16).}
}
@article{WANG2024110363,
title = {Multiple-environment Self-adaptive Network for aerial-view geo-localization},
journal = {Pattern Recognition},
volume = {152},
pages = {110363},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110363},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001146},
author = {Tingyu Wang and Zhedong Zheng and Yaoqi Sun and Chenggang Yan and Yi Yang and Tat-Seng Chua},
keywords = {Cross-view geo-localization, Deep learning, Image retrieval, Multi-source domain generalization, Multi-platform collaboration},
abstract = {Aerial-view geo-localization tends to determine an unknown position through matching the drone-view image with the geo-tagged satellite-view image. This task is mostly regarded as an image retrieval problem. The key underpinning this task is to design a series of deep neural networks to learn discriminative image descriptors. However, existing methods meet large performance drops under realistic weather, such as rain and fog, since they do not take the domain shift between the training data and multiple test environments into consideration. To minor this domain gap, we propose a Multiple-environment Self-adaptive Network (MuSe-Net) to dynamically adjust the domain shift caused by environmental changing. In particular, MuSe-Net employs a two-branch neural network containing one multiple-environment style extraction network and one self-adaptive feature extraction network. As the name implies, the multiple-environment style extraction network is to extract the environment-related style information, while the self-adaptive feature extraction network utilizes an adaptive modulation module to dynamically minimize the environment-related style gap. Extensive experiments on three widely-used benchmarks, i.e., University-1652, SUES-200, and CVUSA, demonstrate that the proposed MuSe-Net achieves a competitive result for geo-localization in multiple environments. Furthermore, we observe that the proposed method also shows great potential to the unseen extreme weather, such as mixing the fog, rain and snow.}
}
@article{CHEN2024110472,
title = {BSDP: Brain-inspired Streaming Dual-level Perturbations for Online Open World Object Detection},
journal = {Pattern Recognition},
volume = {152},
pages = {110472},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110472},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002231},
author = {Yu Chen and Liyan Ma and Liping Jing and Jian Yu},
keywords = {Online incremental learning, Open world object detection, Catastrophic forgetting, Prototype-based perturbation},
abstract = {Humans can easily distinguish the known and unknown categories and can recognize the unknown object by learning it once instead of repeating it many times without forgetting the learned object. Hence, we aim to make deep learning models simulate the way people learn. We refer to such a learning manner as OnLine Open World Object Detection(OLOWOD). Existing OWOD approaches pay more attention to the identification of unknown categories, while the incremental learning part is also very important. Besides, some neuroscience research shows that specific noises allow the brain to form new connections and neural pathways which may improve learning speed and efficiency. In this paper, we take the dual-level information of old samples as perturbations on new samples to make the model good at learning new knowledge without forgetting the old knowledge. Therefore, we propose a simple plug-and-play method, called Brain-inspired Streaming Dual-level Perturbations(BSDP), to solve the OLOWOD problem. Specifically, (1) we first calculate the prototypes of previous categories and use the distance between samples and the prototypes as the sample selecting strategy to choose old samples for replay; (2) then take the prototypes as the streaming feature-level perturbations of new samples, so as to improve the plasticity of the model through revisiting the old knowledge; (3) and also use the distribution of the features of the old category samples to generate adversarial data in the form of streams as the data-level perturbations to enhance the robustness of the model to new categories. We empirically evaluate BSDP on PASCAL VOC and MS-COCO, and the excellent results demonstrate the promising performance of our proposed method and learning manner.}
}
@article{XIE2024110455,
title = {Pairwise difference relational distillation for object re-identification},
journal = {Pattern Recognition},
volume = {152},
pages = {110455},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110455},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002061},
author = {Yi Xie and Hanxiao Wu and Yihong Lin and Jianqing Zhu and Huanqiang Zeng},
keywords = {Knowledge distillation, Object re-identification},
abstract = {Most relationship knowledge distillation methods individually optimize pairwise similarities to improve the accuracy performance of a lightweight student network. However, this optimization approach may not be optimal for object re-identification (Re-ID) which prioritizes ranking. This is because it does not guarantee consistent ranking results between a lightweight student network and a large teacher network. For that, we propose a novel method called pairwise difference relational distillation (PDRD) for object Re-ID. First, we theoretically prove that minimizing the difference relationship between pairwise similarities resulting from student and teacher networks ensures consistent ranking results between the two networks. Second, based on this theoretical foundation, we combine non-linear activation functions on pairwise similarity discrepancies to create a non-linear pairwise difference rational knowledge loss function, which enhances knowledge transfer. Extensive experiments on four public datasets demonstrate that our method achieves state-of-the-art performance. For example, on Market-1501, using ResNet18 as a lightweight student network, our method acquires a rank-1 identification rate of 93.62%.}
}
@article{SHEN2024110429,
title = {HAIC-NET: Semi-supervised OCTA vessel segmentation with self-supervised pretext task and dual consistency training},
journal = {Pattern Recognition},
volume = {151},
pages = {110429},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110429},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001808},
author = {Hailan Shen and Zheng Tang and Yajing Li and Xuanchu Duan and Zailiang Chen},
keywords = {Vessel segmentation, Semi-supervised learning, Self-supervised pretext task, Consistency regularization, Topological connectivity},
abstract = {Optical Coherence Tomography Angiography(OCTA) vessel segmentation is a challenging task. On the one hand, the complex structure of the capillary networks presents significant obstacles to achieving accurate vessel segmentation. On the other hand, current research on OCTA vessel segmentation heavily relies on high-quality manual annotations, especially in fully-supervised approaches. In contrast, pixel-level annotation of OCTA vessels is time-consuming and labor-intensive. To address these issues, we propose a semi-supervised method called HAIC-Net, which integrates self-supervised learning with a homologous augmented image classification pretext task and dual consistency training with data perturbation consistency and topological connectivity consistency. Firstly, we design a self-supervised homologous augmented image classification pretext task that directs the model’s attention to similar vascular features in homologous augmented images, thereby extracting rich vessel information from unlabeled images and reducing the dependence on manual annotations. Secondly, we introduce a dual-consistency structure with topological connectivity consistency to provide constraint from a topological perspective, which is consistent with the topological characteristics of the vascular network, to enhance the segmentation network’s sensitivity to vessel connectivity and decrease the topological errors in segmentation results. We conduct experiments on two publicly available datasets and one private dataset and validate the state-of-the-art performance of the proposed method. On the ROSE-1 dataset, our method achieves 0.9143 accuracy and 0.7658 dice coefficient, surpassing other current semi-supervised methods and approaching the performance of state-of-the-art fully supervised methods. The same result can also be observed on OCTA500 and our private dataset, demonstrating the effectiveness and superiority of our approach.}
}
@article{AKKAYA2024110510,
title = {Enhancing performance of vision transformers on small datasets through local inductive bias incorporation},
journal = {Pattern Recognition},
volume = {153},
pages = {110510},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110510},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002619},
author = {Ibrahim Batuhan Akkaya and Senthilkumar S. Kathiresan and Elahe Arani and Bahram Zonooz},
keywords = {Vision transformer, Inductive bias, Locality, Small dataset},
abstract = {Vision transformers (ViTs) achieve remarkable performance on large datasets, but tend to perform worse than convolutional neural networks (CNNs) when trained from scratch on smaller datasets, possibly due to a lack of local inductive bias in the architecture. Recent studies have therefore added locality to the architecture and demonstrated that it can help ViTs achieve performance comparable to CNNs in the small-size dataset regime. Existing methods, however, are architecture-specific or have higher computational and memory costs. Thus, we propose a module called Local InFormation Enhancer (LIFE) that extracts patch-level local information and incorporates it into the embeddings used in the self-attention block of ViTs. Our proposed module is memory and computation efficient, as well as flexible enough to process auxiliary tokens such as the classification and distillation tokens. Empirical results show that the addition of the LIFE module improves the performance of ViTs on small image classification datasets. We further demonstrate how the effect can be extended to downstream tasks, such as object detection and semantic segmentation. In addition, we introduce a new visualization method, Dense Attention Roll-Out, specifically designed for dense prediction tasks, allowing the generation of class-specific attention maps utilizing the attention maps of all tokens. The code for this project is available on Github (https://github.com/NeurAI-Lab/LIFEhttps://github.com/NeurAI-Lab/LIFE).}
}
@article{FLORINDO2024110499,
title = {ELMP-Net: The successive application of a randomized local transform for texture classification},
journal = {Pattern Recognition},
volume = {153},
pages = {110499},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110499},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002504},
author = {Joao B. Florindo and Andre R. Backes and Acacio Neckel},
keywords = {Extreme learning descriptors, Texture classification, Image descriptors, Local binary patterns},
abstract = {This work proposes a method for texture classification based on the successive application of a local transform presented here for the first time. Such transform comprises two steps: (1) We built a two-layer mapping relating each pixel with its neighborhood, with the weights in the first layer randomly assigned; (2) We use the parameters learned by such mapping to transform the original image. Finally, we extract local descriptors at different stages of the successive application of this transform to compose the texture descriptors. The performance of our method is verified in the classification of benchmark texture databases and compared with state-of-the-art approaches. We also present an application for plant species identification. The results confirm our expectation that a model that is not based on the classical learning-based approach can still be competitive in texture analysis.}
}
@article{ZHANG2024110426,
title = {Cross co-teaching for semi-supervised medical image segmentation},
journal = {Pattern Recognition},
volume = {152},
pages = {110426},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110426},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001778},
author = {Fan Zhang and Huiying Liu and Jinjiang Wang and Jun Lyu and Qing Cai and Huafeng Li and Junyu Dong and David Zhang},
keywords = {Semi-supervised medical image segmentation, Signed distance function, Cross-structure-task collaborative teaching, Adaptive boundary enhancing module, Interactive lateral fusion mechanism},
abstract = {Excellent performance has been achieved on semi-supervised medical image segmentation, but existing algorithms perform relatively poorly for objects with variable topologies and weak boundaries. In this paper, we propose a novel cross co-teaching framework, called Cross-structure-task Collaborative Teaching (CroCT), which not only can effectively handle variable topologies, but also strengthens the learning for weak boundaries of unlabeled data. Specifically, a new cross-structure-task collaborative teaching mechanism is developed based on our designed “E-Net” structure composed of a shared encoder and two decoder branches with distinct learning paradigms, which asks these two branches to regress topology-aware signed distance functions and densely-predicted segmentation masks for each other. Powered by the collaboration across different structural biases and sequence-related tasks, our CroCT can extract more discriminative yet complementary representations from abundant raw medical data to promote the consistency learning generalization, further boosting the performance for tackling highly diverse shapes and topological changes intra-/inter-slices. Besides, it guarantees the diversities from multi-levels, i.e., structure and task perspectives, to preclude prediction uncertainty. In addition, a novel adaptive boundary enhancing (ABE) module is proposed to introduce compact annularly enhanced boundary features into semi-supervised training, which significantly improves weak boundary perception ability for unlabeled data while facilitating collaborative teaching for efficiently propagating complementary knowledge across different branches. The extensive experiments on three challenging medical benchmarks, employing different labeled settings, demonstrate the superiority of our CroCT over recent state-of-the-art competitors.}
}
@article{JI2024110507,
title = {Mirrored X-Net: Joint classification and contrastive learning for weakly supervised GA segmentation in SD-OCT},
journal = {Pattern Recognition},
volume = {153},
pages = {110507},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110507},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002589},
author = {Zexuan Ji and Xiao Ma and Theodore Leng and Daniel L. Rubin and Qiang Chen},
keywords = {Age-related macular degeneration, Geographic atrophy segmentation, SD-OCT images, Weakly supervised learning, Contrastive learning, Multitask learning},
abstract = {Deep learning achieves impressive performance in medical image segmentation, but the training phase requires a large amount of annotated data with precise clinical definitions. Weakly supervised lesion segmentation aims to produce pixel-level masks by learning discriminatory information from weak annotations. In this paper, Mirrored X-Net, a novel segmentation model only supervised by image-level category labels, is proposed to segment the Geographic Atrophy (GA) regions in en-face projection of Spectral-Domain Optical Coherence Tomography (SD-OCT) data. Characterized by the dimension-asymmetric information in SD-OCT images, a novel Anisotropic Downsampling (ADS) is proposed to augment feature shapes. To extract the regions of normal retina for both images with and without lesions, we propose a contrastive learning module to assimilate the deep representation of normal tissue in the SD-OCT images and improve the class-wise difference of deep representation. Based on the contrastive learning module, Anomalous Probability Map (APM) can be obtained to draw the distribution of difference with normal tissue in images. We jointly train the image classification and contrastive learning module, and the final GA segmentation is refined based on the en-face projection of APM. The experimental results on two independent GA datasets demonstrate that the proposed weakly supervised model can produce satisfactory results, and obtain even higher accuracy than fully supervised approaches. The source code is available at https://github.com/maxiao0234/Mirrored-X-Net-pytorch.}
}
@article{LAN2024110535,
title = {Bidirectional correlation-driven inter-frame interaction Transformer for referring video object segmentation},
journal = {Pattern Recognition},
volume = {153},
pages = {110535},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110535},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002863},
author = {Meng Lan and Fu Rong and Zuchao Li and Wei Yu and Lefei Zhang},
keywords = {Referring video object segmentation, Multimodal Transformer, Bidirectional multi-level vision-language interaction, Inter-frame interaction},
abstract = {Referring video object segmentation (RVOS) aims to segment the target object in a video sequence described by a language expression. Typical multimodal Transformer based RVOS approaches process video sequence in a frame-independent manner to reduce the high computational cost, which however restricts the performance due to the lack of inter-frame interaction for temporal coherence modeling and spatio-temporal representation learning of the referred object. Besides, the absence of sufficient cross-modal interactions results in weak correlation between the visual and linguistic features, which increases the difficulty of decoding the target information and limits the performance of the model. In this paper, we propose a bidirectional correlation-driven inter-frame interaction Transformer, dubbed BIFIT, to address these issues in RVOS. Specifically, we design a lightweight and plug-and-play inter-frame interaction module in the Transformer decoder to efficiently learn the spatio-temporal features of the referred object, so as to decode the object information in the video sequence more precisely and generate more accurate segmentation results. Moreover, a bidirectional multi-level vision-language interaction module is implemented before the multimodal Transformer to enhance the correlation between the linguistic and multi-level visual features, thus facilitating the language queries to decode more precise object information from visual features and ultimately improving the segmentation performance. Extensive experimental results on four benchmarks validate the superiority of our BIFIT over state-of-the-art methods and the effectiveness of our proposed modules. The code is available in https://github.com/LANMNG/BIFIT.}
}
@article{SONG2024110483,
title = {CoReFace: Sample-guided Contrastive Regularization for Deep Face Recognition},
journal = {Pattern Recognition},
volume = {152},
pages = {110483},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110483},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002346},
author = {Youzhe Song and Feng Wang},
keywords = {Face recognition, Discriminative representations, Contrastive regularization, Contrastive learning},
abstract = {The discriminability of the feature representation is crucial for face recognition. However, previous methods rely solely on the learnable weights of the classification layer, which represent the identities. This reliance could be problematic as the evaluation process depends on the similarity between pairs of face images and requires minimal identity information learned during training. As a result, there is an inconsistency between the training and evaluation processes, which can confuse the feature encoder and hinder the effectiveness of identity-based methods. To address this problem, we propose a novel approach namely Contrastive Regularization for Face Recognition (CoReFace), which applies sample-level regularization in feature representation learning. Specifically, we employ sample-guided contrastive learning to directly regularize the training based on the sample-sample relationship and thus align it with the evaluation process. To avoid image quality degradation, we augment the embeddings instead of the images in order to integrate contrastive learning into face recognition. Additionally, we introduce a new contrastive loss function for the regularization of representation distribution. This function incorporates an adaptive margin and a supervised contrastive mask to ensure stable loss values and prevent interference with the identity supervision signals. Finally, we explore new pair-coupling protocols in order to overcome the problem of semantically repetitive signals in contrastive learning. Extensive experiments demonstrate the efficacy and efficiency of our CoReFace approach, which achieves competitive results compared to state-of-the-art methods. Code could be found https://github.com/IsidoreSong/CoreFace here.}
}
@article{DONG2024110467,
title = {WRD-Net: Water Reflection Detection using a parallel attention transformer},
journal = {Pattern Recognition},
volume = {152},
pages = {110467},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110467},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002188},
author = {Huijie Dong and Hao Qi and Huiyu Zhou and Junyu Dong and Xinghui Dong},
keywords = {Water Reflection Detection, Symmetry detection, Line detection, Multi-scale deep networks, Parallel attention transformer},
abstract = {In contrast to symmetry detection, Water Reflection Detection (WRD) is less studied. We treat this topic as a Symmetry Axis Point Prediction task which outputs a set of points by implicitly learning Gaussian heat maps and explicitly learning numerical coordinates. We first collect a new data set, namely, the Water Reflection Scene Data Set (WRSD). Then, we introduce a novel Water Reflection Detection Network, i.e., WRD-Net. This network is built on top of a series of Parallel Attention Vision Transformer blocks with the Atrous Spatial Pyramid (ASP-PAViT) that we deliberately design. Each block captures both the local and global features at multiple scales. To our knowledge, neither the WRSD nor the WRD-Net has been used for water reflection detection before. To derive the axis of symmetry, we perform Principal Component Analysis (PCA) on the points predicted. Experimental results show that the WRD-Net outperforms its counterparts and achieves the true positive rate of 0.823 compared with the human annotation.}
}
@article{HE2024110511,
title = {Memory-Adaptive Vision-and-Language Navigation},
journal = {Pattern Recognition},
volume = {153},
pages = {110511},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110511},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002620},
author = {Keji He and Ya Jing and Yan Huang and Zhihe Lu and Dong An and Liang Wang},
keywords = {Vision-and-Language Navigation, Memory bank, History noises, Memory-Adaptive Model},
abstract = {Vision-and-Language Navigation (VLN) requests an agent to navigate in 3D environments following given instructions, where history is critical for decision-making in dynamic navigation process. Particularly, a memory bank storing histories is widely used in existing methods to incorporate with multimodel representations in current scenes for better decision-making. However, by weighting each history with a simple scalar, those methods cannot purely utilize the informative cues that co-exist with detrimental contents in each history, thereby inevitably introducing noises into decision-making. To that end, we propose a novel Memory-Adaptive Model (MAM) that can dynamically restrain the detrimental contents in histories for retaining contents that benefit navigation only. Specifically, two key modules, Visual and Textual Adaptive Modules, are designed to restrain history noises based on scene-related vision and text, respectively. A Reliability Estimator Module is further introduced to refine above adaptation operations. Our experiments on the widely used RxR and R2R datasets show that MAM outperforms its baseline method by 4.0%/2.5% and 2%/1% on the validation unseen/test split, respectively, wrt the SR metric.}
}
@article{ZHOU2024110438,
title = {Adaptive multi-text union for stable text-to-image synthesis learning},
journal = {Pattern Recognition},
volume = {152},
pages = {110438},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110438},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001894},
author = {Yan Zhou and Jiechang Qian and Huaidong Zhang and Xuemiao Xu and Huajie Sun and Fanzhi Zeng and Yuexia Zhou},
keywords = {Adaptive multi-text union learning, Text-to-image synthesis, Cross-modal generation},
abstract = {Generative Adversarial Networks (GANs) have significantly boosted the performance of text-to-image generation tasks in recent years. To train the generator, losses like reconstruction loss or adversarial loss between the generated image and ground truth image are widely adopted by recent works. These losses are all built over one assumption: the given text descriptions can describe the corresponding image perfectly. Unfortunately, this assumption is not satisfied in many cases, especially in datasets like COCO with complicated scenes due to the variance of annotator experience and focal point. This paper addresses this issue by proposing a multi-text-to-image training framework, which adaptively adjusts the weights of all the text descriptions corresponding to a specific image to generate the union description features. With the union description features, the generator can generate more visual-consistent images and mitigate the negative optimization caused by incomplete or inconsistent text descriptions. To better measure the similarity between generated images and multi-text descriptions, we also reformulate the process of multi-modal matching loss to better measure the similarity between image and multi-text descriptions. Extensive experiments on relevant benchmarks CUB and COCO prove the proposed method’s effectiveness and superiority compared to state-of-the-art methods.}
}
@article{CHAN2024110454,
title = {Classification of Childhood Obstructive Sleep Apnea based on X-ray images analysis by Quasi-conformal Geometry},
journal = {Pattern Recognition},
volume = {152},
pages = {110454},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110454},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400205X},
author = {Hei-Long Chan and Hoi-Man Yuen and Chun-Ting Au and Kate Ching-Ching Chan and Albert Martin Li and Lok-Ming Lui},
keywords = {Obstructive sleep apnea, Quasi-conformal theory, Image analysis, Disease classification, Machine learning},
abstract = {Craniofacial profile is one of the anatomical causes of obstructive sleep apnea (OSA). By medical research, cephalometry provides information on patients’ skeletal structures and soft tissues. In this work, a novel approach to cephalometric analysis using quasi-conformal geometry based local deformation information was proposed for OSA classification. Our study was a retrospective analysis based on 60 case-control pairs with accessible lateral cephalometry and polysomnography (PSG) data (mean age: 8.9 ± 2.3 years). By using the quasi-conformal geometry to study the local deformation around 15 landmark points, and combining the results with three linear distances between landmark points, a total of 1218 information features were obtained per subject. A L2 norm based classification model was built. Under experiments, our proposed model achieves high testing accuracy. The accurate classification tool provides us with a reliable and accurate screening tool for childhood OSA.}
}
@article{LIU2024110495,
title = {Joint magnetic resonance imaging artifacts and noise reduction on discrete shape space of images},
journal = {Pattern Recognition},
volume = {153},
pages = {110495},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110495},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002462},
author = {Xiangyuan Liu and Zhongke Wu and Xingce Wang and Quansheng Liu and Jose M. Pozo and Alejandro F. Frangi},
keywords = {Artifact and noise, Discrete shape space of images, Riemannian manifold, PROD detector},
abstract = {Magnetic resonance (MR) images can be corrupted by artifacts and noise, potentially leading to misinterpretation of the images. In this paper, we propose a novel approach based on the discrete shape space of images (DSSI) to jointly reduce artifacts and noise in MR images. The proposed method restores MR images in multiple domains based on the distinct generation mechanisms of noise and artifacts. The images in multiple domains are analyzed in a non-Euclidean space. The DSSI is constructed as a Riemannian manifold to measure the intrinsic properties of images. Images are considered shapes from a geometric perspective, and the impact of similarity transformations (e.g., rotation, scaling, and translation) on image analysis is eliminated. The patch-based rank-ordered difference (PROD) detector is defined in k-space within the framework of DSSI to detect and remove sparse outliers that cause artifacts. In addition, a novel similarity function for images is defined using the DSSI and be used to design the improved filter. Finally, the convergence of the improved filter is theoretically analyzed, indicating that our method offers an effective estimator of the ideal image. The experimental results of various MR images demonstrate that the proposed approach outperforms classical and state-of-the-art methods for artifact correction and noise removal, both qualitatively and quantitatively.}
}
@article{CHEN2024110433,
title = {A distortion model guided adversarial surrogate for recaptured document detection},
journal = {Pattern Recognition},
volume = {151},
pages = {110433},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110433},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001845},
author = {Changsheng Chen and Xijin Li and Baoying Chen and Haodong Li},
keywords = {Presentation attack, Recapturing distortion, Adversarial training},
abstract = {Due to the ever-growing need for e-business, document presentation attack detection (DPAD) is an important forensic task. With deep learning, the performances of DPAD methods have been improved significantly. However, the cross-domain performance under different types of document images is not yet satisfactory. In this work, we propose to study the document recapturing distortion model (DM), which is employed in guiding the training of an end-to-end surrogate distortion model. This surrogate model is incorporated into the DPAD scheme in an adversarial training fashion, yielding the proposed Adversarial Surrogate-based DPAD (AS-DPAD) scheme. The surrogate model actively generates adversarial samples with recapturing distortions that confuse the DPAD model. Meanwhile, the DPAD backbone learns a latent space that considers the distances between the generated and real recaptured samples to achieve a better generalization performance. A challenging cross-domain protocol is conducted in our experiment. The results confirm that our DM improves the efficacy of the trained surrogate distortion model and also validate that the proposed adversarial training strategy leads to a significant performance gain in the evaluation of document images with different contents.}
}
@article{GUO2024110542,
title = {Dynamic heterogeneous federated learning with multi-level prototypes},
journal = {Pattern Recognition},
volume = {153},
pages = {110542},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110542},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002930},
author = {Shunxin Guo and Hongsong Wang and Xin Geng},
keywords = {Heterogeneous federated learning, Multi-level prototypes},
abstract = {Federated learning shows promise as a privacy-preserving collaborative learning technique. Existing research mainly focuses on skewing the class distribution across clients. However, most approaches suffer from catastrophic forgetting and classifier shift, mainly when the global distribution of all classes is extremely unbalanced and the data distribution of the client dynamically evolves over time. In this paper, we study the Dynamic Heterogeneous Federated Learning, which addresses the practical scenario where heterogeneous data distributions exist among different clients and dynamic tasks within the client. Accordingly, we propose a novel federated learning framework named Federated Multi-Level Prototypes and design federated multi-level regularizations. To mitigate classifier shift, we construct semantic prototypes to provide fruitful generalization knowledge. To maintain the model stability and consistency convergence, three regularizations are introduced as training losses, i.e., prototype-based regularization, semantic prototype-based regularization, and federated inter-task regularization. Extensive experiments show that the proposed method achieves advanced performance in various settings.}
}
@article{KHAN2024110490,
title = {Lit me up: A reference free adaptive low light image enhancement for in-the-wild conditions},
journal = {Pattern Recognition},
volume = {153},
pages = {110490},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110490},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002413},
author = {Rizwan Khan and Atif Mehmood and Farah Shahid and Zhonglong Zheng and Mostafa M. Ibrahim},
keywords = {Uneven lighting, Image enhancement, Adaptive learning},
abstract = {Images captured with different devices in uneven conditions (e.g., invariable lighting, low lighting, weather changes, exposure time, etc.) often lead to low image visibility and poor color and contrast, affecting the performance of computer vision and pattern recognition applications. The pre-trained convolutional neural networks (CNNs) solely rely on the training data and lack adaptation due to the uncertainty in the lighting conditions. Moreover, capturing large-scale datasets to train CNNs also raises the computational complexity and overall cost. This work integrates the knowledge and data and proposes a two-stage Uneven-to-Enliven network (U2E-Net), which rapidly learns to see in uneven conditions. A multiple-layered Uneven network learns to distinguish the reflection and illumination in the input images, and an encoder–decoder-based Enliven-Net contextualizes the illumination information. A key component in such ill-posed problems is to obtain information from priors and pairs; however, we present the compelling idea of information trade-off followed by decomposition consistency, thereby progressively improving the visual quality with the subsequent enhancement operations. To this end, we proposed a two-faceted framework that can work without depending on the data type. A novel color and contrast preservation strategy (CPS) is proposed following the decomposition of input data. CPS is integrated within the network to extract contrast in the darkest background regions.}
}
@article{LI2024110485,
title = {Self-reconstruction network for fine-grained few-shot classification},
journal = {Pattern Recognition},
volume = {153},
pages = {110485},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110485},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400236X},
author = {Xiaoxu Li and Zhen Li and Jiyang Xie and Xiaochen Yang and Jing-Hao Xue and Zhanyu Ma},
keywords = {Few-shot learning, Fine-grained image classification, Deep neural network, Self-reconstruction network},
abstract = {Metric-based methods are one of the most common methods to solve the problem of few-shot image classification. However, traditional metric-based few-shot methods suffer from overfitting and local feature misalignment. The recently proposed feature reconstruction-based approach, which reconstructs query image features from the support set features of a given class and compares the distance between the original query features and the reconstructed query features as the classification criterion, effectively solves the feature misalignment problem. However, the issue of overfitting still has not been considered. To this end, we propose a self-reconstruction metric module for diversifying query features and a restrained cross-entropy loss for avoiding over-confident predictions. By introducing them, the proposed self-reconstruction network can effectively alleviate overfitting. Extensive experiments on five benchmark fine-grained datasets demonstrate that our proposed method achieves state-of-the-art performance on both 5-way 1-shot and 5-way 5-shot classification tasks. Code is available at https://github.com/liz-lut/SRM-main.}
}
@article{DONG2024110445,
title = {MFIFusion: An infrared and visible image enhanced fusion network based on multi-level feature injection},
journal = {Pattern Recognition},
volume = {152},
pages = {110445},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110445},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001961},
author = {Aimei Dong and Long Wang and Jian Liu and Guohua Lv and Guixin Zhao and Jinyong Cheng},
keywords = {Image fusion, Multi-level feature injection, Attention mechanism},
abstract = {Infrared and visible image fusion aims to integrate complementary information from both types of images. Existing deep learning-based fusion methods rely solely on the final output of the feature extraction network, which may overlook valuable information presented in the middle layers of the network, ultimately reducing the richness of the fusion results, i.e., detailed texture information might not be fully extracted and integrated into the fused image. This study proposes a multi-level feature injection method based on an image decomposition model for infrared and visible image fusion, termed as MFIFusion. On the one hand, we introduce an attention-guided multi-level feature injection module designed to mitigate information loss during the feature extraction stage of the image scale decomposition process. More specifically, the proposed method integrates multiple fusion branches in the encoder network and employs an attention mechanism to guide the feature fusion process. On the other hand, based on the characteristics that superficial features retain image detail information and profound features are more suitable for extracting semantic information from images, we use distinct fusion strategies in these two phases to adaptively control the intensity distribution of the salient targets and to preserve the texture information in the background region. Qualitative and quantitative results demonstrate that our proposed approach produces fused images that are more visually salient to the target and contain richly detailed textures.}
}
@article{WONG2024110538,
title = {Graph correlated discriminant embedding for multi-source domain adaptation},
journal = {Pattern Recognition},
volume = {153},
pages = {110538},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110538},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002899},
author = {Wai Keung Wong and Yuwu Lu and Zhihui Lai and Xuelong Li},
keywords = {Multi-source domain adaptation, Graph embedding, Discriminative, Correlative},
abstract = {As a main branch of domain adaptation (DA), multi-source DA (MSDA) has attracted increasing attention for exploiting information from multi-source domain data. However, how to effectively explore useful information from each source domain for target tasks is still a key problem. In this paper, to fully explore multiple information of different domain data, we propose a graph correlated discriminant embedding (GCDE) method for MSDA. In GCDE, the category-discriminative information, manifold structure, and correlation learning are fully considered. Specifically, GCDE encodes the within- and between- class information of each domain data, preserves the local and global structure information of the data, and extracts the maximization correlative features from different domains by designing a novel correlative learning scheme. We also extend GCDE to a nonlinear case and obtain kernel GCDE (KGCDE). We have conducted extensive experiments on four public data benchmarks to verify the performance of GCDE and KGCDE. The promising performance on the databases prove the efficiency of our methods with the comparison of the advanced approaches.}
}
@article{LI2024110514,
title = {Feature selection by Universum embedding},
journal = {Pattern Recognition},
volume = {153},
pages = {110514},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110514},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002656},
author = {Chun-Na Li and Ling-Wei Huang and Yuan-Hai Shao and Tingting Guo and Yu Mao},
keywords = {Feature selection, Universum, Support vector machine, Universum support vector machine, Embedded support vector machine},
abstract = {Feature selection in classification is an important task in machine learning. Inspired by the success of Universum support vector machine proposed by Weston et al. on improving the classification ability of classical support vector machine, this paper considers a special type of Universum and further lets it play its role in both useful feature identification and separating hyperplane construction, aiming to improve both the feature selection ability and classification performance of Universum support vector machine. By introducing this special Universum, a redundant feature can be identified by observing whether some Universum sample is useful. In fact, we prove that by observing the dual solution of the optimization problem, useful features can be selected from a set satisfying some properties. Due to the introduction of these extra Universum samples, it needs to cope with a large-scale optimization problem. To improve the training efficiency, we modify the sequential minimal optimization algorithm and further combine it with the coordinate descent technique to solve the proposed model. Experimental results on artificial datasets, benchmark datasets, and text classification datasets demonstrate that the proposed method improves the classification performance of support vector machine and Universum support vector machine, and also has good feature selection ability.}
}
@article{YU2024110526,
title = {Uncertainty-aware hierarchical labeling for face forgery detection},
journal = {Pattern Recognition},
volume = {153},
pages = {110526},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110526},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002772},
author = {Bingyao Yu and Wanhua Li and Xiu Li and Jie Zhou and Jiwen Lu},
keywords = {Face forgery detection, Uncertainty learning, Hierarchical labeling},
abstract = {In this paper, we propose an uncertainty-aware hierarchical labeling method for face forgery detection, which aims to simultaneously explore the traces of forgery contents from a hierarchical level. Unlike existing face forgery detection methods that usually focus on local regions or entire images, our proposed approach takes advantage of hierarchical labeling as auxiliary supervised signals to capture hybrid information from pixel-level, patch-level and image-level. Moreover, we utilize Transformer architecture to extract the patch-level information to build a bridge between the pixel-level and the image-level information. However, the face forgery ground truth always carries data uncertainty due to the existence of the blending step and the compression process during face image manipulation. Thus we introduce an uncertainty learning method to formulate and leverage the data uncertainty. Extensive experimental results on five public datasets demonstrate that our approach not only achieves very competitive performance but also improves the generalization ability.}
}
@article{KIM2024110474,
title = {Depth-aware guidance with self-estimated depth representations of diffusion models},
journal = {Pattern Recognition},
volume = {153},
pages = {110474},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110474},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002255},
author = {Gyeongnyeon Kim and Wooseok Jang and Gyuseong Lee and Susung Hong and Junyoung Seo and Seungryong Kim},
keywords = {Diffusion models, Depth estimation, Diffusion guidance},
abstract = {Diffusion models have recently shown significant advancement in the generative models with their impressive fidelity and diversity. The success of these models can be often attributed to their use of sampling guidance techniques, such as classifier or classifier-free guidance, which provide effective mechanisms to trade-off between fidelity and diversity. However, these methods are not capable of guiding a generated image to be aware of its geometric configuration, e.g., depth, which hinders their application to downstream tasks such as scene understanding that require a certain level of depth awareness. To overcome this limitation, we propose a novel sampling guidance method for diffusion models that uses self-predicted depth information derived from the rich intermediate representations of diffusion models. Concretely, we first present a label-efficient depth estimation framework using internal representations of diffusion models. Subsequently, we propose the incorporation of two guidance techniques during the sampling phase. These methods involve using pseudo-labeling and depth-domain diffusion prior to self-condition the generated image using the estimated depth map. Experiments and comprehensive ablation studies demonstrate the effectiveness of our method in guiding the diffusion models toward the generation of geometrically plausible images. Our project page is available at https://ku-cvlab.github.io/DAG/.}
}
@article{XU2024110450,
title = {Interpretable medical deep framework by logits-constraint attention guiding graph-based multi-scale fusion for Alzheimer’s disease analysis},
journal = {Pattern Recognition},
volume = {152},
pages = {110450},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110450},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002012},
author = {Jinghao Xu and Chenxi Yuan and Xiaochuan Ma and Huifang Shang and Xiaoshuang Shi and Xiaofeng Zhu},
keywords = {Alzheimer’s disease, Attention, Graph neural networks, Multi-scale feature fusion, Structural MRI},
abstract = {Deep learning using structural MRI has been widely applied to early diagnosis study of Alzheimer’s disease. Among existing methods, attention-based 3D subject-level methods can not only provide diagnosis results but also interpret the significant brain regions, thereby attracting considerable attention. However, the performance of previous attention-based methods might be still restricted by: (i) the gap between attention scores and semantic significant regions; (ii) using only single-scale features or simply fusing multi-scale information by addition or concatenation for classification decision-making. To overcome these two issues, we propose an innovative dual-branch model called LA-GMF, which consists of two major modules: logits-constraint attention (LA) and graph-based multi-scale fusion (GMF). The LA module is designed to guide the model to focus on key areas to enhance the diagnostic performance of local lesions, by reducing the inconsistency between attention scores and class prediction probabilities. Meanwhile, by combining the graph neural network and the self-attention mechanism, the GMF module not only introduces the interaction between patches, but also explores the correlation and complementarity between features at different scales, thereby extracting feature representations more comprehensively. Experiments on the popular ADNI and AIBL datasets validate the potential of our model in boosting early AD diagnosis accuracy. Additionally, our interpretation experiments demonstrate the superior interpretability performance of the proposed method over recent state-of-the-art attention-based methods. Our source codes are released at: https://github.com/nollexu/LA-GMF.}
}
@article{QIAO2024110498,
title = {Information filtering and interpolating for semi-supervised graph domain adaptation},
journal = {Pattern Recognition},
volume = {153},
pages = {110498},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110498},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002498},
author = {Ziyue Qiao and Meng Xiao and Weiyu Guo and Xiao Luo and Hui Xiong},
keywords = {Transfer learning, Semi-supervised learning, Domain adaptation},
abstract = {Graph domain adaptation, which falls under the umbrella of graph transfer learning, involves transferring knowledge from a labeled source graph to improve prediction accuracy on an unlabeled target graph, where both graphs have identical label spaces but exhibit distribution discrepancies due to temporal data shifts or distinct data collection methods. This adaptation is complicated by the challenges of graph-specific domain discrepancies and cross-graph label scarcity. This paper proposes a semi-supervised Graph domain adaptation method via Information Filtering and Interpolating (GIFI). Specifically, GIFI utilizes a parameterized graph reduction module and a variational information bottleneck to adequately filter out irrelevant information from the source and target graphs to eliminate distribution discrepancy. GIFI also introduces an interpolation-enhanced pseudo-labeling strategy for cross-graph semi-supervised learning, which can mitigate model over-fitting on domain-specific features and limited labeled nodes, thus improving the model’s adaptation and discriminative capability. Experimental results on various graph domain adaptation benchmarks demonstrate GIFI’s superior performance over state-of-the-art methods. Our code is available at https://github.com/joe817/GIFI.}
}
@article{GUAN2024110458,
title = {Fast main density peak clustering within relevant regions via a robust decision graph},
journal = {Pattern Recognition},
volume = {152},
pages = {110458},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110458},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002097},
author = {Junyi Guan and Sheng Li and Jinhui Zhu and Xiongxiong He and Jiajia Chen},
keywords = {Clustering, Density peak, Decision graph, kNN},
abstract = {Although Density Peak Clustering (DPC) can easily locate cluster centers by detecting density peaks in its decision graph, its allocation strategy may unadvisedly associate irrelevant points, its decision graph may mislead the cluster center selection, and its high computational complexity O(n2) shies itself away from large-scale data. Herein, a Fast Main Density Peak Clustering Within Relevant Regions Via A Robust Decision Graph (R-MDPC) is proposed. R-MDPC assigns points within the relevant regions to avoid the association of irrelevant points. With the removal of regional differences and the attenuation of satellite peaks, a robust decision graph is obtained. Moreover, based on the kNN distance of data points, R-MDPC is believed to be suitable for large-scale data. Experimental results demonstrated the high robustness of R-MDPC’s decision graph in identifying cluster centers, and its outstanding performance and fast running speed in recognizing complex-shaped clusters.}
}
@article{YOUNIS2024110486,
title = {MTS2Graph: Interpretable multivariate time series classification with temporal evolving graphs},
journal = {Pattern Recognition},
volume = {152},
pages = {110486},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110486},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002371},
author = {Raneen Younis and Abdul Hakmeh and Zahra Ahmadi},
keywords = {Multivariate time series, Interpretability, Neural networks, Classification},
abstract = {Conventional time series classification approaches based on bags of patterns or shapelets face significant challenges in dealing with a vast amount of feature candidates from high-dimensional multivariate data. In contrast, deep neural networks can learn low-dimensional features efficiently, and in particular, convolutional neural networks have shown promising results in classifying multivariate time series data. A key factor in the success of deep neural networks is this astonishing expressive power. However, this power comes at the cost of complex, black-boxed models, conflicting with the goals of building reliable and human-understandable models. In this work11The code can be found at https://github.com/raneeny/MTS2Graph., we introduce a new interpretable framework for multivariate time series data that by extracting and clustering the input quantifies the contribution of time-varying input variables and each signal’s role to the classification. We construct a graph that captures the temporal relationship between the extracted patterns for each layer and propose an effective merging strategy to aggregate those graphs into one. Finally, a graph embedding algorithm generates new representations of the created interpretable time-series features. Our extensive experiments indicate the benefit of our time-aware graph-based representation in multivariate time series classification while enriching them with more interpretability.}
}
@article{WANG2024110443,
title = {Automatic calculation of step size and inertia parameter for convolutional dictionary learning},
journal = {Pattern Recognition},
volume = {152},
pages = {110443},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110443},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001948},
author = {Jinjia Wang and Pengyu Li and Yali Zhang and Ze Li and Jingchen Xu and Qian Wang and Jing Li},
keywords = {Image reconstruction, Convolutional sparse representation, Convolutional dictionary learning, Inertial proximal gradient method, Dry friction},
abstract = {The convergent methods available for convolutional dictionary learning (CDL) are the proximal gradient method (PGM) and the inertial proximal gradient method (IPGM). However, it is not trivial and heuristic for IPGM to set the step size and inertia parameter, and IPGM produces local minima and oscillating solutions. To address these issues, in this paper, we introduce a dry friction, which has an oscillation-alleviating property. Specifically, the proposed IPGM with dry friction (IPGM-DF) generates the composite proximal mappings, whose construction and optimization solver are two challenges. An auxiliary function is designed to construct the composite proximal mappings, whose optimization problem is solved by the alternating direction method of multipliers. Fortunately, IPGM-DF obtains the formulas of the step size and inertia parameter. The finite convergence of IPGM-DF is proved. Experimental results of image reconstruction, separation, and fusion demonstrate the superiority of IPGM-DF over IPGM and the state-of-the-art methods. For image reconstruction, the objective function value of IPGM-DF is reduced by about 38.708% than that of IPGM. Throughout alleviating oscillations, IPGM-DF obtains a much lower value of the objective function than IPGM, which indicates that IPGM-DF jumps out of the local minima of IPGM. In addition, the average PSNR of IPGM-DF is about 3.5 dB larger than that of IPGM and the state-of-the-art methods. The code is available.}
}
@article{CHEN2024110419,
title = {Structure-aware neural radiance fields without posed camera},
journal = {Pattern Recognition},
volume = {151},
pages = {110419},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110419},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001705},
author = {Shu Chen and Yang Zhang and Yaxin Xu and Beiji Zou},
keywords = {Neural radiance fields, Depth consistent constraint, Novel view synthesis},
abstract = {The neural radiance fields (NeRF) for realistic novel view synthesis require camera poses to be pre-acquired by a structure-from-motion (SfM) approach. This two-stage strategy is not convenient to use and degrades the performance because the error in the pose extraction can propagate to the view synthesis. We integrate pose extraction and view synthesis into a jointly optimized process so that they can benefit from each other. For network training, only images are given without pre-known camera poses. The camera poses are obtained by the depth-consistent constraint in which the identical feature in different views has the same world coordinates transformed from the local camera coordinates according to the extracted poses. The depth-consistent constraint is jointly optimized with the pixel color constraint. The poses are represented by a CNN-based deep network, whose input is the related frames. This joint optimization enables NeRF to be aware of the scene’s structure, resulting in improved generalization performance. Experiments on three datasets demonstrate the effectiveness of camera pose estimation and novel view synthesis. Code is available at https://github.com/XTU-PR-LAB/SaNerf.}
}
@article{LIU2024110430,
title = {Domain-incremental learning without forgetting based on random vector functional link networks},
journal = {Pattern Recognition},
volume = {151},
pages = {110430},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110430},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400181X},
author = {Chong Liu and Yi Wang and Dong Li and Xizhao Wang},
keywords = {Incremental learning, Domain-incremental learning, RVFL network, Catastrophic forgetting, Privacy preservation},
abstract = {Incremental learning is a paradigm that extends knowledge by learning from new data, often used to add new classes to an existing model or to learn a new domain. It imposes strict limitations on the model’s access to data from previous tasks, making it similar to the human learning process. The main challenge of incremental learning is catastrophic forgetting, where previous knowledge is severely forgotten while learning new tasks. In this work, we propose a novel approach for domain-incremental learning. Inspired by the Normal Equation, we accumulate the Gram Matrix from each task’s hidden layer output to update a simplified RVFL model. This algorithm achieves performance comparable to joint training while strictly adhering to privacy restrictions. With issues such as forgetting, storage requirements and privacy protection be addressed, this algorithm has the potential to play a crucial role in the field of edge computing and other related fields.}
}
@article{ZHANG2024110529,
title = {Scene recovery: Combining visual enhancement and resolution improvement},
journal = {Pattern Recognition},
volume = {153},
pages = {110529},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110529},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002802},
author = {Hao Zhang and Te Qi and Tieyong Zeng},
keywords = {Single image visual enhancement, Resolution improvement, Variational scene recovery model, Various scenes},
abstract = {Visibility enhancement of outdoor images under complex imaging conditions has been a crucial task for computer vision and received growing attention. However, existing image enhancement methods could result in typical block-like artifacts or color distortion. The undesirable impurities might also be significantly magnified after the enhancement task, further reducing the image quality. For enhancing and super-resolving complex real-world degradation, we propose a simultaneous visual enhancement and resolution improvement (VERI) variational scene recovery model for jointly enhancing image visibility and improving the resolution of the degraded image. Particularly, we estimate the scattering light map for degradation images to achieve clean scene radiance and simultaneously seek a high-quality image through a deep super-resolution network. The semi-proximal alternating direction method of multipliers (sPADMM) algorithm is employed for efficiently solving the minimization problems in the proposed model. Extensive experiments illustrate the effectiveness and robustness of the proposed method in dealing with various scenes, such as haze, sandstorm, underwater or low illumination.}
}
@article{CHEN2024110432,
title = {Improving ellipse fitting via multi-scale smoothing and key-point searching},
journal = {Pattern Recognition},
volume = {151},
pages = {110432},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110432},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001833},
author = {Xiao-Diao Chen and Cheng Qian and Mingyang Zhao and Jun-Hai Yong and Dong-Ming Yan},
keywords = {Ellipse fitting, Least-squares, RANSAC, Multi-scale smoothing, Key-point searching},
abstract = {Fast and efficient fitting of accurate ellipses from data points has many applications in pattern recognition, machine vision, and robotics. However, the fitting accuracy may significantly degrade in the existence of outliers, such as the least-squares-based approaches. Despite robust methods attaining more accurate results than the least-squares manner under the contamination of outliers, they typically require the careful tuning of the hyper-parameters for good results. To mitigate the outlier disturbance, in this paper, we propose a conceptually simple yet quite useful preprocessing framework for high-precision ellipse fitting. Firstly, we leverage multi-scale operators to shrink the input image, by which a large number of outliers can be removed, followed by the smoothing of the sub-image to further improve the data quality. Then, we propose a key-point searching method to enhance the fitting precision via the analysis of the discrete pixel data in images. We prove that key-point-based ellipse fitting gives the upper bound of the approximation error generated by other sampled points with the same ellipse. Based on the key-point pairs inside and outside the ellipse, we further calculate their barycentric points and then perform fitting on these points to attain high-precision ellipses. We conduct extensive experiments on synthetic and real-world images to validate the proposed method and compare it with representative state-of-the-art approaches. Quantitative and qualitative results demonstrate that our method has more accurate and robust performance than competitors. Additionally, we employ the proposed method to compared approaches as a preprocessing step. Experiments demonstrate that our method is effective to significantly improve their fitting accuracy. Our source code is freely available at https://github.com/ChengQian09/MSKPF.}
}
@article{WANG2024110501,
title = {Generative data augmentation by conditional inpainting for multi-class object detection in infrared images},
journal = {Pattern Recognition},
volume = {153},
pages = {110501},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110501},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002528},
author = {Peng Wang and Zhe Ma and Bo Dong and Xiuhua Liu and Jishiyu Ding and Kewu Sun and Ying Chen},
keywords = {Data augmentation, Image inpainting, GAN, Infrared image, Multi-class object detection},
abstract = {Multi-class object detection in infrared images is important in military and civilian use. Deep learning methods can obtain high accuracy but require a large-scale dataset. We propose a generative data augmentation framework DOCI-GAN, for infrared multi-class object detection with limited data. Contributions of this paper are four-folds. Firstly, DOCI-GAN is designed as a conditional image inpainting framework, yielding paired infrared multi-class object image and annotation. Secondly, a text-to-image converter is formulated to transform text-format object annotations to bounding box mask images, leading the augmentation to be mask-image-to-raw-image translation. Thirdly, a multiscale morphological erosion-based loss is created to alleviate the intensity inconsistency between inpainted local backgrounds and global background. Finally, for generating diverse images, artificial multi-class object annotations are integrated with real ones during augmentation. Experimental results demonstrated that DOCI-GAN augments dataset with high-quality infrared multi-class object images, consequently improving the accuracy of object detection baselines.}
}
@article{WANG2024110513,
title = {GCNet: Probing self-similarity learning for Generalized Counting Network},
journal = {Pattern Recognition},
volume = {153},
pages = {110513},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110513},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002644},
author = {Mingjie Wang and Yande Li and Jun Zhou and Graham W. Taylor and Minglun Gong},
keywords = {Class-agnostic counting, Exemplar-free scheme, Zero-shot paradigm, Self-similarity learning},
abstract = {The class-agnostic counting (CAC) problem has garnered significant attention recently due to its broad societal applications and formidable challenges. Existing approaches to counting objects of various categories typically rely on user-provided exemplars, which are challenging to obtain and limit their generality. In this paper, our goal is to empower the framework to recognize adaptive exemplars within entire images. To achieve this, we introduce a zero-shot Generalized Counting Network (GCNet), which utilizes a pseudo-Siamese structure to automatically and efficiently learn pseudo exemplar cues from inherent repetition patterns. In addition, a weakly-supervised scheme is presented to reduce the burden of laborious density maps required by all contemporary CAC models, allowing GCNet to be trained using count-level supervisory signals in an end-to-end manner. Without providing any spatial location hints, GCNet is capable of adaptively capturing them through a carefully-designed self-similarity learning strategy. Extensive experiments and ablation studies on the prevailing benchmark FSC147 for zero-shot CAC demonstrate the superiority of our GCNet. It performs on par with existing exemplar-dependent methods and shows stunning cross-dataset generality on crowd-specific datasets, e.g., ShanghaiTech Part A, Part B and UCF_QNRF.}
}
@article{CHEN2024110489,
title = {Improving image segmentation with contextual and structural similarity},
journal = {Pattern Recognition},
volume = {152},
pages = {110489},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110489},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002401},
author = {Xiaoyang Chen and Qin Liu and Hannah H. Deng and Tianshu Kuang and Henry Hung-Ying Lin and Deqiang Xiao and Jaime Gateno and James J. Xia and Pew-Thian Yap},
keywords = {Image segmentation, Inter-voxel relationships, Cone-beam computed tomography, Pancreas segmentation},
abstract = {Deep learning models for medical image segmentation are usually trained with voxel-wise losses, e.g., cross-entropy loss, focusing on unary supervision without considering inter-voxel relationships. This oversight potentially leads to semantically inconsistent predictions. Here, we propose a contextual similarity loss (CSL) and a structural similarity loss (SSL) to explicitly and efficiently incorporate inter-voxel relationships for improved performance. The CSL promotes consistency in predicted object categories for each image sub-region compared to ground truth. The SSL enforces compatibility between the predictions of voxel pairs by computing pair-wise distances between them, ensuring that voxels of the same class are close together whereas those from different classes are separated by a wide margin in the distribution space. The effectiveness of the CSL and SSL is evaluated using a clinical cone-beam computed tomography (CBCT) dataset of patients with various craniomaxillofacial (CMF) deformities and a public pancreas dataset. Experimental results show that the CSL and SSL outperform state-of-the-art regional loss functions in preserving segmentation semantics.}
}
@article{WU2024110427,
title = {Spatial–temporal hypergraph based on dual-stage attention network for multi-view data lightweight action recognition},
journal = {Pattern Recognition},
volume = {151},
pages = {110427},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110427},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400178X},
author = {Zhixuan Wu and Nan Ma and Cheng Wang and Cheng Xu and Genbao Xu and Mingxing Li},
keywords = {Dual-stage attention network, Salient region, Spatial–temporal hypergraph neural network, Multi-view, Action recognition},
abstract = {For the problems of irrelevant frames and high model complexity in action recognition, we propose a Spatial–Temporal Hypergraph based on Dual-Stage Attention Network (STHG-DAN) for multi-view data lightweight action recognition. It includes two stages: Temporal Attention Mechanism based on Trainable Threshold (TAM-TT) and Hypergraph Convolution based on Dynamic Spatial–Temporal Attention Mechanism (HG-DSTAM). In the first stage, TAM-TT uses a learning threshold to extract keyframes from multi-view videos, with the multi-view data serving as a guarantee for providing more comprehensive information subsequently; In the second stage, HG-DSTAM divides the human joints into three parts: trunk, hand and leg to build spatial–temporal hypergraphs, extracts high-order features from spatial–temporal hypergraphs constructed of multi-view human body joints, inputs them into the dynamic spatial–temporal attention mechanism, and learns the intra frame correlation of multi-view data between the joint features of body parts, which can obtain the significant areas of action; We use multi-scale convolution operation and depth separable network, which can realize efficient action recognition with a few trainable parameters. We experiment on the NTU-RGB+D, NTU-RGB+D 120 and the imitating traffic police gesture dataset. The performance and accuracy of the model are better than the existing algorithms, effectively improving the machine and human body language interaction cognitive ability.}
}
@article{DEBOER2024110497,
title = {SurvivalLVQ: Interpretable supervised clustering and prediction in survival analysis via Learning Vector Quantization},
journal = {Pattern Recognition},
volume = {153},
pages = {110497},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110497},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002486},
author = {Jasper {de Boer} and Klest Dedja and Celine Vens},
keywords = {Survival analysis, Clustering, Learning vector quantization (LVQ), Interpretable},
abstract = {Identifying subgroups with similar survival outcomes is a pivotal challenge in survival analysis. Traditional clustering methods often neglect the outcome variable, potentially leading to inaccurate representation of risk profiles. To address this, we present SurvivalLVQ, a novel interpretable method that adapts Learning Vector Quantization (LVQ) to survival analysis. Unlike traditional classification uses of LVQ, SurvivalLVQ groups individuals by survival probabilities and assigns a unique survival curve to each cluster, representing the collective survival behavior within that group. Moreover, it can predict individual survival curves using weighted averages from nearby clusters. When tested on 76 benchmark datasets, it outperformed other clustering methods and showed competitive prediction performance. SurvivalLVQ bridges the gap between clustering techniques and outcome-oriented methods. Its strong clustering performance, coupled with competitive prediction capabilities and with easy to interpret outcomes, make it a promising tool for various applications within survival analysis.}
}
@article{LIU2024110473,
title = {Robust multiple subspaces transfer for heterogeneous domain adaptation},
journal = {Pattern Recognition},
volume = {152},
pages = {110473},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110473},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002243},
author = {Youfa Liu and Bo Du and Yongyong Chen and Lefei Zhang},
keywords = {Heterogeneous domain adaptation, Subspace transfer, Stability, Generalization error, Convergence},
abstract = {Heterogeneous domain adaptation (HDA) aims to execute knowledge transfer from a source domain to a heterogeneous target domain. Previous works typically inject knowledge from the source and target domain into a common subspace. However, this may lead to the ineffectiveness of knowledge transfer due to the existence of heterogeneity. To overcome this drawback, in this paper, we propose a robust multiple subspaces transfer method for heterogeneous domain adaptation. Specifically, knowledge of two domains is projected into a union of multiple subspaces via a self-expressive model, in which joint distribution alignment and dynamic Laplacian regularization on self-repressive coefficients are included in the loss for characterizing transferability. Moreover, we provide a comprehensive analysis of stability, complexity, generalization, and convergence guarantee for the proposed method. Experiments on benchmark vision and Language datasets verify effectiveness of the proposed approach for heterogeneous domain adaptation.}
}
@article{NGUYEN2024110457,
title = {Multi-camera multi-object tracking on the move via single-stage global association approach},
journal = {Pattern Recognition},
volume = {152},
pages = {110457},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110457},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002085},
author = {Pha Nguyen and Kha Gia Quach and Chi Nhan Duong and Son Lam Phung and Ngan Le and Khoa Luu},
abstract = {The development of autonomous vehicles generates a tremendous demand for a low-cost solution based on a complete set of camera sensors to perceive the environment around the car. Towards this solution, it is essential for object detection and tracking to address new challenges in multi-camera settings. To address these challenges, this work introduces novel Single-Stage Global Association Tracking approaches to associate one or more detections from multi-cameras with tracked objects. These approaches aim to solve fragment-tracking issues caused by inconsistent 3D object detection. Moreover, our models also improve the detection accuracy of the standard vision-based 3D object detectors in the nuScenes detection challenge. The extensive experimental results on the nuScenes dataset demonstrate the benefits of the proposed method, which outperforms prior vision-based tracking methods in multi-camera settings.}
}
@article{YI2024110468,
title = {SceneFake: An initial dataset and benchmarks for scene fake audio detection},
journal = {Pattern Recognition},
volume = {152},
pages = {110468},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110468},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400219X},
author = {Jiangyan Yi and Chenglong Wang and Jianhua Tao and Chu Yuan Zhang and Cunhang Fan and Zhengkun Tian and Haoxin Ma and Ruibo Fu},
keywords = {Scene manipulation, Fake audio detection, Speech enhancement, SceneFake dateset},
abstract = {Many datasets have been designed to further the development of fake audio detection. However, fake utterances in previous datasets are mostly generated by altering timbre, prosody, linguistic content or channel noise of original audio. These datasets leave out a scenario, in which the acoustic scene of an original audio is manipulated with a forged one. It will pose a major threat to our society if some people misuse the manipulated audio with malicious purpose. Therefore, this motivates us to fill in the gap. This paper proposes such a dataset for scene fake audio detection named SceneFake, where a manipulated audio is generated by only tampering with the acoustic scene of an real utterance by using speech enhancement technologies. Some scene fake audio detection benchmark results on the SceneFake dataset are reported in this paper. In addition, an analysis of fake attacks with different speech enhancement technologies and signal-to-noise ratios are presented in this paper. The results indicate that scene fake utterances cannot be reliably detected by baseline models trained on the ASVspoof 2019 dataset. Although these models perform well on the SceneFake training set and seen testing set, their performance is poor on the unseen test set. The dataset22https://zenodo.org/record/7663324#.Y_XKMuPYuUk. and benchmark source codes33https://github.com/ADDchallenge/SceneFake. are publicly available.}
}
@article{WANG2024110449,
title = {Multi-target label backdoor attacks on graph neural networks},
journal = {Pattern Recognition},
volume = {152},
pages = {110449},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110449},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002000},
author = {Kaiyang Wang and Huaxin Deng and Yijia Xu and Zhonglin Liu and Yong Fang},
keywords = {Backdoor attack, Graph neural networks, Node classification},
abstract = {Graph neural networks have been shown to have characteristics that make them susceptible to backdoor attacks, and many recent works have proposed feasible graph backdoor attack methods. However, existing graph backdoor attack methods only target one-to-one attack types and lack graph backdoor attack methods that can address one-to-many attack requirements. This paper is the first research work on one-to-many type graph backdoor attacks and proposes the backdoor attack method MLGB, which can achieve multi-target label attacks for GNN node classification tasks. We designed encoding mechanisms to allow MLGB to customize triggers for different target labels and ensure differentiation between triggers for different target labels through loss functions. Additionally, we designed an innovative poisoned node selection method to improve the efficiency of MLGB’s attacks further. Extensive experiments were conducted to validate MLGB’s effectiveness across multiple datasets and model architectures, demonstrating its robustness against graph backdoor attack defense mechanisms. Furthermore, ablation experiments and explainability analyses were conducted to provide deeper insights into MLGB. Our work reveals that graph neural networks are also vulnerable to one-to-many type backdoor attacks, which is important for practitioners to understand model risks comprehensively.}
}
@article{YUAN2024110428,
title = {LCSeg-Net: A low-contrast images semantic segmentation model with structural and frequency spectrum information},
journal = {Pattern Recognition},
volume = {151},
pages = {110428},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110428},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001791},
author = {Haochen Yuan and Junjie Peng},
keywords = {Image semantic segmentation, Low-contrast image, Structural information enhancement, Feature adaptive fusion},
abstract = {Semantic segmentation in low-contrast images is a challenging problem due to the ambiguous boundary of the segmented target and indistinguishable noise. Current models generally segment the target with local features, which leads to the lack of structural information, which reduces the performance. And they directly fuse multi-scale features to keep details, while it may integrate noise into the fused features. These problems reduce the performance of segmentation in the low-contrast image. To solve the above issues, we propose an image semantic segmentation model called Low-Contrast Segmentation Net (LCSeg-Net). The model enhances the structural information with the global context and reduces the noise of the fused features through the adaptive fusion way. Meanwhile, to improve the robustness of LCSeg-Net, we augment the low-frequency spectrum of the fused feature in the training phase. Extensive experiments are conducted on the five public datasets. Results show the proposed model achieves the best comprehensive performance in the low-contrast image.}
}
@article{HE2024110439,
title = {Residual Feature-Reutilization Inception Network},
journal = {Pattern Recognition},
volume = {152},
pages = {110439},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110439},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001900},
author = {Yuanpeng He and Wenjie Song and Lijian Li and Tianxiang Zhan and Wenpin Jiao},
keywords = {Feature-reutilization, Residual connection, Inception},
abstract = {Capturing feature information effectively is of great importance in the field of computer vision. With the development of convolutional neural networks, concepts like residual connection and multiple scales promote continual performance gains in diverse deep learning vision tasks. In this paper, novel residual feature-reutilization inception and split-residual feature-reutilization inception are proposed to improve performance on various vision tasks. It consists of four parallel branches, each with convolutional kernels of different sizes. These branches are interconnected by hierarchically organized channels, similar to residual connections, facilitating information exchange and rich dimensional variations at different levels. This structure enables the acquisition of features with varying granularity and effectively broadens the span of the receptive field in each network layer. Moreover, according to the network structure designed above, split-residual feature-reutilization inceptions can adjust the split ratio of the input information, thereby reducing the number of parameters and guaranteeing the model performance. Specifically, in image classification experiments based on popular vision datasets, such as CIFAR10 (97.94%), CIFAR100 (85.91%), Tiny Imagenet (70.54%) and ImageNet (80.83%), we obtain state-of-the-art results compared with other modern models under the premise that the models’ sizes are approximate and no additional data is used.}
}
@article{LIU2024110435,
title = {UnitModule: A lightweight joint image enhancement module for underwater object detection},
journal = {Pattern Recognition},
volume = {151},
pages = {110435},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110435},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001869},
author = {Zhuoyan Liu and Bo Wang and Ye Li and Jiaxian He and Yunfeng Li},
keywords = {Underwater object detection, Image enhancement, Unsupervised learning, Joint training},
abstract = {Underwater object detection faces the problem of underwater image degradation, which affects the performance of the detector. Underwater object detection methods based on noise reduction and image enhancement usually do not provide images preferred by the detector or require additional datasets. In this paper, we propose a plug-and-play Underwater joint image enhancement Module (UnitModule) that provides the input image preferred by the detector. We design an unsupervised learning loss for the joint training of UnitModule with the detector without additional datasets to improve the interaction between UnitModule and the detector. Furthermore, a color cast predictor with the assisting color cast loss and a data augmentation called Underwater Color Random Transfer (UCRT) are designed to improve the performance of UnitModule on underwater images with different color casts. Extensive experiments are conducted on DUO for different object detection models, where UnitModule achieves the highest performance improvement of 2.6 AP for YOLOv5-S and gains the improvement of 3.3 AP on the brand-new test set (URPCtest). And UnitModule significantly improves the performance of all object detection models we test, especially for models with a small number of parameters. In addition, UnitModule with a small number of parameters of 31K has little effect on the inference speed of the original object detection model. Our quantitative and visual analysis also demonstrates the effectiveness of UnitModule in enhancing the input image and improving the perception ability of the detector for object features. The code is available at https://github.com/LEFTeyex/UnitModule.}
}
@article{LEI2024110423,
title = {Hybrid federated learning with brain-region attention network for multi-center Alzheimer's disease detection},
journal = {Pattern Recognition},
volume = {153},
pages = {110423},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110423},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001742},
author = {Baiying Lei and Yu Liang and Jiayi Xie and You Wu and Enmin Liang and Yong Liu and Peng Yang and Tianfu Wang and ChuanMing Liu and Jichen Du and Xiaohua Xiao and Shuqiang Wang},
keywords = {Alzheimer's disease, Attention, Federated learning, Hybrid learning},
abstract = {Identifying reproducible and interpretable biomarkers for Alzheimer's disease (AD) detection remains a challenge. AD detection using multi-center datasets can expand the sample size to improve robustness but might lead to a data privacy problem. Moreover, due to the high cost of labeling data, a lot of unlabeled data in each center is not fully utilized. To address this, a hybrid FL (HFL) framework is proposed that not only uses unlabeled data to train deep learning networks, but also achieves data privacy protection. We propose a novel Brain-region Attention Network (BANet), which highlights important regions via attention to represent the region of interest (ROIs).Specifically, we use a brain template to extract ROI signals from the preprocessed structure magnetic resonance imaging (sMRI) data. In addition, we add a self-supervised loss to the current loss to guide the attention map generation to learn the representations from unlabeled data. Finally, we evaluate our method on a multi-center database which is constructed using five AD datasets. The experimental results show that the proposed method performs better than state-of-the-art methods, achieving mean accuracy rates of 85.69 %, 63.34 %, and 69.89 % on the AD vs. NC, MCI vs. NC, and AD vs. MCI respectively. The source code is available for reproducibility at: https://github.com/yuliangCarmelo/HFL.}
}
@article{ZHONG2024110492,
title = {Semi-supervised pathological image segmentation via cross distillation of multiple attentions and Seg-CAM consistency},
journal = {Pattern Recognition},
volume = {152},
pages = {110492},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110492},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002437},
author = {Lanfeng Zhong and Xiangde Luo and Xin Liao and Shaoting Zhang and Guotai Wang},
keywords = {Semi-supervised learning, Knowledge distillation, Attention, Multi-task learning},
abstract = {Segmentation of pathological images is a crucial step for accurate cancer diagnosis. However, acquiring dense annotations of such images for training is labor-intensive and time-consuming. To address this issue, Semi-Supervised Learning (SSL) has the potential for reducing the annotation cost, but it is challenged by a large number of unlabeled training images. In this paper, we propose a novel SSL method based on Cross Distillation of Multiple Attentions and Seg-CAM Consistency (CDMA+) to effectively leverage unlabeled images. First, we propose a Multi-attention Tri-decoder Network (MTNet) that consists of a shared encoder and three decoders, with each decoder using a different attention mechanism that calibrates features in different aspects to generate diverse outputs. Second, we introduce Cross Decoder Knowledge Distillation (CDKD) between the three decoders, allowing them to learn from each other’s soft labels to mitigate the negative impact of incorrect pseudo labels during training. Subsequently, motivated by the observation that the Class Activation Maps (CAMs) derived from the classification task could provide a rough segmentation, we employ an auxiliary classification head and introduce a consistency constraint between the CAM and segmentation results, i.e. Seg-CAM consistency. Additionally, uncertainty minimization is applied to the average prediction of the three decoders, which further regularizes predictions on unlabeled images and encourages inter-decoder consistency. Our proposed CDMA+ was compared with eight state-of-the-art SSL methods on two public pathological image datasets, and the experimental results showed that our method outperforms the other approaches under different annotation ratios. The code is available at https://github.com/HiLab-git/CDMA.}
}
@article{TAN2024110447,
title = {Label enhancement via manifold approximation and projection with graph convolutional network},
journal = {Pattern Recognition},
volume = {152},
pages = {110447},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110447},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001985},
author = {Chao Tan and Sheng Chen and Xin Geng and Yunyao Zhou and Genlin Ji},
keywords = {Multi-label classification, Label distribution learning, Manifold learning, Robust linear regression, Graph convolutional network},
abstract = {Label enhancement (LE) aims to enrich logical labels into their corresponding label distributions. But existing LE algorithms fail to fully leverage the structural information in the feature space to improve LE learning. To address this key issue, we first apply manifold learning to map the relatedness between low-dimensional feature samples to the label space. Based on the smoothness assumption of manifolds, the implicit correlation between low-dimensional feature and label spaces effectively promotes the LE process, enabling the learning model to accurately capture the mapping relationship between feature and label manifolds. This leads to an LE based on feature representation (LEFR) algorithm. We also propose an LE algorithm based on graph convolutional network (GCN), called LE-GCN. Inspired by the relationship between threshold connections and label connections, we extend GCN to the LE field for the first time to fully exploit the hidden relationships between nodes and labels. By enhancing node information with threshold connections and label connections, the label learning accuracy reaches a new level. Experiments on real-world datasets show that our LEFR and LE-GCN outperform several state-of-the-art LE algorithms.}
}
@article{WANG2024110480,
title = {Feature decoupling and regeneration towards wifi-based human activity recognition},
journal = {Pattern Recognition},
volume = {153},
pages = {110480},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110480},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002310},
author = {Siyang Wang and Lin Wang and Wenyuan Liu},
keywords = {Wireless sensing, Human activity recognition (HAR), Deep learning, Cross user domain, Channel state information},
abstract = {Gesture recognition using WiFi is vital for human–computer interaction, smart homes, smart spaces, and elderly care. WiFi signals are non-stationary, sensitive to the environment, and traditional pattern recognition-based HAR methods incur exorbitant training and deployment costs due to their reliance on the quality and quantity of training data. While there has been extensive research on enhancing HAR techniques, the efficiency of the system is still limited by the scarcity of training data and identity-action feature coupling. This research focuses on attaining gesture recognition across multiple users with a limited number of samples. Our findings show that the conventional data augmentation methods are incapable of facilitating the system in attaining sample diversity. As a result, we propose a training-free augmentation strategy as a means to provide adequate training data. Unlike the conventional data enhancement approach, this scheme aims to develop data processing methods that differentiate between various samples in practical applications. Consequently, it enhances data pertinent to specific HAR assignments effectively. To effectively extract action features in WiFi samples, an unsupervised cross-user domain sample generation (CUDSG) model is proposed. This model generates virtual gesture samples for new user domains by decoupling and recombining gesture and identity features. This extends the sensing boundary of the system to new user domains without requiring a significant number of users to participate. Model performance was evaluated using various classifiers, such as SVM, KNN and CNN. The results demonstrate a significant improvement in average classification accuracy from 57.3% to 98.4%. This indicates that CUDSG is a highly effective tool for enhancing the performance of existing gesture recognition techniques.}
}
@article{ZHANG2024110528,
title = {Affine Collaborative Normalization: A shortcut for adaptation in medical image analysis},
journal = {Pattern Recognition},
volume = {153},
pages = {110528},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110528},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002796},
author = {Chuyan Zhang and Yuncheng Yang and Hao Zheng and Yawen Huang and Yefeng Zheng and Yun Gu},
keywords = {Fine-tuning, Transfer learning, Transferability estimation, Self-supervised learning},
abstract = {The paradigm of “pretraining-then-finetuning” (PT-FT) has been extensively explored to enhance the performance of clinical applications with limited annotations. A major impediment to applying such workflow to various medical imaging modalities is the lack of parameter-free approaches boosting the transferability against notable domain shifts. The success of normalization techniques in domain adaptation provides a promising solution, while inaccessible source data in finetuning (FT) poses new challenges. In this paper, we revisit the Batch Normalization module in PT-FT and propose a unified framework for both fast transferability estimation and transferability-aware finetuning. We discovered that the vanilla FT suffers from the issue that feature patterns of corresponding channels could be misaligned across domains. Hence, we present an Affine Collaborative Normalization (AC-Norm) to dynamically recalibrate the channels in the target model according to the cross-domain channel-wise correlations without any source data and extra parameters. AC-Norm provides very competitive results compared to state-of-the-art FT methods in six classification/segmentation tasks. We also demonstrate the capability of AC-Norm in estimating the transferability of pretrained models in only single-step backpropagation. Our code is available at https://github.com/EndoluminalSurgicalVision-IMR/ACNorm.}
}
@article{WU2024110504,
title = {Robust deep fuzzy K-means clustering for image data},
journal = {Pattern Recognition},
volume = {153},
pages = {110504},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110504},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002553},
author = {Xiaoling Wu and Yu-Feng Yu and Long Chen and Weiping Ding and Yingxu Wang},
keywords = {Locality preserving, Deep convolutional autoencoder, Laplacian regularization, Unsupervised image clustering},
abstract = {Image clustering is a difficult task with important application value in computer vision. The key to this task is the quality of images features. Most of current clustering methods encounter the challenge. That is, the process of feature learning and clustering operates independently. To address this problem, several researchers have been dedicated to performing feature learning and deep clustering together. However, the obtained features lack discriminability to address high-dimensional data successfully. To deal with this issue, we propose a novel model named as robust deep fuzzy K-means clustering (RD-FKC), which efficiently projects image samples into a representative embedding space and precisely learns membership degrees into a combined framework. Specifically, RD-FKC introduces Laplacian regularization technique to preserve locality properties of data. Moreover, by using an adaptive loss function, the model becomes more robust to diverse types of outliers. Furthermore, to avoid the latent space being distorted and make the extracted features retain the original information as much as possible, the model introduces reconstruction error and adds regularization to network parameters. Finally, an effective algorithm is derived to solve the optimization model. Numerous experiments have been conducted, illustrating the advantages and superiority of RD-FKC over existing clustering approaches.}
}
@article{LIU2024110452,
title = {Zero-shot sketch-based image retrieval via adaptive relation-aware metric learning},
journal = {Pattern Recognition},
volume = {152},
pages = {110452},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110452},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002036},
author = {Yang Liu and Yuhao Dang and Xinbo Gao and Jungong Han and Ling Shao},
keywords = {ZS-SBIR, Relation-aware, Metric learning, Adaptive, Margin},
abstract = {Retrieving natural images with the query sketches under the zero-shot scenario is known as zero-shot sketch-based image retrieval (ZS-SBIR). Most of the best-performing methods adapt the triplet loss to learn projections that map natural images and sketches to a latent embedding space. They nevertheless neglect the modality gap between the hand-drawn sketches and the photos and consider no difference between any two incorrect classes, which limits their performance in real use cases. Towards this end, we put forward a simple and effective model, which adopts relation-aware metric learning to suppress the modality gap between the sketches and the photos. We also propose an adaptive margin that utilizes each anchor in embedding space to improve clustering ability in metric learning. Extensive experiments on the Sketchy and TU-Berlin datasets show the dominant position of our proposed model over SOTA competitors.}
}
@article{YANG2024110488,
title = {Soft independence guided filter pruning},
journal = {Pattern Recognition},
volume = {153},
pages = {110488},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110488},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002395},
author = {Liu Yang and Shiqiao Gu and Chenyang Shen and Xile Zhao and Qinghua Hu},
keywords = {Convolutional neural networks, Filter pruning, Filter independence},
abstract = {Filter pruning (FP) is an effective method for reducing the computational costs of convolutional neural networks, and herein, the most critical task involves evaluating the significance of each convolutional filter and eliminating the less important ones while minimizing performance degradation. Most existing FP methods consider only local information, which may prevent them from accurately recognizing the most important filters. To address this limitation, we propose the soft filter independence (SFI) method, which leverages global information to identify the most important filters using their magnitude and correlation information in different functional layers. The SFI criterion measures the replaceability of filters from a global perspective in a network. Filters with low independence can be represented effectively by others, so their information can be accurately conveyed by other filters. In addition, we introduce a novel SFI-based asymptotic pruning ratio, which improves training and pruning stability. Compared to the most advanced FP methods, our method enables CNNs to achieve higher pruning rates and better classification performance.}
}
@article{LI2024110444,
title = {ACQ: Improving generative data-free quantization via attention correction},
journal = {Pattern Recognition},
volume = {152},
pages = {110444},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110444},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400195X},
author = {Jixing Li and Xiaozhou Guo and Benzhe Dai and Guoliang Gong and Min Jin and Gang Chen and Wenyu Mao and Huaxiang Lu},
keywords = {Generative data-free quantization, Homogenous attention, Attention center matching, Adversarial loss, Consistency penalty},
abstract = {Data-free quantization aims to achieve model quantization without accessing any authentic sample. It is significant in an application-oriented context involving data privacy. Converting noise vectors into synthetic samples through a generator is a popular data-free quantization method, which is called generative data-free quantization. However, there is a difference in attention map between synthetic samples and authentic samples. This is always ignored and restricts the quantization performance. First, since synthetic samples of the same class are prone to have homogenous attention, the quantized network can only learn limited intra-class visual features. Second, synthetic samples in eval mode and training mode exhibit different attention. Hence, the statistical distribution matching tends to be inaccurate. ACQ is proposed in this paper to fix the attention of synthetic samples. Regarding intra-class attention homogeneity, we introduce an attention center matching loss aimed at achieving coarse-grained matching of attention of synthetic samples. Additionally, we have devised an adversarial loss based on pairs of samples with identical conditions. On one hand, this mechanism prevents the generator from mode collapse due to excessive attention on conditional information. On the other hand, it augments the separation between intra-class samples, thus further enhancing intra-class attention diversity. To improve the attention similarity of synthetic samples in different network modes, we introduce a consistency penalty to guarantee accurate statistical distribution matching. The experimental results demonstrate that ACQ effectively improves the attention problems of synthetic samples. Under various training settings, ACQ achieves the best quantization performance. For the 4-bit quantization of Resnet18 and Resnet50, ACQ reaches 67.55 % and 72.23 % accuracy, respectively.}
}
@article{ZHANG2024110502,
title = {Dynamic image super-resolution via progressive contrastive self-distillation},
journal = {Pattern Recognition},
volume = {153},
pages = {110502},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110502},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400253X},
author = {Zhizhong Zhang and Yuan Xie and Chong Zhang and Yanbo Wang and Yanyun Qu and Shaohui Lin and Lizhuang Ma and Qi Tian},
keywords = {Single Image Super-Resolution, Model compression, Model acceleration, Dynamic neural networks},
abstract = {Convolutional neural networks (CNNs) are highly successful for image super-resolution (SR). However, they often require sophisticated architectures with high memory cost and computational overhead, significantly restricting their practical deployments on resource-limited devices. In this paper, we propose a novel dynamic contrastive self-distillation (Dynamic-CSD) framework to simultaneously compress and accelerate various off-the-shelf SR models, and explore using the trained model for dynamic inference. In particular, to build a compact student network, a channel-splitting super-resolution network (CSSR-Net) can first be constructed from a target teacher network. Then, we propose a novel contrastive loss to improve the quality of SR images via explicit knowledge transfer. Furthermore, progressive CSD (Pro-CSD) is developed to extend the two-branch CSSR-Net into multi-branch, leading to a switchable model at runtime. Finally, a difficulty-aware branch selection strategy for dynamic inference is given. Extensive experiments demonstrate that the proposed Dynamic-CSD scheme effectively compresses and accelerates several standard SR models such as EDSR, RCAN and CARN.}
}
@article{TRUONG2024110531,
title = {A survey on handwritten mathematical expression recognition: The rise of encoder-decoder and GNN models},
journal = {Pattern Recognition},
volume = {153},
pages = {110531},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110531},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002826},
author = {Thanh-Nghia Truong and Cuong Tuan Nguyen and Richard Zanibbi and Harold Mouchère and Masaki Nakagawa},
keywords = {Mathematical expression recognition, Handwriting recognition, Symbol recognition, Spatial relationship classification, Structural analysis, Deep neural networks, CROHME competitions, Public dataset},
abstract = {Recognition of handwritten mathematical expressions (HMEs) has attracted growing interest due to steady progress in handwriting recognition techniques and the rapid emergence of pen- and touch-based devices. Math formula recognition may be understood as a generalization of text recognition: formulas represent mathematical statements using a two dimensional arrangement of symbols on writing lines that are organized hierarchically. This survey provides an overview of techniques published in the last decade, including those taking input from handwritten strokes (i.e., ‘online’, as captured by a pen/touch device), raster images (i.e., ‘offline,’ from pixels), or both. Traditionally, HMEs were recognized by performing four structural pattern recognition tasks in separate steps: (1) symbol segmentation, (2) symbol classification, (3) spatial relationship classification, and (4) structural analysis, which identifies the arrangement of symbols on writing lines (e.g., in a Symbol Layout Tree (SLT) or LaTeX string). Recently, encoder–decoder neural network models and Graph Neural Network (GNN) approaches have greatly increased HME recognition accuracy. These newer approaches perform all recognition tasks simultaneously, and utilize contextual features across tasks (e.g., using neural self-attention models). We also discuss evaluation techniques and benchmarks, and explore some implicit dependencies among the four key recognition tasks. Finally, we identify limitations of current systems, and present suggestions for future work, such as using two-dimensional language models rather than the one-dimensional models commonly used in encoder–decoder models.}
}
@article{LEE2024110479,
title = {PLoPS: Localization-aware person search with prototypical normalization},
journal = {Pattern Recognition},
volume = {153},
pages = {110479},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110479},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002309},
author = {Sanghoon Lee and Youngmin Oh and Donghyeon Baek and Junghyup Lee and Bumsub Ham},
keywords = {Normalization, Person search, Person re-identification},
abstract = {Person search involves localizing and re-identifying persons of interest captured by multiple, non-overlapping cameras. Recent approaches to person search are typically built on object detection frameworks to learn joint person representations for detection and re-identification. To this end, the features extracted from pedestrian proposals are projected onto a unit hypersphere using L2 normalization, and positive proposals that sufficiently overlap with the ground truth are equally incorporated for training by exploiting an external lookup table (LUT). We have found that (1) using the L2 normalization technique, without considering feature distributions, can degenerate the discriminative power of person representations, (2) positive proposals often depict distracting details, such as background clutter and person overlaps, and (3) person features in the LUT are not often updated during training. To address these limitations, we propose a novel framework for person search, dubbed PLoPS, using a prototypical normalization layer, ProtoNorm, that calibrates features while considering the long-tail distribution across person IDs. PLoPS also entails a localization-aware learning scheme that prioritizes better-aligned proposals w.r.t the ground truth. We further introduce a LUT calibration technique to continuously adjust the person features in the LUT. Experimental results and analysis on standard benchmarks demonstrate the effectiveness of PLoPS.}
}
@article{ZHANG2024110434,
title = {Towards effective person search with deep learning: A survey from systematic perspective},
journal = {Pattern Recognition},
volume = {152},
pages = {110434},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110434},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001857},
author = {Pengcheng Zhang and Xiaohan Yu and Chen Wang and Jin Zheng and Xin Ning and Xiao Bai},
keywords = {Person search, Deep learning, Pedestrian retrieval, Literature review},
abstract = {Person search detects and retrieves simultaneously a query person across uncropped scene images captured by multiple non-overlapping cameras. In light of the deep learning advancement, person search has emerged as a promising research direction that demonstrates great potential for real-world applications. This paper presents a systematic survey of deep learning methods for person search. Different from existing categorizations, we propose a new taxonomy that dissects person search models into four major components i.e., proposal prediction, feature representation learning, training objectives, and ranking optimization. The most representative works in each component are summarized with highlighted contributions to this field. An in-depth analysis is provided upon evaluation performances of state-of-the-art person search models together with a summary of benchmark datasets. Despite that significant progress has been made to date, practical and extendable person search remains an open task. We conclude with discussions on those under-explored yet challenging datasets and learning mechanisms for real-world demands to inspire future research directions.}
}
@article{CHEN2024110446,
title = {DGFormer: Dynamic graph transformer for 3D human pose estimation},
journal = {Pattern Recognition},
volume = {152},
pages = {110446},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110446},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001973},
author = {Zhangmeng Chen and Ju Dai and Junxuan Bai and Junjun Pan},
keywords = {3D human pose estimation, Transformer, Graph},
abstract = {Despite the significant progress for monocular 3D human pose estimation, it still faces challenges due to self-occlusions and depth ambiguities. To tackle those issues, we propose a novel Dynamic Graph Transformer (DGFormer) to exploit local and global relationships between skeleton joints for pose estimation. Specifically, the proposed DGFormer mainly consists of three core modules: Transformer Encoder (TE), immobile Graph Convolutional Network (GCN), and dynamic GCN. TE module leverages the self-attention mechanism to learn the complex global relationships among skeleton joints. The immobile GCN is responsible for capturing the local physical connections between human joints, while the dynamic GCN concentrates on learning the sparse dynamic K-nearest neighbor interactions according to different action poses. By building the adequately global long-range, local physical, and sparse dynamic dependencies of human joints, experiments on Human3.6M and MPI-INF-3DHP datasets demonstrate that our method can predict 3D pose with lower errors outperforming the recent state-of-the-art image-based performance. Furthermore, experiments on in-the-wild videos demonstrate the impressive generalization abilities of our method. Code will be available at: https://github.com/czmmmm/DGFormer.}
}
@article{GAN2024110515,
title = {RFL-CDNet: Towards accurate change detection via richer feature learning},
journal = {Pattern Recognition},
volume = {153},
pages = {110515},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110515},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002668},
author = {Yuhang Gan and Wenjie Xuan and Hang Chen and Juhua Liu and Bo Du},
keywords = {Change detection, Richer feature learning, Coarse-to-fine, Learnable fusion},
abstract = {Change Detection is a crucial but extremely challenging task of remote sensing image analysis, and much progress has been made with the rapid development of deep learning. However, most existing deep learning-based change detection methods mainly focus on intricate feature extraction and multi-scale feature fusion, while ignoring the insufficient utilization of features in the intermediate stages, thus resulting in sub-optimal results. To this end, we propose a novel framework, named RFL-CDNet, that utilizes richer feature learning to boost change detection performance. Specifically, we first introduce deep multiple supervision to enhance intermediate representations, thus unleashing the potential of backbone feature extractor at each stage. Furthermore, we design the Coarse-To-Fine Guiding (C2FG) module and the Learnable Fusion (LF) module to further improve feature learning and obtain more discriminative feature representations. The C2FG module aims to seamlessly integrate the side prediction from previous coarse-scale into the current fine-scale prediction in a coarse-to-fine manner, while LF module assumes that the contribution of each stage and each spatial location is independent, thus designing a learnable module to fuse multiple predictions. Experiments on several benchmark datasets show that our proposed RFL-CDNet achieves state-of-the-art performance on WHU cultivated land dataset and CDD dataset, and the second best performance on WHU building dataset. The source code and models are publicly available at https://github.com/Hhaizee/RFL-CDNet.}
}
@article{ZHU2024110543,
title = {Self-supervised learning for RGB-D object tracking},
journal = {Pattern Recognition},
volume = {155},
pages = {110543},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110543},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002942},
author = {Xue-Feng Zhu and Tianyang Xu and Sara Atito and Muhammad Awais and Xiao-Jun Wu and Zhenhua Feng and Josef Kittler},
keywords = {RGB-D tracking, Self-supervised learning, Masked image modelling},
abstract = {Recently, there has been a growing interest in RGB-D object tracking thanks to its promising performance achieved by combining visual information with auxiliary depth cues. However, the limited volume of annotated RGB-D tracking data for offline training has hindered the development of a dedicated end-to-end RGB-D tracker design. Consequently, the current state-of-the-art RGB-D trackers mainly rely on the visual branch to support the appearance modelling, with the depth map utilised for elementary information fusion or failure reasoning of online tracking. Despite the achieved progress, the current paradigms for RGB-D tracking have not fully harnessed the inherent potential of depth information, nor fully exploited the synergy of vision-depth information. Considering the availability of ample unlabelled RGB-D data and the advancement in self-supervised learning, we address the problem of self-supervised learning for RGB-D object tracking. Specifically, an RGB-D backbone network is trained on unlabelled RGB-D datasets using masked image modelling. To train the network, the masking mechanism creates a selective occlusion of the input visible image to force the corresponding aligned depth map to help with discerning and learning vision-depth cues for the reconstruction of the masked visible image. As a result, the pre-trained backbone network is capable of cooperating with crucial visual and depth features of the diverse objects and background in the RGB-D image. The intermediate RGB-D features output by the pre-trained network can effectively be used for object tracking. We thus embed the pre-trained RGB-D network into a transformer-based tracking framework for stable tracking. Comprehensive experiments and the analysis of the results obtained on several RGB-D tracking datasets demonstrate the effectiveness and superiority of the proposed RGB-D self-supervised learning framework and the following tracking approach.}
}
@article{GUO2024110491,
title = {UCTNet: Uncertainty-guided CNN-Transformer hybrid networks for medical image segmentation},
journal = {Pattern Recognition},
volume = {152},
pages = {110491},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110491},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002425},
author = {Xiayu Guo and Xian Lin and Xin Yang and Li Yu and Kwang-Ting Cheng and Zengqiang Yan},
keywords = {CNN-Transformer hybrid, Uncertainty, Functional overlap, Masked self-attention, Medical image segmentation},
abstract = {Transformer, born for long-range dependency establishment, has been widely studied as a complementary of convolutional neural networks (CNNs) in medical image segmentation. However, existing CNN-Transformer hybrid approaches simply pursue implicit feature fusion without considering their underlying functional overlap. Medical images typically follow stable anatomical structures, making convolution capable of handling most segmentation targets. Without differentiation, enforcing transformers to operate self-attention for all image patches would result in severe redundancy, hindering global feature extraction. In this paper, we propose a simple yet effective hybrid network named UCTNet where transformers only focus on establishing global dependency for CNN’s unreliable regions predicted through uncertainty estimation. In this way, CNN and transformer are explicitly fused to minimize functional overlap. More importantly, with fewer regions to handle, UCTNet is of better convergence to learn more robust feature representations for hard examples. Extensive experiments on publicly-available datasets demonstrate the superiority of UCTNet against the state-of-the-art approaches, achieving 89.44%, 92.91%, and 91.15% in Dice similarity coefficient on Synapse, ACDC, and ISIC2018 respectively. Furthermore, such a CNN-Transformer hybrid strategy is highly extendable to other frameworks without introducing additional computational burdens. Code is available at https://github.com/innocence0206/UCTNet.}
}
@article{JIANG2024110451,
title = {Mutual Balancing in State-Object Components for Compositional Zero-Shot Learning},
journal = {Pattern Recognition},
volume = {152},
pages = {110451},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110451},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002024},
author = {Chenyi Jiang and Qiaolin Ye and Shidong Wang and Yuming Shen and Zheng Zhang and Haofeng Zhang},
keywords = {Compositional Zero-Shot Learning, Image classification, Visual-attribute, Mutual Balancing},
abstract = {Compositional Zero-Shot Learning (CZSL) aims to recognize unseen compositions from seen states and objects. The disparity between the manually labeled semantic information and its actual visual features causes a significant imbalance of visual deviation in the distribution of various object classes and state classes, which is ignored by existing methods. To ameliorate these issues, we consider the CZSL task as an unbalanced multi-label classification task and propose a novel method called MUtual balancing in STate-object components (MUST) for CZSL, which provides a balancing inductive bias for the model. In particular, we split the classification of the composition classes into two consecutive processes to analyze the entanglement of the two components to get additional knowledge in advance, which reflects the degree of visual deviation between the two components. We use the knowledge gained to modify the model’s training process in order to generate more distinct class borders for classes with significant visual deviations. Extensive experiments demonstrate that our approach significantly outperforms the state-of-the-art on MIT-States, UT-Zappos, and C-GQA when combined with the basic CZSL frameworks, and it can improve various CZSL frameworks. Our code is available at https://github.com/LanchJL/MUST.}
}
@article{YANG2024110527,
title = {Early event detection for facial expression based on infinite mixture prototypes},
journal = {Pattern Recognition},
volume = {153},
pages = {110527},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110527},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002784},
author = {Zhi-Fang Yang and Dai-Yi Chiu},
keywords = {Early event detection, Few-shot learning, Infinite mixture prototypes},
abstract = {In early event detection, the ability to detect events at an earlier stage means to have more ample time for processing. The purpose of this study is to apply few-shot learning to early event detection based on infinite mixture prototypes. we present a design to distribute the early event detection issues across the loss function and the data task setting in few-shot learning, and the adoption of infinite mixture models helps to resolve the multi-model distribution of the pattern of early events. The proposed algorithm is evaluated on the CK＋database. The experimental results demonstrate not only feasible performance in terms of accuracy and detection time but also alignment with our hypothesis through the observation of generated clusters.}
}
@article{PAN2024110475,
title = {Cross-modal de-deviation for enhancing few-shot classification},
journal = {Pattern Recognition},
volume = {152},
pages = {110475},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110475},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002267},
author = {Mei-Hong Pan and Hong-Bin Shen},
keywords = {Cross-modal label assignment, Alternating least squares, Alternative optimization, Closed-form solution},
abstract = {Few-shot learning poses a critical challenge due to the deviation problem caused by the scarcity of available samples. In this work, we aim to address deviations in both feature representations and prototypes. To achieve this, we propose a cross-modal de-deviation framework that leverages class semantic information to provide robust prior knowledge for the samples. This framework begins with a visual-to-semantic autoencoder trained on the labeled samples to predict semantic features for the unlabeled samples. Then, we devise a binary linear programming model to match the initial prototypes with the cluster centers of the unlabeled samples. To circumvent potential mismatches between the cluster centers and the initial prototypes, we perform the label assignment process in the semantic space by transforming the cluster centers into semantic representations and utilizing the class ground truth semantic features as reference points. Moreover, we model a linear classifier with the concatenation of the refined prototypes and the class ground truth semantic features serving as the initial weights. Then we propose a novel optimization strategy based on the alternating least squares (ALS) model. From the ALS model, we can derive two closed-form solutions regarding to the features and weights, facilitating alternative optimization of them. Extensive experiments conducted on few-shot learning benchmarks demonstrate the competitive advantages of our CMDD method over the state-of-the-art approaches, confirming its effectiveness in reducing deviation. The code is available at: https://github.com/pmhDL/CMDD.git.}
}
@article{SERRANO2024110487,
title = {Community detection in the stochastic block model by mixed integer programming},
journal = {Pattern Recognition},
volume = {152},
pages = {110487},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110487},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002383},
author = {Breno Serrano and Thibaut Vidal},
keywords = {Community detection, Stochastic block model, Mixed integer programming, Machine learning, Unsupervised learning, Local search},
abstract = {The Degree-Corrected Stochastic Block Model (DCSBM) is a popular model to generate random graphs with community structure given an expected degree sequence. The standard approach of community detection based on the DCSBM is to search for the model parameters that are the most likely to have produced the observed network data through maximum likelihood estimation (MLE). Current techniques for the MLE problem are heuristics, and therefore do not guarantee convergence to the optimum. We present mathematical programming formulations and exact solution methods that can provably find the model parameters and community assignments of maximum likelihood given an observed graph. We compare these exact methods with classical heuristic algorithms based on expectation–maximization (EM). The solutions given by exact methods give us a principled way of measuring the experimental performance of classical heuristics and comparing different variations thereof.}
}
@article{WANG2024110478,
title = {Augmented skeleton sequences with hypergraph network for self-supervised group activity recognition},
journal = {Pattern Recognition},
volume = {152},
pages = {110478},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110478},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002292},
author = {Guoquan Wang and Mengyuan Liu and Hong Liu and Peini Guo and Ti Wang and Jingwen Guo and Ruijia Fan},
keywords = {Self-supervised learning, Skeleton-based group activity recognition, Multi-person augmentation strategies, Hypergraph learning},
abstract = {Contrastive learning has been widely applied to self-supervised skeleton-based single-person action recognition. However, directly employing single-person contrastive learning techniques for multi-person skeleton-based Group Activity Recognition (GAR) suffers from some challenges. Firstly, single-person data augmentation strategies struggle to capture complex collaborations between actors in multi-person scenarios, resulting in poor generalization. Secondly, real-world uncertainties in the number of people make single-person methods fail to capture changing high-order actor relations. Finally, single-person methods treat each actor with equal importance for recognition, struggling to distinguish imbalanced contributions between individual and group activities. To this end, the coarse-to-fine Augmented Hypergraph Network (AHNet) is proposed for effective self-supervised GAR. Specifically, we introduce multi-person augmentation strategies to enhance the generalization of the model under complex actor collaboration scenarios. Moreover, a knowledge-masked hypergraph network is employed to enhance the adaptability of the model to capture varied high-order actor relations. Finally, coarse-to-fine contrast among key actors is conducted to mitigate the imbalanced contributions between individual and group levels. Extensive experiments on multiple datasets demonstrate that our AHNet achieves substantial improvements over state-of-the-art methods with various backbone architectures. Our code is available at https://github.com/WGQ109/AHNet.}
}
@article{HAN2024110509,
title = {SIAM: A parameter-free, Spatial Intersection Attention Module},
journal = {Pattern Recognition},
volume = {153},
pages = {110509},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110509},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002607},
author = {Gaoge Han and Shaoli Huang and Fang Zhao and Jinglei Tang},
keywords = {3-D attention module, Parameter-free, Convolutional neural networks},
abstract = {Attention mechanisms have been shown to play a crucial role in enhancing visual perception tasks. However, in most existing approaches, channel and spatial attention maps are estimated separately without considering the varying importance of each other. This results in a coarse attention weight for objects of interest from a holistic 3-D perspective. To address this issue, we propose a novel Parameter-free Spatial Intersection Attention Module (SIAM), which estimates 3D attention maps with spatial intersection using a parameter-free way. Specifically, SIAM first generates two independent mean queries from two spatial axes and views input as keys. Then, by computing a dot product between these mean queries and keys, SIAM generates two cross-dimension (channel and spatial) attention maps from two spatial directions and combines them into 3-D attention maps. By doing so, the produced attention maps reason important areas with spatial intersection, which can capture location-aware information to facilitate difficult objects’ location in the images. We evaluate our method in image classification, object detection, and object segmentation tasks. Extensive experimental results consistently demonstrate our approach is superior to its counterparts.}
}
@article{NIE2024110470,
title = {ScopeViT: Scale-Aware Vision Transformer},
journal = {Pattern Recognition},
volume = {153},
pages = {110470},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110470},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002218},
author = {Xuesong Nie and Haoyuan Jin and Yunfeng Yan and Xi Chen and Zhihang Zhu and Donglian Qi},
keywords = {Vision transformer, Multi-scale features, Efficient attention mechanism},
abstract = {Multi-scale features are essential for various vision tasks, such as classification, detection, and segmentation. Although Vision Transformers (ViTs) show remarkable success in capturing global features within an image, how to leverage multi-scale features in Transformers is not well explored. This paper proposes a scale-aware vision Transformer called ScopeViT that efficiently captures multi-granularity representations. Two novel attention with lightweight computation are introduced: Multi-Scale Self-Attention (MSSA) and Global-Scale Dilated Attention (GSDA). MSSA embeds visual tokens with different receptive fields into distinct attention heads, allowing the model to perceive various scales across the network. GSDA enhances model understanding of the global context through token-dilation operation, which reduces the number of tokens involved in attention computations. This dual attention method enables ScopeViT to “see” various scales throughout the entire network and effectively learn inter-object relationships, reducing heavy quadratic computational complexity. Extensive experiments demonstrate that ScopeViT achieves competitive complexity/accuracy trade-offs compared to existing networks across a wide range of visual tasks. On the ImageNet-1K dataset, ScopeViT achieves a top-1 accuracy of 81.1%, using only 7.4M parameters and 2.0G FLOPs. Our approach outperforms Swin (ViT-based) by 1.9% accuracy while saving 42% of the parameters, outperforms MobileViTv2 (Hybrid-based) with a 0.7% accuracy gain while using 50% of the computations, and also beats ConvNeXt V2 (ConvNet-based) by 0.8% with fewer parameters.}
}
@article{KIM2024110441,
title = {Appearance debiased gaze estimation via stochastic subject-wise adversarial learning},
journal = {Pattern Recognition},
volume = {152},
pages = {110441},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110441},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001924},
author = {Suneung Kim and Woo-Jeoung Nam and Seong-Whan Lee},
keywords = {Appearance-based gaze estimation, Generalization, Adversarial loss, Stochastic subject selection, Meta-learning},
abstract = {Recently, appearance-based gaze estimation has been attracting attention in computer vision, and remarkable improvements have been achieved using various deep learning techniques. Despite such progress, most methods aim to infer gaze vectors from images directly, which causes overfitting to person-specific appearance factors. In this paper, we address these challenges and propose a novel framework: Stochastic subject-wise Adversarial gaZE learning (SAZE), which trains a network to generalize the appearance of subjects. We design a Face generalization Network (Fgen-Net) using a face-to-gaze encoder and face identity classifier and a proposed adversarial loss. The proposed loss generalizes face appearance factors so that the identity classifier inferences a uniform probability distribution. In addition, the Fgen-Net is trained by a learning mechanism that optimizes the network by reselecting a subset of subjects at every training step to avoid overfitting. Our experimental results verify the robustness of the method in that it yields state-of-the-art performance, achieving 3.89°and 4.42°on the MPIIFaceGaze and EyeDiap datasets, respectively. Furthermore, we demonstrate the positive generalization effect by conducting further experiments using face images involving different styles generated from the generative model.}
}
@article{SONG2024110506,
title = {Rebalancing network with knowledge stability for class incremental learning},
journal = {Pattern Recognition},
volume = {153},
pages = {110506},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110506},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002577},
author = {Jialun Song and Jian Chen and Lan Du},
keywords = {Class incremental learning, Catastrophic forgetting, Class imbalance, Proxy-based metric learning, Knowledge distillation},
abstract = {Class incremental learning (CIL) has been proposed to solve the problem of learning to classify new classes while maintaining the performance on old classes. A typical strategy is to update the old classification model with entire new class data and a few old exemplars, which face serious performance degradation on old classes. The main reasons come down to class imbalance between old and new classes along with catastrophic forgetting towards old classes. Most existing CIL methods have proposed solving the above two issues in classification space, ignoring their adverse effects of overlapping between old and new classes and confusion among old classes in feature space. In this paper, we propose a rebalancing network with knowledge stability (RNKS), aiming to adequately retain the model performance on old classes in CIL by solving class imbalance and catastrophic forgetting in feature and classification space simultaneously. In detail, the proposed RNKS mainly consists of multi-proxies rebalancing (MPR) and hybrid knowledge distillation (HKD). MPR, focusing on class imbalance, employs multi-proxies metric learning to decrease the feature overlapping between old and new classes, together with balanced data sampling to correct the skewed decision boundary. HKD, coping with catastrophic forgetting, encourages the updated model to reproduce identical feature topologies and predictions of old classes as the old model via feature relation-based and response-based distillations. Experiments on CIFAR-100 and ILSVRC datasets demonstrate the effectiveness of this work against the state-of-the-art approaches.}
}
@article{XU2024110534,
title = {PARDet: Dynamic point set alignment for rotated object detection},
journal = {Pattern Recognition},
volume = {153},
pages = {110534},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110534},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002851},
author = {Yihao Xu and Jifeng Shen and Ming Dai and Wankou Yang},
keywords = {Remote sensing, Rotated object detection, Fine-grained feature, Feature alignment},
abstract = {Due to the inherent mismatch between rotated objects and horizontal features, feature point misalignment has been a challenge in the task of Rotated Object Detection (ROD). Specifically, considering the pattern of convolution, foreground features are often mixed with background noise, which can confuse the model and affect the model from feature point alignment during the training phase. To mitigate this issue, previous methods concentrate on fixed positions derived from predicted boxes by additionally introducing a refinement stage. However, merely learning fixed position priors during training can result in suboptimal alignment and inefficiency during inference. This paper introduces a dynamic point alignment detector to concurrently address issues associated with feature misalignment and inference inefficiency. The method is made up of two components: the fine-grained points generator (FPG) captures key information, and the point alignment module (PAM) derives precise feature representations. Both modules empower the detector with the capability to dynamically perceive rotated objects, extracting more comprehensive and reasoned feature contents from the feature maps. In general, our method enables the model to independently identify and prioritize the valuable features during the training process. Subsequently, during the inference stage, the results can be directly predicted without additional alignment operations. The experimental results demonstrate that our method can achieve competitive and superior results with average precision (AP) values of 79.33%, 95.73%, and 63.37% on the DOTA, HRSC2016, and DIOR-R datasets, respectively. Codes will be publicly available at https://github.com/Xuyihaoby/PARDet.}
}
@article{DONG2024110482,
title = {Video-based face outline recognition},
journal = {Pattern Recognition},
volume = {152},
pages = {110482},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110482},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002334},
author = {Xingbo Dong and Jiewen Yang and Andrew Beng Jin Teoh and Dahai Yu and Xiaomeng Li and Zhe Jin},
keywords = {Face outline motion recognition, Video recognition, Recurrent transformer, Behavioural biometric, Soft biometric},
abstract = {We propose a novel approach for individual recognition that uses the motion traits from face outline-anonymised videos as the identity signatures; we call this type of signature a Temporal-iD. To extract a robust Temporal-iD, a highly lightweight transformer-based model, namely the Temporal-iD-ViT (TiDViT), is devised to capture and aggregate Temporal-iD features from videos. The TiDViT is equipped with a custom-designed multi-head temporal–spatial joint attention module that establishes interaction between the current frame input and the previous hidden state, thereby temporally aggregating the temporal features. The TiDViT processes the face video frame by frame instead of in a fixed batch-wise manner, which requires less computational memory. Moreover, the TiDViT can extract the temporal features of unconstrained face videos and considers ethical and privacy concerns. Extensive experimental results show that the proposed TiDViT model achieves a decent performance on this highly challenging task.}
}
@article{XIE2024110437,
title = {Distillation embedded absorbable pruning for fast object re-identification},
journal = {Pattern Recognition},
volume = {152},
pages = {110437},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110437},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001882},
author = {Yi Xie and Hanxiao Wu and Jianqing Zhu and Huanqiang Zeng},
keywords = {Object re-identification, Knowledge distillation, Network pruning, Re-parameterization},
abstract = {Combining knowledge distillation (KD) and network pruning (NP) shows promise in learning a light network to accelerate object re-identification. However, KD requires an untrained student network to establish more critical connections in early epochs, but NP demands a well-trained student network to avoid destroying critical connections. This presents a dilemma, potentially leading to a collapse of the student network and harming object Re-ID performance. For that, we propose a distillation embedded absorbable pruning (DEAP) method. We design a pruner-convolution-pruner (PCP) unit to resolve the dilemma by loading NP’s sparse regularization on extra untrained pruners. Additionally, we propose an asymmetric relation knowledge distillation method. It readily transfers feature representation knowledge and asymmetric pairwise similarity knowledge without using additional adaptation modules. Finally, we apply re-parameterization to absorb pruners of PCP units to simplify student networks. Experiments demonstrate the superiority of DEAP, such as on the VeRi-776 dataset, with ResNet-101 as a teacher, DEAP saves 73.24% of model parameters and 71.98% of floating-point operations without sacrificing accuracy.}
}
@article{WANG2024110518,
title = {Semi-supervised vanishing point detection with contrastive learning},
journal = {Pattern Recognition},
volume = {153},
pages = {110518},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110518},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002693},
author = {Yukun Wang and Shuo Gu and Yinbo Liu and Hui Kong},
keywords = {Vanishing point detection, Semi-supervised learning, Contrastive learning},
abstract = {Vanishing point (VP) detection in road images plays an important role in driving scenes for advanced driver assistance systems (ADAS) and autonomous vehicles. Because it is still a challenging problem to obtain expensive annotated datasets in deep-learning-based supervised training, in this paper, we propose a semi-supervised VP detection method in road images. Our proposed model first extracts high-resolution VP heatmaps of the road images by fusing multi-level and global–local contrastive learning. Then, we apply the π model as a semi-supervised learning framework and feed the whole global network with a small number of training samples to detect VPs. In the experiment, we used Kong’s dataset for the unstructured road test and the KITTI-VP dataset for the structured road test. Compared with the state-of-the-art methods, our model achieved high accuracy and robustness in road VP detection.}
}
@article{ZHANG2024110466,
title = {Seizure detection via deterministic learning feature extraction},
journal = {Pattern Recognition},
volume = {153},
pages = {110466},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110466},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002176},
author = {Zirui Zhang and Weiming Wu and Chen Sun and Cong Wang},
keywords = {Seizure detection, Feature extraction, Nonlinear system dynamics, Deterministic learning},
abstract = {Epileptic seizures have a significant impact on the well-being of a large number of individuals worldwide. Utilizing electroencephalographic (EEG) signals for automatic seizure detection proves to be a valuable solution. However, dealing with raw EEG signals is inherently complex, necessitating a preliminary step of feature extraction prior to detection. Traditional feature extraction methods often amalgamate various types of features for seizure detection, as each type typically captures specific properties. In contrast, this paper focuses on detecting seizures by analyzing the system dynamics. The proposed Deterministic Learning Feature Extraction (DLFE) method extracts a single type of nonlinear dynamical feature rooted in the EEG system dynamics. DLFE employs deterministic learning to discern the inherent system dynamics of the EEG under both seizure and normal conditions. Through the feature extraction process, the infinite-dimensional system dynamics are transformed into feature vectors, exhibiting distinct distributions in seizure and normal states. This disparity can be effectively utilized for classification using standard classifiers. The performance of the proposed seizure detection method was assessed using the CHB-MIT and Bonn datasets. The average classification accuracy was found to be 98.63% with a specificity of 99.19% and a sensitivity of 98.06% on CHB-MIT dataset. Compared with the latest similar methods, the accuracy, specificity and sensitivity are improved by 0.31%, 0.21% and 0.05% respectively. Moreover, the performance was achieved with the short-time interval EEG signals within a few channels. The average classification accuracy was found to be 99.90% with a 0.22% improvement on Bonn dataset, which indicates the good generalization performance.}
}
@article{BARUFALDI2024110494,
title = {Assessment of volumetric dense tissue segmentation in tomosynthesis using deep virtual clinical trials},
journal = {Pattern Recognition},
volume = {153},
pages = {110494},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110494},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002450},
author = {B. Barufaldi and J.V. Gomes and T.M. {Silva Filho} and T.G. {do Rêgo} and Y. Malheiros and T.L. Vent and A. Gastounioti and A.D.A. Maidment},
keywords = {Anthropomorphic breast phantom, Virtual clinical trials, Machine learning, Digital breast tomosynthesis},
abstract = {The adoption of artificial intelligence (AI) in medical imaging requires careful evaluation of machine-learning algorithms. We propose the use of a “deep virtual clinical trial” (DeepVCT) method to effectively evaluate the performance of AI algorithms. In this paper, DeepVCTs have been proposed to elucidate limitations of AI applications and predictions of clinical outcomes, avoiding biases in study designs. The DeepVCT method was used to evaluate the performance of nnU-Net models in assessing volumetric breast density (VBD) from digital breast tomosynthesis (DBT) images. In total, 2010 anatomical breast models were simulated. Projections were simulated using the acquisition geometry of a clinical DBT system. The projections were reconstructed using 0.1, 0.2, and 0.5 mm plane spacing. nnU-Net models were developed using the center-most planes of the reconstructions with the respective ground-truth. The results show that the accuracy of the nnU-Net improves significantly with DBT images reconstructed with 0.1 mm plane spacing (78.4×205.3×40.1 mm3). The segmentations resulted in Dice values up to 0.84 with area under the receiver operating characteristic curve of 0.92. The optimization of plane spacing for VBD assessment was used as an exemplar of a DeepVCT application, allowing better interpretation of the input parameters and outcomes of the nnU-Net. Thus, DeepVCTs can provide evidence to predict the efficacy of AI algorithms using large-scale simulation-based data.}
}
@article{ZHU2024110532,
title = {Advancements in point cloud data augmentation for deep learning: A survey},
journal = {Pattern Recognition},
volume = {153},
pages = {110532},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110532},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002838},
author = {Qinfeng Zhu and Lei Fan and Ningxin Weng},
keywords = {Point cloud, Augmentation, Deep learning, Detection, Segmentation, Classification},
abstract = {Deep learning (DL) has become one of the mainstream and effective methods for point cloud analysis tasks such as detection, segmentation and classification. To reduce overfitting during training DL models and improve model performance especially when the amount and/or diversity of training data are limited, augmentation is often crucial. Although various point cloud data augmentation methods have been widely used in different point cloud processing tasks, there are currently no published systematic surveys or reviews of these methods. Therefore, this article surveys these methods, categorizing them into a taxonomy framework that comprises basic and specialized point cloud data augmentation methods. Through a comprehensive evaluation of these augmentation methods, this article identifies their potentials and limitations, serving as a useful reference for choosing appropriate augmentation methods. In addition, potential directions for future research are recommended. This survey contributes to providing a holistic overview of the current state of point cloud data augmentation, promoting its wider application and development.}
}
@article{ZHANG2024110519,
title = {PlaneAC: Line-guided planar 3D reconstruction based on self-attention and convolution hybrid model},
journal = {Pattern Recognition},
volume = {153},
pages = {110519},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110519},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400270X},
author = {Jiahui Zhang and Jinfu Yang and Fuji Fu and Jiaqi Ma},
keywords = {Planar 3D reconstruction, Self-attention, Convolution neural network, Transferable knowledge, Line segments},
abstract = {Planar 3D reconstruction aims to simultaneously extract plane instances and reconstruct the local 3D model through the estimated plane parameters. Existing methods achieve promising results either through self-attention or convolution neural network (CNNs), but usually ignore the complementary properties of them. In this paper, we propose a line-guided planar 3D reconstruction method PlaneAC, which leverages the advantage of self-attention and CNNs to capture long-range dependencies and alleviate the computational burden. In addition, explicit connection between two adjacent attention layers is built for better leveraging the transferable knowledge and facilitating the information flow between tokenized feature from different layers. Therefore, the subsequent attention layer can directly interact with previous results. Finally, a line segment filtering method is presented to remove irrelevant guiding information from indistinctive line segments extracted from the image. Extensive experiments on ScanNet and NYUv2 public datasets demonstrate the preferable performance of our proposed method, and the results show that PlaneAC achieves a better trade-off between accuracy and computation cost compared with other state-of-the-art methods.}
}
@article{HASSAN2024110493,
title = {Incremental convolutional transformer for baggage threat detection},
journal = {Pattern Recognition},
volume = {153},
pages = {110493},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110493},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002449},
author = {Taimur Hassan and Bilal Hassan and Muhammad Owais and Divya Velayudhan and Jorge Dias and Mohammed Ghazal and Naoufel Werghi},
keywords = {Threat detection, Transformers, Knowledge distillation, Incremental learning, Incremental instance segmentation, X-ray imagery, Catastrophic forgetting},
abstract = {Detecting cluttered and overlapping contraband items from baggage scans is one of the most challenging tasks, even for human experts. Recently, considerable literature has grown up around the theme of deep learning-based X-ray screening for localizing contraband data. However, the existing threat detection systems are still vulnerable to high occlusion, clutter, and concealment. Furthermore, they require exhaustive training routines on large-scale and well-annotated data in order to produce accurate results. To overcome the above-mentioned limitations, this paper presents a novel convolutional transformer system that recognizes different overlapping instances of prohibited objects in complex baggage X-ray scans via a distillation-driven incremental instance segmentation scheme. Furthermore, unlike its competitors, the proposed framework allows an incremental integration of new item instances while avoiding costly training routines. In addition to this, the proposed framework also outperforms state-of-the-art approaches by achieving a mean average precision score of 0.7896, 0.5974, and 0.7569 on publicly available GDXray, SIXray, and OPIXray datasets for detecting concealed and cluttered baggage threats.}
}
@article{WU2024110440,
title = {Hyper-feature aggregation and relaxed distillation for class incremental learning},
journal = {Pattern Recognition},
volume = {152},
pages = {110440},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110440},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001912},
author = {Ran Wu and Huanyu Liu and Zongcheng Yue and Jun-Bao Li and Chiu-Wing Sham},
keywords = {Class incremental learning, Relaxed knowledge distillation, Hyper-feature aggregation},
abstract = {Although neural networks have been used extensively in pattern recognition scenarios, the pre-acquisition of datasets is still challenging. In most pattern recognition areas, preparing a training dataset that covers all data domains is difficult. Incremental learning was proposed to update neural networks in an online manner, but the catastrophic forgetting issue still needs to be studied. Class-incremental learning is one of the most challenging incremental learning contexts; it trains a unified model to classify all incrementally arriving classes learned thus far equally. Prior studies on class-incremental learning favor model stability over plasticity to realize old knowledge reservation and prevent catastrophic forgetting. Consequently, the model’s plasticity is omitted, leading to difficult generalization on new data. We propose a novel distillation-based method named Hyper-feature Aggregation and Relaxed Distillation (HARD) to realize balanced optimization of old and new knowledge. The aggregation of features is proposed to capture the global semantics while maintaining the diversity of the feature distribution after promoting representations of exemplars to higher dimensions. The proposed algorithm also introduces a relaxed restriction in the hyper-feature space to conditions the hyper-feature space through a normalized comparison of the relation matrices. Following generalization on more classes, the model is encouraged to rebuild the feature distribution when meeting new classes and to fine-tune the feature space to realize more distinct interclass boundaries. Extensive experiments were conducted on two benchmark datasets, and consistent improvements under diverse experimental settings demonstrated the effectiveness of the proposed approach.}
}
@article{WANG2024110469,
title = {Generalization Memorization Machine with Zero Empirical Risk for Classification},
journal = {Pattern Recognition},
volume = {152},
pages = {110469},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110469},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002206},
author = {Zhen Wang and Lan Bai and Yuanhai Shao},
keywords = {Classification, Support vector machine, Generalization memorization mechanism, Memory machine, Generalization memorization kernel},
abstract = {Classifying the training data correctly without over-fitting is one of the goals in machine learning. In this paper, we propose a general Generalization Memorization Machine (GMM) to obtain zero empirical risk with better generalization. The widely applied loss-based learning models can be extended by the GMM to improve their memorization and generalization abilities. Specifically, we propose two new models based on the GMM, called Hard Generalization Memorization Machine (HGMM) and Soft Generalization Memorization Machine (SGMM). Both HGMM and SGMM obtain zero empirical risks with well generalization, and the SGMM further improves the capacity and applicability of HGMM. The optimization problems in the proposed models are quadratic programming problems and could be solved efficiently. Additionally, the recently proposed generalization memorization kernel and the corresponding support vector machine are the special cases of our SGMM. Experimental results demonstrate the effectiveness of the proposed HGMM and SGMM both on memorization and generalization.}
}
@article{SHAO2024110448,
title = {Contrastive domain-adaptive graph selective self-training network for cross-network edge classification},
journal = {Pattern Recognition},
volume = {152},
pages = {110448},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110448},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001997},
author = {Mengqiu Shao and Peng Xue and Xi Zhou and Xiao Shen},
keywords = {Graph neural network, Cross-network edge classification, Graph Domain Adaptation, Pseudo-labeling},
abstract = {The performance of graph Neural Network (GNNs) can degrade significantly when trained on the graphs with noisy edges connecting nodes from different classes. To mitigate the negative effect of noisy edges, previous studies have largely focused on predicting the label agreement between node pairs within a single network. So far, predicting noisy edges across different networks remains largely underexplored. To bridge this gap, our work studies a novel problem of cross-network homophilous and heterophilous edge classification (CNHHEC), aiming to predict the label agreement of edges in an unlabeled target network by transferring the knowledge from a labeled source network. A novel Contrastive Domain-adaptive Graph Self-training Network (CDGSN) is proposed. Firstly, CDGSN learns node and edge embeddings end-to-end by a GNN encoder with adaptive edge weights during neighborhood aggregation. Secondly, to facilitate knowledge transfer across networks, CDGSN employs an adversarial domain adaptation module to align edge embeddings across networks, and also designs a novel contrastive domain adaptation module to conduct class-aware cross-network alignment of node embeddings. As a result, the intra-class domain divergence can be mitigated while the inter-class domain discrepancy can be enlarged to yield network-invariant and label-discriminative node and edge embeddings. Moreover, CDGSN designs a selective positive and negative pseudo-labeling strategy to assign positive (negative) pseudo-labels to the target nodes with extremely high (low) prediction confidence of belonging to each specific class. Such pseudo-labeled target nodes would be employed to iteratively re-train the model in a self-training manner, so as to obtain more reliable target pseudo-labels progressively to guide class-aware domain alignment. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed CDGSN on the CNHHEC problem. The performance of CDGSN is robust against various types of edge embeddings. When adopting three operators to construct edge embeddings, the proposed CDGSN can improve the state-of-the-art method for CNHHEC by an average of 0.5 %, 2.8 %, and 10.8 % in terms of AUC, and 1.3 %, 3.0 %, and 6.3 % in terms of AP, respectively.}
}
@article{LI2024110516,
title = {Irregular text block recognition via decoupling visual, linguistic, and positional information},
journal = {Pattern Recognition},
volume = {153},
pages = {110516},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110516},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400267X},
author = {Ziyan Li and Lianwen Jin and Chengquan Zhang and Jiaxin Zhang and Zecheng Xie and Pengyuan Lyu and Kun Yao},
keywords = {Scene text recognition, Irregular text recognition, Text block recognition, Character spotting, Linkage reasoning},
abstract = {Scene text recognition has made great progress in regular formats, and the recent research has focused on irregular text recognition. In this work, we investigate a new challenge problem of recognizing a text block instance with irregular arrangement of characters, which is referred to as irregular text block recognition (ITBR). This problem is prevalent in daily scenarios, especially with the increasing use of rich text designs in signboards, logos, posters, and other mediums. The primary challenge arises from the weakened position clues and the highly complex reading order, which can often only be deciphered by a heavy reliance on understanding the intrinsic linguistic information. Hence, conventional recognition methods that employ inflexible character grouping rules, coupled with positional information, or constrained by vocabulary reliance, may struggle with the ITBR problem. To this end, we propose a progressive layout reasoning network (PLRN) to recognize the irregular text block by decoupling visual, linguistic, and positional information. PLRN comprises a character spotting module that recognizes the character set based solely on visual features with a new TopK-rank decoding mechanism, and a linkage reasoning module to interpret the character relationships within this set with a progressive refinement strategy. The linkages are initially reasoned by linguistic information and then progressively refined through the incorporation of proximity and tendency information, allowing for explicit decoupling and improved reasoning accuracy. To assess the effectiveness of the proposed method, we construct a new dataset called TextBlock600. This dataset consists of 600 images of irregular text blocks, each with complete sequence annotations. Experimental results demonstrate that PLRN shows promising performance in ITBR, opening up possibilities for further research in this field. Code and datasets will be available at https://github.com/eezyli/PLRN.}
}
@article{LI2024110436,
title = {Temporal pattern mining for knowledge discovery in the early prediction of septic shock},
journal = {Pattern Recognition},
volume = {151},
pages = {110436},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110436},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001870},
author = {Ruoting Li and Joseph K. Agor and Osman Y. Özaltın},
keywords = {Temporal pattern mining, Feature selection, Electronic health records, Knowledge discovery, Sepsis},
abstract = {Temporal pattern mining can be employed to detect patterns and trends in a patient's health status as it evolves over time. However, these methods often produce an overwhelming number of patterns, impeding knowledge discovery and practical implementation in acute care settings. To address this, we propose a framework that focuses on identifying a concise set of relevant temporal patterns and static variables from electronic health records for the early prediction of septic shock. Sepsis is caused by an adverse immune response to infection that triggers widespread inflammation throughout the body, which can progress to septic shock and ultimately result in death if not treated promptly. The analysis of health state patterns in sepsis patients over time offers the potential to predict septic shock prior to its onset, enabling proactive healthcare interventions. Our framework incorporates a temporal pattern mining method and four feature selection techniques. We discover that selecting features based on a model-based wrapper approach yields the highest prediction performance among these techniques. On the other hand, the use of information value identifies more multi-state patterns with abnormal health states, providing healthcare providers with valuable indicators of patient deterioration.}
}
@article{LI2024110505,
title = {A ranking-based problem transformation method for weakly supervised multi-label learning},
journal = {Pattern Recognition},
volume = {153},
pages = {110505},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110505},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002565},
author = {Jiaxuan Li and Xiaoyan Zhu and Weichu Zhang and Jiayin Wang},
keywords = {Multi-label learning, Problem transformation, Pairwise label correlation, Ensemble learning, Multi-label learning with missing labels, Partial multi-label learning},
abstract = {Problem transformation is a simple yet effective framework for multi-label learning, where the original multi-label problem can be transformed into a series of single-label subproblems. However, the existing problem transformation methods have difficulties in handling label defect issues in real applications, e.g. multi-label learning with missing labels, partial multi-label learning and noisy multi-label learning. To deal with these issues, we propose a novel problem transformation method named EPR (i.e., Ensemble of Pairwise Ranking learners) applicable to various multi-label tasks. In EPR, the weakly supervised multi-label problem is converted into an ensemble of supervised single-label patterns due to pairwise label ranking, which successfully enhances label correlation exploration and improves the utilization of instances with defect labels. Moreover, an ensemble pruning mechanism is presented to heuristically balance the model performance and efficiency. Extensive experiments demonstrate the effectiveness of EPR against state-of-the-art algorithms in diverse multi-label learning scenarios.}
}
@article{ZHONG2024110453,
title = {Graph embedding orthogonal decomposition: A synchronous feature selection technique based on collaborative particle swarm optimization},
journal = {Pattern Recognition},
volume = {152},
pages = {110453},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110453},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002048},
author = {Jingyu Zhong and Ronghua Shang and Songhua Xu and Yangyang Li},
keywords = {Clustering label orthogonal decomposition, Graph-embedded, Local structure preserving, Particle swarm optimization},
abstract = {In unsupervised feature selection, the clustering label matrix has the ability to distinguish between projection clusters. However, the latent geometric structure of the clustering labels is often ignored. In addition, the optimal sub-feature selection performance of feature selection techniques relies greatly on the choice of balanced parameters, and the selection range of most technical parameters is limited and fixed. To solve the above-mentioned problems, this paper proposes a synchronous feature selection technique based on graph-embedded cluster label orthogonal decomposition and collaborative particle swarm optimization (GOD-cPSO). First, GOD-cPSO extends the feature selection framework of clustering label orthogonal decomposition by graph embedding to retain the latent geometric structure of clustering labels, thus maintaining the correlation between clustered sample labels. Then, the l2,1-2-norm with strong global convergence is extended to the graph embedding clustering label orthogonal decomposition framework. By imposing this non-convex constraint, GOD-cPSO can achieve low-dimensional sparse and low-redundant sub-features. In addition, the local structure preserving of low-dimensional manifolds is integrated into the graph-embedded clustering label orthogonal decomposition framework to obtain good cluster separation and effectively maintain the latent local structure of the data. Finally, to ensure the adaptive parameter selection over a large range, GOD-cPSO synchronously guides the graph-embedding clustering labeling orthogonal decomposition framework for feature selection through collaborative particle swarm optimization. GOD-cPSO has synchronous parameter optimization and feature selection and picks parameters in a larger range. Comprehensive numerical experiments are performed on nine datasets to test the validity of the GOD-cPSO. The experimental results demonstrate that the sub-features selected by the GOD-cPSO have stronger discriminative power and are superior to other techniques in the clustering assignments.}
}
@article{KE2024110481,
title = {Text-based person search via cross-modal alignment learning},
journal = {Pattern Recognition},
volume = {152},
pages = {110481},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110481},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002322},
author = {Xiao Ke and Hao Liu and Peirong Xu and Xinru Lin and Wenzhong Guo},
keywords = {Person re-identification, Image–text retrieval, Cross-modality, CNN},
abstract = {Text-based person search aims to use text descriptions to search for corresponding person images. However, due to the obvious pattern differences in image and text modalities, it is still a challenging problem to align the two modalities. Most existing approaches only consider semantic alignment within a global context or partial parts, lacking consideration of how to match image and text in terms of differences in model information. Therefore, in this paper, we propose an efficient Modality-Aligned Person Search network (MAPS) to address this problem. First, we suppress image-specific information by image feature style normalization to achieve modality knowledge alignment and reduce information differences between text and image. Second, we design a multi-granularity modal feature fusion and optimization method to enrich the modal features. To address the problem of useless and redundant information in the multi-granularity fused features, we propose a Multi-granularity Feature Self-optimization Module (MFSM) to adaptively adjust the corresponding contributions of different granularities in the fused features of the two modalities. Finally, to address the problem of information inconsistency in the training and inference stages, we propose a Cross-instance Feature Alignment (CFA) to help the network enhance category-level generalization ability and improve retrieval performance. Extensive experiments demonstrate that our MAPS achieves state-of-the-art performance on all text-based person search datasets, and significantly outperforms other existing methods.}
}
@article{FU2024110517,
title = {Weakly privileged learning with knowledge extraction},
journal = {Pattern Recognition},
volume = {153},
pages = {110517},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110517},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002681},
author = {Saiji Fu and Tianyi Dong and Zhaoxin Wang and Yingjie Tian},
keywords = {Learning using privileged information (LUPI), Weakly privileged learning, Knowledge extraction, Privileged information, Support vector machine, Classification},
abstract = {Learning using privileged information (LUPI) has shown promise in improving supervised learning by embedding additional knowledge. However, its reliance on the assumption of readily available privileged information may not hold true in practical scenarios due to limitations in access or confidentiality. To address these challenges, this paper presents a novel weakly privileged learning (WPL) framework, integrating knowledge extraction methods within the LUPI context. An effective strategy is proposed to implement the WPL framework, where knowledge extraction techniques generate a weight matrix as weak privileged information. Extensive experiments employing various existing knowledge extraction techniques demonstrate that the proposed WPL outperforms traditional supervised learning and approaches the performance of standard privileged learning where privileged information is given in advance. This research establishes WPL as a promising learning paradigm, addressing limitations in privileged information availability and advancing the field of machine learning in practical settings.}
}
@article{XU2024110465,
title = {IBVC: Interpolation-driven B-frame video compression},
journal = {Pattern Recognition},
volume = {153},
pages = {110465},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110465},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002164},
author = {Chenming Xu and Meiqin Liu and Chao Yao and Weisi Lin and Yao Zhao},
keywords = {Learned video compression, Video frame interpolation, Artifact reduction, Bi-directional motion compensation},
abstract = {Learned B-frame video compression aims to adopt bi-directional motion estimation and motion compensation (MEMC) coding for middle frame reconstruction. However, previous learned approaches often directly extend neural P-frame codecs to B-frame relying on bi-directional optical-flow estimation or video frame interpolation. They suffer from inaccurate quantized motions and inefficient motion compensation. To address these issues, we propose a simple yet effective structure called Interpolation-driven B-frame Video Compression (IBVC). Our approach only involves two major operations: video frame interpolation and artifact reduction compression. IBVC introduces a bit-rate free MEMC based on interpolation, which avoids optical-flow quantization and additional compression distortions. Later, to reduce duplicate bit-rate consumption and focus on unaligned artifacts, a residual guided masking encoder is deployed to adaptively select the meaningful contexts with interpolated multi-scale dependencies. In addition, a conditional spatio-temporal decoder is proposed to eliminate location errors and artifacts instead of using MEMC coding in other methods. The experimental results on B-frame coding demonstrate that IBVC has significant improvements compared to the relevant state-of-the-art methods. Meanwhile, our approach can save bit rates compared with the random access (RA) configuration of H.266 (VTM). The code will be available at https://github.com/ruhig6/IBVC.}
}