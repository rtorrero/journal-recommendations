@article{HIRCHOUA2023109730,
title = {β-Random Walk: Collaborative sampling and weighting mechanisms based on a single parameter for node embeddings},
journal = {Pattern Recognition},
volume = {142},
pages = {109730},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109730},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004284},
author = {Badr Hirchoua and Saloua {El Motaki}},
keywords = {Node embedding, Random walk, Knowledge representation, Link prediction, Knowledge completion, Node behavior},
abstract = {Graph embedding transforms a graph into vector representations to facilitate subsequent graph-analytic tasks. Existing graph embedding methods ignore efficient node sampling and intelligent node weighting, leading to a weak node representation. This paper introduces the β-random walk model with two main contributions. Firstly, the traditional random walk sampling reveals instability. Thus, we associate a parameter β with each node to balance and stabilize the sampling process, producing high-efficient trajectories. Secondly, we design a weighting mechanism that incorporates these trajectories to generate accurate representations. The designed mechanism models the behavior of each node contextually at each episode, considering the current state and the previous weights to produce the next episode’s weights. The parameter β optimizes the node weights by simulating multiple high-order proximity walks from each node. This approach provides summarized insights about each node’s behavior and its neighbors’ context, which enables a consistent discovery of prominent paths variation in the graph. Experimental results demonstrate that the β-random walk outperforms the state-of-the-art baselines in handling small and large graphs.}
}
@article{MAJUMDAR2023109689,
title = {Uniform misclassification loss for unbiased model prediction},
journal = {Pattern Recognition},
volume = {144},
pages = {109689},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109689},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003874},
author = {Puspita Majumdar and Mayank Vatsa and Richa Singh},
keywords = {Bias, Fairness, Facial attribute prediction, Deep learning, Unbiased predictions},
abstract = {Deep learning algorithms have achieved tremendous success over the past few years. However, the biased behavior of deep models, where the models favor/disfavor certain demographic subgroups, is a major concern in the deep learning community. Several adverse consequences of biased predictions have been observed in the past. One solution to alleviate the problem is to train deep models for fair outcomes. Therefore, in this research, we propose a novel loss function, termed as Uniform Misclassification Loss (UML) to train deep models for unbiased outcomes. The proposed UML function penalizes the model for the worst-performing subgroup for mitigating bias and enhancing the overall model performance. The proposed loss function is also effective while training with imbalanced data as well. Further, a metric, Joint Performance Disparity Measure (JPD) is introduced to jointly measure the overall model performance and the bias in model prediction. Multiple experiments have been performed on four publicly available datasets for facial attribute prediction and comparisons are performed with existing bias mitigation algorithms. Experimental results are reported using performance and bias evaluation metrics. The proposed loss function outperforms existing bias mitigation algorithms that showcase its effectiveness in obtaining unbiased outcomes and improved performance.}
}
@article{WANG2023109634,
title = {Hypercomplex context guided interaction modeling for scene graph generation},
journal = {Pattern Recognition},
volume = {141},
pages = {109634},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109634},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003357},
author = {Zheng Wang and Xing Xu and Yadan Luo and Guoqing Wang and Yang Yang},
keywords = {Scene graph generation, Context guidence, Interaction modeling, Hypercomplex embedding},
abstract = {Intuitively, humans can consciously and subjectively attend to the interactions between objects, and thus infer reasonable visual relations. However, mainstream approaches of Scene Graph Generation (SGG) strive to alleviate the long-tailed distribution problem with various complicated re-weighting strategies, where a simple concatenation of the refined object features is treated as the final representation of visual relations. In spite of their remarkable progress, such an operation overlooks the importance of interaction on relation recognition. To tackle the problem, this work devises a hyperComplex- Context guided Interaction Modeling (CCIM for short) plug-in, which can be successfully assimilated by the existing methods for performance improvement. Specifically, we first extract the contextual relation feature determined by the constraint relation≈union(head,tail)−headobject−tailobject. Then, we encode the features of relations and objects into hypercomplex space, with three imaginary components, to learn more expressive representations for SGG. Next, guided by the context, we can capture the interaction between a head or tail object and their relation through the Hamilton product. We further reinforce the interaction between enhanced hypercomplex-valued representations of the two entities with Quaternion inner product. At last, the concatenation of all components from the learned hypercomplex feature is adopted as our final relation representation. Extensive experiments on the popular benchmark Visual Genome in various existing approaches demonstrate the effectiveness and generalization of our proposed model-agnostic method under comprehensive evaluation metrics.}
}
@article{ZHENG2023109662,
title = {Continuous cross-modal hashing},
journal = {Pattern Recognition},
volume = {142},
pages = {109662},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109662},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003631},
author = {Hao Zheng and Jinbao Wang and Xiantong Zhen and Jingkuan Song and Feng Zheng and Ke Lu and Guo-Jun Qi},
keywords = {Cross-modal hashing, Fast retrieval, Multiple modalities, Continual learning},
abstract = {Generally, multimodal data with new classes arrive continuously in the real world. While advanced cross-modal hashing (CMH) focuses primarily on batch-based data with previously observed classes (ASCs), it disregards the effect of newly arriving classes (ANCs) on hash-code conflicts. In addition, class-level continuous hashing scenarios do not suit themselves well with the generic CMH configuration. To solve the aforementioned issues, we propose a novel framework, called CT-CMH, for the new task of continuous cross-modal hashing. For dealing with ANCs, CMH models require the ability of continuous learning, i.e. they can preserve the knowledge of previously observed data and, more crucially, they can be adapted to unseen data with ANCs. Specifically, we introduce the adaptive weight importance updating (AWIU) mechanism to alleviate the catastrophic forgetting problem of CMH and a new hash-code divergence (HCD) method to eliminate hash-code conflicts between ASCs and ANCs. When CT-CMH is equipped with both AWIU and HCD, it can consistently achieve high retrieval performance. The experiment results and visualization analyses validate the effectiveness of our approach. To the best of our knowledge, we are the first to introduce and implement the task of CCMH for ANCs.}
}
@article{ZHANG2023109701,
title = {Features kept generative adversarial network data augmentation strategy for hyperspectral image classification},
journal = {Pattern Recognition},
volume = {142},
pages = {109701},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109701},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003990},
author = {Mingyang Zhang and Zhaoyang Wang and Xiangyu Wang and Maoguo Gong and Yue Wu and Hao Li},
keywords = {Hyperspectral images (HSIs), Deep learning, Generative adversarial network (GAN), Data augmentation},
abstract = {In recent years, significant breakthroughs have been achieved in hyperspectral image (HSI) processing using deep learning techniques, including classification, object detection, and anomaly detection. However, the practical application of deep learning in HSI processing is limited by challenges such as small-sample size and sample imbalance issues. To mitigate these limitations, we propose a novel data augmentation strategy called Feature-Preserving Generative Adversarial Network Data Augmentation (FPGANDA). What sets our data augmentation strategy apart from existing generative model-based approaches is that we preserve the main spectral bands of HSI data using a newly designed band selection method. Additionally, our proposed generative model generates synthetic spectral bands, which are combined with the real spectral bands using a mixture strategy to create augmented data. This approach ensures that the augmented data retain the main features of the original data while also incorporating diverse features from the generated data. We evaluate our method on three different HSI datasets, comparing it with state-of-the-art techniques. Experimental results demonstrate that our proposed method significantly improves classification performance in most scenes and exhibits remarkable compatibility.}
}
@article{BLANCOMALLO2023109646,
title = {Do all roads lead to Rome? Studying distance measures in the context of machine learning},
journal = {Pattern Recognition},
volume = {141},
pages = {109646},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109646},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003473},
author = {Eva Blanco-Mallo and Laura Morán-Fernández and Beatriz Remeseiro and Verónica Bolón-Canedo},
keywords = {Distance measures, Similarity measures, Classification, Clustering, Machine learning},
abstract = {Many machine learning and data mining tasks are based on distance measures, so a large amount of literature addresses this aspect somehow. Due to the broad scope of the topic, this paper aims to provide an overview of the use of these measures in the most common machine learning problems, pointing out those aspects to consider to choose the most appropriate measure for a particular task. For this purpose, the most recent works addressing the subject were reviewed and seven of the most commonly used measures were analyzed, investigating in detail their main properties and applications. Different experiments were carried out to study their relationships and compare their performance. The degradation of the results in the presence of noise was also considered, as well as the execution time required by each measure.}
}
@article{COMIC2023109693,
title = {Discrete analytical objects in the body-centered cubic grid},
journal = {Pattern Recognition},
volume = {142},
pages = {109693},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109693},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003916},
author = {Lidija Čomić and Gaëlle Largeteau-Skapin and Rita Zrour and Ranita Biswas and Eric Andres},
keywords = {Discrete geometry, BCC Grid, Discrete analytical plane, Discrete analytical sphere, Discrete analytical line, 3D Coordinate system},
abstract = {We propose a characterization of discrete analytical spheres, planes and lines in the body-centered cubic (BCC) grid, both in the Cartesian and in the recently proposed alternative compact coordinate system, in which each integer triplet addresses some voxel in the grid. We define spheres and planes through double Diophantine inequalities and investigate their relevant topological features, such as functionality or the interrelation between the thickness of the objects and their connectivity and separation properties. We define lines as the intersection of planes. The number of the planes (up to six) is equal to the number of the pairs of faces of a BCC voxel that are parallel to the line.}
}
@article{LYU2023109661,
title = {Process-Oriented heterogeneous graph learning in GNN-Based ICS anomalous pattern recognition},
journal = {Pattern Recognition},
volume = {141},
pages = {109661},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109661},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300362X},
author = {Shuaiyi L(y)u and Kai Wang and Liren Zhang and Bailing Wang},
keywords = {Fine-Grained anomaly recognition, Process-Oriented associativity, Heterogeneous graph learning, Industrial control systems},
abstract = {Over the past few years, massive penetrations targeting an Industrial Control System (ICS) network intend to compromise its core industrial processes. So far, numerous advanced methods have been proposed to detect anomalous patterns in the numeric data streams with respect to the heterogeneous field devices involved in the industrial processes. These methods, despite reporting decent results, usually conduct system-wise detection instead of fine-grained anomalous pattern recognition at the device level. Furthermore, lacking explicit consideration of the exclusive process-related features with respect to each differentiated device, the fitness of their application in specified industrial processes is undermined. To tackle these issues, a GNN-based Attributed Heterogeneous Graph Analyzer (the AHGA) is designed to perform device-wise anomalous pattern detection via in-depth process-oriented associativity learning. The AHGA’s framework is constructed with four building blocks: a graph processor, a feature analyzer, a link inference decoder, and an anomaly detector. Its performance is assessed and compared against multiple link inference and anomaly detection baselines over 2 popular ICS datasets (SWaT and WADI). Comparative results demonstrate the AHGA’s reliability in capturing sophisticated process-oriented relations among heterogeneous devices as well as its effectiveness in boosting the performance of anomalous pattern recognition at device-level granularity.}
}
@article{WANG2023109718,
title = {Simultaneous local clustering and unsupervised feature selection via strong space constraint},
journal = {Pattern Recognition},
volume = {142},
pages = {109718},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109718},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004168},
author = {Zheng Wang and Qi Li and Haifeng Zhao and Feiping Nie},
keywords = {Unsupervised feature selection, -Norm constraint optimization, Local structure learning},
abstract = {Clustering is a fashion method applied in machine learning tasks. However, high dimensional data brings many obstacles for clustering approaches. To address such a problem, the unsupervised feature selection (UFS) method can be incorporated into clustering to reduce dimensionality. In general, most of the UFS methods adopt ℓ2,1-norm for subspace sparsity learning. However, its sparsity highly relies on the setting of trade-off parameter, which may lead to instability of ranking results and the difficulty in obtaining the optimal solution of projection matrix. In this paper, we propose to directly learn an absolutely row-sparsity subspace via the ℓ2,0-norm constraint, called Sparse constraint and Local learning for Unsupervised Feature Selection (SLUFS). It is an ideal sparse subspace constraint which can overcome the drawbacks of the ℓ2,1-norm. However, optimizing the ℓ2,0-norm constraint is an NP-hard problem, and at present, only some approximate solutions can be given, but the convergence can not be guaranteed. To tackle this challenge, we design a novel alternative iterative algorithm to directly optimize the ℓ2,0-norm based model. Most importantly, our strategy can obtain a closed-form solution with strict convergence guarantee. Comprehensive experiments are conducted on several real-world datasets to evaluate the performance of SLUFS with comparison to several related state-of-the-art methods.}
}
@article{ZHU2023109674,
title = {GARNet: Global-aware multi-view 3D reconstruction network and the cost-performance tradeoff},
journal = {Pattern Recognition},
volume = {142},
pages = {109674},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109674},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003758},
author = {Zhenwei Zhu and Liying Yang and Xuxin Lin and Lin Yang and Yanyan Liang},
keywords = {3D Reconstruction, Multi-view input, Voxel, Cost-performance tradeoff, Deep learning},
abstract = {Deep learning technology has made great progress in multi-view 3D reconstruction tasks. At present, the mainstream solutions adopt different ways to fusion the features from several views. Among them, attention-based aggregation function performs relatively well and stably, however, it still has an obvious shortcoming the strong independence of each view during predicting the weights for merging leads to a lack of adaption of the global state. In this paper, we propose a global-aware attention-based fusion approach that builds the correlation between each branch and the global feature to provide a comprehensive foundation for weights inference. On the basis of this, we design a complete reconstruction algorithm. Experiments on ShapeNet verify that our method outperforms existing SOTA methods. Furthermore, we propose a view-reduction method based on maximizing diversity and discuss the cost-performance tradeoff of our model to achieve a better performance when facing a heavy input amount and limited computational cost.}
}
@article{XU2023109650,
title = {Tensor train factorization under noisy and incomplete data with automatic rank estimation},
journal = {Pattern Recognition},
volume = {141},
pages = {109650},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109650},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003515},
author = {Le Xu and Lei Cheng and Ngai Wong and Yik-Chung Wu},
keywords = {Bayesian inference, Tensor completion, Tensor train},
abstract = {As a powerful tool in analyzing multi-dimensional data, tensor train (TT) decomposition shows superior performance compared to other tensor decomposition formats. Existing TT decomposition methods, however, either easily overfit with noise, or require substantial fine-tuning to strike a balance between recovery accuracy and model complexity. To avoid the above shortcomings, this paper treats the TT decomposition in a fully Bayesian perspective, which includes automatic TT rank determination and noise power estimation. Theoretical justification on adopting the Gaussian-product-Gamma priors for inducing sparsity on the slices of the TT cores is provided, thus allowing the model complexity to be automatically determined even when the observed tensor data is noisy and contains many missing values. Furthermore, using the variational inference framework, an effective learning algorithm on the probabilistic model parameters is derived. Simulations on synthetic data demonstrate that the proposed algorithm accurately recovers the underlying TT structure from incomplete noisy observations. Further experiments on image and video data also show its superior performance to other existing TT decomposition algorithms.}
}
@article{LIU2023109735,
title = {A Lie group kernel learning method for medical image classification},
journal = {Pattern Recognition},
volume = {142},
pages = {109735},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109735},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004338},
author = {Li Liu and Haocheng Sun and Fanzhang Li},
keywords = {Medical image classification, Feature representation, Lie group manifold, Lie group machine learning, Kernel learning},
abstract = {Medical image classification is a basic step in medical image analysis and has been an essential task in computer-aided diagnosis. Existing classification methods are proved to be effective in conventional image classification tasks, but they often achieve a suboptimal performance when applied to medical images characterizing by complex nonlinear variation. Aiming at this challenge, this paper proposes a Lie group kernel learning method for medical image classification by combining Lie group theory, kernel functions, SVM and KNN classifiers. The method represents each image with a Lie group feature descriptor constructed from low-level features and builds a SVM classifier from the training images. Geodesic distances between categorical pivots and each testing image are calculated with Lie group kernel functions to select either the SVM or a KNN classifier to do the classification. The proposed method is applied to three medical image datasets and the results demonstrate the efficacy of the method.}
}
@article{WANG2023109690,
title = {Extending version-space theory to multi-label active learning with imbalanced data},
journal = {Pattern Recognition},
volume = {142},
pages = {109690},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109690},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003886},
author = {Ran Wang and Shuyue Chen and Yu Yu},
keywords = {Multi-label active learning, Sample-label pairs, Inconsistency, Version space, Imbalanced data},
abstract = {Version space, defined as the subset of the hypothesis space consistent with the training samples, is an important concept in supervised learning. It has been successfully applied for evaluating the informativeness of unlabeled samples in traditional single-label active learning. Specifically, the most inconsistent samples among the version space members can reduce the size of the version space as fast as possible, these samples are given high priority for domain expert annotation, thereby the learner can construct a high-performance classifier by labeling as few samples as possible. We point out that the concept of version space has not been extended to multi-label environments yet, which hinders its application in multi-label active learning. This paper makes an attempt to extend the version space theory from single-label scenario to multi-label scenario, builds up a spatial structure for the multi-label version space, generalizes it from finite case to infinite case, puts forward a simplified representation for it and accordingly proposes a new multi-label active learning algorithm. Moreover, considering the imbalance issue in multi-label data, the algorithm is further improved by allocating different annotation numbers to the labels. Experimental comparisons verify the feasibility and effectiveness of the proposed methods.}
}
@article{AHMAD2023109635,
title = {Robust federated learning under statistical heterogeneity via Hessian spectral decomposition},
journal = {Pattern Recognition},
volume = {141},
pages = {109635},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109635},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003369},
author = {Adnan Ahmad and Wei Luo and Antonio Robles-Kelly},
keywords = {Federated learning, Hessian, Non-IID data},
abstract = {Federated Learning (FL) is a collaborative machine learning paradigm in which a global model is learned via aggregating local ones. Although statistical heterogeneity of the local training data is necessary for the generalisability of the global model, it also introduces local model “drift” that slows down the convergence. Thus, how to optimally aggregate local models in FL remains an open problem. Recognising that training data lends varying evidential credence to different parts of a local model, we propose a novel approach to exploit such evidential asymmetry in FL aggregation in not independent and identically distributed (non-IID) data by applying a unique weight coefficient to each of the local parameter updates. To this end, we measure the parameter-level evidential credence making use of the eigenvalues of the Hessian of the local likelihood function, which are theoretically connected to the observed Fisher information. We employ these eigenpairs to propose a novel aggregation method, which we name FedHess. Our experiments show FedHess achieves smoother and faster convergence to a more accurate global model when compared with popular baselines such as Federated Average (FedAvg), FedProx, SCAFFOLD, Federated Curvature (FedCurv) and FedDF across different types of heterogeneous training data drawn from a number of benchmark datasets.}
}
@article{WANG2023109663,
title = {Local nonlinear dimensionality reduction via preserving the geometric structure of data},
journal = {Pattern Recognition},
volume = {143},
pages = {109663},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109663},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003643},
author = {Xiang Wang and Junxing Zhu and Zichen Xu and Kaijun Ren and Xinwang Liu and Fengyun Wang},
keywords = {Dimensionality reduction, Embedding learning, Geometric preservation, Random walk},
abstract = {Dimensionality reduction has many applications in data visualization and machine learning. Existing methods can be classified into global ones and local ones. The global methods usually learn the linear relationship in data, while the local ones learn the manifold intrinsic geometry structure, which has a significant impact on pattern recognition. However, most of existing local methods obtain an embedding with eigenvalue or singular value decomposition, where the computational complexities are very high in a large amount of high-dimensional data. In this paper, we propose a local nonlinear dimensionality reduction method named Vec2vec, which employs a neural network with only one hidden layer to reduce the computational complexity. We first build a neighborhood similarity graph from the input matrix, and then define the context of data points with the random walk properties in the graph. Finally, we train the neural network with the context of data points to learn the embedding of the matrix. We conduct extensive experiments of data classification and clustering on nine image and text datasets to evaluate the performance of our method. Experimental results show that Vec2vec is better than several state-of-the-art dimensionality reduction methods, except that it is equivalent to UMAP on data clustering tasks in the statistical hypothesis tests, but Vec2vec needs less computational time than UMAP in high-dimensional data. Furthermore, we propose a more lightweight method named Approximate Vec2vec (AVec2vec) with little performance degradation, which employs an approximate method to build the neighborhood similarity graph. AVec2vec is still better than some state-of-the-art local dimensionality reduction methods and competitive with UMAP on data classification and clustering tasks in the statistical hypothesis tests.}
}
@article{HUANG2023109658,
title = {GriT-DBSCAN: A spatial clustering algorithm for very large databases},
journal = {Pattern Recognition},
volume = {142},
pages = {109658},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109658},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300359X},
author = {Xiaogang Huang and Tiefeng Ma and Conan Liu and Shuangzhe Liu},
keywords = {DBSCAN, Clustering, Indexing methods, Spatial databases},
abstract = {DBSCAN is a fundamental spatial clustering algorithm with numerous practical applications. However, a bottleneck of DBSCAN is its O(n2) worst-case time complexity. To address this limitation, we propose a new grid-based algorithm for exact DBSCAN in Euclidean space called GriT-DBSCAN, which is based on the following two techniques. First, we introduce grid tree to organize the non-empty grids for the purpose of efficient non-empty neighboring grids queries. Second, by utilizing the spatial relationships among points, we propose a technique that iteratively prunes unnecessary distance calculations when determining whether the minimum distance between two sets is less than or equal to a certain threshold. We theoretically demonstrate that GriT-DBSCAN has excellent reliability in terms of time complexity. In addition, we obtain two variants of GriT-DBSCAN by incorporating heuristics, or by combining the second technique with an existing algorithm. Experiments are conducted on both synthetic and real-world data sets to evaluate the efficiency of GriT-DBSCAN and its variants. The results show that our algorithms outperform existing algorithms.}
}
@article{GUO2023109639,
title = {Joint A-SNN: Joint training of artificial and spiking neural networks via self-Distillation and weight factorization},
journal = {Pattern Recognition},
volume = {142},
pages = {109639},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109639},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003400},
author = {Yufei Guo and Weihang Peng and Yuanpei Chen and Liwen Zhang and Xiaode Liu and Xuhui Huang and Zhe Ma},
keywords = {Spiking neural networks, Artificial neural networks, Knowledge distillation, Weight factorization},
abstract = {Emerged as a biology-inspired method, Spiking Neural Networks (SNNs) mimic the spiking nature of brain neurons and have received lots of research attention. SNNs deal with binary spikes as their activation and therefore derive extreme energy efficiency on hardware. However, it also leads to an intrinsic obstacle that training SNNs from scratch requires a re-definition of the firing function for computing gradient. Artificial Neural Networks (ANNs), however, are fully differentiable to be trained with gradient descent. In this paper, we propose a joint training framework of ANN and SNN, in which the ANN can guide the SNN’s optimization. This joint framework contains two parts: First, the knowledge inside ANN is distilled to SNN by using multiple branches from the networks. Second, we restrict the parameters of ANN and SNN, where they share partial parameters and learn different singular weights. Extensive experiments over several widely used network structures show that our method consistently outperforms many other state-of-the-art training methods. For example, on the CIFAR100 classification task, the spiking ResNet-18 model trained by our method can reach to 77.39% top-1 accuracy with only 4 time steps.}
}
@article{ALSUMAIDAEE2023109647,
title = {Spatio-temporal modelling with multi-gradient features and elongated quinary pattern descriptor for dynamic facial expression recognition},
journal = {Pattern Recognition},
volume = {142},
pages = {109647},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109647},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003485},
author = {S.A.M. Al-Sumaidaee and M.A.M. Abdullah and R.R.O. Al-Nima and S.S. Dlay and J.A. Chambers},
keywords = {Dynamic facial expression recognition, Gaussian mask, Three orthogonal planes, Block volume, Three-dimensional histogram features},
abstract = {We propose a new spatio-temporal modelling approach for Dynamic Facial Expression Recognition (DFER). We first convert the domain of the spatial images in the sequence to the gradient of magnitude and angle images at different orientations. Robust gradient components are developed to deal with difficult types of illuminations, such as darkness, by forming the eight edge responses of the Gaussian mask. To describe the dynamic Facial Expression (FE) changes we extend the Elongated Quinary Pattern (EQP) descriptor to encode separately the anisotropic structure of the uniform patterns from Three Orthogonal Planes (TOP) of each gradient sequence. Then each encoded sequence is divided into a stack of block volumes in the XY, XT and YT planes. For each plane, the co-occurrence of histogram features are calculated from each block volume and concatenated together. Simple three-dimensional histogram features are generated by concatenating the histogram features of all planes. A Multi Classifier System (MCS) based on a multi-class Support Vector Machine (SVM) is adopted to combine all scores for the encoded sequences. The proposed approach is evaluated with the challenging MMI and Oulu-CASIA databases with different set-ups and advantage has been shown in terms of generalisation to different databases, together with robustness against difficult pose variations and illumination changes. In terms of Recognition Accuracy (RA), a comparison is established with DFER methods in the literature. A high recognition rate of 79.23% is attained in the case of six classes when applied to the MMI database which surpasses all the state-of-the-art results.}
}
@article{RAKESH2023109623,
title = {An improved differential evolution algorithm for quantifying fraudulent transactions},
journal = {Pattern Recognition},
volume = {141},
pages = {109623},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109623},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003242},
author = {Deepak Kumar Rakesh and Prasanta K. Jana},
keywords = {Quantifying fraudulent transactions (QFT), Cost-based feature selection, Multiobjective optimization, Differential evolution},
abstract = {Identification of fraudulent credit card transactions is a complex problem mainly due to the following factors: 1) The relative behavior of customers and fraudsters may alter over time. 2) The ratio of legitimate to fraudulent transactions is highly imbalanced, and 3) Investigators examine a small segment of transactions in a reasonable time frame. Researchers have proposed various algorithms to identify potential fraud in a new incoming transaction. However, these approaches require significant human investigator effort and are sometimes misleading. To address this issue, this paper proposes an improved multiobjective differential evolution (DE) algorithm to estimate the distribution of fraudulent transactions in a set of new incoming transactions, referred to as quantifying fraudulent transactions. Our paper has three major novelties. First, we present the problem formulation of cost-based feature selection with maximum quantification ability. Second, we improve the DE by applying effective trial vector generation algorithms to the random control parameter settings to exploit the advantage of individual DE variants. Third, we develop the maximum-relevancy-minimum-redundancy-based Pareto refining operator to enhance the self-learning ability of individuals in Pareto solutions. We compare our approach against four other modifications of DE and five state-of-the-art evolutionary algorithms on real-time credit datasets in streaming and non-streaming frameworks using hyper-volume, two-set coverage, and spread performance metrics.}
}
@article{WANG2023109723,
title = {A novel classification method combining phase-field and DNN},
journal = {Pattern Recognition},
volume = {142},
pages = {109723},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109723},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004211},
author = {Jian Wang and Ziwei Han and Wenjing Jiang and Junseok Kim},
keywords = {Phase-field-DNN, Phase-field, DNN, Classification},
abstract = {This paper proposes a novel classification method. Firstly, we use the deep neural network (DNN) to classify the training set. After several iterations, we obtain the output vector Y. The component of the largest value in vector Y is represented as the label being classified, which we take as the output value. Because we chose the sigmoid function as our activation function, the output value is between 0 and 1. Therefore, the output value can represents the probability of the classified label by the DNN. Depending on the distribution of output values, we set tolerance values (Tol) that categorize similar output values as the same label in the DNN. If the output value is lower than Tol, we consider it categorically anomalous. Subsequently, we use the Phase-Field model to classify these anomalies and obtain better classification results. As this classification method combines Phase-Field model and DNN, we named it Phase-Field-DNN. In the numerical experiment using MNIST handwritten digit data set as experimental data, the classification accuracy of Phase-Field-DNN model is higher than that of Phase-Field model and DNN model through the analysis of the classification results of binary classification and multi-classification problems with this data. In addition, the model we proposed is used to classify the normal and abnormal brain MRIs, and the classification results are compared with those of others. After comparison, we find that our proposed model achieve the best classification results.}
}
@article{XUE2023109651,
title = {Hybrid neural-like P systems with evolutionary channels for multiple brain metastases segmentation},
journal = {Pattern Recognition},
volume = {142},
pages = {109651},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109651},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003527},
author = {Jie Xue and Qi Li and Xiyu Liu and Yujie Guo and Jie Lu and Bosheng Song and Pu Huang and Qiong An and Guanzhong Gong and Dengwang Li},
keywords = {Hybrid neural-like P system, Evolutionary channels, Segmentation of brain metastases},
abstract = {Neural-like P systems are membrane computing models inspired by natural computing. Spiking neurral (SN) P systems, a kind of neural-like P systems, are viewed as third-generation neural network models. Although real neurons have complex structures, classical SN P systems simplify the structures and corresponding mechanisms to stationary two-dimensional graphs and lack related evolution mechanisms on spikes and channels, which limits the real applications of these models. In this paper, we propose a new hybrid SN P system with evolutionary channels (HN P systems), including three new types of rules for dynamically generating or removing one-one and one-many/many-one channels with related evolutions of spikes on the hybrid neuron structures. Two dynamic regulatory factors are also presented on rules to help guide the optimization of the HN P systems automatically. Based on the new P system, a multiple brain metastases (BMs) segmentation model is developed. The experimental results indicate that the proposed models outperform the state-of-the-art methods on the BMs, which have large variations in sizes, positions and shapes, and low contrast with their surroundings. Performances on the head and neck segmentation dataset also verifies the effectiveness of the HN P system.}
}
@article{HE2023109680,
title = {NRPose: Towards noise resistance for multi-person pose estimation},
journal = {Pattern Recognition},
volume = {142},
pages = {109680},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109680},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003801},
author = {Jianhang He and Junyao Sun and Qiong Liu and Shaowu Peng},
keywords = {Multi-person pose estimation, Noise resistance, Region proposal, Keypoint relation},
abstract = {The high signal-to-noise ratio is one of the main challenges of multi-person pose estimation (MPE) and receives little attention. In this work, we find that MPE suffers from two types of noise: aleatoric noise and epistemic noise. The former represents the noise inherent in the observations, such as the background. The latter indicates the noise brought by the priori hypotheses, such as the inappropriate keypoint relations. Both of them reduce the saliency of information available for keypoint localization. We propose the noise-resistance pose estimation (NRPose) that integrates keypoint-oriented region proposal module (KRPM) and pose-aware sparse relation module (PSRM). To mitigate aleatoric noise, KRPM generates keypoint-level RoIs by circumscribing semantically significant regions. To reduce epistemic noise, PSRM filters out the noisy relations dynamically by modeling the noise propagation and keypoint interaction. NRPose outperforms the state-of-the-art methods by at least 1.0 AP on COCO and OCHuman dataset.}
}
@article{CHEN2023109728,
title = {Rethinking the unpretentious U-net for medical ultrasound image segmentation},
journal = {Pattern Recognition},
volume = {142},
pages = {109728},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109728},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004260},
author = {Gongping Chen and Lei Li and Jianxun Zhang and Yu Dai},
keywords = {Nested U-nets, Ultrasound Image, Breast tumor, Renal ultrasound, Automatic segmentation},
abstract = {Breast tumor segmentation from ultrasound images is one of the key steps that help us characterize and localize tumor regions. However, variable tumor morphology, blurred boundaries, and similar intensity distributions bring challenges for radiologists to segment breast tumors manually. During clinical diagnosis, there are higher demands on the segmentation accuracy and efficiency of breast ultrasound images, so there is an urgent need for an automated method to improve the segmentation accuracy as a technical tool to assist diagnosis. Inspired by the U-net and its many variations, this paper proposed an unpretentious nested U-net (NU-net) for accurate and efficient breast tumor segmentation. The key idea is to utilize U-nets with different depths and shared weights to achieve robust characterization of breast tumors. Specifically, we first utilize the deeper U-net (fifteen layers) as the backbone network to extract more sufficient breast tumor features. Then, we developed a multi-output U-net to be taken as the bond between the encoder and the decoder to enhance the network adaptability for breast tumors with different scales. Finally, the short-connection based on multi-step down-sampling is used to enhance the correlation of long-range information of encoded features. Extensive experimental results with fifteen state-of-the-art segmentation methods on three public breast ultrasound datasets demonstrate that our method has a more competitive segmentation performance on breast tumors. Furthermore, the robustness of our approach is further illustrated by the segmentation of renal ultrasound images. The source code is publicly available on https://github.com/CGPxy/NU-net.}
}
@article{LIU2023109628,
title = {Deepfacelab: Integrated, flexible and extensible face-swapping framework},
journal = {Pattern Recognition},
volume = {141},
pages = {109628},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109628},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003291},
author = {Kunlin Liu and Ivan Perov and Daiheng Gao and Nikolay Chervoniy and Wenbo Zhou and Weiming Zhang},
keywords = {Face swapping, Practical machine learning, Open source},
abstract = {Face swapping has drawn a lot of attention for its compelling performance. However, current deepfake methods suffer the effects of obscure workflow and poor performance. To solve these problems, we present DeepFaceLab, the current dominant deepfake framework for practical face-swapping. It provides the necessary tools as well as an easy-to-use way to conduct high-quality face-swapping. It also offers a flexible and loose coupling structure for people who need to strengthen their pipeline with other features without writing complicated boilerplate code. We detail the principles that drive the implementation of DeepFaceLab and introduce its pipeline. DeepFaceLab could achieve cinema-level results with high fidelity as our supplemental video shows. We also demonstrate the advantage of our system by comparing our approach with other face-swapping methods. Deepfake defense not only requires the research of detection but also requires the efforts of generation methods. As for a popular and practical toolkit, we encourage users to promote harmless deepfake-entertainment content on social media, reminding the public of the existence of deepfake when they are looking for entertainment.}
}
@article{YU2023109656,
title = {Smoothing group L1/2 regularized discriminative broad learning system for classification and regression},
journal = {Pattern Recognition},
volume = {141},
pages = {109656},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109656},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003576},
author = {Dengxiu Yu and Qian Kang and Junwei Jin and Zhen Wang and Xuelong Li},
keywords = {Broad learning system, Discriminative, Sparsity, Smoothing group  regularization, Optimization},
abstract = {This paper presents the framework of the smoothing group L1/2 regularized discriminative broad learning system for pattern classification and regression. The core idea is to improve the sparseness of the standard broad learning system and improve performance on recognition and generalization. First, the ε-dragging technique is introduced into the standard broad learning system to relax regression targets and enlarge distances between categories. Then, we integrate the group L1/2 regularization to optimize the network architecture to achieve sparsity. For the original group L1/2 regularization, the objective function is non-convex and non-smooth, which is hard for theoretical analysis. Therefore, we propose a simple and effective smoothing technique, i.e.,smoothing group L1/2 regularization, which can effectively eliminate the deficiency of the original group L1/2 regularization. As a result, the final weights projection matrix has a compact form and shows discriminative power capability. In addition, the alternating direction method of multipliers was adopted to optimize the algorithm. The simulation results show that the proposed algorithm has redundancy control capability and improved performance on recognition and generalization. The simulation results proves the efficiency of the theoretical analysis.}
}
@article{MIAO2023109687,
title = {Triplet teaching graph contrastive networks with self-evolving adaptive augmentation},
journal = {Pattern Recognition},
volume = {142},
pages = {109687},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109687},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003850},
author = {Jiaxing Miao and Feilong Cao and Ming Li and Bing Yang and Hailiang Ye},
keywords = {Contrastive learning, Graph representation learning, Graph augmentation, Node classification},
abstract = {Unsupervised graph contrastive learning has recently emerged as the solution to the crisis of label information scarcity for graph data in the real world. However, from the general paradigm of graph contrastive learning, most of the existing methods are still flawed in the design and use of augmented views and the design of contrastive targets. Therefore, the works on how to generate reasonable augmented views and utilize them canonically and how to construct efficient and comprehensive contrastive objectives are very meaningful. Based on the teaching concept, this paper proposes a new triplet teaching graph contrastive network with self-evolving adaptive augmentation. Firstly, after carefully analyzing the internal relationships between different augmented perspectives, we present a triple teaching graph neural network framework based on the improved triplet idea. It creates contrastive objectives depending on different contrastive angle levels, providing thorough guidance for graph encoders. Secondly, a self-evolving adaptive graph augmentation scheme based on topology and feature information is proposed. It is worth mentioning that with the continuous deepening of the training process, the scheme can utilize the learnable self-attention mechanism to constantly supply our network framework with an increasing number of reliable augmented views as input. Finally, when designing the contrastive objectives, we introduce a stochastic hybrid module to mine the unexploited information, which opportunely complements the contrastive sample space formed by our network framework. Furthermore, extensive experiments on multiple real-world node classification datasets demonstrate that our model can generate better-quality node embedding for downstream tasks. The implementation of this paper is available at https://github.com/PaperMiao/T-GCSA.}
}
@article{LIANG2023109632,
title = {Multi-view unsupervised feature selection with tensor robust principal component analysis and consensus graph learning},
journal = {Pattern Recognition},
volume = {141},
pages = {109632},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109632},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003333},
author = {Cheng Liang and Lianzhi Wang and Li Liu and Huaxiang Zhang and Fei Guo},
keywords = {Multi-view unsupervised feature selection, Low-rank tensor learning, Spectral embedding, Robust sparse regression model},
abstract = {Recently, multi-view unsupervised feature selection has attracted much attention due to its efficiency and better interpretability in processing high-dimensional multi-view datasets. Most existing methods rely on the constructed similarity matrices to obtain reliable pseudo labels to guide the feature selection. However, the considerable adverse noise in the raw data inevitably impedes the exploration of true underlying similarity structures. Besides, the inter-view correlations are often ignored during the common representation learning, which limits the effective fusion of the essential information from multiple views. To solve these issues, we design a novel robust multi-view unsupervised feature selection framework. Specifically, our method seeks a set of noise-free view-specific similarity matrices by leveraging tensor robust principal component analysis, where the high-order connections among different views are well exploited through the constructed low-rank tensor. Meanwhile, a high-quality consensus similarity matrix is adaptively learned from the view-specific representations within the same unified framework to capture the shared local structures. To enhance the discriminative ability of the feature selection matrix, we further impose a rank constraint on the consensus similarity matrix to obtain reliable pseudo cluster indicators. We present an efficient optimization algorithm ground on the alternating direction method of multipliers to solve the proposed model. Experimental results on six multi-view datasets confirm the superiority of our method.}
}
@article{YANG2023109627,
title = {CPSS-FAT: A consistent positive sample selection for object detection with full adaptive threshold},
journal = {Pattern Recognition},
volume = {141},
pages = {109627},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109627},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300328X},
author = {Xiaobao Yang and Junsheng Wu and Lang He and Sugang Ma and Zhiqiang Hou and Wei Sun},
keywords = {Object detection, Label assignment, Location quality estimation, Adaptive threshold},
abstract = {Recent CNN-based methods for object detection largely focus on training the backbone of the object detector, neglecting the key part of selecting positive/negative samples. Instead, based on our analyses about the limitations and inconsistency of existing positive sample selection, we propose a novel Consistent Positive Sample Selection (CPSS) method to select the positive samples automatically, which joints Box-IoU and normalized central distance of object-anchor by mutual weighting. Meanwhile, we employ a consistent approach to location quality evaluation for suppressing the false-positive predicted box when inferencing. Furthermore, an auxiliary Full Adaptive Threshold (FAT) post-processing is also adopted according to the objects’ occlusion level to improve the recall ratio. We implement the proposed CPSS-FAT detector using MS COCO 2017 and CityScapes datasets, the comparing results indicate that our approaches are effective and robust to the different objects in an open world. Especially, we achieve 52.2% AP and 43.1% ARS, outperforming most existing detectors.}
}
@article{FENG2023109536,
title = {Community Channel-Net: Efficient channel-wise interactions via community graph topology},
journal = {Pattern Recognition},
volume = {141},
pages = {109536},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109536},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002364},
author = {Fan Feng and Qi Liu and Zhanglin Peng and Ruimao Zhang and Rosa H.M. Chan},
keywords = {Deep Neural Networks, Complex Networks, Representation Learning},
abstract = {The layer-wise structure of deep neural networks (DNNs) isolates the channel interactions in the same layer, which significantly impedes the efficient learning of DNNs. Several existing methods enable channel-wise information exchange via learning channel interdependence in a heuristic and empirical manner. Nevertheless, only informative channels are emphasized while other channels are suppressed in these approaches. This results in a low channel diversity, which impeds the generalization of DNNs. Our work aims to learn channel-wise interdependence and keep the channel diversity concurrently via designing optimal channel interaction patterns. We model the channel interaction pattern from a graph perspective, where the interactions can be regarded as information exchange on the channel graph. Based on this framework, we propose the Community Channel-Net (CC-Net), using a community-based graph topology for channel interaction. Each community contains channels with semantic commonalities, and the inter-community connections are activated among critical channels. With this structured and dynamic topology, the channels from the same community can learn channel interdependence, and those critical channels from distinct communities can gain more diverse features. CC-Net outperforms baselines on image classification tasks over various backbones with fewer computational costs.}
}
@article{2024110611,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {153},
pages = {110611},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(24)00362-5},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003625}
}
@article{CHEN2023109668,
title = {Knowledge driven weights estimation for large-scale few-shot image recognition},
journal = {Pattern Recognition},
volume = {142},
pages = {109668},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109668},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003692},
author = {Jingjing Chen and Linhai Zhuo and Zhipeng Wei and Hao Zhang and Huazhu Fu and Yu-Gang Jiang},
keywords = {Few-shot image, Recognition, Knowledge transfer},
abstract = {We study the topic of large-scale few-shot image recognition with semantic-visual relational knowledge-based transfer learning. Compared with classical few-shot learning, which is defined as a k-way (k denotes the number of categories, usually 5/10-way) classification problem, large-scale few-shot recognition contains more categories (100-way +) with few samples per category and is easier to overfit. A promising direction of large-scale few-shot learning is transferring prior relevant semantic/visual knowledge from outside data to accelerate the convergence on limited positives. Inspired by this, we propose a novel Knowledge Driven Weights Estimation framework. Specifically, the framework leverages semantic and visual relations between new few-shot and existed many-shot categories to transfer knowledge trained on many-shot datasets (e.g., ImageNet-1000). We show that the transferred knowledge provides a good initialization for novel few-shot categories leading to faster convergence speed and higher performance than random/imprinting initialization. Experimental results on additional un-seen ImageNet categories (other than the 1000 categories) with few positives show that our method is effective on large-scale few-shot recognition.}
}
@article{2024110663,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {154},
pages = {110663},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(24)00414-X},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400414X}
}
@article{MA2023109716,
title = {Optimal transport based pyramid graph kernel for autism spectrum disorder diagnosis},
journal = {Pattern Recognition},
volume = {143},
pages = {109716},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109716},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004144},
author = {Kai Ma and Shuo Huang and Peng Wan and Daoqiang Zhang},
keywords = {Optimal transport, Brain network, Graph kernel, Autism spectrum disorder, fMRI},
abstract = {Brain network, which characterizes the functional and structural interactions of brain regions with graph theory, has been widely utilized to diagnose brain diseases, such as autism spectrum disorder (ASD). It is a challenge to measure the network (or graph) similarity in brain network analysis. Graph kernel (i.e., kernel defined on graphs) offers an efficient tool for measuring the similarity of paired brain networks and yields the excellent classification performance in brain disease diagnosis. However, most of the existing graph kernels neglected the hierarchical architecture information of brain networks. To address this problem, in this paper, we propose an optimal transport based pyramid graph kernel for measuring brain network similarity and then apply it to brain disease classification. The main idea is to transform brain networks into pyramid structures, which reflect the hierarchical architecture information of the brain network with multi-resolution histograms. The optimal transport distance in pyramid structures is calculated for measuring transport costs between paired brain networks. Finally, the optimal transport based pyramid graph kernel is computed based on this optimal transport distance. To evaluate the effectiveness of the proposed optimal transport based pyramid graph kernel, the extensive experiments are performed in functional magnetic resonance imaging data of brain disease from the Autism Brain Imaging Data Exchange database. The experimental results show that our proposed optimal transport based pyramid graph kernel outperforms the state-of-the-art methods in ASD classification tasks.}
}
@article{FU2023109660,
title = {Multi-stage information diffusion for joint depth and surface normal estimation},
journal = {Pattern Recognition},
volume = {141},
pages = {109660},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109660},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003618},
author = {Zhiheng Fu and Siyu Hong and Mengyi Liu and Hamid Laga and Mohammed Bennamoun and Farid Boussaid and Yulan Guo},
keywords = {Depth estimation, Surface normal estimation, Multitask learning, Attention map, Multi-Stage information fusion},
abstract = {Depth and surface normal estimations are important for 3D geometric perception, which has numerous applications including autonomous vehicles and robots. In this paper, we propose a lightweight Multi-stage Information Diffusion Network (MIDNet) for the simultaneous prediction of depth and surface normals from a single RGB image. To obtain semantic and detail-preserving features, we adopt a high-resolution network as our backbone to learn multi-scale features, which are then fused into shared features for the two tasks. To mutually boost each task, a Cross-Correlation Attention Module (CCAM) is proposed to adaptively integrate information for the prediction of the two tasks in multiple stages, including feature-level information interaction and task-level information interaction. Ablation studies show that the proposed multi-stage information diffusion strategy can boost the performance gain for the two tasks at different levels. Compared to current state-of-the-art methods on the NYU Depth V2, Stanford 2D-3D-Semantic and KITTI datasets, our method achieves superior performance for both monocular depth and surface normal estimation.}
}
@article{ZHANG2023109740,
title = {Data-Driven single image deraining: A Comprehensive review and new perspectives},
journal = {Pattern Recognition},
volume = {143},
pages = {109740},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109740},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004387},
author = {Zhao Zhang and Yanyan Wei and Haijun Zhang and Yi Yang and Shuicheng Yan and Meng Wang},
keywords = {Data-driven single image deraining, Comprehensive review, New perspective of data, Data, Rain model, Network architecture, Solving paradigms, In-depth analysis, Effectiveness of data},
abstract = {Single Image Deraining (SID) aims at recovering the rain-free background from an image degraded by rain streaks. For the powerful fitting ability of deep neural networks and massive training data, data-driven deep SID methods have obtained significant improvement over traditional model/prior-based ones. Current studies usually focus on improving the deraining performance by proposing different categories of deraining networks, while neglecting the interpretation of the solving process. As a result, the generalization ability may still be limited in real-world scenarios, and the deraining results also cannot effectively improve the performance of subsequent high-level tasks (e.g., object detection). To explore these issues, we in this paper re-examine the three important factors (i.e., data, rain model and network architecture) for the SID problem, and specifically analyze them by proposing new and more reasonable criteria (i.e., general vs. specific, synthetical vs. mathematical, black-box vs. white-box). We also study the relationship of the three factors from a new perspective of data, and reveal two different solving paradigms (explicit vs. implicit) for the SID task. We further discuss the current mainstream data-driven SID methods from five aspects, i.e., training strategy, network pipeline, domain knowledge, data preprocessing, and objective function, and some useful conclusions are summarized by statistics. Besides, we profoundly studied one of the three factors, i.e., data, and measured the performance of current methods on different datasets through extensive experiments to reveal the effectiveness of SID data. Finally, with the comprehensive review and in-depth analysis, we draw some valuable conclusions and suggestions for future research.}
}
@article{DONG2023109732,
title = {Momentum contrast transformer for COVID-19 diagnosis with knowledge distillation},
journal = {Pattern Recognition},
volume = {143},
pages = {109732},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109732},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004302},
author = {Aimei Dong and Jian Liu and Guodong Zhang and Zhonghe Wei and Yi Zhai and Guohua Lv},
keywords = {Momentum contrastive learning, Knowledge distillation, Vision transformer, COVID-19 diagnosis},
abstract = {Intelligent diagnosis has been widely studied in diagnosing novel corona virus disease (COVID-19). Existing deep models typically do not make full use of the global features such as large areas of ground glass opacities, and the local features such as local bronchiolectasis from the COVID-19 chest CT images, leading to unsatisfying recognition accuracy. To address this challenge, this paper proposes a novel method to diagnose COVID-19 using momentum contrast and knowledge distillation, termed MCT-KD. Our method takes advantage of Vision Transformer to design a momentum contrastive learning task to effectively extract global features from COVID-19 chest CT images. Moreover, in transfer and fine-tuning process, we integrate the locality of convolution into Vision Transformer via special knowledge distillation. These strategies enable the final Vision Transformer simultaneously focuses on global and local features from COVID-19 chest CT images. In addition, momentum contrastive learning is self-supervised learning, solving the problem that Vision Transformer is challenging to train on small datasets. Extensive experiments confirm the effectiveness of the proposed MCT-KD. In particular, our MCT-KD is able to achieve 87.43% and 96.94% accuracy on two publicly available datasets, respectively.}
}
@article{PAN2023109699,
title = {Hyperspectral image denoising via spectral noise distribution bootstrap},
journal = {Pattern Recognition},
volume = {142},
pages = {109699},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109699},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003977},
author = {Erting Pan and Yong Ma and Xiaoguang Mei and Fan Fan and Jiayi Ma},
keywords = {Hyperspectral image denoising, Image restoration, Spectral distribution, Noise estimation, Noise distribution},
abstract = {Hyperspectral image (HSI) denoising is an ill-posed problem, leading to integrating proper prior knowledge about hyperspectral noise is critical to developing an efficient denoising method. Most existing methods share a common assumption that all bands have equal noise intensity. However, such assumption runs counter to the practical HSIs, leading to unpleasant denoising results. To tackle this, we intend to investigate the intrinsic properties of real HSI noise in the spectral dimension and construct a novel denoising framework bootstrapping by spectral noise distribution N^, termed N^-Net. On the one hand, we develop dense and sparse recurrent calculations, exploiting intrinsic properties of HSI noise (i.e., diversity, dense dependency, and global sparsity) to estimate spectral noise distribution. On the other hand, having the estimated spectral noise distribution, we develop a bootstrap mechanism with a repetitive emphasis on its guidance for subsequent spatial noise separation and clean HSI recovery, ensuring a more delicate denoising effect. In particular, we verify that the proposed denoising framework can achieve promising denoising performances due to the merit of spectral noise distribution bootstrapping, which also promotes new insights for future related research. The code is avaliable at https://github.com/EtPan/N-Net.}
}
@article{FREITAS2023109672,
title = {Time-constrained learning},
journal = {Pattern Recognition},
volume = {142},
pages = {109672},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109672},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003734},
author = {Sergio Freitas and Eduardo Laber and Pedro Lazera and Marco Molinaro},
keywords = {Machine teaching, Time-constrained learning, Classification methods},
abstract = {Consider a scenario in which we have a huge labeled dataset D and a limited time to train a given learner using D. Since we may not be able to use the whole dataset, how should we proceed? We propose TCT, an algorithm for this task, whose design relies on principles from Machine Teaching. We present an experimental study involving 5 different learners and 20 datasets where we show that TCT consistently outperforms alternative teaching/training methods, namely: (1) Training over batches of random samples, until the time limit is reached; (2) The state-of-the-art Machine Teaching algorithm for black-box learners proposed in [Dasgupta et al., ICML 19], and (3) Stochastic Gradient Descent (when applicable). While our work is primarily practical, we also show that a stripped-down version of TCT has provable guarantees.}
}
@article{SUGAHARA2023109657,
title = {Hierarchical co-clustering with augmented matrices from external domains},
journal = {Pattern Recognition},
volume = {142},
pages = {109657},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109657},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003588},
author = {Kai Sugahara and Kazushi Okamoto},
keywords = {Hierarchical co-clustering, Transfer learning, Relational data analysis, Unsupervised machine learning, Information theory, Mutual information},
abstract = {Co-clustering simultaneously classifies row and column objects of data matrices and is considered to have better accuracy than conventional one-way clustering methods. In the era of big data, extracting classification knowledge about objects from several domains has become increasingly feasible. This study proposes a hierarchical co-clustering with augmented matrices (HICCAM), which co-clusters the row and column objects of a target matrix while utilizing the augmented data matrices of these target objects extracted from the external domains. The algorithm is designed to improve classification accuracy by transferring knowledge in augmented matrices and simultaneously improves cluster interpretability using hierarchical cluster structures. Experiments on document clustering confirmed that HICCAM achieved the highest accuracy among comparison methods with and without external knowledge. Its clusters exhibit hierarchical relationships according to their topics. In addition, we provide the experimental results with multiview synthetic datasets that demonstrate a clustering situation in which HICCAM can be effectively identified.}
}
@article{SONG2023109688,
title = {A new methodology in constructing no-reference focus quality assessment metrics},
journal = {Pattern Recognition},
volume = {142},
pages = {109688},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109688},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003862},
author = {Jie Song and Mengjun Liu},
keywords = {Autofocus, Microscopy image analysis, No-reference image quality assessment},
abstract = {This paper proposed a new methodology which converts a full-reference focus quality assessment metric into a no-reference one. The methodology consists of three hypotheses which describe the relationship in focus quality between the original image and its variants. Using the proposed methodology, two no-reference metrics were constructed. The first used Brenner Gradient and the second used a full-reference metric proposed by ourselves. Evaluation was conducted on a public dataset and our own proposed dataset. Comparing with other no-reference metrics, our second one exhibited best performance on both datasets, with calculation time comparable to some fastest metrics considered.}
}
@article{ZHANG2023109633,
title = {Dual-branch spatio-temporal graph neural networks for pedestrian trajectory prediction},
journal = {Pattern Recognition},
volume = {142},
pages = {109633},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109633},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003345},
author = {Xingchen Zhang and Panagiotis Angeloudis and Yiannis Demiris},
keywords = {Pedestrian trajectory prediction, Social interactions, Graph convolutional networks, Graph attention networks, Spatio-temporal graph},
abstract = {Pedestrian trajectory prediction is an important area in computer vision, with wide applications in autonomous driving, robot path planning, and surveillance systems. The core underlying technique of these applications is pattern recognition. A key challenge in this area is modeling social interactions between pedestrians, such as pedestrian view area and group behaviors. However, although many methods have been proposed to model social interactions, pedestrian view area and group behaviors have not been explored together to account for complex situations. Additionally, most existing studies require additional detectors and manual annotations to handle view area and group interactions, respectively. In this paper, we propose a dual-branch spatio-temporal graph neural network to automatically model view area and grouping together. Specifically, a spatio-temporal graph attention network (STGAT) branch is designed to handle pedestrian view area, and a spatio-temporal graph convolutional network (STGCN) branch is designed to model group interactions. The features of these branches are then fused to provide better feature representations, on which a temporal convolution operation (TCN) is performed for trajectory prediction. Experiments on public standard datasets demonstrate that the proposed method achieves very competitive performance and predicts socially acceptable trajectories in different challenging scenarios.}
}
@article{CHOI2023109645,
title = {Semantic-guided de-attention with sharpened triplet marginal loss for visual place recognition},
journal = {Pattern Recognition},
volume = {141},
pages = {109645},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109645},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003461},
author = {Seung-Min Choi and Seung-Ik Lee and Jae-Yeong Lee and In So Kweon},
keywords = {Visual place recognition, Image retrieval, Triplet marginal loss, Attention, De-attention, Semantic guidance, Semantic segmentation},
abstract = {Thanks to Earth-level Street View images from Google Maps, a visual image geo-localization can estimate the coarse location of a query image with a visual place recognition process. However, this can get very challenging when non-static objects change with time, severely degrading image retrieval accuracy. We address the problem of city-scale visual place recognition in complex urban environments crowded with non-static clutters. To this end, we first analyze what clutters degrade similarity matching between the query and database images. Second, we design a self-supervised trainable de-attention module that prevents the network from focusing on non-static objects in an input image. In addition, we propose a novel triplet marginal loss called sharpened triplet marginal loss to make feature descriptors more discriminative. Lastly, due to the lack of geo-tagged public datasets with a high density of non-static objects, we propose a clutter augmentation method to evaluate our approach. The experimental results show that our model has notably improved over the existing attention methods in geo-localization tasks on the public benchmark datasets and on their augmented versions with high population and traffic. Our code is available at https://github.com/ccsmm78/deattention_with_stml_for_vpr.}
}
@article{KANG2023109692,
title = {Self-paced principal component analysis},
journal = {Pattern Recognition},
volume = {142},
pages = {109692},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109692},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003904},
author = {Zhao Kang and Hongfei Liu and Jiangxin Li and Xiaofeng Zhu and Ling Tian},
keywords = {Dimension reduction, Outliers, Manifold learning},
abstract = {Principal Component Analysis (PCA) has been widely used for dimensionality reduction and feature extraction. Robust PCA (RPCA), under different robust distance metrics, such as ℓ1-norm and ℓ2,p-norm, can deal with noise or outliers to some extent. However, real-world data may display structures that can not be fully captured by these simple functions. In addition, existing methods treat complex and simple samples equally. By contrast, a learning pattern typically adopted by human beings is to learn from simple to complex and less to more. Based on this principle, we propose a novel method called Self-paced PCA (SPCA) to further reduce the effect of noise and outliers. Notably, the complexity of each sample is calculated at the beginning of each iteration in order to integrate samples from simple to more complex into training. Based on an alternating optimization, SPCA finds an optimal projection matrix and filters out outliers iteratively. Theoretical analysis is presented to show the rationality of SPCA. Extensive experiments on popular data sets demonstrate that the proposed method can improve the state-of-the-art results considerably.}
}
@article{YING2023109717,
title = {Region-aware RGB and near-infrared image fusion},
journal = {Pattern Recognition},
volume = {142},
pages = {109717},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109717},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004156},
author = {Jiacheng Ying and Can Tong and Zehua Sheng and Bowen Yao and Si-Yuan Cao and Heng Yu and Hui-Liang Shen},
keywords = {Image fusion, RGB and near-infrared, overexposed sky recovery, vegetation enhancement, gradient-domain optimization},
abstract = {This paper proposes a region-aware fusion method, called RaIF, for RGB and near-infrared (NIR) outdoor scenery image fusion. The method is motivated by the observation that current fusion approaches produce gray appearance in overexposed sky regions and distortion in vegetation regions. RaIF generates the region probability maps by exploiting their specific characteristics in the visible and NIR spectra. It recovers the overexposed sky regions by employing the intrinsic channel correlation between RGB and NIR images, and enhances the vegetation regions in an adjustable manner. RaIF formulates image fusion problem as a gradient-domain optimization problem with luminance and chromaticity regularizations. Experimental results validate the superiority of RaIF that produces fused images with improved appearance in the sky and vegetation regions, and achieves the state-of-the-art performance quantitatively and qualitatively. Furthermore, RaIF can act as a refinement module that improves the fusion results of current deep learning based approaches. It is also capable of recovering specular highlight regions other than sky overexposure.}
}
@article{ZHANG2023109741,
title = {Fully context-aware image inpainting with a learned semantic pyramid},
journal = {Pattern Recognition},
volume = {143},
pages = {109741},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109741},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004399},
author = {Wendong Zhang and Yunbo Wang and Bingbing Ni and Xiaokang Yang},
keywords = {Image inpainting, Multi-Scale semantic priors, Learned semantic pyramid, Stochastic semantic inference},
abstract = {Restoring reasonable and realistic content for arbitrary missing regions in images is an important yet challenging task. Although recent image inpainting models have made significant progress in generating vivid visual details, they can still lead to texture blurring or structural distortions due to contextual ambiguity when dealing with more complex scenes. To address this issue, we propose the Semantic Pyramid Network (SPN) motivated by the idea that learning multi-scale semantic priors from specific pretext tasks can greatly benefit the recovery of locally missing content in images. SPN consists of two components. First, it distills semantic priors from a pretext model into a multi-scale feature pyramid, achieving a consistent understanding of the global context and local structures. Within the prior learner, we present an optional module for variational inference to realize probabilistic image inpainting driven by various learned priors. The second component of SPN is a fully context-aware image generator, which adaptively and progressively refines low-level visual representations at multiple scales with the (stochastic) prior pyramid. We train the prior learner and the image generator as a unified model without any post-processing. Our approach achieves the state of the art on multiple datasets, including Places2, Paris StreetView, CelebA, and CelebA-HQ, under both deterministic and probabilistic inpainting setups.}
}
@article{SU2023109700,
title = {Physical model and image translation fused network for single-image dehazing},
journal = {Pattern Recognition},
volume = {142},
pages = {109700},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109700},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003989},
author = {Yan Zhao Su and Chuan He and Zhi Gao Cui and Ai Hua Li and Nian Wang},
keywords = {Single-image dehazing, Image translation, Physical model, Feature fusion},
abstract = {The visibility and contrast of images captured in adverse weather such as haze or fog degrade dramatically, which further hinders the accomplishment of high-level computer vision tasks such as object detection and semantic segmentation in these conditions. Many methods have been proposed to solve image dehazing problem by using image translation networks or physical model embedding in CNNs. However, the physical model cannot effectively describe the hazy generation process in complex scenes and estimating the model parameters with only a hazy image is an ill-posed problem. Image translation-based methods may lead to artefacts or colour shifts in the recovered results without the guidance or constraints of physical model information. In this paper, an end-to-end physical model and image translation fused network is proposed to generate realistic haze-free images. Since the transmission map can express the haze distribution in the scene, the proposed method adopts an encoder with a multiscale residual block to extract hazy image features, and two separate decoders to recover a clear image and to estimate the transmission map. The multiscale features of the transmission map and image translation are fused to guide the decode processes with a conditional attention feature fusion block, which is composed of sequential channelwise and spatialwise attention. Moreover, a multitask and multiscale deep supervision mechanism is adopted to enhance the feature fusion and recover more image details. The algorithm can efficiently fuse the physical model information and the hazy image translation to address the problem existent in the methods only based on physical model embedding or direct image translation. Experimental results on the visual quality enhancement of hazy images and semantic segmentation tasks in hazy scenes demonstrate that our model can efficiently recover haze-free images, while performing on par with state-of-the-art methods.}
}
@article{GIRAUD2023109673,
title = {Generalization of the shortest path approach for superpixel segmentation of omnidirectional images},
journal = {Pattern Recognition},
volume = {142},
pages = {109673},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109673},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003746},
author = {Rémi Giraud and Rodrigo {Borba Pinheiro} and Yannick Berthoumieu},
keywords = {3D Spherical images, Superpixels, Unsupervised segmentation, Shape regularity},
abstract = {With the growing use of image capture devices using wide angles and the need for fast and accurate image analysis in computer vision, there is a demand for dedicated under-representation methods. Most decomposition methods segment an image into a small number of irregular homogeneous regions, called superpixels. Nevertheless, these approaches are generally designed to process natural 2D planar images, i.e., captured with a 90o angle view without distortion. In this work, we present SphSPS, a new general decomposition method (for Spherical Shortest Path-based Superpixels)11Available code at: https://github.com/rgiraud/sphsps, that is dedicated to wide 360o omnidirectional or spherical images. The produced superpixels respect both the geometry of the 3D spherical acquisition space, and the boundaries of image objects. To fastly extract relevant clustering features, we generalize the shortest path approach between a pixel and a superpixel center. We demonstrate that considering the geometry of the acquisition space to compute the shortest path enables to jointly improve the segmentation accuracy and the shape regularity of superpixels. To evaluate this regularity property, we propose a generalization of a standard 2D regularity metric to the spherical space, addressing the limitations of the only existing spherical compactness measure. Finally, SphSPS is validated on reference 360o images from the PSD (Panorama Segmentation Dataset) and also on synthetic road omnidirectional images. Our method significantly outperforms both planar and spherical state-of-the-art approaches in terms of segmentation accuracy, robustness to noise and regularity, providing a very interesting tool for superpixel-based applications on 360o images.}
}
@article{WAN2023109733,
title = {High-order interaction feature selection for classification learning: A robust knowledge metric perspective},
journal = {Pattern Recognition},
volume = {143},
pages = {109733},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109733},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004314},
author = {Jihong Wan and Hongmei Chen and Tianrui Li and Min Li and Xiaoling Yang},
keywords = {Feature selection, Fuzzy rough set, High-order interaction, Robust knowledge metric, Uncertainty measures, Classification},
abstract = {Feature selection is an important learning task in data mining and knowledge discovery. Nevertheless, the fuzziness, uncertainty, and noise presented by the data greatly complicate the construction of learning models. Moreover, most works focus on exploring low-order correlations between variables using low-dimensional mutual information, without paying attention to high-order interaction for multiple variables, resulting in the loss of some potentially important dependency information. Driven by these two issues, a robust knowledge metric approach is invented to perceive and excavate the latent information hidden in interaction. In this study, firstly, a robust fuzzy granularity space is constructed from different granular structures induced by different features, and the robust fuzzy uncertainty measures (RFUMs) are successively devised. Then, RFUMs are used to measure pair-wise, three-order, and even higher-order interaction dependencies among features. Further, a constrained high-order interaction evaluation function inspired by the N-gram language model is formulated, and a corresponding high-order interaction feature selection algorithm with RFUMs (HIFS-RFUMs) is designed. Next, comparative experiments with seven representative algorithms on twenty datasets illustrate its effectiveness. In addition, ablation experiments are conducted on the high-order interaction feature selection algorithm with fuzzy uncertainty measures (HIFS-FUMs) and the relative reduction algorithm with RFUMs (R2-RFUMs), which demonstrate the robustness of the metric and the effectiveness for mining high-order interactive features, respectively.}
}
@article{BENATO2023109649,
title = {Deep feature annotation by iterative meta-pseudo-labeling on 2D projections},
journal = {Pattern Recognition},
volume = {141},
pages = {109649},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109649},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003503},
author = {Bárbara C. Benato and Alexandru C. Telea and Alexandre X. Falcão},
keywords = {Pseudo-labeling, Deep feature annotation, Semi-supervised learning, Feature space projection, Data annotation},
abstract = {The absence of large annotated datasets to train deep neural networks (DNNs) is an issue since manual annotation is time-consuming, expensive, and error-prone. Semi-supervised learning techniques can address the problem propagating pseudo labels from supervised to unsupervised samples. However, they still require training and validation sets with many supervised samples. This work proposes a methodology, namely Deep Feature Annotation (DeepFA), that dismisses the validation set and uses very few supervised samples (e.g., 1% of the dataset). DeepFA modifies the feature spaces of a DNN along with meta-pseudo-labeling iterations in a 2D non-linear projection space using the most confidently labeled samples of an optimum-path forest semi-supervised classifier. We present a comprehensive study on DeepFA and a new variant that detects the best DNN model for generalization during the pseudo-labeling iterations. We evaluate components of DeepFA on eight datasets, finding the best DeepFA approach and showing that it outperforms self-pseudo-labeling.}
}
@article{DU2023109644,
title = {Dual-channel embedding learning model for partially labeled attributed networks},
journal = {Pattern Recognition},
volume = {142},
pages = {109644},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109644},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300345X},
author = {Hangyuan Du and Wenjian Wang and Liang Bai},
keywords = {Partially labeled attributed networks, Network embedding, Mutual information, Graph convolution networks, Information redundancy},
abstract = {Network embedding is an important fundamental work in many network application tasks, which encodes the input network from the high-dimensional and sparse topological space into a low-dimensional and dense vector space. Recently, there has been a growing interest in embedding learning on Partially Labeled Attributed Networks (PLANs) due to the increasing occurrence of node attributes and partially available category labels in real-world networks. Semi-supervised embedding learning is a standard approach employed in PLANs, utilizing category labels to supervise the learning process. However, the semi-supervised learning procedure can fail when labels are scarce, noisy, or unreliable. Additionally, most existing embedding algorithms have not successfully integrated heterogeneous information, such as labels, attributes, and structure. To address these issues, we develop a new model, the Dual-Channel Network Embedding (DcNE), which integrates different types of network information into embeddings from a mutual information (MI) perspective. Specifically, we construct a dual-channel information propagation framework to encode the input network in semi-supervised and self-supervised learning paradigms in parallel. Furthermore, a redundancy elimination module is implemented to capture and eliminate the redundant information between the two encoders. Finally, we propose a unified optimization model that integrates the two learning paradigms to collaborate effectively. In the experiments, we demonstrate the effectiveness of DcNE in various network analysis tasks using real-world datasets, establishing its superiority over state-of-the-art baselines.}
}
@article{ZAHRA2023109669,
title = {Person re-identification: A retrospective on domain specific open challenges and future trends},
journal = {Pattern Recognition},
volume = {142},
pages = {109669},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109669},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003709},
author = {Asmat Zahra and Nazia Perwaiz and Muhammad Shahzad and Muhammad Moazam Fraz},
keywords = {Person re-Identification, Literature survey, Deep learning, Open challenges, Specific application-driven},
abstract = {Person Re-Identification (Re-ID) is a critical aspect of visual surveillance systems, which aims to automatically recognize and locate individuals across a multi-camera network with non-overlapping fields-of-view. Despite significant progress in recent years through the use of deep learning-based approaches, there remain many vision-related challenges, such as occlusion, pose, background clutter, misalignment, scale, viewpoint, low resolution & illumination, and cross-domain generalization across camera modalities, that hinder the accurate identification of individuals. The majority of the proposed approaches directly or indirectly aim to solve one or multiple of these existing challenges. To further advance the development of Re-ID solutions, a comprehensive review of the current approaches is necessary. However, no focused review currently exists that analyses and highlights specific aspects for further development. To fill this gap, we present a systematic challenge-specific literature survey of about 300 papers published between 2015 and 2022, which reviews Re-ID approaches from a solution-oriented perspective. This survey is the first of its kind to provide an in-depth analysis of the different approaches used to address the various challenges in Re-ID. Furthermore, our review highlights several prominent and diverse research trends in the Re-ID domain. These trends offer a visionary perspective regarding ongoing person Re-ID research, and they may eventually lead to the development of practical real-world solutions. We highlighted the AI ethics that must be followed while developing a Re-ID solution, and recently being practiced as well. Another exciting future dimension of person Re-ID research is the long-term Re-ID, which is still under evolution. Overall, our survey aims to serve as a valuable resource for researchers and practitioners working in the field of Re-ID and to inspire the development of innovative and effective Re-ID solutions.}
}
@article{CARBONERALUVIZON2023109714,
title = {SSP-Net: Scalable sequential pyramid networks for real-Time 3D human pose regression},
journal = {Pattern Recognition},
volume = {142},
pages = {109714},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109714},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004120},
author = {Diogo {Carbonera Luvizon} and Hedi Tabia and David Picard},
keywords = {3D Human pose estimation, Neural nets, Computer vision},
abstract = {In this paper we propose a highly scalable convolutional neural networks, end-to-end trainable, for real-time 3D human pose regression from still RGB images. We call this approach Scalable Sequential Pyramid Networks (SSP-Net) as it is trained with refined supervision at multiple scales in a sequential manner. Our network requires a single training procedure and is capable of producing its best predictions at 120 frames per second (FPS), or acceptable predictions at more than 200 FPS when cut at test time. We show that the proposed regression approach is invariant to the size of feature maps, allowing our method to perform multi-resolution intermediate supervisions and reaching results comparable to the state-of-the-art with very low resolution feature maps. We demonstrate the accuracy and the effectiveness of our method by providing extensive experiments on two of the most important publicly available datasets for 3D pose estimation, Human3.6M and MPI-INF-3DHP. Additionally, we provide relevant insights about our decisions on the network architecture and show its flexibility to meet the best precision-speed compromise.}
}
@article{LI2023109638,
title = {Correlated and individual feature learning with contrast-enhanced MR for malignancy characterization of hepatocellular carcinoma},
journal = {Pattern Recognition},
volume = {142},
pages = {109638},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109638},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003394},
author = {Yunling Li and Shangxuan Li and Hanqiu Ju and Tatsuya Harada and Honglai Zhang and Ting Duan and Guangyi Wang and Lijuan Zhang and Lin Gu and Wu Zhou},
keywords = {Multimodal fusion, Hepatocellular carcinoma, Deep feature, Malignancy characterization, Contrast-enhanced MR},
abstract = {Malignancy characterization of hepatocellular carcinoma (HCC) is of great importance in patient management and prognosis prediction. In this study, we propose an end-to-end correlated and individual feature learning framework to characterize the malignancy of HCC from Contrast-enhanced MR. From the phases of pre-contrast, arterial and portal venous, our framework simultaneously and explicitly learns both the shareable and phase-specific features that are discriminative to malignancy grades. We evaluate our method on the Contrast enhanced MR of 112 consecutive patients with 117 histologically proven HCCs. Experimental results demonstrate that arterial phase yields better results than portal vein and pre-contrast phase. Furthermore, phase specific components show better discriminant ability than the shareable components. Finally, combining the extracted shareable and individual features components has yielded significantly better performance than traditional feature fusion methods. We also conduct t-SNE analysis and feature scoring analysis to qualitatively assess the effectiveness of the proposed method for malignancy characterization.}
}
@article{WENG2023109670,
title = {A Decomposition Dynamic graph convolutional recurrent network for traffic forecasting},
journal = {Pattern Recognition},
volume = {142},
pages = {109670},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109670},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003710},
author = {Wenchao Weng and Jin Fan and Huifeng Wu and Yujie Hu and Hao Tian and Fu Zhu and Jia Wu},
keywords = {Traffic forecasting, Dynamic graph generation, Residual decomposition, Segmented learning, Graph convolution network},
abstract = {Our daily lives are greatly impacted by traffic conditions, making it essential to have accurate predictions of traffic flow within a road network. Traffic signals used for forecasting are usually generated by sensors along roads, which can be represented as nodes on a graph. These sensors typically produce normal signals representing normal traffic flows and abnormal signals indicating unknown traffic disruptions. Graph convolution networks are widely used for traffic prediction due to their ability to capture correlations between network nodes. However, existing approaches use a predefined or adaptive adjacency matrix that does not accurately reflect real-world relationships between signals. To address this issue, we propose a decomposition dynamic graph convolutional recurrent network (DDGCRN) for traffic forecasting. DDGCRN combines a dynamic graph convolution recurrent network with an RNN-based model that generates dynamic graphs based on time-varying traffic signals, allowing for the extraction of both spatial and temporal features. Additionally, DDGCRN separates abnormal signals from normal traffic signals and models them using a data-driven approach to further improve predictions. Results from our analysis of six real-world datasets demonstrate the superiority of DDGCRN compared to the current state-of-the-art. The source codes are available at: https://github.com/wengwenchao123/DDGCRN.}
}
@article{LIU2023109682,
title = {GlobalAP: Global average precision optimization for person re-identification},
journal = {Pattern Recognition},
volume = {142},
pages = {109682},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109682},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003795},
author = {Yifei Liu and Yaling Liang and Pengfei Wang and Ziheng Chen and Changxing Ding},
keywords = {Person re-identification, Image retrieval, Average precision},
abstract = {Average Precision (AP) measures the overall performance on the Person Re-Identification (ReID) task. Optimizing AP using all instances in the training set is accordingly an excellent choice for learning a discriminative ReID model. However, exploiting this method directly is unacceptable in practice due to the high cost of computation on the entire dataset. To this end, this paper proposes an effective and easy-to-use approach called GlobalAP that optimizes AP globally at negligible computational cost. More specifically, GlobalAP adopts a memory module to acquire the embedding features of all instances in the training set. To reduce the required computational complexity, GlobalAP utilizes only a few instances with high similarities to the query, to compute AP; this is because we observe that only these instances significantly affect AP and model optimization. Moreover, we propose to gradually increase the difficulty of GlobalAP to further encourage intra-class compactness and inter-class separability. Ultimately, GlobalAP can globally optimize AP and dramatically boost the model performance at negligible computational cost. We evaluate GlobalAP on six large-scale ReID datasets. Experimental results show that GlobalAP exhibits obvious advantages in terms of both computational efficiency and ReID accuracy.}
}
@article{WANG2023109626,
title = {RPI-CapsuleGAN: Predicting RNA-protein interactions through an interpretable generative adversarial capsule network},
journal = {Pattern Recognition},
volume = {141},
pages = {109626},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109626},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003278},
author = {Yifei Wang and Xue Wang and Cheng Chen and Hongli Gao and Adil Salhi and Xin Gao and Bin Yu},
keywords = {RNA-protein interactions, elastic net, multi-information fusion, generative adversarial capsule network, interpretable, convolutional block attention module},
abstract = {ABSTRACT
RNA-protein interactions (RPI) play a crucial regulatory role in cellular physiological processes. The study and prediction of RPIs can be insightful for exploring disease mechanisms and drug target design. Traditional RPI prediction methods relied mainly on tedious and expensive biological experiments, and there is an increasing interest in developing more cost-effective computational methods to predict RPIs. This work proposes an interpretable RPI-CapsuleGAN method for RPI prediction based on a generative adversarial capsule network with a convolutional block attention module. First, RPI-CapsuleGAN extracts and fuses multiple features to characterize RNA and protein sequences. Subsequently, the elastic net feature selection method is used to retain features that are highly informative to RPI prediction. Finally, we introduce a convolutional attention mechanism into the generative adversarial capsule network for the first time in order to construct the RPI prediction framework, which is shown to improve the model feature learning of interpretable and expression ability, and effectively solves the problem of the disappearance of the model spatial structure hierarchy. Based on a five-fold cross-validation test, the prediction accuracy of the RPI-CapsuleGAN method reaches 97.1%, 88.8%, 92.5%, 97.3%, and 87.8% for datasets RPI488, RPI369, RPI2241, RPI1807, and RPI1446. The RPI-CapsuleGAN method has higher accuracy than state-of-the-art RPI prediction methods that use the same datasets. In the test dataset NPInter227 constructed in this paper, five groups of test sets are composed of positive samples and five groups of negative samples, the prediction accuracy reaches 97.38%, 96.48%, 97.38%, 97.81%, and 97.15%, respectively, outperforming other mainstream deep learning algorithms. In addition, RPI-CapsuleGAN obtained better results for the prediction of independent test datasets. Extensive experiments detailed here show that RPI-CapsuleGAN can provide an efficient, accurate, and stable method for RPI prediction.}
}
@article{WANG2023109654,
title = {Class-specific and self-learning local manifold structure for domain adaptation},
journal = {Pattern Recognition},
volume = {142},
pages = {109654},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109654},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003552},
author = {Wei Wang and Mengzhu Wang and Xiao Dong and Long Lan and Quannan Zu and Xiang Zhang and Cong Wang},
keywords = {Domain adaptation, Wrongly labeled, Feature-corrupted, Local manifold, Global discriminative, Class-specific local manifold, Self-learning},
abstract = {Domain adaptation (DA) is a powerful technology that allows a classifier trained on a well-labeled source domain to be adapted to an unlabeled target domain with different distributions. Existing DA methods aim to enhance feature transferability by reducing distribution distance, and they often rely on preserving the global discriminative (GD) structure to boost feature discriminability. However, strict GD loss may degrade transferability, and poor discriminability may result from wrongly labeled samples. To address these issues, we propose a new approach called class-specific and self-learning local manifold structure (CSSL-LM), to extract more desirable features for better DA effects. Specifically, it draws two data points close with large weight if they are from the same class and close in the original feature space. This approach is more relaxed than GD and thus mitigates the negative effect on transferability. Moreover, CSSL-LM is more robust to wrongly labeled samples than GD since two data points that are wrongly labeled as the same classes have small weight and are not required to be close. Inspired by previous adaptive local manifold learning, we utilize a self-learning mechanism to model CSSL-LM more accurately and reliably, particularly for feature-corrupted samples. Extensive experiments on four DA benchmarks verify that CSSL-LM outperforms some state-of-the-art methods. We also construct DA datasets that are randomly wrongly labeled or feature-corrupted to further evaluate CSSL-LM’s robustness.}
}
@article{ARCE2023109685,
title = {Learning an artificial neural network to discover bit-quad-based formulas to compute basic object properties},
journal = {Pattern Recognition},
volume = {142},
pages = {109685},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109685},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003837},
author = {Fernando Arce and Wilfrido Gómez-Flores and Uriel Escalona and Humberto Sossa},
keywords = {Shape analysis, Artificial neural network, Computation time, Bit-quads, Area, Perimeter, Contact perimeter},
abstract = {Shape analysis requires estimating object properties in many applications, including optical character recognition, tumor classification, skin cancer recognition, leaf and plant identification, and cell analysis. In particular, bit-quad-based linear expressions proposed by Gray and Duda for calculating the area and perimeter of binary images are widely used in the literature. Nevertheless, these formulas require computing 14 or 15 bit-quad patterns out of 16 possible, becoming critical in applications with limited computing resources. Hence, this paper introduces a method based on a single-layer artificial neural network (ANN) to discover new expressions to calculate the area and perimeter with fewer bit-quads than the original formulas without losing measuring accuracy. Besides, an iterative elimination process removes irrelevant bit-quads whose corresponding weights approach zero. After that, an inductive analysis from observing the learned weights provides interpretable formulas for the area and perimeter. Furthermore, the proposed approach is also applied to find bit-quad-based formulas for directly computing the contact perimeter property, whose original formula requires precomputing Gray’s area and perimeter of the object. In addition, aiming to show our method’s versatility in other applications, we address a real-world problem to discover a bit-quad-based formula to distinguish between two classes of loss of bone density caused by hyperthyroidism and aging in rats. The experimental results show that the proposed approach reduces by approximately half the bit-quads needed to calculate the area and perimeter of Gray and Duda. Likewise, the number of bit-quads to compute the contact perimeter is reduced from nine to six. Besides, the estimated value on test sets by all the found formulas is the same as their original counterparts. On the other hand, the classification of loss of bone density type using the found bit-quad-based formula reaches an accuracy of 100% in the test set. Therefore, the proposed method is an alternative to finding linear expressions with few bit-quads to measure basic object properties.}
}
@article{SHI2023109702,
title = {Global- and local-aware feature augmentation with semantic orthogonality for few-shot image classification},
journal = {Pattern Recognition},
volume = {142},
pages = {109702},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109702},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004004},
author = {Boyao Shi and Wenbin Li and Jing Huo and Pengfei Zhu and Lei Wang and Yang Gao},
keywords = {Few-shot image classification, Transfer learning, Feature augmentation, Semantic orthogonal learning},
abstract = {As for few-shot image classification, recently, some works revisit the standard transfer learning paradigm, i.e., pre-training and fine-tuning, and have achieved some success. However, we find that this kind of methods heavily relies on a naive image-level data augmentation (e.g., cropping and flipping) at the fine-tuning stage, which will easily suffer from the overfitting problem because of the limited-data regime. To tackle this issue, in this paper, we attempt to perform a novel feature-level semantic augmentation at the fine-tuning stage and propose a Global- and Local-aware Feature Augmentation method (GLFA) from both the channel- and spatial-wise perspectives. In addition, at the pre-training stage, we further propose a Semantic Orthogonal Learning Framework (SOLF) to make the learned feature channels more independently, orthogonal and diverse. Extensive experiments demonstrate that the proposed method can obtain significant performance improvements over the state of the arts. Code is available at https://github.com/onlyyao/GLFA-SOLF.}
}
@article{SUN2023109726,
title = {Attentional prototype inference for few-shot segmentation},
journal = {Pattern Recognition},
volume = {142},
pages = {109726},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109726},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004247},
author = {Haoliang Sun and Xiankai Lu and Haochen Wang and Yilong Yin and Xiantong Zhen and Cees G.M. Snoek and Ling Shao},
keywords = {Few-shot segmentation, Variational inference, Probabilistic model, Latent attention},
abstract = {This paper aims to address few-shot segmentation. While existing prototype-based methods have achieved considerable success, they suffer from uncertainty and ambiguity caused by limited labeled examples. In this work, we propose attentional prototype inference (API), a probabilistic latent variable framework for few-shot segmentation. We define a global latent variable to represent the prototype of each object category, which we model as a probabilistic distribution. The probabilistic modeling of the prototype enhances the model’s generalization ability by handling the inherent uncertainty caused by limited data and intra-class variations of objects. To further enhance the model, we introduce a local latent variable to represent the attention map of each query image, which enables the model to attend to foreground objects while suppressing the background. The optimization of the proposed model is formulated as a variational Bayesian inference problem, which is established by amortized inference networks. We conduct extensive experiments on four benchmarks, where our proposal obtains at least competitive and often better performance than state-of-the-art prototype-based methods. We also provide comprehensive analyses and ablation studies to gain insight into the effectiveness of our method for few-shot segmentation.}
}
@article{YU2023109666,
title = {eX-ViT: A Novel explainable vision transformer for weakly supervised semantic segmentation},
journal = {Pattern Recognition},
volume = {142},
pages = {109666},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109666},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003679},
author = {Lu Yu and Wei Xiang and Juan Fang and Yi-Ping Phoebe Chen and Lianhua Chi},
keywords = {Explainable, Attention map, Transformer, Weakly supervised},
abstract = {Recently vision transformer models have become prominent models for a multitude of vision tasks. These models, however, are usually opaque with weak feature interpretability, making their predictions inaccessible to the users. While there has been a surge of interest in the development of post-hoc solutions that explain model decisions, these methods can not be broadly applied to different transformer architectures, as rules for interpretability have to change accordingly based on the heterogeneity of data and model structures. Moreover, there is no method currently built for an intrinsically interpretable transformer, which is able to explain its reasoning process and provide a faithful explanation. To close these crucial gaps, we propose a novel vision transformer dubbed the eXplainable Vision Transformer (eX-ViT), an intrinsically interpretable transformer model that is able to jointly discover robust interpretable features and perform the prediction. Specifically, eX-ViT is composed of the Explainable Multi-Head Attention (E-MHA) module, the Attribute-guided Explainer (AttE) module with the self-supervised attribute-guided loss. The E-MHA tailors explainable attention weights that are able to learn semantically interpretable representations from tokens in terms of model decisions with noise robustness. Meanwhile, AttE is proposed to encode discriminative attribute features for the target object through diverse attribute discovery, which constitutes faithful evidence for the model predictions. Additionally, we have developed a self-supervised attribute-guided loss for our eX-ViT architecture, which utilizes both the attribute discriminability mechanism and the attribute diversity mechanism to enhance the quality of learned representations. As a result, the proposed eX-ViT model can produce faithful and robust interpretations with a variety of learned attributes. To verify and evaluate our method, we apply the eX-ViT to several weakly supervised semantic segmentation (WSSS) tasks, since these tasks typically rely on accurate visual explanations to extract object localization maps. Particularly, the explanation results obtained via eX-ViT are regarded as pseudo segmentation labels to train WSSS models. Comprehensive simulation results illustrate that our proposed eX-ViT model achieves comparable performance to supervised baselines, while surpassing the accuracy and interpretability of state-of-the-art black-box methods using only image-level labels.}
}
@article{MA2023109697,
title = {Inter-layer transition in neural architecture search},
journal = {Pattern Recognition},
volume = {143},
pages = {109697},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109697},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003953},
author = {Benteng Ma and Jing Zhang and Yong Xia and Dacheng Tao},
keywords = {Image processing, Image classification, Neural network, Neural architecture search},
abstract = {Neural architecture search (NAS) attracts much research attention, contributing to its ability to identify better architectures than manually-designed ones. Recently, differential neural architecture search methods have been widely used due to their impressive effectiveness and performance. They represent the network architecture as a repetitive proxy-directed acyclic graph (DAG) and optimize the network weights and architecture weights alternatively in a differential manner. However, existing methods model the architecture weights on each edge (i.e., a layer in the network) as statistically independent variables, ignoring the dependency between edges in DAG induced by their directed topological connections. In this paper, we make the first attempt to investigate such a dependency or relationship by proposing a novel inter-layer transition NAS method. It casts the architecture optimization into a sequential decision process where the dependency between the architecture weights of connected edges is explicitly modeled. Specifically, edges are divided into inner and outer groups according to whether or not their predecessor edges are in the same cell. While the architecture weights of outer edges are optimized independently, those of inner edges are derived sequentially based on the architecture weights of their predecessor edges and the learnable transition matrices in an attentive probability transition manner. Experiments on five benchmark classification datasets, four searching spaces, and NAS-Bench-201 confirm the value of modeling inter-layer dependency and demonstrate the proposed method outperforms other methods.}
}
@article{SEPULVEDA2023109642,
title = {Generalization of deep learning models for natural gas indication in 2D seismic data},
journal = {Pattern Recognition},
volume = {141},
pages = {109642},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109642},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003436},
author = {Luis Fernando Marin Sepulveda and Marcelo Gattass and Aristofanes Correa Silva and Roberto Quevedo and Diogo Michelon and Carlos Siedschlag and Roberto Ribeiro},
keywords = {Autoencoder, Generalizability, Dataset training recommendation, 2D Seismic onshore data, Deep learning, Clustering},
abstract = {Methods based on Machine Learning and Deep Learning are increasingly popular to help interpret large volumes of data that belong to various areas and seek to fulfill multiple tasks. One of these areas studies seismic data in the search for hydrocarbon reserves, for which Deep Learning models are trained, showing acceptable results for low study data. However, these models present generalization problems. Their performance tends to decrease when used on seismic data from new exploration. This tendency is particularly true for 2D data, which have many features. This work presents a method to improve the generalization of the Deep Learning model for the indication of natural gas in 2D seismic data based on the recommendation of training data and hyperparameter operations of the model. The tests used a database of the Parnaíba basin in northeast Brazil. Experiments showed an increase in the correct indication of natural gas that varies according to the metric 8%≤Recall≤37%, with a fluctuation in the increase of false positives of −2%≤Precision≤13%. It is an improvement in the generalization of the Deep Learning model of up to 11% according to the F1 score metric or up to 10% according to the IoU metric.}
}
@article{ZHANG2023109667,
title = {Slide deep reinforcement learning networks: Application for left ventricle segmentation},
journal = {Pattern Recognition},
volume = {141},
pages = {109667},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109667},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003680},
author = {Wanjun Zhang and Xiaohua Ding and Yang Liu and Baojun Qiao},
keywords = {Slide deep reinforcement learning, Pixelwise image segmentation, Left ventricle, Reinforcement learning state, Reinforcement learning action},
abstract = {Automatic segmentation of the left ventricle (LV) in four-chamber view images is critical for computer-aided cardiac disease diagnosis. The complex structure of the cardiac image and the encoder-decoder networks may cause coarse segmentation results. High-accuracy LV segmentation is still a challenge with existing automatic LV segmentation methods. In this paper, we propose a slide deep reinforcement learning segmentation network for pixelwise LV segmentation. The main architecture of the slide reinforcement learning networks consists of a slider item combined state, a group of morphology transforming actions and an agent network. The specifically designed reinforcement learning state comprises an image item and a slider item, which contains both original image information and network act information. The reinforcement learning actions proposed in this paper enable accurate and fast formulation of the binary segment result for each frame by controlling the length and location of the slider. Additionally, the confidence branch proposed in our experiment provides a continuous frame series environment, and the identification algorithm avoids losing the segmentation target. The segmentation result reveals that the proposed method outperforms FCN, SegNet, U-Net and TransUnet. The IoU improved by 23.01%, 15.4%, 11.24% and 6.9%. Additionally, we demonstrate how the proposed method can be used as a semisupervised method, which is more convenient for the image annotation process.}
}
@article{NADEEM2023109655,
title = {Cross domain 2D-3D descriptor matching for unconstrained 6-DOF pose estimation},
journal = {Pattern Recognition},
volume = {142},
pages = {109655},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109655},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003564},
author = {Uzair Nadeem and Mohammed Bennamoun and Roberto Togneri and Ferdous Sohel and Aref {Miri Rekavandi} and Farid Boussaid},
keywords = {2D-3D Matching, Cross-Domain feature matching, 6-DOF Pose estimation, Image localization, Camera localization, Visual localization},
abstract = {This paper presents a novel approach for cross-domain descriptor matching between 2D and 3D modalities. The 2D-3D matching is applied to localize 2D images in 3D point clouds. Direct cross-domain matching allows our technique to localize images in any type of 3D point cloud without any constraints on the nature or mechanism by which it is obtained. We propose a learning based framework, called Desc-Matcher, to directly match features between the two modalities. A dataset of 2D and 3D features with corresponding locations in images and point clouds is generated to train the Desc-Matcher. To estimate the pose of an image in any 3D cloud, keypoints and feature descriptors are extracted from the query image and the point cloud. The trained Desc-Matcher is then used to match the features from the image and the point cloud. A robust pose estimator is used to predict the location and orientation of the query image from the corresponding positions of the matched 2D and 3D features. We carried out an extensive evaluation of the proposed method for indoor and outdoor scenarios and with different types of point clouds to verify the feasibility of our approach. Experimental results show that the proposed approach can reliably estimate the 6-DOF poses of query cameras in any type of 3D point cloud with high precision. We achieved average median errors of 1.09cm/0.27∘ and 19cm/0.39∘ on the Stanford and Cambridge datasets, respectively.}
}
@article{LI2023109631,
title = {Multi-hypothesis representation learning for transformer-based 3D human pose estimation},
journal = {Pattern Recognition},
volume = {141},
pages = {109631},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109631},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003321},
author = {Wenhao Li and Hong Liu and Hao Tang and Pichao Wang},
keywords = {3D Human pose estimation, Transformer, Multi-Hypothesis, Self-Hypothesis, Cross-Hypothesis},
abstract = {Despite significant progress, estimating 3D human poses from monocular videos remains a challenging task due to depth ambiguity and self-occlusion. Most existing works attempt to solve both issues by exploiting spatial and temporal relationships. However, those works ignore the fact that it is an inverse problem where multiple feasible solutions (i.e., hypotheses) exist. To relieve this limitation, we propose a Multi-Hypothesis Transformer that learns spatio-temporal representations of multiple plausible pose hypotheses. In order to effectively model multi-hypothesis dependencies and build strong relationships across hypothesis features, we introduce a one-to-many-to-one three-stage framework: (i) Generate multiple initial hypothesis representations; (ii) Model self-hypothesis communication, merge multiple hypotheses into a single converged representation and then partition it into several diverged hypotheses; (iii) Learn cross-hypothesis communication and aggregate the multi-hypothesis features to synthesize the final 3D pose. Through the above processes, the final representation is enhanced and the synthesized pose is much more accurate. Extensive experiments show that the proposed method achieves state-of-the-art results on two challenging datasets: Human3.6M and MPI-INF-3DHP. The code and models are available at https://github.com/Vegetebird/MHFormer.}
}
@article{LIU2023109739,
title = {FedCL: Federated contrastive learning for multi-center medical image classification},
journal = {Pattern Recognition},
volume = {143},
pages = {109739},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109739},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004375},
author = {Zhenbing Liu and Fengfeng Wu and Yumeng Wang and Mengyu Yang and Xipeng Pan},
keywords = {Federated learning, Contrastive learning, Image classification},
abstract = {Federated learning, which allows distributed medical institutions to train a shared deep learning model with privacy protection, has become increasingly popular recently. However, in practical application, due to data heterogeneity between different hospitals, the performance of the model will be degraded in the training process. In this paper, we propose a federated contrastive learning (FedCL) approach. FedCL integrates the idea of contrastive learning into the federated learning framework. Specifically, it combines the local model and the global model for contrastive learning, so that the local model gradually approaches the global model with the increase of communication rounds, which improves the generalization ability of the model. We validate our method on two public datasets. Extensive experiments show that our method is superior to other federated learning algorithms in medical image classification.}
}
@article{SU2023109686,
title = {Neighborhood-based credibility anchor learning for universal domain adaptation},
journal = {Pattern Recognition},
volume = {142},
pages = {109686},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109686},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003849},
author = {Wan Su and Zhongyi Han and Rundong He and Benzheng Wei and Xueying He and Yilong Yin},
keywords = {Universal domain adaptation, Threshold-free, Neighborhood learning},
abstract = {Universal domain adaptation (UniDA) aims to transfer knowledge from the source domain to the target domain in the presence of distribution shift and class mismatch. Most existing works design threshold-relied methods to reject target private classes by carefully-proposed uncertainty scoring functions which are very sensitive to thresholds. To overcome this problem, a few threshold-free methods are proposed but ignore the neighborhood structure information of the target domain, leading to poor performance. In this paper, we propose Neighborhood-based Credibility Anchor Learning (NCAL), a new threshold-free framework that fully mines the neighborhood structure information to explore better target representations. NCAL contains three key components: a class anchor learning module to learn target class distribution, a credibility-weighted conditional adversarial module to learn class-invariant features of common classes, and an open-set neighborhood clustering module to learn well-clustered features. Extensive experiments demonstrate that our method outperforms the state-of-the-art.}
}
@article{KIM2023109659,
title = {Improved robustness of vision transformers via prelayernorm in patch embedding},
journal = {Pattern Recognition},
volume = {141},
pages = {109659},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109659},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003606},
author = {Bum Jun Kim and Hyeyeon Choi and Hyeonah Jang and Dong Gu Lee and Wonseok Jeong and Sang Woo Kim},
keywords = {Vision transformer, Patch embedding, Contrast enhancement, Robustness, Layer normalization, Convolutional neural network, Deep learning},
abstract = {Vision Transformers (ViTs) have recently demonstrated state-of-the-art performance in various vision tasks, replacing convolutional neural networks (CNNs). However, because ViT has a different architectural design than CNN, it may behave differently. To investigate whether ViT has a different performance or robustness, we tested ViT and CNN under various imaging conditions in practical vision tasks. We confirmed that for most image transformations, ViT’s robustness was comparable or even better than that of CNN. However, for contrast enhancement, ViT performed particularly poorly. We show that this is because positional embedding in ViT’s patch embedding can work improperly when the color scale changes. We demonstrate that the use of PreLayerNorm, a modified patch embedding structure, ensures the consistent behavior of ViT. Results demonstrate that ViT with PreLayerNorm exhibited improved robustness in the contrast-varying environments.}
}
@article{MENG2023109630,
title = {SiamRank: A siamese based visual tracking network with ranking strategy},
journal = {Pattern Recognition},
volume = {141},
pages = {109630},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109630},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300331X},
author = {Feiyu Meng and Xiaomei Gong and Yi Zhang},
keywords = {Visual tracking, Siamese, Ranking, Classification, State estimation},
abstract = {Visual tracking is one of the most fundamental and active research topics in the field of computer vision with industrial applications. It has to solve 2 core problems, namely classification and state estimation. Most of the existing trackers utilize deep networks to extract the features of the object. Especially, Siamese based approaches have prevailed in tracking tasks, which generate labels for both positive and negative samples. However, these approaches introduce ambiguities and inaccurate semantic information at the same time, which may cause failure in classification. To address this problem, we present SiamRank by adding the sequential information of different samples in one image. We apply the proposed network to 2 backbones (AlexNet and GoogLeNet) to testify its general performance. Extensive experiments have been carried out on 7 popular benchmarks, including OTB100, LaSOT, GOT-10 K, TrackingNet, NFS, UAV123 and VOT2019, and our tracker achieves state-of-the-art results. Specifically, on both large-scale TrackingNet dataset and long-time LaSOT dataset, SiamRank surpasses the previous approaches with a relative gain of 10%, while running at 65 FPS.}
}
@article{MONDAL2023109698,
title = {Dataset agnostic document object detection},
journal = {Pattern Recognition},
volume = {142},
pages = {109698},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109698},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003965},
author = {Ajoy Mondal and Madhav Agarwal and C.V. Jawahar},
keywords = {Document object detection, Table detection, Figure detection, Equation detection, Cascade Mask , Deformable convolution},
abstract = {Localizing document objects such as tables, figures, and equations is a primary step for extracting information from document images. We propose a novel end-to-end trainable deep network, termed Document Object Localization Network (dolnet), for detecting various objects present in the document images. The proposed network is a multi-stage extension of Mask r-cnn with a dual backbone having deformable convolution for detecting document objects with high detection accuracy at a higher IoU threshold. We also empirically evaluate the proposed dolnet on the publicly available benchmark datasets. The proposed DOLNet achieves state-of-the-art performance for most of the bench-mark datasets under various existing experimental environments. Our solution has three important properties: (i) a single trained model dolnet‡ that performs well across all the popular benchmark datasets, (ii) reports excellent performances across multiple, including with higher IoU thresholds, and (iii) consistently demonstrate the superior quantitative performance by following the same protocol of the recent works for each of the benchmarks.}
}
@article{AHERRAHROU2023109643,
title = {A novel cancelable finger vein templates based on LDM and RetinexGan},
journal = {Pattern Recognition},
volume = {142},
pages = {109643},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109643},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003448},
author = {N. Aherrahrou and H. Tairi},
keywords = {Cancelable biometric, Security, Template protection, Finger vein, LDM, RetinexGan},
abstract = {In this paper, we propose a new biometric template protection scheme, which can deal with the finger vein biometric security threats, through using the LDM and RetinexGAN model. The RetinexGAN model is mainly used to handle the illumination and low contrast problems effectively, while efficiently extracting discriminative features from the finger vein images. The projection of extracted features into dissimilarity space is done using Local Dissimilarity Map (LDM). LDM is an efficient way for finger vein features representation, which investigates the relationships and correlation inter and intra classes, while effectively coming up with the accidental shifts/rotations caused by the arbitrary position of the finger during image acquisition. The proposed approach is successfully evaluated in terms of non-invertibility, non-linkability, revocability and performances. Experimental results and comparison analysis with the state of arts methods confirm that the proposed framework can achieve promising results.}
}
@article{ZHANG2023109671,
title = {Orthonormal product quantization network for scalable face image retrieval},
journal = {Pattern Recognition},
volume = {141},
pages = {109671},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109671},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003722},
author = {Ming Zhang and Xuefei Zhe and Hong Yan},
keywords = {Product quantization, Face image retrieval, Orthonormal codewords, Convolutional neural networks},
abstract = {Existing deep quantization methods provided an efficient solution for large-scale image retrieval. However, the significant intra-class variations, like pose, illumination, and expressions in face images, still pose a challenge. In light of this, face image retrieval requires sufficiently powerful learning metrics, which are absent in current deep quantization works. Moreover, to tackle the growing unseen identities in the query stage, face image retrieval drives more demands regarding model generalization and scalability than general image retrieval tasks. This paper integrates product quantization with orthonormal constraints into an end-to-end deep learning framework to effectively retrieve face images. Specifically, we propose a novel scheme that uses predefined orthonormal vectors as codewords to enhance the quantization informativeness and reduce codewords’ redundancy. A tailored loss function maximizes discriminability among identities in each quantization subspace for both the quantized and original features. An entropy-based regularization term is imposed to reduce the quantization error. Experiments are conducted on four commonly-used face datasets under both seen and unseen identity retrieval settings. Our method outperforms all the compared state-of-the-art under both settings. The proposed orthonormal codewords consistently boost both models’ standard retrieval performance and generalization ability, demonstrating the superiority of our method for scalable face image retrieval.}
}
@article{DONG2023109720,
title = {Generalization capacity of multi-class SVM based on Markovian resampling},
journal = {Pattern Recognition},
volume = {142},
pages = {109720},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109720},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004181},
author = {Zijie Dong and Chen Xu and Jie Xu and Bin Zou and Jingjing Zeng and Yuan Yan Tang},
keywords = {MSVM, Markovian resampling, Learning rate, Generalization bound},
abstract = {The generalization performance of “All-in-one” Multi-class SVM (AIO-MSVM) based on uniformly ergodic Markovian chain (u.e.M.c.) samples is considered. We establish the fast learning rate of AIO-MSVM algorithm with u.e.M.c. samples and prove that AIO-MSVM algorithm with u.e.M.c. samples is consistent. We also propose a novel AIO-MSVM algorithm based on q-times Markovian resampling (AIO-MSVM-MR), and show the numerical investigation on the learning performance of AIO-MSVM-MR based on public datasets. The experimental studies indicate that compared to the classical AIO-MSVM algorithm and other MSVM algorithms, the proposed AIO-MSVM-MR algorithm has not only smaller misclassification rate, but also less sampling and training total time. We present some discussions on the case of unbalanced training samples, the choices of q and two technical parameters, and present some explanations on the learning performance of the proposed algorithm.}
}
@article{RASHIDI2023109694,
title = {An active foveated gaze prediction algorithm based on a Bayesian ideal observer},
journal = {Pattern Recognition},
volume = {143},
pages = {109694},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109694},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003928},
author = {Shima Rashidi and Weilun Xu and Dian Lin and Andrew Turpin and Lars Kulik and Krista Ehinger},
keywords = {Eye movements, Visual search, Bayesian ideal observer},
abstract = {Predicting human eye movements is a crucial task for understanding human behavior and has numerous applications in machine vision. Most current models for predicting eye movements are data-driven and require large datasets of recorded eye movements, which can be expensive and time-consuming to collect. In this paper, we present a novel theory-based model for predicting eye movements in a foveated visual system that maximizes information gain at each fixation. Our model uses a region-proposal network and eccentricity-based max pooling to account for the loss of detail in peripheral vision. We apply our model to predict human fixations in a visual search task for objects in real-world scenes. Unlike data-driven models, our model does not require training on large eye movement datasets and can generalize to any set of natural images and targets. We evaluate the generalization capability of our model by demonstrating its results on two publicly available visual search datasets, Ehinger and COCO-search18, without any further training on those datasets. Our model outperforms or performs comparably to data-driven models that are directly trained on human eye movement datasets.}
}
@article{XIANG2023109608,
title = {Margin-aware rectified augmentation for long-tailed recognition},
journal = {Pattern Recognition},
volume = {141},
pages = {109608},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109608},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003096},
author = {Liuyu Xiang and Jungong Han and Guiguang Ding},
keywords = {Long-tailed recognition, Data augmentation, Mixup},
abstract = {The long-tailed data distribution is prevalent in real world and it poses great challenge on deep neural network training. In this paper, we propose Margin-aware Rectified Augmentation (MRA) to tackle this problem. Specifically, the MRA consists of two parts. From the data perspective, we analyze that data imbalance will cause the decision boundary be biased, and we propose a novel Margin-aware Rectified mixup (MR-mixup) that adaptively rectifies the biased decision boundary. Furthermore, from the model perspective, we analyze that the imbalance will also lead to consistent ‘gradient suppression’ on minority class logits. Then we propose Reweighted Mutual Learning (RML) that provides extra ‘soft target’ as supervision signal and augments the ‘encouraging gradients’ on the minority classes. We conduct extensive experiments on benchmark datasets CIFAR-LT, ImageNet-LT and iNaturalist18. The results demonstrate that the proposed MRA not only achieves state-of-the-art performance, but also yields a better-calibrated prediction.}
}
@article{LIU2023109636,
title = {BDNet: A BERT-based dual-path network for text-to-image cross-modal person re-identification},
journal = {Pattern Recognition},
volume = {141},
pages = {109636},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109636},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003370},
author = {Qiang Liu and Xiaohai He and Qizhi Teng and Linbo Qing and Honggang Chen},
keywords = {Person re-identification, Image-text retrieval, Cross-modality, Attention},
abstract = {Text-to-image person re-identification (TI-ReID) aims to provide a descriptive sentence to find a specific person in the gallery. The task is very challenging due to the huge feature differences between both image and text descriptions. Currently, most approaches use the idea of combining global and local features to get more fine-grained features. However, these methods usually acquire local features with the help of human pose or segmentation models, which makes it difficult to use in realistic scenarios due to the introduction of additional models or complex training evaluation strategies. To facilitate practical applications, we propose a BERT-based framework for dual-path TI-ReID. Without the help of additional models, our approach directly employs visual attention in the global feature extraction network to allow the network to adaptively learn to focus on salient local features in image and text descriptions, which enhances the network’s attention to local information through a visual attention mechanism, thus strengthening the global feature representation and effectively improving the global feature representation. In addition, to learn text and image modality invariant feature representations, we propose a convolutional shared network (CSN) to learn image and text features together. To optimize cross-modal feature distances more effectively, we propose a global hybrid modal triplet global metric loss. In addition to combining local metric learning and global metric learning, we also introduce the CMPM loss and CMPC loss to jointly optimize the proposed model. Extensive experiments on the CUHK-PEDES dataset show that the proposed method performs significantly better than the current research results, achieving a Rank-1/mAP accuracy of 66.27%/ 57.04%.}
}
@article{ZHANG2023109664,
title = {Faster OreFSDet: A lightweight and effective few-shot object detector for ore images},
journal = {Pattern Recognition},
volume = {141},
pages = {109664},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109664},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003655},
author = {Yang Zhang and Le Cheng and Yuting Peng and Chengming Xu and Yanwei Fu and Bo Wu and Guodong Sun},
keywords = {Ore images, Few-shot object detection, Real-time, Light-weight},
abstract = {For the ore particle size detection, obtaining a sizable amount of high-quality ore labeled data is time-consuming and expensive. General object detection methods often suffer from severe over-fitting with scarce labeled data. Despite their ability to eliminate over-fitting, existing few-shot object detectors encounter drawbacks such as slow detection speed and high memory requirements, making them difficult to implement in a real-world deployment scenario. To this end, we propose a lightweight and effective few-shot detector to achieve competitive performance with general object detection with only a few samples for ore images. First, the proposed support feature mining block characterizes the importance of location information in support features. Next, the relationship guidance block makes full use of support features to guide the generation of accurate candidate proposals. Finally, the dual-scale semantic aggregation module retrieves detailed features at different resolutions to contribute with the prediction process. Experimental results show that our method consistently exceeds the few-shot detectors with an excellent performance gap on all metrics. Moreover, our method achieves the smallest model size of 19 MB as well as being competitive at 50 FPS detection speed compared with general object detectors.}
}
@article{NAPOLES2023109640,
title = {Presumably correct decision sets},
journal = {Pattern Recognition},
volume = {141},
pages = {109640},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109640},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003412},
author = {Gonzalo Nápoles and Isel Grau and Agnieszka Jastrzębska and Yamisleydi Salgueiro},
keywords = {Data analysis, Granular computing, Decision sets, Rough sets},
abstract = {The paper presents the presumably correct decision sets as a tool to analyze uncertainty in the form of inconsistency in decision systems. As a first step, problem instances are gathered into three regions containing weak members, borderline members, and strong members. This is accomplished by using the membership degrees of instances to their neighborhoods while neglecting their actual labels. As a second step, we derive the presumably correct and incorrect sets by contrasting the decision classes determined by a neighborhood function with the actual decision classes. We extract these sets from either the regions containing strong members or the whole universe, which defines the strict and relaxed versions of our theoretical formalism. These sets allow isolating the instances difficult to handle by machine learning algorithms as they are responsible for inconsistent patterns. The simulations using synthetic and real-world datasets illustrate the advantages of our model compared to rough sets, which is deemed a solid state-of-the-art approach to cope with inconsistency. In particular, it is shown that we can increase the accuracy of selected classifiers up to 36% by weighting the presumably correct and incorrect instances during the training process.}
}
@article{ZIHAO2023109648,
title = {Multi-directional broad learning system for the unsupervised stereo matching method},
journal = {Pattern Recognition},
volume = {142},
pages = {109648},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109648},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003497},
author = {Zhang zihao and Niu Ying and Meng Fanman and Yang Tiejun and Fan Chao and Ren Xiaozhen and Wu Ruiqi and Cao Kun and Wang Haocheng},
keywords = {Multi-directional broad learning system, Unsupervised stereo matching, Local gravity weight method},
abstract = {The supervised stereo matching methods usually rely on ground truth disparity maps as the training labels, this limits its practical application in many situations. In this study, we propose a novel unsupervised stereo matching method based on a multi-directional broad learning system. A multi-directional broad learning system was constructed to generate multiple candidate disparity maps. During the generation of each candidate disparity map, an update criterion is proposed for the disparity value based on the maximum similarity of the inverse mapping region to remove the abnormal disparity values of the training samples. Subsequently, multi-direction consistency verification is performed to further eliminate abnormal disparity values, which are based on the uniqueness principle of disparity truth values at the same location. Finally, an invalid depth redefinition based on a local gravity weight method is introduced to select the appropriate disparity value to fill the invalid pixel positions from their neighborhood, which is calculated based on the local region of the color, matching cost, and geometric spaces in the stereo images. We provide the results of experiments on both indoor and outdoor scenarios to demonstrate the effectiveness and flexibility of our approach, including comparisons with state-of-the-art methods.}
}
@article{HUANG2023109676,
title = {Robust unsupervised feature selection via data relationship learning},
journal = {Pattern Recognition},
volume = {142},
pages = {109676},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109676},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003771},
author = {Pei Huang and Zhaoming Kong and Mengying Xie and Xiaowei Yang},
keywords = {Unsupervised feature selection, Outlier, Robustness},
abstract = {Unsupervised feature selection robust to many outliers is a challenging task. The crucial difficulty is learning a robust subspace, which preserves local structure. The most common solution is to reduce fitting error by applying different robust norms. However, there are three shortcomings. Firstly, they are not robust enough when outliers distributed both randomly and concentratedly are widely present. Secondly, outlier removal is not considered. Thirdly, it is not easy to understand and choose an euclidean distance threshold that decides a sample as an outlier in different scenarios. The first two shortcomings make previous methods fail to achieve their expected learning results, and the third one increases the application difficulty in different fields. To address these issues, a robust unsupervised feature selection via data relationship learning (RUFSDR) is proposed in this paper. Specifically, scores representing the data’s importance will be learned and assigned to each sample. Inliers will be given different positive scores. Outliers will be given 0 such that a subspace, which preserves the local structure better, can be learned without prior knowledge about the distance threshold. The experiments conducted on various datasets with several scenarios show the superiority of RUFSDR.}
}
@article{LIAO2023109624,
title = {Tensor completion via convolutional sparse coding with small samples-based training},
journal = {Pattern Recognition},
volume = {141},
pages = {109624},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109624},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003254},
author = {Tianchi Liao and Zhebin Wu and Chuan Chen and Zibin Zheng and Xiongjun Zhang},
keywords = {Tensor completion, Convolutional sparse coding, High-pass filter, Inexact ADMM},
abstract = {Tensor data often suffer from missing value problems due to the complex high-dimensional structure while acquiring them. To complete the missing information, lots of Low-Rank Tensor Completion (LRTC) methods have been proposed, most of which depend on the low-rank property of tensor data. In this way, the low-rank component of the original data could be recovered roughly. However, the shortcoming is that the detailed information can not be fully restored, no matter the Sum of the Nuclear Norm (SNN) nor the Tensor Nuclear Norm (TNN) based methods. On the contrary, in the field of signal processing, Convolutional Sparse Coding (CSC) can provide a good representation of the high-frequency component of the image, which is generally associated with the detail component of the data. To this end, we propose two novel methods, LRTC-CSC-I and LRTC-CSC-II, which adopt CSC as a supplementary regularization for LRTC to capture the high-frequency components. Therefore, the LRTC-CSC methods can not only solve the missing value problem but also recover the details. Moreover, the regularizer CSC can be trained with small samples due to the sparsity characteristic. Extensive experiments show the effectiveness of LRTC-CSC methods, and quantitative evaluation indicates that the performance of our models are superior to state-of-the-art methods.}
}
@article{LI2023109652,
title = {Knowledge transduction for cross-domain few-shot learning},
journal = {Pattern Recognition},
volume = {141},
pages = {109652},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109652},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003539},
author = {Pengfang Li and Fang Liu and Licheng Jiao and Shuo Li and Lingling Li and Xu Liu and Xinyan Huang},
keywords = {Few-shot learning, Domain adaptation, Feature adaptation, Feature transduction, Feed-forward attention, Deep sparse representation},
abstract = {Cross-Domain Few-Shot Learning (CDFSL) aims to classify new categories from new domains with few samples. It confronts a greater domain shift than Few-Shot Learning (FSL). Based on the transfer learning framework, we propose a Knowledge Transduction method (KT) to alleviate domain shift and achieve few-shot recognition. First, a feature adaptation module based on feed-forward attention is constructed to learn domain-adapted features. The feature adaptation module weakens domain shift by transducing knowledge from an auxiliary dataset to the new dataset. Second, a feature transduction module based on deep sparse representation is developed to gather class semantics from limited support images. The feature transduction module transduces knowledge from support images to query images for few-shot recognition. In addition, a stochastic image augmentation method is proposed for FSL to train a more generalized model through consistency representation learning. Our method achieves competitive accuracy on four CDFSL datasets and four FSL datasets compared to state-of-the-art methods. The source code is available at https://github.com/XDUpfLi/KT.}
}
@article{HAGHPANAH2023109683,
title = {Determining the trustworthiness of DNNs in classification tasks using generalized feature-based confidence metric},
journal = {Pattern Recognition},
volume = {142},
pages = {109683},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109683},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003813},
author = {Mohammad Amin Haghpanah and Mehdi {Tale Masouleh} and Ahmad Kalhor},
keywords = {Machine learning, Deep learning, Confidence metric, Generalized feature-based confidence, Model trust score, Feature quality evaluation},
abstract = {Determining the confidence of Deep Neural Networks in predictions is crucial for building reliable and robust systems. However, it has received minor attention among other areas related to Deep Learning. The confidence of DNNs in predictions is highly correlated with their ability in feature extraction. Consequently, a more robust feature extractor in DNNs leads to a more confident and trustworthy model. In this study, a method is designed in order to determine the trustworthiness of DNNs based on the quality of their feature extraction components. The concept of feature quality is defined based on the models’ confidence in predictions. In a situation where two DNNs have approximately the same accuracy, the superior model has more confidence in its predictions. Hence, it is less influenced by overfitting, making it more robust and reliable in unseen and noisy environments. Determining such a model is not always possible with the well-known accuracy metric. Accordingly, a novel metric named Generalized Feature-Based Confidence Metric is proposed, which is capable of profoundly evaluating the models’ confidence in predictions. It analyzes layer-by-layer feature vectors generated by DNNs and evaluates their quality. Altogether, these utilities boost assessing and comparing different models with varying widths and depths, improving them, and picking the best one. The practicality of the proposed method and metric is investigated through four significantly diverse case studies and empirically proved. Three of them are reputable benchmarking datasets, namely, CIFAR-10, CIFAR-100, and Fashion-MNIST. Moreover, a new high-quality dataset for the Hand Rubbing problem (made by the authors) is used to analyze the proposed method’s performance in a real-world application. Overall, the proposed metric is able to distinguish between different models from about 1% to 8% in terms of confidence in predictions where the models possess almost the same accuracy (0.5% difference or lower).}
}
@article{XIE2023109681,
title = {FBN: Federated Bert Network with client-server architecture for cross-lingual signature verification},
journal = {Pattern Recognition},
volume = {142},
pages = {109681},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109681},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300376X},
author = {Liyang Xie and Zhongcheng Wu and Xian Zhang and Yong Li},
keywords = {Online signature verification, Federated Bert Network, Client-server architecture, Length Alignment Algorithm, Federated Average Algorithm with Reward-Punishment Mechanism},
abstract = {Online signature verification has a great challenge due to the poor performance of deep learning techniques on cross-lingual datasets under privacy constraints. In this paper, we propose a novel Federated Bert Network (FBN) by embedding the Bidirectional Encoder Representations from Transformers (Bert) into a Federated Learning (FL) framework with client-server architecture. A new Length Alignment Algorithm is employed to unify the signature pairs’ sequence length, and the input representations are fed into the different clients to complete the independent learning of local-models. In addition, the server (coordinator) uses the improved Federated Average Algorithm with Reward-Punishment Mechanism (FedAvgRP) to aggregate these local-models and further generate a global-model. After multiple iterations, the optimal model can be obtained and cross-tested on four datasets (SVC 2004, MCYT-330, BioecurID, and Ours) with skilled forged (random forged) EERs of 7.65% (4.76%), 10.73% (8.46%), 10.09% (7.13%), and 8.28% (5.74%), respectively, far higher than that of the independent learning of state-of-the-art methods. Compared with the domain adaptation and improved FL models, our FBN model performs best in random and skilled forgery scenarios. Moreover, the FedAvgRP algorithm helps our model maintain high performance in the face of data attacks.}
}
@article{LI2023109691,
title = {Multi-Scale correlation module for video-based facial expression recognition in the wild},
journal = {Pattern Recognition},
volume = {142},
pages = {109691},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109691},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003898},
author = {Tankun Li and Kwok-Leung Chan and Tardi Tjahjadi},
keywords = {Facial expression recognition, Convolutional neural networks, Motion estimation, Adaptive fusion},
abstract = {The detection of facial muscle movements (e.g., mouth opening) is crucial for facial expression recognition (FER). However, extracting these facial motion features is challenging for a deep-learning recognition system for the following reasons: (1) without explicit labels of motion for training, there is no guarantee that convolutional neural networks (CNNs) can extract motions effectively; (2) compared to human action recognition (e.g., the object moving from left to right), some facial motions (e.g., raising eyebrows) are more subtle and thus harder to extract; and (3) the use of optical flow to extract motion features is time-consuming when using a commonly-used camera. In this work, we propose a Multi-Scale Correlation Module (MSCM) together with an adaptive fusion. Firstly, large as well as small facial motions are extracted by MSCM and encoded by CNNs. Then, an adaptive fusion module is used to aggregate motion features. With these modules, our recognition network is able to model both subtle and large motion features for video-based FER with only the RGB image frames as input. Experiments on two datasets, AFEW and DFEW, show that the network achieves state-of-art performances on the benchmarks.}
}
@article{SOLTANZADEH2023109721,
title = {Addressing the class-imbalance and class-overlap problems by a metaheuristic-based under-sampling approach},
journal = {Pattern Recognition},
volume = {143},
pages = {109721},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109721},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004193},
author = {Paria Soltanzadeh and M. Reza Feizi-Derakhshi and Mahdi Hashemzadeh},
keywords = {Imbalanced classification, Imbalanced datasets, Class overlap, Class imbalance, Metaheuristic algorithms, Under-sampling},
abstract = {The problem of imbalanced class distribution in real-world datasets severely impairs the performance of classification algorithms. The learning task becomes more complicated and challenging when there is also the class-overlap problem in imbalanced data. This research tackles these problems by presenting an under-sampling approach based on a metaheuristic method in which the under-sampling problem is mapped into an optimization problem. The proposed approach aims to select an optimal subset of the majority samples to handle the imbalanced and the class-overlap problems simultaneously while avoiding the excessive elimination of majority samples, especially in overlapped regions. The quality of the generated solutions is evaluated by a classifier and optimized in an evolutionary process. Unlike most existing under-sampling methods, the majority samples are not removed only from the overlapped regions; the classifier performance determines the desired regions for eliminating the majority samples. Extensive experiments conducted on 66 synthetic and 24 real-world datasets with different imbalance ratios and overlapping degrees and two large high-dimensional datasets show a significant performance improvement from the proposed method compared to the competitors.}
}
@article{ZHANG2023109737,
title = {Deep representation learning for domain generalization with information bottleneck principle},
journal = {Pattern Recognition},
volume = {143},
pages = {109737},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109737},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004351},
author = {Jiao Zhang and Xu-Yao Zhang and Chuang Wang and Cheng-Lin Liu},
keywords = {Domain generalization, Information bottleneck, Representation learning},
abstract = {Although deep neural networks have achieved superior performance on many classical tasks, they deteriorate in real applications due to the unpredictable distribution shift. Domain generalization (DG) focuses on improving the generalization ability of the predictive model in unseen domains by training on multiple available source domains. All these domains share the same categories but commonly obey different distributions. In this paper, we establish a new theoretical framework for domain generalization from the perspective of the information bottleneck (IB) principle, which links representation learning in DG with domain-invariant representation learning and maximizing feature entropy (MFE). Based on the theoretical framework, we provide a feasible solution by class-wise instance discrimination combined with inter-dimension decorrelation and intra-dimension uniformity to learn the desired representation for domain generalization, which achieves excellent performance on multiple datasets without knowing domain labels. Extensive experiments show that the proposed regularization rule (MFE) can improve invariance-based DG methods consistently. Moreover, as an extreme case of domain generalization, we also show that MFE is promising to improve adversarial robustness.}
}
@article{LI2023109637,
title = {Robust rank-one matrix completion with rank estimation},
journal = {Pattern Recognition},
volume = {142},
pages = {109637},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109637},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003382},
author = {Ziheng Li and Feiping Nie and Rong Wang and Xuelong Li},
keywords = {Matrix completion, Robust, Rank-one matrix pursuit, Rank estimation},
abstract = {Matrix completion aims at estimating the missing entries of a low-rank and incomplete data matrix. It frequently arises in many applications such as computer vision, pattern recognition, recommendation system, and data mining. Most of the existing methods face two problems. Firstly, the data matrix in real world is often disturbed by noise. Noise may change the date structure of the incomplete matrix, thereby degrade the performance of matrix completion algorithms. Secondly, some existing methods need to preset a reasonable rank as input, and the value of rank will affect the performance of the algorithms. Therefore, we proposed a robust rank-one matrix completion method with rank estimation in this paper. To mitigate the influence of noise, we divide the incomplete and noisy data matrix into two parts iteratively: low-rank and sparse parts. Besides, we use a weighted rank-one matrix pursuit algorithm to approximate the low-rank part of the data matrix, and the rank of the matrix can be estimated with the adaptive weight vector. The performance of the proposed method is demonstrated by experiments on both synthetic datasets and image datasets. The experimental results demonstrate the performance of the proposed method with incompleted matrices distrubed by sparse noise.}
}
@article{UCHIGASAKI2023109696,
title = {Deep image compression using scene text quality assessment},
journal = {Pattern Recognition},
volume = {142},
pages = {109696},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109696},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003941},
author = {Shohei Uchigasaki and Tomo Miyazaki and Shinichiro Omachi},
keywords = {Image compression, Scene text image, Text quality, Quality assessment, Regression model},
abstract = {Image compression is a fundamental technology for Internet communication engineering. However, a high compression rate with general methods may degrade images, resulting in unreadable texts. In this paper, we propose an image compression method for maintaining text quality. We developed a scene text image quality assessment model to assess text quality in compressed images. The assessment model iteratively searches for the best-compressed image holding high-quality text. Objective and subjective results showed that the proposed method was superior to existing methods. Furthermore, the proposed assessment model outperformed other deep-learning regression models.}
}
@article{ALI2023109641,
title = {A k nearest neighbour ensemble via extended neighbourhood rule and feature subsets},
journal = {Pattern Recognition},
volume = {142},
pages = {109641},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109641},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003424},
author = {Amjad Ali and Muhammad Hamraz and Naz Gul and Dost Muhammad Khan and Saeed Aldahmani and Zardad Khan},
keywords = {Features subset, Nearest Neighbours Rule, NN Ensemble, Classification},
abstract = {kNN based ensemble methods minimise the effect of outliers by identifying a set of data points in the given feature space that are nearest to an unseen observation in order to predict its response by using majority voting. The ordinary ensembles based on kNN find out the k nearest observations in a region (bounded by a sphere) based on a predefined value of k. This scenario, however, might not work in situations where the test observation follows the pattern of the closest data points with the same class that lie on a certain path not contained in the given sphere. This paper proposes a k nearest neighbour ensemble where the neighbours are determined in k steps. Starting from the first nearest observation of the test point, the algorithm identifies a single observation that is closest to the observation at the previous step. At each base learner in the ensemble, this search is extended to k steps on a random bootstrap sample with a random subset of features selected from the feature space. The final predicted class of the test point is determined by using a majority vote in the predicted classes given by all base models. This new ensemble method is applied on 20 benchmark datasets and compared with other classical methods, including kNN based models, in terms of classification accuracy, kappa and Brier score as performance metrics. Boxplots are also utilised to illustrate the difference in the results given by the proposed and other state-of-the-art methods. The proposed method outperformed the considered classical methods in the majority of cases. The proposed method is further assessed through a detailed simulation study.}
}
@article{KORBAN2023109713,
title = {A Multi-Modal Transformer network for action detection},
journal = {Pattern Recognition},
volume = {142},
pages = {109713},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109713},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004119},
author = {Matthew Korban and Peter Youngs and Scott T. Acton},
keywords = {Action detection, Transformer network, Optical flow, Motion features},
abstract = {This paper proposes a novel multi-modal transformer network for detecting actions in untrimmed videos. To enrich the action features, our transformer network utilizes a new multi-modal attention mechanism that computes the correlations between different spatial and motion modalities combinations. Exploring such correlations for actions has not been attempted previously. To use the motion and spatial modality more effectively, we suggest an algorithm that corrects the motion distortion caused by camera movement. Such motion distortion, common in untrimmed videos, severely reduces the expressive power of motion features such as optical flow fields. Our proposed algorithm outperforms the state-of-the-art methods on two public benchmarks, THUMOS14 and ActivityNet. We also conducted comparative experiments on our new instructional activity dataset, including a large set of challenging classroom videos captured from elementary schools.}
}
@article{CHEN2023109677,
title = {Multi-semantic hypergraph neural network for effective few-shot learning},
journal = {Pattern Recognition},
volume = {142},
pages = {109677},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109677},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003783},
author = {Hao Chen and Linyan Li and Fuyuan Hu and Fan Lyu and Liuqing Zhao and Kaizhu Huang and Wei Feng and Zhenping Xia},
keywords = {Hypergraph, Few-shot learning, Multi-semantic learning, Orthogonal training},
abstract = {Recently, Graph-based Few-Shot Learning (FSL) methods exhibit good generalization by mining relations among few samples with Graph Neural Networks. However, most Graph-based FSL methods consider only binary relations and ignore the multi-semantic information of the global context knowledge. We propose a framework of Multi-Semantic Hypergraph for FSL (MSH-FSL) to explore complex latent high-order multi-semantic relations among the few samples. By mining the complex relationship structure of multi-node and multi-semantics, more refined feature representation can be learned, which yields better classification robustness. Specifically, we first construct a novel Multi-Semantic Hypergraph by obtaining associated instances with different semantic features via orthogonal mapping. With the constructed hypergraph, we then develop the Hyergraph Neural Network along with a novel multi-generation hypergraph message passing so as to better leverage the complex latent semantic relations among samples. Finally, after a number of generations, the hyper-node representations embedded in the learned hypergraph become more accurate for obtaining few-shot prediction. In the 5-way 1-shot task of ResNet-12 on mini-Imagenet dataset, the multi-semantic hypergraph outperforms single-semantic graph by 3.1%, and with the proposed semantic-distribution message passing, the improvement can further reach 6.1%.}
}
@article{SUN2023109625,
title = {Multi-view prototype-based disambiguation for partial label learning},
journal = {Pattern Recognition},
volume = {141},
pages = {109625},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109625},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003266},
author = {Shiding Sun and Xiaotong Yu and Yingjie Tian},
keywords = {Multi-view learning, Partial label learning, Weakly supervised learning},
abstract = {In this work, we study the multi-view partial label learning (MVPLL) problem, where each instance is depicted by different view features and associated with a set of candidate labels, among which a true label exists but is inaccessible in the training phase. Most existing PLL methods only consider single-view case, which learn view classifier independently and neglect the view correlations, thus can not be applied to solve MVPLL problem. Due to the non-deep framework, traditional MVPLL approach is weak in the representation ability, so its performance is still to be improved. To solve the MVPLL problem, a deep multi-view prototype-based disambiguation approach is proposed in this paper. Specifically, we innovatively employ the deep neural network for multi-view ambiguously-labeled image classification to enhance the representation ability, which makes use of the information fusion between multiple views. To improve the discriminative ability, we propose multi-view prototype-based label disambiguation algorithm. On theoretical aspect, an estimation error bound for view-risk estimator is established, which is shown to be larger than that for fuse-risk estimator. Experiments demonstrate the superiorities of our proposed method in terms of the prediction accuracy.}
}
@article{ZOU2023109653,
title = {Adaptive reweighted quaternion sparse learning for data recovery and classification},
journal = {Pattern Recognition},
volume = {142},
pages = {109653},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109653},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003540},
author = {Cuiming Zou and Kit Ian Kou and Yuan Yan Tang and Hao Deng},
keywords = {Quaternion sparse representation, Weight learning, Supervised learning},
abstract = {Sparse representation (SR) methods in quaternion space have been attracting increasing interests recently. However, most existing quaternion SR methods adopt the quaternion ℓ1 norm, which penalizes all the entries of the quaternion sparse vector equally and ignores the differences and significance of different entries. Ideally, the entries with large magnitude should be less penalized while those with small magnitude (such as zero entries) should be more penalized. Therefore, we propose an Adaptive Weighted Quaternion Sparse Representation (AWQSR) method in this paper, which can learn weights for distinct entries of the quaternion sparse entries in an adaptive manner. Due to the noncommutativity of quaternion multiplication, it is difficult to tackle the resulting optimization problem of AWQSR. For this reason, we devise an effective iteratively reweighted optimization algorithm based on quaternion operators. To further improve the classification performance, we also develop a Supervised AWQSR based Classification (SAWQSRC) method by leveraging the label information of training samples to learn discriminative weights. Theoretical analysis of SAWQSRC has also been established to show that SAWQSRC succeeds in classification under appropriate conditions. The experiments on simulated data and real data prove the validity of the proposed methods for quaternion signal recovery and classification.}
}
@article{VIDAL2023109695,
title = {End-to-End page-Level assessment of handwritten text recognition},
journal = {Pattern Recognition},
volume = {142},
pages = {109695},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109695},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300393X},
author = {Enrique Vidal and Alejandro H. Toselli and Antonio Ríos-Vila and Jorge Calvo-Zaragoza},
keywords = {Handwritten text recognition, Full-Page end-to-End text image transcription, Layout analysis, Text line detection, Reading order, Evaluation measures, Bag of words, Regularised hungarian algorithm, Word error rate},
abstract = {The evaluation of Handwritten Text Recognition (HTR) systems has traditionally used metrics based on the edit distance between HTR and ground truth (GT) transcripts, at both the character and word levels. This is very adequate when the experimental protocol assumes that both GT and HTR text lines are the same, which allows edit distances to be independently computed to each given line. Driven by recent advances in pattern recognition, HTR systems increasingly face the end-to-end page-level transcription of a document, where the precision of locating the different text lines and their corresponding reading order (RO) play a key role. In such a case, the standard metrics do not take into account the inconsistencies that might appear. In this paper, the problem of evaluating HTR systems at the page level is introduced in detail. We analyse the convenience of using a two-fold evaluation, where the transcription accuracy and the RO goodness are considered separately. Different alternatives are proposed, analysed and empirically compared both through partially simulated and through real, full end-to-end experiments. Results support the validity of the proposed two-fold evaluation approach. An important conclusion is that such an evaluation can be adequately achieved by just two simple and well-known metrics: the Word Error Rate (WER), that takes transcription sequentiality into account, and the here re-formulated Bag of Words Word Error Rate (bWER), that ignores order. While the latter directly and very accurately assess intrinsic word recognition errors, the difference between both metrics (ΔWER) gracefully correlates with the Normalised Spearman’s Foot Rule Distance (NSFD), a metric which explicitly measures RO errors associated with layout analysis flaws. To arrive to these conclusions, we have introduced another metric called Hungarian Word Word Rate (hWER), based on a here proposed regularised version of the Hungarian Algorithm. This metric is shown to be always almost identical to bWER and both bWER and hWER are also almost identical to WER whenever HTR transcripts and GT references are guarantee to be in the same RO.}
}
@article{LI2023109684,
title = {Truncated attention-aware proposal networks with multi-scale dilation for temporal action detection},
journal = {Pattern Recognition},
volume = {142},
pages = {109684},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109684},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003825},
author = {Ping Li and Jiachen Cao and Li Yuan and Qinghao Ye and Xianghua Xu},
keywords = {Temporal action detection, Attention mechanism, Graph convolution, Multi-scale dilation, Proposal network},
abstract = {Detecting actions temporally in untrimmed videos is very challenging, and it accomplishes action classification and localization simultaneously. Capturing the relations among action proposals (i.e., candidate video segments) is of vital importance. While there have been several attempts to encode such relations, they neglect the adverse effects of those irrelevant or negative relations among proposals. Besides, there is a crucial fact that action durations are flexible in videos, which has not been well explored. For the former, we develop a truncated attention mechanism that learns positive proposal relations by dynamically adjusting edge weights of proposal nodes in a graph, and construct the proposal network model using graph convolution networks to suppress disadvantageous relations of proposal pairs by truncating negative attention scores. For the latter, we devise a light multi-scale dilation module shared by all proposals to handle different action durations by enlarging temporal receptive field, thus capturing temporal context to increase the representation capacity of proposals. Unifying these considerations, we present the Multi-scale Dilation based Truncated Attention Proposal Network (MD-TAPN) model for temporal action detection. Our model achieves state-of-the-art performances of detecting actions on two benchmark databases, and especially it outperforms the most competitive method by a significant gain of 3.6% mAP at tIoU0.5 on THUMOS14.}
}
@article{CHENG2023109629,
title = {TAT: Targeted backdoor attacks against visual object tracking},
journal = {Pattern Recognition},
volume = {142},
pages = {109629},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109629},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003308},
author = {Ziyi Cheng and Baoyuan Wu and Zhenya Zhang and Jianjun Zhao},
keywords = {Backdoor attack, Visual object tracking, Targeted attack},
abstract = {Visual object tracking (VOT) is a fundamental computer vision task that aims to track a target in a sequence of video frames. It has been broadly adopted in safety- and security-critical applications, such as self-driving systems and traffic control systems. However, the VOT models (i.e., the trackers) that rely on third-party training resources face a severe threat of backdoor attacks, which refer to the type of the attacks that poison a portion of training data and mislead the tracker to track a wrong target. A surge of research interest has arisen in backdoor attacks in the domain of image classification, as a measure to expose the potential security risks of the classifiers and inspire new defense techniques. Despite the prosperity of the research in backdoor attacks in image classification, there still lacks investigation in backdoor attacks against VOT, due to their unique challenges: first, the architecture of a VOT model is much more complicated than that of an image classifier; second, VOT targets a sequence of video frames rather than individual images. To bridge the gap, we propose a novel and effective targeted backdoor attack approach TAT specifically against VOT tasks. In particular, TAT includes a basic version TAT-BA that can achieve effective and stealthy backdoor attacks against VOT trackers, and an advanced version TAT-DA that can evade two representative defense techniques. Our large-scale experimental evaluation demonstrates the effectiveness and the stealthiness of TAT. Moreover, we also demonstrate the performances of TAT-BA under real-world settings and the abilities of TAT-DA to counter defense techniques. The code will be available at https://github.com/MisakaZipi/TAT.}
}
@article{ZHANG2023109725,
title = {Interpreting vulnerabilities of multi-instance learning to adversarial perturbations},
journal = {Pattern Recognition},
volume = {142},
pages = {109725},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109725},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004235},
author = {Yu-Xuan Zhang and Hua Meng and Xue-Mei Cao and Zhengchun Zhou and Mei Yang and Avik Ranjan Adhikary},
keywords = {Customized perturbation, Multi-instance learning, Universal perturbation, Vulnerability},
abstract = {Multi-instance learning (MIL) is a recent machine learning paradigm which is immensely useful in various real-life applications, like image analysis, video anomaly detection, text classification, etc. It is well known that most of the existing machine learning classifiers are highly vulnerable to adversarial perturbations. Since MIL is a weakly supervised learning, where information is available for a set of instances, called bag and not for every instance, adversarial perturbations can be fatal. In this paper, we have proposed two adversarial perturbation methods to analyze the effect of adversarial perturbations to interpret the vulnerabilities of MIL methods. Out of the two algorithms, one can be customized for every bag, and the other is a universal one, which can affect all bags in a given data set and thus has some generalizability. Furthermore, through simulations, we have demonstrated the efficacy of the proposed algorithms in fooling state-of-the-art MIL approaches, such that these models make incorrect predictions regarding the label assigned to the bag. Finally, we have discussed, through experiments, about taking care of these kind of adversarial perturbations through a simple strategy. Source codes are available at https://github.com/InkiInki/MI-UAP.}
}
@article{ZHOU2023109665,
title = {Feature fusion and latent feature learning guided brain tumor segmentation and missing modality recovery network},
journal = {Pattern Recognition},
volume = {141},
pages = {109665},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109665},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003667},
author = {Tongxue Zhou},
keywords = {Brain tumor segmentation, Multimodal feature fusion, Missing modalities, Spatial consistency, Latent feature learning},
abstract = {Accurate brain tumor segmentation is an essential step for clinical diagnosis and surgical treatment. Multimodal brain tumor segmentation strongly relies on an effective fusion method and an excellent segmentation network. However, it is common to have some missing MR modalities in clinical scenarios due to image corruption, acquisition protocol, scanner availability and scanning cost, which can heavily decrease the tumor segmentation accuracy, and also cause information loss for down-streaming disease analysis. To address this issue, I propose a novel multimodal feature fusion and latent feature learning guided deep neural network. On the one hand, the proposed network can help to segment brain tumors when one or more modalities are missing. On the other hand, it can retrieve the missing modalities to compensate for incomplete data. The proposed network consists of three key components. First, a Multimodal Feature Fusion Module (MFFM) is proposed to effectively fuse the complementary information from different modalities, consisting of a Cross-Modality Fusion Module (CMFM) and a Multi-Scale Fusion Module (MSFM). Second, a Spatial Consistency-based Latent Feature Learning Module (SC-LFLM) is presented to exploit multimodal latent correlation and extract the relevant features to benefit segmentation. Third, the Multi-Task Learning (MTL) paths are integrated to supervise the segmentation and recover the missing modalities. The proposed method is evaluated on BraTS 2018 dataset, and it can achieve superior segmentation results when one or more modalities are missing, compared with the state-of-the-art methods. Furthermore, the proposed modules can be easily adapted to other multimodal network architectures and research fields.}
}