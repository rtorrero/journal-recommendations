@article{LIU2023109048,
title = {Unauthorized AI cannot recognize me: Reversible adversarial example},
journal = {Pattern Recognition},
volume = {134},
pages = {109048},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109048},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005283},
author = {Jiayang Liu and Weiming Zhang and Kazuto Fukuchi and Youhei Akimoto and Jun Sakuma},
keywords = {Adversarial example, Reversible data hiding, AI security},
abstract = {In this study, we propose a new methodology to control how user’s data is recognized and used by AI via exploiting the properties of adversarial examples. For this purpose, we propose reversible adversarial example (RAE), a new type of adversarial example. A remarkable feature of RAE is that the image can be correctly recognized and used by the AI model specified by the user because the authorized AI can recover the original image from the RAE exactly by eliminating adversarial perturbation. On the other hand, other unauthorized AI models cannot recognize it correctly because it functions as an adversarial example. Moreover, RAE can be considered as one type of encryption to computer vision since reversibility guarantees the decryption. To realize RAE, we combine three technologies, adversarial example, reversible data hiding for exact recovery of adversarial perturbation, and encryption for selective control of AIs who can remove adversarial perturbation. Experimental results show that the proposed method can achieve comparable attack ability with the corresponding adversarial attack method and similar visual quality with the original image, including white-box attacks and black-box attacks.}
}
@article{LI2023109129,
title = {Dirichlet process mixture of Gaussian process functional regressions and its variational EM algorithm},
journal = {Pattern Recognition},
volume = {134},
pages = {109129},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109129},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006094},
author = {Tao Li and Jinwen Ma},
keywords = {Gaussian process, Dirichlet process, Functional data, Non-parametric Bayesian model, EM algorithm, Variational inference, Multi-modal data},
abstract = {Gaussian Process Functional Regression (GPFR) is a powerful tool in functional data analysis. In practical applications, functional data may be generated from different signal sources, and a single GPFR is not flexible enough to accurately model the data. To tackle the heterogeneity problem, a finite mixture of Gaussian Process Functional Regressions (mix-GPFR) was suggested. However, the number of components in mix-GPFR needs to be specified a priori, which is difficult to determine in practice. In this paper, we propose a Dirichlet Process Mixture of Gaussian Process Functional Regressions (DPM-GPFR), in which there are potentially infinite many GPFR components dominated by a Dirichlet process. Thus, DPM-GPFR is far more flexible than a single GPFR, and sidestep the model selection problem in mix-GPFR. We further develop a fully Bayesian treatment for learning DPM-GPFR based on the Variational Expectation-Maximization (VEM) algorithm. Experimental results on both synthetic datasets and real-world datasets demonstrate the effectiveness of our proposed method.}
}
@article{PINTOR2023109064,
title = {ImageNet-Patch: A dataset for benchmarking machine learning robustness against adversarial patches},
journal = {Pattern Recognition},
volume = {134},
pages = {109064},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109064},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005441},
author = {Maura Pintor and Daniele Angioni and Angelo Sotgiu and Luca Demetrio and Ambra Demontis and Battista Biggio and Fabio Roli},
keywords = {Adversarial machine learning, Adversarial patches, Neural networks, Defense, Detection},
abstract = {Adversarial patches are optimized contiguous pixel blocks in an input image that cause a machine-learning model to misclassify it. However, their optimization is computationally demanding, and requires careful hyperparameter tuning, potentially leading to suboptimal robustness evaluations. To overcome these issues, we propose ImageNet-Patch, a dataset to benchmark machine-learning models against adversarial patches. The dataset is built by first optimizing a set of adversarial patches against an ensemble of models, using a state-of-the-art attack that creates transferable patches. The corresponding patches are then randomly rotated and translated, and finally applied to the ImageNet data. We use ImageNet-Patch to benchmark the robustness of 127 models against patch attacks, and also validate the effectiveness of the given patches in the physical domain (i.e., by printing and applying them to real-world objects). We conclude by discussing how our dataset could be used as a benchmark for robustness, and how our methodology can be generalized to other domains. We open source our dataset and evaluation code at https://github.com/pralab/ImageNet-Patch.}
}
@article{WANG2023109146,
title = {A Learnable Gradient operator for face presentation attack detection},
journal = {Pattern Recognition},
volume = {135},
pages = {109146},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109146},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006252},
author = {Caixun Wang and Bingyao Yu and Jie Zhou},
keywords = {Face presentation attack detection, Learnable gradient operator, Depth-supervised network},
abstract = {Face presentation attack detection (PAD) aims to protect the security of face recognition systems. The existing depth-supervised method using stacked vanilla convolutions cannot explicitly extract efficient fine-grained information (e.g., spatial gradient magnitude) for the distinction between bona fide and attack presentations. To address this issue, the Sobel operator has been demonstrated effective to acquire gradient magnitude due to the fast calculation capacity for high-frequency information. However, the Sobel operator is hand-crafted so cannot deal with complex textures. Differently, we develop a learnable gradient operator (LGO) to adaptively learn gradient information in a data-driven way, which is a generalization of existing gradient operators and effectively captures detailed discriminative clues from raw pixels. In parallel, we propose an adaptive gradient loss for better optimization. Extensive experimental comparisons with the state-of-the-art methods on the widely used Replay-Attack, CASIA-FASD, OULU-NPU, and SiW datasets demonstrate the superior performance of the proposed approach.}
}
@article{GE2023109088,
title = {Unsupervised Domain Adaptation via Deep Conditional Adaptation Network},
journal = {Pattern Recognition},
volume = {134},
pages = {109088},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109088},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005684},
author = {Pengfei Ge and Chuan-Xian Ren and Xiao-Lin Xu and Hong Yan},
keywords = {Deep learning, Domain adaptation, Feature extraction, Conditional maximum mean discrepancy, Kernel method},
abstract = {Unsupervised domain adaptation (UDA) aims to generalize the supervised model trained on a source domain to an unlabeled target domain. Previous works mainly rely on the marginal distribution alignment of feature spaces, which ignore the conditional dependence between features and labels, and may suffer from negative transfer. To address this problem, some UDA methods focus on aligning the conditional distributions of feature spaces. However, most of these methods rely on class-specific Maximum Mean Discrepancy or adversarial training, which may suffer from mode collapse and training instability. In this paper, we propose a Deep Conditional Adaptation Network (DCAN) that aligns the conditional distributions by minimizing Conditional Maximum Mean Discrepancy, and extracts discriminant information from the target domain by maximizing the mutual information between samples and the prediction labels. Conditional Maximum Mean Discrepancy measures the difference between conditional distributions directly through their conditional embedding in Reproducing Kernel Hilbert Space, thus DCAN can be trained stably and converge fast. Mutual information can be expressed as the difference between the entropy and conditional entropy of the predicted category variable, thus DCAN can extract the discriminant information of individual and overall distributions in the target domain, simultaneously. In addition, DCAN can be used to address a special scenario, Partial UDA, where the target domain category is a subset of the source domain category. Experiments on both UDA and Partial UDA show that DCAN achieves superior classification performance over state-of-the-art methods.}
}
@article{BICEGO2023109036,
title = {DisRFC: a dissimilarity-based Random Forest Clustering approach},
journal = {Pattern Recognition},
volume = {133},
pages = {109036},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109036},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005167},
author = {Manuele Bicego},
keywords = {Random forests clustering, Dissimilarities, Unsupervised learning, Clustering, Non-vectorial representation},
abstract = {In this paper we present a novel Random Forest Clustering approach, called Dissimilarity Random Forest Clustering (DisRFC), which requires in input only pairwise dissimilarities. Thanks to this characteristic, the proposed approach is appliable to all those problems which involve non-vectorial representations, such as strings, sequences, graphs or 3D structures. In the proposed approach, we first train an Unsupervised Dissimilarity Random Forest (UD-RF), a novel variant of Random Forest which is completely unsupervised and based on dissimilarities. Then, we exploit the trained UD-RF to project the patterns to be clustered in a binary vectorial space, where the clustering is finally derived using fast and effective K-means procedures. In the paper we introduce different variants of DisRFC, which are thoroughly and positively evaluated on 12 different problems, also in comparison with alternative state-of-the-art approaches.}
}
@article{FAN2023109133,
title = {GraphDPI: Partial label disambiguation by graph representation learning via mutual information maximization},
journal = {Pattern Recognition},
volume = {134},
pages = {109133},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109133},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006136},
author = {Jinfu Fan and Yang Yu and Linqing Huang and Zhongjie Wang},
keywords = {Partial label learning, GraphDPI, Mutual Information, Triplet loss},
abstract = {Partial label learning (PLL) is a weakly supervised learning framework where each training instance is associated with more than one candidate label, and only one of them is the true label. Most of the existing PLL algorithms directly disambiguate the candidate labels according to the instance feature similarity, but fail to discover the latent semantic relationship over the entire dataset. In this paper, method GraphDPI, an innovative deep partial label disambiguation by graph representation via mutual information maximization, is proposed. This method can capture the semantic clusters with the most unique information in the latent space and automatically adapt to different feature distributions. Specifically, a new sampling method based on the graph is proposed to estimate mutual information, extending GCN to the field of weakly supervised learning. Therefore, the graph representation of the data can contain more distinguishing information to disambiguate candidate labels by maximizing the mutual information of the local graph representation and the global one. Furthermore, the triplet loss is introduced to fully exploit the relationship between instances and extract the latent embedding representation over the entire dataset. It thereby can make the model output as large as possible on the inter-class variation and as small as possible on the intra-class variation. Finally, the candidate labels can be disambiguated by the difference between semantic clusters. Experiments reveal the overwhelming performances of GraphDPI.}
}
@article{CAI2023109063,
title = {Robust learning from noisy web data for fine-Grained recognition},
journal = {Pattern Recognition},
volume = {134},
pages = {109063},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109063},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200543X},
author = {Zhenhuang Cai and Guo-Sen Xie and Xingguo Huang and Dan Huang and Yazhou Yao and Zhenmin Tang},
keywords = {Fine-grained, Web-supervised, Noisy web data, Robust learning},
abstract = {Due to DNNs’ memorization effect, label noise lessens the performance of the web-supervised fine-grained visual categorization task. Previous literature primarily relies on small-loss instances for subsequent training. The current state-of-the-art approach JoCoR additionally employs explicit consistency constraints to make clean samples more confident. However, a joint loss designed for both sample selection criteria and parameter updating is not competent for training a robust model in the presence of web noise. Especially, false positives are assigned with larger weights, causing the model to pay more attention to misclassified noisy images. Besides, leveraging weight decay to forget discarded noisy instances is too slow and implicit to take effect. Therefore, we propose a simple yet effective approach named MS-DeJOR (Multi-Scale training with Decoupled Joint Optimization and Refurbishment). In contrast to JoCoR, we decouple sample selection from training procedure to handle the above problems. Specifically, a negative entropy term is applied to prevent false positives from being overemphasized. The model can explicitly forget those samples identified as noise by imposing such a regularization term on all training data. Furthermore, we use accumulated predictions to refurbish the noisy labels and re-weight training images to boost the model performance. A multi-scale feature enhancement module is adopted to extract discriminative and subtle feature representations. Extensive experiments show that MS-DeJOR yields state-of-the-art performances on three web-supervised fine-grained datasets, demonstrating the effectiveness of our approach. The data and source code have been available at https://github.com/msdejor/MS-DeJOR.}
}
@article{ZHANG2023109052,
title = {Density peaks clustering based on balance density and connectivity},
journal = {Pattern Recognition},
volume = {134},
pages = {109052},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109052},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005325},
author = {Qinghua Zhang and Yongyang Dai and Guoyin Wang},
keywords = {Clustering, Mutual nearest neighbor, Connectivity between data points, Fast search strategy},
abstract = {Density peaks clustering (DPC) algorithm regards the density peaks as the potential cluster centers, and assigns the non-center point into the cluster of its nearest higher-density neighbor. Although DPC can discover clusters with arbitrary shapes, it has some limitations. On the one hand, the density measure of DPC fails to eliminate the density difference among different clusters, which affects the accuracy of recognizing cluster center. On the other hand, the nearest higher-density point is determined without considering connectivity, which leads to continuously clustering errors. Therefore, DPC fails to obtain satisfactory clustering results on datasets with great density difference among clusters. In order to eliminate these limitations, a novel DPC algorithm based on balance density and connectivity (BC-DPC) is proposed. First, the balance density is proposed to eliminate the density difference among different clusters to accurately recognize cluster centers. Second, the connectivity between a data point and its nearest higher-density point is guaranteed by mutual nearest neighbor relationship to avoid continuously clustering errors. Finally, a fast search strategy is proposed to find the nearest higher-density point. The experimental results on synthetic, UCI, and image datasets demonstrate the efficiency and effectiveness of the proposed algorithm in this paper.}
}
@article{HAN2023109076,
title = {ML-DSVM+: A meta-learning based deep SVM+ for computer-aided diagnosis},
journal = {Pattern Recognition},
volume = {134},
pages = {109076},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109076},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005568},
author = {Xiangmin Han and Jun Wang and Shihui Ying and Jun Shi and Dinggang Shen},
keywords = {Deep neural network, Support vector machine plus, Learning using privileged information, Meta-learning},
abstract = {Transfer learning (TL) can improve the performance of a single-modal medical imaging-based computer-aided diagnosis (CAD) by transferring knowledge from related imaging modalities. Support vector machine plus (SVM+) is a supervised TL classifier specially designed for TL between the paired data in the source and target domains with shared labels. In this work, a novel deep neural network (DNN) based SVM+ (DSVM+) algorithm is proposed for single-modal imaging-based CAD. DSVM+ integrates the bi-channel DNNs and SVM+ classifier into a unified framework to improve the performance of both feature representation and classification. In particular, a new coupled hinge loss function is developed to conduct bidirectional TL between the source and target domains, which further promotes knowledge transfer together with the feature representation under the guidance of shared labels. To alleviate the overfitting caused by the increased parameters in DNNs for limited training samples, the meta-learning based DSVM+ (ML-DSVM+) is further developed, which designs randomly selecting samples from the training data instead of other CAD tasks for meta-tasks. This sampling strategy also can avoid the issue of class imbalance. ML-DSVM+ is evaluated on three medical imaging datasets. It achieves the best results of 88.26±1.40%, 90.45±5.00%, and 87.63±5.56% on accuracy, sensitivity and specificity, respectively, on the Bimodal Breast Ultrasound Image dataset, 90.00±1.05%, 72.55±3.87%, and 96.40±2.26% of the corresponding indices on the Alzheimer's Disease Neuroimaging Initiative dataset, and 85.76±3.12% of classification accuracy, 88.73±7.22% of sensitivity, and 82.60±1.56% of specificity for the Autism Brain Imaging Data Exchange dataset.}
}
@article{ROMEROMEDRANO2023109116,
title = {Multi-Source Change-Point Detection over Local Observation Models},
journal = {Pattern Recognition},
volume = {134},
pages = {109116},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109116},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005969},
author = {Lorena Romero-Medrano and Antonio Artés-Rodríguez},
keywords = {Change-point detection, Multi-source data, Heterogeneous data, Latent variable models},
abstract = {In this work, we address the problem of change-point detection (CPD) on high-dimensional, multi-source, and heterogeneous sequential data with missing values. We present a new CPD methodology based on local latent variable models and adaptive factorizations that enhances the fusion of multi-source observations with different statistical data-type and face the problem of high dimensionality. Our motivation comes from behavioral change detection in healthcare measured by smartphone monitored data and Electronic Health Records. Due to the high dimension of the observations and the differences in the relevance of each source information, other works fail in obtaining reliable estimates of the change-points location. This leads to methods that are not sensitive enough when dealing with interspersed changes of different intensity within the same sequence or partial missing components. Through the definition of local observation models (LOMs), we transfer the local CP information to homogeneous latent spaces and propose several factorizations that weight the contribution of each source to the global CPD. With the presented methods we demonstrate a reduction in both the detection delay and the number of not-detected CPs, together with robustness against the presence of missing values on a synthetic dataset. We illustrate its application on real-world data from a smartphone-based monitored study and add explainability on the degree of each source contributing to the detection.}
}
@article{PHUTKE2023109040,
title = {Image inpainting via spatial projections},
journal = {Pattern Recognition},
volume = {133},
pages = {109040},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109040},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005209},
author = {Shruti S Phutke and Subrahmanyam Murala},
keywords = {Spatial projections, Inpainting, Object removal},
abstract = {Image inpainting is now-a-days sought after due to its wide variety of applications in the reconstruction of the corrupted image, occlusion removal, reflection removal, etc. Existing image inpainting approaches utilize different types of attention mechanisms to inpaint the image and produce visibly admirable results. These methods are more concerned at weighing the feature maps of the hole region with some weight from the non-hole region. But, due to the lack of spatial contextual correlation in the attention maps, the inpainted image may suffer from the inconsistencies among hole and non-hole regions. Transformer-based inpainting methods give significant results by capturing the relationship between the patches with a compromise of high computational complexity. In this context, we propose a novel spatial projection layer (SPL) without any attention mechanism to project the spatial contextual information in the hole region from non-hole regions for producing a spatially plausible inpainted image. The SPL is proposed mainly to focus on the non-hole spatial information in the high-level feature maps for filling the hole regions efficiently. Also, while training the network, we propose the use of edge loss with a Canny edge operator for image inpainting to focus on the relevant edges instead of noise contents. Analysis with the extensive experiments, ablation, and user study on the proposed architecture demonstrates the superiority over existing state-of-the-art methods for image inpainting. The code is available at: https://github.com/shrutiphutke/spatial_projection_inpainting.}
}
@article{SAHU2023109128,
title = {Egocentric video co-summarization using transfer learning and refined random walk on a constrained graph},
journal = {Pattern Recognition},
volume = {134},
pages = {109128},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109128},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006082},
author = {Abhimanyu Sahu and Ananda S. Chowdhury},
keywords = {Egocentric video, Transfer learning, Constrained graph, Random walks, Label refinement},
abstract = {In this paper, we address the problem of egocentric video co-summarization. We show how a shot level accurate summary can be obtained in a time-efficient manner using random walk on a constrained graph in transfer learned feature space with label refinement. While applying transfer learning, we propose a new loss function capturing egocentric characteristics in a pre-trained ResNet on the set of auxiliary egocentric videos. Transfer learning is used to generate i) an improved feature space and ii) a set of labels to be used as seeds for the test egocentric video. A complete weighted graph is created for a test video in the new transfer learned feature space with shots as the vertices. We derive two types of cluster label constraints in form of Must-Link (ML) and Cannot-link (CL) based on the similarity of the shots. ML constraints are used to prune the complete graph which is shown to result in substantial computational advantage, especially, for the long duration videos. We derive expressions for the number of vertices and edges for the ML-constrained graph and show that this graph remains connected. Random walk is applied to obtain labels of the unmarked shots in this new graph. CL constraints are applied to refine the cluster labels. Finally, shots closest to individual cluster centres are used to build the summary. Experiments on the short duration videos as in CoSum and TVSum datasets and long duration videos as in ADL and EPIC-Kitchens datasets clearly demonstrate the advantage of our solution over several state-of-the-art methods.}
}
@article{SU2023109047,
title = {From Distortion Manifold to Perceptual Quality: a Data Efficient Blind Image Quality Assessment Approach},
journal = {Pattern Recognition},
volume = {133},
pages = {109047},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109047},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005271},
author = {Shaolin Su and Qingsen Yan and Yu Zhu and Jinqiu Sun and Yanning Zhang},
keywords = {Image quality assessment, No-Reference, Generalizability, Distortion manifold},
abstract = {Though current no-reference image quality assessment (NR-IQA) approaches have achieved impressive performance gain thanks to deep learning techniques, it is claimed that the risk of over-fitting exists. To improve model generalization ability, most of the current researches incorporate mass data to train or tune the data-driven models. However, the process of image data collection and quality label annotation is quite time-consuming and labour-intensive. Therefore, in this paper, we explore an alternative solution to promote model generalizability but with relatively small fractions of training data. Compared with previous approaches which make effort to approximate the whole complex image distribution, we propose to explicitly learn an image distortion manifold first, which lies in a much lower dimension space and also representative in capturing general degradation patterns. We then project the images to their perceived quality from the learned manifold to obtain quality predictions. Since the manifold embeds general distortion features despite of varying image contents, it can be learned with relatively small amount of samples. In order to learn the manifold and quality projection, we introduce a two-branched network to learn both low level distortions and high level semantics. We also propose a simple but efficient training framework, composing of a masked labelling strategy and a gradual weighting curriculum to fulfill the task. Thanks to the learned distortion manifold, the proposed model achieves superior generalizability compared with previous models. Extensive experiments demonstrate its effectiveness in terms of training with limited data, testing on large scale images, and with unseen types of distorted images.}
}
@article{HUANG2023109145,
title = {Exploring modality-shared appearance features and modality-invariant relation features for cross-modality person Re-IDentification},
journal = {Pattern Recognition},
volume = {135},
pages = {109145},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109145},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006240},
author = {Nianchang Huang and Jianan Liu and Yongjiang Luo and Qiang Zhang and Jungong Han},
keywords = {Cross-modality person Re-IDentification, Visible images, Thermal infrared images, Modality-shared appearance features, Modality-invariant relation features},
abstract = {Most existing cross-modality person Re-IDentification works rely on discriminative modality-shared features for reducing cross-modality variations and intra-modality variations. Despite their preliminary success, such modality-shared appearance features cannot capture enough modality-invariant discriminative information due to a massive discrepancy between RGB and IR images. To address this issue, on top of appearance features, we further capture the modality-invariant relations among different person parts (referred to as modality-invariant relation features), which help to identify persons with similar appearances but different body shapes. To this end, a Multi-level Two-streamed Modality-shared Feature Extraction (MTMFE) sub-network is designed, where the modality-shared appearance features and modality-invariant relation features are first extracted in a shared 2D feature space and a shared 3D feature space, respectively. The two features are then fused into the final modality-shared features such that both cross-modality variations and intra-modality variations can be reduced. Besides, a novel cross-modality center alignment loss is proposed to further reduce the cross-modality variations. Experimental results on several benchmark datasets demonstrate that our proposed method exceeds state-of-the-art algorithms by a wide margin.}
}
@article{SUN2023109087,
title = {Learning isometry-invariant representations for point cloud analysis},
journal = {Pattern Recognition},
volume = {134},
pages = {109087},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109087},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005672},
author = {Xiao Sun and Yang Huang and Zhouhui Lian},
keywords = {3D Shape analysis, Isometry invariant, Non-rigid},
abstract = {3D shape analysis has drawn broad attention due to its increasing demands in various fields. Despite that impressive performance has been achieved on several databases, most researchers focus their efforts on improving the performance of shape classification, retrieval, segmentation, etc. They neglect the fact that the disturbances, such as orientation and deformation, may impact much on the perception, restricting the capacity of generalizing to real applications where the prior of orientation and pose is often unknown. In this paper, we conduct shape analysis on point clouds and propose the point projection feature, which is rotation-invariant. Specifically, a novel architecture is designed to mine features of different levels. We adopt a PointNet-based backbone to extract global feature for the point cloud, and the graph aggregation operation to perceive local pose variance in the Euclidean space or geodesic space. An efficient key point descriptor is designed to assign each point with different response and help recognize the overall geometry. Furthermore, a novel dataset, PKUnon-rigid, is built that is composed of non-rigid 3D objects, based on which we evaluate the capacity of several mainstream methods in terms of processing non-rigid shapes. Mathematical analyses and experimental results demonstrate that the proposed method can extract isometry-invariant representations for 3D shape analysis tasks without rotation augmentation, and outperforms other state-of-the-art methods. The proposed dataset is publicly available at https://github.com/tasx0823/PKUnon-rigid.}
}
@article{XIANG2023109046,
title = {Deep learning for image inpainting: A survey},
journal = {Pattern Recognition},
volume = {134},
pages = {109046},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109046},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200526X},
author = {Hanyu Xiang and Qin Zou and Muhammad Ali Nawaz and Xianfeng Huang and Fan Zhang and Hongkai Yu},
keywords = {Image inpainting, Image restoration, Generative adversarial network, Convolutional neural network},
abstract = {Image inpainting has been widely exploited in the field of computer vision and image processing. The main purpose of image inpainting is to produce visually plausible structure and texture for the missing regions of damaged images. In the past decade, the success of deep learning has brought new opportunities to many vision tasks, which promoted the development of a large number of deep learning-based image inpainting methods. Although these methods have many similarities, they also have their own characteristics due to the differences in data types, application scenarios, computing platforms, etc. It is necessary to classify and summarize these methods to provide a reference for the research community. In this survey, we present a comprehensive overview of recent advances in deep learning-based image inpainting. First, we categorize the deep learning-based techniques from multiple perspectives: inpainting strategies, network structures, and loss functions. Second, we summarize the open source codes and representative public datasets, and introduce the evaluation metrics for quantitative comparisons. Third, we summarize the real-world applications of image inpainting in different scenarios, and give a detailed analysis on the performance of different inpainting algorithms. At last, we conclude the survey and discuss about the future directions.}
}
@article{FEI2023109051,
title = {DcTr: Noise-robust point cloud completion by dual-channel transformer with cross-attention},
journal = {Pattern Recognition},
volume = {133},
pages = {109051},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109051},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005313},
author = {Ben Fei and Weidong Yang and Lipeng Ma and Wen-Ming Chen},
keywords = {Point cloud, 3D Vision, Transformer, Cross-attention, Dual-channel transformer},
abstract = {Current point cloud completion research mainly utilizes the global shape representation and local features to recover the missing regions of 3D shape for the partial point cloud. However, these methods suffer from inefficient utilization of local features and unstructured points prediction in local patches, hardly resulting in a well-arranged structure for points. To tackle these problems, we propose to employ Dual-channel Transformer and Cross-attention (CA) for point cloud completion (DcTr). The DcTr is apt at using local features and preserving a well-structured generation process. Specifically, the dual-channel transformer leverages point-wise attention and channel-wise attention to summarize the deconvolution patterns used in the previous Dual-channel Transformer Point Deconvolution (DCTPD) stage to produce the deconvolution in the current DCTPD stage. Meanwhile, we employ cross-attention to convey the geometric information from the local regions of incomplete point clouds for the generation of complete ones at different resolutions. In this way, we can generate the locally compact and structured point cloud by capturing the structure characteristic of 3D shape in local patches. Our experimental results indicate that DcTr outperforms the state-of-the-art point cloud completion methods under several benchmarks and is robust to various kinds of noise.}
}
@article{SEMENOGLOU2023109132,
title = {Data augmentation for univariate time series forecasting with neural networks},
journal = {Pattern Recognition},
volume = {134},
pages = {109132},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109132},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006124},
author = {Artemios-Anargyros Semenoglou and Evangelos Spiliotis and Vassilios Assimakopoulos},
keywords = {Time series, Forecasting, Data augmentation, Neural networks, M4 competition},
abstract = {Neural networks have been proven particularly accurate in univariate time series forecasting settings, requiring however a significant number of training samples to be effectively trained. In machine learning applications where available data are limited, data augmentation techniques have been successfully used to generate synthetic data that resemble and complement the original train set. Since the potential of data augmentation has been largely neglected in univariate time series forecasting, in this study we investigate nine data augmentation techniques, ranging from simple transformations and adjustments to sophisticated generative models and a novel upsampling approach. We empirically evaluate the impact of data augmentation on forecasting accuracy considering both shallow and deep feed-forward neural networks and time series data sets of different sizes from the M4 and the Tourism competitions. Our results suggest that certain data augmentation techniques that build on upsampling and time series combinations can improve forecasting performance, especially when deep networks are used. However, these improvements become less significant as the initial size of the train set increases.}
}
@article{LIU2023109059,
title = {Joint Graph Learning and Matching for Semantic Feature Correspondence},
journal = {Pattern Recognition},
volume = {134},
pages = {109059},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109059},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005398},
author = {He Liu and Tao Wang and Yidong Li and Congyan Lang and Yi Jin and Haibin Ling},
keywords = {Feature correspondence, Attention network, Graph matching, Graph learning},
abstract = {In recent years, powered by the learned discriminative representation via graph neural network (GNN) models, deep graph matching methods have made great progresses in the task of matching semantic features. However, these methods usually rely on heuristically generated graph patterns, which may introduce unreliable relationships to hurt the matching performance. In this paper, we propose a joint graph learning and matching network, named GLAM, to explore reliable graph structures for boosting graph matching. GLAM adopts a pure attention-based framework for both graph learning and graph matching. Specifically, it employs two types of attention mechanisms, self-attention and cross-attention for the task. The self-attention discovers the relationships between features to further update feature representations over the learnt structures; and the cross-attention computes cross-graph correlations between the two feature sets to be matched for feature reconstruction. Moreover, the final matching solution is directly derived from the output of the cross-attention layer, without employing a specific matching decision module. The proposed method is evaluated on three popular visual matching benchmarks (Pascal VOC, Willow Object and SPair-71k), and it outperforms previous state-of-the-art graph matching methods on all benchmarks. Furthermore, the graph patterns learnt by our model are validated to be able to remarkably enhance previous deep graph matching methods by replacing their handcrafted graph structures with the learnt ones.}
}
@article{LI2023109075,
title = {Frequency domain regularization for iterative adversarial attacks},
journal = {Pattern Recognition},
volume = {134},
pages = {109075},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109075},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005556},
author = {Tengjiao Li and Maosen Li and Yanhua Yang and Cheng Deng},
keywords = {Adversarial examples, Transfer-based attack, Black-box attack, Frequency-domain characteristics},
abstract = {Adversarial examples have attracted more and more attentions with the prosperity of convolutional neural networks. The transferability of adversarial examples is an important property that makes black-box attacks possible in real-world applications. On the other side, many adversarial defense methods have been proposed to improve the robustness, leading to the requirement for more transferable adversarial examples. Inspired by the regularization term for network parameters at training process, we treat adversarial attacks as training process of inputs and propose regularization constraint for inputs to prevent adversarial examples from overfitting the white-box networks and enhance the transferability. Specifically, we find a universal attribute that the outputs of convolutional neural networks have consistency to the low frequencies of inputs, and based on this, we construct a frequency domain regularization to inputs for iterative attacks. In this way, our method is compatible with existing iterative attack methods and can learn more transferable adversarial examples. Extensive experiments on ImageNet validate the superiority of our method, and compared with several attacks, we achieve attack success rate improvements of 8.0% and 11.5% on average to normal models and defense methods respectively.}
}
@article{LI2023109120,
title = {Robust sparse and low-redundancy multi-label feature selection with dynamic local and global structure preservation},
journal = {Pattern Recognition},
volume = {134},
pages = {109120},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109120},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006008},
author = {Yonghao Li and Liang Hu and Wanfu Gao},
keywords = {Feature selection, Multi-label learning, Sparse learning, Label correlations},
abstract = {Recent years, joint feature selection and multi-label learning have received extensive attention as an open problem. However, there exist three general issues in previous multi-label feature selection methods. First of all, existing methods either consider local label correlations or global label correlations when they design multi-label feature selection methods, in fact, both two types of label correlations are significant for feature selection; second, previous methods use the low-quality graph to excavate local label correlations so that the results of these methods are under-performing; third, feature redundancy is ignored by most of the sparse learning methods. To overcome these challenges, we preserve global label correlations and dynamic local label correlations by preserving graph structure. Additionally, the l2,1-norm and an inner product regularization term are imposed onto the objective function to preserve robust high row-sparsity and to select low redundant features. All the above terms are integrated into one learning framework, and then we utilize a simple yet effective scheme to optimize the framework. Experimental results demonstrate the classification superiority of the proposed method.}
}
@article{FANG2023109099,
title = {UDNet: Uncertainty-aware deep network for salient object detection},
journal = {Pattern Recognition},
volume = {134},
pages = {109099},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109099},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005799},
author = {Yuming Fang and Haiyan Zhang and Jiebin Yan and Wenhui Jiang and Yang Liu},
keywords = {Salient object detection, Contour uncertainty, Feature interaction},
abstract = {Most of the existing deep learning based salient object detection (SOD) models adopt multi-level feature fusion strategies, and have achieved remarkable progress. However, current SOD models still suffer from the uncertainty dilemma in predicting salient probabilities of the pixels surrounding the contour of salient objects. To solve this issue, we propose a novel uncertainty-aware SOD model, where multiple supervision signals, i.e., internal contour uncertainty map, saliency map and external contour uncertainty map, are used to guide the network to not only focus on the pixels in the salient object but also shift its partial attention to the pixels surrounding the contour of salient objects. Furthermore, we introduce a new feature interaction module to aggregate internal contour uncertainty features, saliency features and external contour uncertainty features in the decoding stage, aiming to enhance the model’s ability in dealing with the “uncertain” pixels. Extensive experiments on four public benchmark datasets demonstrate the superiority of the proposed method over the existing state-of-the-art SOD methods. Furthermore, the proposed method shows better attribute-based performance on the SOC dataset, suggesting that the proposed model can also handle challenging scenarios in SOD.}
}
@article{SEGU2023109115,
title = {Batch normalization embeddings for deep domain generalization},
journal = {Pattern Recognition},
volume = {135},
pages = {109115},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109115},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005957},
author = {Mattia Segu and Alessio Tonioni and Federico Tombari},
keywords = {Domain generalization, Domain representation learning, Learning from multiple sources},
abstract = {Domain generalization aims at training machine learning models to perform robustly across different and unseen domains. Several methods train models from multiple datasets to extract domain-invariant features, hoping to generalize to unseen domains. Instead, first we explicitly train domain-dependent representations leveraging ad-hoc batch normalization layers to collect independent domain’s statistics. Then, we propose to use these statistics to map domains in a shared latent space, where membership to a domain is measured by means of a distance function. At test time, we project samples from an unknown domain into the same space and infer properties of their domain as a linear combination of the known ones. We apply the same mapping strategy at training and test time, learning both a latent representation and a powerful but lightweight ensemble model. We show a significant increase in classification accuracy over current state-of-the-art techniques on popular domain generalization benchmarks: PACS, Office-31 and Office-Caltech.}
}
@article{LU2023109127,
title = {Cross-domain structure learning for visual data recognition},
journal = {Pattern Recognition},
volume = {134},
pages = {109127},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109127},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006070},
author = {Yuwu Lu and Xingping Luo and Jiajun Wen and Zhihui Lai and Xuelong Li},
keywords = {Domain adaptation, Cross-domain, Classwise structure learning, Sample reweighting},
abstract = {Unsupervised domain adaptation methods are used to train an effect model by utilizing available knowledge from a labeled source domain for solving tasks in an unlabeled target domain. The most difficult challenge is determining methods to reduce distribution discrepancies and extract the largest number of domain-invariant features between the source and target domains to improve model performance. With the aim of minimizing the domain shift and maximizing domain-invariant feature extraction, we propose a cross-domain structure learning (CDSL) method for visual data recognition, which incorporates global distribution alignment and local discriminative structure preservation to capture the common, underlying features between domains. Specifically, we design a simple but effective classwise structure learning strategy with a specific compactness hierarchy to promote intraclass knowledge transfer and reduce the risk of negative transfer between domains. We also extend CDSL to different kinds of kernelization to address complex situations in the real world. Extensive experiments on several visual data benchmarks demonstrate the effectiveness of our proposed method.}
}
@article{CHEN2023109086,
title = {Domain Generalization by Joint-Product Distribution Alignment},
journal = {Pattern Recognition},
volume = {134},
pages = {109086},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109086},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005660},
author = {Sentao Chen and Lei Wang and Zijie Hong and Xiaowei Yang},
keywords = {Distribution alignment, Distribution divergence, Domain generalization, Feature transformation},
abstract = {In this work, we address the problem of domain generalization for classification, where the goal is to learn a classification model on a set of source domains and generalize it to a target domain. The source and target domains are different, which weakens the generalization ability of the learned model. To tackle the domain difference, we propose to align a joint distribution and a product distribution using a neural transformation, and minimize the Relative Chi-Square (RCS) divergence between the two distributions to learn that transformation. In this manner, we conveniently achieve the alignment of multiple domains in the neural transformation space. Specifically, we show that the RCS divergence can be explicitly estimated as the maximal value of a quadratic function, which allows us to perform joint-product distribution alignment by minimizing the divergence estimate. We demonstrate the effectiveness of our solution through comparison with the state-of-the-art methods on several image classification datasets.}
}
@article{WAN2023109034,
title = {Low-rank 2D local discriminant graph embedding for robust image feature extraction},
journal = {Pattern Recognition},
volume = {133},
pages = {109034},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109034},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005143},
author = {Minghua Wan and Xueyu Chen and Tianming Zhan and Guowei Yang and Hai Tan and Hao Zheng},
keywords = {Feature extraction, Two-dimensional locality preserving projections (2DLPP), Low-rank, Graph embedding (GE), Discrimination information},
abstract = {As a popular feature extraction algorithm, the 2D local preserving projections (2DLPP) algorithm has been successfully applied in many fields. Using 2D image representation, the 2DLPP algorithm preserves the manifold attributes and retains the local information of high-dimensional space data. However, the 2DLPP algorithm may encounter some problems in real-world applications, such as a lack of discriminatory ability, singularity problems, and sensitivity to occlusion and noise in data. Therefore, this paper introduces low-rank into the 2DLPP algorithm and proposes a new feature extraction algorithm, which is the low-rank two-dimensional local discriminant graph embedding (LR-2DLDGE), to solve these problems. To improve the LR-2DLDGE algorithm robustness, we fuse the discriminant information in graph embedding and the low-rank properties of the data. The algorithm has three advantages: First, the algorithm uses a graph embedding (GE) framework to maintain the local neighbourhood discrimination information between data. Second, the LR-2DLDGE algorithm ensures that the data points are as independent as possible from different classes in the feature space. Finally, the algorithm uses the L1-norm as a constraint and reduces the influence of noise and corruption through low-rank learning. The theoretical computational complexity and convergence of the algorithm are explicated and proved. Extensive experimental results on three occluded and noisy image datasets confirm the effectively and robustness of LR-2DLDGE, respectively.}
}
@article{YU2023109131,
title = {Mix-ViT: Mixing attentive vision transformer for ultra-fine-grained visual categorization},
journal = {Pattern Recognition},
volume = {135},
pages = {109131},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109131},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006112},
author = {Xiaohan Yu and Jun Wang and Yang Zhao and Yongsheng Gao},
keywords = {Ultra-fine-grained visual categorization, Vision transformer, Self-supervised learning, Attentive mixing},
abstract = {Ultra-fine-grained visual categorization (ultra-FGVC) moves down the taxonomy level to classify sub-granularity categories of fine-grained objects. This inevitably poses a challenge, i.e., classifying highly similar objects with limited samples, which impedes the performance of recent advanced vision transformer methods. To that end, this paper introduces Mix-ViT, a novel mixing attentive vision transformer to address the above challenge towards improved ultra-FGVC. The core design is a self-supervised module that mixes the high-level sample tokens and learns to predict whether a token has been substituted after attentively substituting tokens. This drives the model to understand the contextual discriminative details among inter-class samples. Via incorporating such a self-supervised module, the network gains more knowledge from the intrinsic structure of input data and thus improves generalization capability with limited training sample. The proposed Mix-ViT achieves competitive performance on seven publicly available datasets, demonstrating the potential of vision transformer compared to CNN for the first time in addressing the challenging ultra-FGVC tasks. The code is available at https://github.com/Markin-Wang/MixViT}
}
@article{MALDONADO2023109058,
title = {Mitigating the effect of dataset shift in clustering},
journal = {Pattern Recognition},
volume = {134},
pages = {109058},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109058},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005386},
author = {Sebastián Maldonado and Ramiro Saltos and Carla Vairetti and José Delpiano},
keywords = {Induced ordered weighted average, Kernel k-means, OWA operators, Dataset shift, Clustering},
abstract = {Dataset shift is a relevant topic in unsupervised learning since many applications face evolving environments, causing an important loss of generalization and performance. Most techniques that deal with this issue are designed for data stream clustering, whose goal is to process sequences of data efficiently under Big Data. In this study, we claim dataset shift is an issue for static clustering tasks in which data is collected over a long period. To mitigate it, we propose Time-weighted kernel k-means, a k-means variant that includes a time-dependent weighting process. We do this via the induced ordered weighted average (IOWA) operator. The weighting process acts as a gradual forgetting mechanism, prioritizing recent examples over outdated ones in the clustering algorithm. The computational experiments show the potential Time-weighted kernel k-means has in evolving environments.}
}
@article{FANG2023109139,
title = {M2RNet: Multi-modal and multi-scale refined network for RGB-D salient object detection},
journal = {Pattern Recognition},
volume = {135},
pages = {109139},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109139},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006197},
author = {Xian Fang and Mingfeng Jiang and Jinchao Zhu and Xiuli Shao and Hongpeng Wang},
keywords = {Saliency detection, Deep learning, Multi-modal feature, Multi-scale feature, Loss function},
abstract = {Salient object detection is a fundamental topic in computer vision, which has promising application prospects. The previous methods based on RGB-D may potentially suffer from the incompatibility of multi-modal feature fusion and the insufficiency of multi-scale feature aggregation. To tackle these two dilemmas, we propose a novel multi-modal and multi-scale refined network (M2RNet). Specifically, three essential components are presented in this network. The nested dual attention module (NDAM) explicitly exploits the combined features of RGB and depth flows. The adjacent interactive aggregation module (AIAM) gradually integrates the neighbor features of high, middle and low levels. The joint hybrid optimization loss (JHOL) makes the predictions have a prominent outline. Extensive experiments quantitatively and qualitatively demonstrate that our method outperforms other state-of-the-art approaches.}
}
@article{TIAN2023109050,
title = {Multi-stage image denoising with the wavelet transform},
journal = {Pattern Recognition},
volume = {134},
pages = {109050},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109050},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005301},
author = {Chunwei Tian and Menghua Zheng and Wangmeng Zuo and Bob Zhang and Yanning Zhang and David Zhang},
keywords = {Image denoising, CNN, Wavelet transform, Dynamic convolution, Signal processing},
abstract = {Deep convolutional neural networks (CNNs) are used for image denoising via automatically mining accurate structure information. However, most of existing CNNs depend on enlarging depth of designed networks to obtain better denoising performance, which may cause training difficulty. In this paper, we propose a multi-stage image denoising CNN with the wavelet transform (MWDCNN) via three stages, i.e., a dynamic convolutional block (DCB), two cascaded wavelet transform and enhancement blocks (WEBs) and a residual block (RB). DCB uses a dynamic convolution to dynamically adjust parameters of several convolutions for making a tradeoff between denoising performance and computational costs. WEB uses a combination of signal processing technique (i.e., wavelet transformation) and discriminative learning to suppress noise for recovering more detailed information in image denoising. To further remove redundant features, RB is used to refine obtained features for improving denoising effects and reconstruct clean images via improved residual dense architectures. Experimental results show that the proposed MWDCNN outperforms some popular denoising methods in terms of quantitative and qualitative analysis. Codes are available at https://github.com/hellloxiaotian/MWDCNN.}
}
@article{LI2023109074,
title = {Multi-label feature selection via robust flexible sparse regularization},
journal = {Pattern Recognition},
volume = {134},
pages = {109074},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109074},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005544},
author = {Yonghao Li and Liang Hu and Wanfu Gao},
keywords = {Multi-label learning, Feature selection, Sparse regularization, Classification},
abstract = {Multi-label feature selection is an efficient technique to deal with the high dimensional multi-label data by selecting the optimal feature subset. Existing researches have demonstrated that l1-norm and l2,1-norm are promising roles for multi-label feature selection. However, two important issues are ignored when existing l1-norm and l2,1-norm based methods select discriminative features for multi-label data. First, l1-norm can enforce sparsity on each feature across all instances while numerous selected features lack discrimination due to the generated zero weight values. Second, l2,1-norm not only neglects label-specific features but also ignores the redundancy among features. To this end, we design a Robust Flexible Sparse Regularization norm (RFSR), furthermore, proposing a global optimization framework named Robust Flexible Sparse regularized multi-label Feature Selection (RFSFS) based on RFSR. Finally, an efficient alternating multipliers based optimization scheme is developed to iteratively optimize RFSFS. Empirical studies on fifteen benchmark multi-label data sets demonstrate the effectiveness and efficiency of RFSFS.}
}
@article{ZHANG2023109098,
title = {AugFCOS: Augmented fully convolutional one-stage object detection network},
journal = {Pattern Recognition},
volume = {134},
pages = {109098},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109098},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005787},
author = {Xiuwei Zhang and Wei Guo and Yinghui Xing and Wenna Wang and Hanlin Yin and Yanning Zhang},
keywords = {Feature pyramid network, Object detection, Sample selection, Attention module},
abstract = {As a pioneering work of introducing the idea of full convolutional network into the field of object detection, the fully convolutional one-stage object detection network (FCOS) has the advantage of excellent performance with low memory overhead. However, there are certain problems with FCOS that merit more research: the centerness quality assessment loss does not decrease during the late training stage, and its adaptive training sample selection (ATSS) relies heavily on the hyperparameter. To solve the aforementioned problems, we propose a novel object detection network, named augmented fully convolutional one-stage object detection network (AugFCOS). First of all, we propose an improved dynamic optimization loss (DOL) to mitigate the impact of the original centerness loss not decreasing. Then, a Robust Training Sample Selection (RTSS) is proposed to get rid of the dependence of hyper-parameter in ATSS of FCOS. Finally, a novel mixed attention feature pyramid network (MAFPN) is presented to enhance the multi-scale representation ability of feature pyramid network (FPN) and further improve the ability of multi-scale detection. The experimental results on MS COCO demonstrate the effectiveness of our proposed AugFCOS, where AugFCOS achieves approximate 2.0% to 2.9% increase when compared with ATSS and FCOS.}
}
@article{WU2023109114,
title = {Semi-supervised adaptive kernel concept factorization},
journal = {Pattern Recognition},
volume = {134},
pages = {109114},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109114},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005945},
author = {Wenhui Wu and Junhui Hou and Shiqi Wang and Sam Kwong and Yu Zhou},
keywords = {Concept factorization, Semi-supervised learning, Clustering, Nonnegative matrix factorization, Kernel method},
abstract = {Kernelized concept factorization (KCF) has shown its advantage on handling data with nonlinear structures; however, the kernels involved in the existing KCF-based methods are empirically predefined, which may compromise the performance. In this paper, we propose semi-supervised adaptive kernel concept factorization (SAKCF), which integrates the data representation and kernel learning into a unified model to make the two learning processes adapt to each other. SAKCF extends traditional KCF in a semi-supervised manner, which encourages the high-dimensional representation to be consistent with both the limited supervisory and local geometric information. Besides, an alternating iterative algorithm is proposed to solve the resulting constrained optimization problem. Experimental results on six real-world data sets verify the effectiveness and advantages of our SAKCF over state-of-the-art methods when applied on the clustering task.}
}
@article{HOU2023109062,
title = {Towards Parameter-Free Clustering for Real-World Data},
journal = {Pattern Recognition},
volume = {134},
pages = {109062},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109062},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005428},
author = {Jian Hou and Huaqiang Yuan and Marcello Pelillo},
keywords = {Clustering, Real-world data, Dominant set, Density peak},
abstract = {While many clustering algorithms have been published, existing algorithms are often afflicted by some problems in processing real-world data. We present an algorithm to deal with two of these problems in this paper. First, the majority of clustering algorithms depend on one or more parameters. Second, some algorithms are not suitable for clusters of Gaussian distribution, whereas clusters of many real datasets are of Gaussian distribution approximately. Our algorithm generates clusters sequentially, and each cluster is obtained by expanding an initial cluster. The initial cluster is extracted with the dominant set algorithm, and we study the correlation between the pairwise data similarity matrix and clustering result to determine the involved scaling parameter adaptively. In expanding the initial cluster, we improve the density peak algorithm so that the expansion will not cross the boundary between two clusters, and the involved density parameter has little influence on clustering results. In our algorithm, the cluster expansion enables our algorithm to work well with clusters of Gaussian distribution, and two involved parameters can be fixed or determined adaptively. Our algorithm goes a step forward in parameter-free clustering for real-world data, and it is shown to perform better than or comparably to some commonly used algorithms with parameters in experiments with synthetic datasets composed of Gaussian clusters and real datasets.}
}
@article{DEHANDSCHUTTER2023109102,
title = {A consistent and flexible framework for deep matrix factorizations},
journal = {Pattern Recognition},
volume = {134},
pages = {109102},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109102},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005829},
author = {Pierre {De Handschutter} and Nicolas Gillis},
keywords = {Deep matrix factorization, Loss functions, Constrained optimization, First-order methods, Hyperspectral unmixing},
abstract = {Deep matrix factorizations (deep MFs) are recent unsupervised data mining techniques inspired by constrained low-rank approximations. They aim to extract complex hierarchies of features within high-dimensional datasets. Most of the loss functions proposed in the literature to evaluate the quality of deep MF models and the underlying optimization frameworks are not consistent because different losses are used at different layers. In this paper, we introduce two meaningful loss functions for deep MF and present a generic framework to solve the corresponding optimization problems. We illustrate the effectiveness of this approach through the integration of various constraints and regularizations, such as sparsity, nonnegativity and minimum-volume. The models are successfully applied on both synthetic and real data, namely for hyperspectral unmixing and extraction of facial features.}
}
@article{PANG2023109138,
title = {Cross-modal co-feedback cellular automata for RGB-T saliency detection},
journal = {Pattern Recognition},
volume = {135},
pages = {109138},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109138},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006185},
author = {Yu Pang and Hao Wu and Chengdong Wu},
keywords = {RGB-T saliency detection, Cellular automata, Cross-modal co-feedback framework, Pixel-wise refinement},
abstract = {Saliency cellular automata (CA), a temporally evolving model to efficiently locate salient object, has achieved great progress in saliency detection task. However, most the previous CA models originate from RGB data and are thus limited in some extreme scenes. Inspired by the observation that thermal infrared data (T) can overcome the limitation of RGB data themselves in some cases and RGB-T saliency detection has gained more and more attention. In this paper, we contribute a novel RGB-T saliency detection approach via Cross-modal Co-feedback Cellular Automata (C3A). Before this, we firstly present a novel weighted background-based map (WBM) to give each superpixel(image patch) an initial saliency value. Then C3A is proposed to improve the quality of the WBM. Specifically, it firstly establishes two complementary cellular automata (CA) mechanisms dependent on RGB and thermal infrared data, which respectively refine the WBM based on two different perspectives. To bridge RGB-T modalities, an iterative cross-modal co-feedback framework is contributed to optimize constantly their results. In other words, we regard the result of one modality(RGB or T)-induced CA as important feedback to update and optimize another modality(T or RGB)-induced CA during the iteration. Two modalities constantly pull out the useful and confident data to the opposite side, and so two CAs’ results are constantly refined until a stable state is generated, we then integrate the results of two modalities-induced CAs into the saliency map. Finally, a novel boundary-guided pixel-wise refinement (BPR) technology is proposed to further overcome the influence of inaccurate superpixel segmentation to the C3A and refine the saliency map generated by our C3A. For fairness, the proposed method is compared with other state-of-the-art methods on three RGB-T datasets, experimental results show the superiority of our model.}
}
@article{GAVINI2023109069,
title = {Thermal to Visual Person Re-Identification Using Collaborative Metric Learning Based on Maximum Margin Matrix Factorization},
journal = {Pattern Recognition},
volume = {134},
pages = {109069},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109069},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005490},
author = {Yaswanth Gavini and Arun Agarwal and B.M. Mehtre},
keywords = {Thermal to visual person re-identification, Cross-domain image retrieval, Collaborative metric learning, Matrix factorization},
abstract = {Thermal to visual person re-identification (T2V-ReID) is a cross-domain image retrieval problem. In this problem, the matching of a person’s image takes place, where the image is taken by different cameras (thermal and visual) at different times. This problem has numerous applications in night-time security surveillance. It is challenging due to the large intra-class variations and cross-domain discrepancies. Recently, deep metric learning methods are proposed for this problem. Still, there is a scope to improve the metric learning by generalizing the metric. In this paper, we have proposed the collaborative metric learning using Maximum Margin Matrix Factorization. It uses the group-wise similarities and collaboratively predicts the similarities. We can learn a more generalized metric by utilizing the maximized margin in this method. The proposed method is tested on the RegDB and RGB-D-T data sets, and the method outperforms the existing works in the few-shot learning settings.}
}
@article{PEIS2023109130,
title = {Unsupervised learning of global factors in deep generative models},
journal = {Pattern Recognition},
volume = {134},
pages = {109130},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109130},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006100},
author = {Ignacio Peis and Pablo M. Olmos and Antonio Artés-Rodríguez},
keywords = {VAE, Deep generative models, Global factors, Unsupervised learning, Disentanglement, Representation learning},
abstract = {We present a novel deep generative model based on non i.i.d. variational autoencoders that captures global dependencies among observations in a fully unsupervised fashion. In contrast to the recent semi-supervised alternatives for global modeling in deep generative models, our approach combines a mixture model in the local or data-dependent space and a global Gaussian latent variable, which lead us to obtain three particular insights. First, the induced latent global space captures interpretable disentangled representations with no user-defined regularization in the evidence lower bound (as in β-VAE and its generalizations). Second, we show that the model performs domain alignment to find correlations and interpolate between different databases. Finally, we study the ability of the global space to discriminate between groups of observations with non-trivial underlying structures, such as face images with shared attributes or defined sequences of digits images.}
}
@article{LIU2023109109,
title = {Towards open-set text recognition via label-to-prototype learning},
journal = {Pattern Recognition},
volume = {134},
pages = {109109},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109109},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005891},
author = {Chang Liu and Chun Yang and Hai-Bo Qin and Xiaobin Zhu and Cheng-Lin Liu and Xu-Cheng Yin},
keywords = {Open-set recognition, Scene text recognition, Low-shot recognition},
abstract = {Scene text recognition is a popular research topic which is also extensively utilized in the industry. Although many methods have achieved satisfactory performance for the close-set text recognition challenges, these methods lose feasibility in open-set scenarios, where collecting data or retraining models for novel characters could yield a high cost. For example, annotating samples for foreign languages can be expensive, whereas retraining the model each time when a “novel” character is discovered from historical documents costs both time and resources. In this paper, we introduce and formulate a new open-set text recognition task which demands the capability to spot and recognize novel characters without retraining. A label-to-prototype learning framework is also proposed as a baseline for the new task. Specifically, the framework introduces a generalizable label-to-prototype mapping function to build prototypes (class centers) for both seen and unseen classes. An open-set predictor is then utilized to recognize or reject samples according to the prototypes. The implementation of rejection capability over out-of-set characters allows automatic spotting of unknown characters in the incoming data stream. Extensive experiments show that our method achieves promising performance on a variety of zero-shot, close-set, and open-set text recognition datasets.}
}
@article{FANG2023109057,
title = {End-to-end kernel learning via generative random Fourier features},
journal = {Pattern Recognition},
volume = {134},
pages = {109057},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109057},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005374},
author = {Kun Fang and Fanghui Liu and Xiaolin Huang and Jie Yang},
keywords = {Generative random Fourier features, Kernel learning, End-to-end, One-stage, Generative network, Adversarial robustness},
abstract = {Random Fourier features (RFFs) provide a promising way for kernel learning in a spectral case. Current RFFs-based kernel learning methods usually work in a two-stage way. In the first-stage process, learning an optimal feature map is often formulated as a target alignment problem, which aims to align the learned kernel with a pre-defined target kernel (usually the ideal kernel). In the second-stage process, a linear learner is conducted with respect to the mapped random features. Nevertheless, the pre-defined kernel in target alignment is not necessarily optimal for the generalization of the linear learner. Instead, in this paper, we consider a one-stage process that incorporates the kernel learning and linear learner into a unifying framework. To be specific, a generative network via RFFs is devised to implicitly learn the kernel, followed by a linear classifier parameterized as a full-connected layer. Then the generative network and the classifier are jointly trained by solving an empirical risk minimization (ERM) problem to reach a one-stage solution. This end-to-end scheme naturally allows deeper features, in correspondence to a multi-layer structure, and shows superior generalization performance over the classical two-stage, RFFs-based methods in real-world classification tasks. Moreover, inspired by the randomized resampling mechanism of the proposed method, its enhanced adversarial robustness is investigated and experimentally verified.}
}
@article{GAO2023109073,
title = {Video Object Segmentation using Point-based Memory Network},
journal = {Pattern Recognition},
volume = {134},
pages = {109073},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109073},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005532},
author = {Mingqi Gao and Jungong Han and Feng Zheng and James J.Q. Yu and Giovanni Montana},
keywords = {Video object segmentation, Point-based feature matching, Adaptive matching module},
abstract = {Recent years have witnessed the prevalence of memory-based methods for Semi-supervised Video Object Segmentation (SVOS) which utilise past frames efficiently for label propagation. When conducting feature matching, fine-grained multi-scale feature matching has typically been performed using all query points, which inevitably results in redundant computations and thus makes the fusion of multi-scale results ineffective. In this paper, we develop a new Point-based Memory Network, termed as PMNet, to perform fine-grained feature matching on hard samples only, assuming that easy samples can already obtain satisfactory matching results without the need for complicated multi-scale feature matching. Our approach first generates an uncertainty map from the initial decoding outputs. Next, the fine-grained features at uncertain locations are sampled to match the memory features on the same scale. Finally, the matching results are further decoded to provide a refined output. The point-based scheme works with the coarsest feature matching in a complementary and efficient manner. Furthermore, we propose an approach to adaptively perform global or regional matching based on the motion history of memory points, making our method more robust against ambiguous backgrounds. Experimental results on several benchmark datasets demonstrate the superiority of our proposed method over state-of-the-art methods.}
}
@article{LIN2023109021,
title = {Self-Supervised Leaf Segmentation under Complex Lighting Conditions},
journal = {Pattern Recognition},
volume = {135},
pages = {109021},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109021},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005015},
author = {Xufeng Lin and Chang-Tsun Li and Scott Adams and Abbas Z. Kouzani and Richard Jiang and Ligang He and Yongjian Hu and Michael Vernon and Egan Doeven and Lawrence Webb and Todd Mcclellan and Adam Guskich},
keywords = {Self-supervised learning, Convolutional neural networks, Image-based plant phenotyping, Leaf segmentation, Color correction, Cannabis},
abstract = {As an essential prerequisite task in image-based plant phenotyping, leaf segmentation has garnered increasing attention in recent years. While self-supervised learning is emerging as an effective alternative to various computer vision tasks, its adaptation for image-based plant phenotyping remains rather unexplored. In this work, we present a self-supervised leaf segmentation framework consisting of a self-supervised semantic segmentation model, a color-based leaf segmentation algorithm, and a self-supervised color correction model. The self-supervised semantic segmentation model groups the semantically similar pixels by iteratively referring to the self-contained information, allowing the pixels of the same semantic object to be jointly considered by the color-based leaf segmentation algorithm for identifying the leaf regions. Additionally, we propose to use a self-supervised color correction model for images taken under complex illumination conditions. Experimental results on datasets of different plant species demonstrate the potential of the proposed self-supervised framework in achieving effective and generalizable leaf segmentation.}
}
@article{MAO2023109097,
title = {Enhancing 3D-2D Representations for Convolution Occupancy Networks},
journal = {Pattern Recognition},
volume = {134},
pages = {109097},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109097},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005775},
author = {Qing Mao and Rui Li and Yu Zhu and Jinqiu Sun and Yanning Zhang},
keywords = {Implicit 3D representation, Multi-scale 3D position encoding, 3D Correlation-Guided Attentions},
abstract = {Convolutional Occupancy Networks (ConvONet) have gained popularity in object-level and scene-level reconstruction. However, how to better represent the 3D features for ConvONet remains an open question. In this paper, we propose to improve the representation for ConvONet by enhancing both 3D positional information and 3D-2D correlations. Considering that position information acts as the fundamental component of a 3D shape, we propose a Position-Aware Transformer (PAT) architecture that incorporates the Adaptive Multi-Scale Position Encoding (AMSPE) into the self-attention computation. By leveraging both global and local position aggregations in a multi-level manner, AMSPE enables better representations of both coarse and fine structures of the 3D shape. Meanwhile, since projecting 3D features to 2D planes for convolution inevitably introduces ambiguous or noisy representations, we propose a 3D Correlation-Guided Enhancement (CGE) network to bridge the gap between 3D and 2D shape representations. Specifically, we leverage the projected 3D correlations from PAT as the structural guidance, then compute the 3D Correlation-Guided Attentions (CGAs) to enhance the most representative features in the 2D space. In this way, the proposed architecture preserves the most informative structural representations while alleviating the impact of the mis-projected and noisy features. Experiments on ShapeNet and indoor scene dataset demonstrate the superiority of our method. Both quantitative and qualitative experiments show that our method achieves state-of-the-art performance for implicit-based 3D reconstruction.}
}
@article{NG2023109045,
title = {Fuzzy Superpixel-based Image Segmentation},
journal = {Pattern Recognition},
volume = {134},
pages = {109045},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109045},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005258},
author = {Tsz Ching Ng and Siu Kai Choy and Shu Yan Lam and Kwok Wai Yu},
keywords = {Fuzzy algorithm, Graph theory, Mean-shift, Segmentation, Superpixel},
abstract = {This article presents a multi-phase image segmentation methodology based on fuzzy superpixel decomposition, aggregation and merging. First, a collection of layers of dense fuzzy superpixels is generated by the variational fuzzy decomposition algorithm. Then a layer of refined superpixels is extracted by aggregating various layers of dense fuzzy superpixels using the hierarchical normalized cuts. Finally, the refined superpixels are projected into the low dimensional feature spaces by the multidimensional scaling and the segmentation result is obtained via the mean-shift-based merging approach with the spatial bandwidth adjustment strategy. Our algorithm utilizes the superimposition of fuzzy superpixels to impose more accurate spatial constraints on the final segmentation through the fuzzy superpixel aggregation. The fuzziness of superpixels also provides spatial features to measure affinities between fuzzy superpixels and refined superpixels, and guide the merging process. Comparative experiments with the existing approaches reveal a superior performance of the proposed method.}
}
@article{DUONG2023109126,
title = {Deep MinCut: Learning Node Embeddings by Detecting Communities},
journal = {Pattern Recognition},
volume = {134},
pages = {109126},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109126},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006069},
author = {Chi Thang Duong and Thanh Tam Nguyen and Trung-Dung Hoang and Hongzhi Yin and Matthias Weidlich and Quoc Viet Hung Nguyen},
keywords = {Node embedding, Graph representation learning, Community detection, Interpretable machine learning},
abstract = {We present Deep MinCut (DMC), an unsupervised approach to learn node embeddings for graph-structured data. It derives node representations based on their membership in communities. As such, the embeddings directly provide insights into the graph structure, so that a separate clustering step is no longer needed. DMC learns both, node embeddings and communities, simultaneously by minimizing the mincut loss, which captures the number of connections between communities. Striving for high scalability, we also propose a training process for DMC based on minibatches. We provide empirical evidence that the communities learned by DMC are meaningful and that the node embeddings are competitive in different node classification benchmarks.}
}
@article{DAI2023109108,
title = {PFEMed: Few-shot medical image classification using prior guided feature enhancement},
journal = {Pattern Recognition},
volume = {134},
pages = {109108},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109108},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200588X},
author = {Zhiyong Dai and Jianjun Yi and Lei Yan and Qingwen Xu and Liang Hu and Qi Zhang and Jiahui Li and Guoqiang Wang},
keywords = {Deep learning, Domain adaption, Few-shot learning, Medical image classification, Variational autoencoder},
abstract = {Deep learning-based methods have recently demonstrated outstanding performance on general image classification tasks. As optimization of these methods is dependent on a large amount of labeled data, their application in medical image classification is limited. To address this issue, we propose PFEMed, a novel few-shot classification method for medical images. To extract general and specific features from medical images, this method employs a dual-encoder structure, that is, one encoder with fixed weights pre-trained on public image classification datasets and another encoder trained on the target medical dataset. In addition, we introduce a novel prior-guided Variational Autoencoder (VAE) module to enhance the robustness of the target feature, which is the concatenation of the general and specific features. Then, we match the target features extracted from both the support and query medical image samples and predict the category attribution of the query examples. Extensive experiments on several publicly available medical image datasets show that our method outperforms current state-of-the-art few-shot methods by a wide margin, particularly outperforming MetaMed on the Pap smear dataset by over 2.63%.}
}
@article{YU2023109113,
title = {Detecting group concept drift from multiple data streams},
journal = {Pattern Recognition},
volume = {134},
pages = {109113},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109113},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005933},
author = {Hang Yu and Weixu Liu and Jie Lu and Yimin Wen and Xiangfeng Luo and Guangquan Zhang},
keywords = {Concept drift, Data streams, Online learning, Hypothesis test},
abstract = {Concept drift may lead to a sharp downturn in the performance of streaming in data-based algorithms, caused by unforeseeable changes in the underlying distribution of data. In this paper, we are mainly concerned with concept drift across multiple data streams, and in situations where the drift of each data stream cannot be detected in time, due to slight underlying distribution drifts. We call this group concept drift. When compared to the detection of concept drift for a single data stream, the challenges of detecting group concept drift arise from three aspects: first, the training data become more complex; second, the underlying distribution becomes more complex; and third, the correlations between data streams become more complex. To address these challenges, the key idea of our method is to construct a distribution free test statistic, free from any underlying distribution in multiple data streams. Then, for streaming data, we design an online learning algorithm to obtain this test statistic, thereby determining the concept drift caused by the hypothesis test. The experiment evaluations with both synthetic and real-world datasets prove that our method can accurately detect concept drift from multiple data streams.}
}
@article{JUNG2023109061,
title = {Conditional GAN with 3D discriminator for MRI generation of Alzheimer’s disease progression},
journal = {Pattern Recognition},
volume = {133},
pages = {109061},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109061},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005416},
author = {Euijin Jung and Miguel Luna and Sang Hyun Park},
keywords = {Conditional GAN, Alzheimer’s disease, 3D Discriminator, Magnetic resonance image generation, Adaptive identity loss},
abstract = {Many studies aim to predict the degree of deformation on affected brain regions as Alzheimer’s disease (AD) progresses. However, those studies have been often limited since it is difficult to obtain sequential longitudinal MR data of affected patients. Recently, conditional generative adversarial networks (cGANs) have been used to estimate the changes between unpaired images by modeling their differences. However, generating high-quality 3D magnetic resonance (MR) brain images with cGANs requires a large amount of computation. Previous models have been mostly designed to operate in 2D space taking individual slices or down-sampled 3D space, but these approaches often cause spatial artifacts such as discontinuities between slices or unnatural changes in 3D space. To address these limitations, we propose a novel cGAN that can synthesize high-quality 3D MR images at different stages of AD by integrating an additional module that ensures smooth and realistic transitions in 3D space. Specifically, the proposed cGAN model consists of an attention-based 2D generator, a 2D discriminator, and a 3D discriminator that is able to synthesize continuous 2D slices along the axial view resulting in good quality 3D MR volumes. Moreover, we propose an adaptive identity loss so that relevant transformations take place without compromising the features to identify patients. In our experiments, the proposed method showed better image generation performance than previously proposed GAN methods in terms of image quality and image generation suitable for the condition.}
}
@article{HUO2023109032,
title = {Collaborative Learning with Unreliability Adaptation for Semi-Supervised Image Classification},
journal = {Pattern Recognition},
volume = {133},
pages = {109032},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109032},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200512X},
author = {Xiaoyang Huo and Xiangping Zeng and Si Wu and Wenjun Shen and Hau-San Wong},
keywords = {Semi-supervised learning, Image classification, Unreliability adaptation, Collaborative learning},
abstract = {Constructing training goals for unlabeled data is crucial for image classification in the semi-supervised setting. Consistency regularization typically encourages a model to produce consistent predictions with the given training goals, while unreliability adaptation aims to learn the transition probabilities from model predictions to training goals, instead of enforcing their consistency. In this paper, we present a model of Collaborative learning with Unreliability Adaptation (CoUA), in which multiple constituent networks collaboratively learn with each other by adapting their predictions. Toward this end, an additional adaptation module is incorporated into each network to learn a transition probability from its own prediction to that of the paired network. Therefore, the networks can exchange training experience, without being overly sensitive to the unreliability of predictions. To further enhance the collaborative learning, each network is encouraged to produce consistent predictions with the consensus results, while being resistant to the adversarial perturbations against others. Therefore, the networks are able to mutually reinforce each other. We perform extensive experiments on multiple image classification benchmarks to verify the superiority of the co-adaptation based collaborative learning mechanism.}
}
@article{HELM2023109085,
title = {Distance-based positive and unlabeled learning for ranking},
journal = {Pattern Recognition},
volume = {134},
pages = {109085},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109085},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005659},
author = {Hayden S. Helm and Amitabh Basu and Avanti Athreya and Youngser Park and Joshua T. Vogelstein and Carey E. Priebe and Michael Winding and Marta Zlatic and Albert Cardona and Patrick Bourke and Jonathan Larson and Marah Abdin and Piali Choudhury and Weiwei Yang and Christopher W. White},
keywords = {Positive-and-unlabeled learning, ranking, network analysis},
abstract = {Learning to rank – producing a ranked list of items specific to a query and with respect to a set of supervisory items – is a problem of general interest. The setting we consider is one in which no analytic description of what constitutes a good ranking is available. Instead, we have a collection of representations and supervisory information consisting of a (target item, interesting items set) pair. We demonstrate analytically, in simulation, and in real data examples that learning to rank via combining representations using an integer linear program is effective when the supervision is as light as “these few items are similar to your item of interest.” While this nomination task is quite general, for specificity we present our methodology from the perspective of vertex nomination in graphs. The methodology described herein is model agnostic.}
}
@article{LIU2023109101,
title = {A dense light field reconstruction algorithm for four-dimensional optical flow constraint equation},
journal = {Pattern Recognition},
volume = {134},
pages = {109101},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109101},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005817},
author = {Jian Liu and Na Song and Zhengde Xia and Bin Liu and Jinxiao Pan and Abdul Ghaffar and Jianbin Ren and Ming Yang},
keywords = {Light field, Optical flow, A dense reconstruction},
abstract = {Dense light field sampling is an important basis for refocusing, depth estimation and 3-D imaging. It is difficult to obtain high resolution dense light field with a large-scale camera array and expensive equipment. At the same time, the current storage devices and transmission bandwidth also limit this technology's post-processing and application. In order to effectively reconstruct the angle domain of the light field based on the sparse light field data, this paper analyzes the correlation and constraint relationship between the optical flow field and the motion field of multi-view images in the same scene, extends the traditional optical flow constraint equation of two-dimensional imaging to the optical flow constraint equation of four-dimensional light field, and establishes an effective mathematical model. The coordinate position of the original pixel in the new angle image is determined by coordinate search, and its intensity is reconstructed. The experimental results of multi-scene dense reconstruction show that the proposed method can reconstruct the texture, shadow and color information in the light field of a long-baseline scene with high quality. The quantitative evaluation results show that the algorithm can be applied to dense light field reconstruction of complex scenes. The algorithm in this paper is only suitable for the case of optical flow constraint in a linear light field, and the follow-up research will focus on the case of nonlinear optical flow constraint.}
}
@article{QIN2023109125,
title = {Weakly supervised adversarial learning via latent space for hyperspectral target detection},
journal = {Pattern Recognition},
volume = {135},
pages = {109125},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109125},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006057},
author = {Haonan Qin and Weiying Xie and Yunsong Li and Kai Jiang and Jie Lei and Qian Du},
keywords = {Hyperspectral image, Target detection, Weakly supervised learning, Adversarial learning, Latent space},
abstract = {As an advanced technique in remote sensing, hyperspectral target detection (HTD) is widely concerned in civilian and military applications. However, the limitation of prior and mixed pixels phenomenon makes HTD models sensitive to data corruption under various interference from environment. In this work, a novel two-stage detection framework based on adversarial learning is proposed, which extracts spectral features in latent space through background reconstruction under weak supervision. To address the issues of insufficient utilization of both background information and limited prior knowledge, the generative adversarial network (GAN) is applied to estimate background in a weakly supervised manner with target-based constraints and channel-wise attention, which produces the detection proposal in the first stage. Then, a refined result is produced in the second stage, in which the input data consists of the refined data and refined feature map based on previous detection proposal. To provide samples for weakly supervised learning (WSL), the pseudo datasets are produced by a coarse sample selection procedure, which makes full use of limited prior information. Finally, an exponential constrained nonlinear function is adopted to acquire pixel-level prediction via suppressing the background and combining features from different stages. Experiments on real hyperspectral images (HSIs) captured by different sensors at various scenes verify the effectiveness of the proposed framework.}
}
@article{ZHAO2023109056,
title = {Continuous label distribution learning},
journal = {Pattern Recognition},
volume = {133},
pages = {109056},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109056},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005362},
author = {Xingyu Zhao and Yuexuan An and Ning Xu and Xin Geng},
keywords = {Label distribution learning, Continuous label distribution, Label ambiguity, Label encoding, Label correlations},
abstract = {Label distribution learning (LDL) is a suitable paradigm to deal with label ambiguity through learning the correlations among different labels. Most existing label distribution learning methods consider the labels to be discrete and directly establish the mapping from features to labels. However, in many real-world applications, labels naturally form a continuous distribution, which is ignored by the existing methods. As a result, the distribution information of labels can not be accurately described and finally affects the whole learning system. The goal of this paper is to propose a novel approach which can capture the continuous distribution of different labels explicitly and effectively. Specifically, we propose Continuous Label Distribution Learning (CLDL) which describes labels as a continuous density function and learns the distribution information of the labels in the latent space. In this way, the high-order correlations among different labels can be effectively extracted and only a few parameters for describing the continuous distribution need to be learned. Extensive description degree prediction experiments on real-world datasets validate the superiority of CLDL over the existing approaches.}
}
@article{ALALIMI2023109096,
title = {IDA: Improving distribution analysis for reducing data complexity and dimensionality in hyperspectral images},
journal = {Pattern Recognition},
volume = {134},
pages = {109096},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109096},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005763},
author = {Dalal AL-Alimi and Mohammed A.A. Al-qaness and Zhihua Cai and Eman Ahmed Alawamy},
keywords = {Feature reduction, Hyperspectral image, Classification, Feature fusion, Feature extraction, Dimensionality reduction},
abstract = {Hyperspectral images (HSIs) are known for their high dimensionality and wide spectral bands that increase redundant information and complicate classification. Outliers and mixed data are common problems in HSIs. Thus, preprocessing methods are essential in enhancing and reducing data complexity, redundant information, and the number of bands. This study introduces a novel feature reduction method (FRM) called improving distribution analysis (IDA). IDA works to increase the correlation between related data, decrease the distance between big and small data, and correct each value's location to be inside its group range. In IDA, the input data passes through three stages. Getting rid of outliers and improving data correlation is the first step. The second stage involves increasing the variance. The third is to simplify the data and normalize the distribution. IDA is compared with four popular FRMs in four available HSIs. It is also tested and evaluated in various classification models, including spatial, spectral, and spectral-spatial models. The experimental results demonstrate that IDA performs admirably in enhancing data distribution, reducing complexity, and accelerating performance.}
}
@article{LI2023109044,
title = {Rethinking referring relationships from a perspective of mask-level relational reasoning},
journal = {Pattern Recognition},
volume = {133},
pages = {109044},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109044},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005246},
author = {Chengyang Li and Liping Zhu and Gangyi Tian and Yi Hou and Heng Zhou},
keywords = {Referring relationship, Multimodal learning, Image and text, Visual grounding, Deep learning},
abstract = {Referring relationship aims at localizing subject and object entities in an image, according to a triple text <subject, predicate, object>. Previous methods use iterative attention to shift between image regions for modeling predicate. However, predicate sometimes is implicit and difficult to be represented in the image domain. Convolution modeling method to express predicate is simple and inappropriate. Besides, relational reasoning information in the text itself is not fully utilized. To this end, we rethink referring relationship from a mask-level relational reasoning perspective to improve model interpretability. For text-to-image reasoning, we design Mask Generate and Mask Transfer modules, so as to fully integrate the text priors into the reasoning and prediction of masks. For image-to-text reasoning, we propose an unsupervised triple reconstruction method to guide text-to-image reasoning and improve multimodal generalization. By bi-directional reasoning between image and text, the proposed method MRR fully conforms to the multimodal relational reasoning process. Experiments show that MRR achieves state-of-the-art performance on two datasets of referring relationships, VRD and Visual Genome.}
}
@article{VAQUERO2023109141,
title = {Real-time siamese multiple object tracker with enhanced proposals},
journal = {Pattern Recognition},
volume = {135},
pages = {109141},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109141},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006215},
author = {Lorenzo Vaquero and Víctor M. Brea and Manuel Mucientes},
keywords = {Multiple visual object tracking, Siamese CNN, Motion estimation},
abstract = {Maintaining the identity of multiple objects in real-time video is a challenging task, as it is not always feasible to run a detector on every frame. Thus, motion estimation systems are often employed, which either do not scale well with the number of targets or produce features with limited semantic information. To solve the aforementioned problems and allow the tracking of dozens of arbitrary objects in real-time, we propose SiamMOTION. SiamMOTION includes a novel proposal engine that produces quality features through an attention mechanism and a region-of-interest extractor fed by an inertia module and powered by a feature pyramid network. Finally, the extracted tensors enter a comparison head that efficiently matches pairs of exemplars and search areas, generating quality predictions via a pairwise depthwise region proposal network and a multi-object penalization module. SiamMOTION has been validated on five public benchmarks, achieving leading performance against current state-of-the-art trackers. Code available at: https://www.github.com/lorenzovaquero/SiamMOTION}
}
@article{XI2023109068,
title = {Learning comprehensive global features in person re-identification: Ensuring discriminativeness of more local regions},
journal = {Pattern Recognition},
volume = {134},
pages = {109068},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109068},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005489},
author = {Jiali Xi and Jianqiang Huang and Shibao Zheng and Qin Zhou and Bernt Schiele and Xian-Sheng Hua and Qianru Sun},
keywords = {Person re-identification, Baseline, Comprehensive},
abstract = {Person re-identification (Re-ID) aims to retrieve person images from a large gallery given a query image of a person of interest. Global information and fine-grained local features are both essential for the representation. However, global embedding learned by naive classification model tends to be trapped in the most discriminative local region, leading to poor evaluation performance. To address the issue, we propose a novel baseline network that learns strong global feature termed as Comprehensive Global Embedding (CGE), ensuring more local regions of global feature maps to be discriminative. In this work, two key modules are proposed including Non-parameterized Local Classifier (NLC) and Global Logits Revise (GLR). The NLC is designed to obtain a score vector of each local region on feature maps in a non-parametric manner. The GLR module directly revises the logits such that the subsequent cross entropy loss up-weights the loss assigned to samples with hard-to-learn local regions. The convergence of the deep model indicates more local regions (the number of local regions is manually defined) on the feature maps of each sample are discriminative. We implement these two modules on two strong baseline methods including the BagTricks (BOT) [1] and AGW [2]. The network achieves 65.9% mAP, 85.1% rank1 on MSMT17, 86.4% mAP, 87.4% rank1 on CUHK03 labeled, 84.2% mAP, 85.9% rank1 on CUHK03 detected, and 92.2% mAP, 96.3% rank1 on Market-1501. The results show that the proposed baseline achieves a new state-of-the-art when using only global embedding during inference without any re-ranking technique.}
}
@article{LI2023109072,
title = {SC-GAN: Subspace Clustering based GAN for Automatic Expression Manipulation},
journal = {Pattern Recognition},
volume = {134},
pages = {109072},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109072},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005520},
author = {Shuai Li and Liang Liu and Ji Liu and Wenfeng Song and Aimin Hao and Hong Qin},
keywords = {Facial attribute manipulation, GANs, Subspace clustering, SIFT K-means cluster},
abstract = {In recent years, the topics of facial attribute manipulation and decomposition have gained great popularity in computer vision and human computer interaction. Even though such methods have been preliminarily employed in some photo beautification applications, it still remains challenging due to the highly-versatile facial attributes and their drastic appearance changes subject to variation of deformation, illumination, pose, etc. The prevailing problems are especially severe when we are faced with group photos involving many faces. To overcome such critical limitations and discover more meaningful visual attributes and their possible decompositions, we develop a subspace clustering based generative adversarial network (SC-GAN) in this paper. Our SC-GAN can simultaneously decompose multiple subspaces and generate diverse samples correspondingly, thus the training of the generative models could be more effectively guided by facial attribute and its decomposition and manipulation in a natural and meaningful fashion. Our SC-GAN incorporates the SIFT K-means cluster, which could split the holistic semantic facial space into different subspaces without supervision, and help the new GAN generate more convincing results within specific subspaces. Extensive experiments and comprehensive evaluations confirm that, our method can greatly reduce the unexpected influences caused by portrait diversities and outperform the state-of-the-art facial attribute manipulation approaches.11https://github.com/buaaswf/SC-GAN/}
}
@article{TAN2023109112,
title = {A label distribution manifold learning algorithm},
journal = {Pattern Recognition},
volume = {135},
pages = {109112},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109112},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005921},
author = {Chao Tan and Sheng Chen and Xin Geng and Genlin Ji},
keywords = {Multi-label learning, Label distribution learning, Manifold learning, Dimension reduction, Linear regression},
abstract = {In this paper, we propose a novel label distribution manifold learning (LDML) method for solving the multilabel distribution learning problem. First, using manifold learning, we extract the accurate and reduced-dimension features of the training data. Second, we estimate the unknown label distributions associated with the extracted reduced-dimension features based on multi-output kernel regression. Third, we use the extracted reduced-dimension features and their associated estimated label distributions to form an enhanced maximum entropy model, which enables us to accurately and efficiently estimate the unknown true label distributions for the training data. We refer to this algorithm as the LDML. We also propose to apply the tangent space alignment regression in the second stage, and the resulting algorithm is called the LDML-R. The LDML-R has better label distribution learning performance than the LDML but imposes higher complexity than the latter. We evaluate the proposed LDML and LDML-R algorithms on 15 real-world data sets with ground-truth label distributions, and the experimental results obtained show that our method has advantages in terms of learning accuracy compared to the latest multi-label distribution learning approaches. We also use another 10 real-world multi-class data sets, which do not have the ground-truth label distributions, to demonstrate the superior multilabel classification performance of our LDML-R algorithm over the existing state-of-the-art multi-label classification algorithms.}
}
@article{LI2023109060,
title = {A framework based on local cores and synthetic examples generation for self-labeled semi-supervised classification},
journal = {Pattern Recognition},
volume = {134},
pages = {109060},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109060},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005404},
author = {Junnan Li and MingQiang Zhou and Qingsheng Zhu and Quanwang Wu},
keywords = {Semi-supervised learning, Semi-supervised classification, Self-labeled techniques, Examples generation, Representatives},
abstract = {Self-labeled techniques are semi-supervised classification models that overcome the shortage of labeled samples via an iterative process. Most relevant proposals are inspired by boosting schemes to iteratively enlarge labeled data, but these methods are constrained by the number and distribution of the initial labeled data. Up to the present, the only exceptions which can solve the above problem are SEG-SSC, k-means-SSC and LC-SSC. However, SEG-SSC relies on too many parameters. Besides, it is hard to improve the distribution of the initial labeled data when the initial labeled set can not roughly represent the distribution of the original data. k-means-SSC and LC-SSC fail to significantly improve the number of the initial labeled data by a limited number of representative points. To address the above issues, this paper proposes a framework based on local cores and synthetic examples generation for self-labeled semi-supervised classification (LCSEG-SSC). First, a new method for finding local cores on labeled and unlabeled data is proposed to improve the distribution of the initial labeled data. Second, STOPF or active labeling is used to predict found local cores. Third, a new example generation technique is proposed to create synthetic labeled samples, intending to improve the number of the initial labeled data. After that, any self-labeled with boosting schemes can be executed on the improved labeled data effectively. Intensive experiments prove that LCSEG-SSC outperforms state-of-the-art methods, especially in a relatively low ratio of labeled data.}
}
@article{YAO2023109084,
title = {Regularizing autoencoders with wavelet transform for sequence anomaly detection},
journal = {Pattern Recognition},
volume = {134},
pages = {109084},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109084},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005647},
author = {Yueyue Yao and Jianghong Ma and Yunming Ye},
keywords = {Sequence anomaly detection, Autoencoder, Discrete wavelet transform, Frequency domain regularization, Sample-adaptive regularization weight},
abstract = {Nowadays, systems or entities are usually monitored by devices, generating large amounts of time series. Detecting anomalies in them help prevent potential losses, thus arousing much research interest. Existing studies have adopted autoencoders to detect anomalies, where reconstruction errors are used to indicate outliers. However, sometimes autoencoders may also reconstruct anomalies well due to the learned general features in latent spaces. To solve the above problem, we propose to regularize autoencoders to grasp specific features of normal sequences. Specifically, spectral unique patterns are captured by statistical analysis on discrete wavelet transform (DWT) coefficients of input sequences, restricting latent spaces to reflect unique patterns of normal sequences in both time and frequency domains. Furthermore, a Weight Controller calculating sample-adaptive regularization weights is designed to fully utilize the regularization effect. Extensive experiments on three public benchmarks demonstrate the effectiveness and superiority of the proposed model compared with state-of-the-art algorithms.}
}
@article{WANG2023109100,
title = {Simultaneous Robust Matching Pursuit for Multi-view Learning},
journal = {Pattern Recognition},
volume = {134},
pages = {109100},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109100},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005805},
author = {Yulong Wang and Kit Ian Kou and Hong Chen and Yuan Yan Tang and Luoqing Li},
keywords = {Greedy algorithm, Multi-view learning, M-estimator, Sparse learning},
abstract = {Joint sparse representation (JSR) has attracted massive attention with many successful applications in pattern recognition recently. In this paper, we propose a novel robust multi-view JSR method referred to as Simultaneous Robust Matching Pursuit (SRMP) based on the outlier-resistant M-estimator originating from robust statistics. Because of the complexity of the objective function, we design an efficient optimization algorithm to implement SRMP based on the half-quadratic theory. In addition, we have also extended the proposed method for the problems of multi-view subspace clustering and multi-view pattern classification, respectively. The experimental results corroborate the efficacy and robustness of SRMP for multi-view data recovery, subspace clustering and classification.}
}
@article{GAO2023109111,
title = {A unified low-order information-theoretic feature selection framework for multi-label learning},
journal = {Pattern Recognition},
volume = {134},
pages = {109111},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109111},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200591X},
author = {Wanfu Gao and Pingting Hao and Yang Wu and Ping Zhang},
keywords = {Feature selection, Multi-label learning, Information theory, Low-order information-theoretic terms, Probability distribution assumption},
abstract = {The approximation of low-order information-theoretic terms for feature selection approaches has achieved success in addressing high-dimensional multi-label data. However, three critical issues exist in such type of approaches: (1) existing approaches are devised based on single heuristic variable correlation assumption, which biases towards some specific scene; (2) high-order variable correlations are ignored by cumulative summation low-order information-theoretic terms; (3) abundant approaches confuse researchers to devise and utilize appropriate approaches. To address these issues, two types of probability distribution assumption in terms of candidate features and labels are derived based on low-order variable correlations. Afterwords, clearing up all information-theoretic terms, we propose a unified feature selection framework including three low-order information-theoretic terms for multi-label learning named Selected Terms of Feature Selection (STFS). STFS contains high-order variable correlations in the form of low-order information-theoretic terms. Furthermore, many previous multi-label feature selection approaches can be reduced to special forms of STFS. Finally, extensive experiments conducted on twelve benchmark data sets in comparison to seven state-of-the-art approaches demonstrate the classification superiority of STFS.}
}
@article{CHEN2023109124,
title = {An efficient point-set registration algorithm with dual terms based on total least squares},
journal = {Pattern Recognition},
volume = {134},
pages = {109124},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109124},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006045},
author = {Qing-Yan Chen and Da-Zheng Feng and Wei-Xing Zheng and Xiang-Wei Feng},
keywords = {Point-set registration, Total least squares, Dual terms, Errors-in-variables (EIV), Bilateral outliers},
abstract = {Point set registration (PSR) is competitive with related techniques because it purposefully captures the overall structure between two point-set patterns. Typically, the point set registration problem can be divided into two sub-problems: (1) search point set correspondence (PSC); (2) estimate spatial transformation matrix (STM). Searching for the best PSC is a classical combinatorial explosion problem, and estimating the STM is a continuous space optimization problem. Also, two feature point sets detected by two low-quality images include point-position errors and often involve bilateral outliers composed of such feature points that cannot form a correspondence relationship. To address the above problems, we propose an efficient PSR algorithm with dual (symmetrical) terms based on the total least squares (DT-TLS), which can correct errors-in-variables and suppress multiple outliers. Meanwhile, the framework of soft decision-making is presented, and a TLS-based criterion is constructed to efficiently exploit the probability and global structures of two-point sets. Such TLS-based criterion with single row-orthonormal STM includes two interesting dual (symmetrical) terms that can be conveniently exploited to suppress bilateral outliers. The experimental results show that DT-TLS achieves better performance than the state-of-the-art algorithms in some multi-view computer vision tasks, indicating that our proposed algorithm is suitable for solving PSR problems.}
}
@article{CHOI2023109055,
title = {Disentangling the correlated continuous and discrete generative factors of data},
journal = {Pattern Recognition},
volume = {133},
pages = {109055},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109055},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005350},
author = {Jaewoong Choi and Geonho Hwang and Myungjoo Kang},
keywords = {Variational autoencoder, Disentanglement, Generative model, Representation learning},
abstract = {Real-world data typically include discrete generative factors, such as category labels and the existence of objects, as well as continuous generative factors. Continuous generative factors may be dependent on or independent of discrete generative factors. For instance, an intra-class variation of a category is dependent on the discrete generative factor, whereas a common variation of all categories is not. Most previous attempts to integrate discrete generative factors into disentanglement assumed statistical independence between the continuous and discrete variables. In this paper, we propose a Variational Autoencoder(VAE) model capable of disentangling both continuous generative factors. To represent these generative factors, we introduce two sets of continuous latent variables: a private variable and a public variable. The private and public variables represent the intra-class variations and common variations in categories, respectively. Our proposed framework models the private variable as a Gaussian mixture and the public variable as a Gaussian. Each mode of the private variable is responsible for a class of discrete variables. Our proposed model, called Discond-VAE, DISentangles the class-dependent CONtinuous factors from the Discrete factors by introducing private variables. The experiments showed that Discond-VAE could discover private and public factors from the data. Moreover, even under the dataset with only public factors, Discond-VAE does not fail and adapts private variables to represent public factors.}
}
@article{LIU2023109079,
title = {Curvilinear Structure Tracking Based on Dynamic Curvature-penalized Geodesics},
journal = {Pattern Recognition},
volume = {134},
pages = {109079},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109079},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005593},
author = {Li Liu and Mingzhu Wang and Shuwang Zhou and Minglei Shu and Laurent D. Cohen and Da Chen},
keywords = {Curvature-penalized geodesics, Local bending constraint, Coherence penalization, Curvilinear structures, Retinal vessels},
abstract = {Geodesic models are considered as a fundamental and powerful tool in the applications of curvilinear structure extraction, where the target structures are usually modeled as geodesic paths connecting prescribed points. Despite great advances in geodesic models, it still remains an unsolved problem of detecting weak curvilinear structures from complicated scenarios. In this paper, a dynamic high-order geodesic model for curvilinear structure extraction is introduced to alleviate the shortcuts or short branches combination problems suffered in the classical geodesic approaches. For that purpose, we take into account the nonlocal pattern of curvilinear structures and the local curvature of geodesic paths for the construction of geodesic metrics. Accordingly, the proposed model is able to blend the benefits from the on-the-fly nonlocal smoothness property, curvature regularization and appearance coherence penalization. The nonlocal smoothness property carried out via a local bending operator is constructed to provide a quantitative measure of geodesic advancing directions, meanwhile the coherence penalization is established to guarantee the consistency of the local appearance features extracted via a vessel detector. The experiment results on synthetic and real images illustrate that the proposed method obtains outperformance when compared to the classical geodesic-based tracing algorithms.}
}
@article{DUAN2023109140,
title = {Iterative embedding distillation for open world vehicle recognition},
journal = {Pattern Recognition},
volume = {135},
pages = {109140},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109140},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006203},
author = {Junxian Duan and Xiang Wu and Yibo Hu and Chaoyou Fu and Zi Wang and Ran He},
keywords = {Vehicle reidentification, Iterative embedding distillation},
abstract = {Vehicle recognition poses a practical but challenging problem in many real-world surveillance applications. Since vehicle recognition is an open-set problem, it is a critical issue to learn a discriminative visual embedding space rather than a well-performing classifier. In this paper, we propose an iterative embedding distillation (IED) framework for open-set vehicle recognition. The soft target in knowledge distillation is utilized to establish the interclass relations from an instance level rather than a category level. Towards the open-set problem, we extend knowledge distillation to embedding distillation in an iterative learning way, in which three types of loss functions are studied to iteratively transfer the distributions of embeddings from the teacher network to the student network. To demonstrate the universal nature of IED, we implement the IED framework on two basic convolutional neural networks and verify it using the cross-dataset testing protocols without retraining or fine-tuning. Extensive experimental results show that IED obtains quite encouraging results and outperforms state-of-the-art methods on various large-scale vehicle recognition datasets including VeRi-776, Vehicle-ID, Vehicle-1M, VD1 and VD2.}
}
@article{YAN2023109119,
title = {Towards deeper match for multi-view oriented multiple kernel learning},
journal = {Pattern Recognition},
volume = {134},
pages = {109119},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109119},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005994},
author = {Wenzhu Yan and Yanmeng Li and Ming Yang},
keywords = {Multi-view representation, Deep kernel, Feature fusion, Classification},
abstract = {Multi-view representation learning aims to exploit the complementary information underlying multiple view data to enhance the expressive power of data representation. Given that kernels in multiple kernel learning naturally correspond to different views, previous shallow similarity learning models cannot fully capture the complex hierarchical information. This work presents an effective deeper match model for multi-view oriented kernel (DMMV) learning which brings a deeper insight into the kernel match for similarity based multi-view representation fusion. Specifically, we propose local deep view-specific self-kernel (LDSvK) by mimicking the deep neural networks to faithfully characterize the local similarity between view-specific samples. Thus, the representation capacity of each view can be saliently analyzed. We build the global deep multi-view fusion kernel (GDMvK) by learning deep fusion of LDSvKs to learn a comprehensive measurement of the cross-view similarity. Notably, the proposed learning framework of the deeper local information extraction and global deep multiple kernel fusion provides a robust way in fitting multi-view data, and yields better learning performance. Experimental results on several multi-view benchmark datasets well demonstrate the effectiveness of our DMMV over other state-of-the-art methods.}
}
@article{CAI2023109067,
title = {High-order manifold regularized multi-view subspace clustering with robust affinity matrices and weighted TNN},
journal = {Pattern Recognition},
volume = {134},
pages = {109067},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109067},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005477},
author = {Bing Cai and Gui-Fu Lu and Liang Yao and Hua Li},
keywords = {High-order manifold regularization, Robust affinity matrices, Multi-view subspace clustering, Weighted TNN},
abstract = {Multi-view subspace clustering achieves impressive performance for high-dimensional data. However, many of these models do not sufficiently mine the intrinsic information among samples and consider the robustness problem of the affinity matrices, resulting in the degradation of clustering performance. To address these problems, we propose a novel high-order manifold regularized multi-view subspace clustering with robust affinity matrices and a weighted tensor nuclear norm (TNN) model (termed HMRMSC) to characterize real-world data. Specifically, all the similarity matrices of different views are first stacked into a third-order tensor. However, the constructed tensor may contain an additional inter-class representation since the data are usually noisy. Then, we use a technique similar to tensor principal component analysis (TPCA) to obtain a more robust similarity tensor, which is constrained by the so-called weighted TNN since the original TNN treats each singular value equally and usually considers no prior information of singular values. In addition, a high-order manifold regularized term is also added to utilize the manifold information of data. Finally, all the steps are unified into a framework, which is resolved by the augmented Lagrange multiplier (ALM) method. Experimental results on six representative datasets show that our model outperforms several state-of-the-art counterparts.}
}
@article{TANG2023109135,
title = {Video representation learning for temporal action detection using global-local attention},
journal = {Pattern Recognition},
volume = {134},
pages = {109135},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109135},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200615X},
author = {Yiping Tang and Yang Zheng and Chen Wei and Kaitai Guo and Haihong Hu and Jimin Liang},
keywords = {Temporal action detection, Video representation, Untrimmed video analysis},
abstract = {Video representation is of significant importance for temporal action detection. The two sub-tasks of temporal action detection, i.e., action classification and action localization, have different requirements for video representation. Specifically, action classification requires video representations to be highly discriminative, so that action features and background features are as dissimilar as possible. For action localization, it is crucial to obtain information about the action itself and the surrounding context for accurate prediction of action boundaries. However, the previous methods failed to extract the optimal representations for the two sub-tasks, whose representations for both sub-tasks are obtained in a similar way. In this paper, a Global-Local Attention (GLA) mechanism is proposed to produce a more powerful video representation for temporal action detection without introducing additional parameters. The global attention mechanism predicts each action category by integrating features in the entire video that are similar to the action while suppressing other features, thus enhancing the discriminability of video representation during the training process. The local attention mechanism uses a Gaussian weighting function to integrate each action and its surrounding contextual information, thereby enabling precise localization of the action. The effectiveness of GLA is demonstrated on THUMOS’14 and ActivityNet-1.3 with a simple one-stage action detection network, achieving state-of-the-art performance among the methods using only RGB images as input. The inference speed of the proposed model reaches 1373 FPS on a single Nvidia Titan Xp GPU. The generalizability of GLA to other detection architectures is verified using R-C3D and Decouple-SSAD, both of which achieve consistent improvements. The experimental results demonstrate that designing representations with different properties for the two sub-tasks leads to better performance for temporal action detection compared to the representations obtained in a similar way.}
}
@article{RAHMAN2023109043,
title = {Tripartite sub-image histogram equalization for slightly low contrast gray-tone image enhancement},
journal = {Pattern Recognition},
volume = {134},
pages = {109043},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109043},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005234},
author = {Hafijur Rahman and Gour Chandra Paul},
keywords = {Contrast enhancement, Slightly low contrast image, Histogram equalization, Mean brightness sustainability, Image quality assessment, Bit-plane specific measure},
abstract = {In this paper, a neoteric tripartite sub-image histogram equalization method is proposed to enhance slightly low contrast gray-tone images, which is a less explored area in the literature. An image is decomposed into three sub-images to preserve its mean brightness, and the histograms of the sub-images are calculated. Then, the snipping procedure is applied to each histogram to constrain the pace of contrast enhancement. Subsequently, the equalization of the three histograms is performed independently, and finally, the three equalized sub-images are composed into a single image. The proposed method offers better outcomes as compared to several common and state-of-the-art histogram equalization-based methods regarding contrast improvement, blind/reference-less image spatial quality evaluator, mean brightness preservation, peak signal-to-noise ratio, mean structural similarity, gradient magnitude similarity deviation, feature similarity, bit-plane to bit-plane similarity, and visual image quality.}
}
@article{KITTLER2024110297,
title = {Edwin Hancock},
journal = {Pattern Recognition},
volume = {150},
pages = {110297},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110297},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324000487},
author = {Josef Kittler and Richard Wilson}
}
@article{LI2023109083,
title = {Auto-weighted Tensor Schatten p-Norm for Robust Multi-view Graph Clustering},
journal = {Pattern Recognition},
volume = {134},
pages = {109083},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109083},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005635},
author = {Xingfeng Li and Zhenwen Ren and Quansen Sun and Zhi Xu},
keywords = {Multi-view clustering, Adaptive neighbors graph learning, Low-rank tensor learning, Noise estimation},
abstract = {Recently, tensor-singular value decomposition based tensor-nuclear norm (t-TNN) has achieved impressive performance for multi-view graph clustering. This primarily ascribes the superiority of t-TNN in exploring high-order structure information among views. However, 1) t-TNN cannot ideally approximate to the original rank minimization, which produces the suboptimal graph tensor; in addition, t-TNN treats different singular values equally, such that the larger singular values corresponding to certain significant feature information (i.e., prior information) has not been utilized fully; 2) the data of original high-dimensional space are often corrupted by noise and outliers, which always makes adaptive neighbors graph learning (ANGL) generate low-quality affinity graphs. To address these issues, we propose a novel multi-view graph clustering method termed auto-weighted tensor Schatten p-norm (t-ATSN) for robust multi-view graph clustering (t-ATSN-RMGC). Concretely, we first propose t-SVD based t-ATSN with 0<p<1 to make the learned graph tensor better approximate the target rank than t-TNN. Meanwhile, it can also automatically and appropriately shrink singular values for constructing a more refined graph tensor, so as to fully capture spatial structure in the graph tensor. Moreover, we introduce the Geman McClure loss function to enhance the robustness of ANGL for noise and outliers. Experimental results on benchmarks across different scenarios and sizes show that the proposed method consistently outperforms state-of-the-art methods.}
}
@article{MANDEL2023109107,
title = {Detection confidence driven multi-object tracking to recover reliable tracks from unreliable detections},
journal = {Pattern Recognition},
volume = {135},
pages = {109107},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109107},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005878},
author = {Travis Mandel and Mark Jimenez and Emily Risley and Taishi Nammoto and Rebekka Williams and Max Panoff and Meynard Ballesteros and Bobbie Suarez},
keywords = {Multi-object tracking, Model uncertainty, Performance evaluation, Scarce data, Dataset, Marine science applications},
abstract = {Multi-object tracking (MOT) systems often rely on accurate object detectors; however, accurate detectors are not available in every application domain. We present Robust Confidence Tracking (RCT), an offline MOT algorithm designed for settings where detection quality is poor. Whereas prior methods simply threshold and discard detection confidence information, RCT relies on the exact detection confidence values to increase track quality throughout the entire tracking pipeline. This innovation (along with some simple and well-studied heuristics) allows RCT to achieve robust performance with minimal identity switches, even when provided with completely unfiltered detections. To compare trackers in the presence of unreliable detections, we present a challenging real-world underwater fish tracking dataset, FISHTRAC. In an large-scale evaluation across FISHTRAC, UA-DETRAC, and MOTChallenge data, RCT outperforms a wide variety of trackers, including deep trackers and more classic approaches. We have publically released our FISHTRAC codebase and training dataset at https://github.com/tmandel/fish-detrac, which will facilitate comparing trackers on understudied problems.}
}
@article{LIU2023109071,
title = {Center and Scale Prediction: Anchor-free Approach for Pedestrian and Face Detection},
journal = {Pattern Recognition},
volume = {135},
pages = {109071},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109071},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005519},
author = {Wei Liu and Irtiza Hasan and Shengcai Liao},
keywords = {Object Detection, Convolutional Neural Networks, Feature Detection, anchor-free, Anchor-free},
abstract = {Object detection traditionally requires sliding-window classifier in modern deep learning based approaches. However, both of these approaches requires tedious configurations in bounding boxes. Generally speaking, single-class object detection is to tell where the object is, and how big it is. Traditional methods combine the ”where” and ”how” subproblems into a single one through the overall judgement of various scales of bounding boxes. In view of this, we are interesting in whether the ”where” and ”how” subproblems can be separated into two independent subtasks to ease the problem definition and the difficulty of training. Accordingly, we provide a new perspective where detecting objects is approached as a high-level semantic feature detection task. Like edges, corners, blobs and other feature detectors, the proposed detector scans for feature points all over the image, for which the convolution is naturally suited. However, unlike these traditional low-level features, the proposed detector goes for a higher-level abstraction, that is, we are looking for central points where there are objects, and modern deep models are already capable of such a high-level semantic abstraction. Like blob detection, we also predict the scales of the central points, which is also a straightforward convolution. Therefore, in this paper, pedestrian and face detection is simplified as a straightforward center and scale prediction task through convolutions. This way, the proposed method enjoys an anchor-free setting, considerably reducing the difficulty in training configuration and hyper-parameter optimization. Though structurally simple, it presents competitive accuracy on several challenging benchmarks, including pedestrian detection and face detection. Furthermore, a cross-dataset evaluation is performed, demonstrating a superior generalization ability of the proposed method.}
}
@article{PEI2023109148,
title = {Person-Specific Face Spoofing Detection Based on a Siamese Network},
journal = {Pattern Recognition},
volume = {135},
pages = {109148},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109148},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006276},
author = {Mingtao Pei and Bin Yan and Huiling Hao and Meng Zhao},
keywords = {Face spoofing detection, Identity information, Siamese network},
abstract = {Face spoofing detection is an essential prerequisite for face recognition applications. Previous face spoofing detection methods usually trained a binary classifier to classify the input face as a spoof face or a real face before face recognition, and client identity information was not utilized. In this paper, we propose a person-specific face spoofing detection method to employ client identity information for face spoofing detection. In our method, face spoofing is detected after face recognition rather than before face recognition; that is, the input face is recognized first, and the client identity is used to assist face spoofing detection. We train a deep Siamese network with image pairs. Each image pair consists of two real face images or one real and one spoof face image. The face images in each pair come from the same client. The deep Siamese network is trained by joint Bayesian loss together with contrastive loss and softmax loss. In testing, an input face image is recognized first, then the real face image of the identified client is retrieved, and an image pair is formed by the test face image and the retrieved real face image. The image pair is classified by the trained Siamese network to determine whether the input test image is a real face or not. The experimental results demonstrate the effectiveness of our method.}
}
@article{YU2023109078,
title = {A Lie algebra representation for efficient 2D shape classification},
journal = {Pattern Recognition},
volume = {134},
pages = {109078},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109078},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005581},
author = {Xiaohan Yu and Yongsheng Gao and Mohammed Bennamoun and Shengwu Xiong},
keywords = {Lie algebra, 2D Shape classification, Covariance matrix, Lie group of SPD matrix},
abstract = {Riemannian manifold plays a vital role as a powerful mathematical tool in computer vision, with important applications in curved shape analysis and classification. Significant progress has recently been made by Riemannian framework based methods that achieved state-of-the-art classification accuracy and robustness. However, these Riemannian manifold and Lie group methods require a very high computational complexity and do not include a description of the shape regions. This paper presents a novel mathematical tool, called Block Diagonal Symmetric Positive Definite Matrix Lie Algebra (BDSPDMLA) to represent curves, which extends the existing Lie group representations to a compact yet informative Lie algebra representation. The proposed Lie algebra based method addresses the computational bottleneck problem of the Riemannian framework based methods. In addition, it allows the natural fusion of various regions information with curved shape features for a more discriminative shape description. Here the region information is represented by values of distance maps, local binary patterns (LBP) and image intensity. Extensive experiments on five publicly available databases demonstrate that the proposed Lie algebra based method can achieve a speed of over ten thousand times faster than the Riemannian manifold and Lie group based baseline methods, while obtaining comparable accuracies for 2D shape classification.}
}
@article{XING2023109123,
title = {Binary feature learning with local spectral context-aware attention for classification of hyperspectral images},
journal = {Pattern Recognition},
volume = {134},
pages = {109123},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109123},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006033},
author = {Changda Xing and Chaowei Duan and Zhisheng Wang and Meiling Wang},
keywords = {Classification of hyperspectral images, Local spectrum modules, Spectral context-awareness, Binary feature learning},
abstract = {The classification of hyperspectral images (HSIs) has achieved success in applications. For many approaches, features are directly extracted from whole spectral pixels, which can not well describe local characteristics. These methods are also susceptible to noise since each feature code is learned individually. Accordingly, a binary feature learning method with local spectral context-aware attention (BFLSC) is proposed for the classification. Specifically, for training samples, we first build the local spectrum models (LSMs) to describe local spectral properties, where each training sample is segmented into some parts and the difference between the central value and its neighborhoods is calculated in each part. Then, we construct the BFLSC model to learn a projection and binary features of training samples. In such model, the spectral context-awareness attention is established to collaboratively learn binary feature codes by enforcing one shift between 0/1 of each LSM, which enhances the robustness and stability of binary leaning. We also introduce the loss constraint, even distribution constraint, and variance constraint to reduce information loss and improve the quality of learned feature distribution. Additionally, an optimization scheme is designed to obtain the solution of the BFLSC model. Further, the learned binary features are added to train the support vector machine (SVM). For each testing sample, the LSMs are first extracted, and then mapped into binary features by the learned projection. The trained SVM is finally used for the mapped binary features to predict the label of the testing sample. Experimental results validate that our BFLSC realizes the better performance compared with some advanced approaches.}
}
@article{LIN2023109042,
title = {Exploratory Adversarial Attacks on Graph Neural Networks for Semi-Supervised Node Classification},
journal = {Pattern Recognition},
volume = {133},
pages = {109042},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109042},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005222},
author = {Xixun Lin and Chuan Zhou and Jia Wu and Hong Yang and Haibo Wang and Yanan Cao and Bin Wang},
keywords = {Gradient-based attacks, Maximal gradient, Graph neural networks, Semi-supervised node classification},
abstract = {Graph neural networks (GNNs) have been successfully used to analyze non-Euclidean network data. Recently, there emerge a number of works to investigate the robustness of GNNs by adding adversarial noises into the graph topology, where the gradient-based attacks are widely studied due to their inherent efficiency and high effectiveness. However, the gradient-based attacks often lead to sub-optimal results due to the discrete structure of graph data. To address this issue, we propose a novel exploratory adversarial attack (termed as EpoAtk) to boost the gradient-based perturbations on graphs. The exploratory strategy in EpoAtk includes three phases, generation, evaluation and recombination, with the goal of sidestepping the possible misinformation that the maximal gradient provides. In particular, our evaluation phase introduces a self-training objective containing three effective evaluation functions to fully exploit the useful information of unlabeled nodes. EpoAtk is evaluated on multiple benchmark datasets for the task of semi-supervised node classification in different attack settings. Extensive experimental results demonstrate that the proposed method achieves consistent and significant improvements over the state-of-the-art adversarial attacks with the same attack budgets.}
}
@article{ZHAO2023109118,
title = {Clean affinity matrix learning with rank equality constraint for multi-view subspace clustering},
journal = {Pattern Recognition},
volume = {134},
pages = {109118},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109118},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005982},
author = {Jinbiao Zhao and Gui-Fu Lu},
keywords = {Low-rank representation, Robust principal component analysis, Outliers value, Affinity matrix, Low-rank matrix decomposition},
abstract = {The existing multi-view subspace clustering (MVSC) algorithm still has certain limitations. First, the affinity matrix obtained by them is not clean and robust enough since the original multi-view data usually contain noise. Second, they also have defects in exploring the consistency between views. To compensate for these two shortcomings, we propose a novel MVSC, i.e., clean affinity matrix learning with rank equality constraint (CAMR) for MVSC. By borrowing the idea from robust principal component analysis (RPCA), the representation matrix of each view obtained by low-rank representation (LRR) is first cleaned up to obtain a cleaner and more robust affinity matrix. In addition, the rank constraint is utilized to explore the same clustering properties between different views. An objective function solution method based on an augmented Lagrange multiplier (ALM) is designed and tested on four widely employed datasets to verify that CAMR has better clustering performance than certain state-of-the-art methods. We provide the code of CAMR at https://github.com/zhaojinbiao/CAMR.}
}
@article{KORBAN2023109066,
title = {TAA-GCN: A temporally aware Adaptive Graph Convolutional Network for age estimation},
journal = {Pattern Recognition},
volume = {134},
pages = {109066},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109066},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005465},
author = {Matthew Korban and Peter Youngs and Scott T. Acton},
keywords = {Age estimation, Graph convolutional network, Facial graphs, Skeletal graphs},
abstract = {This paper proposes a novel age estimation algorithm, the Temporally-Aware Adaptive Graph Convolutional Network (TAA-GCN). Using a new representation based on graphs, the TAA-GCN utilizes skeletal, posture, clothing, and facial information to enrich the feature set associated with various ages. Such a novel graph representation has several advantages: First, reduced sensitivity to facial expression and other appearance variances; Second, robustness to partial occlusion and non-frontal-planar viewpoint, which is commonplace in real-world applications such as video surveillance. The TAA-GCN employs two novel components, (1) the Temporal Memory Module (TMM) to compute temporal dependencies in age; (2) Adaptive Graph Convolutional Layer (AGCL) to refine the graphs and accommodate the variance in appearance. The TAA-GCN outperforms the state-of-the-art methods on four public benchmarks, UTKFace, MORPHII, CACD, and FG-NET. Moreover, the TAA-GCN showed reliability in different camera viewpoints and reduced quality images.}
}
@article{LEI2023109106,
title = {Multi-scale enhanced graph convolutional network for mild cognitive impairment detection},
journal = {Pattern Recognition},
volume = {134},
pages = {109106},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109106},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005866},
author = {Baiying Lei and Yun Zhu and Shuangzhi Yu and Huoyou Hu and Yanwu Xu and Guanghui Yue and Tianfu Wang and Cheng Zhao and Shaobin Chen and Peng Yang and Xuegang Song and Xiaohua Xiao and Shuqiang Wang},
keywords = {Mild cognitive impairment detection, Multimodal brain connectivity networks, Multi-scale enhanced graph convolutional network},
abstract = {As an early stage of Alzheimer's disease (AD), mild cognitive impairment (MCI) is able to be detected by analyzing the brain connectivity networks. For this reason, we devise a new framework via multi-scale enhanced graph convolutional network (MSE-GCN) for MCI detection, which integrates the structural and functional information from the diffusion tensor imaging (DTI) and resting-state functional magnetic resonance imaging (R-fMRI), respectively. Specifically, both information in the brain connective networks is first integrated based on the local weighted clustering coefficients (LWCC), which is concatenated as the feature vector for representing a population graph's vertice. Simultaneously, the gender and age information in each subject are integrated with the structural and functional features to construct a sparse graph. Then, various parallel graph convolutional network (GCN) layers with multiple inputs are designed from the embedding from random walk embeddings in the GCN to identify the essential MCI graph information. Finally, all GCN layers’ outputs are concatenated via the fully connection layer to perform disease detection. The experimental results on the public Alzheimer's Disease Neuroimaging Initiative (ADNI) database show that our method is promising to detect MCI and superior to other competing algorithms, with a mean classification accuracy of 90.39% in the detection tasks.}
}
@article{YU2023109054,
title = {Improving adversarial robustness by learning shared information},
journal = {Pattern Recognition},
volume = {134},
pages = {109054},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109054},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005349},
author = {Xi Yu and Niklas Smedemark-Margulies and Shuchin Aeron and Toshiaki Koike-Akino and Pierre Moulin and Matthew Brand and Kieran Parsons and Ye Wang},
keywords = {Adversarial robustness, Information bottleneck, Multi-view learning, Shared information,},
abstract = {We consider the problem of improving the adversarial robustness of neural networks while retaining natural accuracy. Motivated by the multi-view information bottleneck formalism, we seek to learn a representation that captures the shared information between clean samples and their corresponding adversarial samples while discarding these samples’ view-specific information. We show that this approach leads to a novel multi-objective loss function, and we provide mathematical motivation for its components towards improving the robust vs. natural accuracy tradeoff. We demonstrate enhanced tradeoff compared to current state-of-the-art methods with extensive evaluation on various benchmark image datasets and architectures. Ablation studies indicate that learning shared representations is key to improving performance.}
}
@article{LI2023109077,
title = {Deep graph clustering with multi-level subspace fusion},
journal = {Pattern Recognition},
volume = {134},
pages = {109077},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109077},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200557X},
author = {Wang Li and Siwei Wang and Xifeng Guo and En Zhu},
keywords = {Graph clustering, Subspace, Self-expressive learning, Fusion},
abstract = {Attributed graph clustering combines both node attributes and graph structure information of data samples and has demonstrated satisfactory performance in various applications. However, how to choose the proper neighborhood for attributed graph clustering remains to be a challenge. A larger neighborhood may cause over-smoothed representations with less discrimination for clustering while the short-range ignore distant nodes and fails to capture the global information. In this paper, we propose a novel deep attributed graph clustering network with a multi-level subspace fusion module to address this issue. The first contribution of our work is to insert multiple self-expressive modules between low-level and high-level layers to promote more favorable features for clustering. The constraint of shared self-expressive matrix facilitates to preserve intrinsic structure without pre-defined neighborhoods as the previous methods do. Moreover, we introduce a novel loss function that leverages traditional reconstruction and the proposed structure fusion loss to effectively preserve multi-level clustering structures with both global and local discriminative features. Extensive experiments on public benchmark datasets validate the effectiveness of our proposed model compared with the state-of-the-art attribute graph clustering competitors by considerable margins.}
}
@article{TROMBINI2023109082,
title = {A goal-driven unsupervised image segmentation method combining graph-based processing and Markov random fields},
journal = {Pattern Recognition},
volume = {134},
pages = {109082},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109082},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005623},
author = {Marco Trombini and David Solarna and Gabriele Moser and Silvana Dellepiane},
keywords = {Graph signal processing, Segmentation, Markovian modeling, Parametric model estimation, Pattern recognition, Synthetic aperture radar, Magnetic resonance imagery},
abstract = {Image segmentation is the process of partitioning a digital image into a set of homogeneous regions (according to some homogeneity criterion) to facilitate a subsequent higher-level analysis. In this context, the present paper proposes an unsupervised and graph-based method of image segmentation, which is driven by an application goal, namely, the generation of image segments associated with a user-defined and application-specific goal. A graph, together with a random grid of source elements, is defined on top of the input image. From each source satisfying a goal-driven predicate, called seed, a propagation algorithm assigns a cost to each pixel on the basis of similarity and topological connectivity, measuring the degree of association with the reference seed. Then, the set of most significant regions is automatically extracted and used to estimate a statistical model for each region. Finally, the segmentation problem is expressed in a Bayesian framework in terms of probabilistic Markov random field (MRF) graphical modeling. An ad hoc energy function is defined based on parametric models, a seed-specific spatial feature, a background-specific potential, and local-contextual information. This energy function is minimized through graph cuts and, more specifically, the alpha-beta swap algorithm, yielding the final goal-driven segmentation based on the maximum a posteriori (MAP) decision rule. The proposed method does not require deep a priori knowledge (e.g., labelled datasets), as it only requires the choice of a goal-driven predicate and a suited parametric model for the data. In the experimental validation with both magnetic resonance (MR) and synthetic aperture radar (SAR) images, the method demonstrates robustness, versatility, and applicability to different domains, thus allowing for further analyses guided by the generated products.}
}
@article{ZHANG2023109070,
title = {Specialized re-ranking: A novel retrieval-verification framework for cloth changing person re-identification},
journal = {Pattern Recognition},
volume = {134},
pages = {109070},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109070},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005507},
author = {Renjie Zhang and Yu Fang and Huaxin Song and Fangbin Wan and Yanwei Fu and Hirokazu Kato and Yang Wu},
keywords = {Cloth changing person re-identification, Verification network, Re-Rank, Specialized features, Part-based comparison},
abstract = {Cloth changing person re-identification(Re-ID) can work under more complicated scenarios with higher security than normal Re-ID and biometric techniques and is therefore extremely valuable in applications. Meanwhile, the wide range of appearance flexibility results in more similar-looking, confusing images, which is the weakness of the widely used retrieval methods. In this work, we shed light on how to handle these similar images. Specifically, we propose a novel retrieval-verification framework. Given an image, the retrieval module will search for a shot list of similar images quickly. Our proposed verification network will then compare the probe image with these candidate images by contrasting local details for their similarity scores. An innovative ranking strategy is also introduced to achieve a good balance between retrieval and verification results. Comprehensive experiments are conducted to show the effectiveness of our framework and its capability in improving the state-of-the-art methods remarkably on both synthetic and realistic datasets.}
}
@article{BAI2023109110,
title = {Automatically detecting human-object interaction by an instance part-level attention deep framework},
journal = {Pattern Recognition},
volume = {134},
pages = {109110},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109110},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005908},
author = {Lin Bai and Fenglian Chen and Yang Tian},
keywords = {Human-object interaction, Instance part-level correlations, Self-attention-based model, Image context},
abstract = {Automatically detecting human-object interactions (HOIs) from an image is a very important but challenging task in computer vision. One of the significant problems in HOI detection is that similar human-object interactions are difficult to distinguish. Recently, many instance-centric HOI detection schemes, based on appearance features and coarse spatial information, have been proposed. These methods, however, lack the capacity of capturing and analyzing the fine-grained context between human poses and object parts, which plays a crucial role in HOI detection. To address these problems, we propose a novel instance part-level attention deep framework for HOI detection. Specifically, our approach consists of a human/object-part detection phase and an HOI detection phase. In the former phase, a part-level visual pattern estimation model is designed for capturing the fine-grained human body parts and object parts. In the latter phase, a self-attention-based deep network is proposed to learn the visual composite around the human-object pair that implicitly expresses the consistent spatial, scale, co-occurrence, and viewpoint relationships among human body parts and object parts across images, which are effective for predicting HOI. To the best of our knowledge, we are the first to propose a framework where the fine-grained part-level mutual context of a human-object pair is extracted to improve HOI detection. By comparing our approach with state-of-the-art HOI detection methods on benchmark datasets, we demonstrated that our proposed framework outperformed the existing HOI detection methods, such as significantly improving the performance of part-level visual pattern estimation, HOI detection, and the quality of the self-attention-based deep network structure.}
}
@article{XUE2023109041,
title = {Investigating intrinsic degradation factors by multi-branch aggregation for real-world underwater image enhancement},
journal = {Pattern Recognition},
volume = {133},
pages = {109041},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109041},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005210},
author = {Xinwei Xue and Zexuan Li and Long Ma and Qi Jia and Risheng Liu and Xin Fan},
keywords = {Underwater image enhancement, Multi-branch learning, Real-world underwater images, Comprehensive evaluation},
abstract = {Recently, improving the visual quality of underwater images has received extensive attentions in both computer vision and ocean engineering fields. However, existing works mostly focus on directly learning clear images from degraded observations but without careful investigations on the intrinsic degradation factors, thus require mass training data and lack generalization ability. In this work, we propose a new method, named Multi-Branch Aggregation Network (termed as MBANet) to partially address the above issue. Specifically, by analyzing underwater degradation factors from the perspective of both color distortions and veil effects, MBANet first constructs a multi-branch multi-variable architecture to obtain one intermediate coarse result and two degraded factors. We then establish a physical model inspired process to fully utilize our estimated degraded factors and thus obtain the desired clear output images. A series of evaluations on multiple datasets show the superiority of our method against existing state-of-the-art approaches, both in execution speed and accuracy. Furthermore, we demonstrate that our MBANet can significantly improve the performance of salience object detection in the underwater environment.}
}
@article{LIU2023109122,
title = {A novel soft-coded error-correcting output codes algorithm},
journal = {Pattern Recognition},
volume = {134},
pages = {109122},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109122},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006021},
author = {Kun-Hong Liu and Jie Gao and Yong Xu and Kai-Jie Feng and Xiao-Na Ye and Sze-Teng Liong and Li-Yan Chen},
keywords = {Error-correcting output codes, Self-adaptive Strategy, Soft codes, Coverage measure, Subordination degree},
abstract = {Error-Correcting Output Codes (ECOC) algorithms enable multiclass classification by reassigning multiple classes to the positive/negative group with the class reassignment schemes being recorded as binary/ternary hard-coded (HC) codematrices. Different classes tend to get diverse subordination degrees to the positive/negative group, providing clues to correct potential errors. However, the HC codematrices are unable to provide the information in the subordination degrees. In this paper, a Soft-Coded ECOC (SC-ECOC) scheme, namely, the Sequential Forward Floating Selection algorithm, is proposed by filling codematrices with real values instead of hard codes to improve classification performance. This algorithm divides multiple classes into two groups by maximizing the ratio of inter-group distance to intra-group distance. Then a new measure coverage is designed to evaluate the subordination degrees of different classes to both groups, which are set as the elements to form a codematrix. Furthermore, a self-adaptive strategy adjusts the value of each element to fit learners better. Experiments are carried out to verify the performance of our algorithm on various data sets, and results confirm that our algorithm can achieve more balanced results compared with the traditional HC ECOC algorithms. Besides, the values of soft codes correlate with the difficulty level of various classes to improve the multiclass classification ability.}
}
@article{MUNJAL2023109049,
title = {Query-guided networks for few-shot fine-grained classification and person search},
journal = {Pattern Recognition},
volume = {133},
pages = {109049},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109049},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005295},
author = {Bharti Munjal and Alessandro Flaborea and Sikandar Amin and Federico Tombari and Fabio Galasso},
keywords = {Meta-learning, Few-shot learning, Fine-grained classification, Person search, Person re-identification},
abstract = {Few-shot fine-grained classification and person search appear as distinct tasks and literature has treated them separately. But a closer look unveils important similarities: both tasks target categories that can only be discriminated by specific object details; and the relevant models should generalize to new categories, not seen during training. We propose a novel unified Query-Guided Network (QGN) applicable to both tasks. QGN consists of a Query-guided Siamese-Squeeze-and-Excitation subnetwork which re-weights both the query and gallery features across all network layers, a Query-guided Region Proposal subnetwork for query-specific localisation, and a Query-guided Similarity subnetwork for metric learning. QGN improves on a few recent few-shot fine-grained datasets, outperforming other techniques on CUB by a large margin. QGN also performs competitively on the person search CUHK-SYSU and PRW datasets, where we perform in-depth analysis.}
}
@article{SATHYASEELAN2023109134,
title = {Sequence patterns and HMM profiles to predict proteome wide zinc finger motifs},
journal = {Pattern Recognition},
volume = {135},
pages = {109134},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109134},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006148},
author = {Chakkarai Sathyaseelan and L Ponoop Prasad Patro and Thenmalarchelvi Rathinavelan},
keywords = {Zinc finger classification, Zinc finger motif, Zinc finger proteome, Pfam HMM profile, Zinc finger prediction},
abstract = {Zinc finger (ZnF) is an important class of nucleic acid and protein recognition domain, wherein, zinc ion is the inorganic co-factor that forms a tetrahedral geometry with the cysteine and/or histidine residues. ZnF domains take up diverse architectures with different ZnF motifs and have a wide range of biological functions. Nonetheless, predicting the ZnF motif(s) from the sequence is quite challenging. To this end, 74 unique ZnF sequence patterns are collected from the literature and classified into 32 different classes. Since the shorter length of ZnF sequence patterns leads to inaccurate predictions, ZnF domain Pfam HMM profiles defined under 6 ZnF Pfam clans (215 HMM profiles) and a few undefined Pfam clans (74 HMM profiles) are used for the prediction. A web server, namely, ZnF-Prot (https://project.iith.ac.in/znprot/) is developed which can predict the presence of 31 ZnF domains in a protein/proteome sequence of any organism. The use of ZnF sequence patterns and Pfam HMM profiles resulted in an accurate prediction of 610 test cases (taken randomly from 249 organisms) considered here. Additionally, the application of ZnF-Prot is demonstrated by considering Arabidopsis thaliana, Homo sapiens, Saccharomyces cerevisiae, Caenorhabditis elegans and Ciona intestinalis proteomes as test cases, wherein, 87–96% of the predicted ZnF motifs are cross-validated.}
}
@article{ACENA2023109117,
title = {Support subsets estimation for support vector machines retraining},
journal = {Pattern Recognition},
volume = {134},
pages = {109117},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109117},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005970},
author = {Víctor Aceña and Isaac {Martín de Diego} and Rubén {R． Fernández} and Javier {M． Moguerza}},
keywords = {Support subset, , Incremental learning, Retraining, Alpha seeding},
abstract = {The availability of new data in previously trained Machine Learning (ML) models usually requires retraining and adjustment of the model. Support Vector Machines (SVMs) are widely used in ML because of their strong mathematical foundations and flexibility. However, SVM training is computationally expensive, both in time and memory. Hence, the training phase might be a limitation in problems where the model is updated regularly. As a solution, new methods for training and updating SVMs have been proposed in the past. In this paper, we introduce the concept of Support Subset and a new retraining methodology for SVMs. A Support Subset is a subset of the training set, such that retraining a ML model with this subset and the new data is equivalent to training with all the data. The performance of the proposal is evaluated in a variety of experiments on simulated and real datasets in terms of time, quality of the solution, resultant support vectors, and amount of employed data. The promising results provide a new research line for improving the effectiveness and adaptability of the proposed technique, including its generalization to other ML models.}
}
@article{YU2023109065,
title = {Meta-learning-based adversarial training for deep 3D face recognition on point clouds},
journal = {Pattern Recognition},
volume = {134},
pages = {109065},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109065},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005453},
author = {Cuican Yu and Zihui Zhang and Huibin Li and Jian Sun and Zongben Xu},
keywords = {Deep 3D face recognition, Point clouds, Adversarial samples, Meta-learning},
abstract = {Recently, deep face recognition using 2D face images has made great advances mainly due to the readily available large-scale face data. However, deep face recognition using 3D face scans, especially on point clouds, has been far from fully explored. In this paper, we propose a novel meta-learning-based adversarial training (MLAT) algorithm for deep 3D face recognition (3DFR) on point clouds. It consists of two alternate modules: adversarial sample generating for 3D face data augmentation and meta-learning-based deep network training. In the first module, adversarial samples of given 3D face scans are dynamically generated based on current deep 3DFR model. In the second module, a meta-learning framework is designed to avoid the performance decrease caused by the generated adversarial samples. Overall, MLAT algorithm combines the adversarial sample generating and meta-learning-based network training in a uniform framework, in which adversarial samples and network parameters are optimized alternately. Thus, it can continuously generate diverse and suitable adversarial samples, and then the meta-learning framework can further improve the accuracy of 3DFR model. Comprehensive experimental results show that the proposed approach consistently achieves competitive rank-one recognition accuracies on the BU-3DFE (100%), Bosphorus (99.78%), BU-4DFE (98.02%) and FRGC v2 (98.01%) database, and thereby substantiate its superiority.}
}
@article{STRAGAPEDE2023109089,
title = {BehavePassDB: Public Database for Mobile Behavioral Biometrics and Benchmark Evaluation},
journal = {Pattern Recognition},
volume = {134},
pages = {109089},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109089},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005696},
author = {Giuseppe Stragapede and Ruben Vera-Rodriguez and Ruben Tolosana and Aythami Morales},
keywords = {Mobile authentication, Continuous authentication, Behavioral biometrics, BehavePassDB, Device bias},
abstract = {Mobile behavioral biometrics have become a popular topic of research, reaching promising results in terms of authentication, exploiting a multimodal combination of touchscreen and background sensor data. However, there is no way of knowing whether state-of-the-art classifiers in the literature can distinguish between the notion of user and device. In this article, we present a new database, BehavePassDB, structured into separate acquisition sessions and tasks to mimic the most common aspects of mobile Human-Computer Interaction (HCI). BehavePassDB is acquired through a dedicated mobile app installed on the subjects devices, also including the case of different users on the same device for evaluation. We propose a standard experimental protocol and benchmark for the research community to perform a fair comparison of novel approaches with the state of the art11https://github.com/BiDAlab/MobileB2C_BehavePassDB/.. We propose and evaluate a system based on Long-Short Term Memory (LSTM) architecture with triplet loss and modality fusion at score level.}
}
@article{XIANG2023109151,
title = {Similarity learning with deep CRF for person re-identification},
journal = {Pattern Recognition},
volume = {135},
pages = {109151},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109151},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006306},
author = {Jun Xiang and Ziyuan Huang and Xiaoping Jiang and Jianhua Hou},
keywords = {Person re-identification, Deep learning, Conditional random field (CRF), Group-wise similarities},
abstract = {The core of person re-identification (Re-ID) lies in robustly estimating similarities for each probe-gallery image pair. A common practice in existing works is to calculate the similarity of each image pair independently, ignoring relations between different probe-gallery pairs. In this paper, we present a deep learning conditional random field (Deep-CRF) graph to model group-wise similarities within a batch of images, and regard the Re-ID task as a CRF node labeling problem. Unlike the existing deep CRF based approach where the CRF inference is only involved in the training stage, our method intends to fully exploit the potential of CRF model, exhibiting inference consistency in both training and testing. Specifically, we design unary potentials for computing each probe-gallery similarity separately. To efficiently encode relationships between different probe-gallery pairs, pairwise potentials are built on an arbitrary node pair whose learning is achieved by a joint matching strategy using bidirectional LSTM. We pose the CRF inference as a RNN learning process, where unary and pairwise potentials are jointly optimized in an end-to-end manner. Extensive experiments on three large-scale person Re-ID datasets demonstrate the effectiveness of the proposed method. Our Deep-CRF achieves the best results compared with the previous graph-based deep learning approaches and substantially exceeds the existing deep CRF framework by 8% in Rank1 accuracy on CUHK03 dataset. It also behaves competitive among the current state-of-the-art methods.}
}
@article{SACHDEVA2023109121,
title = {ScanMix: Learning from Severe Label Noise via Semantic Clustering and Semi-Supervised Learning},
journal = {Pattern Recognition},
volume = {134},
pages = {109121},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109121},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200601X},
author = {Ragav Sachdeva and Filipe Rolim Cordeiro and Vasileios Belagiannis and Ian Reid and Gustavo Carneiro},
keywords = {Noisy label learning, Semi-supervised learning, Semantic clustering, Self-supervised Learning, Expectation maximisation},
abstract = {We propose a new training algorithm, ScanMix, that explores semantic clustering and semi-supervised learning (SSL) to allow superior robustness to severe label noise and competitive robustness to non-severe label noise problems, in comparison to the state of the art (SOTA) methods. ScanMix is based on the expectation maximisation framework, where the E-step estimates the latent variable to cluster the training images based on their appearance and classification results, and the M-step optimises the SSL classification and learns effective feature representations via semantic clustering. We present a theoretical result that shows the correctness and convergence of ScanMix, and an empirical result that shows that ScanMix has SOTA results on CIFAR-10/-100 (with symmetric, asymmetric and semantic label noise), Red Mini-ImageNet (from the Controlled Noisy Web Labels), Clothing1M and WebVision. In all benchmarks with severe label noise, our results are competitive to the current SOTA.}
}
@article{ABHISHEK2023109081,
title = {Parzen Window Approximation on Riemannian Manifold},
journal = {Pattern Recognition},
volume = {134},
pages = {109081},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109081},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005611},
author = { Abhishek and Rakesh {Kumar Yadav} and Shekhar Verma},
keywords = {Parzen window, Data affinity, Graph Laplacian regularization, Manifold regularization},
abstract = {In graph motivated learning, label propagation largely depends on data affinity represented as edges between connected data points. The affinity assignment implicitly assumes even distribution of data on the manifold. This assumption may not hold and may lead to inaccurate metric assignment due to drift towards high-density regions. The drift affected heat kernel based affinity with a globally fixed Parzen window either discards genuine neighbors or forces distant data points to become a member of the neighborhood. This yields a biased affinity matrix. In this paper, the bias due to uneven data sampling on the Riemannian manifold is catered to by a variable Parzen window determined as a function of neighborhood size, ambient dimension, flatness range, etc. Additionally, affinity adjustment is used which offsets the effect of uneven sampling responsible for the bias. An affinity metric which takes into consideration the irregular sampling effect to yield accurate label propagation is proposed. Extensive experiments on synthetic and real-world data sets confirm that the proposed method increases the classification accuracy significantly and outperforms existing Parzen window estimators in graph Laplacian manifold regularization methods.}
}
@article{YANG2023109053,
title = {Lane Detection with Versatile AtrousFormer and Local Semantic Guidance},
journal = {Pattern Recognition},
volume = {133},
pages = {109053},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109053},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005337},
author = {Jiaxing Yang and Lihe Zhang and Huchuan Lu},
keywords = {Lane detection, Global AtrousFormer, Local AtrousFormer, Enhanced feature extractor, Local semantic guided decoder},
abstract = {Lane detection is one of the core functions in autonomous driving and has aroused widespread attention recently. The networks to segment lane instances, especially with bad appearance, must be able to explore lane distribution properties. Most existing methods tend to resort to CNN-based techniques. A few have a try on incorporating the recent adorable, the seq2seq Transformer [1]. However, their innate drawbacks of weak global information collection ability and exorbitant computation overhead prohibit a wide range of the further applications. In this work, we propose Global Atrous Transformer (AtrousFormer) to solve the problem. Its variant local AtrousFormer is interleaved into feature extractor to enhance extraction. Their collecting information first by rows and then by columns in a dedicated manner finally equips our network with stronger information gleaning ability and better computation efficiency. To further improve the performance, we also propose a local semantic guided decoder to delineate the identities and shapes of lanes more accurately. Extensive results on three challenging benchmarks (CULane, TuSimple, and BDD100K) show that our network performs favorably against the state of the arts.}
}
@article{SHI2023109080,
title = {Self-paced resistance learning against overfitting on noisy labels},
journal = {Pattern Recognition},
volume = {134},
pages = {109080},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109080},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200560X},
author = {Xiaoshuang Shi and Zhenhua Guo and Kang Li and Yun Liang and Xiaofeng Zhu},
keywords = {Convolutional neural networks, Self-paced resistance, Model overfitting, Noisy labels},
abstract = {Noisy labels composed of correct and corrupted ones are pervasive in practice. They might significantly deteriorate the performance of convolutional neural networks (CNNs), because CNNs are easily overfitted on corrupted labels. To address this issue, inspired by an observation, deep neural networks might first memorize the probably correct-label data and then corrupt-label samples, we propose a novel yet simple self-paced resistance framework to resist corrupted labels, without using any clean validation data. The proposed framework first utilizes the memorization effect of CNNs to learn a curriculum, which contains confident samples and provides meaningful supervision for other training samples. Then it adopts selected confident samples and a proposed resistance loss to update model parameters; the resistance loss tends to smooth model parameters’ update or attain equivalent prediction over each class, thereby resisting model overfitting on corrupted labels. Finally, we unify these two modules into a single loss function and optimize it in an alternative learning. Extensive experiments demonstrate the significantly superior performance of the proposed framework over recent state-of-the-art methods on noisy-label data. Source codes of the proposed method are available on https://github.com/xsshi2015/Self-paced-Resistance-Learning.}
}