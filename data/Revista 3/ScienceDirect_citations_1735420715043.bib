@article{ZHENGGUANG2025110957,
title = {GC3: Grouped convolutional color constancy},
journal = {Pattern Recognition},
volume = {158},
pages = {110957},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110957},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007088},
author = {Song Zhengguang and Li Zhijiang and Cao Liqin and Jiao Lei and Zhang Xuan},
keywords = {Color constancy, White balancing, Illuminant estimation, Deep learning},
abstract = {Computational color constancy is an ill-posed problem since different combinations of objects and illumination may appear the same color in an image. Most methods produce a single illuminant estimate and cannot explain the potential ambiguity between possible illuminant solutions. In this work, a grouped convolutional color constancy (GC3) method is proposed. It approaches the computational color constancy as a grouped regression problem, dealing with ambiguity by generating multiple possible illuminant solutions. The GC3 consists of two components: fast feature extraction (FFE) and grouped illuminant regression (GIR). FFE uses small convolution kernels and rapid down-sampling scheme for quick feature extraction, while GIR employs grouped convolution layers for feature grouping regression, which estimates the illuminant solution and its probability for each group with two separate branches. The final illumination estimation is obtained by averaging all illuminant solutions, weighted by the normalized probability for each group. The proposed method yields competitive results on the ColorChecker dataset and achieves the state-of-the-art accuracy on the NUS and Cube+ datasets. Moreover, when compared with the state-of-the-art methods, our proposed approach exhibits superior time efficiency, making it the most efficient method available.}
}
@article{HAO2025110888,
title = {Embedded feature fusion for multi-view multi-label feature selection},
journal = {Pattern Recognition},
volume = {157},
pages = {110888},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110888},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006393},
author = {Pingting Hao and Wanfu Gao and Liang Hu},
keywords = {Multi-view learning, Multi-label learning, Feature selection, Feature fusion},
abstract = {With the explosive growth of data sources, multi-view multi-label learning (MVML) has garnered significant attention. However, the task of selecting informative features in MVML becomes more challenging as the dimensionality increase. Existing methods often extract information separately from the consensus part and the complementary part, potentially leading to noise attributed to ambiguous segmentation. In this paper, we propose an embedded feature selection model that combines with two aspects, which are the feature fusion between views and feature enhancement. Firstly, we calculate the adaptive weight of each view based on the local structure relations, and integrate it into one unified feature matrix. Subsequently, the mapping between unified feature matrix and ground-truth label matrix is established. Furthermore, a regularizer for the feature weight of each view is constructed to emphasize its characteristic, respectively. As a result, the relationship for inter-view and intra-view has been simultaneously considered, preserving comprehensive information of features by minimizing the difference between two types of feature weight. Experimental results demonstrate the superior performance of our method in coping with feature selection.}
}
@article{CHEN2025110932,
title = {Density change consistency clustering from density extreme},
journal = {Pattern Recognition},
volume = {158},
pages = {110932},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110932},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006836},
author = {Mei Chen and Jinhong Zhang and Chi Zhang and Xueyan Ma and Luoxiong Qian},
keywords = {Density-based clustering, Density extreme, Heterogeneous density, Density change consistency, Cluster merging},
abstract = {Density-based clustering algorithms can identify clusters with arbitrary shapes and sizes. However, the algorithms still have difficulty detecting clusters with heterogeneous density within or between clusters. To overcome the weakness, we propose an effective clustering algorithm, called ETCD. First, starting from the density extremes, ETCD identifies the points satisfying density change consistency to generate the initial clusters, in which the density differences between neighboring points are slight and the points density decrease from center to boundary. Next, some initial clusters are merged based on density difference between clusters and attachness of clusters borders. To comprehensively demonstrate the performance of ETCD, we benchmarked ETCD on over 40 datasets with 4 classical baselines and 7 State-Of-The-Arts. Experimental results indicate ETCD outperforms the eleven baselines in most cases. ETCD is robust to density change within and between clusters, as well as clusters with different sizes, arbitrary shapes and various compactness.}
}
@article{ZHAO2025110880,
title = {CompNET: Boosting image recognition and writer identification via complementary neural network post-processing},
journal = {Pattern Recognition},
volume = {157},
pages = {110880},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110880},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006319},
author = {Bocheng Zhao and Xuan Cao and Wenxing Zhang and Xujie Liu and Qiguang Miao and Yunan Li},
keywords = {Classification task, Probability mass function, Model integration, Post-processing method},
abstract = {In current classification tasks, an important method to improve accuracy is to pre-train the model using a large-scale domain-specific dataset. However, many tasks such as writer identification (writerID) lack suitable large-scale datasets in practical scenarios. To address this issue, this paper proposes a method that can improve prediction accuracy without relying on significant pre-training but leveraging the diversity of probability distributions predicted by multiple networks, and enhancing the top-1 accuracy through complementary post-processing. Specifically, top-k distributions are sampled from the multiple probability mass functions separately. When the distribution differences of top-k are maximized, the intersection other than the correct category can be narrowed down. Finally, the correct target with suboptimal probability can be rectified by the only intersection. Furthermore, our method has exhibited an intriguing trait during experimentation. Its prediction accuracy enhances concurrently with the incorporation of novel SOTA methods, ultimately surpassing the performance of these new methods.}
}
@article{ZHU2025110903,
title = {DC-Net: Divide-and-conquer for salient object detection},
journal = {Pattern Recognition},
volume = {157},
pages = {110903},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110903},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400654X},
author = {Jiayi Zhu and Xuebin Qin and Abdulmotaleb Elsaddik},
keywords = {Saliency detection, Salient object detection},
abstract = {In this paper, to guide the model’s training process to explicitly present a progressive trend, we first introduce the concept of Divide-and-Conquer into Salient Object Detection (SOD) tasks, called DC-Net. Our DC-Net guides multiple encoders to solve different subtasks and then aggregates the feature maps with different semantic information obtained by multiple encoders into the decoder to predict the final saliency map. The decoder of DC-Net consists of newly designed two-level Residual nested-ASPP (ResASPP2) modules, which improve the sparse receptive field existing in ASPP and the disadvantage that the U-shape structure needs downsampling to obtain a large receptive field. Based on the advantage of Divide-and-Conquer’s parallel computing, we parallelize DC-Net through reparameterization, achieving competitive performance on five LR-SOD and five HR-SOD datasets under high efficiency (60 FPS and 55 FPS). Codes and results are available: https://github.com/PiggyJerry/DC-Net.}
}
@article{YANG2025110916,
title = {Deep blind super-resolution for hyperspectral images},
journal = {Pattern Recognition},
volume = {157},
pages = {110916},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110916},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006678},
author = {Pei Yang and Yong Ma and Xiaoguang Mei and Qihai Chen and Minghui Wu and Jiayi Ma},
keywords = {Hyperspectral image, Blind super-resolution, Degenerate model, Deep learning},
abstract = {Current deep learning methods for single hyperspectral image super-resolution are non-blind ones, which adopt the simplistic bicubic degradation model. These models have poor generalization performance and cannot handle unknown degradation. Additionally, blind super-resolution methods for RGB images neglect rich spectral information in hyperspectral images, which causes spectral distortion in results. To address these issues, we consider the degeneration estimation and propose a single hyperspectral image blind super-resolution algorithm. Specifically, we first use a blur kernel estimation network and a deblurring network to obtain the image without blur. We change the receptive field by exchanging spatial channel information when estimating the kernel, thereby obtaining blurry information at different scales. In addition, to reduce the errors introduced by kernel mismatch and the relevant information among bands ignored by the group convolution, we fuse the spatial information of the blurred image to compensate for the errors and then utilize the spatial and spectral information of other bands to guide the feature extraction of the current band. Finally, we integrate the feature information from different branches in a global fusion network to improve spatial and spectral information fidelity further. Extensive experiments on synthetic and real-world hyperspectral datasets unequivocally demonstrate that our approach outperforms other state-of-the-art approaches in terms of both quantitative and qualitative evaluations. Our code is publicly available at https://github.com/YoungP2001/DBSR.}
}
@article{DU2025110944,
title = {Joint local smoothness and low-rank tensor representation for robust multi-view clustering},
journal = {Pattern Recognition},
volume = {157},
pages = {110944},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110944},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006952},
author = {Yangfan Du and Gui-Fu Lu},
keywords = {Subspace clustering, Tensor, Tensor nuclear norm, Total variation},
abstract = {Low-rank tensor representation (LRTR) has become a significant method for achieving improved multi-view clustering (MVC) performance. Generally, most LRTR methods impose a tensor low-rank constraint (TLRC) on a tensor, which is spliced by the representation matrix of each view, to explore the low-rank prior hidden in these representation matrices. However, two problems remain unsolved. On the one hand, these representation matrices still contain noise since the raw data usually possess noise; on the other hand, and more importantly, is there any other prior information that can be explored among these representation matrices? Since samples located in the same cluster are similar, the coefficients of their representation matrices should exhibit similar. That is, these representation matrices should be local smoothness (LS), which is also verified by our numerical tests. In addition, the use of the LS prior helps to denoise the noise contain in the data. Then, in this paper, we propose a joint LS and LRTR for robust MVC method (LS-LRTR), which can simultaneously exploit the low-rank and LS priors. Specifically, we utilize a TLRC to explore the low-rank prior in the representation matrices. Subsequently, to mine the LS prior and further reduce the influence of noise, we introduce the Total Variation (TV) norm to the constraint representation matrices. Then, we fuse the TLRC and TV norm into a unified framework. Additionally, we apply an Augmented Lagrange Multiplier to solve the optimization problem of LS-LRTR. Experiments conducted on several datasets indicate that LS-LRTR outperforms the state-of-the-art clustering methods.}
}
@article{ZHAN2025110892,
title = {Co-regularized optimal high-order graph embedding for multi-view clustering},
journal = {Pattern Recognition},
volume = {157},
pages = {110892},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110892},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006435},
author = {Senwen Zhan and Hao Jiang and Dong Shen},
keywords = {Multi-view, Graph embedding, Second-order Laplacian matrix, Co-regularization},
abstract = {Real-world applications frequently involve multiple data modalities in the same samples, which are regarded as multi-view data. Multi-view clustering has been studied extensively in recent years to demonstrate embedded heterogeneity. However, most existing methods emphasize low-order correlation in multiple views, whereas approaches that incorporate high-order correlation are limited by the equal view-specific significance problem or a trade-off between global and local consistency. In this paper, we propose a co-regularized optimal graph-based clustering method known as Co-MSE, which integrates the correlation of different orders. By integrating the first-order and second-order similarities, the local structure is preserved, while an optimized embedding representation for multi-view data is obtained simultaneously through co-regularization. We demonstrate that Co-MSE can aid in providing a more suitable embedding representation and further enable satisfactory clustering performance. Extensive experiments on real-world datasets confirm the effectiveness and advantages of the proposed method.}
}
@article{XUE2025110973,
title = {ASF-Net: Robust video deraining via temporal alignment and online adaptive learning},
journal = {Pattern Recognition},
volume = {158},
pages = {110973},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110973},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007246},
author = {Xinwei Xue and Jia He and Long Ma and Xiangyu Meng and Wenlin Li and Risheng Liu},
keywords = {Video deraining, Temporal shift, Learning strategy},
abstract = {In recent times, learning-based methods for video deraining have demonstrated commendable results. However, there are two critical challenges that these methods are yet to address: exploiting temporal correlations among adjacent frames and ensuring adaptability to unknown real-world scenarios. To overcome these challenges, we explore video deraining from a paradigm design perspective to learning strategy construction. Specifically, we propose a new computational paradigm, Alignment-Shift-Fusion Network (ASF-Net), which incorporates a temporal shift module. This module is novel to this field and provides deeper exploration of temporal information by facilitating the exchange of channel-level information within the feature space. To fully discharge the model’s characterization capability, we further construct a LArge-scale RAiny video dataset (LARA) which also supports the development of this community. On the basis of the newly-constructed dataset, we explore the parameters learning process by developing an innovative re-degraded learning strategy. This strategy bridges the gap between synthetic and real-world scenes, resulting in stronger scene adaptability. Our approach exhibits superior performance in three benchmarks and compelling visual quality in real-world scenarios, underscoring its efficacy. The code is available at https://github.com/vis-opt-group/ASF-Net.}
}
@article{ZHU2025110949,
title = {FINet: Handwriting trajectory reconstruction of Chinese characters based on the font imitate network},
journal = {Pattern Recognition},
volume = {157},
pages = {110949},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110949},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007003},
author = {Yuanping Zhu and Shengnan Li and Hui Wang and Feilong Wei},
keywords = {Handwriting trajectory reconstruction, Spatial encoder, Temporal decoder, Stroke feature network, Handwritten Chinese character recognition},
abstract = {Recovering online handwriting trajectory from Chinese handwritten images is still a difficult problem because of abundant strokes, complex structures and different writing styles of Chinese characters. To address this problem, this paper proposes a handwriting trajectory reconstruction method for Chinese characters called the Font Imitate Network. The Font Imitate Network integrates two networks, the spatial encoder uses HRNet to encode the spatial association information of each position on the offline image; the temporal decoder consists of a multilayer perceptron and a recurrent neural network. The stroke feature network is designed to help predict the next handwriting point by focusing on the stroke path in the form of heatmap. Experiments are conducted on the CASIA-OLHWDB1.1 with DTW, LDTW, RMSE and character recognition accuracy as the evaluation metrics. The experimental results show our FINet outperforms the state-of-the-art methods and the recognition accuracy of offline characters is improved by the handwriting reconstruction.}
}
@article{AN2025110928,
title = {FastUNet: Fast hierarchical multi-patch underwater enhancement network for industrial recirculating aquaculture},
journal = {Pattern Recognition},
volume = {157},
pages = {110928},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110928},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006794},
author = {Shunmin An and Lihong Xu and Zhichao Deng},
keywords = {Recirculating aquaculture, Hierarchical multi-patch, Prior constraint, Underwater enhancement},
abstract = {The industrialized aquaculture with recirculating water systems has gained increasing attention and rapid development in recent years. However, the existing underwater enhancement methods are time-consuming and severely hinder their online application in industrialized aquaculture due to the requirement for real-time processing. We found that their main limitations are the insufficient utilization of the hierarchical network approach and valuable prior information in underwater scenarios. To address this problem, we propose a novel architecture called FastUNet and introduce a visual enhancement benchmark dataset specifically designed for industrialized aquaculture with recirculating water systems. Surprisingly, by learning the fast multi-patch hierarchical module and employing prior constraints including color loss and homology loss, FastUNet achieves comparable or even better results than many deep learning methods. In terms of runtime, it is up to 30 times faster than current underwater enhancement methods. The processing time for images with a resolution of 5474 × 3653 is only 0.137s. The real application scenario of factory recirculating aquaculture is considered by FastUNet, and the embedding of prior information under the fast network framework is explored. In this paper, 14 state-of-the-art underwater enhancement methods are compared, and excellent results are obtained on 2 Full-reference datasets and 2 No-referenced datasets. The application test results of SIFT significant point detection, geometric rotation, edge detection, target segmentation and target detection were also presented, which strongly verified the feasibility of FastUNet in the field of factory recirculating aquaculture.}
}
@article{YANG2025110876,
title = {Learning confidence measure with transformer in stereo matching},
journal = {Pattern Recognition},
volume = {157},
pages = {110876},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110876},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006277},
author = {Jini Yang and Minjung Yoo and Jaehoon Cho and Sunok Kim},
keywords = {Stereo confidence estimation, Stereo matching, Deep learning, Transformer},
abstract = {We introduce a novel approach for stereo confidence estimation, called ConFormer, leveraging the Transformer architecture. Recent confidence estimation methods commonly adopt convolutional neural networks (CNNs) and learned confidence features with limited receptive fields, thereby having limited capability to model global contexts. Benefiting from global understanding and the long-range dependencies of the attention mechanism, we effectively learn confidence features that take into account global relationships through the Transformer networks. Specifically, in the disparity feature extraction module, we extract global confidence features that encode global interactions with self-attention using a global pooling transformer. To complement local information and capture fine details, we additionally incorporate local prior features into the pooling transformer with an injection scheme. We further extract color confidence features using Transformer blocks to model the global interaction of the color image. The output confidence features from disparity and color image are effectively fused in a weighted attention manner in fusion networks. Experimental results demonstrate that this model outperforms the state-of-the-art CNN-based methods on various benchmarks.}
}
@article{DAGES2025110904,
title = {A model is worth tens of thousands of examples for estimation and thousands for classification},
journal = {Pattern Recognition},
volume = {157},
pages = {110904},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110904},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006551},
author = {Thomas Dagès and Laurent D. Cohen and Alfred M. Bruckstein},
keywords = {Deep learning, Model-based methods, Sample complexity},
abstract = {Traditional signal processing methods relying on mathematical data generation models have been cast aside in favour of deep neural networks, which require vast amounts of data. Since the theoretical sample complexity is nearly impossible to evaluate, these amounts of examples are usually estimated with crude rules of thumb. However, these rules only suggest when the networks should work, but do not relate to the traditional methods. In particular, an interesting question is: how much data is required for neural networks to be on par or outperform, if possible, the traditional model-based methods? In this work, we empirically investigate this question in three simple examples covering estimation and classification, where the data is generated according to precisely defined mathematical models, and where well-understood optimal or state-of-the-art mathematical data-agnostic solutions are known. A first problem is deconvolving one-dimensional Gaussian signals, a second one is estimating a circle’s radius and location in random grayscale images of disks, and a third one both classifies the presence of a line and locates it when present in a binary random dot image. By training various networks, either naive custom designed or well-established ones, with various amounts of training data, we find that networks require tens of thousands of examples for estimation in comparison to the traditional methods and thousands for classification, whether the networks are trained from scratch or even with transfer-learning or finetuning.}
}
@article{LI2025110956,
title = {WB-LRP: Layer-wise relevance propagation with weight-dependent baseline},
journal = {Pattern Recognition},
volume = {158},
pages = {110956},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110956},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007076},
author = {Yanshan Li and Huajie Liang and Lirong Zheng},
keywords = {Layer-Wise Relevance Propagation (LRP), Interpretation},
abstract = {DeepLift is a special Layer-wise Relevance Propagation (LRP) algorithm that assigns importance to features by evaluating the impact of small perturbations in input features on the output. However, we discover that only perturbations parallel to the plane defined by weights and input features affect the output. To address this issue, we propose a new LRP algorithm that offers more accurate instance-level explanation for neural networks. Firstly, we derive the baselines for various existing LRP algorithms using DeepLift theory and identify that these baselines often disregard model weights, sample features, or both. Secondly, the Weight-dependent Baseline Layer-wise Relevance Propagation (WB-LRP) algorithm is proposed to address this problem, by deriving the norm-invariant rule (m-rule) according to our discovery. Then, the m-rule is extended to the positive norm-invariant rule (m+-rule), focusing specifically on the positive parts of the weights. Finally, we extend the WB-LRP theory to accommodate various baseline settings, creating a framework that integrates nearly all existing LRP algorithms. Experimental results show that the relevance interpretation depends on both model weights and sample features. The proposed WB-LRP algorithm can be designed with different baselines to adjust the proportions of model weights and sample features in the visualization results, thus enabling separate visualization of the relevance of sample features and model weights to the target category.}
}
@article{HUA2025110887,
title = {MMDG-DTI: Drug–target interaction prediction via multimodal feature fusion and domain generalization},
journal = {Pattern Recognition},
volume = {157},
pages = {110887},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110887},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006381},
author = {Yang Hua and Zhenhua Feng and Xiaoning Song and Xiao-Jun Wu and Josef Kittler},
keywords = {Drug–target interaction, Multimodal feature fusion, Domain Generalization},
abstract = {Recently, deep learning has become the essential methodology for Drug–Target Interaction (DTI) prediction. However, the existing learning-based representation methods embed the prior knowledge encapsulated by the training data and, as such, acquire redundant domain information irrelevant to the DTI prediction, compromising the generalization capability of a deep network to unfamiliar instances. To tackle this limitation, we propose a novel DTI prediction method using Multi-Modal feature fusion and Domain Generalization (MMDG-DTI). Specifically, we first use pre-trained Large Language Models (LLMs) to obtain generalized textual features covering nearly the whole biological text vocabulary, with the capacity to handle unseen samples and extract robust and discriminative features. In parallel, we propose a unified structural feature extractor based on a hybrid Graph Neural Network (GNN). The extractor improves the performance of MMDG-DTI by extracting features from another modality and discarding the representation of domain-specific prior substructures. Then, we integrate the complementary information of LLM and GNN features using an innovative classifier, while enhancing the generalization ability with the help of Domain Adversarial Training (DAT) and contrastive learning. Last, we propose a novel cross-domain evaluation protocol to enhance the predication fidelity in practical applications. The quantitative and visualization results obtained on several benchmarking datasets demonstrate that MMDG-DTI achieves state-of-the-art performance under both single-domain and cross-domain settings, confirming the superiority of the proposed method. The datasets and codes will be available on the project homepage: https://github.com/JU-HuaY/MMDG-DTI.}
}
@article{JANG2025110879,
title = {FCGNet: Foreground and Class Guided Network for human parsing},
journal = {Pattern Recognition},
volume = {157},
pages = {110879},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110879},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006307},
author = {Jaehyuk Jang and Yooseung Wang and Changick Kim},
keywords = {Human parsing, Semantic segmentation, Graph convolutional network},
abstract = {Understanding the inherent hierarchical human structure is key to human parsing. To capture the human-specific characteristic, it is necessary to focus on the spatial and class information corresponding to the foreground (i.e., human) in an image. Inspired by these insights, we introduce two supervision signals, spatial foreground information and existent class information in the image. By utilizing foreground information as guidance, the network is guided to generate a human-focused feature map and capture the pixel-wise hierarchical characteristics by computing correlations between pixels. Furthermore, we guide the network to consider class information in the image at the feature level and capture the class-wise relationship by calculating correlations between channels. Moreover, during the training phase, we prevent the network from misclassifying pixels into confusing classes by providing the existent class information in the image to the network at the prediction level. Our model achieves state-of-the-art performance with significantly reduced parameters and Multiply-Accumulate Operations (MACs) in three public benchmarks.}
}
@article{LIU2025110915,
title = {RADAP: A Robust and Adaptive Defense Against Diverse Adversarial Patches on face recognition},
journal = {Pattern Recognition},
volume = {157},
pages = {110915},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110915},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006666},
author = {Xiaoliang Liu and Furao Shen and Jian Zhao and Changhai Nie},
keywords = {Face recognition, Adversarial patches, Defense mechanism, Deep learning, Robustness},
abstract = {Face recognition (FR) systems powered by deep learning have become widely used in various applications. However, they are vulnerable to adversarial attacks, especially those based on local adversarial patches that can be physically applied to real-world objects. In this paper, we propose RADAP, a robust and adaptive defense mechanism against diverse adversarial patches in both closed-set and open-set FR systems. RADAP employs innovative techniques, such as FCutout and F-patch, which use Fourier space sampling masks to improve the occlusion robustness of the FR model and the performance of the patch segmenter. Moreover, we introduce an edge-aware binary cross-entropy (EBCE) loss function to enhance the accuracy of patch detection. We also present the split and fill (SAF) strategy, which is designed to counter the vulnerability of the patch segmenter to complete white-box adaptive attacks. We conduct comprehensive experiments to validate the effectiveness of RADAP, which shows significant improvements in defense performance against various adversarial patches, while maintaining clean accuracy higher than that of the undefended Vanilla model.}
}
@article{LIU2025110943,
title = {Class incremental learning with self-supervised pre-training and prototype learning},
journal = {Pattern Recognition},
volume = {157},
pages = {110943},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110943},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006940},
author = {Wenzhuo Liu and Xin-Jian Wu and Fei Zhu and Ming-Ming Yu and Chuang Wang and Cheng-Lin Liu},
keywords = {Class incremental learning, Catastrophic forgetting, Prototype learning, Self-supervised learning},
abstract = {Deep Neural Networks (DNNs) have achieved great success on classification tasks of closed class sets. However, new classes, like new categories of social media topics, are continual added to the real world, making it necessary to learn incrementally. This is hard for DNNs because they tend to overfit new classes while ignoring old classes, a phenomenon known as catastrophic forgetting. State-of-the-art (SOTA) methods rely on knowledge distillation and data replay techniques but still have limitations. In this work, we analyze the causes of catastrophic forgetting in class incremental learning, which refers to representation drift, representation confusion, and classifier distortion. Based on this view, we propose a two-stage learning framework with a fixed encoder and an incrementally updated prototype classifier. The encoder generates a feature space with high intrinsic dimensionality while the prototype classifier preserves the decision boundary. Experimental results on public image datasets show that our non-exemplar-based method significantly outperforms SOTA exemplar-based methods.}
}
@article{BEHROUZI2025110891,
title = {Maskrenderer: 3D-infused multi-mask realistic face reenactment},
journal = {Pattern Recognition},
volume = {157},
pages = {110891},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110891},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006423},
author = {Tina Behrouzi and Atefeh Shahroudnejad and Payam Mousavi},
keywords = {Generative models, Deepfake, Face reenactment, Multi-scale occlusion masks, Triplet loss, 3D morphable model},
abstract = {We present a novel end-to-end identity-agnostic face reenactment system, MaskRenderer, that can generate realistic, high fidelity frames in real-time. Although recent face reenactment works have shown promising results, there are still significant challenges such as identity leakage and imitating mouth movements, especially for large pose changes and occluded faces. MaskRenderer tackles these problems by using (i) a 3DMM to model 3D face structure to better handle pose changes, occlusion, and mouth movements compared to 2D representations; (ii) a triplet loss function to embed the cross-reenactment during training for better identity preservation; and (iii) multi-scale occlusion, improving inpainting and restoring missing areas. Comprehensive quantitative and qualitative experiments conducted on the VoxCeleb1 test set, demonstrate that MaskRenderer outperforms state-of-the-art models on unseen faces, especially when the Source and Driving identities are very different.}
}
@article{PANG2025110886,
title = {Imbalanced ensemble learning leveraging a novel data-level diversity metric},
journal = {Pattern Recognition},
volume = {157},
pages = {110886},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110886},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400637X},
author = {Ying Pang and Lizhi Peng and Haibo Zhang and Zhenxiang Chen and Bo Yang},
keywords = {Diversity measurement, Imbalanced learning, Classification},
abstract = {Ensemble learning is one of the best solutions for imbalanced classification problems. Diversity is a key factor that affects the performance of ensemble learning. Most existing diversity metrics such as Q-statistics measure diversity based on the outputs of the base classifiers, incurring high complexity on model training due to the need to re-train base classifiers to achieve satisfactory diversity. We propose a new diversity measure, named Instance Euclidean Distance metric (IED), to evaluate diversity directly based on the training data without training base classifiers, which can significantly cut down the time costs of diversity measuring. A new imbalanced ensemble learning algorithm named P-EUSBagging is proposed to reduce training complexity and improve learning performance by combining IED with population-based incremental learning to generate training datasets with the maximal data-level diversity. Experimental results demonstrate that the diversities measured by IED and three classifier-based diversity measures exhibit a mean absolute correlation coefficient of 0.94, and P-EUSBagging significantly reduces training time and improves learning performance on both Geometric Mean (G-Mean) and Area Under the Curve (AUC).}
}
@article{LI2025110972,
title = {Object matching of visible–infrared image based on attention mechanism and feature fusion},
journal = {Pattern Recognition},
volume = {158},
pages = {110972},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110972},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007234},
author = {Wuxin Li and Qian Chen and Guohua Gu and Xiubao Sui},
keywords = {Siamese network, Infrared image, Visible image, Image match, Feature fusion},
abstract = {Object matching can be viewed as an image patch matching problem, which is widely employed in image fusion, image retrieval and other computer vision fields. In this paper, we regard image matching as a regression and classification task and propose a visible and infrared image matching network named AMFFNet. AMFFNet adopts a Siamese network structure and utilizes a residual network with an attention mechanism to extract features from the input images to obtain feature maps, and fuses these feature maps. AMFFNet then performs classification and regression on the fused feature maps to achieve matching. The classification operation identifies whether the predicted area is the object, while the regression operation determines the coordinates and size of the predicted box. To improve the matching performance of the network, we utilize the center-ness branch in the network and use Generalized Intersection over Union (GIoU) loss during training. We rearrange the existing dataset to provide a sufficient visible–infrared image matching dataset. Experimental results demonstrate that the proposed method achieves superior matching mean Average Precision (mAP) compared to other methods.}
}
@article{WANG2025110927,
title = {Distilling interaction knowledge for semi-supervised egocentric action recognition},
journal = {Pattern Recognition},
volume = {157},
pages = {110927},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110927},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006782},
author = {Haoran Wang and Jiahao Yang and Baosheng Yu and Yibing Zhan and Dapeng Tao and Haibin Ling},
keywords = {Knowledge distillation, Semi-supervised learning, Egocentric action recognition},
abstract = {Egocentric action recognition, the identification of actions within video content obtained from a first-person perspective, is receiving increasing attention due to the widespread adoption of wearable camera technology. Nonetheless, the task of annotating actions within a video characterized by a cluttered background and the presence of various objects is labor-intensive. In this paper, we consider learning for egocentric action recognition in a semi-supervised manner. Inspired by the fact that videos captured from first-person viewpoint usually contain rich contents about how human hands interact with objects, we thus propose to employ a popular teacher–student framework and distill the interaction knowledge between hand and objects for semi-supervised egocentric action recognition. We refer to the proposed method as Interaction Knowledge Distillation or IKD. Specifically, the teacher network takes hands and action-related objects in the labeled videos as input, and uses graph neural networks to capture their spatial–temporal relations as graph edge features. The student network then takes the detected hands/objects from both labeled and unlabeled videos as input and mimics the teacher network to learn from the interactions to improve model performance. Experiments are performed on two popular egocentric action recognition datasets, Something-Something-V2 and EPIC-KITCHENS-100, which show that our proposed approach consistently outperforms recent state-of-the-art methods in typical semi-supervised settings.}
}
@article{MA2025110875,
title = {Towards trustworthy dataset distillation},
journal = {Pattern Recognition},
volume = {157},
pages = {110875},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110875},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006265},
author = {Shijie Ma and Fei Zhu and Zhen Cheng and Xu-Yao Zhang},
keywords = {Dataset distillation, Out-of-distribution detection, Data-efficient learning, Trustworthy learning},
abstract = {Efficiency and trustworthiness are two eternal pursuits when applying deep learning in practical scenarios. Considering efficiency, dataset distillation (DD) endeavors to reduce training costs by distilling large datasets into tiny ones. However, existing methods concentrate on in-distribution (InD) classification, disregarding out-of-distribution (OOD) samples. On the other hand, OOD detection aims to enhance models’ trustworthiness, which is inefficiently achieved in full-data settings. For the first time, we consider both issues and propose a novel paradigm called Trustworthy Dataset Distillation (TrustDD). By distilling both InD samples and outliers, the condensed datasets are capable of training models competent in both InD classification and OOD detection. To alleviate the requirement of real outlier data, we further propose to corrupt InD samples to generate pseudo-outliers for TrustDD, namely Pseudo-Outlier Exposure. Comprehensive experiments demonstrate the effectiveness of TrustDD. TrustDD is more trustworthy and applicable to real open-world scenarios. Our code is available at https://github.com/mashijie1028/TrustDD.}
}
@article{ZHANG2025110963,
title = {HierCode: A lightweight hierarchical codebook for zero-shot Chinese text recognition},
journal = {Pattern Recognition},
volume = {158},
pages = {110963},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110963},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007143},
author = {Yuyi Zhang and Yuanzhi Zhu and Dezhi Peng and Peirong Zhang and Zhenhua Yang and Zhibo Yang and Cong Yao and Lianwen Jin},
keywords = {Chinese text recognition, Zero-shot learning, Hierarchical information embedding, Optical character recognition},
abstract = {Text recognition, especially for complex scripts like Chinese, faces unique challenges due to its intricate character structures and vast vocabulary. Traditional one-hot encoding methods struggle with the representation of hierarchical radicals, recognition of Out-Of-Vocabulary (OOV) characters, and on-device deployment due to their computational intensity. To address these challenges, we propose HierCode, a novel and lightweight codebook that exploits the innate hierarchical nature of Chinese characters. HierCode employs a multi-hot encoding strategy, leveraging hierarchical binary tree encoding and prototype learning to create distinctive, informative representations for each character. This approach not only facilitates zero-shot recognition of OOV characters by utilizing shared radicals and structures but also excels in line-level recognition tasks by computing similarity with visual features, a notable advantage over existing methods. Extensive experiments across diverse benchmarks, including handwritten, scene, document, web, and ancient text, have showcased HierCode’s superiority for both conventional and zero-shot Chinese character or text recognition, exhibiting state-of-the-art performance with significantly fewer parameters and lower floating-point operations (FLOPs).}
}
@article{WANG2025110958,
title = {Decisive vector guided column annotation},
journal = {Pattern Recognition},
volume = {158},
pages = {110958},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110958},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400709X},
author = {Xiaobo Wang and Shuo Wang and Yanyan Liang and Zhen Lei},
keywords = {Column annotation, Noisy labels, Decisive vector learning},
abstract = {Column annotation aims to annotate tabular column data with semantic labels and plays an essential role in a variety of applications such as data cleaning, data discovery, schema matching, semantic search and data governance. Recently, deep learning techniques have been introduced for column annotation and have successfully achieved substantial gains in performance. However, their incapability of combating with noisy labels is becoming thorny increasingly, as training data corrupted with noisy labels is ubiquitous in the industrial scenarios. To cope with this issue, this paper develops a novel learning paradigm, which exploits decisive (i.e., semi-hard) vectors to guide the model training with noisy labels. Extensive experimental results on the same benchmarks have demonstrated the effectiveness of our method over the state-of-the-art alternatives.}
}
@article{ZENG2025110866,
title = {Cloud-GAN: Cloud Generation Adversarial Networks for anomaly detection},
journal = {Pattern Recognition},
volume = {157},
pages = {110866},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110866},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006174},
author = {Xianhua Zeng and Yang Zhuo and Tianxing Liao and Jueqiu Guo},
keywords = {Anomaly detection, Cloud-GAN, Adaptive weighting, Generative adversarial},
abstract = {Abnormal detection means identifying data that is different from the normal data. In recent work, there have been many methods using deep autoencoders or variational autoencoders to detect abnormal data, and good progress has been made. However, these methods often have the following problems: firstly, the value of low dimensional representations of different layers of potential space is different. And secondly, the effective information is not fully preserved, resulting in unsatisfactory reconstruction results. In this paper, we propose an anomaly detection method based on an adaptive cloud generation adversarial networks (Cloud-GAN), which uses three digital characteristics: expected value, entropy, and hyper entropy to represent the low dimensional representation of data in a more detailed manner in the generator. At the same time, the reconstruction loss weights of different layers are automatically learned. Then we input the reconstruction errors corresponding to the input and output data, as well as the low-dimensional representation of the last layer of the encoder, into the Gaussian kernel density model for density estimation. In a large number of experiments on six public benchmark datasets, we have verified that our method is equivalent to or superior compared to the state-of-the-art models. In particular, the effect is significant in high-dimensional dataset Arrhythmia with fewer training samples and the standard F1-score can be increased by up to 13% compared with the classical method LAKE.}
}
@article{WANG2025110878,
title = {Probabilistic deep metric learning for hyperspectral image classification},
journal = {Pattern Recognition},
volume = {157},
pages = {110878},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110878},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006290},
author = {Chengkun Wang and Wenzhao Zheng and Xian Sun and Jie Zhou and Jiwen Lu},
keywords = {Hyperspectral image classification, Deep learning, Probabilistic metric learning},
abstract = {This paper proposes a probabilistic deep metric learning (PDML) framework for hyperspectral image classification (HSIC). The core problem for HSIC is the spectral variability between intraclass materials and the spectral similarity between interclass materials, motivating the further incorporation of spatial information to differentiate a pixel based on its surrounding patch. However, different pixels and even the same pixel in one patch might not encode the same material due to the low spatial resolution of most hyperspectral sensors, leading to an inconsistent judgment of a specific pixel. To address this issue, we propose a PDML framework to model the categorical uncertainty of the spectral distribution of an observed pixel. We propose to learn a global probabilistic distribution for each pixel in the patch and a probabilistic metric to model the distance between distributions. We treat each pixel in a patch as a training sample to exploit more information from the patch. Our framework can be readily applied to existing HSIC methods. Extensive experiments on IN, UP, KSC, and Houston 2013 datasets demonstrate that our framework improves the performance of existing methods and further achieves the state of the art.}
}
@article{PEIXOTO2025110946,
title = {Low-correlation multilinear dimensionality reduction applied to volcano-seismic classification},
journal = {Pattern Recognition},
volume = {158},
pages = {110946},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110946},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006976},
author = {Antonio Augusto Teixeira Peixoto and Carlos Alexandre Rolim Fernandes and Pablo Espinoza Lara and Adolfo Inza},
keywords = {Dimensionality reduction, Tensor decompositions, Correlation, Classification, Seismic event},
abstract = {In the present work, a technique for reducing the dimensionality of tensor data, called Low-Correlation Multilinear Dimensionality Reduction (LC-MDR), is proposed. The method optimizes a cost function that takes into account the data correlation, generating variables with low correlation. The LC-MDR fits the input data correlation into a new tensor decomposition denoted by Even-Order Nested PARAFAC Decomposition (EONPD), a proposed extension of the Nested PARAFAC Decomposition (NPD) for higher-order tensors. In addition, a generalization of the EONPD, denoted by Higher-Order Nested PARAFAC Decomposition (HONPD), is also presented. Contrarily to existing approaches that use orthogonal transformation matrices, the LC-MDR does not impose this constraint on the transformation matrices in order to minimize the output correlation. The proposed technique was evaluated in a classification system of volcano-seismic events using data obtained from the Ubinas volcano in 2009 using a full tensorial classification framework. The results showed significant gains for the LC-MDR when compared with concurrent techniques in terms of accuracy, data correlation and processing time.}
}
@article{TANG2025110894,
title = {A pure MLP-Mixer-based GAN framework for guided image translation},
journal = {Pattern Recognition},
volume = {157},
pages = {110894},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110894},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006459},
author = {Hao Tang and Bin Ren and Nicu Sebe},
keywords = {GANs, MLP-mixer, Guided image translation},
abstract = {Traditional guided image translation methods, based on encoder–decoder or U-Net structures, often struggle with complex or contrasting images. To address this, we introduce a novel dual-stage strategy. First, we use a cascaded cross-gating MLP-Mixer to merge image and semantic guidance codes, generating intermediate results influenced by these cues. Second, we implement a refined pixel-level loss function to handle semantic guidance noise, along with a new cross-attention gating mechanism for detail refinement. Additionally, our framework utilizes an MLP-Mixer-based discriminator, ensuring that the entire system is built on the MLP-Mixer architecture. Our results in cross-view image translation and person image synthesis outperform current benchmarks, demonstrating the effectiveness of our method.}
}
@article{HAN2025110922,
title = {Complementary branch fusing class and semantic knowledge for robust weakly supervised semantic segmentation},
journal = {Pattern Recognition},
volume = {157},
pages = {110922},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110922},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006733},
author = {Woojung Han and Seil Kang and Kyobin Choo and Seong Jae Hwang},
keywords = {Weakly supervised semantic segmentation, Representation learning, Contrastive learning},
abstract = {Leveraging semantically precise pseudo masks derived from image-level class knowledge for segmentation, namely image-level Weakly Supervised Semantic Segmentation (WSSS), remains challenging. Class Activation Maps (CAMs) using CNNs enhance WSSS by focusing on specific class parts like only the face of a human, whereas Vision Transformers (ViT) capture broader semantic parts but often miss complete class-specific details, such as human bodies with nearby objects like dogs. In this work, we propose a Complementary Branch (CoBra), a novel dual-branch framework consisting of two distinct architectures that provide valuable complementary knowledge of class (from CNN) and semantics (from ViT). In particular, we learn Class-Aware Projection (CAP) for the CNN branch and Semantic-Aware Projection (SAP) for the ViT branch, combining their insights to facilitate new patch-level supervision and create effective pseudo masks integrating class and semantic information. Extensive experiments qualitatively and quantitatively investigate how each branch complements the other, showing a significant result. Project Page and code are available: https://micv-yonsei.github.io/cobra2024/.}
}
@article{MIAO2025110870,
title = {COLAFormer: Communicating local–global features with linear computational complexity},
journal = {Pattern Recognition},
volume = {157},
pages = {110870},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110870},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006216},
author = {Zhengwei Miao and Hui Luo and Meihui Li and Jianlin Zhang},
keywords = {Computer vision, Vision transformer, Image classification, Computational complexity, Feature clustering},
abstract = {Local and sparse attention effectively reduce the high computational cost of global self-attention. However, they suffer from non-global dependency and coarse feature capturing, respectively. While some subsequent models employ effective interaction techniques for better classification performance, we observe that the computation and memory of these models overgrow as the resolution increases. Consequently, applying them to downstream tasks with large resolutions takes time and effort. In response to this concern, we propose an effective backbone network based on a novel attention mechanism called Concatenating glObal tokens in Local Attention (COLA) with a linear computational complexity. The implementation of COLA is straightforward, as it incorporates global information into the local attention in a concatenating manner. We introduce a learnable condensing feature (LCF) module to capture high-quality global information. LCF possesses the following properties: (1) performing a function similar to clustering, aggregating image patches into a smaller number of tokens based on similarity. (2) a constant number of aggregated tokens regardless of the image size, ensuring that it is a linear complexity operator. Based on COLA, we build COLAFormer, which achieves global dependency and fine-grained feature capturing with linear computational complexity and demonstrates impressive performance across various vision tasks. Specifically, our COLAFormer-S achieves 84.5% classification accuracy, surpassing other advanced models by 0.4% with similar or less resource consumption. Furthermore, our COLAFormer-S can achieve a better object detection performance while consuming only 1/4 of the resources compared to other state-of-the-art models. The code and models will be made publicly available.}
}
@article{XING2025110917,
title = {Multi-scale feature extraction and fusion with attention interaction for RGB-T tracking},
journal = {Pattern Recognition},
volume = {157},
pages = {110917},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110917},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400668X},
author = {Haijiao Xing and Wei Wei and Lei Zhang and Yanning Zhang},
keywords = {Single-object tracking, RGB-T tracking, Feature fusion},
abstract = {RGB-T single-object tracking aims to track objects utilizing both RGB images and thermal infrared(TIR) images. Though the siamese-based RGB-T tracker shows its advantage in tracking speed, its accuracy still cannot be compared with other state-of-the-art trackers (e.g., MDNet). In this study, we revisit the existing siamese-based RGB-T tracker and find that such fall behind comes from insufficient feature fusion between RGB image and TIR image, as well as incomplete interactions between template frame and search frame. Inspired by this, we propose a multi-scale feature extraction and fusion network with Temporal-Spatial Memory (MFATrack). Instead of fusing RGB image and TIR image with the single-scale feature map or only high-level features from the multi-scale feature map, MFATrack proposes a new fusion strategy by fusing features from all scales, which can capture contextual information in shallow layers and details in the deep layer. To learn the feature better for tracking tasks, MFATrack fuses the features via several consecutive frames. In addition, we also propose a self-attention interaction module specifically designed for the search frame, highlighting the features in the search frame that are relevant to the target and thus facilitating rapid convergence for target localization. Experimental results demonstrate the proposed MFATrack is not only fast, but also can obtain better tracking accuracy compared with other competing methods including MDNet-based methods and other siamese-based trackers.}
}
@article{MAO2025110951,
title = {POSTER++: A simpler and stronger facial expression recognition network},
journal = {Pattern Recognition},
volume = {157},
pages = {110951},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110951},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007027},
author = {Jiawei Mao and Rui Xu and Xuesong Yin and Yuanqi Chang and Binling Nie and Aibin Huang and Yigang Wang},
keywords = {Facial expression recognition, Industry deployment, Window attention mechanism},
abstract = {The POSTER has achieved SOTA performance in facial expression recognition (FER) by effectively combining facial landmarks and image features through its two-stream pyramid cross-fusion design. However, the architecture of POSTER is undeniably complex. Specifically, the two-stream design and pyramidal architecture of POSTER involve a significant number of parameters. Moreover, the cross-attention mechanism used in POSTER’s cross-fusion has a square-level computational complexity. These designs result in expensive computing cost for POSTER, which are not conducive to its industrial deployment in FER. To address the computational challenges associated with POSTER, this paper introduces POSTER++. POSTER++ simplifies the two-stream design employed in POSTER and introduces a lightweight multi-scale interaction (MSI) network as a replacement for POSTER’s pyramidal architecture. MSI directly integrates facial information at different scales into the token embeddings through attentional interaction. This allows POSTER++ to allocate more attention to the token embedding at the scale that is most useful for the current facial image, thus improving discrimination. Furthermore, we propose a window-based cross-attention mechanism with linear complexity as an alternative to the vanilla cross-attention mechanism. Unlike Swin Transformer, POSTER++ uses single window query based on whole facial landmark features to interact with all windows of facial features, facilitating better capture of subtle changes in expression. Compared to POSTER, POSTER++ with the above improvements shows SOTA FER performance with less 39.1% parameters, 46.4% FLOPs, and 64.5% inference time. This proves the potential of POSTER++ as the backbone of FER industry deployment.}
}
@article{SHEIKHPOUR2025110882,
title = {Sparse feature selection using hypergraph Laplacian-based semi-supervised discriminant analysis},
journal = {Pattern Recognition},
volume = {157},
pages = {110882},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110882},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006332},
author = {Razieh Sheikhpour and Kamal Berahmand and Mehrnoush Mohammadi and Hassan Khosravi},
keywords = {Semi-supervised feature selection, Semi-supervised discriminant analysis, Hypergraph-Laplacian, Trace ratio, Sparse models},
abstract = {Feature selection, as a dimension reduction technique in data mining and pattern recognition, aims to select the most discriminative features and improve the learning performance. With an abundance of unlabeled data readily available across various applications, semi-supervised feature selection has emerged as a promising approach. While most semi-supervised feature selection methods rely on simple graphs to preserve the geometrical structure of data, this approach often fails in capturing the high-order relationships present in many real-world applications. In contrast, hypergraphs offer the ability to encode more complex structures of data beyond what a simple graph can achieve. In this paper, we propose a feature selection method formulated in the trace ratio form, integrating hypergraph Laplacian-based semi-supervised discriminant analysis (SDA) and the mixed convex and non-convex ℓ2,p-norm (0<p≤1) regularization. The proposed trace ratio-based method, called HSDAFS, leverages the discriminative information from labeled data to maximize class separability while also utilizing the hypergraph Laplacian to capture the geometrical structure and high-order relationships within both labeled and unlabeled data. The ℓ2,p-norm regularization in the proposed HSDAFS provides improved sparsity over the ℓ2,1-norm. It ensures that the projection matrix is row-sparse, enabling the effective joint selection of discriminative features across all data. To solve the trace ratio-based HSDAFS method, we convert it into a trace difference method and propose an iterative algorithm. Experiments on several datasets demonstrate that HSDAFS is more effective in selecting the most discriminative features compared to other methods.}
}
@article{GU2025110962,
title = {Dual-scale enhanced and cross-generative consistency learning for semi-supervised medical image segmentation},
journal = {Pattern Recognition},
volume = {158},
pages = {110962},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110962},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007131},
author = {Yunqi Gu and Tao Zhou and Yizhe Zhang and Yi Zhou and Kelei He and Chen Gong and Huazhu Fu},
keywords = {Medical image segmentation, Semi-supervised learning, Scale-enhanced consistency, Cross-generative consistency},
abstract = {Medical image segmentation plays a crucial role in computer-aided diagnosis. However, existing methods heavily rely on fully supervised training, which requires a large amount of labeled data with time-consuming pixel-wise annotations. Moreover, accurately segmenting lesions poses challenges due to variations in shape, size, and location. To address these issues, we propose a novel Dual-scale Enhanced and Cross-generative consistency learning framework for semi-supervised medical image Segmentation (DEC-Seg). First, we propose a Cross-level Feature Aggregation (CFA) module that integrates features from adjacent layers to enhance the feature representation ability across different resolutions. To address scale variation, we present a scale-enhanced consistency constraint, which ensures consistency in the segmentation maps generated from the same input image at different scales. This constraint helps handle variations in lesion sizes and improves the robustness of the model. Furthermore, we propose a cross-generative consistency scheme, in which the original and perturbed images can be reconstructed using cross-segmentation maps. This consistency constraint allows us to mine effective feature representations and boost the segmentation performance. To further exploit the scale information, we propose a Dual-scale Complementary Fusion (DCF) module that integrates features from two scale-specific decoders operating at different scales to help produce more accurate segmentation maps. Extensive experimental results on multiple medical segmentation tasks (polyp, skin lesion, and brain glioma) demonstrate the effectiveness of our DEC-Seg against other state-of-the-art semi-supervised segmentation approaches. The implementation code will be released at https://github.com/taozh2017/DECSeg.}
}
@article{ZHENG2025110865,
title = {A novel image dehazing algorithm for complex natural environments},
journal = {Pattern Recognition},
volume = {157},
pages = {110865},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110865},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006162},
author = {Yuanzhou Zheng and Long Qian and Yuanfeng Zhang and Jingxin Cao and Xinyu Liu and Yong Ma},
keywords = {Image dehazing, Intelligent ship, Hybrid dilated convolution, Hybrid attention, Mixed loss, Visual perception},
abstract = {As an essential preprocessing step in computer vision tasks, image dehazing is preparatory work in many practical applications. To ensure the navigation safety of intelligent ships in complex haze weather, this paper proposes a lightweight real-time dehazing method for the natural environment based on an all-in-one dehazing network. Specifically, a hybrid dilated convolution is used to construct a dehazing network, which effectively expands the receptive field and improves the ability to extract characteristic information without increasing the calculation amount of the model. Secondly, a hybrid attention module is developed to enhance the model's ability to remove haze while retaining more detailed features by giving weights to light haze and heavy haze areas in the image. Finally, considering the subjective perception of human eyes, a mixed loss function is designed to address image dimness and color distortion after dehazing. Through experiments on both synthetic and real-world haze datasets, the proposed method attains ideal dehazing results and has high real-time performance. It can dehaze under different haze conditions, address the critical challenge of degraded visual perception in severe weather conditions of intelligent ship navigation on inland rivers in complex environments, and promote intelligent ships' development.}
}
@article{YANG2025110933,
title = {Semi-supervised pivotal-aware nonnegative matrix factorization with label and pairwise constraint propagation for data clustering},
journal = {Pattern Recognition},
volume = {157},
pages = {110933},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110933},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006848},
author = {Xiaojun Yang and Tuoji Zhu and Siyuan Peng and Feiping Nie and Zhiping Lin},
keywords = {Nonnegative matrix factorization, Semi-supervised learning, Dual constraint propagation, Data clustering},
abstract = {Semi-supervised nonnegative matrix factorization (NMF) methods have found extensive utility in data clustering applications. However, these existing methods encounter challenges in effectively leveraging the limited supervisory information to enhance the performance of clustering. In particular, the majority of these methods tend to solely utilize either the label information or the pairwise constraint information, neglecting the potential benefits of their joint utilization. To address this issue, a novel algorithm named semi-supervised pivotal-aware nonnegative matrix factorization (SPNMF) is proposed in this paper, which utilizes the label information and the pairwise constraint information simultaneously to enhance the performance in data clustering tasks. The proposed method has two main innovations: (1) adopting the dual constraint propagation (DCP) algorithm (i.e., label propagation and pairwise constraint propagation) to effectively utilize a meager amount of label information for learning a compact data representation; (2) incorporating the pivotal-aware technique to mitigate the negative impact of outliers. Notably, the DCP algorithm not only propagates limited label information to a multitude of unlabeled samples, but also disseminates the pairwise constraint supervisory information obtained from the labels to unconstrained samples, allowing for the acquisition of richer supervisory information in the form of pointwise and pairwise constraints. Furthermore, a comprehensive analysis of SPNMF is conducted. Extensive experimental results are executed on eight real-world image datasets. The results underscore the superior performance of SPNMF in comparison to several state-of-the-art methods.}
}
@article{WANG2025110945,
title = {Polarized reflection removal with dual-stream attention guidance},
journal = {Pattern Recognition},
volume = {157},
pages = {110945},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110945},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006964},
author = {Xin Wang and Yong Zhang and Yanchu Chen},
keywords = {Reflection removal, Deep learning, Polarization image, Convolutional neural networks},
abstract = {Addressing the challenge of removing reflections from images captured through glass surfaces holds significant importance across various practical applications. Existing methods often fall short due to reliance on intermediate predictions or special constraints, leading to undesirable artifacts. This paper addresses this issue by recognizing the inherent correlation between background and reflection. Leveraging the distinct optical properties of polarized images, we propose a novel dual-stream network with feature attention guidance for reflection removal. The model, taking multi-channel polarized images as input, exploits the optical characteristics of transmitted and reflected light to tackle the ill-posed problem. Our dual-stream structure predicts both reflectance and transmission images simultaneously, capitalizing on the unique relationship between these two elements. Additionally, we introduce the innovative differential feature fusion (DFF) block to enhance communication between the two streams. Our model shows superior performance to the state-of-the-art methods when tested on real-world polarization datasets.}
}
@article{LING2025110893,
title = {Patient teacher can impart locality to improve lightweight vision transformer on small dataset},
journal = {Pattern Recognition},
volume = {157},
pages = {110893},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110893},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006447},
author = {Jun Ling and Xuan Zhang and Fei Du and Linyu Li and Weiyi Shang and Chen Gao and Tong Li},
keywords = {Vision transformer, Knowledge distillation, Curriculum learning, Small dataset},
abstract = {Vision Transformer (ViT) has achieved unprecedented success in vision tasks with the assistance of abundant data. However, the lack of inductive bias in lightweight ViT makes learning locality challenging on small datasets, leading to poor performance. This limitation impedes the application of lightweight ViT in scenarios with limited datasets and computational power. Knowledge Distillation (KD) allows student models to benefit from the teacher model. However, in the progressive learning stage, traditional single-stage KD methods are usually suboptimal for delivering fixed knowledge to the growing student model. To address these issues, we propose a simple yet effective two-stage KD method called Curriculum Information Knowledge Distillation (CIKD) for the first time. Specifically, we incorporate a curriculum learning framework, progressing from easy to difficult, in the KD curriculum. At the first stage, i.e., Attention Locality Imitation (ALI), the student model learns locality from the low-level semantic features of the teacher model through self-attention distillation. Afterward, at the second stage, i.e., Logit Mimicking (LM), the student model learns label information and high-level semantic logit from the teacher model. Without bells and whistles, our approach achieves state-of-the-art results on 8 small-scale datasets with ViT-Tiny (5.0M). Our code and model weights are available at: https://github.com/newLLing/CIKD.}
}
@article{LIU2025110961,
title = {SparseComm: An Efficient Sparse Communication Framework for Vehicle-Infrastructure Cooperative 3D Detection},
journal = {Pattern Recognition},
volume = {158},
pages = {110961},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110961},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400712X},
author = {Haizhuang Liu and Huazhen Chu and Junbao Zhuo and Bochao Zou and Jiansheng Chen and Huimin Ma},
keywords = {Collaborative perception, 3D object detection, Sparse communication},
abstract = {Collaborative perception, aiming at achieving a comprehensive perception range through inter-agent communication, faces challenges such as high communication costs and domain gaps between multiple agents. This paper introduces SparseComm, an innovative sparse communication collaborative perception framework designed to mitigate these challenges. SparseComm efficiently operates in sparse feature spaces and aggregates features related to the same objects by a sparse instance communication module. Meanwhile, a sparse 3D cooperation module is incorporated to enhance 3D feature representation during communication, thus improving detection performance. Furthermore, a bounding box restoration module is designed to recover undetected bounding boxes due to feature fusion and to address the quality drop issue caused by domain gaps at minimal additional communication cost. Extensive experiments conducted on the DAIR-V2X and V2XSet demonstrate the efficacy of SparseComm, achieving 47.12% at 211.46 communication bytes on DAIR-V2X and 78.03% at 216.63 communication bytes on V2XSet. Notably, SparseComm reduces communication consumption from 10× to 1000× compared with the prior methods while maintaining the detection performance.}
}
@article{HAN2025110869,
title = {Feature aggregation and connectivity for object re-identification},
journal = {Pattern Recognition},
volume = {157},
pages = {110869},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110869},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006204},
author = {Dongchen Han and Baodi Liu and Shuai Shao and Weifeng Liu and Yicong Zhou},
keywords = {Object re-identification, Graph convolutional networks, Feature aggregation},
abstract = {In recent years, object re-identification (ReID) performance based on deep convolutional networks has reached a very high level and has seen outstanding progress. The existing methods merely focus on the robustness of features and classification accuracy but ignore the relationship among different features (i.e., the relationship between gallery–gallery pairs or probe–gallery pairs). In particular, a probe located at the decision boundary is the key to suppressing object ReID performance. We consider this probe as a hard sample. Recent studies have shown that Graph Convolutional Networks (GCN) significantly improve the relationship among features. However, applying the GCN to object ReID is still an open question. This paper proposes two learnable GCN modules: the Feature Aggregation Graph Convolutional Network (FA-GCN) and the Evaluation Connectivity Graph Convolutional Network (EC-GCN). Specifically, the pre-work selects an arbitrary feature extraction network to extract features in the object ReID dataset. Given a probe, FA-GCN aggregates neighboring nodes through the affinity graph of the gallery set. Afterward, EC-GCN uses a random probability gallery sampler to construct subgraphs for evaluating the connectivity of probe–gallery pairs. Finally, we jointly aggregate the node features and connectivity ratios as a new distance matrix. Experimental results on two person ReID datasets (Market-1501 and DukeMTMC-ReID) and one vehicle ReID dataset (VeRi-776) show that the proposed method achieves state-of-the-art performance.}
}
@article{SHOU2025110974,
title = {Masked contrastive graph representation learning for age estimation},
journal = {Pattern Recognition},
volume = {158},
pages = {110974},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110974},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007258},
author = {Yuntao Shou and Xiangyong Cao and Huan Liu and Deyu Meng},
keywords = {Age estimation, Contrastive learning, Graph neural network, Mask mechanism, Image processing},
abstract = {Age estimation of face images is a crucial task with various practical applications in areas such as video surveillance and Internet access control. While deep learning-based age estimation methods like Convolutional Neural Networks (CNN), Multilayer Perceptrons (MLP), and Transformers have shown remarkable performance, they have limitations when modeling complex or irregular objects, and often contain redundant information. Although existing graph neural networks (GNNs) can alleviate this issue, they only considers the structural information of the image and ignores the semantic features, thus the feature representation capability of graph nodes is limited. To address these issues, this paper proposes a novel Masked Contrastive Graph Representation Learning (MCGRL) method for accurate age estimation of face images. Our approach leverages CNN to extract semantic features of the image, which are then partitioned into patches that serve as nodes in the graph. We use a masked graph convolutional network (GCN) to derive image-based node representations that capture rich structural information. Finally, we incorporate multiple losses to explore the complementary relationship between structural information and semantic features, which improves the feature representation capability of GCN. Experimental results on real-world face image datasets demonstrate the superiority of our proposed method over other state-of-the-art age estimation approaches. Our code is available at https://github.com/yuntaoshou/MCGRL}
}
@article{ZHAO2025110950,
title = {Dual-mask: Progressively sparse multi-task architecture learning},
journal = {Pattern Recognition},
volume = {158},
pages = {110950},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110950},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007015},
author = {Jiejie Zhao and Tongyu Zhu and Leilei Sun and Bowen Du and Haiquan Wang and Lei Huang},
keywords = {Multi-task learning, Neural network, Sparse network},
abstract = {Multi-task architecture learning has achieved significant success by learning optimal sharing architectures for different tasks. However, previous works to learn branched architectures for different tasks can sometimes lead to unsatisfying multi-task performance, as not all detailed branches are relevant to a specific task. Task-relevant architectures can be sparse, including only partial channels or layers in the entire architecture (i.e., a sub-network). In addition, most previous works rely on a heuristic architecture selection procedure that could not support continuous architecture optimization. To this end, in this paper, we propose dual-mask, a progressively sparse multi-task architecture learning method. Starting with a task-free architecture, it identifies the informative features along two-level, channels and layers, for each task, while suppressing conflicting or noisy parts in a differentiable manner, so that better task-specific sub-networks are captured. Specifically, the channel and layer selection modules produce respective hybrid binary and real value masks, designed to pick salient channels and layers for each task, respectively. To jointly optimize masks with model parameters, we propose an importance-guided relaxation method for solving the stochastic binary optimization problem, after which the interference or noise parts can be pruned by masks. Additionally, a progressive training strategy with continuation is provided that gradually sparsity the task-specific sub-networks. Experiments show that dual-mask achieves superior performance than SOTA multi-task methods.}
}
@article{PARK2025110877,
title = {Fast video anomaly detection via context-aware shortcut exploration and abnormal feature distance learning},
journal = {Pattern Recognition},
volume = {157},
pages = {110877},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110877},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006289},
author = {Chaewon Park and Donghyeong Kim and MyeongAh Cho and Minjung Kim and Minseok Lee and Seungwook Park and Sangyoun Lee},
keywords = {Video anomaly detection, Surveillance system, Distance learning, Self-supervised learning, Autoencoder},
abstract = {Surveillance systems are essential in computer vision, with video anomaly detection (VAD) critical for real-world security. Conventional approaches anticipate abnormal frames using normal patterns learned with normal training data and determine anomalies by comparing the output and input. However, they often incur high computational costs due to pre-trained networks and complex modules. Also, they suffer from the tendency to overlook anomalies due to strong generalizing ability. To address these, we propose two innovations: Anomaly Distance Learning (ADL) and Context-Aware Skip Connection (CONASkip), maximizing the distinction between normal and abnormal samples. ADL increases the feature gap by learning to separate normal and abnormal features, while CONASkip selectively connects layers to preserve normal information and differentiate output quality. These methods operate on self-supervised signals, replacing complex and costly modules. Additionally, ADL is excluded during testing, enabling fast inference. Our model achieves 130 frames per second and superior performance on three benchmarks, addressing real-world efficiency challenges.}
}
@article{BHAVANA2025110905,
title = {Temporal Matrix Factorization: A polynomial approach to latent factor estimation},
journal = {Pattern Recognition},
volume = {157},
pages = {110905},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110905},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006563},
author = {Prasad Bhavana and Vineet Padmanabhan},
keywords = {Temporal matrix factorization, Recommender systems, Temporal latent factor estimation},
abstract = {Matrix Factorization (MF) is a simple and efficient Machine Learning (ML) technique to discover latent factors that help in explaining the underlying behaviour of actors (for instance, in the domain of recommender systems the actors could be users/items). The technique uses observed data, generated by the actors, to derive the latent factors. With time, more and more observed data that spans across a wide range of time period gets accumulated. The abundance of this data, allows us to discover new behavioural patterns. One such pattern is that, the latent factor values change with time. Most of the existing matrix factorization techniques are agnostic to time of occurrence of the event and fail to foresee the temporal changes in the actor behaviour. Deep neural networks are effective in predicting the recommendations with time, however, the ability to explain the underlying pattern and the associated rationale is poor. In our work, we propose a novel matrix factorization technique to estimate the temporal changes in the user behaviour and demonstrate its effectiveness in achieving time dependent recommendations.}
}
@article{HAN2025110881,
title = {A new data complexity measure for multi-class imbalanced classification tasks},
journal = {Pattern Recognition},
volume = {157},
pages = {110881},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110881},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006320},
author = {Mingming Han and Husheng Guo and Wenjian Wang},
keywords = {Data characteristic, Skewed distribution, Correlation, Multi-class},
abstract = {The skewed class distribution and data complexity may severely affect the imbalanced classification results. The cost of classification can be significantly reduced if these data complexity are measured and pre-processed prior to training, particularly when dealing with large-scale and high-dimensional datasets. Although many methods have been proposed to evaluate data complexity, most of them fail to fully consider the interaction among different data characteristics, or the connection between class imbalance and these characteristics, thus posing a serious challenge to effectively evaluate the difficulty of classification. This paper presents a new data complexity measure MFII (multi-factor imbalance index), which measures the combined effects of the skewed class distribution and data characteristics on classification difficulty. In particular, it further comprehensively investigates the impact of overlap size, confusion degree, and sub-cluster structure. VoR (value of resolution) and DoC (degree of consistency) are also proposed to evaluate the resolution and interpretability of complexity measures. The experimental results demonstrate that MFII has lower VoR and a stronger correlation with classification metrics, which indicates that MFII can more accurately evaluate the difficulty of multi-class imbalanced classification tasks.}
}
@article{WANG2025110868,
title = {Progressive expansion for semi-supervised bi-modal salient object detection},
journal = {Pattern Recognition},
volume = {157},
pages = {110868},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110868},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006198},
author = {Jie Wang and Zihao Zhang and Nana Yu and Yahong Han},
keywords = {Bi-modal salient object detection, Cross-model fusion, Semi-supervised learning},
abstract = {Existing bi-modal salient object detection (SOD) methods primarily rely on fully supervised training strategies that require extensive manual annotation. Undoubtedly, extensive manual annotation is time-consuming and laborious, and the fully supervised strategy is also prone to overfitting on the training set. Therefore, we introduce a semi-supervised learning architecture (SSLA) to alleviate these problems while ensuring detection performance. Considering that the inherent training mode and concise architecture of basic SSLA will limit its ability to effectively explore the learning potential of the model, we further propose two optimization strategies, dynamic adjustment and active expansion. Specifically, we dynamically adjust the supervision scheme for unlabeled samples during training so that the model can continuously utilize the model’s gains (pseudo labels) to supervise and guide the model to further explore the unlabeled samples. Furthermore, the active expansion strategy enables the model to acquire more beneficial supervised information and focuses its attention on difficult-to-segment samples. In summary, an effective progressive expansion network (PENet) architecture for semi-supervised bi-modal SOD is proposed. Extensive experiments indicate that our PENet architecture, while effectively alleviating 90% of annotation burdens, has achieved highly competitive results in RGB-T and RGB-D tasks compared to fully supervised methods. The performance is even more pronounced during cross-dataset testing.}
}
@article{YUAN2025110896,
title = {Distance-aware network for physical-world object distribution estimation and counting},
journal = {Pattern Recognition},
volume = {157},
pages = {110896},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110896},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006472},
author = {Yuan Yuan and Haojie Guo and Junyu Gao},
keywords = {Crowd counting, Deep learning, Vanishing point, Inverse perspective mapping},
abstract = {Crowd counting has become popular due to its applications in congested scenes. Current methods excel with specialized datasets but often ignore the density distribution affected by perspective. To investigate the true distribution of crowd density in the physical world, this paper introduces a framework leveraging head size variations to estimate real crowd distribution and count crowds. First, a convolutional neural network is designed to generate predicted density maps and corresponding bounding boxes across different datasets. Second, a filter kernel is employed to identify the most crowded area in the input image based on the obtained boxes. Finally, the vanishing point is computed by calculating the intersection of the two generated lines, which is then used to obtain the inverse perspective of the predicted density map. The Grid Mean Relative Error (GMRE) metric is proposed for evaluating transformation accuracy. In comparison with the Grid Average Mean Absolute Error (GAME), GMRE is distance-aware and more suitable for evaluating differences between distinct coordinate systems. Additionally, extensive experiments are conducted to validate the counting capabilities of the proposed network. Experimental results show the network’s competitive counting ability and reduced transformation error.}
}
@article{JIAO2025110953,
title = {Cross-modality segmentation of ultrasound image with generative adversarial network and dual normalization network},
journal = {Pattern Recognition},
volume = {157},
pages = {110953},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110953},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007040},
author = {Weiwei Jiao and Hong Han and Yehua Cai and Haihao He and Haobo Chen and Hong Ding and Wenping Wang and Qi Zhang},
keywords = {Elastographic ultrasound, Cross-modality segmentation, Deep learning},
abstract = {Elastographic ultrasound (EUS) evaluates lesion stiffness, providing valuable diagnostic information for various diseases. However, accessibility, cost, and visual clarity of tissue structures are limitations of EUS compared to conventional B-mode ultrasound (BUS), which hinders the training of segmentation models based on EUS. To address this challenge, the BUS is utilized as the source domain to generate two distinct new domains, one similar to the style of EUS and the other dissimilar, through the implementation of a style transfer module containing the cycle-consistent generative adversarial network, resulting in effective data augmentation. The dual-normalization module is integrated into the segmentation model to capture both the domain-invariant and domain-specific distribution information from the generated domains, enabling the model to generalize to the target domain, namely the EUS. We conduct the cross-modality segmentation on two EUS datasets of Achilles tendons and lymph nodes. The experiments have shown that our method outperforms other advanced segmentation methods.}
}
@article{WANG2025110940,
title = {NLA-GNN: Non-local information aggregated graph neural network for heterogeneous graph embedding},
journal = {Pattern Recognition},
volume = {158},
pages = {110940},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110940},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006915},
author = {Siheng Wang and Guitao Cao and Wenming Cao and Yan Li},
keywords = {Graph neural networks, Heterogeneous graph, Node classification, Semi-supervised learning},
abstract = {Heterogeneous graphs are ubiquitous in the real world. Recent methods aim to obtain meaningful low-dimensional node embeddings from heterogeneous graphs to facilitate downstream applications. However, most existing methods tend to consider the local information but ignore the non-local information. This paper proposes a novel Non-Local Information Aggregated Graph Neural Network (NLA-GNN) that aggregate not only the local information from neighbor nodes but also non-local information from distant nodes. Specifically, Local aggregation modules in NLA-GNN utilize the attention mechanism to generate potentially valuable metapaths and use them to aggregate local information. In contrast, non-local aggregation modules adopt a two-step approach, and each step uses an attention-guided method to sort nodes into node sequences and aggregate information with the methods designed for sequential data. Experiment results on three heterogeneous graph datasets demonstrate the performance of NLA-GNN over state-of-the-arts and the necessity of non-local aggregation in heterogeneous graphs.}
}
@article{LI2025110969,
title = {SwapInpaint2: Towards high structural consistency in identity-guided inpainting via background-preserving GAN inversion},
journal = {Pattern Recognition},
volume = {158},
pages = {110969},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110969},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007209},
author = {Honglei Li and Yifan Zhang and Wenmin Wang and Shenyong Zhang and Shixiong Zhang},
keywords = {Identity-guided face inpainting, GAN inversion, Image synthesis},
abstract = {In this work, we propose SwapInpaint2 to enhance the naturalness of identity-guided face inpainting. The previous version, SwapInpaint, relied on a generic inpainting model for content infer, bringing the problems of unnatural structure to the results. Our method addresses this issue by utilizing an inversion-based Background-preserving Attribute Extractor and an improved Embedding Integration Generator to provide high-quality attribute embeddings and produce high-naturalistic results. Comparison experiments with state-of-the-art models demonstrate that our new method achieves superior structural consistency and stronger style alignment both in terms of quality and quantity.}
}
@article{LIANG2025110924,
title = {Learning subjective time-series data via Utopia Label Distribution Approximation},
journal = {Pattern Recognition},
volume = {157},
pages = {110924},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110924},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006757},
author = {Xuefeng Liang and Wenxin Xu and Hexin Jiang and Ying Zhou and Yin Zhao and Jie Zhang},
keywords = {Label distribution bias, Subjective time-series regression, Utopia Label Distribution Approximation, Time-slice normal sampling, Convolution weighted loss},
abstract = {Subjective time-series regression (STR) tasks have gained increasing attention recently. However, most existing methods overlook the label distribution bias in STR data, which results in biased models. Emerging studies on imbalanced regression tasks, such as age and depth estimations, hypothesize the label distribution is uniform and known. But in reality, the label distribution of test set in STR tasks is usually non-uniform and unknown. Moreover, the time-series data exhibits continuity in both temporal context and label spaces, which has not been addressed by existing methods. To tackle these issues, we propose a Utopia Label Distribution Approximation (ULDA) method, which approximates the training label distribution to the real-world but unknown (utopia) label distribution for calibrating the training and test sets. The utopia label distribution is generated by convolving the original one using a Gaussian kernel. ULDA also has two new devised modules (Time-slice Normal Sampling (TNS) generating required new samples and Convolution Weighted Loss (CWL) lowering learning weights for redundant samples), which not only assist the model training, but also maintain the sample continuity in temporal context space. Extensive experiments demonstrate that ULDA lifts the state-of-the-art performance on STR tasks and shows a considerable generalization ability to other time-series tasks.}
}
@article{PU2025110900,
title = {GADNet: Improving image–text matching via graph-based aggregation and disentanglement},
journal = {Pattern Recognition},
volume = {157},
pages = {110900},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110900},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006514},
author = {Xiao Pu and Zhiwen Wang and Lin Yuan and Yu Wu and Liping Jing and Xinbo Gao},
keywords = {Image–text matching, Aggregation reasoning, Disentanglement mechanism, Multi-granularity semantic learning, Consistency learning},
abstract = {Image–text matching is a fundamental yet challenging task in multimedia systems, requiring effective handling of large visual-semantic discrepancies between modalities. Existing approaches often struggle to adequately distinguish co-occurrence knowledge and modal-specific information cross the two modalities. In this paper, we propose a novel approach to improve image–text matching through a Graph-based Aggregation-Disentanglement framework (GADNet). GADNet comprises two key components: a heterogeneous graph-based Aggregation network (A-net) to enhance semantic co-occurrence knowledge, and a representation Disentanglement network (D-net) to emphasize modal-specific information. In addition, we introduce a Multi-Granularity Consistency Learning (MGCL) mechanism to compute the overall discrepancy between images and text based on the aggregated and disentangled features learned from the GADNet. Extensive experiments conducted on two public datasets, Flickr30K and MS-COCO, demonstrate the effectiveness of our proposed approach in both image-to-text and text-to-image retrieval tasks.}
}
@article{LIU2025110936,
title = {Relation-Specific Feature Augmentation for unbiased scene graph generation},
journal = {Pattern Recognition},
volume = {157},
pages = {110936},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110936},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006873},
author = {Zhihong Liu and Jianji Wang and Hui Chen and Yongqiang Ma and Nanning Zheng},
keywords = {Image understanding, Scene graph generation, Long-tailed distribution, Feature augmentation},
abstract = {Scene Graph Generation (SGG) models suffer from the long-tailed distribution of relations, which results in biased predictions that favor head relations (e.g., on) over informative tail ones (e.g., sitting on, laying on, standing on). Existing solutions typically adopt class re-balancing strategies to balance data distribution. However, they do not essentially solve the lack of information due to insufficient tail data. To this end, we propose a Relation-Specific Feature Augmentation (RSFA) framework to mitigate the long-tailed bias by augmenting relations in the feature space. To perform augmentation effectively, we design an augmentation scheme and a novel Dual Attention Network (DAN). The augmentation scheme augments each relation uniformly based on the reciprocal number of samples to avoid over-fitting. By extracting relation-specific information from new object features generated by a Conditional Variational AutoEncoder (CVAE), DAN generates reliable virtual relation representations to provide useful information to guide optimizing relation classifier. Extensive ablation studies and comprehensive analysis demonstrate the effectiveness of our method in debiasing. And results on the Visual Genome benchmark show that our method significantly outperforms the existing state-of-the-art methods.}
}
@article{SUN2025110884,
title = {A multi-agent curiosity reward model for task-oriented dialogue systems},
journal = {Pattern Recognition},
volume = {157},
pages = {110884},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110884},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006356},
author = {Jingtao Sun and Jiayin Kou and Wenyan Hou and Yujei Bai},
keywords = {Task-oriented dialogue systems, Reinforcement learning, Curiosity rewards, Exploration and exploitation},
abstract = {In practical decision-making dialogues, reinforcement learning methods face hurdles due to delays and sparse reward feedback for agents, and in some cases, lack of rewards altogether. These issues can impede efficient learning of dialogue strategies and compromise the performance of the model. To address this challenge, this paper introduces the Multi-Agent Curiosity Reward Model (MACRM) for task-oriented dialog systems. Firstly, in terms of dialog reward mechanisms, a forward dynamics model generates curiosity rewards, which are integrated with extrinsic rewards from the dialog environment feedback to mitigate the problem of sparse rewards resulting from inadequate agent exploration. Secondly, regarding the dialogue strategy training mechanism, an exploration-exploitation approach inspired by organismic exploration is adopted. This approach involves fully exploring the dialogue environment in the early stages and optimally exploiting learned knowledge later, thereby balancing exploration and exploitation and enhancing dialogue strategy learning efficiency. To assess the proposed model's effectiveness, experiments are conducted using the MultiWOZ corpus across three reward environments: (1) extrinsic rewards only, (2) curiosity rewards only, and (3) a combination of both. The experimental results demonstrate that agents employing MACRM exhibit faster learning of dialogue strategies compared to those relying on a single exploratory reward method, effectively addressing reward sparsity and delay issues in practical decision-making scenarios.}
}
@article{FANG2025110912,
title = {Source-free collaborative domain adaptation via multi-perspective feature enrichment for functional MRI analysis},
journal = {Pattern Recognition},
volume = {157},
pages = {110912},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110912},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006630},
author = {Yuqi Fang and Jinjian Wu and Qianqian Wang and Shijun Qiu and Andrea Bozoki and Mingxia Liu},
keywords = {Source-free domain adaptation, Feature enrichment, Collaborative learning, Unsupervised pretraining},
abstract = {Resting-state functional MRI (rs-fMRI) is increasingly employed in multi-site research to analyze neurological disorders, but there exists cross-site/domain data heterogeneity caused by site effects such as differences in scanners/protocols. Existing domain adaptation methods that reduce fMRI heterogeneity generally require accessing source domain data, which is challenging due to privacy concerns and/or data storage burdens. To this end, we propose a source-free collaborative domain adaptation (SCDA) framework using only a pretrained source model and unlabeled target data. Specifically, a multi-perspective feature enrichment method (MFE) is developed to dynamically exploit target fMRIs from multiple views. To facilitate efficient source-to-target knowledge transfer without accessing source data, we initialize MFE using parameters of a pretrained source model. We also introduce an unsupervised pretraining strategy using 3,806 unlabeled fMRIs from three large-scale auxiliary databases. Experimental results on three public and one private datasets show the efficacy of our method in cross-scanner and cross-study prediction.}
}
@article{WANG2025110965,
title = {Dealing with partial labels by knowledge distillation},
journal = {Pattern Recognition},
volume = {158},
pages = {110965},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110965},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007167},
author = {Guangtai Wang and Jintao Huang and Yiqiang Lai and Chi-Man Vong},
keywords = {Partial label learning, Knowledge distillation, Over-confidence},
abstract = {Partial label learning (PLL) is a weakly supervised methodology dealing with tasks that have annotation problems by replacing the single label with a collection of candidate labels. Compared to single labels, utilizing partial labels faces challenges: (1) The limited supervision and sensitivity to the false positive candidates; (2) Situations where the ground truth is not in the candidate label sets (noisy PLs). However, in the case that there exists a subset of samples that can be easily labeled (referred to as clean samples), the existing PLL paradigm needlessly assigns these instances with partial labels randomly. To better utilize the clean samples, and alleviate the obstacles of adopting partial labels, we proposed a specific Partial Label Knowledge Distillation (PLKD) framework to distill the knowledge from the samples with low annotating cost, further guiding partial label samples with limited supervision in these scenarios. The teacher model of PLKD was pre-trained on the clean samples with a single label, which can reduce the effect of noisy PLs when training on the remaining PLL samples. Additionally, recognizing that the existing candidate labels are sampled under the uniform distribution, which may not reflect real-life scenarios, we also proposed a label-specific candidate generation method. Correspondingly, a new loss function based on our generation method is presented to evaluate the distinction between partial labels and predictions. Furthermore, we also present a partial-label guided version, denoted as PLKD-pl, to alleviate the teacher’s risk of over-confidence when the distribution between the clean set and partial label set varies widely. Extensive experimental evaluations have been conducted to demonstrate the superiority of PLKD over six state-of-the-art counterparts.}
}
@article{ZHANG2025110964,
title = {Joint estimation for multisource Gaussian graphical models based on transfer learning},
journal = {Pattern Recognition},
volume = {158},
pages = {110964},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110964},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007155},
author = {Yuqi Zhang and Yuehan Yang},
keywords = {Transfer learning, Multisource data, Gaussian graphical models, Penalized regressions},
abstract = {This study considers data from multiple sources for Gaussian graphical models with one target graph and several auxiliary graphs. We propose a method called joint estimation for multisource Gaussian graphical models (JEM-GGM) to achieve a stable and accurate estimate of the target graph. Using the information from the auxiliary graphs, the proposed method is used to effectively solve the problem of small sample sizes. In this method, equivalent regression models are developed for graphs and the difference between the auxiliary and target graphs was penalized to ensure computational efficiency and improve estimation accuracy. Simulations revealed that the proposed method always outperformed other methods in terms of estimation and prediction accuracy. The application of this method to breast and lymphatic cancer cell lines reveals that the proposed method always obtains a sparse collection of important genome pairs.}
}
@article{LI2025110867,
title = {A novel q-rung orthopair fuzzy MAGDM method for healthcare waste treatment based on three-way decisions},
journal = {Pattern Recognition},
volume = {157},
pages = {110867},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110867},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006186},
author = {Tengbiao Li and Junsheng Qiao},
keywords = {Multi-attribute group decision-making, -Rung orthopair fuzzy sets, Three-way decisions, Dombi probabilistic ordered weighted averaging operator, Cross entropy},
abstract = {In multi-attribute group decision-making (MAGDM) problems, q-rung orthopair fuzzy sets (q-ROFSs) can describe imprecise information more flexibly, and three-way decisions (TWDs) can deal with increasingly complex decision-making problems, and the integration of q-ROFSs and TWDs is the main trend of MAGDM research. In addition, cross entropy (CE), a common methodology in pattern recognition (PR), plays an important role in the information analysis process of MAGDM. In order to realize MAGDM under multigranulation q-rung orthopair fuzzy incomplete information system (MGq-ROFIIS), a new TWDs-based and CE-based MAGDM method is established. First, the concept of MGq-ROFIIS is given, and an information completion technique is proposed by considering missing scenarios. On this basis, the distance-based calculation method of expert weight and attribute weight with low computational overhead is given. Second, a new q-rung orthopair fuzzy Dombi probabilistic ordered weighted averaging (q-ROFDPOWA) operator is proposed, which considers both expert attitude and probability information, and realizes preference information fusion in different group environments. Third, for the aggregated MGq-ROFIIS, TWDs is used to divide three decision regions, CE is introduced as the ranking standard, and a new MAGDM method is constructed under both active and passive scenarios. Then, by applying the obtained results into healthcare waste treatment technologies (HCWTTs) selection, the feasibility and effectiveness of the method are illustrated, some suggestions for related departments in the aspects of technology selection and final decision-making are put forward, and the validity, stability and scientificity of the method are verified by reliability, sensitivity and comparative analyses. Finally, our method is applied to other classic MAGDM scenarios, which further proves the superior performance of the method.}
}
@article{YANG2025110895,
title = {Camouflaged Object Detection via Dual-branch Fusion and Dual Self-similarity constraints},
journal = {Pattern Recognition},
volume = {157},
pages = {110895},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110895},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006460},
author = {Haozhe Yang and Yuan Zhu and Ke Sun and Haoyang Ding and Xianming Lin},
keywords = {Camouflaged Object Detection, Dual-branch feature fusion, Self-similarity, Edge enhancement, Small-scale object localization},
abstract = {Camouflaged Object Detection (COD) remains a challenging task due to the inherent difficulty in distinguishing concealed objects from their surroundings, primarily owing to their high visual similarity. Current research efforts have been insufficient in distinguishing the similarity differences between camouflaged objects and their corresponding backgrounds, leading to suboptimal performance, particularly when locating small-scale objects. To solve this issue, we introduce a novel Dual-branch Fusion and Dual Self-similarity Network (DSNet) comprising three modules. The first, a dual-branch fusion, mimics human observation of camouflaged objects and extracts multi-angle information. Dual-branch features are then decoded using a symmetric joint decoder module with channel interaction via multi-stage inter-group interaction. Inspired by natural organisms’ self-similarity, the self-similarity constraint module employs global and mutual constraints to identify subtle foreground-background differences. DSNet shows superior performance in experiments, and the constraint module can enhance other models as a plug-and-play component, further boosting overall performance.}
}
@article{LI2025110976,
title = {TBNet: A texture and boundary-aware network for small weak object detection in remote-sensing imagery},
journal = {Pattern Recognition},
volume = {158},
pages = {110976},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110976},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007271},
author = {Zheng Li and Yongcheng Wang and Dongdong Xu and Yunxiao Gao and Tianqi Zhao},
keywords = {Deep learning, Remote sensing object detection, Small weak object, Texture information, Boundary feature, Feature decoupling},
abstract = {Object detection is of great importance for remote sensing image interpretation work and has received significant attention. However, small weak object detection has always been a challenge. The main reason is that the critical information of these objects, such as textures and boundaries, is suppressed by the background and cannot effectively express their own characteristics. To address this issue, we introduce a novel texture and boundary-aware network (TBNet) in this paper. Firstly, we propose a texture-aware enhancement module (TAEM) to explore the texture details within the images. TAEM captures pixel correlations to perceive the distribution of texture in the objects. Secondly, a boundary-aware fusion module (BAFM) is introduced to emphasize spatial positions. BAFM can extract the edge information to guide the prediction of small weak objects. Finally, a task-decoupled RCNN (TD-RCNN) is designed to separate classification and regression tasks. TD-RCNN achieves fine-grained detection, avoiding compromises between subtasks. Comprehensive experiments on four public datasets, DIOR NWPU VHR-10, RSOD, and AI-TOD, demonstrate that TBNet achieves state-of-the-art performance compared to competitors. The model is also evaluated on UAVOD-10, which collects numerous small weak objects. TBNet achieves state-of-the-art results while significantly outperforming competitors, proving its ability to detect small weak objects.}
}
@article{CHU2025110939,
title = {Occlusion-guided multi-modal fusion for vehicle-infrastructure cooperative 3D object detection},
journal = {Pattern Recognition},
volume = {157},
pages = {110939},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110939},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006903},
author = {Huazhen Chu and Haizhuang Liu and Junbao Zhuo and Jiansheng Chen and Huimin Ma},
keywords = {3D object detection, Vehicle-infrastructure cooperative, Vehicle-vehicle cooperative, Occlusion, Autonomous driving},
abstract = {In autonomous driving, leveraging sensor data (e.g. camera, LiDAR data) from both the vehicle and the infrastructure significantly improves perception capabilities. However, this integration traditionally results in increased demands on communication bandwidth. To address these challenges, we introduce Fusion2comm, an occlusion-guided feature fusion approach designed to optimize vehicle-infrastructure cooperative 3D object detection. Our innovative strategy employs an intelligent fusion of camera and LiDAR data to enhance the expressiveness of features. Subsequently, it leverages a segmentation model to extract foreground features and utilizes an occlusion-based selection of communication content, effectively easing bandwidth constraints. We propose a multimodal foreground feature fusion architecture that selectively processes and transmits critical information, substantially reducing irrelevant background data transfer. An innovative occlusion confidence-aware communication technique dynamically adjusts communication regions based on occlusion levels, ensuring efficient data exchange. Fusion2comm sets a new benchmark in the DAIR-V2X dataset, achieving an average precision of 71.25% with minimal bandwidth usage of 221.04 bytes. Our comprehensive experimental evaluations confirm that Fusion2comm substantially advances detection precision while simultaneously improving communication efficiency.}
}
@article{TABEJAMAAT2025110934,
title = {EEG classification with limited data: A deep clustering approach},
journal = {Pattern Recognition},
volume = {157},
pages = {110934},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110934},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400685X},
author = {Mohsen Tabejamaat and Hoda Mohammadzade and Farhood Negin and Francois Bremond},
keywords = {Deep neural network, Overfitting issue, Electroencephalography, Data scarcity},
abstract = {In this paper, we propose a novel training strategy designed to prevent deep neural networks from overfitting when trained on a very limited number of samples. This approach enables the use of very deep networks in applications like EEG classification, where data collection is challenging, leading to significant overfitting issues. To address this issue, we introduce a classification network that leverages a deep clustering operation in its latent space. The network utilizes a set of dictionary elements as auxiliary inputs and learns to guide each input sample toward one of the clusters, each derived from a dictionary element. This process facilitates the grouping of samples from the same class within the same region of the latent space, thereby reducing the risk of complex class boundaries and overfitting. Additionally, we introduce two probabilistic models within our clustering approach to mitigate outlier issues and address augmentation challenges commonly encountered with EEG signals. Experimental evaluations on two widely recognized databases, EEG-ImageNet and BCIIV2a, demonstrate the effectiveness and superiority of our method compared to state-of-the-art approaches, achieving 97.9% accuracy on EEG-ImageNet and 84.1% accuracy on BCIIV2a.}
}
@article{LIU2025110947,
title = {Outlier detection using local density and global structure},
journal = {Pattern Recognition},
volume = {157},
pages = {110947},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110947},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006988},
author = {Huawen Liu and Shichao Zhang and Zongda Wu and Xuelong Li},
keywords = {Outlier detection, Potential energy, Data density, Data hub, Random walk},
abstract = {Outlier detection, a critical process for enhancing data quality, has been applied to a broad spectrum of real-world applications. This study introduces a novel and effective outlier detection method that incorporates both local density and global structural information. Specifically, we employ the concept of potential energy, a well-established principle in physics, to represent local densities or neighbor distributions of data. Additionally, we also exploit the notion of hubness, a measure explored in network science, to further capture global structural information of data. To facilitate this, we construct a strongly connected digraph by introducing a virtual node along with an associated set of virtual edges. This digraph underpins a tailored Markov random walk process that is used to estimate the hubness scores of data objects. Subsequently, the importance degrees of data objects are estimated according to their potential energies and hubness scores. The data objects with lower importance degrees are deemed to be outliers. Extensive experiments conducted on thirteen publicly accessible real-world datasets against nine popular outlier detection algorithms show that the proposed method achieved encouraging and competitive performance in comparing the state-of-the-art outlier detection algorithms.}
}
@article{SUVEGES2025110923,
title = {Unsupervised mapping and semantic user localisation from first-person monocular video},
journal = {Pattern Recognition},
volume = {158},
pages = {110923},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110923},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006745},
author = {Tamas Suveges and Stephen McKenna},
keywords = {Egocentric (first-person) vision, Unsupervised Learning, Mapping and localisation},
abstract = {We propose an unsupervised probabilistic framework for learning a human-centred representation of a person’s environment from first-person video. Specifically, non-geometric maps modelled as hierarchies of probabilistic place graphs and view graphs are learned. Place graphs model a user’s patterns of transition between physical locations whereas view graphs capture an aspect of user behaviour within those locations. Furthermore, we describe an implementation in which the notion of place is divided into stations and the routes that interconnect them. Stations typically correspond to rooms or areas where a user spends time. Visits to stations are temporally segmented based on qualitative visual motion. We describe how to learn maps online in an unsupervised manner, and how to localise the user within these maps. We report experiments on two datasets, including comparison of performance with and without view graphs, and demonstrate better online mapping than when using offline clustering.}
}
@article{FRANZONI2025110871,
title = {Evolving meta-correlation classes for binary similarity},
journal = {Pattern Recognition},
volume = {157},
pages = {110871},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110871},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006228},
author = {Valentina Franzoni and Giulio Biondi and Yang Liu and Alfredo Milani},
keywords = {Evolutionary computation, Network topology, Complex networks, Link prediction, Binary similarity},
abstract = {In the field of machine learning and pattern recognition, the use of binary correlation indices is essential for accurate prediction and modelling. This work presents a novel evolutionary method to address the problem of discovering binary correlation indices in different application domains. The proposed approach introduces the concept of meta-correlation, a parametric formula representing classes of binary similarity indices, and optimizes it through an evolutionary scheme. The method has been experimented with and validated in the context of the link prediction problem based on local topological similarity (i.e. graph neighbourhood). A Differential Evolution optimization algorithm finds the evolved correlations that perform best in a given domain. Experiments conducted across different network domains have shown that the instances of the discovered meta-correlations generally outperform state-of-the-art binary correlation indices for all the experimented domains. This approach effectively explores the correlation space and can find a unique pattern that adapts to the domains under consideration. The meta-correlation classes can be applied to both topological and semantic similarity problems, taking into account local information without requiring complete knowledge of the graph.}
}
@article{ZHANG2025110899,
title = {DES-AS: Dynamic ensemble selection based on algorithm Shapley},
journal = {Pattern Recognition},
volume = {157},
pages = {110899},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110899},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006502},
author = {Zhong-Liang Zhang and Yun-Hao Zhu and Xing-Gang Luo},
keywords = {Dynamic ensemble selection, Classifier competence, Shapley value, Monte Carlo simulation, Dynamic weighting},
abstract = {Dynamic ensemble selection (DES) effectively improves aggregation performance by dynamically finding the most appropriate subset of classifiers for each query sample. The key component of this strategy is to define an appropriate criterion for evaluating the competence of the candidate classifiers. In this study, we present a new group-based criterion, synergy competence, to comprehensively measure the magnitude of the intricate synergy effect among classifiers. In addition, we introduce the algorithm Shapley, a variant of the Shapley value (SV) in the game theory, to measure the synergy competence of each classifier, which helps in the construction of a synergy-based DES system, called DES-AS. We argue that candidate classifiers with positive algorithm Shapley should be selected as they contribute to the ensemble. After selecting a subset of classifiers, the normalized algorithm Shapley is employed to calculate the voting weights for a cooperative final classification decision. The experimental analyses conducted on forty real-world datasets not only indicate that the synergy competence estimated by the algorithm Shapley is more applicable than classic group-based metrics, but also show that DES-AS is substantially more effective and robust than state-of-the-art models. The statistical test results also indicate that the estimated synergy competence reflects the real competence of each classifier better than the baseline models.}
}
@article{ZHANG2025110952,
title = {CosineTR: A dual-branch transformer-based network for semantic line detection},
journal = {Pattern Recognition},
volume = {158},
pages = {110952},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110952},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007039},
author = {Yuqi Zhang and Bole Ma and Luyang Jin and Yuancheng Yang and Chao Tong},
keywords = {Semantic line, Detection model, Visual property, Semantic features},
abstract = {Semantic line is a straight line based representation designed to well capture the spatial layout or structural shape of the scene in an image that is valuable as a high-level visual property. In this paper, we propose an efficient end-to-end trainable semantic line detection model named Complementary semantic line TRansformer (CosineTR), which is designed according to an old proverb “two heads are better than one”. CosineTR adopts a dual-branch framework to detect semantic lines with a coarse to fine strategy. These two branches are built based on well-designed attention modules to capture multi-scale line semantic features locally and globally, and are equipped with heatmap prediction head and parameter regression head respectively to perform semantic line detection from two different perspectives. In addition, we introduce bilateral region attention and Gaussian prior cross-attention modules to reinforce semantic contexts extracted by the two branches, and couple them to form complementary feature representations by leveraging a feature interaction method. Extensive experiments demonstrate that our approach is effective and achieves competitive semantic line detection performance on multiple datasets.}
}
@article{ZHU2025110975,
title = {Driving mutual advancement of 3D reconstruction and inpainting for masked faces},
journal = {Pattern Recognition},
volume = {158},
pages = {110975},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110975},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400726X},
author = {Guosong Zhu and Zhen Qin and Erqiang Zhou and Yi Ding and Zhiguang Qin},
keywords = {3D face reconstruction, Image inpainting},
abstract = {Target occlusion or pollution has always been a common and difficult problem in 3D reconstruction, seriously affecting the reconstruction effect, especially in single image scenario. To address the issues of incomplete reconstruction caused by pixel missing, a novel framework promoting mutual enhancement between 3D reconstruction and inpainting is proposed, which is capable of reconstructing a realistic 3D face with pixel completion from single image without prior assumptions. The framework is composed of an inpainting module for facial pixel completion and a reconstruction module for 3D face modeling. The inpainting module provides complete texture and accurate 3D geometry inferring premise for 3D reconstruction, and the reconstruction module is also involved in face inpainting, constraining the inpainting module to fill plausible pixels that are closer to the ground truth. Experiments show that the accurate 3D faces with complete and fine texture can be reconstructed by the proposed framework from largely masked images, with a competitive performance even surpassing most of state-of-the-art methods with unmasked images as input. Furthermore, this framework has achieved state-of-the-art performance in image inpainting on multiple face datasets.}
}
@article{SU2025110935,
title = {A review of deep-learning-based super-resolution: From methods to applications},
journal = {Pattern Recognition},
volume = {157},
pages = {110935},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110935},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006861},
author = {Hu Su and Ying Li and Yifan Xu and Xiang Fu and Song Liu},
keywords = {Deep learning, Super-resolution, Single image super-resolution, Multiple image super-resolution, Degradation model},
abstract = {Super-resolution (SR), aiming to super-resolve degraded low-resolution image to recover the corresponding high-resolution counterpart, is an important and challenging task in computer vision, and with various applications. The emergence of deep learning (DL) has significantly advanced SR methods, surpassing the performance of traditional techniques. This paper presents a comprehensive survey of DL-based SR methods encompassing single image super resolution (SISR) and multiple image super resolution (MISR) methods, along with their applications and limitations. In SISR methods, addressing individual images independently, we review blind and non-blind SR methods. Additionally, within MISR, we delve into multi-frame, multi-view, and reference-based SR methods. DL-based SR methods are categorized from the application perspective and a taxonomy is proposed. Finally, we present research prospects and future directions.}
}
@article{YOON2025110883,
title = {Rethinking convolutional neural networks for trajectory refinement},
journal = {Pattern Recognition},
volume = {157},
pages = {110883},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110883},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006344},
author = {Hanbit Yoon and Usman Ali and Joonhee Choi and Eunbyung Park},
keywords = {Trajectory refinement, Trajectory prediction, Convolutional neural network, Multishape refinement},
abstract = {In this work, we revisit CNN architectures for sequence modeling, focusing on human trajectory prediction tasks. Forecasting human trajectories has been extensively explored as a sequence modeling problem, with many studies utilizing CNNs. Unlike conventional approaches that apply 1D convolution or 2D convolution over heatmap representations, we propose a novel architecture that applies 2D convolution directly over raw trajectory coordinates. Our method employs a coarse-to-fine strategy to refine trajectory predictions. We evaluated our approach on the ETH/UCY and Stanford Drone Datasets, demonstrating significant improvements. Our method sets a new state-of-the-art on the Stanford Drone Dataset, improving prediction accuracy and outperforming existing methods.}
}
@article{ZHANG2025110926,
title = {Towards reliable domain generalization: Insights from the PF2HC benchmark and dynamic evaluations},
journal = {Pattern Recognition},
volume = {157},
pages = {110926},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110926},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006770},
author = {Jiao Zhang and Xiang Ao and Xu-Yao Zhang and Cheng-Lin Liu},
keywords = {Distribution shift, Structure learning, Dataset, Evaluation},
abstract = {Deep neural networks (DNNs) are easily biased towards the training set, which causes substantial performance degradation for out-of-distribution data. Many methods are studied to generalize under various distribution shifts in the literature of domain generalization (DG). To facilitate practical DG research, we construct a large-scale non-independent and identically distributed Chinese characters dataset called PaHCC (Printed and Handwritten Chinese Characters) for a real application scenario (generalization from Printed Fonts to Handwritten Characters, PF2HC) of DG methods. We evaluate eighteen DG methods on the proposed PaHCC dataset and demonstrate that the performance of the current algorithms on this dataset remains inadequate. To improve the performance, we propose a radical-based multi-label learning method by integrating structure learning into statistical methods. Moreover, in the dynamic evaluation settings, we discover additional properties of DG methods and demonstrate that many algorithms suffer from unstable performances. We advocate that researchers in the DG community pay attention not only to accuracy under the fixed leave-one-domain-out protocol but also to algorithmic stability across variable training domains in future studies. Our dataset, method, and evaluations bring valuable insights to the DG community and could promote the development of realistic and stable algorithms.}
}
@article{DARBAN2025110874,
title = {CARLA: Self-supervised contrastive representation learning for time series anomaly detection},
journal = {Pattern Recognition},
volume = {157},
pages = {110874},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110874},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006253},
author = {Zahra Zamanzadeh Darban and Geoffrey I. Webb and Shirui Pan and Charu C. Aggarwal and Mahsa Salehi},
keywords = {Anomaly detection, Time series, Deep learning, Contrastive learning, Representation learning, Self-supervised learning},
abstract = {One main challenge in time series anomaly detection (TSAD) is the lack of labelled data in many real-life scenarios. Most of the existing anomaly detection methods focus on learning the normal behaviour of unlabelled time series in an unsupervised manner. The normal boundary is often defined tightly, resulting in slight deviations being classified as anomalies, consequently leading to a high false positive rate and a limited ability to generalise normal patterns. To address this, we introduce a novel end-to-end self-supervised ContrAstive Representation Learning approach for time series Anomaly detection (CARLA). While existing contrastive learning methods assume that augmented time series windows are positive samples and temporally distant windows are negative samples, we argue that these assumptions are limited as augmentation of time series can transform them to negative samples, and a temporally distant window can represent a positive sample. Existing approaches to contrastive learning for time series have directly copied methods developed for image analysis. We argue that these methods do not transfer well. Instead, our contrastive approach leverages existing generic knowledge about time series anomalies and injects various types of anomalies as negative samples. Therefore, CARLA not only learns normal behaviour but also learns deviations indicating anomalies. It creates similar representations for temporally close windows and distinct ones for anomalies. Additionally, it leverages the information about representations’ neighbours through a self-supervised approach to classify windows based on their nearest/furthest neighbours to further enhance the performance of anomaly detection. In extensive tests on seven major real-world TSAD datasets, CARLA shows superior performance (F1 and AU-PR) over state-of-the-art self-supervised, semi-supervised, and unsupervised TSAD methods for univariate time series and multivariate time series. Our research highlights the immense potential of contrastive representation learning in advancing the TSAD field, thus paving the way for novel applications and in-depth exploration.}
}
@article{ZHANG2025110955,
title = {ESMformer: Error-aware self-supervised transformer for multi-view 3D human pose estimation},
journal = {Pattern Recognition},
volume = {158},
pages = {110955},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110955},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007064},
author = {Lijun Zhang and Kangkang Zhou and Feng Lu and Zhenghao Li and Xiaohu Shao and Xiang-Dong Zhou and Yu Shi},
keywords = {3D human pose estimation, Multi-view fusion, Self-supervision, Transformer},
abstract = {Multi-view 3D human pose estimation (HPE) currently faces several key challenges. Information from different viewpoints exhibits high variability due to complex environmental factors, posing difficulties in cross-view feature extraction and fusion. Additionally, multi-view 3D labeled pose data is rather scarce, and the impact of input 2D poses on 3D HPE accuracy has received little attention. To address these issues, we propose an Error-aware Self-supervised transformer framework for Multi-view 3D HPE (ESMformer). Firstly, we introduce a single-view multi-level feature extraction module to enhance pose features in individual viewpoints, which incorporates a novel relative attention mechanism for representative feature extraction at different levels. Subsequently, we develop multi-view intra-level and cross-level fusion modules to exploit spatio-temporal feature dependencies among human joints, and progressively fuse pose information from all views and levels. Furthermore, we explore an error-aware self-supervised learning strategy to reduce the model’s reliance on 3D pose annotations and mitigate the impact of incorrect 2D poses. This strategy adaptively selects reliable input 2D poses based on 3D pose prediction errors. Experiments on three popular benchmarks show that ESMformer achieves state-of-the-art results and maintains cost-effective computational complexity. Notably, ESMformer does not rely on any 3D pose annotations or prior human body knowledge, making it highly versatile and adaptable in practical applications.11The code and models are available at https://github.com/z0911k/ESMformer.}
}
@article{LI2025110967,
title = {HTR-VT: Handwritten text recognition with vision transformer},
journal = {Pattern Recognition},
volume = {158},
pages = {110967},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110967},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007180},
author = {Yuting Li and Dexiong Chen and Tinglong Tang and Xi Shen},
keywords = {Handwritten text recognition, Vision transformer, Mask strategy, Sharpness-aware minimization, Data-efficient},
abstract = {We explore the application of Vision Transformer (ViT) for handwritten text recognition. The limited availability of labeled data in this domain poses challenges for achieving high performance solely relying on ViT. Previous transformer-based models required external data or extensive pre-training on large datasets to excel. To address this limitation, we introduce a data-efficient ViT method that uses only the encoder of the standard transformer. We find that incorporate a Convolutional Neural Network (CNN) for feature extraction instead of the original patch embedding and employ Sharpness-Aware Minimization (SAM) optimizer to ensure that the model can converge towards flatter minima yield notable enhancements. Furthermore, our introduction of the span mask technique, which masks interconnected features in the feature map, acts as an effective regularizer. Empirically, our approach competes favorably with traditional CNN-based models on small datasets like IAM and READ2016. Additionally, it establishes a new benchmark on the LAM dataset, currently the largest dataset with 19,830 training text lines. The code will be publicly available at: https://github.com/YutingLi0606/HTR-VT.}
}
@article{DIAO2025110942,
title = {Self-distillation enhanced adaptive pruning of convolutional neural networks},
journal = {Pattern Recognition},
volume = {157},
pages = {110942},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110942},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006939},
author = {Huabin Diao and Gongyan Li and Shaoyun Xu and Chao Kong and Wei Wang and Shuai Liu and Yuefeng He},
keywords = {Convolutional neural networks, Self-distillation, Adaptive pruning},
abstract = {Convolutional neural networks (CNNs) suffer from issues of large parameter size and high computational complexity. To address this, we propose an adaptive pruning algorithm based on self-distillation. The algorithm introduces a trainable parameter for each channel to control channel pruning and integrates the pruning process into network training, enabling pruning and fine-tuning in a single training iteration to derive the final pruned model. Moreover, this framework requires only a single overall pruning rate to achieve adaptive pruning for each layer, avoiding tedious hyperparameter tuning for a less iterative, simple and efficient pruning process. Additionally, self-distillation is utilized in the pruning algorithm, leveraging the knowledge from the pretrained CNN to guide its own pruning process, facilitating the recovery from performance degradation caused by pruning and achieving higher accuracy. Extensive pruning experiments on various CNN models over different datasets demonstrate that at least 75% of redundant parameters can be reduced without sacrificing model accuracy.}
}
@article{KE2025110889,
title = {Kernel-guided injection deep network for blind fusion of multispectral and panchromatic images},
journal = {Pattern Recognition},
volume = {157},
pages = {110889},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110889},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400640X},
author = {Chengjie Ke and Zhiyuan Zhang and Wei Zhang and Jun Chen and Xin Tian},
keywords = {Image fusion, Blind fusion, Detail injection, Deep learning},
abstract = {Deep learning (DL)-based pansharpening methods show advantages in generating a high-resolution multispectral image (MS) by fusing low-resolution MS and high-resolution panchromatic (PAN) images. However, most current research only considers a single fixed kernel in the training process, thus they may fail to handle unknown degradations by using a single model for testing, i.e., blind pansharpening. Consequently, poor performance will occur when encountering the mismatched degradation problem. Therefore, building a single network that can tackle blind pansharpening is more realistic and challenging. To this end, we design an end-to-end blind pansharpening network composed of a kernel estimation sub-network and a fusion sub-network, named BPNet. In particular, different from existing DL-based methods, we control injected details by utilizing prior degradation information produced by kernel estimation sub-network, thus it can adapt to different types of degradation. Besides, we design an effective fusion sub-network with a strong interpretability, which injects details of PAN image into MS image by a learnable injection matrix. Especially, the estimated kernel that indicates the blur level of MS image is used to update the learnable injection matrix. To preserve important features well in the deep network, channel-Transformer is further utilized as a skip-connection in the fusion sub-network. With the combination of kernel estimation and fusion sub-networks, the proposed BPNet has great advantages in complex degradation situations, as the network’s adaptability to diverse unknown degradations is enhanced. Extensive experiments on simulated and real datasets demonstrate that BPNet outperforms other state-of-the-art methods in terms of visual results and objective quality analysis.}
}
@article{ZHANG2025110902,
title = {Learning from open-set noisy labels based on multi-prototype modeling},
journal = {Pattern Recognition},
volume = {157},
pages = {110902},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110902},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006538},
author = {Yue Zhang and Yiyi Chen and Chaowei Fang and Qian Wang and Jiayi Wu and Jingmin Xin},
keywords = {Deep learning, Nosiy label, Out-of-distribution, Prototype learning},
abstract = {In this paper, we propose a novel method to address the challenge of learning deep neural network models in the presence of open-set noisy labels, which include mislabeled samples from out-of-distribution categories. Previous methods relied on the distances between sample-wise predictions and labels to identify mislabeled samples and distinguish between in-distribution (ID) and out-of-distribution (OOD) noisy samples, which struggle to promptly identify the two types of noisy samples. To overcome these limitations, we propose a novel method that utilizes feature information and cross-instance relationships, enabling a more comprehensive distinction between ID and OOD noisy samples. Our approach involves a multi-prototype modeling mechanism, where each class is represented by multiple prototypes to account for the diversity within categories. This mechanism helps in distinguishing in-distribution and out-of-distribution noisy samples by comparing sample features with class prototypes. We introduce an online algorithm for updating prototypes and enhancing model optimization with cross-augmentation consistency and a noise-robust contrastive siamese learning technique. Our extensive experiments on datasets like CIFAR100, Clothing1M, and Food101N show our method’s superiority in handling noisy labels compared to existing approaches. The code will be available at https://github.com/daisyarg/LNL-MPM.}
}
@article{ALEXIOU2025110930,
title = {Kyrtos: A methodology for automatic deep analysis of graphic charts with curves in technical documents},
journal = {Pattern Recognition},
volume = {157},
pages = {110930},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110930},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006812},
author = {Michail S. Alexiou and Nikolaos G. Bourbakis},
keywords = {Chart analysis, Chart recognition, Chart reverse engineering, Data mining in charts, Document processing and analysis},
abstract = {Deep Understanding of Technical Documents (DUTD) has become a very attractive field with great potential due to large amounts of accumulated documents and the valuable knowledge contained in them. In addition, the holistic understanding of technical documents depends on the accurate analysis of its particular modalities, such as graphics, tables, diagrams, text, etc. and their associations. In this paper, we introduce the Kyrtos methodology for the automatic recognition and analysis of charts with curves in graphics images of technical documents. The recognition processing part adopts a clustering based approach to recognize middle-points that delimit the line-segments that construct the illustrated curves. The analysis processing part parses the extracted line-segments of curves to capture behavioral features such as direction, trend and etc. These associations assist the conversion of recognized segments’ relations into attributed graphs, for the preservation of the curves’ structural characteristics. The graph relations are also are expressed into natural language (NL) text sentences, enriching the document’s text and facilitating their conversion into Stochastic Petri-net (SPN) graphs, which depict the internal functionality represented in the chart image. Extensive evaluation results demonstrate the accuracy of Kyrtos’ recognition and analysis methods by measuring the structural similarity between input chart curves and the approximations generated by Kyrtos for charts with multiple functions.}
}
@article{XUE2025110959,
title = {Multimodal self-supervised learning for remote sensing data land cover classification},
journal = {Pattern Recognition},
volume = {157},
pages = {110959},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110959},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007106},
author = {Zhixiang Xue and Guopeng Yang and Xuchu Yu and Anzhu Yu and Yinggang Guo and Bing Liu and Jianan Zhou},
keywords = {Remote sensing image, Unsupervised pre-training, Multimodal self-supervised, Feature learning},
abstract = {Deep learning has revolutionized the remote sensing image processing techniques over the past few years. Nevertheless, annotating high-quality samples is difficult and time-consuming, which limits the performance of deep neural networks because of insufficient supervision information. Aiming to solve this contradiction, we investigate the multimodal self-supervised learning (MultiSSL) paradigm for pre-training and classification of remote sensing image. Specifically, the proposed self-supervised feature learning model consists of asymmetric encoder–decoder structure, in which deep unified encoder learns high-level key information characterizing multimodal remote sensing data and task-specific lightweight decoders are developed to reconstruct original data. To further enhance feature extraction capability, the cross-attention layers are utilized to exchange information contained in heterogeneous characteristics, thus learning more complementary information from multimodal remote sensing data. In fine-tuning stage, the pre-trained encoder and cross-attention layer serve as feature extractor, and leaned characteristics are combined with corresponding spectral information for land cover classification through a lightweight classifier. The self-supervised pre-training model can learn high-level key features from unlabeled samples, thereby utilizing the feature extraction capability of deep neural networks while reducing their dependence on annotated samples. Compared with existing classification paradigms, the proposed multimodal self-supervised pre-training and fine-tuning scheme achieves superior performance for remote sensing image land cover classification.}
}
@article{LIU2025110938,
title = {Improving deep representation learning via auxiliary learnable target coding},
journal = {Pattern Recognition},
volume = {157},
pages = {110938},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110938},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006897},
author = {Kangjun Liu and Ke Chen and Kui Jia and Yaowei Wang},
keywords = {Image classification, Representation learning, Target coding},
abstract = {Deep representation learning is a subfield of machine learning that focuses on learning meaningful and useful representations of data through deep neural networks. However, existing methods for semantic classification typically employ pre-defined target codes such as the one-hot and the Hadamard codes, which can either fail or be less flexible to model inter-class correlation. In light of this, this paper introduces a novel learnable target coding as an auxiliary regularization of deep representation learning, which can not only incorporate latent dependency across classes but also impose geometric properties of target codes into representation space. Specifically, a margin-based triplet loss and a correlation consistency loss on the proposed target codes are designed to encourage more discriminative representations owing to enlarging between-class margins in representation space and favoring equal semantic correlation of learnable target codes respectively. Experimental results on several popular visual classification and retrieval benchmarks can demonstrate the effectiveness of our method on improving representation learning, especially for imbalanced data.}
}
@article{CHOI2025110914,
title = {Analyzing the latent space of GAN through local dimension estimation for disentanglement evaluation},
journal = {Pattern Recognition},
volume = {157},
pages = {110914},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110914},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006654},
author = {Jaewoong Choi and Geonho Hwang and Hyunsoo Cho and Myungjoo Kang},
keywords = {Generative Adversarial Network, Disentanglement, Semantic factorization, Dimension estimation, Grassmannian},
abstract = {The impressive success of style-based GANs (StyleGANs) in high-fidelity image synthesis has motivated research to understand the semantic properties of their latent spaces. In this paper, we approach this problem through a geometric analysis of latent spaces as a manifold. In particular, we propose a local dimension estimation algorithm for arbitrary intermediate layers in a pre-trained GAN model. The estimated local dimension is interpreted as the number of possible semantic variations from this latent variable. Moreover, this intrinsic dimension estimation enables unsupervised evaluation of disentanglement for a latent space. Our proposed metric, called Distortion, measures an inconsistency of intrinsic tangent space on the learned latent space. Distortion is purely geometric and does not require any additional attribute information. Nevertheless, Distortion shows a high correlation with the global-basis-compatibility and supervised disentanglement score. Our work is the first step towards selecting the most disentangled latent space among various latent spaces in a GAN without attribute labels.}
}
@article{YANG2024110862,
title = {SLSG: Industrial image anomaly detection with improved feature embeddings and one-class classification},
journal = {Pattern Recognition},
volume = {156},
pages = {110862},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110862},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006137},
author = {Minghui Yang and Jing Liu and Zhiwei Yang and Zhaoyang Wu},
keywords = {Anomaly detection, One-class classification, Self-supervised learning, Graph convolutional network},
abstract = {Industrial image anomaly detection under the setting of one-class classification has significant practical value. However, most existing models face challenges in extracting separable feature representations when performing feature embedding and in constructing compact descriptions of normal features when performing one-class classification. One direct consequence is that most models perform poorly in detecting logical anomalies that violate contextual relationships. Focusing on more effective and comprehensive anomaly detection, we propose a network based on self-supervised learning and self-attentive graph convolution (SLSG). SLSG uses a generative pre-training network to assist the encoder in learning the embedding of normal patterns and the reasoning of positional relationships. Subsequently, we introduce pseudo-prior knowledge of anomalies in SLSG using simulated abnormal samples. By comparing the simulated anomalies, SLSG can better summarize the normal patterns and narrow the hypersphere used for one-class classification. In addition, with the construction of a more general graph structure, SLSG comprehensively models the dense and sparse relationships among the elements in an image, which further strengthens the detection of logical anomalies. Extensive experiments on benchmark datasets show that SLSG achieves superior anomaly detection performance, demonstrating the effectiveness of our method.}
}
@article{BYUN2025110890,
title = {Improving the utility of differentially private clustering through dynamical processing},
journal = {Pattern Recognition},
volume = {157},
pages = {110890},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110890},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006411},
author = {Junyoung Byun and Yujin Choi and Jaewook Lee},
keywords = {Clustering, Differential privacy, Dynamical processing, Morse theory},
abstract = {This study aims to alleviate the trade-off between utility and privacy of differentially private clustering. Existing works focus on simple methods, which show poor performance for non-convex clusters. To fit complex cluster distributions, we propose sophisticated dynamical processing inspired by Morse theory, with which we hierarchically connect the Gaussian sub-clusters obtained through existing methods. Our theoretical results imply that the proposed dynamical processing introduces little to no additional privacy loss. Experiments show that our framework can improve the clustering performance of existing methods at the same privacy level.}
}
@article{CAO2025110971,
title = {Semantic-Constraint Matching for transformer-based weakly supervised object localization},
journal = {Pattern Recognition},
volume = {158},
pages = {110971},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110971},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007222},
author = {Yiwen Cao and Yukun Su and Wenjun Wang and Yanxia Liu and Qingyao Wu},
keywords = {Weakly-supervised object localization, Vision transformer, Image matching, Vision language model},
abstract = {Weakly supervised object localization (WSOL) strives to localize objects with only image-level supervision. WSOL often faces challenges such as incomplete localization due to classifier bias and over-localization in real scenes where objects and backgrounds are strongly associated or structurally similar. While the latest Transformer-based methods effectively enhance localization performance by leveraging long-range feature dependencies, they may inadvertently amplify divergent background activation and remain susceptible to classification bias. To this end, we proposed a novel Semantic-Constraint Matching (SeCM) plug-in module tailored for transformer-based approaches. In detail, a local patch shuffle strategy is first introduced to disentangle partial contextual linkages, thereby creating image pairs. Then a semantic matching module extracts co-object knowledge from the primal-shuffled image pairs, drives the network to identify the association of foreground with semantic label to suppress divergent activation. Moreover, to alleviate incomplete localization and prevent excessive suppression of activation, we propose leveraging multi-modal class-specific textual representations to guide object localization by complementing intra-class priori diverse knowledge. Extensive experimental results conducted on CUB-200-2011 and ILSVRC datasets show that our method can achieve the new state-of-the-art performance.}
}
@article{SU2025110898,
title = {Semantic-driven dual consistency learning for weakly supervised video anomaly detection},
journal = {Pattern Recognition},
volume = {157},
pages = {110898},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110898},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006496},
author = {Yong Su and Yuyu Tan and Simin An and Meng Xing and Zhiyong Feng},
keywords = {Video anomaly detection, Weakly-supervised, Dual consistency, Cross-modal},
abstract = {Video anomaly detection presents a significant challenge in computer vision, with the aim of distinguishing various anomaly events from numerous normal ones. Weakly supervised video anomaly detection has recently emerged as a promising solution, enabling the detection of anomaly snippets with only video-level annotations. However, knowledge about anomaly annotation remains underutilized, resulting in a gap between visual space and semantic understanding of anomalies, thus failing to capture the clear boundary between anomalies and normalities. Therefore, we propose a weakly supervised paradigm of cross-modal detection and consistency learning, leveraging dual consistency to provide discriminative representations for anomalies at both the semantic-to-target and target-to-snippet levels. Specifically, we introduce a cross-modal detection network, which detects the targets in each frame according to given semantic rules, to derive semantic-consistent visual embeddings. To depict the clear boundary between anomalies and normalities, a cross-domain alignment module is proposed to enhance the discriminative representation of abnormal targets by learning the contextual consistency between the target and snippet embeddings. Our architecture integrates the detection of semantic-consistent targets based on variable semantic rules, ensuring transferable deployment across scenarios and enabling comprehensive identification, localization, and recognition of abnormal events through a “when-where-which” pipeline. The evaluation of our approach is conducted on four widely used public benchmarks: ShanghaiTech, UCSD Ped2, CUHK Avenue, and UBnormal through extensive qualitative and quantitative analyzes. The results demonstrate the remarkable performance of our approach in dealing with the VAD task.}
}
@article{LIN2025110897,
title = {Human–object interaction detection via recycling of ground-truth annotations},
journal = {Pattern Recognition},
volume = {157},
pages = {110897},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110897},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006484},
author = {Xue Lin and Qi Zou and Xixia Xu},
keywords = {Human–object interaction, Self-supervised learning, Graph-based reasoning},
abstract = {Human–object interaction (HOI) detection is important to understand human-centric scenes, the hard core of which lies in learning the structure information from various types of relations. Existing works tackle it by introducing spatial context, extra knowledge or graph-based propagation based on the original hard labels. However, they still face challenges in dealing with action co-occurrence and complex HOIs. In this paper, we creatively propose to recycle the ground-truth annotations to get implied information for structure representations in HOIs. The action-aware closeness labeling (ACL) task is designed to capture the contextual information based on the statistic of action co-occurrence from the data source. Furthermore, we present a H-O relation graph supervision (RGS) to get more reliable relations in complicated scenes by constraining the attention weights of the H-O relation graph. Such a direct supervision on mutual relations is ignored in existing works. Experiments for HOI detection on the V-COCO and HICO-DET datasets indicate the superiority of the proposed method even without any extra knowledge.}
}
@article{QIAN2025110873,
title = {Visible–infrared person re-identification via patch-mixed cross-modality learning},
journal = {Pattern Recognition},
volume = {157},
pages = {110873},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110873},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006241},
author = {Zhihao Qian and Yutian Lin and Bo Du},
keywords = {Visible–infrared person re-identification, Patch-mix},
abstract = {Visible–infrared person re-identification (VI-ReID) aims to retrieve images of the same pedestrian from different modalities, where the challenges lie in the significant modality discrepancy. To alleviate the modality gap, recent methods generate intermediate images by GANs, grayscaling, or mixup strategies. However, these methods could introduce extra data distribution, and the semantic correspondence between the two modalities is not well learned. In this paper, we propose a Patch-Mixed Cross-Modality framework (PMCM), where two images of the same person from two modalities are split into patches and stitched into a new one for model learning. A part-alignment loss is introduced to regularize representation learning, and a patch-mixed modality learning loss is proposed to align between the modalities. In this way, the model learns to recognize a person through patches of different styles, thereby the modality semantic correspondence can be inferred. In addition, with the flexible image generation strategy, the patch-mixed images freely adjust the ratio of different modality patches, which could further alleviate the modality imbalance problem. On two VI-ReID datasets, we report new state-of-the-art performance with the proposed method.}
}
@article{LI2025110978,
title = {Trajectory-User Linking via Multi-Scale Graph Attention Network},
journal = {Pattern Recognition},
volume = {158},
pages = {110978},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110978},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007295},
author = {Yujie Li and Tao Sun and Zezhi Shao and Yiqiang Zhen and Yongjun Xu and Fei Wang},
keywords = {Trajectory-user linking, Graph neural network, Trajectory classification, Spatio-temporal data mining, Check-in data},
abstract = {Trajectory-User Linking (TUL) aims to link anonymous trajectories to their owners, which is considered an essential task in discovering human mobility patterns. Although existing TUL studies have shown promising results, they still have specific defects in the perception of spatio-temporal properties of trajectories, which manifested in the following three problems: missing context of the original trajectory, ignorance of spatial information, and high computational complexity. To address those issues, we revisit the characteristics of the trajectory and propose a novel model called TULMGAT (TUL via Multi-Scale Graph Attention Network) based on masked self-attention graph neural networks. Specifically, TULMGAT consists of four components: construction of check-in oriented graphs, node embedding, trajectory embedding, and trajectory user linking. Sufficient experiments on two publicly available datasets have shown that TULMGAT is the state-of-the-art model in task TUL compared to the baselines with an improvement of about 8% in accuracy and only a quarter of the fastest baseline in runtime. Furthermore, model validity experiments have verified the role of each module.}
}
@article{ZHANG2025110954,
title = {Defending adversarial attacks in Graph Neural Networks via tensor enhancement},
journal = {Pattern Recognition},
volume = {158},
pages = {110954},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110954},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007052},
author = {Jianfu Zhang and Yan Hong and Dawei Cheng and Liqing Zhang and Qibin Zhao},
keywords = {Graph Neural Networks, Adversarial robustness, Tensor decomposition},
abstract = {Graph Neural Networks (GNNs) have demonstrated remarkable success across diverse fields, yet remain susceptible to subtle adversarial perturbations that significantly degrade performance. Addressing this vulnerability remains a formidable challenge. Current defense strategies focus on edge-specific regularization within adversarial graphs, often overlooking the inter-edge structural dependencies and the interplay of various robustness attributes. This paper introduces a novel tensor-based framework for GNNs, aimed at reinforcing graph robustness against adversarial influences. By employing tensor approximation, our method systematically aggregates and compresses diverse predefined robustness features of adversarial graphs into a low-rank representation. This approach harmoniously combines the integrity of graph structure and robustness characteristics. Comprehensive experiments on real-world graph datasets demonstrate that our framework not only effectively counters diverse types of adversarial attacks but also surpasses existing leading defense mechanisms in performance.}
}
@article{LI2025110941,
title = {M3ixup: A multi-modal data augmentation approach for image captioning},
journal = {Pattern Recognition},
volume = {158},
pages = {110941},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110941},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006927},
author = {Yinan Li and Jiayi Ji and Xiaoshuai Sun and Yiyi Zhou and Yunpeng Luo and Rongrong Ji},
keywords = {Image captioning, Multi-modal mixup, Data augmentation, Discriminate captioning},
abstract = {Despite the great success, most models in image captioning (IC) are still stuck in the dilemma of generating simple and non-discriminative captions. In this paper, we study this problem from the perspective of data augmentation and propose a novel method called Multi-modal Mixup (M3ixup). Compared with the original Mixup strategy designed for image classification, the proposed M3ixup has three novel designs to mix IC samples from the aspects of visual features, sentence embeddings and loss values, respectively. In practice, M3ixup can not only enrich the diversity of IC training data, but also enforce the model to focus more on visual information for captioning, thereby alleviating the negative effect of dataset bias and addressing the issue of simple captioning. To validate M3ixup, we apply it to three baseline models and conduct extensive experiments on MS COCO. The experimental results demonstrate that our proposed M3ixup can not only improve the discriminability and quality of generated captions, but also help the baseline models obtain obvious performance gains, i.e., improving the CIDEr scores of the state-of-the-art model from 133.8 to 135.3 on off-line testing and 135.4 to 137.1 on online testing.}
}
@article{LI2025110925,
title = {GraphMLP: A graph MLP-like architecture for 3D human pose estimation},
journal = {Pattern Recognition},
volume = {158},
pages = {110925},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110925},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006769},
author = {Wenhao Li and Mengyuan Liu and Hong Liu and Tianyu Guo and Ti Wang and Hao Tang and Nicu Sebe},
keywords = {3D human pose estimation, Multi-layer perceptron, Graph convolutional network},
abstract = {Modern multi-layer perceptron (MLP) models have shown competitive results in learning visual representations without self-attention. However, existing MLP models are not good at capturing local details and lack prior knowledge of human body configurations, which limits their modeling power for skeletal representation learning. To address these issues, we propose a simple yet effective graph-reinforced MLP-Like architecture, named GraphMLP, that combines MLPs and graph convolutional networks (GCNs) in a global-local-graphical unified architecture for 3D human pose estimation. GraphMLP incorporates the graph structure of human bodies into an MLP model to meet the domain-specific demand of the 3D human pose, while allowing for both local and global spatial interactions. Furthermore, we propose to flexibly and efficiently extend the GraphMLP to the video domain and show that complex temporal dynamics can be effectively modeled in a simple way with negligible computational cost gains in the sequence length. To the best of our knowledge, this is the first MLP-Like architecture for 3D human pose estimation in a single frame and a video sequence. Extensive experiments show that the proposed GraphMLP achieves state-of-the-art performance on two datasets, i.e., Human3.6M and MPI-INF-3DHP. Code and models are available at https://github.com/Vegetebird/GraphMLP.}
}
@article{WEN2025110901,
title = {Characteristic discriminative prototype network with detailed interpretation for classification},
journal = {Pattern Recognition},
volume = {157},
pages = {110901},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110901},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006526},
author = {Jiajun Wen and Heng Kong and Zhihui Lai and Zhijie Zhu},
keywords = {Classification, Prototype learning, Deep learning},
abstract = {Existing prototype learning methods provide limited interpretation on which patches from input images are similar to the corresponding prototypes. Moreover, these methods do not consider the diversities among the prototypes, which leads to low classification accuracy. To address these problems, this paper proposes Characteristic Prototype Network (CDPNet) with clear interpretation of local regions and characteristic. The network designs the feature prototype to represent the discriminative feature and the characteristic prototype to characterize the prototype’s properties among different individuals. In addition, two novel strategies, dynamic region learning and similarity score minimization among similar intra-class prototypes, are designed to learn the prototypes so as to improve their diversity. Therefore, CDPNet can explain which kind of characteristic within the image is the most important one for classification tasks. The experimental results on well-known datasets show that CDPNet can provide clearer interpretations and obtain state-of-the-art classification performance in prototype learning.}
}
@article{LIU2025110948,
title = {Dynamic and static mutual fitting for action recognition},
journal = {Pattern Recognition},
volume = {157},
pages = {110948},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110948},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400699X},
author = {Wenxuan Liu and Xuemei Jia and Xian Zhong and Kui Jiang and Xiaohan Yu and Mang Ye},
keywords = {Dynamic temporal-aware erasing, Mutual fitting, Static augmentation, Pseudo-occlusion, Action recognition},
abstract = {Action recognition is intended to classify a video into a certain category by aggregating and summarizing its temporal and spatial information. Existing methods have achieved remarkable performance on the standard datasets. However, how to accurately recognize the action under occluded scenarios remains the major challenge in practical applications and is barely explored. Although previous augmentation methods have made some efforts by introducing additional modalities to deal with occlusion cases, the continuous occlusion related to temporal in the video is rarely focused. To tackle this issue, this paper takes the sample augmentation and feature representation into consideration. Specifically, we design a Dynamic Temporal-aware Erasing (DTE) augmentation strategy to enrich the occlusion samples. The proposed DTE builds upon an explicit analysis conditioned with temporal dimension along with the actors’ trajectories, which ensures the capability of carrying the temporal relation to an arbitrary spatial augmentation setting. Specifically, DTE makes the added augmentation samples more temporally consistent with obtaining the motion trajectories of the actors, enhancing the robustness against occlusions. Besides, we revisit the necessity of diverse backgrounds and propose Dynamic and Static Mutual Fitting (DSMF) to optimize the action recognition model. DSMF incorporates background auxiliary mutual fitting actors to distinguish features, which learns a smooth representation with the global temporal consistency. Extensive experiments conducted on standard benchmarks have proved that the proposed DSMF achieves competitive performance over powerful competitors.}
}
@article{BAI2025110929,
title = {Joint learning of RGBW color filter arrays and demosaicking},
journal = {Pattern Recognition},
volume = {157},
pages = {110929},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110929},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006800},
author = {Chenyan Bai and Faqi Liu and Jia Li},
keywords = {Demosaicking, Color filter array (CFA), RGBW, End-to-end},
abstract = {RGBW color filter arrays (CFAs) have gained widespread attention for their superior performance in low-light conditions. Most existing demosaicking methods are tailored for specific RGBW CFAs or involve manual joint design with CFAs. The close relationship between sampling and reconstruction means that restricting the search space through predefined CFAs and demosaicking methods severely limits the ability to achieve optimal performance. In this paper, we propose a new approach for joint learning of RGBW CFA and demosaicking. This approach can simultaneously learn optimal CFAs and demosaicking methods of any size, while also being capable of reconstructing mosaicked images of any size. We use a surrogate function and arbitrary CFA interpolation to ensure end-to-end learning of the RGBW CFA. We also propose a dual-branch fusion reconstruction network that utilizes the W channel to guide the reconstruction of R, G, and B channels, reducing color errors while preserving more image details. Extensive experiments demonstrate the superiority of our proposed method. Our code is available at: https://github.com/fql0528/learncfa.}
}
@article{YANG2025110872,
title = {Image analysis by fractional-order weighted spherical Bessel-Fourier moments},
journal = {Pattern Recognition},
volume = {157},
pages = {110872},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110872},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400623X},
author = {Tengfei Yang and Zhiquan Liu and Jingjing Guo and Yong Yu and Fang Ren and Teng Wang},
keywords = {Fractional-order weighted spherical Bessel-Fourier moments, Quaternion fractional-order weighted spherical Bessel-Fourier moments, Image reconstruction, Image recognition},
abstract = {Moment and moment invariants as effective feature descriptors have been widely applied in image analysis, pattern recognition and computer vision applications. Scholars have demonstrated that fractional-order orthogonal moments outperform their corresponding moments in representing the fine details of a given image. To this end, we propose a new fractional-order orthogonal moments based on fractional-order weighted spherical Bessel polynomial in this paper, which is named as Fractional-order weighted Spherical Bessel-Fourier Moments (FrSBFMs). Moreover, to address the issue of color image analysis, FrSBFMs are integrated with quaternion theory to develop Quaternion FrSBFMs (QFrSBFMs). In addition, the rotation invariance and parameter α analysis of FrSBFMs and QFrSBFMs are discussed in detail. The experimental results show the effectiveness of the proposed FrSBFMs and QFrSBFMs in terms of image reconstruction capability, pattern recognition accuracy and zero-watermark verification.}
}
@article{WU2025110937,
title = {Complementary phase interleaving-based fringe order recognition for temporal phase unwrapping},
journal = {Pattern Recognition},
volume = {157},
pages = {110937},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110937},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006885},
author = {Haitao Wu and Yiping Cao},
keywords = {Complementary phase interleaving, Fringe order recognition, Temporal phase unwrapping, Phase error suppression, Back-calculation},
abstract = {This paper introduces a complementary phase interleaving-based fringe order recognition method, designed to rectify phase errors encountered in temporal phase unwrapping. Addressing challenges arising from noise interference or misalignment between fringe order and the wrapped phase, this method employs an innovative integration of a smooth-wrapped phase and the interleaved complementary phases with precise π phase shifts. Furthermore, it maximizes measurement accuracy and prevents filtering-induced degradation by back-calculating the fringe order of noise-disturbed phases from the smooth-continuous phase. Its remarkable adaptability across various temporal phase unwrapping methods, without necessitating additional structural patterns, underscores its versatility in addressing a broad spectrum of phase error suppression challenges. Experimental validation substantiates the accuracy and adaptability of this method, demonstrating its efficacy in handling diverse types of phase errors within TPU methods. This advancement stands as a pivotal solution for achieving precise and comprehensive phase error suppression, particularly for multiple isolated objects.}
}
@article{LIU2025110885,
title = {A three-dimensional feature-based fusion strategy for infrared and visible image fusion},
journal = {Pattern Recognition},
volume = {157},
pages = {110885},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110885},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006368},
author = {Xiaowen Liu and Hongtao Huo and Xin Yang and Jing Li},
keywords = {Image fusion, Contrastive learning, Convolution neural network},
abstract = {Due to the lacking of attention to the scene’s essential characteristics, the existing fusion methods suffer from the deficiency of scene distortion. In addition, the lack of groundtruth can cause an inadequate representation of vital information. To this end, we propose a novel infrared and visible image fusion network based on three-dimensional feature fusion strategy (D3Fuse). In our method, we consider the scene semantic information in the source images and extract the commonality contents of the two images as the third-dimensional feature to extend the feature space for fusion tasks. Specifically, a commonality feature extraction module (CFEM) is designed to extract the scene commonality features. Subsequently, the scene commonality features are utilized together with modality features to construct the fusion image. Moreover, to ensure the independence and diversity of distinct features, we employ a contrastive learning strategy with multiscale PCA coding, which stretches the feature distance in an unsupervised manner, prompting the encoder to extract more discriminative information without incurring additional parameters and computational costs. Furthermore, a contrastive enhancement strategy is utilized to ensure adequate representation of modality information. The results of the qualitative and quantitative evaluations on the three datasets show that the proposed method has better visual performance and higher objective metrics with lower computational cost. The object detection experiments show that our results have superior performance on high-level semantic tasks.}
}
@article{QIANG2025110913,
title = {Split-and-merge model selection of mixtures of Gaussian processes with RJMCMC},
journal = {Pattern Recognition},
volume = {157},
pages = {110913},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110913},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324006642},
author = {Zhe Qiang and Jinwen Ma and Di Wu},
keywords = {Mixture of Gaussian processes, Model selection, Curve clustering, Penalized prior RJMCMC, Full Bayesian inference},
abstract = {The mixture of Gaussian processes is a powerful statistical learning model that can be effectively applied to curve clustering and prediction. However, the corresponding model selection problem, that is, selecting an appropriate number of components in the mixture, is rather difficult to solve. In our previous work, we established the split-and-merge automatic model selection algorithm for mixtures of Gaussian processes along the output space under the framework of Reversible Jump Markov Chain Monte Carlo (RJMCMC), which can not only determine the number of actual Gaussian processes but also dynamically adjust the Gaussian process components to avoid dependence on parameter initialization and initial partitioning of the dataset during the parameter learning on a given dataset. In this study, we propose two algorithms: Penalized Likelihood RJMCMC and Penalized Prior RJMCMC. The former integrates a penalized term into the likelihood, while the latter incorporates a penalized term into the prior and operates within the full Bayesian inference framework, both aiming to focus more sharply on determining the number of components in the convergence process. Furthermore, we prove the geometric ergodicity of the RJMCMC algorithm for the mixture of Gaussian processes model, ensuring convergence of the posterior distribution with sufficient iterations. The experimental results further demonstrate the robustness of our PP-RJMCMC algorithm in model selection, showing superior performance compared to traditional approaches in curve classification and clustering. Additionally, the prediction performance is comparable to the EM algorithm. Although not directly explored in this study, the RJMCMC results can be used to initialize the EM algorithm, which could potentially improve prediction accuracy and accelerate computation.}
}
@article{SALAMAH2025110966,
title = {A coded knowledge distillation framework for image classification based on adaptive JPEG encoding},
journal = {Pattern Recognition},
volume = {158},
pages = {110966},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110966},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007179},
author = {Ahmed H. Salamah and Shayan Mohajer Hamidi and En-Hui Yang},
abstract = {In knowledge distillation (KD), a lightweight student model yields enhanced test accuracy by mimicking the behavior of a pre-trained large model (teacher). However, the cumbersome teacher model often makes over-confident responses, resulting in poor generalization when presented with unseen data. Consequently, a student trained by such a teacher also inherits this problem. To mitigate this issue, in this paper, we present a new framework of KD dubbed coded knowledge distillation (CKD) in which the student is trained to mimic instead the behavior of a coded teacher. Compared to the teacher in KD, the coded teacher in CKD has an additional adaptive encoding layer in the front, which adaptively encodes an input image into a compressed version (using JPEG encoding for instance) and then feeds the compressed input image to the pre-trained teacher. Comprehensive experimental results show the effectiveness of CKD over KD. In addition, we extend the deployment of a coded teacher to other knowledge transfer methods, showcasing its ability to enhance test accuracy across these methods.}
}