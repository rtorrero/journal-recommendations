@article{YANG2023109526,
title = {Triple-attention interaction network for breast tumor classification based on multi-modality images},
journal = {Pattern Recognition},
volume = {139},
pages = {109526},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109526},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002261},
author = {Xiao Yang and Xiaoming Xi and Kesong Wang and Liangyun Sun and Lingzhao Meng and Xiushan Nie and Lishan Qiao and Yilong Yin},
keywords = {Breast tumor classification, Multi-modality fusion, Triple inter-modality interaction, Intra-modality interaction},
abstract = {Breast cancer can be diagnosed using medical imaging. Classification performance of medical imaging can be improved by multi-modality image fusion. However, existing fusion algorithm fail to consider the importance of modality interactions and cannot fully utilize multi-modality information. Attention mechanisms can effectively explore and combine multi-modality information. Thus, we propose a novel triple-attention interaction network for breast tumor classification based on diffusion-weighted imaging (DWI) and apparent dispersion coefficient (ADC) images. A triple inter-modality interaction mechanism is proposed to fully fuse the multi-modality information. Three modal interactions were performed through the developed inter-modality relation module, channel interaction module, and multi-level attention fusion module to explore the correlation, complementary, and discriminative information, respectively. Additionally, we introduce a novel dual parallel-attention module for the incorporation of spatial and channel attention to improve the discriminative ability of single-modality features. Using these mechanisms, the proposed algorithm can mine and explore useful multi-modality information fully, to improve classification performance. Experimental results demonstrate that our algorithm outperforms other multi-modality fusion algorithm, and extensive ablation studies were conducted to verify the advantages of our algorithm. The area under the receiver operating characteristic curve, accuracy, specificity, and sensitivity were 90.5%, 89.0%, 85.6%, and 92.4%, respectively.}
}
@article{WEN2023109606,
title = {Multi-level progressive transfer learning for cervical cancer dose prediction},
journal = {Pattern Recognition},
volume = {141},
pages = {109606},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109606},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003072},
author = {Lu Wen and Jianghong Xiao and Jie Zeng and Chen Zu and Xi Wu and Jiliu Zhou and Xingchen Peng and Yan Wang},
keywords = {Radiation therapy, Dose prediction, Transfer learning, Deep neural network},
abstract = {Recently, deep learning has accomplished the automation of radiation therapy planning, enhancing its quality and efficiency. However, such progress comes at the cost of a large amount of clinical data. For some low-incidence cancers, i.e., cervical cancer, with limited available data, current data-hungry deep models fail to achieve satisfactory performance. To address this, in this paper, considering that cervical cancer and rectum cancer share the same scanning area and organs at risk (OARs), we resort to transfer learning to transfer the knowledge acquired from rectum cancer (source domain) to cervical cancer (target domain) to perform dose map prediction task. To overcome the possible negative transferring problem, we design a two-phase paradigm to progressively transfer knowledge. In the first phase, we aggregate the data of the two domains by linear interpolation and pre-train an aggregated network with the aggregated data to perceive the target dose distribution beforehand. In the second phase, we elaborately design two modules, i.e., a Feature-level Transfer (FT) Module, and an Image-level Transfer (IT) Module, to selectively transfer knowledge in multi-level. Specifically, the FT module aims to preserve those filters that are more helpful while the IT module tries to highlight those samples with more target-specific knowledge. Extensive experiments proclaim the exemplary performance of our proposed method compared with other state-of-the-art methods.}
}
@article{ZHANG2023109525,
title = {Crowdmeta: Crowdsourcing truth inference with meta-Knowledge transfer},
journal = {Pattern Recognition},
volume = {140},
pages = {109525},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109525},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300225X},
author = {Jing Zhang and Sunyue Xu and Victor S. Sheng},
keywords = {Crowdsourcing, Truth inference, Transfer learning, Meta learning},
abstract = {Crowdsourcing provides a fast and low-cost solution to collect annotations for training data in computer vision. However, there are two challenges in crowdsourced image annotation: First, when crowdsourced workers perform annotation tasks in an unfamiliar domain, their accuracy will dramatically decline due to the lack of expertise; Second, the difficulties of tasks may be different due to the noises in images, which is only related to the features of images themselves and will affect the judgment of workers. It is well known that transferring knowledge from relevant domains can form a better representation for training samples, which benefits the estimation of workers’ expertise in truth inference models. However, the existing knowledge transfer processes for crowdsourcing require a considerable number of well-collected samples in source domains. Comprehensively considering the above issues, this paper proposes a novel probabilistic model for crowdsourcing truth inference, which fuses few-shot meta-learning and transfer learning. The proposed model transfers meta-knowledge from the source domain to form better high-level representations of the instances in the target domain. Simultaneously utilizing both high-level representations and instance features, the quality of workers and the difficulty of instances can be better modeled and inferred. Experimental results on a number of datasets show that the proposed model not only outperforms the state-of-the-art models but also significantly reduces the number of instances required in the source domain.}
}
@article{MO2023109485,
title = {Scatter matrix decomposition for jointly sparse learning},
journal = {Pattern Recognition},
volume = {140},
pages = {109485},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109485},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001851},
author = {Dongmei Mo and Zhihui Lai and Jie Zhou and Hu Qinghua},
keywords = {Feature extraction, Pattern recognition, Classification, Linear discriminant analysis, Joint sparsity},
abstract = {Orthogonal Linear Discriminant Analysis (OLDA) based on generalized Eigen-equation is widely used in the field of computer vision and pattern recognition. However, the performance of OLDA for feature extraction and classification needs to be improved as it lacks sparsity for better interpretation of the features. Moreover, computing the orthogonal sparse projections based on LDA is very difficult and is still unsolved. To solve these problems, in this paper, we propose a method called Jointly Sparse Orthogonal Linear Discriminant Analysis (JSOLDA). Different from the existing OLDA, JSOLDA is proposed from a novel viewpoint of scatter matrix decomposition. Theoretical analysis shows that OLDA can be derived by the constrained scatter matrix decomposition. In addition, by imposing L2,1-norm on the penalty term, the proposed JSOLDA can obtain the jointly sparse orthogonal projections to perform feature extraction. We also design an iterative algorithm to obtain the optimal solution. Systematic theoretical analysis between the OLDA and JSOLDA are uncovered. Both of convergence and computational complexity are also discussed. Experimental results on four data sets (i.e., COIL100, USPS, ICADAR2003 and CMU PIE) indicate that JSOLDA outperforms several well-known LDA-based and L2,1-norm based methods.}
}
@article{NANFACK2023109610,
title = {Learning Customised Decision Trees for Domain-knowledge Constraints},
journal = {Pattern Recognition},
volume = {142},
pages = {109610},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109610},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003114},
author = {Géraldin Nanfack and Paul Temple and Benoît Frénay},
keywords = {Decision trees, Constraints, Domain knowledge},
abstract = {When applied to critical domains, machine learning models usually need to comply with prior knowledge and domain-specific requirements. For example, one may require that a learned decision tree model should be of limited size and fair, so as to be easily interpretable, trusted, and adopted. However, most state-of-the-art models, even on decision trees, only aim to maximising expected accuracy. In this paper, we propose a framework in which a diverse family of prior and domain knowledge can be formalised and imposed as constraints on decision trees. This framework is built upon a newly introduced tree representation that leads to two generic linear programming formulations of the optimal decision tree problem. The first one targets binary features, while the second one handles continuous features without the need for discretisation. We theoretically show how a diverse family of constraints can be formalised in our framework. We validate the framework with constraints on several applications and perform extensive experiments, demonstrating empirical evidence of comparable performance w.r.t. state-of-the-art tree learners.}
}
@article{XUE2023109538,
title = {Joint optimization for attention-based generation and recognition of chinese characters using tree position embedding},
journal = {Pattern Recognition},
volume = {140},
pages = {109538},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109538},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002388},
author = {Mobai Xue and Jun Du and Bin Wang and Bo Ren and Yu Hu},
keywords = {Chinese character generation and recognition, Radical analysis, Joint optimization, Tree position embedding},
abstract = {Despite the growing interest in Chinese character generation, creating a nonexistent character remains an open challenge. Radical-based Chinese character generation is still a novel task while radical-based Chinese character recognition is more technologically advanced. To fully utilize the knowledge of recognition task, we first propose an attention-based generator. The generator chooses the most relevant radical to generate each zone with an attention mechanism. Then, we present a joint optimization approach to training generation-recognition models, which can help the generator and recognizer learn from each other effectively. The joint optimization is implemented via contrastive learning and dual learning. Considering the symmetry of the generation and recognition, contrastive learning aims to strengthen the performance of the encoder of recognizer and the decoder of generator. Since the generation and recognition tasks can form a closed loop, dual learning feeds the output from one to another as input. Based on the feedback signals generated during the two tasks, we can iteratively update the two models until convergence. Finally, as our model ignores the order information of a sequence, we exploit position embedding to extend the image representation ability and propose tree position embedding to represent the positional information for tree structure captions of Chinese characters. The experimental results in printed and nature scenes show that the proposed method improves the quality of the generating images and increases the recognition accuracy for Chinese characters.}
}
@article{LIU2023109566,
title = {A novel relation aware wrapper method for feature selection},
journal = {Pattern Recognition},
volume = {140},
pages = {109566},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109566},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002662},
author = {Zhaogeng Liu and Jielong Yang and Li Wang and Yi Chang},
keywords = {Feature selection, Sample relation, Feature relation, Classification},
abstract = {Feature selection, aiming at eliminating irrelevant and redundant features, is an important data preprocessing technology for downstream tasks, e.g., classification. With the explosive growth of data in various fields, some data are high-dimensional and contain critical and complex hidden relationships, which brings new challenges to feature selection: i) How to find out the underlying available relationships from the data, and ii) how to use the learned relations to better select features? To deal with these challenges, we propose a novel wrapper feature selection method named Relation Aware Feature Selection Method (ERASE), which can learn and use the underlying sample relations and feature relations for feature selection. Different from existing methods, our method jointly learns sample relationships and feature relationships through a graph of samples and trees of features. Furthermore, it uses the relations to select the optimal feature subset according to the new proposed Relation-based Sequence Floating Selection Strategy. Extensive experimental results on nine datasets from different domains demonstrate that our method achieves the best performance in most cases compared with other feature selection methods, including state-of-the-art wrapper methods.}
}
@article{LIU2023109514,
title = {Bi-RRNet: Bi-level recurrent refinement network for camouflaged object detection},
journal = {Pattern Recognition},
volume = {139},
pages = {109514},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109514},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002145},
author = {Yan Liu and Kaihua Zhang and Yaqian Zhao and Hu Chen and Qingshan Liu},
keywords = {Camouflaged object detection, Convolutional neural networks, Recurrent refinement network, Dense prediction},
abstract = {In this paper, we present a lightweight Bi-level Recurrent Refinement Network (Bi-RRNet) for Camouflaged Object Detection (COD) that consists of a Lower-level RRNet (L-RRN) and an Up-level RRNet (U-RRN) to progressively refine the multi-level context features for precise dense prediction. In particular, the L-RRN recursively refines the deeper layer high-level semantic features with the high-resolution low-level features from the earlier layers in a top-down manner, and the U-RRN progressively polishes the refined features from the L-RRN in a recurrent manner, producing the high-resolution semantic features that are essential to accurate COD. Moreover, we develop a Multi-scale Scene Perception Module (MSPM) that, in order to deal with target appearance variation, first compresses the global scene context information at each layer into a learnable weight vector and then modulates the multi-scale context features produced by a filter bank with various local receptive fields using the learned weights. Meanwhile, we design a Region-Consistency Enhancement Module (RCEM) that makes use of high-level semantic features to direct filtering out the cluttered information in the lower-layer features. This module can highlight the regions of camouflaged objects, maximizing the inter-class contrast between the objects and their surroundings. Extensive experiments on four challenging benchmark datasets, including CHAMELEON, CAMO, COD10K, and NC4K, show that our Bi-RRNet outperforms a variety of state-of-the-art methods in terms of accuracy and model parameters. Our Bi-RRNet, in particular, is lightweight, with 14.95M parameters that are only half the size of the state-of-the-art BSA-Net.}
}
@article{LIU2023109593,
title = {FontTransformer: Few-shot high-resolution Chinese glyph image synthesis via stacked transformers},
journal = {Pattern Recognition},
volume = {141},
pages = {109593},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109593},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002947},
author = {Yitian Liu and Zhouhui Lian},
keywords = {Font generation, Style transfer, Transformers},
abstract = {Automatic generation of high-quality Chinese fonts from a few online training samples is a challenging task, especially when the amount of samples is very small. Existing few-shot font generation methods can only synthesize low-resolution glyph images that often possess incorrect topological structures or/and incomplete strokes. To address the problem, this paper proposes FontTransformer, a novel few-shot learning model, for high-resolution Chinese glyph image synthesis by using stacked Transformers. The key idea is to apply the parallel Transformer to avoid the accumulation of prediction errors and utilize the serial Transformer to enhance the quality of synthesized strokes. Meanwhile, we also design a novel encoding scheme to feed more glyph information and prior knowledge to our model, which further enables the generation of high-resolution and visually-pleasing glyph images. Both qualitative and quantitative experimental results demonstrate the superiority of our method compared to other existing approaches in few-shot Chinese font synthesis task.}
}
@article{ZHU2023109578,
title = {Topic-aware video summarization using multimodal transformer},
journal = {Pattern Recognition},
volume = {140},
pages = {109578},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109578},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002789},
author = {Yubo Zhu and Wentian Zhao and Rui Hua and Xinxiao Wu},
keywords = {Topic-aware video summarization, Multimodal transformer, Video summarization dataset},
abstract = {Video summarization aims to generate a short and compact summary to represent the original video. Existing methods mainly focus on how to extract a general objective synopsis that precisely summaries the video content. However, in real scenarios, a video usually contains rich content with multiple topics and people may cast diverse interests on the visual contents even for the same video. In this paper, we propose a novel topic-aware video summarization task that generates multiple video summaries with different topics. To support the study of this new task, we first build a video benchmark dataset by collecting videos from various types of movies and annotate them with topic labels and frame-level importance scores. Then we propose a multimodal Transformer model for the topic-aware video summarization, which simultaneously predicts topic labels and generates topic-related summaries by adaptively fusing multimodal features extracted from the video. Experimental results show the effectiveness of our method.}
}
@article{ZHANG2023109554,
title = {Rigorous non-disjoint discretization for naive Bayes},
journal = {Pattern Recognition},
volume = {140},
pages = {109554},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109554},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002546},
author = {Huan Zhang and Liangxiao Jiang and Geoffrey I. Webb},
keywords = {Naive Bayes, Singleton interval, Proportional weighting, Discretization},
abstract = {Naive Bayes is a classical machine learning algorithm for which discretization is commonly used to transform quantitative attributes into qualitative attributes. Of numerous discretization methods, Non-Disjoint Discretization (NDD) proposes a novel perspective by forming overlapping intervals and always locating a value toward the middle of an interval. However, existing approaches to NDD fail to adequately consider the effect of multiple occurrences of a single value — a commonly occurring circumstance in practice. By necessity, all occurrences of a single value fall within the same interval. As a result, it is often not possible to discretize an attribute into intervals containing equal numbers of training instances. Current methods address this issue in an ad hoc manner, reducing the specificity of the resulting atomic intervals. In this study, we propose a non-disjoint discretization method for NB, called Rigorous Non-Disjoint Discretization (RNDD), that handles multiple occurrences of a single value in a systematic manner. Our extensive experimental results suggest that RNDD significantly outperforms NDD along with all other existing state-of-the-art competitors.}
}
@article{LIU2023109530,
title = {Cloud-VAE: Variational autoencoder with concepts embedded},
journal = {Pattern Recognition},
volume = {140},
pages = {109530},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109530},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002303},
author = {Yue Liu and Zitu Liu and Shuang Li and Zhenyao Yu and Yike Guo and Qun Liu and Guoyin Wang},
keywords = {Variational autoencoder, Disentangled representation, Concept embedded, Cloud Model, Deep Learning Interpretability},
abstract = {Variational Autoencoder (VAE) has been widely and successfully used in learning coherent latent representation of data. However, the lack of interpretability in the latent space constructed by the VAE under the prior distribution is still an urgent problem. This paper proposes a VAE with understandable concept embedding named Cloud-VAE, which constructs interpretable latent space by disentangling the latent variables and considering their uncertainty based on cloud model. Firstly, cloud model-based clustering algorithm cast initial constraint of latent space into a prior distribution of concept which can be embedded into the latent space of the VAE to disentangle the latent variables. Secondly, reparameterization trick based on forward cloud transformation algorithm is designed to estimate the latent space concept by increasing the randomness of latent variables. Furthermore, variational lower bound of Cloud-VAE is derived to guide the training process to construct concepts of latent space, realizing the mutual mapping between latent space and concept space. Finally, experimental results on 6 benchmark datasets show that Cloud-VAE has good clustering and reconstruction performance, which can explicitly explain the aggregation process of the model and discover more interpretable disentangled representations.}
}
@article{WEI2023109503,
title = {A Composite Network Model for Face Super-Resolution with Multi-Order Head Attention Facial Priors},
journal = {Pattern Recognition},
volume = {139},
pages = {109503},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109503},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002030},
author = {Feng Wei and Song Wang and Jucheng Yang and Xiao Sun and Yuan Wang and Yarui Chen},
keywords = {Face super-resolution, FSR, Multi-order head attention, Facial components, Prior information, Transformer},
abstract = {Face super-resolution (FSR) aims to reconstruct high-resolution face images from low-resolution (LR) ones. Despite the progress made by deep convolutional neural networks (DCNNs) on FSR, convolutions struggle to relate spatially distant concepts and what is more, all image pixels and prior information (e.g., landmarks and facial component heatmaps) are treated equally regardless of importance, causing inaccuracy and decreasing the quality of face image recovery. To address these issues, in this paper we propose a composite network model for FSR with multi-order head attention facial priors. The proposed model contains a face hallucination transformer (FHT)-based network and a multi-order head attention (MOHA)-based DCNN. The FHT-based network can capture long-range dependencies and gradually increase resolution to achieve efficient and effective inference, while the MOHA-based DCNN exploits detailed and two-dimensional information of LR face images. Moreover, the novel generic submodule of the MOHA-based DCNN, namely Multi-Order Head Attention Network, can accurately model the relationship of facial components between spatial and channel dimensions. The proposed composite network model seamlessly integrates the advantages of DCNNs and transformers to super-resolve LR face images. When compared with state-of-the-art FSR methods on public benchmark datasets, the proposed model shows competitive recovery performance.}
}
@article{SOUTOARIAS2023109607,
title = {AIDA: Analytic isolation and distance-based anomaly detection algorithm},
journal = {Pattern Recognition},
volume = {141},
pages = {109607},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109607},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003084},
author = {Luis Antonio {Souto Arias} and Cornelis W. Oosterlee and Pasquale Cirillo},
keywords = {Outlier detection, Anomaly explanation, Isolation, Distance, Ensemble methods},
abstract = {Many unsupervised anomaly detection algorithms rely on the concept of nearest neighbours to compute the anomaly scores. Such algorithms are popular because there are no assumptions about the data, making them a robust choice for unstructured datasets. However, the number (k) of nearest neighbours, which critically affects the model performance, cannot be tuned in an unsupervised setting. Hence, we propose the new and parameter-free Analytic Isolation and Distance-based Anomaly (AIDA) detection algorithm, that combines the metrics of distance with isolation. Based on AIDA, we also introduce the Tempered Isolation-based eXplanation (TIX) algorithm, which identifies the most relevant features characterizing an outlier, even in large multi-dimensional datasets, improving the overall explainability of the detection mechanism. Both AIDA and TIX are thoroughly tested and compared with state-of-the-art alternatives, proving to be useful additions to the existing set of tools in anomaly detection.}
}
@article{ZHANG2023109611,
title = {Adaptive fusion affinity graph with noise-free online low-rank representation for natural image segmentation},
journal = {Pattern Recognition},
volume = {141},
pages = {109611},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109611},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003126},
author = {Yang Zhang and Moyun Liu and Huiming Zhang and Guodong Sun and Jingwu He},
keywords = {Graph, Image segmentation, Sparse subspace clustering, Low-rank representation, Affinity propagation},
abstract = {Affinity graph-based segmentation methods have become a major trend in computer vision. The performance of these methods rely on the constructed affinity graph, with particular emphasis on the neighborhood topology and pairwise affinities among superpixels. However, these graph-based methods ignore the noisy data from images, that influence the accuracy of pairwise similarities. Multiscale combinatorial grouping and graph fusion also generate a higher computational complexity. In this paper, we propose an adaptive fusion affinity graph with noise-free low-rank representation in an online manner for natural image segmentation. An input image is first over-segmented into superpixels at different scales and then filtered by an improved kernel density estimation method. Moreover, we select global nodes of these superpixels on the basis of their subspace-preserving presentation, which reveals the feature distribution of superpixels exactly. To reduce time complexity while improving performance, a sparse representation of global nodes based on noise-free online low-rank representation is used to obtain a global graph at each scale. Experimental results on BSD300, BSD500, MSRC, SBD, and PASCAL VOC show the effectiveness of our method in comparison with the state-of-the-art approaches. The code is available at https://github.com/Yangzhangcst/AFA-graph.}
}
@article{THAKARE2023109567,
title = {RareAnom: A Benchmark Video Dataset for Rare Type Anomalies},
journal = {Pattern Recognition},
volume = {140},
pages = {109567},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109567},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002674},
author = {Kamalakar Vijay Thakare and Debi Prosad Dogra and Heeseung Choi and Haksub Kim and Ig-Jae Kim},
keywords = {Video anomaly detection, Unsupervised learning, Temporal encoding, Rare anomalies, Anomaly classification},
abstract = {Existing video anomaly detection methods and datasets suffer from restricted anomaly categories containing single-source (CCTV) videos recorded in controlled environment, inadequate annotations, and lack of adequate supervision. To mitigate these problems, we introduce a new dataset (RareAnom) containing 17 rare types of real-world anomalies (2200 videos) recorded using multiple sources (e.g., CCTV, handheld cameras, dash-cams, and mobile phones) with rich temporal annotations. A new fully unsupervised anomaly detection and classification method has been proposed. It has three stages: training of a 3D Convolution Autoencoder using pseudo-labelled video segments, anomaly detection using latent features, and classification. Unlike the existing datasets, we have benchmarked RareAnom using three levels of supervision: fully, weakly, and unsupervised. It has been compared with UCF-Crime and XD-Violence datasets. The proposed anomaly detection and classification method beats the latest unsupervised methods by 4.49%, 8.66%, and 6.77% on RareAnom, UCF-Crime, and XD-violence datasets, respectively.}
}
@article{CHANG2023109515,
title = {Elaborate multi-task subspace learning with discrete group constraint},
journal = {Pattern Recognition},
volume = {139},
pages = {109515},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109515},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002157},
author = {Wei Chang and Feiping Nie and Rong Wang and Xuelong Li},
keywords = {Multi-task learning, Negative transfer, Subspace learning, Re-weighted method},
abstract = {In multi-task learning (MTL), multiple related tasks can be learned simultaneously under the shared information to improve the generalization performance. However, most of MTL methods assume that all the learning tasks are related indeed and appropriate for joint learning. In some real situations, this assumption may not hold and further lead to the problem of negative transfer. Therefore, in this paper, we not only focus on researching the problem of robustly learning the common feature structure shared by tasks, but also discriminate with which tasks one task should share. By combining with the idea of subspace learning, we propose an elaborate multi-task subspace learning model (EMTSL) with discrete group structure constraint, which can cluster the learned tasks into a set of groups. By introducing the Schatten p-norm instead of trace norm, our model EMTSL can better approximate the low-rank constraint and also avoid the trivial solution. Furthermore, we design an efficient algorithm based on the re-weighted method to solve the proposed model. In addition, the convergence analysis of our algorithm is given in this paper. Experimental results on both synthetic and real-world datasets demonstrate the superiority of our method.}
}
@article{ZHU2023109543,
title = {Learning relation-based features for fine-grained image retrieval},
journal = {Pattern Recognition},
volume = {140},
pages = {109543},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109543},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002431},
author = {Yingying Zhu and Gang Cao and Zhanyuan Yang and Xiufan Lu},
keywords = {Fine-grained image retrieval, Implicit relation, Feature aggregation},
abstract = {Fine-Grained Image Retrieval (FGIR) is a fundamental yet challenging task that has recently received considerable attention. However, two critical issues remain unresolved. On the one hand, convolutional neural networks (CNNs) trained with image-level labels tend to focus on the most discriminative image patches but overlook the implicit relation among them. On the other hand, existing large models developed for FGIR are computationally expensive and difficult to learn discriminative features. To address these issues without additional object-level annotations or localization sub-networks, we propose a novel unified framework for fine-grained image retrieval. Specifically, a novel Relation-based Convolutional Descriptor Aggregation (RCDA) method for extracting subtle yet discriminative features from fine-grained images is introduced. The RCDA method consists of a local feature generation network and a relation extraction (RE) module that models both explicit information and implicit relations. The explicit information is modeled by computing feature similarities, while the implicit relation is mined via an expectation-maximization algorithm. Moreover, we further leverage the knowledge distillation technique to optimize the parameters of the feature generation network and speed up the fine-tuning procedure by transferring knowledge from a large model to a smaller model. Experimental results on three benchmark datasets (CUB-200-2011, Stanford-Car and FGVC-Aircraft) demonstrate that the proposed method not only achieves a significant improvement over baseline models but also outperforms state-of-the-art methods by a large margin (6.4%, 1.3%, 23.2%, respectively).}
}
@article{ZHAO2023109579,
title = {RA-YOLOX: Re-parameterization align decoupled head and novel label assignment scheme based on YOLOX},
journal = {Pattern Recognition},
volume = {140},
pages = {109579},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109579},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002790},
author = {Zuopeng Zhao and Chen He and Guangming Zhao and Jie Zhou and Kai Hao},
keywords = {Object detection, YOLO series, Decoupled head, Label assignment},
abstract = {YOLOX is a state-of-the-art one-stage object detection model for real-time applications that employs a decoupled head and advanced label assignment. Despite its impressive performance, YOLOX has limitations that prevent it from achieving optimal accuracy in real-time settings. To improve these limitations, we propose a new approach called re-parameterization align YOLOX (RA-YOLOX). Our approach employs a novel re-parameterization align decoupled head to align the classification and regression tasks, enhancing the learning of connection information between classification and regression. In addition, we propose a novel label assignment(LA) scheme that effectively defines positive and negative samples and precisely designs loss weight function. Our LA scheme enables the detector to focus on high-quality positive samples and filter out low-quality positive samples during training. We provide three sizes of lite models, namely RA-YOLOX-s, RA-YOLOX-tiny, and RA-YOLOX-nano, all of which outperform YOLOX models of similar size by an average precision of 2.3%, 1.5%, and 1.7%, respectively, on the MS COCO-2017 validation set, demonstrating the efficacy of our approach. Our code is available at github.com/hcmyhc/RA-YOLOX.}
}
@article{CHEN2023109527,
title = {Towards Automatic Model Compression via a Unified Two-Stage Framework},
journal = {Pattern Recognition},
volume = {140},
pages = {109527},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109527},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002273},
author = {Weihan Chen and Peisong Wang and Jian Cheng},
keywords = {Deep neural networks, Model compression, Quantization, Pruning},
abstract = {Deep Neural Networks have become ubiquitous in various domains. Meanwhile, the problems of massive storage and computation costs have hindered the deployment of these models to real-world applications. This paper proposes a novel and unified two-stage framework for automatic model compression. To determine the compression ratio of each layer, we improve the optimization from two aspects. First, to predict the performance of each compression policy, we propose Dynamic BN, which improves the correlation significantly with little computation overhead. Second, to search for the compression ratio allocation, we propose an efficient and hyperparameter-free solving algorithm based on the proposed Hessian matrix approximation and Knapsack problem reformulation. Moreover, comprehensive experiments and analyses are conducted on the CIFAR-100&ImageNet datasets and various network architectures to demonstrate its performance advantages over existing model compression methods under the quantization-only, pruning-only, and pruning-quantization settings.}
}
@article{PAN2023109594,
title = {Few-shot classification with task-adaptive semantic feature learning},
journal = {Pattern Recognition},
volume = {141},
pages = {109594},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109594},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002959},
author = {Mei-Hong Pan and Hong-Yi Xin and Chun-Qiu Xia and Hong-Bin Shen},
keywords = {Few-shot learning, Multi-modality, Task-adaptive training, Semantic feature learner},
abstract = {Few-shot classification aims to learn a classifier that categorizes objects of unseen classes with limited samples. One general approach is to mine as much information as possible from limited samples. This can be achieved by incorporating data from multiple modalities. However, existing multi-modality methods only use additional modality in support samples while adhering to a single modal in query samples. Such approach could lead to information imbalance between support and query samples, which confounds model generalization from support to query samples. Towards this problem, we propose a task-adaptive semantic feature learning mechanism to incorporate semantic features for both support and query samples. The semantic feature learner is trained episodic-wisely by regressing from the feature vectors of the support samples. It is utilized to predict semantic features for the query samples. Such method maintains a consistent training scheme between support and query samples and enables direct model transfer from support to query data, which significantly improves model generalization. We conduct extensive experiments on four benchmarks in both inductive and transductive settings. Results show that the proposed TasNet outperforms state-of-the-art methods with an improvement of 1% to 5% in classification accuracy, demonstrating the superiority of our method. The exhaustive ablation studies further validate the effectiveness of our framework. The code is available at: https://github.com/pmhDL/TasNet}
}
@article{CAO2023109542,
title = {GraphRevisedIE: Multimodal information extraction with graph-revised network},
journal = {Pattern Recognition},
volume = {140},
pages = {109542},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109542},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300242X},
author = {Panfeng Cao and Jian Wu},
keywords = {Document information extraction, Graph convolutional network, Transformer},
abstract = {Key information extraction (KIE) from visually rich documents (VRD) has been a challenging task in document intelligence because of not only the complicated and diverse layouts of VRD that make the model hard to generalize but also the lack of methods to exploit the multimodal features in VRD. In this paper, we propose a light-weight model named GraphRevisedIE that effectively embeds multimodal features such as textual, visual, and layout features from VRD and leverages graph revision and graph convolution to enrich the multimodal embedding with global context. Extensive experiments on multiple real-world datasets show that GraphRevisedIE generalizes to documents of varied layouts and achieves comparable or better performance compared to previous KIE methods. We also publish a business license dataset that contains both real-life and synthesized documents to facilitate research of document KIE.}
}
@article{ZHOU2023109555,
title = {Cross-level Feature Aggregation Network for Polyp Segmentation},
journal = {Pattern Recognition},
volume = {140},
pages = {109555},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109555},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002558},
author = {Tao Zhou and Yi Zhou and Kelei He and Chen Gong and Jian Yang and Huazhu Fu and Dinggang Shen},
keywords = {Polyp segmentation, boundary-aware features, cross-level feature fusion, boundary aggregated module},
abstract = {Accurate segmentation of polyps from colonoscopy images plays a critical role in the diagnosis and cure of colorectal cancer. Although effectiveness has been achieved in the field of polyp segmentation, there are still several challenges. Polyps often have a diversity of size and shape and have no sharp boundary between polyps and their surrounding. To address these challenges, we propose a novel Cross-level Feature Aggregation Network (CFA-Net) for polyp segmentation. Specifically, we first propose a boundary prediction network to generate boundary-aware features, which are incorporated into the segmentation network using a layer-wise strategy. In particular, we design a two-stream structure based segmentation network, to exploit hierarchical semantic information from cross-level features. Furthermore, a Cross-level Feature Fusion (CFF) module is proposed to integrate the adjacent features from different levels, which can characterize the cross-level and multi-scale information to handle scale variations of polyps. Further, a Boundary Aggregated Module (BAM) is proposed to incorporate boundary information into the segmentation network, which enhances these hierarchical features to generate finer segmentation maps. Quantitative and qualitative experiments on five public datasets demonstrate the effectiveness of our CFA-Net against other state-of-the-art polyp segmentation methods. The source code and segmentation maps will be released at https://github.com/taozh2017/CFANet.}
}
@article{LYU2023109531,
title = {FETNet: Feature erasing and transferring network for scene text removal},
journal = {Pattern Recognition},
volume = {140},
pages = {109531},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109531},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002315},
author = {Guangtao Lyu and Kun Liu and Anna Zhu and Seiichi Uchida and Brian Kenji Iwana},
keywords = {Scene text removal, Text segmentation, One-stage, Self-attention},
abstract = {The scene text removal (STR) task aims to remove text regions and recover the background smoothly in images for private information protection. Most existing STR methods adopt encoder-decoder-based CNNs, with direct copies of the features in the skip connections. However, the encoded features contain both text texture and structure information. The insufficient utilization of text features hampers the performance of background reconstruction in text removal regions. To tackle these problems, we propose a novel Feature Erasing and Transferring (FET) mechanism to reconfigure the encoded features for STR in this paper. In FET, a Feature Erasing Module (FEM) is designed to erase text features. An attention module is responsible for generating the feature similarity guidance. The Feature Transferring Module (FTM) is introduced to transfer the corresponding features in different layers based on the attention guidance. With this mechanism, a one-stage, end-to-end trainable network called FETNet is constructed for scene text removal. In addition, to facilitate research on both scene text removal and segmentation tasks, we introduce a novel dataset, Flickr-ST, with multi-category annotations. A sufficient number of experiments and ablation studies are conducted on the public datasets and Flickr-ST. Our proposed method achieves state-of-the-art performance using most metrics, with remarkably higher quality scene text removal results.}
}
@article{ZHU2023109589,
title = {FSConv: Flexible and separable convolution for convolutional neural networks compression},
journal = {Pattern Recognition},
volume = {140},
pages = {109589},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109589},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300290X},
author = {Yangyang Zhu and Luofeng Xie and Zhengfeng Xie and Ming Yin and Guofu Yin},
keywords = {CNNs compression, Representative feature maps, Redundant feature maps, Intrinsic information, Tiny hidden details},
abstract = {Because of limited computation resources, convolutional neural networks (CNNs) are difficult to deploy on mobile devices. To overcome this issue, many methods have successively reduced parameters in CNNs with the idea of removing redundancy among feature maps. We observe similarities between feature maps at the same layer but not complete consistency. Intuitively, the difference between similar feature maps is an essential ingredient for the success of CNNs. Therefore, we propose a flexible and separable convolution (FSConv) in a different perspective to embrace redundancy while requiring less computation, which can implicitly cluster feature maps into different clusters without introducing similarity measurements. Our proposed model extracts intrinsic information from the representative part through ordinary convolution in each cluster and reveals tiny hidden details from the redundant part through groupwise/depthwise convolution. Experimental results demonstrate that FSConv-equipped networks always perform better than previous state-of-the-art CNNs compression algorithms. Code is available at https://github.com/Clarkxielf/FSConv-Flexible-and-Separable-Convolution-for-Convolutional-Neural-Networks-Compression.}
}
@article{TIAN2023109548,
title = {Bi-Attention enhanced representation learning for image-text matching},
journal = {Pattern Recognition},
volume = {140},
pages = {109548},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109548},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002480},
author = {Yumin Tian and Aqiang Ding and Di Wang and Xuemei Luo and Bo Wan and Yifeng Wang},
keywords = {Image-text matching, Bi-attention, Polynomial loss},
abstract = {Image-text matching has become a research hotspot in recent years. The key point of image-text matching is to accurately measure the similarity between an image and a sentence. However, most existing methods either focus on the inter-modality similarities between regions in image and words in text or the intra-modality similarities within image regions or words, such that they cannot well exploit detailed correlations between images and texts. Furthermore, existing methods typically train their models using a triplet ranking loss, which relies on the similarity of randomly sampled triples. Since the weights of positive and negative samples are not adjusted, it cannot provide enough gradient information for training, resulting in slow convergence and limited performance. To address the above problems, we propose an image-text matching method named Bi-Attention Enhanced Representation Learning (BAERL). It builds a self-attention learning sub-network to exploit intra-modality correlations within image regions or words and a co-attention learning sub-network to exploit inter-modality correlations between image regions and words. Then, representations obtained by two sub-networks capture holistic correlations between images and texts. In addition, BAERL uses the self-similarity polynomial loss instead of triplet ranking loss to train the model. The self-similarity polynomial loss can adaptively assign appropriate weights to different pairs based on their similarity scores so as to further improve the retrieval performance. Experiments on two benchmark datasets demonstrate the superior performance of the proposed BAERL method over several state-of-the-art methods.}
}
@article{YANG2023109552,
title = {HeadPose-Softmax: Head pose adaptive curriculum learning loss for deep face recognition},
journal = {Pattern Recognition},
volume = {140},
pages = {109552},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109552},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002522},
author = {Jifan Yang and Zhongyuan Wang and Baojin Huang and Jinsheng Xiao and Chao Liang and Zhen Han and Hua Zou},
keywords = {Face recognition, Multi-view face, Curriculum learning, Pose-aware},
abstract = {Face recognition has been one of the most popular applications in the field of target detection. Currently, frontal faces can be easily detected, but multi-view face detection remains a difficult task because of various factors such as illumination, various poses, occlusions, and facial expressions. Margin-based loss functions are used to increase the feature margins between different classes, thus enhancing the discriminability of face recognition models, but the performance in face detection in complex scenes (e.g., high pitch angle face detection in surveillance environments) can be significantly degraded. Recently, the idea of a mining-based strategy to emphasize hard samples has been used to achieve good results in multi-view face detection. However, most of the existing methods do not explicitly emphasize samples based on their importance, resulting in the underutilization of hard samples. In this paper, we propose a curriculum learning loss function (HeadPose-Softmax) to classify the difficulty of a sample based on its facial pose, and embed the concept of curriculum learning into the loss function to implement a novel training strategy for deep face recognition. The loss function explicitly emphasizes the importance of the samples according to the different difficulty of each sample, which allows the model to make fuller use of hard samples, focus on learning pose invariant features, and improve the accuracy of the model in multi-view face detection tasks. Specifically, our HeadPose-Softmax dynamically adjusts the relative importance of the hard samples according to the pose angle of the face in the hard samples during the training phase. At each stage, different samples are assigned different importance according to their corresponding difficulty. Extensive experimental results under popular benchmarks show that our HeadPose-Softmax can enhance the accuracy of the model in multi-view face detection and outperform the state-of-the-art competitors.}
}
@article{GAO2023109479,
title = {Multicycle disassembly-based decomposition algorithm to train multiclass support vector machines},
journal = {Pattern Recognition},
volume = {140},
pages = {109479},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109479},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001796},
author = {Tong Gao and Hao Chen},
keywords = {Multicycle disassembly-based decomposition algorithm, Multiclass support vector machine, Decomposition algorithm, Support vector machine training},
abstract = {Employing the classic optimization solver to train a multiclass support vector machine (SVM) requires prohibitive training time as the sample size and number of categories increase. It has been proposed to develop the corresponding decomposition algorithm (DA) as it is efficient for training SVMs. However, the dual problem of multiclass SVM comprises complex constraints that complicate DA design, so no corresponding DA has yet been developed. We propose a multicycle disassembly-based DA (MCD-DA) to efficiently solve the training problem of multiclass SVM. First, a graph model is constructed to re-express the constraints in multiclass SVM. Then, the original complex feasible region is partitioned into several simple sub-feasible regions, and multiple cycle-based disassembly strategies are designed to update the working variables analytically within each specific sub-feasible region. We mathematically verify that MCD-DA can stop within a finite number of cycle disassemblies and reach the τ-optimal solution satisfying relaxed Karush–Kuhn–Tucker conditions. Remarkably, MCD-DA as a universal decomposition algorithm can be used to solve many other SVM variants, including C-SVM, v-SVM, and one-class SVM. Experimental results using six UCI datasets demonstrate that MCD-DA outperforms typical optimization algorithms for more sample cases.}
}
@article{SHABANI2023109604,
title = {Augmented bilinear network for incremental multi-stock time-series classification},
journal = {Pattern Recognition},
volume = {141},
pages = {109604},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109604},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003059},
author = {Mostafa Shabani and Dat Thanh Tran and Juho Kanniainen and Alexandros Iosifidis},
keywords = {Deep learning, Low rank tensor decomposition, Limit order book data, Financial time-series analysis},
abstract = {Deep Learning models have become dominant in tackling financial time-series analysis problems, overturning conventional machine learning and statistical methods. Most often, a model trained for one market or security cannot be directly applied to another market or security due to differences inherent in the market conditions. In addition, as the market evolves over time, it is necessary to update the existing models or train new ones when new data is made available. This scenario, which is inherent in most financial forecasting applications, naturally raises the following research question: How to efficiently adapt a pre-trained model to a new set of data while retaining performance on the old data, especially when the old data is not accessible? In this paper, we propose a method to efficiently retain the knowledge available in a neural network pre-trained on a set of securities and adapt it to achieve high performance in new ones. In our method, the prior knowledge encoded in a pre-trained neural network is maintained by keeping existing connections fixed, and this knowledge is adjusted for the new securities by a set of augmented connections, which are optimized using the new data. The auxiliary connections are constrained to be of low rank. This not only allows us to rapidly optimize for the new task but also reduces the storage and run-time complexity during the deployment phase. The efficiency of our approach is empirically validated in the stock mid-price movement prediction problem using a large-scale limit order book dataset. Experimental results show that our approach enhances prediction performance as well as reduces the overall number of network parameters.}
}
@article{AGHAJANZADEH2023109587,
title = {Task weighting based on particle filter in deep multi-task learning with a view to uncertainty and performance},
journal = {Pattern Recognition},
volume = {140},
pages = {109587},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109587},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002881},
author = {Emad Aghajanzadeh and Tahereh Bahraini and Amir Hossein Mehrizi and Hadi Sadoghi Yazdi},
keywords = {Multi task learning, Uncertainty, Hyper-parameter tuning, Deep learning, Particle filter, Bayesian estimation},
abstract = {Recently multi-task learning (MTL) has been widely used in different applications to build more robust models by sharing knowledge across several related tasks. However, one challenge that arises is the variability in the learning pace of different tasks causing the inefficiency of naively training all tasks. Therefore, it is of great importance to consider some coefficients to balance tasks in the process of learning, but, due to the large search space and the significance of setting them properly, conventional search methods such as grid or random search are no longer effective. In this paper, we propose a learning mechanism for these coefficients based on the high efficiency of the particle filter (PF) algorithm to deal with nonlinear search problems. PF considers each state of the tasks’ coefficients as a particle and recursively converges coefficients to an optimum point. While in most previous works coefficients were evaluated to only increase performance, to address the recent concerns related to applying AI in real-world applications, we also incorporate uncertainty alongside our method to prevent learning coefficients leading to unstable outcomes. This mechanism is independent of the models main learning process and can be easily added to every learning system without changing its training algorithm. Extensive experiments on real-world data sets demonstrate the superiority of the proposed method over the state-of-the-art methods on both performance and uncertainty. We also proved the acceptable performance of the method using Cramer Rao lower bound theory.}
}
@article{GAO2023109512,
title = {Not All Samples Are Born Equal: Towards Effective Clean-Label Backdoor Attacks},
journal = {Pattern Recognition},
volume = {139},
pages = {109512},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109512},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002121},
author = {Yinghua Gao and Yiming Li and Linghui Zhu and Dongxian Wu and Yong Jiang and Shu-Tao Xia},
keywords = {Backdoor attack, Clean-label attack, Sample selection, Trustworthy ML, AI Security, Deep learning},
abstract = {Recent studies demonstrated that deep neural networks (DNNs) are vulnerable to backdoor attacks. The attacked model behaves normally on benign samples, while its predictions are misled whenever adversary-specified trigger patterns appear. Currently, clean-label backdoor attacks are usually regarded as the most stealthy methods in which adversaries can only poison samples from the target class without modifying their labels. However, these attacks can hardly succeed. In this paper, we reveal that the difficulty of clean-label attacks mainly lies in the antagonistic effects of ‘robust features’ related to the target class contained in poisoned samples. Specifically, robust features tend to be easily learned by victim models and thus undermine the learning of trigger patterns. Based on these understandings, we propose a simple yet effective plug-in method to enhance clean-label backdoor attacks by poisoning ‘hard’ instead of random samples. We adopt three classical difficulty metrics as examples to implement our method. We demonstrate that our method can consistently improve vanilla attacks, based on extensive experiments on benchmark datasets.}
}
@article{YANG2023109564,
title = {DyGAT: Dynamic stroke classification of online handwritten documents and sketches},
journal = {Pattern Recognition},
volume = {141},
pages = {109564},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109564},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002649},
author = {Yu-Ting Yang and Yan-Ming Zhang and Xiao-Long Yun and Fei Yin and Cheng-Lin Liu},
keywords = {Stroke classification, Sketch semantic segmentation, Document layout analysis, Diagram recognition, Streaming recognition},
abstract = {Online handwriting is widely used in human-machine interface, education, office automation, and so on. Stroke classification for online handwritten documents and sketches aims to divide strokes into several semantic categories and is a necessary step for document recognition and understanding. Previous methods are essentially static in that they have to wait for the user to finish the whole sketch before making prediction. However, in practice, the more user-friendly way is to make real-time prediction as the user is writing. In this paper, we introduce Dynamic Graph ATtention network (DyGAT) to solve the dynamic stroke classification problem. The core of our method is to formalize a document/sketch into a multi-feature graph, in which nodes represent strokes, edges represent the relationships between strokes, and multiple nodes are applied to one stroke to control the information flow. The proposed method is general and is applicable to online handwritten data of many types. We conduct experiments on popular public datasets to perform sketch semantic segmentation, document layout analysis and diagram recognition, and experimental results show competitive performance. Particularly, the proposed method achieves stroke classification accuracies which are only slightly lower than those of static classification.}
}
@article{LV2023109591,
title = {Learning cross-domain semantic-visual relationships for transductive zero-shot learning},
journal = {Pattern Recognition},
volume = {141},
pages = {109591},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109591},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002923},
author = {Fengmao Lv and Jianyang Zhang and Guowu Yang and Lei Feng and Yufeng Yu and Lixin Duan},
keywords = {Zero-shot learning, Transfer learning, Domain adaptation},
abstract = {Zero-Shot Learning (ZSL) learns models for recognizing new classes. One of the main challenges in ZSL is the domain discrepancy caused by the category inconsistency between training and testing data. Domain adaptation is the most intuitive way to address this challenge. However, existing domain adaptation techniques cannot be directly applied into ZSL due to the disjoint label space between source and target domains. This work proposes the Transferrable Semantic-Visual Relation (TSVR) approach towards transductive ZSL. TSVR redefines image recognition as predicting the similarity/dissimilarity labels for semantic-visual fusions consisting of class attributes and visual features. After the above transformation, the source and target domains can have the same label space, which hence enables to quantify domain discrepancy. For the redefined problem, the number of similar semantic-visual pairs is significantly smaller than that of dissimilar ones. To this end, we further propose to use Domain-Specific Batch Normalization to align the domain discrepancy.}
}
@article{DAI2023109540,
title = {Global spatio-temporal synergistic topology learning for skeleton-based action recognition},
journal = {Pattern Recognition},
volume = {140},
pages = {109540},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109540},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002406},
author = {Meng Dai and Zhonghua Sun and Tianyi Wang and Jinchao Feng and Kebin Jia},
keywords = {Action recognition, Spatio-temporal synergistic, Skeleton, Topology learning},
abstract = {Compared to RGB video-based action recognition, skeleton-based action recognition algorithm has attracted much more attention due to being more lightweight, better generalization and robustness. The extraction of temporal and spatial features is a crucial factor for skeleton-based action recognition. However, existing feature extraction methods suffer from two limitations: (1) the isolated extraction of temporal and spatial feature cannot capture temporal feature connections among non-adjacent joints and (2) convolution-limited perceptual fields cannot capture global temporal features of joints effectively. In this work, we propose a global spatio-temporal synergistic feature learning module (GSTL), which generates global spatio-temporal synergistic topology of joints by spatio-temporal feature fusion. By further combining the GSTL with a temporal modeling unit, we develop a powerful global spatio-temporal synergistic topology learning network (GSTLN), and it achieves competitive performance with fewer parameters on three challenge datasets: NTU RGB + D, NTU RGB + D 120, and NW-UCLA.}
}
@article{WANG2023109586,
title = {GGD-GAN: Gradient-Guided dual-Branch adversarial networks for relic sketch generation},
journal = {Pattern Recognition},
volume = {141},
pages = {109586},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109586},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300287X},
author = {Jun Wang and Erlei Zhang and Shan Cui and Jiaxin Wang and Qunxi Zhang and Jianping Fan and Jinye Peng},
keywords = {Relics digital protection, Sketch generation, Image translation, Deep learning},
abstract = {Sketch reflects the main content and structure of the painted cultural relics, which help researcher understand the original drawing intention and painting skills. Although the existing automatic sketch extraction methods can improve efficiency, most of them are based on edge detection leading the limitations in incomplete details, serious disease noise and blurring. In this paper, a gradient guided dual-branch generative adversarial networks (GANs) is proposed for high-quality relic sketch generation. The sketch generation branch (SGB) and auxiliary gradient-image generation branch (GGB) were designed via two independent GANs for learning different and complement characteristics. We also designed a feature transmission module for transferring context features from SGB to GGB, and a fusion block to realize the gradient guidance from GGB to SGB, forcing SGB to pay more attention to shape, details, and noise suppression. Both branches not only learn different characteristics independently, but also affect and complement each other, which generates rich and clean high-quality sketches. In our experiments, we compared our method with eight state-of-the-art algorithms quantitatively and qualitatively, analysis the effects of the gradient guidance feature transfer, and the generalization of the proposed dual-branch GAN framework. Experiments show the proposed framework is promising in its ability to extract a clear, coherent, and complete sketch of painted cultural relics.}
}
@article{HOFMEYR2023109471,
title = {Incremental estimation of low-density separating hyperplanes for clustering large data sets},
journal = {Pattern Recognition},
volume = {139},
pages = {109471},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109471},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001711},
author = {David P. Hofmeyr},
keywords = {Clustering, Low-density separation, Big data, Stochastic gradient descent, Smoothing kernel, High dimensionality},
abstract = {An efficient unsupervised method for obtaining low-density hyperplane separators is proposed. The method is based on a modified stochastic gradient descent applied on a convolution of the empirical distribution function with a smoothing kernel. Low-density hyperplanes are motivated by the fact that they avoid intersecting high density regions, and so tend to pass between high density clusters, thus separating them from one another, while keeping the individual clusters intact. Multiple hyperplanes can be combined in a hierarchical model to obtain a complete clustering solution. A simple post-processing of solutions induced by large collections of hyperplanes yields an efficient and accurate clustering method, capable of automatically selecting the number of clusters. Experiments show that the proposed method is highly competitive in terms of both speed and accuracy when compared with relevant benchmarks. Code is available in the form of an R package at https://github.com/DavidHofmeyr/iMDH.}
}
@article{SUN2023109524,
title = {MSCA-Net: Multi-scale contextual attention network for skin lesion segmentation},
journal = {Pattern Recognition},
volume = {139},
pages = {109524},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109524},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002248},
author = {Yongheng Sun and Duwei Dai and Qianni Zhang and Yaqi Wang and Songhua Xu and Chunfeng Lian},
keywords = {Skin lesion segmentation, Multi-scale bridge module, Global-local channel spatial attention module, Scale-aware deep supervision module},
abstract = {Lesion segmentation algorithms automatically outline lesion areas in medical images, facilitating more effective identification and assessment of the clinically relevant features, and improving the efficacy and diagnosis accuracy. However, most fully convolutional network based segmentation methods suffer from spatial and contextual information loss when decreasing image resolution. To overcome this shortcoming, this paper proposes a skin lesion segmentation model, namely, the Multi-Scale Contextual Attention Network (MSCA-Net), which can exploit the multi-scale contextual information in images. Inspired by the skip connection of U-Net, we design a multi-scale bridge (MSB) module which interacts with multi-scale features to effectively fuse the multi-scale contextual information of the encoder and decoder path features. We further propose a global-local channel spatial attention module (GL-CSAM), aiming at capturing global contextual information. In addition, to take full advantage of the multi-scale features of the decoder, we propose a scale-aware deep supervision (SADS) module to achieve hierarchical iterative deep supervision. Comprehensive experimental results on the public dataset of ISIC 2017, ISIC 2018, and PH2 show that our proposed method outperforms other state-of-the-art methods, demonstrating the efficacy of our method in skin lesion segmentation. Our code is available at https://github.com/YonghengSun1997/MSCA-Net.}
}
@article{QIU2023109580,
title = {Underestimation modification for intrinsic dimension estimation},
journal = {Pattern Recognition},
volume = {140},
pages = {109580},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109580},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002807},
author = {Haiquan Qiu and Youlong Yang and Hua Pan},
keywords = {Intrinsic dimension, Parameter selection, Estimation method, Underestimation modification, Smooth manifold},
abstract = {The intrinsic dimension is the dimension of the low-dimensional manifold where the high-dimensional data is located. Accurately estimating the intrinsic dimension of the data set is helpful for data-dimensionality reduction and preprocessing. Due to the unknown spatial distribution of data and the limited sample size of a dataset, estimation methods which only use distance information tend to underestimate the intrinsic dimension of dataset. To reduce the estimation complexity and improve the accuracy, two estimation algorithms based on ID(κ) are proposed, where κ is the scaling ratio of the neighborhood radius of the sample point. First, according to the selection criteria of parameter κ, an improved algorithm for selecting the optimal scaling ratio κ is proposed, which reduces the computational complexity and improves the stability of estimation. Second, using simulation datasets with the same sample size and known intrinsic dimensions, the relationship between the estimated dimension and the true intrinsic dimension is obtained, and an underestimation modification method for intrinsic dimension estimation is proposed. Results of comparative experiments on simulation and real datasets indicate that the underestimation modification algorithm has high estimation accuracy and robustness.}
}
@article{XIONG2023109549,
title = {A black-box reversible adversarial example for authorizable recognition to shared images},
journal = {Pattern Recognition},
volume = {140},
pages = {109549},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109549},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002492},
author = {Lizhi Xiong and Yue Wu and Peipeng Yu and Yuhui Zheng},
keywords = {Reversible adversarial example, Reversible data hiding, Prediction-error histogram},
abstract = {Shared images on the Internet are easily collected, classified, and analyzed by unauthorized commercial companies through Deep Neural Networks (DNNs). The illegal use of these data damages the rights and interests of authorized companies and individuals. How to ensure that network-shared data is legally used by authorized users and not used by unauthorized DNNs has become an urgent problem. Reversible Adversarial Example (RAE) provides an effective solution, which can mislead the classification of unauthorized DNNs and does not affect the authorized users. The existing RAE schemes assumed that we could know the parameters of the target model and thus generate reversible adversarial examples. However, model parameters are often protected to avoid leakage, increasing the difficulty of generating accurate RAEs. In this paper, we first propose a Black-box Reversible Adversarial Example (B-RAE) scheme to generate robust reversible adversarial examples. We aim to protect image privacy while maintaining data usability in real scenarios. Experimental results and analysis have demonstrated that the proposed B-RAE is more effective and robust compared with the existing schemes.}
}
@article{KLONECKI2023109605,
title = {Cost-constrained feature selection in multilabel classification using an information-theoretic approach},
journal = {Pattern Recognition},
volume = {141},
pages = {109605},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109605},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003060},
author = {Tomasz Klonecki and Paweł Teisseyre and Jaesung Lee},
keywords = {Multilabel learning, Multilabel feature selection, Cost-constrained feature selection, Cost factor optimization, Mutual information},
abstract = {Feature selection is one of the key steps in building a predictive model in multi-label classification. However, most of the existing methods do not take into account information about the costs associated with considered features, such as the costs of performing diagnostic medical tests. We consider a problem of cost-constrained multilabel feature selection, which aims to select a feature subset relevant to multiple labels while satisfying a user-specific maximal admissible budget. This approach allows for building a model with high predictive power, for which the cost of making a prediction for a single instance does not exceed the user-specified budget. In this problem, the balance between the feature subset relevance and its cost should be considered concurrently, which is nontrivial in practice because their optimal balance is unknown. In this paper, we propose a novel criterion for cost-constrained multilabel feature selection that combines the relevance and cost of the candidate feature. The relevance measure is derived using the lower bound of the mutual information between the feature subset and label vector. Moreover, we propose an effective method for determining the cost-factor value that controls the trade-off between relevancy and cost. The experimental results on multilabel datasets with various characteristics demonstrate the superiority of the proposed method over conventional methods.}
}
@article{SHAO2023109509,
title = {FGPNet: A weakly supervised fine-grained 3D point clouds classification network},
journal = {Pattern Recognition},
volume = {139},
pages = {109509},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109509},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002091},
author = {Huihui Shao and Jing Bai and Rusong Wu and Jinzhe Jiang and Hongbo Liang},
keywords = {3D point clouds, Fine-grained classification, Context-aware feature extraction, SimAM-Capsule aggregation, Local geometric details, Spatial relationships},
abstract = {3D point clouds classification has been a hot research topic and received great progress in recent years. However, due to the similar data distributions and subtle differences among various sub-categories in a meta-category, the 3D point clouds classification at a fine-grained level is still very challenging, especially without the annotations of part locations or attributes. In this paper, we propose a novel weakly supervised network for fine-grained 3D point clouds classification, namely FGPNet. Different from the previous supervised fine-grained classification methods that use class labels and other manual annotation information, FGPNet develops a unified framework to address both local geometric details and global spatial structures only using the class labels as input. Specifically, FGPNet firstly employs a context-aware discriminative feature extraction (CDFE) module, which extract contextual contrasted information across differential receptive fields hierarchically, and further capture discriminative local details from point clouds. Subsequently, an SimAM-Capsule Aggregation (SCA) module is introduced to highlight the significant local features and capture their spatial relationships. Quantitative and qualitative experimental results on fine-grained dataset including three categories Airplane, Chair and Car demonstrate that FGPNet outperforms the state-of-the-art methods on fine-grained 3D point clouds classification tasks.}
}
@article{LIANG2023109609,
title = {L1-norm discriminant analysis via Bhattacharyya error bounds under Laplace distributions},
journal = {Pattern Recognition},
volume = {141},
pages = {109609},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109609},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003102},
author = {Zhizheng Liang and Lei Zhang},
keywords = {Laplace distributions, Bhattacharyya error bound, Discriminant criteria, Kernel functions, Data classification},
abstract = {L1-norm discriminant analysis has been proposed to enhance the robustness of classical LDA in the presence of outliers. This paper develops L1-norm discriminant analysis by exploring Bhattacharyya error bounds under Laplace distributions. Unlike some previous models, we assume that the samples of each class in the projected space follow Laplace distributions. It is the first time that the Bhattacharyya error bound is derived under Laplace distributions. It is interesting to note that this bound has a closed-form expression in a one-dimensional projected space. We relax the Bhattacharyya error bound to achieve another bound that can facilitate the design of a tractable model. Since the parameters of Laplace distributions are estimated by the maximum likelihood estimation, this yields the problem of estimating class centroids in our model. We employ a simple yet effective strategy to estimate class centroids in the original space. To address the small-sample-size problem, we transform the original model into a difference criterion by introducing additional parameters. We achieve an alternative representation of our model and design an effective algorithm to solve it. In addition, we also extend our model to its kernel version. The experiments on a series of data sets are done to demonstrate the effectiveness of our method in dealing with contaminated data.}
}
@article{XU2023109588,
title = {MoCA: Incorporating domain pretraining and cross attention for textbook question answering},
journal = {Pattern Recognition},
volume = {140},
pages = {109588},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109588},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002893},
author = {Fangzhi Xu and Qika Lin and Jun Liu and Lingling Zhang and Tianzhe Zhao and Qi Chai and Yudai Pan and Yi Huang and Qianying Wang},
keywords = {Textbook question answering, Multimodal, Pretraining, Attention},
abstract = {Textbook Question Answering (TQA) is a complex multimodal task to infer answers given large context descriptions and abundant diagrams. Compared with Visual Question Answering (VQA), TQA contains a large number of uncommon terminologies and various diagram inputs. It brings new challenges to the representation capability of language model for domain-specific spans. Also, it requires the model to take fully advantage of the complementary information of different diagram types, which pushes the multimodal fusion task to a more complex level. To tackle the above issues, we propose a novel model named MoCA, which incorporates Multi-stage domain pretraining and Cross-guided multimodal Attention for the TQA task. Firstly, we introduce a multi-stage domain pretraining module to conduct unsupervised post-pretraining with a span mask strategy and supervised pre-finetune. Especially for domain post-pretraining, we propose a heuristic generation algorithm to employ the terminology corpus. Secondly, to fully consider the rich inputs of context and diagrams, we propose a cross-guided multimodal attention mechanism to update the features of text, question diagram and instructional diagram based on a progressive strategy. Further, a dual gating mechanism is adopted to improve the model ensemble of three background retrievals. The experimental results show the superiority of our model, which outperforms the state-of-the-art methods on the validation and test split respectively. Also, ablation and comparison experiments verify the effectiveness of each module proposed in our model.}
}
@article{ZHANG2023109537,
title = {Line graph contrastive learning for link prediction},
journal = {Pattern Recognition},
volume = {140},
pages = {109537},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109537},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002376},
author = {Zehua Zhang and Shilin Sun and Guixiang Ma and Caiming Zhong},
keywords = {Line graph, Contrastive learning, Link prediction, Node classification, Mutual information},
abstract = {Link prediction tasks focus on predicting possible future connections. Most existing researches measure the likelihood of links by different similarity scores on node pairs and predict links between nodes. However, the similarity-based approaches have some challenges in information loss on nodes and generalization ability on similarity indexes. To address the above issues, we propose a Line Graph Contrastive Learning (LGCL) method to obtain rich information with multiple perspectives. LGCL obtains a subgraph view by h-hop subgraph sampling with target node pairs. After transforming the sampled subgraph into a line graph, the link prediction task is converted into a node classification task, which graph convolution progress can learn edge embeddings from graphs more effectively. Then we design a novel cross-scale contrastive learning framework on the line graph and the subgraph to maximize the mutual information of them, so that fuses the structure and feature information. The experimental results demonstrate that the proposed LGCL outperforms the state-of-the-art methods and has better performance on generalization and robustness.}
}
@article{GUO2023109565,
title = {Robust semi-supervised multi-view graph learning with sharable and individual structure},
journal = {Pattern Recognition},
volume = {140},
pages = {109565},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109565},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002650},
author = {Wei Guo and Zhe Wang and Wenli Du},
keywords = {Semi-supervised learning, Multi-view learning, Clean data, Manifold structure},
abstract = {The construction of a high-quality multi-view consensus graph is key to graph-based semi-supervised multi-view learning (GSSMvL) methods. However, most existing GSSMvL methods explore sample relationships in the original multi-view feature space, which obtains a contaminated graph that cannot reveal the underlying manifold structure of the samples. Moreover, traditional GSSMvL methods fail to explore the diverse structures of multi-view features, which may lose their complementary information and lead to a suboptimal graph. In this paper, we propose a novel unified robust semi-supervised multi-view graph learning framework based on the sharable and individual structure (RSSMvSI), which can eliminate the influence of noise and exploit the knowledge of multi-view data in a reasonable manner. Specifically, we first learn clean data by manipulating sparse noise with l2,1 norm. We then simultaneously explore the sharable and individual self-representation subspace on the learned clean multi-view data. The key point is that noisy data does not participate in subspace learning, which improves the robustness of the proposed method. By constructing the optimal consensus graph with the learned sharable and individual subspace, RSSMvSI can better utilize the complementary information of multi-view data and approximate the manifold structure of samples. To the best of our knowledge, this is the first attempt to learn the self-representation subspace on recovered multi-view clean data. Extensive experiments on various real-world multi-view datasets demonstrate the superiority and robustness against state-of-the-art methods.}
}
@article{WANG2023109513,
title = {Dual similarity pre-training and domain difference encouragement learning for vehicle re-identification in the wild},
journal = {Pattern Recognition},
volume = {139},
pages = {109513},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109513},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002133},
author = {Qi Wang and Yuling Zhong and Weidong Min and Haoyu Zhao and Di Gai and Qing Han},
keywords = {Vehicle re-identification, Unsupervised domain adaptation, Dual constraint label smoothing regularization loss, Domain difference penalty term, Pseudo label refinement},
abstract = {Existing unsupervised domain adaptation (UDA) tasks require extensive annotated wild data in the source domain to be generalized to the target domain. Additionally, the large gap between the source and target domains hinder the clustering performance. Our work concentrates on few-shot UDA task to train a robust Re-ID model from practical vehicle re-identification (Re-ID). That is to say, this task learns discriminative representations from a few labeled source data to unlabeled target data. In this paper, a novel progressive few-shot UDA learning framework for vehicle Re-ID is proposed, which consists of two branches. In source branch, the dual prior model is used to gain the color and IDs in unlabeled source data. A dual constraint label smoothing regularization (DCLSR) loss is designed to supervise extensive unlabeled source data during pre-training phase, which considers color and ID constraints to mine the similarity between unlabeled source data and a few labeled ones. The target branch develops a progressive domain difference encouragement learning method to optimize the cross-domain capability of Re-ID model. The domain difference penalty term (DDPT) is encoded by the feature variations before and after style transfer, which improves clustering results and refines the pseudo label. Comprehensive experimental results verify that the proposed approach outperforms other ones in the practical UDA task.}
}
@article{GUO2023109508,
title = {Sensitivity pruner: Filter-Level compression algorithm for deep neural networks},
journal = {Pattern Recognition},
volume = {140},
pages = {109508},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109508},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300208X},
author = {Suhan Guo and Bilan Lai and Suorong Yang and Jian Zhao and Furao Shen},
keywords = {Filter pruning, Saliency-based pruning, End-to-end pruning framework, Sampling bias},
abstract = {As neural networks get deeper for better performance, the demand for deployable models on resource-constrained devices also grows. In this work, we propose eliminating less sensitive filters to compress models. The previous method evaluates neuron importance using the connection matrix gradient in a single shot. To mitigate the sampling bias, we integrate this measure into the previously proposed “pruning while fine-tuning” framework. Besides classification errors, we introduce the difference between the learned and the single-shot strategy as the second loss component with a self-adjustive hyper-parameter that balances the training goal between improving accuracy and pruning more filters. Our Sensitivity Pruner (SP) adapts the unstructured pruning saliency metric to structured pruning tasks and enables the strategy to be derived sequentially to accommodate the updating sparsity. Experimental results demonstrate that SP significantly reduces the computational cost and the pruned models give comparable or better performance on CIFAR10, CIFAR100, and ILSVRC-12 datasets.}
}
@article{HU2023109592,
title = {Holistic transformer: A joint neural network for trajectory prediction and decision-making of autonomous vehicles},
journal = {Pattern Recognition},
volume = {141},
pages = {109592},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109592},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002935},
author = {Hongyu Hu and Qi Wang and Zhengguang Zhang and Zhengyi Li and Zhenhai Gao},
keywords = {Autonomous vehicles, Decision-making, Holistic transformer, Multiple cues, Trajectory prediction},
abstract = {Trajectory prediction and behavioral decision-making are two important tasks for autonomous vehicles that require a good understanding of the environmental context. Notably, behavioral decisions are better made by referring to the outputs of trajectory predictions. However, most current solutions perform these tasks separately. Therefore, this paper proposes a new joint holistic transformer network that combines multiple cues to predict trajectories and make behavioral decisions simultaneously. To better explore the intrinsic relationships among cues, the network uses existing knowledge and adopts three kinds of attention mechanisms: the sparse multi-head type for reducing noise impact, feature selection sparse type for optimally using partial prior knowledge, and multi-head with sigmoid activation type for optimally using posteriori knowledge. Compared with other trajectory prediction models, the proposed model has a better comprehensive performance and good interpretability. Perceptual noise robustness experiments demonstrate that the proposed model has good noise robustness. Thus, simultaneous trajectory prediction and behavioral decision-making combining multiple cues are accomplished, which reduces computational costs and enhances semantic relationships between scenes and agents.}
}
@article{CHO2023109541,
title = {Ambiguity-aware robust teacher (ART): Enhanced self-knowledge distillation framework with pruned teacher network},
journal = {Pattern Recognition},
volume = {140},
pages = {109541},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109541},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002418},
author = {Yucheol Cho and Gyeongdo Ham and Jae-Hyeok Lee and Daeshik Kim},
keywords = {Knowledge distillation, Self-knowledge distillation, Network pruning, Teacher-student model, Long-tail samples, Ambiguous samples, Sample ambiguity, Data augmentation},
abstract = {Self-knowledge distillation (self-KD) methods, which use a student model itself as the teacher model instead of a large and complex teacher model, are currently a subject of active study. Since most previous self-KD approaches relied on the knowledge of a single teacher model, if the teacher model incorrectly predicted confusing samples, poor-quality knowledge was transferred to the student model. Unfortunately, natural images are often ambiguous for teacher models due to multiple objects, mislabeling, or low quality. In this paper, we propose a novel knowledge distillation framework named ambiguity-aware robust teacher knowledge distillation (ART-KD) that provides refined knowledge, that reflects the ambiguity of the samples with network pruning. Since the pruned teacher model is simply obtained by copying and pruning the teacher model, re-training process is unnecessary in ART-KD. The key insight of ART-KD lies in the predictions of a teacher model and pruned teacher model for ambiguous samples providing different distributions with low similarity. From these two distributions, we obtain a joint distribution considering the ambiguity of the samples as teacher’s knowledge for distillation. We comprehensively evaluate our method on public classification benchmarks, as well as more challenging benchmarks for fine-grained visual recognition (FGVR), achieving much superior performance to state-of-the-art counterparts.}
}
@article{LIU2023109496,
title = {Joint spatial and scale attention network for multi-view facial expression recognition},
journal = {Pattern Recognition},
volume = {139},
pages = {109496},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109496},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001966},
author = {Yuanyuan Liu and Jiyao Peng and Wei Dai and Jiabei Zeng and Shiguang Shan},
keywords = {Facial expression recognition, Joint spatial and scale attention mechanism, Expression-related regions, Pose variation, Dynamically multi-task learning},
abstract = {Multi-view facial expression recognition (FER) is a challenging task because the appearance of an expression varies greatly due to poses. To alleviate the influences of poses, recently developed methods perform pose normalization, learn pose-invariant features, or learn pose-specific FER classifiers. However, these methods usually rely on a prerequisite pose estimator or expressive region detector that is independent of the subsequent expression analysis. Different from existing methods, we propose a joint spatial and scale attention network (SSA-Net) to localize proper regions for simultaneous head pose estimation (HPE) and FER. Specifically, SSA-Net discovers the regions most relevant to the facial expression at hierarchical scales by a spatial attention mechanism, and the most informative scales are selected in a scale attention learning manner to learn the joint pose-invariant and expression-discriminative representations. Then, we employ a dynamically constrained multi-task learning mechanism with a delicately designed constrain regulation to properly and adaptively train the network to optimize the representations, thus achieving accurate multi-view FER. The effectiveness of the proposed SSA-Net is validated on three multi-view datasets (BU-3DFE, Multi-PIE, and KDEF) and three in-the-wild FER datasets (AffectNet, SFEW, and FER2013). Extensive experiments demonstrate that the proposed framework outperforms existing state-of-the-art methods under both within-dataset and cross-dataset settings, with relative accuracy gains of 2.36%, 1.33%, 3.11%, 2.84%, 15.7%, and 7.57%, respectively.}
}
@article{GONCALVES2023109577,
title = {Regression by Re-Ranking},
journal = {Pattern Recognition},
volume = {140},
pages = {109577},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109577},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002777},
author = {Filipe Marcel Fernandes Gonçalves and Daniel Carlos Guimarães Pedronette and Ricardo {da Silva Torres}},
keywords = {Regression, Re-ranking, Prediction, Manifold, Unsupervised learning},
abstract = {Several approaches based on regression have been developed in the past few years with the goal of improving prediction results, including the use of ranking strategies. Re-ranking has been exploited and successfully employed in several applications, improving rankings by encoding the manifold structure and redefining distances among elements from a dataset. Despite the promising results observed, re-ranking has not been evaluated in regressions tasks. This paper proposes a novel, generic, and customizable framework entitled Regression by Re-ranking (RbR), which explores the ability of re-ranking algorithms in determining relevant rankings of objects in prediction tasks. The framework relies on the integration of a base regressor, unsupervised re-ranking learning techniques, and predictions associated with nearest neighbours weighted according to their ranking positions. The RbR framework was evaluated under a rigorous experimental protocol and presented significant results in improving the prediction when compared to state-of-the-art approaches.}
}
@article{JU2023109553,
title = {Keep an eye on faces: Robust face detection with heatmap-Assisted spatial attention and scale-Aware layer attention},
journal = {Pattern Recognition},
volume = {140},
pages = {109553},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109553},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002534},
author = {Lei Ju and Josef Kittler and Muhammad Awais Rana and Wankou Yang and Zhenhua Feng},
keywords = {Face detection, Supervised spatial attention, Heatmap prediction},
abstract = {Modern anchor-based face detectors learn discriminative features using large-capacity networks and extensive anchor settings. In spite of their promising results, they are not without problems. First, most anchors extract redundant features from the background. As a consequence, the performance improvements are achieved at the expense of a disproportionate computational complexity. Second, the predicted face boxes are only distinguished by a classifier supervised by pre-defined positive, negative and ignored anchors. This strategy may ignore potential contributions from cohorts of anchors labeled negative/ignored during inference simply because of their inferior initialisation, although they can regress well to a target. In other words, true positives and representative features may get filtered out by unreliable confidence scores. To deal with the first concern and achieve more efficient face detection, we propose a Heatmap-assisted Spatial Attention (HSA) module and a Scale-aware Layer Attention (SLA) module to extract informative features using lower computational costs. To be specific, SLA incorporates the information from all the feature pyramid layers, weighted adaptively to remove redundant layers. HSA predicts a reshaped Gaussian heatmap and employs it to facilitate a spatial feature selection by better highlighting facial areas. For more reliable decision-making, we merge the predicted heatmap scores and classification results by voting. Since our heatmap scores are based on the distance to the face centres, they are able to retain all the well-regressed anchors. The experiments obtained on several well-known benchmarks demonstrate the merits of the proposed method.}
}
@article{FAN2023109518,
title = {Markov clustering regularized multi-hop graph neural network},
journal = {Pattern Recognition},
volume = {139},
pages = {109518},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109518},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002182},
author = {Xiaolong Fan and Maoguo Gong and Yue Wu},
keywords = {Graph data mining, Graph neural network, Graph-level representation learning, Graph pattern recognition},
abstract = {Graph Neural Networks (GNNs) have shown great potential for graph data analysis. In this paper, we focus on multi-hop graph neural networks and aim to extend existing models to a high-order multi-hop form for graph-level representation learning. However, such a directly extending method suffers from two limitations, i.e., computational inefficiency and limited representation ability of the multi-hop neighbor. For the former limitation, we utilize an iteration approach to approximate the power of a complex adjacency matrix to achieve linear computational complexity. For the latter limitation, we introduce the Regularized Markov Clustering (R-MCL) to regularize the flow matrix, i.e., the adjacency matrix, in each iteration step. With these two strategies, we construct Markov Clustering Regularized Multi-hop Graph Neural Network (MCMGN) for graph-level representation learning tasks. Specifically, MCMGN consists of a multi-hop message passing phase and a readout phase, where the multi-hop message passing phase aims to learn multi-hop node embedding, and then the readout phase aggregates multi-hop node representations to generate graph embedding for graph-level representation learning tasks. Extensive experiments on eight graph benchmark datasets strongly demonstrate the effectiveness of Markov Clustering Regularized Multi-hop Graph Neural Network, leading to superior performance on graph classification.}
}
@article{ZHU2023109597,
title = {Factorized multi-Graph matching},
journal = {Pattern Recognition},
volume = {140},
pages = {109597},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109597},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002984},
author = {Liangliang Zhu and Xinwen Zhu and Xiurui Geng},
keywords = {Graph matching, Multi-graph matching, Tensor, Factorization},
abstract = {In recent years, multi-graph matching has become a popular yet challenging task in graph theory. There exist two major problems in multi-graph matching, i.e., the cycle-consistency problem, and the high time and space complexity problem. On one hand, the pairwise-based multi-graph matching methods are of low time and space complexity, but in order to keep the cycle-consistency of the matching results, they need additional constraints. Besides, the accuracy of the pairwise-based multi-graph matching is highly dependent on the selected optimization algorithms. On the other hand, the tensor-based multi-graph matching methods can avoid the cycle-consistency problem, while their time and space complexity is extremely high. In this paper, we found the equivalence between the pairwise-based and the tensor-based multi-graph matching methods under some specific circumstances. Based on this finding, we proposed a new multi-graph matching method, which not only avoids the cycle-consistency problem, but also reduces the complexity. In addition, we further improved the proposed method by introducing a lossless factorization of the affinity matrix in the multi-graph matching methods. Synthetic and real data experiments demonstrate the superiority of our method.}
}
@article{QI2023109546,
title = {Unsupervised generalizable multi-source person re-identification: A Domain-specific adaptive framework},
journal = {Pattern Recognition},
volume = {140},
pages = {109546},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109546},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002467},
author = {Lei Qi and Jiaqi Liu and Lei Wang and Yinghuan Shi and Xin Geng},
keywords = {Unsupervised domain generalization person ReID, Domain-specific adaptive normalization},
abstract = {Domain generalization (DG) has attracted much attention in person re-identification (ReID) recently. It aims to make a model trained on multiple source domains generalize to an unseen target domain. Although achieving promising progress, existing methods usually need the source domains to be labeled, which could be a significant burden for practical ReID tasks. In this paper, we turn to investigate “unsupervised” domain generalization for ReID, by assuming that no label is available for any source domains. To address this challenging setting, we propose a simple and efficient domain-specific adaptive framework, and realize it with an adaptive normalization module designed upon the batch and instance normalization techniques. In doing so, we successfully yield reliable pseudo-labels to implement training and also enhance the domain generalization capability of the model as required. In addition, we show that our framework can even be applied to improve person ReID under the settings of supervised domain generalization and unsupervised domain adaptation, demonstrating competitive performance with respect to relevant methods. Extensive experimental study on benchmark datasets is conducted to validate the proposed framework. A significance of our work lies in that it shows the potential of unsupervised domain generalization for person ReID and sets a strong baseline for the further research on this topic. The code is available at https://github.com/Qi5Lei/DSAF.}
}
@article{ALI2023109522,
title = {Boundary-constrained robust regularization for single image dehazing},
journal = {Pattern Recognition},
volume = {140},
pages = {109522},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109522},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002224},
author = {Usman Ali and Jeongdan Choi and KyoungWook Min and Young-Kyu Choi and Muhammad Tariq Mahmood},
keywords = {Image dehazing, Transmission map, Nonconvex energy function, Regularization, Boundary constraints},
abstract = {Generally, for single image dehazing, regularization-based schemes improve the initial transmission map iteratively by using a guidance map as a structural prior. We conducted experiments on a large number of hazy images and observed that a constrained transmission map affects the quality of the recovered image. However, regularization-based methods do not constrain the transmission map to its physically valid range during the iterative process. It degrades its robustness to outliers, and consequently, deteriorates the quality of the recovered image. In addition, conventional methods fuse the structural information of the guidance and initial transmission map without considering any structural differences between them. To address these issues, in this paper, we present a robust regularization scheme that constraints the transmission map during its enhancement. In the proposed scheme, a nonconvex energy function is constructed that leverages the mutual structural information of the guidance and transmission map. The nonconvex problem is solved by a majorize-minimization algorithm, and the intermediate transmission maps are constrained through the appropriate lower and upper bounds. The retrieved transmission map has better edge-preserving properties, and ultimately, results in a high-quality haze-free image that has faithful colors and fine details. The proposed scheme is tested on benchmark datasets and results are evaluated through quantitative metrics. The comparative analysis has revealed the effectiveness of the proposed scheme.}
}
@article{ZHOU2023109602,
title = {SurroundNet: Towards effective low-light image enhancement},
journal = {Pattern Recognition},
volume = {141},
pages = {109602},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109602},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003035},
author = {Fei Zhou and Xin Sun and Junyu Dong and Xiao Xiang Zhu},
keywords = {Image processing, Image enhancement, Convolution Neural Network, Surround function, Lightweight},
abstract = {Although Convolution Neural Networks (CNNs) have made substantial progress in the low-light image enhancement task, one critical problem of CNNs is the paradox of model complexity and performance. This paper presents a novel SurroundNet that only involves less than 150K parameters (about 80–98 percent size reduction compared to SOTAs) and achieves very competitive performance. The proposed network comprises several Adaptive Retinex Blocks (ARBlock), which can be viewed as a novel extension of Single Scale Retinex in feature space. The core of our ARBlock is an efficient illumination estimation function called Adaptive Surround Function (ASF). It can be regarded as a general form of surround functions and be implemented by convolution layers. In addition, we also introduce a Low-Exposure Denoiser (LED) to smooth the low-light image before the enhancement. We evaluate the proposed method on two real-world low-light datasets. Experimental results demonstrate the superiority of our submitted SurroundNet in both performance and network parameters against State-of-the-Art low-light image enhancement methods. The code is available at https://github.com/ouc-ocean-group/SurroundNet.}
}
@article{HOU2023109558,
title = {CANet: Contextual Information and Spatial Attention Based Network for Detecting Small Defects in Manufacturing Industry},
journal = {Pattern Recognition},
volume = {140},
pages = {109558},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109558},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002583},
author = {Xiuquan Hou and Meiqin Liu and Senlin Zhang and Ping Wei and Badong Chen},
keywords = {Small defect detection, Contextual information, Spatial attention, Multi-scale feature fusion, Automatic visual inspection},
abstract = {Despite the promising development of Automatic Visual Inspection (AVI) in the manufacturing industry, detecting small-sized defects with fewer pixels coverage remains a challenging problem due to its insufficient attention and lack of semantic information. Most exsiting convolutional inspection methods overlook the long-range dependence of context and lack adaptive fusion strategies to exploit heterogeneous features. To address these issues in AVI, this paper proposes a novel contextual information and spatial attention based network (CANet), which consists of two steps, namely CAblock and LaplacianFPN, for effective perception and exploitation of small defect features. Specifically, CAblock extracts semantic information with rich context by encoding spatial long-range dependence and decoding contextual information as channel-specific bias through a Spatial Attention Encoder (SAE) and a Context Block Decoder (CBD), respectively. LaplacianFPN further performs adaptive feature fusion considering both feature consistency and heterogeneity via two parallel branches. As a benchmark, a self-built Engine Surface Defects (ESD) dataset collected in real industry containing 89.70% small defects is constructed. Experimental results show that CANet achieves mAP-50 improvements of 1.5% and 4.3% compared to state-of-the-art methods on NEU-DET and ESD, which demonstrates the effectiveness of the proposed method. The code is now available at https://github.com/xiuqhou/CANet.}
}
@article{CHEN2023109506,
title = {Multi-task semi-supervised crowd counting via global to local self-correction},
journal = {Pattern Recognition},
volume = {140},
pages = {109506},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109506},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002066},
author = {Jiwei Chen and Zengfu Wang},
keywords = {Crowd counting, Semi-supervised, Pseudo labels, Global to local self-correction},
abstract = {In this paper, we propose a novel multi-task semi-supervised method. To sufficiently exploit massive unlabeled data, multi-task pseudo-labels and global to local self-correction strategy are proposed. Specifically, labeled images and massive amounts of unlabeled images with proposed multi-task pseudo-labels are leveraged for model optimization. The density level of the whole image is predicted in classification task. The density is estimated in density regression task. The crowd area is segmented out in segmentation task. To suppress incorrect predictions caused by the inevitable noises from some unlabeled data misleading the model, the counting relationship between classification task and density task is exploited to propose the global self-correction strategy, and the semantic consistency between density task and segmentation task is mined to propose the local self-correction strategy. The classification task and segmentation task contribute in generating the final highly refined density map from the density task. Extensive experiments on six benchmark datasets indicate the superiority of our method over the SOTA methods in semi-supervised paradigm.}
}
@article{LI2023109534,
title = {Towards better long-tailed oracle character recognition with adversarial data augmentation},
journal = {Pattern Recognition},
volume = {140},
pages = {109534},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109534},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002340},
author = {Jing Li and Qiu-Feng Wang and Kaizhu Huang and Xi Yang and Rui Zhang and John Y. Goulermas},
keywords = {Oracle character recognition, Long tail, Data imbalance, Data augmentation, Mixup strategy, Generative adversarial networks},
abstract = {Deciphering oracle bone script is of great significance to the study of ancient Chinese culture as well as archaeology. Although recent studies on oracle character recognition have made substantial progress, they still suffer from the long-tailed data situation that results in a noticeable performance drop on the tail classes. To mitigate this issue, we propose a generative adversarial framework to augment oracle characters in the problematic classes. In this framework, the generator produces synthetic data through convex combinations of all the available samples in the corresponding classes, and is further optimized through adversarial learning with the classifier and simultaneously the discriminator. Meanwhile, we introduce Repatch to generalize samples in the generator. Since tail classes do not have sufficient data for convex combinations, we propose the TailMix mechanism to generate suitable tail class samples from other classes. Experimental results show that our proposed algorithm obtains remarkable performance in oracle character recognition and achieves new state-of-the-art average (total) accuracy with 86.03% (89.46%), 86.54% (93.86%), 95.22% (96.17%) on the three datasets Oracle-AYNU, OBC306 and Oracle-20K, respectively.}
}
@article{MA2023109585,
title = {Crowd counting from single images using recursive multi-pathway zooming and foreground enhancement},
journal = {Pattern Recognition},
volume = {141},
pages = {109585},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109585},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002868},
author = {Junjie Ma and Yaping Dai and Zhiyang Jia and Fuchun Sun and Yap-Peng Tan and Jun Liu},
keywords = {Crowd counting, Density estimation, Multi-Pathway zooming, Foreground enhancement},
abstract = {Crowd counting is a challenging task due to many challenges such as scale variations and noisy background. To handle these challenges, we propose a novel framework named Multi-Pathway Zooming Network (MZNet) in this paper. The proposed framework recursively optimizes multi-scale features using multiple zooming pathways and progressively enhances the foreground information to improve crowd counting performance. Each zooming pathway comprises two zooming directions, zooming in and zooming out. Convolutional features at different resolutions are propagated to optimize the context information at each specific level. By sequentially integrating and interacting multi-observation information, the optimized features are powerful in handling the scale variation issue, and thus the crowd counting performance can be enhanced. To address the noisy background in many scenarios, we also introduce a new scheme to enhance the foreground information by incorporating a masked input image into the network, which is formed by a mask that element-wise multiplies with the original image. Finally, the context information, incorporated with an output density map, is recursively finetuned in our network to boost the counting performance. Extensive experiments evaluated on challenging benchmark datasets show competitive performances for both crowded and sparse scenarios.}
}
@article{WANG2023109562,
title = {Coloring anime line art videos with transformation region enhancement network},
journal = {Pattern Recognition},
volume = {141},
pages = {109562},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109562},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002625},
author = {Ning Wang and Muyao Niu and Zhi Dou and Zhihui Wang and Zhiyong Wang and Zhaoyan Ming and Bin Liu and Haojie Li},
keywords = {, Anime video, Synthesis, GAN},
abstract = {Automatic colorization of anime line art videos aims to produce color frames given line art frames and reference color images, which is challenging due to various motions and geometric transformations across frame sequences. Existing methods usually utilize the feature maps of reference images directly and treat all the regions in an image equally. However, this may overlook the details of the regions undergoing geometric transformations. To emphasize the regions with significant transformations between the reference and target frames, we propose a Transformation Region Enhancement Network (TRE-Net) to exploit useful reference information and enhance the colorization of key transformation regions with Region Localization Module (RLM) and Feature Enhancement Module (FEM). Specifically, we propose Multi-scale Euclidean Distance Difference (Multi-scale EDD) Maps in RLM which effectively locate geometric transformation regions by contrasting the Euclidean Distance Maps of two line arts and aggregating representations at multiple scales of the network. In addition, FEM is devised to enhance feature learning in the regions with geometric transformation and to ensure proper color alignment. FEM learns locally enhanced features through an attention-gating operation at a low computational cost. With the well-represented key geometric transformation regions, our method exploits the multi-scale reference information well for color alignment, thus produces perceptually pleasing frames. Comprehensive experimental results show that our proposed method is superior to existing methods in terms of the overall quality of colorized anime line art videos.}
}
@article{AN2023109510,
title = {Patch loss: A generic multi-scale perceptual loss for single image super-resolution},
journal = {Pattern Recognition},
volume = {139},
pages = {109510},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109510},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002108},
author = {Tai An and Binjie Mao and Bin Xue and Chunlei Huo and Shiming Xiang and Chunhong Pan},
keywords = {Single-image super-resolution, Multi-scale loss functions, Image visual perception, Perceptual metrics},
abstract = {In single image super-resolution (SISR), although PSNR is a key metric for signal fidelity, images with high PSNR do not necessarily render high visual quality. As a result, current perception-driven SISR methods employ perceptual metrics close to the human eye to measure the quality of the generated images. Unfortunately, the perceptual loss and adversarial loss, widely used by the perception-driven SISR methods, still underperform on these non-differentiable perceptual metrics. To this end, we propose a generic multi-scale perceptual loss, i.e., the patch loss, which can be easily plugged into off-the-shelf SISR methods to improve a broad range of perceptual metrics. Specifically, the proposed patch loss minimizes the multi-scale similarity of image patches and enhances the restoration of regions with complex textures and sharp edges via parameter-free adaptive patch-wise attention. Our proposed patch loss introduces more realistic details compared to the perceptual loss and fewer artifacts compared to the adversarial loss.}
}
@article{LIU2023109550,
title = {Learn from each other to Classify better: Cross-layer mutual attention learning for fine-grained visual classification},
journal = {Pattern Recognition},
volume = {140},
pages = {109550},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109550},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002509},
author = {Dichao Liu and Longjiao Zhao and Yu Wang and Jien Kato},
keywords = {Fine-grained recognition, Image classification, Deep features},
abstract = {Fine-grained visual classification (FGVC) is valuable yet challenging. The difficulty of FGVC mainly lies in its intrinsic inter-class similarity, intra-class variation, and limited training data. Moreover, with the popularity of deep convolutional neural networks, researchers have mainly used deep, abstract, semantic information for FGVC, while shallow, detailed information has been neglected. This work proposes a cross-layer mutual attention learning network (CMAL-Net) to solve the above problems. Specifically, this work views the shallow to deep layers of CNNs as “experts” knowledgeable about different perspectives. We let each expert give a category prediction and an attention region indicating the found clues. Attention regions are treated as information carriers among experts, bringing three benefits: (i) helping the model focus on discriminative regions; (ii) providing more training data; (iii) allowing experts to learn from each other to improve the overall performance. CMAL-Net achieves state-of-the-art performance on three competitive datasets: FGVC-Aircraft, Stanford Cars, and Food-11. The source code is available at https://github.com/Dichao-Liu/CMAL}
}
@article{LIU2023109519,
title = {MEP-3M: A large-scale multi-modal E-commerce product dataset},
journal = {Pattern Recognition},
volume = {140},
pages = {109519},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109519},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002194},
author = {Fan Liu and Delong Chen and Xiaoyu Du and Ruizhuo Gao and Feng Xu},
keywords = {Dataset, E-commerce product classification, Fine-grained learning, Hierarchical classification, Automatic Checkout},
abstract = {The product categories are vital for the E-commerce platforms due to the core applications on automatic product category assignment, personalized product recommendations, etc. In this paper, we construct a large-scale Multi-modal E-commerce Products classification dataset MEP-3M, which is large-scale, hierarchical-categorized, multi-modal, fine-grained, and long-tailed. Statistically, MEP-3M consists of over 3 million products, thus achieves the largest data scale in comparison to the existing E-commerce product datasets. The products in MEP-3M are represented in three modalities: image, textual description, and OCR text, and labeled with tree-like labels. The third level labels are extremely fine-grained. In addition, we exploit four novel practical tasks on this dataset, Product classification, Hierarchical Product Classification, Fine-grained Product Classification, and Product Representation Learning. For each task, we present some image-only, text-only, and multi-modal baseline performances for further researches. The MEP-3M dataset will be released at https://github.com/ChenDelong1999/MEP-3M.}
}
@article{WANG2023109523,
title = {Feature clustering-Assisted feature selection with differential evolution},
journal = {Pattern Recognition},
volume = {140},
pages = {109523},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109523},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002236},
author = {Peng Wang and Bing Xue and Jing Liang and Mengjie Zhang},
keywords = {Differential evolution, Feature selection, Multiple optimal feature subsets, Classification},
abstract = {Modern data collection technologies may produce thousands of or even more features in a single dataset. The high dimensionality of data poses a barrier to determining discriminating features due to the curse of dimensionality. Thanks to the global search ability, many population-based feature selection approaches have been proposed. However, very few studies pay attention on that a feature selection task has multiple optimal feature subsets. To search for multiple optimal feature subsets, we propose a feature clustering-assisted feature selection method. The proposed method employs the knowledge of correlation measures to group features. And, this correlation knowledge is embedded into the encoding method and the search process. A niching-based mutation operator is also used to explore the vicinity of a target individual. The aim is to find different feature subsets with very similar or the same classification performance. In addition, a modification operator is proposed aiming to increase the population diversity to improve the feature selection performance. The experiments on 16 datasets show that the proposed algorithm outperforms other popular feature selection methods in terms of classification accuracy and feature subset size.}
}
@article{LI2023109551,
title = {Dense light field reconstruction based on epipolar focus spectrum},
journal = {Pattern Recognition},
volume = {140},
pages = {109551},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109551},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002510},
author = {Yaning Li and Xue Wang and Hao Zhu and Guoqing Zhou and Qing Wang},
keywords = {Light field representation, Epipolar focus spectrum (EFS), Dense light field reconstruction, Depth independent, Frequency domain},
abstract = {Existing light field (LF) representations, such as epipolar plane image (EPI) and sub-aperture images, do not consider the structural characteristics across the views, so they usually require additional disparity and spatial structure cues for follow-up tasks. Besides, they have difficulties dealing with occlusions or large disparity scenes. To this end, this paper proposes a novel Epipolar Focus Spectrum (EFS) representation by rearranging the EPI spectrum. Different from the classical EPI representation where an EPI line corresponds to a specific depth, there is a one-to-one mapping from the EFS line to the view. By exploring the EFS sampling task, the analytical function is derived for constructing a non-aliasing EFS. To demonstrate its effectiveness, we develop a trainable EFS-based pipeline for light field reconstruction, where a dense light field can be reconstructed by compensating the missing EFS lines given a sparse light field, yielding promising results with cross-view consistency, especially in the presence of severe occlusion and large disparity. Experimental results on both synthetic and real-world datasets demonstrate the validity and superiority of the proposed method over SOTA methods.}
}
@article{THAKUR2023109603,
title = {Multi scale pixel attention and feature extraction based neural network for image denoising},
journal = {Pattern Recognition},
volume = {141},
pages = {109603},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109603},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003047},
author = {Ramesh Kumar Thakur and Suman Kumar Maji},
keywords = {Blind Gaussian noise removal, Deep convolutional residual network, Convolutional layer, Residual architecture, Dilated convolution, Skip connection},
abstract = {In this paper, we propose a blind Gaussian denoising network that utilize the features of the input image and its negative for generating denoised output of the same. The proposed network is a dual path model which employs a multi-scale pixel attention (MSPA) block on one path and a multi-scale feature extraction (MSFE) block on another. The concept of using the features of a negative image (that it highlights the low contrast region) in blind Gaussian denoising network is, to the best of our knowledge, a first such attempt. The proposed MSPA and MSFE blocks are designed to focus on the features of the image at multiple scales. The MSPA block focuses on the important features of the negative of the input image whereas the MSFE block focuses on extracting features of the input noisy image. The features of both the images are then combined and a final residual noise is obtained, subtracting which from the input noisy image produces the final denoised result. The proposed network is lightweight and fast, due to the low number of convolutional layers involved, and produces superior results (both quantitatively and qualitatively) when compared with various traditional and learning based blind Gaussian denoising techniques. The code of this paper can be downloaded from https://github.com/RTSIR/NIFBGDNet.}
}
@article{WANG2023109559,
title = {Identifying effective trajectory predictions under the guidance of trajectory anomaly detection model},
journal = {Pattern Recognition},
volume = {140},
pages = {109559},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109559},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002595},
author = {Chunnan Wang and Chen Liang and Xiang Chen and Hongzhi Wang},
keywords = {Stochastic trajectory prediction, Anomaly detection, Trajectory anomaly detection, Automated machine learning},
abstract = {Trajectory Prediction (TP) is an important research topic in computer vision and robotics fields. Recently, many stochastic TP models have been proposed to deal with this problem and have achieved better performance than the traditional models with deterministic trajectory outputs. However, these stochastic models can generate a number of future trajectories with different qualities. They are lack of self-evaluation ability, that is, to examine the rationality of their prediction results, thus failing to guide users to identify high-quality ones from their candidate results. This hinders them from playing their best in real applications. In this paper, we make up for this defect and propose TPAD, a novel TP evaluation method based on the trajectory Anomaly Detection (AD) technique. In TPAD, we firstly combine the Automated Machine Learning (AutoML) technique and the experience in the AD and TP field to automatically design an effective trajectory AD model. Then, we utilize the learned trajectory AD model to examine the rationality of the predicted trajectories, and screen out good TP results for users. Extensive experimental results demonstrate that TPAD can effectively identify near-optimal prediction results, improving stochastic TP models’ practical application effect.}
}
@article{ZHANG2023109507,
title = {Doubly contrastive representation learning for federated image recognition},
journal = {Pattern Recognition},
volume = {139},
pages = {109507},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109507},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002078},
author = {Yupei Zhang and Yunan Xu and Shuangshuang Wei and Yifei Wang and Yuxin Li and Xuequn Shang},
keywords = {Image recognition, Doubly contrastive learning, Federated machine learning, Representation learning, Non-IID data classification},
abstract = {This paper focuses on the problem of personalized federated learning (FL) with the schema of contrastive learning (CL), which is to implement collaborative pattern classification by many clients. The traditional FL frameworks mostly facilitate the global model for the server and the local models for the clients to be similar, often ignoring the data heterogeneity of the clients. Aiming at achieving better performance in clients, this study introduces a personalized federated contrastive learning model, dubbed PerFCL, by proposing a new approach to doubly contrastive representation learning (DCL). Concretely, PerFCL borrows the DCL scheme, where one CL loss compares the shared parts of local models with the global model and the other CL loss compares the personalized parts of local models with the global model. To encourage the difference between the two parts, we created a double optimization problem composed of maximizing the comparison agreement for the former and minimizing the comparison agreement for the latter. We evaluated the proposed model on three publicly available data sets for federated image classification. Experiment results show that PerFCL benefits from the proposed DCL strategy and performs better than the state-of-the-art federated-learning models.}
}
@article{GAO2023109535,
title = {Transfer learning on stratified data: joint estimation transferred from strata},
journal = {Pattern Recognition},
volume = {140},
pages = {109535},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109535},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002352},
author = {Yimiao Gao and Yuehan Yang},
keywords = {Transfer learning, Stratified data, Penalized regression, Semiparametric regression},
abstract = {This paper studies the target model with the help of auxiliary models from different but possibly related groups. Inspired by transfer learning, we propose a method called joint estimation transferred from strata (JETS). To obtain a sparse solution, JETS constructs a penalized framework combining a term that penalizes the target model and an additional term that penalizes the differences between auxiliary models and the target model. In this way, JETS overcomes the challenge caused by the limited samples in high-dimensional study, and obtains stable and accurate estimates regardless of whether auxiliary samples contain noisy information. We demonstrate that this method enjoys the computational advantage of the traditional methods such as the lasso. During simulations and applications, the proposed method is compared with several existing methods and JETS outperforms others.}
}
@article{TABEALHOJEH2023109563,
title = {RMAML: Riemannian meta-learning with orthogonality constraints},
journal = {Pattern Recognition},
volume = {140},
pages = {109563},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109563},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002637},
author = {Hadi Tabealhojeh and Peyman Adibi and Hossein Karshenas and Soumava Kumar Roy and Mehrtash Harandi},
keywords = {Meta-learning, Geometry-aware optimization, Riemannian manifolds, Few-shot image classification},
abstract = {Meta-learning is the core capability that enables intelligent systems to rapidly generalize their prior experience to learn new tasks. In general, the optimization-based methods formalize the meta-learning as a bi-level optimization problem, that is a nested optimization framework, in which meta-parameters are optimized (or learned) at the outer-level, while the inner-level optimizes the task-specific parameters. In this paper, we introduce RMAML, a meta-learning method that enforces orthogonality constraints to the bi-level optimization problem. We develop a geometry aware framework that generalizes the bi-level optimization problem to the Riemannian (constrained) setting. Using the Riemannian operations such as orthogonal projection, retraction and parallel transport, the bi-level optimization is reformulated so that it respects the Riemannian geometry. Moreover, we observe that a superior stable optimization and an improved generalization ability can be achieved when the parameters and meta-parameters of the method are modeled using a Stiefel Manifold. We empirically show that RMAML can easily reach competitive performances against several state of the art algorithms for few-shot classification and consistently outperforms its Euclidean counterpart, MAML. For example, by using the geometry of the Stiefel manifold to structure the fully-connected layers in a deep neural network, a 7% increase in single-domain few-shot classification accuracy is achieved. For the cross-domain few-shot learning, RMAML outperforms MAML by up to 9% of accuracy. Our ablation study also demonstrates the effectiveness of RMAML over MAML in terms of higher accuracy with a reduced number of tasks and (or) inner-level updates.}
}
@article{ALFARO2023109590,
title = {Pairwise learning for the partial label ranking problem},
journal = {Pattern Recognition},
volume = {140},
pages = {109590},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109590},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002911},
author = {Juan C. Alfaro and Juan A. Aledo and José A. Gámez},
keywords = {Preference learning, (Partial) label ranking, Supervised classification, Pairwise decomposition, Optimal bucket order problem},
abstract = {The partial label ranking problem is a particular preference learning scenario that focuses on learning preference models from data, such that they predict a complete ranking with ties defined over the values of the class variable for a given input instance. This work proposes to transform the rankings into preference relations among pairs of class labels and to learn a standard classifier for each of them. This classifier is then used to estimate the probability of each event from the preference relation between the two compared class labels. Finally, the probabilities obtained for each preference comparison are used to compute a preference matrix utilized to solve the corresponding rank aggregation problem and so obtain the ranking among all the class labels. The experimental evaluation shows that the proposed method is ranked ahead of competing algorithms in accuracy while obtaining similar CPU time results.}
}
@article{LUO2023109598,
title = {Self-information of radicals: A new clue for zero-shot Chinese character recognition},
journal = {Pattern Recognition},
volume = {140},
pages = {109598},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109598},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002996},
author = {Guo-Feng Luo and Da-Han Wang and Xia Du and Hua-Yi Yin and Xu-Yao Zhang and Shunzhi Zhu},
keywords = {Chinese character recognition, Zero-shot learning, Self-information of radicals, Character uncertainty elimination, Radical information embedding},
abstract = {Zero-shot Chinese character recognition (ZSCCR) is an important research topic in Chinese character recognition as it attempts to recognize unseen Chinese characters. As basic components and mid-level representations, radicals are significant for ZSCCR. However, previous methods treat the importance of radicals equally, ignoring the different contributions of radicals in distinguishing characters. In this paper, we propose the self-information of radicals (SIR) to measure the importance of radicals in recognizing Chinese characters. The proposed SIR can be easily adopted by two commonly used radical-based ZSCCR frameworks, i.e., sequence matching based and attribute embedding based. For sequence matching based ZSCCR, we propose a novel Chinese character uncertainty elimination (CUE) framework to alleviate the radical sequence mismatch problem. For attribute embedding based ZSCCR, we propose a novel radical information embedding (RIE) method that can highlight the importance of indispensable radicals and weaken the influence of some unnecessary radicals. We conducted comprehensive experiments on the CASIA-HWDB, ICDAR2013, CTW datasets, and AHCDB datasets to evaluate the proposed method. Experiments show that our proposed methods can achieve superior performance to the state-of-the-art methods, which demonstrate the effectiveness and the high extensibility of the proposed SIR.}
}
@article{WANG2023109547,
title = {AA-trans: Core attention aggregating transformer with information entropy selector for fine-grained visual classification},
journal = {Pattern Recognition},
volume = {140},
pages = {109547},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109547},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002479},
author = {Qi Wang and JianJun Wang and Hongyu Deng and Xue Wu and Yazhou Wang and Gefei Hao},
keywords = {Fine-grained visual, Image classification, Vision transformer, Attention aggregator, Information entropy},
abstract = {The task of fine-grained visual classification (FGVC) is to distinguish targets from subordinate classifications. Since fine-grained images have the inherent characteristic of large inter-class variances and small intra-class variances, it is considered an extremely difficult task. Most existing approaches adopt CNN-based networks as feature extractors, which causes the extracted discriminative regions to contain most parts of the object in this way, thus failing to locate the really important parts. Recently, the vision transformer (ViT) has demonstrated its power on a wide range of image tasks, which uses an attention mechanism to capture global contextual information to establish a remote dependency on the target and thus extract more powerful features. Nevertheless, the ViT model still focuses more on global coarse-grained information rather than local fine-grained information, which may lead to its undesirable performance in fine-grained image classification. To this end, we redesigned an attention aggregating transformer (AA-Trans) to better capture minor differences among images by improving the ViT structure in this paper. In detail, we propose a core attention aggregator (CAA), which enables better information sharing between each transformer layer. Besides, we further propose an innovative information entropy selector (IES) to guide the network in acquiring discriminative parts of the image precisely. Extensive experiments show that our proposed model structure can achieve a new state-of-the-art performance on several mainstream datasets.}
}
@article{LIU2023109568,
title = {Distributional and spatial-temporal robust representation learning for transportation activity recognition},
journal = {Pattern Recognition},
volume = {140},
pages = {109568},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109568},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002686},
author = {Jing Liu and Yang Liu and Wei Zhu and Xiaoguang Zhu and Liang Song},
keywords = {Transportation activity recognition, Multimodal sensing, Deep learning, Statistical feature, Spatial-temporal feature},
abstract = {Transportation activity recognition (TAR) provides valuable support for intelligent transportation applications, such as urban transportation planning, driving behavior analysis, and traffic prediction. There are many advantages of movable sensor-based TAR, and the key challenge is to capture salient features from segmented data for representing diverse patterns of activity. Although existing methods based on statistical information are efficient, they usually rely on domain knowledge to construct high-quality features manually. Likewise, the methods based on spatial-temporal relationships achieve good performance but fail to extract statistical features. The features extracted by these two methods have proven to be crucial for the classification of activity. How to combine them to acquire a more robust representation remains an open question. In this work, we introduce a novel parallel model named Distributional and Spatial-Temporal Robust Representation (DSTRR), which combines automatic learning of statistical, spatial, and temporal features into a unified framework. This model leads to three optimized subnets and thus obtains a robust representation specific to TAR. Extensive experiments performed on three public datasets show that DSTRR is a state-of-the-art method compared with the baseline methods. The results of ablation study and visualization not only demonstrate the effectiveness of each component in DSTRR, but also show the model remains robust to a wide range of parameter variations.}
}
@article{WANG2023109516,
title = {A uniform transformer-based structure for feature fusion and enhancement for RGB-D saliency detection},
journal = {Pattern Recognition},
volume = {140},
pages = {109516},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109516},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002169},
author = {Yue Wang and Xu Jia and Lu Zhang and Yuke Li and James H. Elder and Huchuan Lu},
keywords = {Saliency detection, RGB-D image, Transformer, Attention},
abstract = {RGB-D saliency detection integrates information from both RGB images and depth maps to improve the prediction of salient regions under challenging conditions. The key to RGB-D saliency detection is to fully mine and fuse information at multiple scales across the two modalities. Previous approaches tend to apply the multi-scale and multi-modal fusion separately via local operations, which fails to capture long-range dependencies. Here we propose a transformer-based structure to address this issue. The proposed architecture is composed of two modules: an Intra-modality Feature Enhancement Module (IFEM) and an Inter-modality Feature Fusion Module (IFFM). IFFM conducts a sufficient feature fusion by integrating features from multiple scales and two modalities over all positions simultaneously. IFEM enhances feature on each scale by selecting and integrating complementary information from other scales within the same modality before IFFM. We show that transformer is a uniform operation which presents great efficacy in both feature fusion and feature enhancement, and simplifies the model design. Extensive experimental results on five benchmark datasets demonstrate that our proposed network performs favorably against most state-of-the-art RGB-D saliency detection methods. Furthermore, our model is efficient for having relatively smaller FLOPs and model size compared with other methods.}
}
@article{KORBAN2023109595,
title = {Semantics-enhanced early action detection using dynamic dilated convolution},
journal = {Pattern Recognition},
volume = {140},
pages = {109595},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109595},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002960},
author = {Matthew Korban and Xin Li},
keywords = {Early action detection, Action semantics, Dilated convolutional network},
abstract = {This paper proposes a new pipeline to perform early action detection from skeleton-based untrimmed videos. Our pipeline includes two new technical components. The first is a new Dynamic Dilated Convolutional Network (DDCN), which supports dynamic temporal sampling and makes feature learning more robust against temporal scale variance in action sequences. The second is a new semantic referencing module, which uses identified objects in the scene and their co-existence relationship with actions to adjust the probabilities of inferred actions. Such semantic guidance can help distinguish many ambiguous actions, which is a core challenge in the early detection of incomplete actions. Our pipeline achieves state-of-the-art performance in early action detection in two widely used skeleton-based untrimmed video benchmarks. The source codes are available at: https://github.com/Powercoder64/DDCN_SRM.}
}
@article{ZHANG2023109544,
title = {Motif Entropy Graph Kernel},
journal = {Pattern Recognition},
volume = {140},
pages = {109544},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109544},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002443},
author = {Liang Zhang and Longqiang Yi and Yu Liu and Cheng Wang and Da Zhou},
keywords = {Graph representation, Motif entropy, Graph kernel, Wasserstein distance},
abstract = {Graph kernels have achieved excellent performance in graph classification tasks. In this paper, we propose a novel deep motif entropy graph kernel for the purpose of graph classification. For better capturing the differences between substructures, we gauge detailed information through a family of K-layer expansion motifs rooted at each node and combine the Weisfeiler-Lehman algorithm to subdivide motifs, which is further enhanced by motif entropy. Experiments on eight graph-structured datasets demonstrate that our method is able to outperform the state-of-the-art kernel methods for the tasks of graph classification.}
}
@article{QIAO2023109539,
title = {Hierarchical disentangling network for object representation learning},
journal = {Pattern Recognition},
volume = {140},
pages = {109539},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109539},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300239X},
author = {Shishi Qiao and Ruiping Wang and Shiguang Shan and Xilin Chen},
keywords = {Object understanding, Hierarchical learning, Representation disentanglement, Generative adversarial network, Network interpretability},
abstract = {An object can be described as the combination of primary visual attributes. Disentangling such underlying primitives is the long-term objective of representation learning. It is observed that categories have natural hierarchical characteristics, i.e., any two objects can share some common primitives at a particular category level while possess unique traits at another. However, previous works usually operate in a flat manner (i.e., at a particular level) to disentangle the representations of objects. Even though they may obtain the primitives to constitute objects as the categories at that level, their results are obviously not efficient and complete. In this paper, we propose a Hierarchical Disentangling Network (HDN) to exploit the rich hierarchical characteristics among categories to divide the disentangling process in a coarse-to-fine manner (i.e., level-wise), such that each level only focuses on learning the specific representations and finally the common and unique representations at all levels jointly constitute the raw object. Specifically, HDN is designed based on an encoder-decoder architecture. To simultaneously ensure the level-wise disentanglement and interpretability of the encoded representations, a novel hierarchical Generative Adversarial Network (GAN) is introduced. Quantitative and qualitative evaluations on popular object datasets validate the effectiveness of our method.}
}
@article{BAYER2023109520,
title = {An incremental facility location clustering with a new hybrid constrained pseudometric},
journal = {Pattern Recognition},
volume = {141},
pages = {109520},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109520},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002200},
author = {Tomáš Bayer and Ivana Kolingerová and Markéta Potůčková and Miroslav Čábelka and Eva Štefanová},
keywords = {Facility location, Clusterization, Pseudometric, Detection, Simplification, Point cloud},
abstract = {The Euclidean metric, one of the classical similarity measures applied in clustering algorithms, has drawbacks when applied to spatial clustering. The resulting clusters are spherical and similarly sized, and the edges of objects are considerably smoothed. This paper proposes a novel hybrid constrained pseudometric formed by the linear combination of the Euclidean metric and a pseudometric plus penalty. The pseudometric is used in a new deterministic incremental heuristic facility location algorithm (IHFL). Our method generates larger, isotropic, and partially overlapping clusters of different sizes and spatial densities, better adapting to the surface complexity than the classical non-deterministic clustering. Cluster properties are used to derive new features for supervised/unsupervised learning. Possible applications are the classification of point clouds, their simplification, detection, filtering, and extraction of different structural patterns or sampled objects. Experiments were run on point clouds derived from laser scanning and images.}
}
@article{FOUCART2023109600,
title = {Evaluating participating methods in image analysis challenges: Lessons from MoNuSAC 2020},
journal = {Pattern Recognition},
volume = {141},
pages = {109600},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109600},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003011},
author = {Adrien Foucart and Olivier Debeir and Christine Decaestecker},
keywords = {Challenge, Competition, Digital pathology, Image analysis, Performance metrics},
abstract = {Biomedical image analysis competitions often rank the participants based on a single metric that combines assessments of different aspects of the task at hand. While this is useful for declaring a single winner for a competition, it makes it difficult to assess the strengths and weaknesses of participating algorithms. By involving multiple capabilities (detection, segmentation and classification) and releasing the prediction masks provided by several teams, the MoNuSAC 2020 challenge provides an interesting opportunity to look at what information may be lost by using entangled metrics. We analyse the challenge results based on the “Panoptic Quality” (PQ) used by the organizers, as well as on disentangled metrics that assess the detection, classification and segmentation abilities of the algorithms separately. We show that the PQ hides interesting aspects of the results, and that its sensitivity to small changes in the prediction masks makes it hard to interpret these results and to draw useful insights from them. Our results also demonstrate the necessity to have access, as much as possible, to the raw predictions provided by the participating teams so that challenge results can be more easily analysed and thus more useful to the research community.}
}
@article{HEDEGAARD2023109528,
title = {Continual spatio-temporal graph convolutional networks},
journal = {Pattern Recognition},
volume = {140},
pages = {109528},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109528},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002285},
author = {Lukas Hedegaard and Negar Heidari and Alexandros Iosifidis},
keywords = {Graph convolutional networks, Continual inference, Efficient deep learning, Skeleton-based action recognition},
abstract = {Graph-based reasoning over skeleton data has emerged as a promising approach for human action recognition. However, the application of prior graph-based methods, which predominantly employ whole temporal sequences as their input, to the setting of online inference entails considerable computational redundancy. In this paper, we tackle this issue by reformulating the Spatio-Temporal Graph Convolutional Neural Network as a Continual Inference Network, which can perform step-by-step predictions in time without repeat frame processing. To evaluate our method, we create a continual version of ST-GCN, CoST-GCN, alongside two derived methods with different self-attention mechanisms, CoAGCN and CoS-TR. We investigate weight transfer strategies and architectural modifications for inference acceleration, and perform experiments on the NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets. Retaining similar predictive accuracy, we observe up to 109× reduction in time complexity, on-hardware accelerations of 26×, and reductions in maximum allocated memory of 52% during online inference.}
}
@article{HAO2023109504,
title = {Multi-dimensional Graph Neural Network for Sequential Recommendation},
journal = {Pattern Recognition},
volume = {139},
pages = {109504},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109504},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002042},
author = {Yongjing Hao and Jun Ma and Pengpeng Zhao and Guanfeng Liu and Xuefeng Xian and Lei Zhao and Victor S. Sheng},
keywords = {Sequential Recommendation, Graph Neural Networks, Self-attention Networks, Graph Embedding},
abstract = {Graph neural networks (GNNs) technology has been widely used in recommendation systems because most information in recommendation systems has a graph structure in nature, and GNNs have advantages in graph representation learning. In sequential recommendation, the relationships between interacting items can be constructed as an isomorphic graph, and (GNNs) can capture high-order information between graph nodes. Many models have used graph-based methods for sequential recommendation, and achieved great success. However, the existing research only considers the number of interactions between items when constructing the item graph. As such, revisions are needed to capture the multi-dimensional transformation relationships between items. Hence, we emphasize the importance of multi-dimensional information, and we propose a Category and Time information integrated Graph Neural Network (CTGNN), which combines the item category and interaction time information with a multi-layer graph convolution network to form multi-dimensional fine-grained item representations. In addition, we design a temporal self-attention network to model the dynamic user preference and make the next-item recommendation. Finally, we conduct extensive experiments on three real-world datasets, and the results demonstrate the excellent performance of the proposed model.}
}
@article{DING2023109532,
title = {An enhanced vision transformer with wavelet position embedding for histopathological image classification},
journal = {Pattern Recognition},
volume = {140},
pages = {109532},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109532},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002327},
author = {Meidan Ding and Aiping Qu and Haiqin Zhong and Zhihui Lai and Shuomin Xiao and Penghui He},
keywords = {Histopathological image classification, Vision transformer, Convolutional neural network, Wavelet position embedding, External multi-head attention},
abstract = {Histopathological image classification is a fundamental task in pathological diagnosis workflow. It remains a huge challenge due to the complexity of histopathological images. Recently, hybrid methods combining convolutional neural networks(CNN) with vision transformers(ViT) are proposed to this field. These methods can well represent the global and local contextual information and achieve excellent classification performances. However, the downsampling operation like max-pooling which ignores the sampling theorem transmits the jagged artifacts into transformer, which would lead to an aliasing phenomenon. It makes the subsequent feature maps focus on the incorrect regions and influences the final classification results. In this work, we propose an enhanced vision transformer with wavelet position embedding to tackle this challenge. In particular, a wavelet position embedding module, which introduces the wave transform into position embedding, is employed to enhance the smoothness of discontinuous feature information by decomposing sequences into amplitude and phase in pathological feature maps. In addition, an external multi-head attention is proposed to replace self-attention in the transformer block with two linear layers. It reduces the cost of computation and excavates potential correlations between different samples. We evaluate the proposed method on three public histopathological classification challenging datasets, and perform a quantitative comparison with previous state-of-the-art methods. The results empirically demonstrate that our method achieves the best accuracy. Furthermore, it has the least parameters and a very low FLOPs. In conclusion, the enhanced vision transformer shows high classification performances and demonstrates significant potential for assisting pathologists in pathological diagnosis.}
}
@article{LIU2023109599,
title = {Local multi-scale feature aggregation network for real-time image dehazing},
journal = {Pattern Recognition},
volume = {141},
pages = {109599},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109599},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300300X},
author = {Yong Liu and Xiaorong Hou},
keywords = {Deep learning, Feature aggregation, Image restoration, Lightweight network, Multi-scale},
abstract = {Haze causes visual degradation and obscures image information, which gravely affects the reliability of computer vision tasks in real-time systems. Leveraging an enormous number of learning parameters as the restoration costs, learning-based methods have gained significant success, but they are runtime intensive or memory inefficient. In this paper, we propose a local multi-scale feature aggregation network, called LMFA-Net, which has a lightweight model structure and can be used for real-time dehazing. By learning the local mapping relationship between the clean value of a haze image at a certain point and its surrounding local region, LMFA-Net can directly restore the final haze-free image. In particular, we adopt a novel multi-scale feature extraction sub-network (M-Net) to extract features from different scales. As a lightweight network, LMFA-Net can achieve fast and efficient dehazing. Extensive experiments demonstrate that our proposed LMFA-Net surpasses previous state-of-the-art lightweight dehazing methods in both quantitatively and qualitatively.}
}
@article{LIU2023109560,
title = {Improve Temporal Action Proposals using Hierarchical Context},
journal = {Pattern Recognition},
volume = {140},
pages = {109560},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109560},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002601},
author = {Qinying Liu and Zilei Wang and Shenghai Rong},
keywords = {Temporal action proposal, Contexts, Attention model},
abstract = {Temporal action proposal (TAP) aims to generate accurate candidates of action instances in an untrimmed video. It has been proved that contexts are critically important to this task. In this paper, we propose a novel hierarchical context network (HCN) to further explore the snippet-level and proposal-level contexts, which are used to improve the representations of snippets and proposals, respectively. First, we pinpoint that different scales of snippet-level contexts are not equally important for different action instances. To this end, we incorporate a novel gating mechanism into the U-Net structure to capture the content-adaptive snippet-level contexts. Second, to exploit the proposal-level contexts, we propose a task-specific self-attention model with high efficiency. By stacking multiple attention models, we can deeply explore the proposal-level contexts in a wide range. Finally, to leverage both levels of context, we equip HCN with three branches to evaluate proposals from local to global perspectives. Our experiments on the ActivityNet-1.3 and THUMOS14 datasets show that HCN significantly outperforms previous TAP methods. Additionally, further experiments demonstrate that our method can substantially improve the state-of-the-art action detection performance when combined with existing action classifiers.}
}
@article{XIANG2023109487,
title = {Self-supervised learning of scene flow with occlusion handling through feature masking},
journal = {Pattern Recognition},
volume = {139},
pages = {109487},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109487},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001875},
author = {Xuezhi Xiang and Rokia Abdein and Ning Lv and Jie Yang},
keywords = {Optical flow, Depth estimation, Camera pose, Rigidity segmentation, Occlusion handling, Deformable decoder},
abstract = {In this work, we improve the optical flow and depth based on the important observation that they should share the geometric structure of the reference image. We initially propose a feature masking method to reduce the occlusion impact on the optical flow and depth by producing preliminary motion features that share the structure of the reference image. In addition, we propose a deformable decoder that learns the geometrical structure of the reference image in the form of a group of offsets and uses them to adapt the motion features of flow and depth maps, thereby preventing incorrect propagation in occluded regions and providing more structural details in the other regions. Furthermore, we recursively update the optical flow with self-supervised cues learned from the rigid flow and optical flow. Our method achieves a new state-of-the-art result for the optical flow on the KITTI 2015 benchmark with F1 = 11.17%.}
}
@article{HAN2023109517,
title = {The impact of isolation kernel on agglomerative hierarchical clustering algorithms},
journal = {Pattern Recognition},
volume = {139},
pages = {109517},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109517},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002170},
author = {Xin Han and Ye Zhu and Kai Ming Ting and Gang Li},
keywords = {Agglomerative hierarchical clustering, Varied densities, Dendrogram purity, Isolation kernel, Gaussian kernel},
abstract = {Agglomerative hierarchical clustering (AHC) is one of the popular clustering approaches. AHC generates a dendrogram that provides richer information and insights from a dataset than partitioning clustering. However, a major problem with existing distance-based AHC methods is: it fails to effectively identify adjacent clusters with varied densities, regardless of the cluster extraction methods applied to the resultant dendrogram. This paper aims to reveal the root cause of this issue and provides a solution by using a data-dependent kernel. We analyse the condition under which existing AHC methods fail to effectively extract clusters, and give the reason why the data-dependent kernel is an effective remedy. This leads to a new approach to kernerlise existing hierarchical clustering algorithms including the traditional AHC algorithms, HDBSCAN, GDL, PHA and HC-OT. Our extensive empirical evaluation shows that the recently introduced Isolation Kernel produces a higher quality or purer dendrogram than distance, Gaussian Kernel and adaptive Gaussian Kernel in all the above mentioned AHC algorithms.}
}
@article{HU2023109569,
title = {A Noising-Denoising Framework for Point Cloud Upsampling via Normalizing Flows},
journal = {Pattern Recognition},
volume = {140},
pages = {109569},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109569},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002698},
author = {Xin Hu and Xin Wei and Jian Sun},
keywords = {Point cloud, Arbitrary ratio upsampling, Normalizing flows},
abstract = {Point cloud upsampling aims to generate dense and uniform point cloud from the sparse input point cloud. One challenge is how to flexibly upsample the sparse point cloud in arbitrary ratios, even without the given supervised high resolution point cloud. To address this challenge, we propose a noising-denoising framework, dubbed ND-PUFlow, for arbitrary 3D point cloud upsampling (3DPU) in supervised and self-supervised settings. It consists of two stages, i.e., dense noisy points generation and noisy points denoising via continuous normalizing flows (CNFs). In the first stage, noisy points are generated by adding noise to the input points. In the second stage, CNFs move each noisy point to the underlying surface, forming a dense and clean point cloud. Extensive experiments show that our method is competitive in both supervised and self-supervised settings, and in most cases, it achieves the best performance on benchmark datasets for 3DPU.}
}
@article{WANG2023109596,
title = {GSAL: Geometric structure adversarial learning for robust medical image segmentation},
journal = {Pattern Recognition},
volume = {140},
pages = {109596},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109596},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002972},
author = {Kun Wang and Xiaohong Zhang and Yuting Lu and Wei Zhang and Sheng Huang and Dan Yang},
keywords = {Medical image segmentation, Geometric structure learning, Adversarial learning, Computer-Aided diagnosis (CAD)},
abstract = {Automatic medical image segmentation plays a crucial role in clinical diagnosis and treatment. However, it is still a challenging task due to the complex interior characteristics (e.g., inconsistent intensity, low contrast, texture heterogeneity) and ambiguous external boundary structures. In this paper, we introduce a novel geometric structure learning mechanism (GSLM) to overcome the limitations of existing segmentation models that lack learning ”focus, path, and difficulty.” The geometric structure in this mechanism is jointly characterized by the skeleton-like structure extracted by the mask distance transform (MDT) and the boundary structure extracted by the mask distance inverse transform (MDIT). Among them, the skeleton-like and boundary pay attention to the trend of interior characteristics consistency and external structure continuity, respectively. With this idea, we design GSAL, a novel end-to-end geometric structure adversarial learning for robust medical image segmentation. GSAL has four components: a geometric structure generator, which yields the geometric structure to learn the most discriminative features that preserve interior characteristics consistency and external boundary structure continuity, skeleton-like and boundary structure discriminators, which enhance and correct the characterization of internal and external geometry to mutually promote the capture of global contextual dependencies, and a geometric structure fusion sub-network, which fuses the two complementary and refined skeleton-like and boundary structures to generate the high-quality segmentation results. The proposed approach has been successfully applied to three different challenging medical image segmentation tasks, including polyp segmentation, COVID-19 lung infection segmentation, and lung nodule segmentation. Extensive experimental results demonstrate that the proposed GSAL achieves favorably against most state-of-the-art methods under different evaluation metrics. The code is available at: https://github.com/DLWK/GSAL.}
}
@article{KONG2023109545,
title = {Low-Tubal-Rank tensor recovery with multilayer subspace prior learning},
journal = {Pattern Recognition},
volume = {140},
pages = {109545},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109545},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002455},
author = {Weichao Kong and Feng Zhang and Wenjin Qin and Jianjun Wang},
keywords = {Tensor robust principal component analysis, Tensor completion, Multilayer subspace prior information, ADMM, T-SVD},
abstract = {Currently, low-rank tensor recovery employing the subspace prior information is an emerging topic, which has attracted considerable attention. However, existing studies cannot flexibly and fully utilize the accessible subspace prior information, thereby leading to suboptimal restored performance. Aiming at addressing this issue, based on the tensor singular value decomposition (t-SVD), this article presents a novel strategy that integrates more than two layers of subspace knowledge about columns and rows of target tensor into one unified recovery framework. Specially, we first design a multilayer subspace prior learning scheme, and then apply it to two common low-rank tensor recovery problems, i.e., tensor completion and tensor robust component principal analysis. Crucially, we prove that our approach can achieve exact recovery of tensors under a significantly weaker incoherence assumption than the analogous conditions previously proposed. Furthermore, two efficient algorithms with convergence guarantees based on alternating direction method of multipliers (ADMM) are proposed to solve the corresponding models. The experimental results on synthetic and real tensor data show that the proposed algorithms outperform other state-of-the-art algorithms in terms of both qualitative and quantitative metrics.}
}
@article{YANG2023109521,
title = {Discriminative semi-supervised learning via deep and dictionary representation for image classification},
journal = {Pattern Recognition},
volume = {140},
pages = {109521},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109521},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002212},
author = {Meng Yang and Jie Ling and Jiaming Chen and Mao Feng and Jian Yang},
keywords = {Semi-supervised, Deep and dictionary learning, Image classification},
abstract = {Supervised dictionary learning and deep learning have achieved promising performance in the classification task. However, in many real-world applications there usually exist very limited labeled training samples, although abundant unlabeled data is relatively easy to collect. How to effectively exploit the discrimination of unlabeled data is still an open question, hence semi-supervised learning has attracted much attention from wide fields. Semi-supervised deep feature learning has well exploited the feature discrimination from only the discriminative viewpoint, while dictionary representation-based classification has also been applied to semi-supervised learning but with shallow features. In this paper, we propose a novel discriminative semi-supervised learning via deep and dictionary representation (DSSLDDR), which jointly utilizes the discrimination of dictionary representation for data reconstruction and the distinguishing feature of each sample. To exploit the powerful discrimination of dictionary representation, class-specific dictionaries are required to discriminatively reconstruct a sample, with the reconstruction error to predict the sample’s class label. To exploit the semantic information, the deep neural network extracts discriminative features by using multiple nonlinear transformations to generate the powerful descriptor. Then the class-specific dictionary learning and deep network learning are integrated together to conduct more accurate class estimation for unlabeled data and learn a more discriminative classifier, where an entropy regularization is designed to balance and control the class estimation of unlabeled data. Furthermore, we propose the DSSLDDR++, the extension model of DSSLDDR based on consistency/contrastive learning to further improve the accuracy of class estimation for unlabeled data, making a more powerful semi-supervised learning classifier. Extensive experiments on benchmark datasets show the effectiveness of the proposed methods.}
}
@article{WANG2023109601,
title = {Low-rank kernel regression with preserved locality for multi-class analysis},
journal = {Pattern Recognition},
volume = {141},
pages = {109601},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109601},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003023},
author = {Yingxu Wang and Long Chen and Jin Zhou and Tianjun Li and Yufeng Yu},
keywords = {Kernel ridge regression, Low-rank learning, Locality preserving, Random feature space},
abstract = {Kernel ridge regression (KRR) is a kind of efficient supervised algorithm for multi-class analysis. However, limited by the implicit kernel space, current KRR methods have weak abilities to deal with redundant features and hidden local structures. Thus, they may get indifferent results when applied to analyze the data with complicated components. To overcome this weakness and obtain better multi-class regression performance, we propose a new method named low-rank kernel regression with preserved locality (RLRKRR). In this method, data are mapped into an explicit feature space by using the random Fourier feature technique to discover the non-linear relationship between data samples. In addition, during the training of the regression coefficient matrix, the low-rank components of this explicit feature space are simultaneously extracted for reducing the effect of the redundancy. Moreover, the graph regularization is performed on the extracted low-rank components to preserve local structures. Furthermore, the l2,p norm is imposed on the regression error term for relieving the impact of outliers. Based on these strategies, RLRKRR is capable to achieve rewarding results in complicated multi-class data analysis. In the comprehensive experiments conducted on various types of datasets, RLRKRR outperforms several state-of-the-art regression methods in terms of classification accuracy (CA).}
}
@article{DECAUX2023109529,
title = {Semi-automatic muscle segmentation in MR images using deep registration-based label propagation},
journal = {Pattern Recognition},
volume = {140},
pages = {109529},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109529},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002297},
author = {Nathan Decaux and Pierre-Henri Conze and Juliette Ropars and Xinyan He and Frances T. Sheehan and Christelle Pons and Douraied {Ben Salem} and Sylvain Brochard and François Rousseau},
keywords = {Semi-automatic segmentation, Musculoskeletal system, Label propagation, Deep registration},
abstract = {Fully automated approaches based on convolutional neural networks have shown promising performances on muscle segmentation from magnetic resonance (MR) images, but still rely on an extensive amount of training data to achieve valuable results. Muscle segmentation for pediatric and rare diseases cohorts is therefore still often done manually. Producing dense delineations over 3D volumes remains a time-consuming and tedious task, with significant redundancy between successive slices. In this work, we propose a segmentation method relying on registration-based label propagation, which provides 3D muscle delineations from a limited number of annotated 2D slices. Based on an unsupervised deep registration scheme, our approach ensures the preservation of anatomical structures by penalizing deformation compositions that do not produce consistent segmentation from one annotated slice to another. Evaluation is performed on MR data from lower leg and shoulder joints. Results demonstrate that the proposed semi-automatic multi-label segmentation model outperforms state-of-the-art techniques.}
}
@article{SINGHA2023109557,
title = {A real-time semantic segmentation model using iteratively shared features in multiple sub-encoders},
journal = {Pattern Recognition},
volume = {140},
pages = {109557},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109557},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002571},
author = {Tanmay Singha and Duc-Son Pham and Aneesh Krishna},
keywords = {Semantic segmentation, Deep convolution neural networks, Multi-encoder, Decoder, Feature scaling, Feature aggregation, Feature reuse, Resource-constrained applications, Mobile devices},
abstract = {Recent studies show a significant growth in semantic segmentation. However, many semantic segmentation models still have a large number of parameters, making them unsuitable for resource-constrained embedded devices. To address this issue, we propose an efficient Shared Feature Reuse Segmentation (SFRSeg) model containing several novelties: a new yet effective shared-branch multiple sub-encoders design, a context mining module and a semantic aggregating module for better context granularity. In particular, our shared-branch approach improves the entire feature hierarchy by sharing the spatial and context knowledge in both shallow and deep branches. After every shared point in each sub-encoder, a proposed cascading context mining (CCM) module is deployed to filter out the noisy spatial details from the feature maps and provides a diverse size of receptive fields for capturing the latent context between multi-scale geometric shapes in the scene. To overcome the gradient vanishing issue at the early stage, we reduce the number of layers in the first sub-encoder and employ a unique multiple sub-encoders design which reprocesses the rich global feature maps through multiple sub-encoders for better feature refinement. Later, the rich semantic features generated by the efficient sub-encoders at different levels are fused by the proposed Hybrid Path Attention Semantic Aggregation (HPA-SA) module that effectively reduces the semantic gap between feature maps at different levels and alleviate the well-known boundary degeneration effect. To make it computationally efficient for resource-constrained embedded devices, a series of lightweight methods such as a lightweight encoder, a squeeze-and-excitation design, separable convolution filters, channel reduction (CR) are carefully exploited. With an exceptional performance on Cityscapes (70.6% test mIoU) and CamVid (74.7% test mIoU) data sets, the proposed model is shown to be superior over existing light real-time semantic segmentation models whilst having only 1.6 million parameters.}
}
@article{KAPOOR2023109505,
title = {Aeriform in-action: A novel dataset for human action recognition in aerial videos},
journal = {Pattern Recognition},
volume = {140},
pages = {109505},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109505},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002054},
author = {Surbhi Kapoor and Akashdeep Sharma and Amandeep Verma and Sarbjeet Singh},
keywords = {UAV, Dataset, Human detection, Human action recognition, Aerial videos},
abstract = {Human actions being diverse in nature cannot be generalized, thus making it quite difficult to train a machine to recognize such diversified actions. This challenge is further compounded by the lack of availability of datasets for aerial surveillance, as collecting and annotating a large dataset is a formidable task. This paper aims to solve the problem of data scarcity by introducing a new dataset, Aeriform in-action for recognizing human actions from aerial videos. The proposed dataset consists of 32 high-resolution videos containing 13 action classes with 55,477 frames (without augmentation) and almost 400,000 annotations. It includes complex and aggressive actions such as kicking and punching, as well as drone signaling actions like waving and handshaking. The dataset also includes human-object interactions like carrying and reading. In addition to the dataset, this paper also presents a two-step deep learning framework for recognizing human actions based on the integration of human detection and action recognition module. The action recognition module adopts a modified version of the ResNeXt101 architecture (M-ResNext101) to recognize human actions in aerial videos. The performance of the proposed M-ResNext101 model is compared with 13 other deep learning models, and it outperforms all of them with an accuracy of 76.44% on the test data. The proposed dataset for human action recognition in aerial videos is available on https://surbhi-31.github.io/Aeriform-in-action/.}
}
@article{HUANG2023109533,
title = {Reciprocal normalization for domain adaptation},
journal = {Pattern Recognition},
volume = {140},
pages = {109533},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109533},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002339},
author = {Zhiyong Huang and Kekai Sheng and Ke Li and Jian Liang and Taiping Yao and Weiming Dong and Dengwen Zhou and Xing Sun},
keywords = {Domain adaptation, Feature normalization, Deep neural network},
abstract = {Batch normalization (BN) is widely used in modern deep neural networks, which has been shown to represent the domain-related knowledge, and thus is ineffective for cross-domain tasks like unsupervised domain adaptation (UDA). Existing BN variant methods aggregate source and target domain knowledge in the same channel in normalization module. However, the misalignment between the features of corresponding channels across domains often leads to a sub-optimal transferability. In this paper, we exploit the cross-domain relation and propose a novel normalization method, Reciprocal Normalization (RN). Specifically, RN first presents a Reciprocal Compensation (RC) module to acquire the compensatory for each channel in both domains based on the cross-domain channel-wise correlation. Then RN develops a Reciprocal Aggregation (RA) module to adaptively aggregate the feature with its cross-domain compensatory components. As an alternative to BN, RN is more suitable for UDA problems and can be easily integrated into popular domain adaptation methods. Experiments show that the proposed RN outperforms existing normalization counterparts by a large margin and helps state-of-the-art adaptation approaches achieve better results. The source code is available on https://github.com/Openning07/reciprocal-normalization-for-DA.}
}
@article{SUN2023109561,
title = {Exemplar-free class incremental learning via discriminative and comparable parallel one-class classifiers},
journal = {Pattern Recognition},
volume = {140},
pages = {109561},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109561},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002613},
author = {Wenju Sun and Qingyong Li and Jing Zhang and Danyu Wang and Wen Wang and YangLi-ao Geng},
keywords = {Incremental learning, Continual learning, Lifelong learning, One-class learning, Image classification},
abstract = {The exemplar-free class incremental learning (IL) requires classification models to learn new-class knowledge incrementally without retaining any old samples. Recently, the IL framework based on parallel one-class classifiers (POC) has demonstrated promising performance. It trains a one-class classifier (OCC) for each category and thus is immune to the catastrophic forgetting problem. However, the single-class training strategy may incur weak discriminability and low comparability between different classifiers in POC. To meet this challenge, we propose a new IL framework, referred to as Discriminative and Comparable Parallel One-class Classifiers (DCPOC). Instead of ordinary OCCs (e.g., deep SVDD) used in other POC methods, DCPOC adopts variational auto-encoders (VAE) as OCCs because VAEs can be used not only to identify classes for given samples but also to generate pseudo samples for the trained classes. With this advantage, DCPOC trains a new-class VAE in contrast with the old-class VAEs, which benefits the new-class VAE to reconstruct better for new-class samples but worse for old-class pseudo samples, thus enhancing the comparability. Furthermore, DCPOC introduces a hinge reconstruction loss to reinforce the discriminability. We evaluate our method on MNIST, CIFAR10, CIFAR100, Tiny-ImageNet, and ImageNet. The experimental results show that DCPOC achieves state-of-the-art performance on these datasets.11The source code is publicly available at https://github.com/SunWenJu123/DCPOC}
}
@article{LIN2023109556,
title = {Diluted binary neural network},
journal = {Pattern Recognition},
volume = {140},
pages = {109556},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109556},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300256X},
author = {Yuhan Lin and Lingfeng Niu and Yang Xiao and Ruizhi Zhou},
keywords = {Model compression, Network quantization, Binary neural network, Ternary neural network, Sparse regularization},
abstract = {Binary neural networks (BNNs) are promising on resource-constrained devices because they reduce memory consumption and accelerate inference effectively. However, they are still potential on performance improvement. Prior studies attribute performance degradation of BNNs to limited representation ability and gradient mismatch. In this paper, we find that it also results from the mandatory representation of small full-precision auxiliary weights to large values. To tackle with this issue, we propose an approach dubbed as Diluted Binary Neural Network (DBNN). Besides avoiding mandatory representation effectively, the proposed DBNN also alleviates sign flip problem to a large extent. For activations, we jointly minimize quantization error and maximize information entropy to develop the binarization scheme. Compared with existing sparsity-binarization approaches, DBNN trains network from scratch without other procedures and achieves larger sparsity. Experiments on several datasets with various networks demonstrate the superiority of our approach.}
}