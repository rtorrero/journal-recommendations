@article{ZHONG2023109349,
title = {Self-taught Multi-view Spectral Clustering},
journal = {Pattern Recognition},
volume = {138},
pages = {109349},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109349},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300050X},
author = {Guo Zhong and Chi-Man Pun},
keywords = {Graph clustering, spectral rotation, spectral clustering, multi-view clustering},
abstract = {By integrating multiple views, i.e., multi-view learning (ML), we can discover the underlying data structures so that the performance of learning tasks can improve. As a basic and important branch of ML, multi-view clustering has achieved great success recently in pattern recognition and machine learning communities. Most existing multi-view spectral clustering methods heavily adopt the relax-and-discretize strategy to obtain discrete cluster labels (clustering results), i.e., using predefined similarity graphs to learn a consensus Laplacian embedding shared by all views for K-means clustering. However, the above clustering strategy may significantly affect clustering performance since there is information loss between independent steps. In this paper, we establish a novel Self-taught Multi-view Spectral Clustering (SMSC) framework to address the above issue. As the main contributions of this paper, we provide two versions of SMSC based on convex combination and centroid graph fusion schemes. Specifically, a self-taught mechanism is introduced in SMSC, which can effectively feedback the manifold structure induced by Laplacian embedding and the cluster information hidden in the discrete indicator matrix to learn an optimal consensus similarity graph for graph partitioning. The effectiveness of the proposed methods has been evaluated on real-world multi-view datasets, and experimental results show that our methods outperform other state-of-the-art baselines.}
}
@article{CHEN2023109307,
title = {A local tangent plane distance-based approach to 3D point cloud segmentation via clustering},
journal = {Pattern Recognition},
volume = {137},
pages = {109307},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109307},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000080},
author = {Hui Chen and Tingting Xie and Man Liang and Wanquan Liu and Peter Xiaoping Liu},
keywords = {3D point cloud, Plane segmentation, Tangent distance, Adaptive clustering},
abstract = {This paper proposes an effective measure for the planar segmentation problem based on the clustering method. It uses the distance from a point to the local plane as a metric to characterize the relationship between data. As a result, the data points of the coplanar have a high similarity to distinguish each plane. A dissimilarity matrix of the input point cloud can be evaluated, and multidimensional scaling analysis is performed to reconstruct the correlation information between data points in the 3D Euclidean space. The obtained reconstructed point cloud shows the separation between different planes. An adaptive DBSCAN clustering method based on density stratification is developed to perform cluster segmentation on the reconstructed point cloud. Experimental results show that the proposed method can effectively solve the over-segmentation problem, and at the same time provide high segmentation accuracy.}
}
@article{ZHANG2023109279,
title = {Weakly supervised foreground learning for weakly supervised localization and detection},
journal = {Pattern Recognition},
volume = {137},
pages = {109279},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109279},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007580},
author = {Chen-Lin Zhang and Yin Li and Jianxin Wu},
keywords = {Weakly supervised object localization, Weakly supervised object detection, Foreground learning},
abstract = {Modern deep learning models require large amounts of accurately annotated data, which is often difficult to satisfy. Hence, weakly supervised tasks, including weakly supervised object localization (WSOL) and detection (WSOD), have recently received attention in the computer vision community. In this paper, we motivate and propose the weakly supervised foreground learning (WSFL) task by showing that both WSOL and WSOD can be greatly improved if groundtruth foreground masks are available. More importantly, we propose a complete WSFL pipeline with low computational cost, which generates pseudo boxes, learns foreground masks, and does not need any localization annotations. With the help of foreground masks predicted by our WSFL model, we achieve 74.37% correct localization accuracy on CUB for WSOL, and 55.7% mean average precision on VOC07 for WSOD, thereby establish new state-of-the-art for both tasks. Our WSFL model also shows excellent transfer ability.}
}
@article{WANG2023109243,
title = {Diverse image inpainting with disentangled uncertainty},
journal = {Pattern Recognition},
volume = {137},
pages = {109243},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109243},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007221},
author = {Wentao Wang and Lu He and Li Niu and Jianfu Zhang and Yue Liu and Haoyu Ling and Liqing Zhang},
keywords = {Image inpainting, Diverse image inpainting, Disentangled representation},
abstract = {Most existing inpainting methods repair a corrupted image to a single output, which gives people no choice to select the most satisfactory result. However, image inpainting is essentially a multi-modal problem because the inpainted results could have multiple possibilities. To generate both diverse and realistic inpainted results, we propose a diverse image inpainting framework with disentangled uncertainty. We disentangle the uncertainty of the missing region into two aspects: structure and appearance. Correspondingly, we divide the process of diverse image inpainting into two stages: diverse structure inpainting and diverse appearance inpainting. In the first stage, we restore the structure of the missing region, producing diverse complete edge maps. In the second stage, using a complete edge map as the guidance, we fill in diverse appearance information of the missing region. We also design a light-weighted disentangling subnetwork to disentangle structure information and appearance information. Besides, we propose a novel style-based masked residual block to better deal with the uncertainty. Experiments on CelebA-HQ, Paris Street View, and Places2 demonstrate that our method can repair the corrupted image with higher fidelity and diversity than other existing methods.}
}
@article{WANG2023109335,
title = {Memory-augmented appearance-motion network for video anomaly detection},
journal = {Pattern Recognition},
volume = {138},
pages = {109335},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109335},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000365},
author = {Le Wang and Junwen Tian and Sanping Zhou and Haoyue Shi and Gang Hua},
keywords = {Anomaly detection, Memory network, Autoencoder, Abnormal events},
abstract = {Video anomaly detection is a promising yet challenging task, where only normal events are observed in the training phase. Without any explicit classification boundary between normal and abnormal events, anomaly detection can be turned into an outlier detection problem by regarding any event that does not conform to the normal patterns as an anomaly. Most of the existing works mainly focus on improving the representation of normal events, while ignore the relationship between normal and abnormal events. Besides, the lack of restrictions on classification boundaries also leads to performance degradation. To address the above problems, we design a novel autoencoder-based Memory-Augmented Appearance-Motion Network (MAAM-Net), which consists of a novel end-to-end network to learn appearance and motion feature of a given input frame, a fused memory module to build a bridge for normal and abnormal events, a well-designed margin-based latent loss to relieve the computation costs, and a pointed Patch-based Stride Convolutional Detection (PSCD) algorithm to eliminate the degradation phenomenon. Specifically, the memory module is embedded between the encoder and decoder, which serves as a sparse dictionary of normal patterns, therefore it can be further employed to reintegrate abnormal events during inference. To further distort the reintegration quality of abnormal events, the margin-based latent loss is leveraged to enforce the memory module to select a sparse set of critical memory items. Last but not least, the simple yet effective detection method focuses on patches rather than the overall frame responses, which can benefit from the distortion of abnormal events. Extensive experiments and ablation studies on three anomaly detection benchmarks, i.e., UCSD Ped2, CUHK Avenue, and ShanghaiTech, demonstrate the effectiveness and efficiency of our proposed MAAM-Net. Notably, we achieve superior AUC performances on UCSD Ped2 (0.977), CHUK Avenue (0.909), and ShanghaiTech (0.713). The code is publicly available at https://github.com/Owen-Tian/MAAM-Net.}
}
@article{LIN2023109283,
title = {Rectified Euler k-means and beyond},
journal = {Pattern Recognition},
volume = {137},
pages = {109283},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109283},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007622},
author = {Yunxia Lin and Songcan Chen},
keywords = {Kernel -means, Euler kernel, Pseudo centroid, Rectified euler -means},
abstract = {Euler k-means (EulerK) first maps data onto the unit hyper-sphere surface of equi-dimensional space via a complex mapping which induces the robust Euler kernel and next employs the popular k-means. Consequently, besides enjoying the virtues of k-means such as simplicity and scalability to large data sets, EulerK is also robust to noises and outliers. Although so, the centroids captured by EulerK deviate from the unit hyper-sphere surface and thus in strict distributional sense, actually are outliers. This weird phenomenon also occurs in some generic kernel clustering methods. Intuitively, using such outlier-like centroids should not be quite reasonable but it is still seldom attended. To eliminate the deviation, we propose two Rectified Euler k-means methods, i.e., REK1 and REK2, which retain the merits of EulerK while acquiring real centroids residing on the mapped space to better characterize the data structures. Specifically, REK1 rectifies EulerK by imposing the constraint on the centroids while REK2 views each centroid as the mapped image from a pre-image in the original space and optimizes these pre-images in Euler kernel induced space. Undoubtedly, our proposed REKs can methodologically be extended to solve problems of such a category. Finally, the experiments validate the effectiveness of REK1 and REK2.}
}
@article{LAI2023109311,
title = {Efficient sampling using feature matching and variable minimal structure size},
journal = {Pattern Recognition},
volume = {137},
pages = {109311},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109311},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000122},
author = {Taotao Lai and Alireza Sadri and Shuyuan Lin and Zuoyong Li and Riqing Chen and Hanzi Wang},
keywords = {Model fitting, Guided sampling, Feature matching, Multiple structure data},
abstract = {Greedy search-based guided sampling is a promising research field in model fitting to data with multiple structures in the presence of a large number of outliers. However, these greedy search-based guided sampling algorithms are sensitive to the fixed minimal (acceptable) structure size and the initial model hypothesis: when the fixed minimal structure size is too small, data subsets sampled by these algorithms are not representative. In contrast, when it is too large, data subsets might be contaminated by outliers. Furthermore, these algorithms may fail to obtain an accurate model hypothesis, if the initial model hypothesis is far from the true model. In this paper, we address the above-mentioned two issues by proposing two greedy search-based strategies: one aims to adaptively estimate minimal structure sizes and the other aims to generate effective initial model hypotheses. Specifically, on one hand, to avoid using the fixed minimal structure size, a strategy is proposed to adaptively estimate minimal structure sizes by using previously obtained ones. On the other hand, to reduce the impact of outliers, a strategy is proposed to filter out outliers to obtain a reduced data subset by using a feature matching algorithm. Then, this strategy generates promising initial model hypotheses by using a proximity sampling on the reduced data subset. Finally, an efficient sampling algorithm based on the two proposed greedy search-based strategies is applied to three vision tasks, i.e., fundamental matrix estimation, homography plane detection and 3D motion segmentation. Extensive experimental results demonstrate the effectiveness of the proposed sampling algorithm.}
}
@article{CHEN2023109271,
title = {Riemannian representation learning for multi-source domain adaptation},
journal = {Pattern Recognition},
volume = {137},
pages = {109271},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109271},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007506},
author = {Sentao Chen and Lin Zheng and Hanrui Wu},
keywords = {Convex optimization, Hellinger distance, Multi-source domain adaptation, Representation learning, Riemannian manifold},
abstract = {Multi-Source Domain Adaptation (MSDA) aims at training a classification model that achieves small target error, by leveraging labeled data from multiple source domains and unlabeled data from a target domain. The source and target domains are described by related but different joint distributions, which lie on a Riemannian manifold named the statistical manifold. In this paper, we characterize the joint distribution difference by the Hellinger distance, which bears strong connection to the Riemannian metric defined on the statistical manifold. We show that the target error of a neural network classification model is upper bounded by the average source error of the model and the average Hellinger distance, i.e., the average of multiple Hellinger distances between the source and target joint distributions in the network representation space. Motivated by the error bound, we introduce Riemannian Representation Learning (RRL): An approach that trains the network model by minimizing (i) the average empirical Hellinger distance with respect to the representation function, and (ii) the average empirical source error with respect to the network model. Specifically, we derive the average empirical Hellinger distance by constructing and solving unconstrained convex optimization problems whose global optimal solutions are easy to find. With the network model trained, we expect it to achieve small error in the target domain. Our experimental results on several image datasets demonstrate that the proposed RRL approach is statistically better than the comparison methods.}
}
@article{XU2023109347,
title = {A Comprehensive Survey of Image Augmentation Techniques for Deep Learning},
journal = {Pattern Recognition},
volume = {137},
pages = {109347},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109347},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000481},
author = {Mingle Xu and Sook Yoon and Alvaro Fuentes and Dong Sun Park},
keywords = {Image augmentation, Deep learning, Image variation, Vicinity distribution, Data augmentation, Computer vision},
abstract = {Although deep learning has achieved satisfactory performance in computer vision, a large volume of images is required. However, collecting images is often expensive and challenging. Many image augmentation algorithms have been proposed to alleviate this issue. Understanding existing algorithms is, therefore, essential for finding suitable and developing novel methods for a given task. In this study, we perform a comprehensive survey of image augmentation for deep learning using a novel informative taxonomy. To examine the basic objective of image augmentation, we introduce challenges in computer vision tasks and vicinity distribution. The algorithms are then classified among three categories: model-free, model-based, and optimizing policy-based. The model-free category employs the methods from image processing, whereas the model-based approach leverages image generation models to synthesize images. In contrast, the optimizing policy-based approach aims to find an optimal combination of operations. Based on this analysis, we believe that our survey enhances the understanding necessary for choosing suitable methods and designing novel algorithms.}
}
@article{SHAO2024110379,
title = {Corrigendum to “FGPNet: A weakly supervised fine-grained 3D point clouds classification network” [Pattern Recognition 139 (2023) 109509]},
journal = {Pattern Recognition},
volume = {151},
pages = {110379},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110379},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001304},
author = {Huihui Shao and Jing Bai and Rusong Wu and Jinzhe Jiang and Hongbo Liang}
}
@article{ZHANG2023109339,
title = {Learning visual question answering on controlled semantic noisy labels},
journal = {Pattern Recognition},
volume = {138},
pages = {109339},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109339},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000407},
author = {Haonan Zhang and Pengpeng Zeng and Yuxuan Hu and Jin Qian and Jingkuan Song and Lianli Gao},
keywords = {Visual question answering, Noisy datasets, Semantic labels, Contrastive learning},
abstract = {Visual Question Answering (VQA) has made great progress recently due to the increasing ability to understand and encode multi-modal inputs based on deep learning. However, existing VQA models are usually based on assumptions of clean labels, and it is contradictory to real scenarios where labels are expensive and inevitably contain noises. In this paper, we take the lead in addressing this issue by establishing the first benchmark of controlled semantic noisy labels for VQA task, evaluating existing methods, and coming up with corresponding solutions. Specifically, through analyzing human labels of existing VQA datasets, we first design a controlled semantic label noise by imitating human mislabeling behavior, which is more reasonable than conventional random noise. Then, we evaluate several popular VQA models on these new benchmark datasets and show that their performance degrades significantly compared to the original setting. To this end, we propose a Semantic Noisy Label Correction (SNLC) to mitigate impacts of noisy labels, including a Semantic Cross-Entropy (SCE) loss and a Semantic Embedding Contrastive (SEC) loss. Extensive experiments demonstrate the effectiveness of the proposed method SNLC. The proposed approach achieves a stable improvement on several existing models. The source code is available at https://github.com/zchoi/SNLC.}
}
@article{TANG2023109295,
title = {TCCFusion: An infrared and visible image fusion method based on transformer and cross correlation},
journal = {Pattern Recognition},
volume = {137},
pages = {109295},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109295},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007749},
author = {Wei Tang and Fazhi He and Yu Liu},
keywords = {Image fusion, Transformer, Deep learning, Infrared image, Cross correlation},
abstract = {Infrared and visible image fusion aims to obtain a synthetic image that can simultaneously exhibit salient objects and provide abundant texture details. However, existing deep learning-based methods generally depend on convolutional operations, which indeed have good local feature extraction ability, but the restricted receptive field limits its capability in modeling long-range dependencies. To conquer this dilemma, we propose an infrared and visible image fusion method based on Transformer and cross correlation, named TCCFusion. Specifically, we design a local feature extraction branch (LFEB) to preserve local complementary information, in which a dense-shape network is introduced to reuse the information that may be lost during the convolutional operation. To avoid the limitation of the receptive field and to fully extract the global significant information, a global feature extraction branch (GFEB) is devised that consists of three Transformer blocks for long-range relationship construction. In addition, LFEB and GFEB are arranged in a parallel fashion to maintain local and global useful information in a more effective way. Furthermore, we design a cross correlation loss to train the proposed fusion model in an unsupervised manner, with which the fusion result can obtain adequate thermal radiation information in an infrared image and ample texture details in a visible image. Massive experiments on two mainstream datasets illustrate that our TCCFusion outperforms state-of-the-art algorithms not only on visual quality but also on quantitative assessments. Ablation experiments on the network framework and objective function demonstrate the effectiveness of the proposed method.}
}
@article{NGUYEN2023109351,
title = {On a linear fused Gromov-Wasserstein distance for graph structured data},
journal = {Pattern Recognition},
volume = {138},
pages = {109351},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109351},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000523},
author = {Dai Hai Nguyen and Koji Tsuda},
keywords = {Linear optimal transport, Graph structured data, Kernel method},
abstract = {We present a framework for embedding graph structured data into a vector space, taking into account node features and structures of graphs into the optimal transport (OT) problem. Then we propose a novel distance between two graphs, named LinearFGW, defined as the Euclidean distance between their embeddings. The advantages of the proposed distance are twofold: 1) it takes into account node features and structures of graphs for measuring the dissimilarity between graphs in a kernel-based framework, 2) it is more efficient for computing a kernel matrix than pairwise OT-based distances, particularly fused Gromov-Wasserstein [1], making it possible to deal with large-scale data sets. Our theoretical analysis and experimental results demonstrate that our proposed distance leads to an increase in performance compared to the existing state-of-the-art graph distances when evaluated on graph classification and clustering tasks.}
}
@article{CAO2023109346,
title = {Unsupervised class-to-class translation for domain variations},
journal = {Pattern Recognition},
volume = {138},
pages = {109346},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109346},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300047X},
author = {Zhiyi Cao and Wei Wang and Lina Huo and Shaozhang Niu},
keywords = {Contrastive learning, Image-to-image translation, Adversarial learning, Image translation},
abstract = {The majority of image-to-image translation models tend to struggle in varying domain settings. For one varying domain, samples vary significantly in shape and size and have no domain labels. This paper proposes an unsupervised class-to-class translation model based on conditional contrastive learning to tackle the domain variations problem. The initial hypothesis is that the latent modalities of two varying domains are categorizable by style differences of different samples and turn the image-to-image translation problem into class-to-class translation. Firstly, unsupervised semantic clustering is performed for each domain to divide them into multiple classes and then leverage the classification features of different classes to perform class-to-class translation. Two conditional contrastive learning loss functions for each domain are proposed to perform unsupervised semantic clustering and decompose it into multiple classes. Then in the class-to-class translation stage, the classification features of different classes are employed to learn the latent modalities. The proposed model outperforms state-of-the-art baseline methods by employing the latent modalities of different classes. The sample code is available at https://github.com/c1a1o1/ucct.}
}
@article{WANG2023109379,
title = {Improving pseudo labels with intra-class similarity for unsupervised domain adaptation},
journal = {Pattern Recognition},
volume = {138},
pages = {109379},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109379},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000808},
author = {Jie Wang and Xiao-Lei Zhang},
keywords = {Unsupervised domain adaptation, Intra-class similarity, Spanning trees, Pseudo labels},
abstract = {Unsupervised domain adaptation (UDA) transfers knowledge from a label-rich source domain to a different but related fully-unlabeled target domain. To address the problem of domain shift, more and more UDA methods adopt pseudo labels of the target samples to improve the generalization ability on the target domain. However, inaccurate pseudo labels of the target samples may yield suboptimal performance with error accumulation during the optimization process. Moreover, once the pseudo labels are generated, how to remedy the generated pseudo labels is far from explored. In this paper, we propose a novel approach to improve the accuracy of the pseudo labels in the target domain. It first generates coarse pseudo labels by a conventional UDA method. Then, it iteratively exploits the intra-class similarity of the target samples for improving the generated coarse pseudo labels, and aligns the source and target domains with the improved pseudo labels. The accuracy improvement of the pseudo labels is made by first deleting dissimilar samples, and then using spanning trees to eliminate the samples with the wrong pseudo labels in the intra-class samples. We have applied the proposed approach to several conventional UDA methods as an additional term. Experimental results demonstrate that the proposed method can boost the accuracy of the pseudo labels and further lead to more discriminative and domain invariant features than the conventional baselines.}
}
@article{GUO2023109308,
title = {A comprehensive evaluation framework for deep model robustness},
journal = {Pattern Recognition},
volume = {137},
pages = {109308},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109308},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000092},
author = {Jun Guo and Wei Bao and Jiakai Wang and Yuqing Ma and Xinghai Gao and Gang Xiao and Aishan Liu and Jian Dong and Xianglong Liu and Wenjun Wu},
keywords = {Adversarial examples, Evaluation metrics, Model robustness},
abstract = {Deep neural networks (DNNs) have achieved remarkable performance across a wide range of applications, while they are vulnerable to adversarial examples, which motivates the evaluation and benchmark of model robustness. However, current evaluations usually use simple metrics to study the performance of defenses, which are far from understanding the limitation and weaknesses of these defense methods. Thus, most proposed defenses are quickly shown to be attacked successfully, which results in the “arm race” phenomenon between attack and defense. To mitigate this problem, we establish a model robustness evaluation framework containing 23 comprehensive and rigorous metrics, which consider two key perspectives of adversarial learning (i.e., data and model). Through neuron coverage and data imperceptibility, we use data-oriented metrics to measure the integrity of test examples; by delving into model structure and behavior, we exploit model-oriented metrics to further evaluate robustness in the adversarial setting. To fully demonstrate the effectiveness of our framework, we conduct large-scale experiments on multiple datasets including CIFAR-10, SVHN, and ImageNet using different models and defenses with our open-source platform. Overall, our paper provides a comprehensive evaluation framework, where researchers could conduct comprehensive and fast evaluations using the open-source toolkit, and the analytical results could inspire deeper understanding and further improvement to the model robustness.}
}
@article{EJIOGU2023109290,
title = {Real time iris segmentation quality evaluation using medoids},
journal = {Pattern Recognition},
volume = {137},
pages = {109290},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109290},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007695},
author = {Ugochi U.C. Ejiogu and Ogechukwu N. Iloanusi},
keywords = {Iris segmentation-quality estimation, Biometric vision and computing iris dataset, K-medoids clustering, Iris datasets, Iris ground-truth mask, Eccentricity, Annulus radii ratio, Euclidian distance, Iris mask, medoids},
abstract = {Integrating an efficient and robust iris segmentation-quality estimation module in iris biometric systems will undoubtedly enhance its performance and competitive advantage. It proffers a real time detection of segmentation errors to forestall their propagation to the subsequent modules. Hence, we propose a novel automatic iris segmentation-quality estimation model using medoids. The performance of the proposed model was empirically evaluated with reference to three published models, using three benchmarked iris datasets and our novel proprietary iris dataset – Biometrics Vision and Computing Iris dataset. The proposed medoids based model was experimentally demonstrated to be effective, robust and relatively efficient in estimating iris segmentation-quality. Specifically, the proposed model recorded the best classification accuracy rate of 95.27% on one of the datasets. Also, it consistently recorded the least classification error rate across several iris datasets with diverse segmentation-errors, which suggest that the medoids based model is relatively more robust than the examined counterparts.}
}
@article{LI2023109364,
title = {MinEnt: Minimum entropy for self-supervised representation learning},
journal = {Pattern Recognition},
volume = {138},
pages = {109364},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109364},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000651},
author = {Shuo Li and Fang Liu and Zehua Hao and Licheng Jiao and Xu Liu and Yuwei Guo},
keywords = {Self-supervised learning, Minimum entropy, Unsupervised representation learning, Image classification},
abstract = {Self-supervised representation learning is becoming more and more popular due to its superior performance. According to the information entropy theory, the smaller the information entropy of a feature, the more certain it is and the less redundant it is. Based on this, we propose a simple yet effective self-supervised representation learning method via Minimum Entropy (MinEnt). From the perspective of reducing information entropy, our MinEnt takes the output of the projector towards its nearest minimum entropy as the optimization target. The core of our MinEnt consists of three important steps: 1) normalize along the batch dimension to avoid model collapse, 2) compute the nearest minimum entropy to get the target, 3) compute the loss and backpropagate to optimize the network. Our MinEnt can learn efficient representations, even without the need for techniques such as negative sample pairs, predictors, momentum encoders, cross-correlation matrices, etc. Experimental results on four widely used datasets show that our method achieves competitive results in a simple manner.}
}
@article{ZHOU2023109312,
title = {Communication-efficient and Byzantine-robust distributed learning with statistical guarantee},
journal = {Pattern Recognition},
volume = {137},
pages = {109312},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109312},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000134},
author = {Xingcai Zhou and Le Chang and Pengfei Xu and Shaogao Lv},
keywords = {Distributed learning, Byzantine failure, Communication efficiency, Surrogate likelihood, Proximal algorithm},
abstract = {Communication efficiency and robustness are two major issues in modern distributed learning frameworks. This is due to the practical situations where some computing nodes may have limited communication power or may behave adversarial behaviors. To address the two issues simultaneously, this paper develops two communication-efficient and robust distributed learning algorithms for convex problems. Our motivation is based on surrogate likelihood framework and the median and trimmed mean operations. Particularly, the proposed algorithms are provably robust against Byzantine failures, and also achieve optimal statistical rates for strong convex losses and convex (non-smooth) penalties. For typical statistical models such as generalized linear models, our results show that statistical errors dominate optimization errors in finite iterations. Simulated and real data experiments are conducted to demonstrate the numerical performance of our algorithms.}
}
@article{SONG2023109278,
title = {Object detection based on cortex hierarchical activation in border sensitive mechanism and classification-GIou joint representation},
journal = {Pattern Recognition},
volume = {137},
pages = {109278},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109278},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007579},
author = {Yaoye Song and Peng Zhang and Wei Huang and Yufei Zha and Tao You and Yanning Zhang},
keywords = {Border sensitive mechanism, Cortex hierarchical activation, Object detection, Classification-GIoU joint representation},
abstract = {By imitating the brain neurons for object perception, the deep networks enable a comprehensive feature characterization in the task of object detection. Considering such a perceptual ability is usually bounded in a box area for feature extraction, the balance of dimension reduction and feature information retaining has been taken into account in more recent studies, especially for the information preservation in the border areas. Motivated by the mechanism of neuron cortex activation, in this work, a novel function based on cortex hierarchical activation is proposed to achieve more effective border sensitive mechanism by joint pooling in backbone networks. In order to avoid the parameter solidification, this strategy is also capable to benefit the feature extraction on the border without unnecessary model re-training. Furthermore, by replacing the square kernel with a designed band shape kernel, more adequate feature description can be obtained on the border via the combination of the strip hierarchical pooling and strip max pooling. With an extension of the proposed activation function on classification-GIoU joint representation, the overall detection accuracy has been further improved. Experimental evaluations on the COCO benchmark datasets have shown that the proposed work has a superior performance in comparison to other state-of-the-art detection approaches.}
}
@article{NGUYEN2023109336,
title = {Robust detectors of rotationally symmetric shapes based on novel semi-shape signatures},
journal = {Pattern Recognition},
volume = {138},
pages = {109336},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109336},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000377},
author = {Thanh Phuong Nguyen and Thanh Tuan Nguyen},
keywords = {Rotational symmetry detection, LIP/-signature, Radon},
abstract = {Efficient detectors of rotationally symmetric shapes are proposed by introducing a novel concept of semi-shape signatures to overcome the main problem of projection-based approaches for studying the rotationally symmetric properties of an arbitrary binary shape. Indeed, the fact that the projection cues in these conventional approaches are periodical with a period of π has restricted an applicable exploitation of rotational symmetry detection. To this end, we propose a new concept of the profile of semi-shapes as a shape signature together with a simple yet efficient technique so that the rotational symmetry of the binary shape can be determined by considering the correlation between this signature and its circular shift. Moreover, a new meaningful measure, ranging from 0 to 1, is also introduced to indicate how perfect the rotational symmetry would be. Experimental results of detecting on single/compound shapes have clearly corroborated the competence of our proposal.}
}
@article{WANG2023109254,
title = {Neurodynamics-driven supervised feature selection},
journal = {Pattern Recognition},
volume = {136},
pages = {109254},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109254},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007336},
author = {Yadi Wang and Jun Wang and Dacheng Tao},
keywords = {Feature selection, Biconvex Optimization, Information-theoretic measures, Neurodynamic optimization},
abstract = {Feature selection is an important dimensionality reduction technique in machine learning, pattern recognition, image processing, and data mining. Most existing feature selection methods are greedy in nature thus are prone to sub-optimality. Though some feature selection methods based on global optimization of unsupervised redundancy may potentiate performance improvements, they may or may not be relevant to classification as the information on pairwise features with class labels is missing. In this paper, based on a supervised similarity measure, a biconvex optimization problem is formulated for holistic feature section with a quadratically weighted objective function subject to linear equality and nonnegativity constraints. In addition, an iteratively reweighted convex quadratic program is reformulated. A two-timescale duplex neurodynamic system is applied to solve the formulated biconvex optimization problem and a projection neural network is customized to solve the iteratively reweighted convex optimization problem. Experimental results of the proposed neurodynamics-based supervised feature selection are elaborated in comparison with several existing feature selection methods based on twenty benchmark datasets to substantiate the efficacy and superiority of the neurodynamics-based method for selecting informative features in classification.}
}
@article{CHAVOSHINEJAD2023109282,
title = {Self-supervised semi-supervised nonnegative matrix factorization for data clustering},
journal = {Pattern Recognition},
volume = {137},
pages = {109282},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109282},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007610},
author = {Jovan Chavoshinejad and Seyed Amjad Seyedi and Fardin {Akhlaghian Tab} and Navid Salahian},
keywords = {Nonnegative matrix factorization, Semi-supervised learning, Self-supervised learning, Ensemble clustering},
abstract = {Semi-supervised nonnegative matrix factorization exploits the strengths of matrix factorization in successfully learning part-based representation and is also able to achieve high learning performance when facing a scarcity of labeled data and a large amount of unlabeled data. Its major challenge lies in how to learn more discriminative representations from limited labeled data. Furthermore, self-supervised learning has been proven very effective at learning representations from unlabeled data in various learning tasks. Recent research works focus on utilizing the capacity of self-supervised learning to enhance semi-supervised learning. In this paper, we design an effective Self-Supervised Semi-Supervised Nonnegative Matrix Factorization (S4NMF) in a semi-supervised clustering setting. The S4NMF directly extracts a consensus result from ensembled NMFs with similarity and dissimilarity regularizations. In an iterative process, this self-supervisory information will be fed back to the proposed model to boost semi-supervised learning and form more distinct clusters. The proposed iterative algorithm is used to solve the given problem, which is defined as an optimization problem with a well-formulated objective function. In addition, the theoretical and empirical analyses investigate the convergence of the proposed optimization algorithm. To demonstrate the effectiveness of the proposed model in semi-supervised clustering, we conduct extensive experiments on standard benchmark datasets. The source code for reproducing our results can be found at https://github.com/ChavoshiNejad/S4NMF.}
}
@article{YANG2023109348,
title = {Sparse possibilistic c-means clustering with Lasso},
journal = {Pattern Recognition},
volume = {138},
pages = {109348},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109348},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000493},
author = {Miin-Shen Yang and Josephine B.M. Benjamin},
keywords = {Clustering, Possibilistic c-means (PCM), Feature weights, Sparsity, Lasso, Spare PCM (S-PCM)},
abstract = {Krishnapuram and Keller first proposed possibilistic c-means (PCM) clustering in 1993. Afterward, PCM was widely studied with various extensions. The PCM algorithm and its extensions always treat feature components under equal importance, but, in real applications, different features may better have different weights. Recently, Yang and Benjamin in 2021 proposed a feature-weighted PCM clustering with feature reduction. Although Yang and Benjamin (2021) can reduce feature dimensions, it still encounters the curse of dimensionality for high dimensional data. One possible way to address this problem is to conduct a sparse clustering technique. In this paper, we further study the PCM clustering by incorporating the idea of sparsity with different feature weights. We propose two approaches that use the PCM clustering with the least absolute shrinkage and selection operator (Lasso). The first one is the sparse PCM subject to a Lasso constraint of feature weights, called S-PCM1. The second is the sparse PCM by adding a Lasso penalty term of feature weights in the objective function, called S-PCM2. We show that S-PCM1 and S-PCM2 are theoretically the same, and both can induce sparsity in features, but they use different procedures in algorithms. Synthetic and real data sets are used to compare S-PCM1 and S-PCM2 with some existing sparsity clustering algorithms. Experimental results and comparisons demonstrate the effectiveness and usefulness of the proposed S-PCM1 and S-PCM2 clustering algorithms.}
}
@article{HSU2023109294,
title = {Recurrent wavelet structure-preserving residual network for single image deraining},
journal = {Pattern Recognition},
volume = {137},
pages = {109294},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109294},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007737},
author = {Wei-Yen Hsu and Wei-Chi Chang},
keywords = {Single image deraining, Recurrent wavelet residual network, Structure preservation, Image weighted blending},
abstract = {The combination of deep learning and image prior has been widely used in single image deraining since 2017. Recent studies have demonstrated an excellent deraining effect on the high-frequency part of rain images, but less attention was paid to the low-frequency part of rain images. The rain streaks remain in the low-frequency part of rain images, thus limiting the deraining effect. Since the rain streaks in rain images are often mixed with object edges and background scenes, it is challenging to separate rain from them by directly learning the deraining function in the image domain. To solve these problems, we propose a novel Recurrent Wavelet Structure-preserving Residual Network (RWSRNet), which mainly preserves and introduces the low-frequency sub-images of each level into the low-frequency rain removal sub-networks that are greatly different from the state-of-the-art approaches introducing wavelet transform. In addition, we also share the low-frequency structure information to the high-frequency sub-networks through block connection, which further enriches the detailed information, facilitates convergence, and strengthens the ability of our network to remove rain streaks in high frequency. Finally, we fuse the derained low-frequency sub-images of each level through the proposed image weighted blending module and finally reconstruct the low- and high-frequency sub-images into clean images through inverse wavelet transform recursively. The experimental results indicate that the proposed method achieves an excellent deraining effect on both low- and high-frequency parts of rain images and has better performance in low-frequency preservation and high-frequency enhancement in comparison with the state-of-the-art approaches on synthetic and real image datasets.}
}
@article{ZHAO2023109352,
title = {Improving generalization of double low-rank representation using Schatten-p norm},
journal = {Pattern Recognition},
volume = {138},
pages = {109352},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109352},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000535},
author = {Jiaoyan Zhao and Yongsheng Liang and Shuangyan Yi and Qiangqiang Shen and Xiaofeng Cao},
keywords = {Low-rank representation, Schatten- norm, Feature extraction, Subspace clustering},
abstract = {Low-rank representation reveals a highly-informative entailment of sparse matrices, where double low-rank representation (DLRR) presents an effective solution by adopting nuclear norm. However, it is a special constraint of Schatten-p norm with p=1 which equally treats all singular values, deviating from the optimal low-rank representation that considers p=0. Thus, this paper improves the DLRR generalization of DLRR by relaxing p=1 into 0<p≤1 to tighten the low-rank constraint of the Schatten-p norm. With such a relaxation, low-rank optimization is then accelerated, resulting in a lower bound on the calculation complexity. Experiments on unsupervised feature extraction and subspace clustering demonstrate that our low-rank optimization taking 0<p≤1 achieves a superior performance against state-of-the-art methods.}
}
@article{QIU2023109300,
title = {Hierarchical nearest neighbor descent, in-tree, and clustering},
journal = {Pattern Recognition},
volume = {137},
pages = {109300},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109300},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000018},
author = {Teng Qiu and Yongjie Li},
keywords = {Clustering, In-tree, Hierarchical nearest neighbor descent, Mass cytometry},
abstract = {Recently, we have proposed a physically-inspired graph-theoretical method, called the Nearest Descent (ND), which is capable of organizing a dataset into an in-tree graph structure. Due to some beautiful and effective features, the constructed in-tree proves well-suited for data clustering. Although there exist some undesired edges (i.e., the inter-cluster edges) in this in-tree, those edges are usually very distinguishable, in sharp contrast to the cases in the famous Minimal Spanning Tree (MST). Here, we propose another graph-theoretical method, called the Hierarchical Nearest Neighbor Descent (HNND). Like ND, HNND also organizes a dataset into an in-tree, but in a more efficient way. Consequently, HNND-based clustering (HNND-C) is more efficient than ND-based clustering (ND-C) as well. This is well proved by the experimental results on five high-dimensional and large-size mass cytometry datasets. The experimental results also show that HNND-C achieves overall better performance than some state-of-the-art clustering methods.}
}
@article{COTOGNI2023109249,
title = {TreEnhance: A tree search method for low-light image enhancement},
journal = {Pattern Recognition},
volume = {136},
pages = {109249},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109249},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007282},
author = {Marco Cotogni and Claudio Cusano},
keywords = {Low-light image enhancement, Deep reinforcement learning, Automatic image retouching, Image processing, Tree search},
abstract = {In this paper we present TreEnhance, an automatic method for low-light image enhancement capable of improving the quality of digital images. The method combines tree search theory, and in particular the Monte Carlo Tree Search (MCTS) algorithm, with deep reinforcement learning. Given as input a low-light image, TreEnhance produces as output its enhanced version together with the sequence of image editing operations used to obtain it. During the training phase, the method repeatedly alternates two main phases: a generation phase, where a modified version of MCTS explores the space of image editing operations and selects the most promising sequence, and an optimization phase, where the parameters of a neural network, implementing the enhancement policy, are updated. Two different inference solutions are proposed for the enhancement of new images: one is based on MCTS and is more accurate but more time and memory consuming; the other directly applies the learned policy and is faster but slightly less precise. As a further contribution, we propose a guided search strategy that “reverses” the enhancement procedure that a photo editor applied to a given input image. Unlike other methods from the state of the art, TreEnhance does not pose any constraint on the image resolution and can be used in a variety of scenarios with minimal tuning. We tested the method on two datasets: the Low-Light dataset and the Adobe Five-K dataset obtaining good results from both a qualitative and a quantitative point of view.}
}
@article{YUAN2023109289,
title = {A lightweight network for smoke semantic segmentation},
journal = {Pattern Recognition},
volume = {137},
pages = {109289},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109289},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007683},
author = {Feiniu Yuan and Kang Li and Chunmei Wang and Zhijun Fang},
keywords = {Smoke semantic segmentation, Deep learning, Attention mechanism, Lightweight network, Channel split and shuffle},
abstract = {To obtain real-time performance on computation limited devices, we propose a lightweight network for smoke segmentation. To enhance the ability of feature encoding, we first propose an Attention Encoding Module (AEM) by designing a Channel Split and Shuffle Attention Module (CSSAM), which can extract powerful features and reduce computations simultaneously. CSSAM adopts Channel split and shuffle to greatly reduce learnable parameters for improving computation speed, and uses attention mechanism to focus on salient objects to enhance the effectiveness of features. In addition, AEM repeatedly stacks CSSAM in different encoding stages to achieve scale invariance. For the middle-level features of encoding stages, we propose a Spatial Enhancement Module (SEM) to boost the representation ability of spatial details. SEM concatenates feature maps produced by average and maximum pooling to achieve dominant and global responses, which are then weighted by the activated output of global average pooling to generate attention features. In the highest level of encoding stages, we present a Channel Attention Module (CAM) to explicitly model interdependency between channels. By reshaping 2D features into 1D features, we use element-wise matrix multiplications to reduce computation complexity for extracting channel-related information. Finally, we design a Feature Fusion Module (FFM) and a Global Coefficient Path (GCP) to fuse the outputs of SEM and CAM in an attention way for further improving robustness of final features. Experiments show that our method is significantly superior to existing state-of-the-art algorithms in smoke datasets, and also obtains excellent results in both synthetic and real smoke datasets. However, our method has less than 1 M network parameters.}
}
@article{JIANG2023109265,
title = {Sparse norm regularized attribute selection for graph neural networks},
journal = {Pattern Recognition},
volume = {137},
pages = {109265},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109265},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007440},
author = {Bo Jiang and Beibei Wang and Bin Luo},
keywords = {Graph neural networks, Feature selection, Sparse regularization, Semi-supervised learning},
abstract = {Graph Neural Networks (GNNs) have been widely used for graph learning tasks. The main aspect of GNN’s layer-wise message passing is conducting attribute/feature propagation on graph. Most existing GNNs generally conduct feature propagation across all feature dimensions. However, in many real applications, attributes usually contain irrelevant and redundant noise. In this case, attribute/feature selection is desired to extract meaningful features and eliminate noisy ones for GNN’s layer-wise propagation. Based on this observation, in this paper, we combine ℓ2,1/ℓ1-norm regularized attribute selection and GNNs together and propose a novel Attribute selection guided GNNs (AsGNNs) for graph data representation. AsGNNs aim to adaptively select some desired meaningful features/attributes that best serve GNNs. Moreover, an effective optimization framework has also been derived to train the proposed AsGNNs. The proposed AsGNNs provide a general framework which can incorporate any GNNs to conduct feature selection for layer-wise propagation. In this paper, we implement AsGNNs on both graph convolutional network (GCN) and graph attention network (GAT) and develop AsGCN and AsGAT for graph learning. Experimental results on several benchmark datasets demonstrate the effectiveness of the proposed AsGNNs (AsGCN, AsGAT) on semi-supervised learning tasks.}
}
@article{JIANG2023109277,
title = {Lightweight Semi-supervised Network for Single Image Rain Removal},
journal = {Pattern Recognition},
volume = {137},
pages = {109277},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109277},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007567},
author = {Nanfeng Jiang and Jiawei Luo and Junhong Lin and Weiling Chen and Tiesong Zhao},
keywords = {Rain Removal, Image Processing, Lightweight Network, Semi-supervised Learning},
abstract = {Deep learning technologies have shown their advantages in Single Image Rain Removal (SIRR) tasks. However, the derained results of most methods are limited to some challenges. First, due to the lack of real-world rainy/clean image pairs, many methods seriously rely on the labeled synthetic training images and will not effectively remove complex rain streaks in real-world scenarios. Second, most existing SIRR models require high computing power, which considerably limits their real-world applications. To address these issues, we propose a Lightweight Semi-supervised Network (LSNet) for SIRR. Our LSNet utilizes a compact semi-supervised framework to improve generalization ability in real-world rainy images removal. Meanwhile, in our semi-supervised framework, we also design a cascaded sub-network, which progressively removes complex rain streaks via a multi-stage manner. Specially, the multi-stage manner is based on a series of cascaded blocks, where we conduct recursive learning strategy to reduce model parameters. Extensive experimental results demonstrate that our method achieves comparable performance to the state-of-the-arts while has fewer parameters.}
}
@article{HERRMANN2023109333,
title = {Amercing: An intuitive and effective constraint for dynamic time warping},
journal = {Pattern Recognition},
volume = {137},
pages = {109333},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109333},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000341},
author = {Matthieu Herrmann and Geoffrey I. Webb},
keywords = {Time series, Dynamic time warping, Elastic distance},
abstract = {Dynamic Time Warping (DTW) is a time series distance measure that allows non-linear alignments between series. Constraints on the alignments in the form of windows and weights have been introduced because unconstrained DTW is too permissive in its alignments. However, windowing introduces a crude step function, allowing unconstrained flexibility within the window, and none beyond it. While not entailing a step function, a multiplicative weight is relative to the distances between aligned points along a warped path, rather than being a direct function of the amount of warping that is introduced. In this paper, we introduce Amerced Dynamic Time Warping (ADTW), a new, intuitive, DTW variant that penalizes the act of warping by a fixed additive cost. Like windowing and weighting, ADTW constrains the amount of warping. However, it avoids both abrupt discontinuities in the amount of warping allowed and the limitations of a multiplicative penalty. We formally introduce ADTW, prove some of its properties, and discuss its parameterization. We show on a simple example how it can be parameterized to achieve an intuitive outcome, and demonstrate its usefulness on a standard time series classification benchmark. We provide a demonstration application in C++ Herrmann(2021)[1].}
}
@article{SHAO2023109253,
title = {Twin SVM for conditional probability estimation in binary and multiclass classification},
journal = {Pattern Recognition},
volume = {136},
pages = {109253},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109253},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007324},
author = {Yuan-Hai Shao and Xiao-Jing Lv and Ling-Wei Huang and Lan Bai},
keywords = {Support vector machine, Twin support vector machines, Conditional probability, Binary classification, Multiclass classification},
abstract = {In this paper, we estimate the conditional probability function by presenting a new twin SVM model (CPTWSVM) in binary and multiclass classification problems. The motivation of CPTWSVM is to implement the empirical risk minimization on training data, which is hard to realize in traditional twin SVMs. In each subproblem of CPTWSVM, it measures the empirical risk and outputs the corresponding probability estimate of each class, which eliminates the problems of inconsistent measurement in twin SVMs. Though an additional discriminant objective function is introduced, the optimization problem size of each subproblem is smaller than conditional probability SVM, and is solved by block decomposition algorithm efficiently. In addition, we extend CPTWSVM to multiclass classification by estimating the conditional probability of each class, and maintaining the above properties. Numerical experiments on benchmark and real application datasets demonstrate that CPTWSVM outputs the estimate of probability and the data projection well, resulting in better generalization ability than some leading TWSVMs communities, in terms of binary and multiclass classification.}
}
@article{JIA2023109357,
title = {Multi-dimensional multi-label classification: Towards encompassing heterogeneous label spaces and multi-label annotations},
journal = {Pattern Recognition},
volume = {138},
pages = {109357},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109357},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000584},
author = {Bin-Bin Jia and Min-Ling Zhang},
keywords = {Machine learning, Supervised learning, Multi-dimensional classification, Multi-label classification, Multi-dimensional multi-label classification},
abstract = {In traditional classification framework, the semantics of each object is usually characterized by annotating a single class label from one homogeneous label space. Nonetheless, objects with rich semantics naturally arise in real-world applications whose properties need to be characterized in a more sophisticated manner. In this paper, a new classification framework named Multi-Dimensional Multi-Label (MDML) classification is investigated which models objects with rich semantics by encompassing heterogeneous label spaces and multi-label annotations. Specifically, MDML generalizes the traditional classification framework by assuming a number of heterogeneous label spaces to characterize semantics from different dimensions, where each object is further annotated with multiple class labels from each heterogeneous label space. To learn from MDML training examples, a first attempt named CLIM is proposed based on an augmented stacking strategy. Firstly, CLIM induces a base multi-label predictive model w.r.t. each label space by maximizing the likelihood of the observed multiple class labels. Secondly, the thresholding predictions from all base models are used to augment the original feature space to yield stacked multi-label predictive models. The two-level models are refined alternately via empirical threshold tuning. Experiments on four real-world MDML data sets validate the effectiveness of CLIM in learning from training examples with heterogeneous label spaces and multi-label annotations.}
}
@article{KE2023109305,
title = {Granularity-aware distillation and structure modeling region proposal network for fine-grained image classification},
journal = {Pattern Recognition},
volume = {137},
pages = {109305},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109305},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000067},
author = {Xiao Ke and Yuhang Cai and Baitao Chen and Hao Liu and Wenzhong Guo},
keywords = {Fine-grained visual classification, Multi-granularity feature learning, Knowledge distillation, Structure modeling},
abstract = {Fine-grained visual classification (FGVC) aims to identify objects belonging to multiple sub-categories of the same super-category. The key to solving fine-grained classification problems is to learn discriminative visual feature representation with only subtle differences. Although previous work based on refined feature learning has made great progress, however, high-level semantic features often lack key information for fine-grained visual object nuances. How to efficiently integrate semantic information of different granularities from classification networks is a critical. In this paper, we propose Granularity-aware Distillation and Structure Modeling region Proposal Network(GDSMP-Net). Our solution integrates multi-granularity hierarchical information through a multi-granularity fusion learning strategy to enhance feature representation. In view of the inherent challenges of large intra-class differences in FGVC, a cross-layer self-distillation regularization is proposed to to strengthen the connection between high-level semantics and low-level semantics for robust multi-granularity feature learning. On this basis, we use a weakly supervised method to generate local branches, and the collaborative learning of discriminative semantics and structural semantics based on local regions, facilitating model to perceive contextual information to capture structural interactions between local semantics. Comprehensive experiments show that our method achieves state-of-the-art performance on four widely-used challenging datasets.(CUB-200-2011, Stanford Cars, FGVC-Aircraft and NA-birds).}
}
@article{WU2023109293,
title = {Geometric-aware dense matching network for 6D pose estimation of objects from RGB-D images},
journal = {Pattern Recognition},
volume = {137},
pages = {109293},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109293},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007725},
author = {Chenrui Wu and Long Chen and Shenglong Wang and Han Yang and Junjie Jiang},
keywords = {6D pose estimation, Metric learning, Triplet loss, Dense correspondences, Geometric constraint},
abstract = {6D pose estimation for certain targets from RGB-D images is a fundamental problem in computer vision. Current methods emphasize learning the overall expression of the targets, which leads to poor performance under occlusion and truncation conditions. In this paper, we propose using a geometric-aware dense matching network to obtain visible dense correspondences between a RGB-D image and 3D model to address difficult predictions from unseen keypoints. Two geometrical structures are considered for dense matching. (1) The neighbor area of the correspondences is treated as suboptimal matches in addition to the correspondence to reduce the influence of the error caused by ground truth calibration. (2) The distance consistency of the correspondences is leveraged to eliminate the ambiguity from the symmetrical objects. Experiments on LM-O dataset (77.1% ADD(S)-0.1d) and YCB-V dataset (97.6% ADD(S)) show the effectiveness and advantages of our proposed method.11The source code will soon be available at https://github.com/Ray0089/geometric-aware-dense-matching.}
}
@article{GAN2023109317,
title = {Characters as graphs: Interpretable handwritten Chinese character recognition via Pyramid Graph Transformer},
journal = {Pattern Recognition},
volume = {137},
pages = {109317},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109317},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000183},
author = {Ji Gan and Yuyan Chen and Bo Hu and Jiaxu Leng and Weiqiang Wang and Xinbo Gao},
keywords = {Handwritten Chinese character Recognition, Transformer, Graph convolutional network, Pyramid graph},
abstract = {It is meaningful but challenging to teach machines to recognize handwritten Chinese characters. However, conventional approaches typically view handwritten Chinese characters as either static images or temporal trajectories, which may ignore the inherent geometric semantics of characters. Instead, here we first propose to represent handwritten characters as skeleton graphs, explicitly considering the natural characteristics of characters (i.e., characters as graphs). Furthermore, we propose a novel Pyramid Graph Transformer (PyGT) to specifically process the graph-structured characters, which fully integrates the advantages of Transformers and graph convolutional networks. Specifically, our PyGT can learn better graph features through (i) capturing the global information from all nodes with graph attention mechanism and (ii) modelling the explicit local adjacency structures of nodes with graph convolutions. Furthermore, the PyGT learns the multi-resolution features by constructing a progressive shrinking pyramid. Compared with existing approaches, it is more interpretable to recognize characters as geometric graphs. Moreover, the proposed method is generic for both online and offline handwritten Chinese character recognition (HCCR), and it also can be feasibly extended to handwritten text recognition. Extensive experiments empirically demonstrate the superiority of PyGT over the prevalent approaches including 2D-CNN, RNN/1D-CNN, and Vision Transformer (ViT) for HCCR. The code is available at https://github.com/ganji15/PyGT-HCCR.}
}
@article{2024110462,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {151},
pages = {110462},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(24)00213-9},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002139}
}
@article{ELHARROUSS2023109361,
title = {Refined edge detection with cascaded and high-resolution convolutional network},
journal = {Pattern Recognition},
volume = {138},
pages = {109361},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109361},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000626},
author = {Omar Elharrouss and Youssef Hmamouche and Assia Kamal Idrissi and Btissam {El Khamlichi} and Amal {El Fallah-Seghrouchni}},
keywords = {Edge detection, Convolutional neural networks, Deep learning, Scale-representation, Backbone},
abstract = {Edge detection is represented as one of the most challenging tasks in computer vision, due to the complexity of detecting the edges or boundaries in real-world images that contains objects of different types and scales like trees, building as well as various backgrounds. Edge detection is represented also as a key task for many computer vision applications. Using a set of backbones as well as attention modules, deep-learning-based methods improved the detection of edges compared with traditional methods like Sobel or Canny. However, images of complex scenes still represent a challenge for these methods. Also, the detected edges using the existing approaches suffer from non-refined results with erroneous edges. In this paper, we attempted to overcome these challenges for refined edge detection using a cascaded and high-resolution network named (CHRNet). By maintaining the high resolution of edges during the training process, and conserving the resolution of the edge image during the network stage, sub-blocks are connected at every stage with the output of the previous layer. Also, after each layer, we use batch normalization layer with an active affine parameter as an erosion operation for the homogeneous region in the image. The proposed method is evaluated using the most challenging datasets including BSDS500, NYUD, and Multicue. The obtained results outperform the designed edge detection networks in terms of performance metrics and quality of output images.The code is available at: https://github.com/elharroussomar/chrnet/}
}
@article{WANG2023109309,
title = {ProbSAP: A comprehensive and high-performance system for student academic performance prediction},
journal = {Pattern Recognition},
volume = {137},
pages = {109309},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109309},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000109},
author = {Xinning Wang and Yuben Zhao and Chong Li and Peng Ren},
keywords = {Student academic performance, SAP prediction, Educational data mining (EDM), Imbalanced data management, XGBoost-Enhanced method},
abstract = {The student academic performance prediction is becoming an indispensable service in the computer supported intelligent education system. But conventional machine learning-based methods can only exploit the sparse discriminative features of student behaviors in imbalanced academic datasets to predict student academic performance (SAP). Furthermore, there is a lack of imbalanced data processing mechanisms that can efficiently capture student characteristics and achievement. Therefore, we propose a comprehensive and high-performance prediction framework to probe SAP characteristics (ProbSAP) on massive educational data, which can resolve imbalanced data issue and improve academic prediction performance for making course final mark prediction. It consists of three main components: collaborative data processing module for enhancing the data quality, scalable metadata clustering module for alleviating the imbalance of academic features, and XGBoost-enhanced SAP prediction module for academic performance forecasting. The collaborative data processing module integrates multi-dimensional academic data, which sustains a good supply for clustering and modeling in the ProbSAP framework. The comparative evaluation results demonstrate that ProbSAP delivers superior accuracy and efficiency improvement for the course final mark prediction of college students over other state-of-the-art methods such as CNN, SVR, RFR, XGBoost, Catboost-SHAP, and AS-SAN. On average, ProbSAP reduces the mean absolute error (MAE) by 84.76%, 72.11%, and 66.49% compared with XGBoost, Catboost-SHAP, and AS-SAN, respectively. It also leads to a better out-sample fit that minimizes prediction errors between 1% and 9% with over 98% of actual samples.}
}
@article{HOUFAR2023109281,
title = {Automatically weighted binary multi-view clustering via deep initialization (AW-BMVC)},
journal = {Pattern Recognition},
volume = {137},
pages = {109281},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109281},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007609},
author = {Khamis Houfar and Djamel Samai and Fadi Dornaika and Azeddine Benlamoudi and Khaled Bensid and Abdelmalik Taleb-Ahmed},
keywords = {Multi-view clustering, Large scale, Anchors, Discrete representation and BD-FFT},
abstract = {Clustering is inherently a process of exploratory data analysis. It has attracted more attention recently because much real-world data consists of multiple representations or views. However, it becomes increasingly problematic when dealing with large and heterogeneous data. It is worth noting that several approaches have been developed to increase computational efficiency, although most of them have some drawbacks: (1) Most existing techniques consider equal or static weights to quantify importance across different views and samples, so common and complementary features cannot be used. (2) The clustering task is performed by arbitrary initialization without caring about the rich structure of the joint discrete representation, and thus poorly executed. In this paper, we propose a novel approach called “Auto-Weighted Binary Multi-View Clustering Via Deep Initialization” for large-scale multi-view clustering based on two main scenarios. First, we consider the distinction between different views based on the importance of samples, and therefore apply a dynamic learning strategy for the automatic weighting of views and samples. Second, in the context of initializing binary clustering, we develop a new CNN feature and use a low-dimensional binary embedding by exploiting the efficient capabilities of Fourier mapping. Moreover, our approach simultaneously learns a joint discrete representation and performs direct clustering using a constrained binary matrix factorization; the optimization problem is perfectly solved in a unified learning model. Experimental results conducted on several challenging datasets demonstrate the effectiveness and superiority of the proposed approach over state-of-the-art methods in terms of accuracy, normalized mutual information, and purity.}
}
@article{ZHANG2023109345,
title = {Frequency learning attention networks based on deep learning for automatic modulation classification in wireless communication},
journal = {Pattern Recognition},
volume = {137},
pages = {109345},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109345},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000468},
author = {Duona Zhang and Yuanyao Lu and Yundong Li and Wenrui Ding and Baochang Zhang and Jing Xiao},
keywords = {Frequency learning, Attention mechanism, Automatic modulation classification, Wireless communication},
abstract = {Deep neural networks have been recently applied in automatic modulation classification task and achieved remarkable success. However, Existing neural networks mainly focus on the purely data-driven architecture design, and fail to explore the hand-crafted feature mechanisms which are particularly significant for radio signal presentation in wireless communication. Inspired by digital signal processing theories, we propose frequency learning attention networks (FLANs) to analyze the radio spectral bias from frequency perspective, based on a multi-spectral attention mechanism for learning-based frequency components selection. FLANs are the general case of classical global average pooling and leverage identical structures of the popular neural networks. Extensive experiments have been conducted to validate the superiority of FLANs for automatic modulation classification over a wide variety of state-of-the-art methods on RADIOML 2018.01A dataset.}
}
@article{CHEN2023109321,
title = {Non-residual unrestricted pruned ultra-faster line detection for edge devices},
journal = {Pattern Recognition},
volume = {137},
pages = {109321},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109321},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000225},
author = {Pengpeng Chen and Dongjingdian Liu and Shouwan Gao},
keywords = {Line detection, Network slimming, Edge computing, Network interpretability},
abstract = {Line detection with deep learning is a popular visual task that focuses mostly on lane detection. It requires quicker inference speed and lower consumption, especially for high-speed edge device applications. Based on the UFAST, we propose the Non-Residual Unrestricted Pruned Ultra-faster (NRUPU) line detection via a novel model compression method including non-interference structural reconstruction (NISR), shallow channel priority reservation (SCPR) pruning and non-residual equivalent transformation (NRET). NISR is a structure reconstruction scheme allocating residual branches into each layer to solve the cross-layer channel interference in ResNet-18. SCPR pruning directly uses the factors of BN layers to build channel importance evaluation for backbone and designs channel selection method for head based on data distribution consistency, reducing the parameters of each layer independently. Then NRET losslessly converts the multi-branch model to a single-branch one containing only convolution, linear, and relu, which reduces implementation complexity on edge devices. These designs follow the theoretical foundations: the data distribution transformation trend and effect of gradient back-propagation on model learning ability. Compared with previous pruning methods, our method optimizes not only the parameters of the model but also the structure of the model. We train NRUPU in RTX2080Ti and deploy tests on edge devices NVIDIA Jetson Xavier NX (NJXN) and Atlas 200 DK (A2DK). Extensive experiments are conducted on the dataset TuSimple, CULane and our belt dataset with 11,894 data. Results show that NRUPU achieves over 96% speed increase and over 66% parameter reduction on all datasets within 0.7% accuracy loss. The FPS can reach 749, 665 and 783 on RTX2080Ti, 133, 117 and 143 on NJNX, 178, 161 and 183 on A2DK respectively. The code is released at https://anonymous.4open.science/r/NRUPU-29B0.}
}
@article{2024110405,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {150},
pages = {110405},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(24)00156-0},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001560}
}
@article{CELESTINO2023109288,
title = {2D Image head pose estimation via latent space regression under occlusion settings},
journal = {Pattern Recognition},
volume = {137},
pages = {109288},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109288},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007671},
author = {José Celestino and Manuel Marques and Jacinto C. Nascimento and João Paulo Costeira},
keywords = {Head pose estimation, Occlusion, Latent space, Euler angles},
abstract = {Head orientation is a challenging Computer Vision problem that has been extensively researched having a wide variety of applications. However, current state-of-the-art systems still underperform in the presence of occlusions and are unreliable for many task applications in such scenarios. This work proposes a novel deep learning approach for the problem of head pose estimation under occlusions. The strategy is based on latent space regression as a fundamental key to better structure the problem for occluded scenarios. Our model surpasses several state-of-the-art methodologies for occluded HPE, and achieves similar accuracy for non-occluded scenarios. We demonstrate the usefulness of the proposed approach with: (i) two synthetically occluded versions of the BIWI and AFLW2000 datasets, (ii) real-life occlusions of the Pandora dataset, and (iii) a real-life application to human-robot interaction scenarios where face occlusions often occur. Specifically, the autonomous feeding from a robotic arm.}
}
@article{DHAMANASKAR2023109358,
title = {Enhancing egocentric 3D pose estimation with third person views},
journal = {Pattern Recognition},
volume = {138},
pages = {109358},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109358},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000596},
author = {Ameya Dhamanaskar and Mariella Dimiccoli and Enric Corona and Albert Pumarola and Francesc Moreno-Noguer},
keywords = {3D pose estimation, Self-supervised learning, Egocentric vision},
abstract = {We propose a novel approach to enhance the 3D body pose estimation of a person computed from videos captured from a single wearable camera. The main technical contribution consists of leveraging high-level features linking first- and third-views in a joint embedding space. To learn such embedding space we introduce First2Third-Pose, a new paired synchronized dataset of nearly 2000 videos depicting human activities captured from both first- and third-view perspectives. We explicitly consider spatial- and motion-domain features, combined using a semi-Siamese architecture trained in a self-supervised fashion. Experimental results demonstrate that the joint multi-view embedded space learned with our dataset is useful to extract discriminatory features from arbitrary single-view egocentric videos, with no need to perform any sort of domain adaptation or knowledge of camera parameters. An extensive evaluation demonstrates that we achieve significant improvement in egocentric 3D body pose estimation performance on two unconstrained datasets, over three supervised state-of-the-art approaches. The collected dataset and pre-trained model are available for research purposes.11https://github.com/nudlesoup/First2Third-Pose}
}
@article{LI2023109306,
title = {DeepSIR: Deep semantic iterative registration for LiDAR point clouds},
journal = {Pattern Recognition},
volume = {137},
pages = {109306},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109306},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000079},
author = {Qing Li and Cheng Wang and Chenglu Wen and Xin Li},
keywords = {Feature learning, 3D registration, LiDAR point clouds, Point score, Semantic segmentation},
abstract = {This paper proposes DeepSIR, a novel learning-based iterative registration framework for real-world 3D LiDAR point clouds. Specifically, a front-end semantic feature extraction (Semantic-feat) model is designed to fully explore semantic information in LiDAR data. To highlight the recognized objects of interest, we propose a novel point score that uses semantic and geometric information. To effectively integrate the extracted semantic features, geometric features, and point scores, we introduce an aggregation module to learn a hybrid feature on each point. Meanwhile, our method dynamically explores feature descriptions and optimizes poses through an iterative pipeline. Extensive experiments on outdoor driving datasets demonstrate that our DeepSIR achieves comparable performance to state-of-the-art methods and runs at a much faster speed. The source code will be made publicly available.11https://github.com/LeoQLi/DeepSIR}
}
@article{FU2023109310,
title = {Knowledge aggregation networks for class incremental learning},
journal = {Pattern Recognition},
volume = {137},
pages = {109310},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109310},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000110},
author = {Zhiling Fu and Zhe Wang and Xinlei Xu and Dongdong Li and Hai Yang},
keywords = {Class incremental learning, Catastrophic forgetting, Dual-branch network, Knowledge aggregation, Model compression},
abstract = {Most existing class incremental learning methods rely on storing old exemplars to avoid catastrophic forgetting. However, these methods inevitably face the gradient conflict problem, the inherent conflict between new streaming knowledge and existing knowledge in the gradient direction. To alleviate gradient conflict, this paper reuses the previous knowledge and expands the branch to accommodate new concepts instead of fine-tuning the original models. Specifically, this paper designs a novel dual-branch network called Knowledge Aggregation Networks. The previously trained model is frozen as a branch to retain existing knowledge, and a consistent trainable network is constructed as the other branch to learn new concepts. An adaptive feature fusion module is adopted to dynamically balance the two branches’ information during training. Moreover, a model compression stage maintains the dual-branch structure. Extensive experiments on CIFAR-100, ImageNet-Sub, and ImageNet show that our method significantly outperforms the other methods and effectively balances stability and plasticity.}
}
@article{SONG2023109276,
title = {Deep continual hashing with gradient-aware memory for cross-modal retrieval},
journal = {Pattern Recognition},
volume = {137},
pages = {109276},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109276},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007555},
author = {Ge Song and Xiaoyang Tan and Ming Yang},
keywords = {Cross-modal retrieval, Deep hashing, Continual learning, Multi-label},
abstract = {Cross-modal hashing (CMH) has become widely used for large-scale multimedia retrieval. However, most current CMH methods focus on the closed retrieval scenario, not the real-world environments, i.e., complex and changing semantics. When data containing new class objects emerge, the current CMH has to retrain the model on all history training data, not the new data, to accommodate new semantics, but the never-stop upload of data on the Internet makes this impractical. In this paper, we devise a deep hashing method called Continual Cross-Modal Hashing with Gradient Aware Memory (CCMH-GAM) for learning binary codes of multi-label cross-modal data with increasing categories. CCMH-GAM is a two-step hashing architecture, one hashing network learns to hash the increasing semantics of data, i.e., label, into the semantic codes, and other modality-specific hashing networks learn to map data into the corresponding semantic codes. Specifically, to keep the encoding ability for old semantics, a regularization based on accumulating low-storage label-code pairs is designed for the former network. For the modality-specific networks, we propose a memory construction method via approximating the full episodic gradients of all data by some exemplars and derive its fast implementation with the upper bound of approximation error. Based on this memory, we propose a gradient projection method to theoretically improve the probability of old data’s code being unchanged after updating the model. Extensive experiments on three datasets demonstrate that CCMH-GAM can continually learn hash functions and yield state-of-the-art retrieval performance.}
}
@article{CHEN2023109386,
title = {RACL: A robust adaptive contrastive learning method for conversational satisfaction prediction},
journal = {Pattern Recognition},
volume = {138},
pages = {109386},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109386},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000870},
author = {Gang Chen and Xiangge Li and Shuaiyong Xiao and Chenghong Zhang and Xianghua Lu}
}
@article{MENSI2023109334,
title = {Detecting outliers from pairwise proximities: Proximity isolation forests},
journal = {Pattern Recognition},
volume = {138},
pages = {109334},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109334},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000353},
author = {Antonella Mensi and David M.J. Tax and Manuele Bicego},
keywords = {Random forest, Outlier detection, Isolation, Pairwise distances},
abstract = {Because outliers are very different from the rest of the data, it is natural to represent outliers by their distances to other objects. Furthermore, there are many scenarios in which only pairwise distances are known, and feature-based outlier detection methods cannot directly be applied. Considering these observations, and given the success of Isolation Forests for (feature-based) outlier detection, we propose Proximity Isolation Forest, a proximity-based extension. The methodology only requires a set of pairwise distances to work, making it suitable for different types of data. Analogously to Isolation Forest, outliers are detected via their early isolation in the trees; to encode the isolation we design nine training strategies, both random and optimized. We thoroughly evaluate the proposed approach on fifteen datasets, successfully assessing its robustness and suitability for the task; additionally we compare favourably to alternative proximity-based methods.}
}
@article{KIM2023109292,
title = {Uncertainty-aware semi-supervised few shot segmentation},
journal = {Pattern Recognition},
volume = {137},
pages = {109292},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109292},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007713},
author = {Soopil Kim and Philip Chikontwe and Sion An and Sang Hyun Park},
keywords = {Few shot segmentation, Meta learning, Uncertainty estimation, Semi-supervised learning, Prototype},
abstract = {Few shot segmentation (FSS) aims to learn pixel-level classification of a target object in a query image using only a few annotated support samples. This is challenging as it requires modeling appearance variations of target objects and the diverse visual cues between query and support images with limited information. To address this problem, we propose a semi-supervised FSS strategy that leverages additional prototypes from unlabeled images with uncertainty guided pseudo label refinement. To obtain reliable prototypes from unlabeled images, we meta-train a neural network to jointly predict segmentation and estimate the uncertainty of predictions. We employ the uncertainty estimates to exclude predictions with high degrees of uncertainty for pseudo label construction to obtain additional prototypes from the refined pseudo labels. During inference, query segmentation is predicted using prototypes from both support and unlabeled images including low-level features of the query images. Our approach can easily supplement existing approaches without the requirement of additional training when employing unlabeled samples. Extensive experiments on PASCAL-5i and COCO-20i demonstrate that our model can effectively remove unreliable predictions to refine pseudo labels and significantly improve upon baseline performance.}
}
@article{DUAN2023109318,
title = {Arbitrary Order Total Variation for Deformable Image Registration},
journal = {Pattern Recognition},
volume = {137},
pages = {109318},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109318},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000195},
author = {Jinming Duan and Xi Jia and Joseph Bartlett and Wenqi Lu and Zhaowen Qiu},
keywords = {Image Registration, Nonlinear Optimisation, ADMM, Total Variation, Arbitrary Order Derivatives},
abstract = {In this work, we investigate image registration in a variational framework and focus on regularization generality and solver efficiency. We first propose a variational model combining the state-of-the-art sum of absolute differences (SAD) and a new arbitrary order total variation regularization term. The main advantage is that this variational model preserves discontinuities in the resultant deformation while being robust to outlier noise. It is however non-trivial to optimize the model due to its non-convexity, non-differentiabilities, and generality in the derivative order. To tackle these, we propose to first apply linearization to the model to formulate a convex objective function and then break down the resultant convex optimization into several point-wise, closed-form subproblems using a fast, over-relaxed alternating direction method of multipliers (ADMM). With this proposed algorithm, we show that solving higher-order variational formulations is similar to solving their lower-order counterparts. Extensive experiments show that our ADMM is significantly more efficient than both the subgradient and primal-dual algorithms particularly when higher-order derivatives are used, and that our new models outperform state-of-the-art methods based on deep learning and free-form deformation. Our code implemented in both Matlab and Pytorch is publicly available at https://github.com/j-duan/AOTV.}
}
@article{RASOOL2023109287,
title = {Overcoming weaknesses of density peak clustering using a data-dependent similarity measure},
journal = {Pattern Recognition},
volume = {137},
pages = {109287},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109287},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200766X},
author = {Zafaryab Rasool and Sunil Aryal and Mohamed Reda Bouadjenek and Richard Dazeley},
keywords = {Clustering, Density peak clustering, Similarity measure, Data-dependent similarity},
abstract = {Density Peak Clustering (DPC) is a popular state-of-the-art clustering algorithm, which requires pairwise (dis)similarity of data objects to detect arbitrary shaped clusters. While it is shown to perform well for many applications, DPC remains: (i) not robust for datasets with clusters having different densities, and (ii) sensitive to the change in the units/scales used to represent data. These drawbacks are mainly due to the use of the data-independent similarity measure based on the Euclidean distance. In this paper, we address these issues by proposing an effective data-dependent similarity measure based on Probability Mass, which we call MP-Similarity, and by incorporating it in DPC to create MP-DPC, a data-dependent variant of DPC. We evaluate and compare MP-DPC against diverse baselines using several clustering metrics and datasets. Our experiments demonstrate that: (a) MP-DPC produces better clustering results than DPC using the Euclidean distance and existing data-dependent similarity measures; (b) MP-Similarity coupled with Shared-Nearest-Neighbor-based density metric in DPC further enhances the quality of clustering results; and (c) unlike DPC with existing data-independent and data-dependent similarity measures, MP-DPC is robust to the change in the units/scales used to represent data. Our findings suggest that MP-Similarity provides a more viable solution for DPC in datasets with unknown distribution or units/scales of features, which is often the case in many real-world applications.}
}
@article{REN2023109362,
title = {Surface normal and Gaussian weight constraints for indoor depth structure completion},
journal = {Pattern Recognition},
volume = {138},
pages = {109362},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109362},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000638},
author = {Dongran Ren and Meng Yang and Jiangfan Wu and Nanning Zheng},
keywords = {Depth structure completion, Surface normal, Gaussian weight, Convolution neural network, Markov random field, Kernel least-square method},
abstract = {Raw depth maps captured by depth sensors generally contain missing contents due to glossy, transparent, and sparsity problems. Recent methods well completed flat regions of raw depth maps; however, ignored the accuracy of depth structures. In this paper, an effective depth structure completion method is developed to infer missing depth structures. First, a raw depth map is divided into flat regions and depth structures based on a structure prediction network. Second, two local features including surface normals and Gaussian weights are extracted from a reference RGB image to impose constraints on flat regions and depth structures, separately. Third, a kernel least-square module is adopted to handle the texture-copy artifacts problem. Finally, an iterative optimization model is developed by embedding the two constraints into a Markov random field. The cost function of the model comprises three terms, which limit data fidelity between completed depth map and raw depth map, smoothness of flat regions, and accuracy of depth structures, respectively. The proposed method is evaluated on four indoor datasets including Matterport3D, RealSense, ScanNet, and NYUv2, and compared with eight recent baselines. Quantitative results demonstrate that RMSE and MAE of completed depth maps are considerably reduced by 22.0% and 45.3%, respectively. Visual results show the superiority in completing depth structures and suppressing texture-copy artifacts. Generalization test verify the effectiveness on unseen datasets.}
}
@article{WANG2023109350,
title = {Self-supervised clustering with assistance from off-the-shelf classifier},
journal = {Pattern Recognition},
volume = {138},
pages = {109350},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109350},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000511},
author = {Hanxuan Wang and Na Lu and Huan Luo and Qinyang Liu},
keywords = {Deep clustering, Classification, Self-supervised, Sample selection},
abstract = {Deep clustering outperforms conventional clustering by mutually promoting representation learning and cluster assignment. However, most existing deep clustering methods suffer from two major drawbacks. Firstly, most cluster assignment methods are highly dependent on the intermediate target distribution generated by a handcrafted nonlinear mapping function. Secondly, the clustering results can be easily guided towards wrong direction by the misassigned samples in each cluster. The existing deep clustering methods are incapable of discriminating such samples. These facts largely limit the possible performance that deep clustering methods can reach. To address these issues, a novel Self-Supervised Clustering (SSC) framework is constructed, which boosts the clustering performance by classification in an unsupervised manner. Fuzzy theory is used to score the membership of each sample to the clusters in terms of probability in each training epoch, which evaluates the intermediate clustering result certainty of each sample. The most reliable samples can be selected with the help of a sample selection method according to the membership and enhanced by data augmentation method. These augmented data are employed to fine-tune an off-the-shelf deep network classifier with the labels provided by the clustering in a self-supervised way. The classification results of the original dataset are used as the target distribution to guide the training process of the deep clustering model. The proposed framework can efficiently discriminate sample outliers and generate better target distribution with the assistance of the powerful classifier. Extensive experiments indicate that the proposed framework remarkably outperforms state-of-the-art deep clustering methods on four benchmark datasets.}
}
@article{CHENG2023109270,
title = {Hybrid routing transformer for zero-shot learning},
journal = {Pattern Recognition},
volume = {137},
pages = {109270},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109270},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200749X},
author = {De Cheng and Gerong Wang and Bo Wang and Qiang Zhang and Jungong Han and Dingwen Zhang},
keywords = {Zero-shot learning, Hybrid routing, Transformer, Attention},
abstract = {Zero-shot learning (ZSL) aims to learn models that can recognize unseen image semantics based on the training of data with seen semantics. Recent studies either leverage the global image features or mine discriminative local patch features to associate the extracted visual features to the semantic attributes. However, due to the lack of the necessary top-down guidance and semantic alignment for ensuring the model attend to the real attribute-correlation regions, these methods still encounter a significant semantic gap between the visual modality and the attribute modality, which makes their prediction on unseen semantics unreliable. To solve this problem, this paper establishes a novel transformer encoder-decoder model, called hybrid routing transformer (HRT). In HRT encoder, we embed an active attention, which is constructed by both the bottom-up and the top-down dynamic routing pathways to generate the attribute-aligned visual feature. While in HRT decoder, we use static routing to calculate the correlation among the attribute-aligned visual features, the corresponding attribute semantics, and the class attribute vectors to generate the final class label predictions. This design makes the presented transformer model a hybrid of 1) top-down and bottom-up attention pathways and 2) dynamic and static routing pathways. Comprehensive experiments on three widely-used benchmark datasets, namely CUB, SUN, and AWA2, are conducted. The obtained experimental results demonstrate the effectiveness of the proposed method. Our code is released in https://github.com/KORIYN/HRT.}
}
@article{WANG2023109319,
title = {Reducing bi-level feature redundancy for unsupervised domain adaptation},
journal = {Pattern Recognition},
volume = {137},
pages = {109319},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109319},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000201},
author = {Mengzhu Wang and Shanshan Wang and Wei Wang and Li Shen and Xiang Zhang and Long Lan and Zhigang Luo},
keywords = {Feature redundancy, Unsupervised domain adaptation, Whitening, Orthogonality},
abstract = {Unsupervised domain adaptation (UDA) deals with the problem of transferring knowledge from a labeled source domain to an unlabeled target domain when the two domains have distinct data distributions. Therefore, the purpose of domain adaptation is to mitigate the distribution divergence between the two domains. Many existing UDA methods only use the traditional batch normalization layer, but this may lead to a large number of feature redundancy and lead performance degradation. In this paper, we introduce a novel deep learning paradigm called feature redundancy in UDA to enhance adaptation ability. Specifically, we first show that feature redundancy also exists on unsupervised domain adaptation (UDA), which has been ignored by most previous efforts. We utilize feature similarity as a metric to measure feature redundancy and then analyze the relationship between uniform feature spectrum and minimal feature similarity. Based on this relationship, we intend to reduce cross-domain feature redundancy for UDA by making the distribution of feature spectrum uniforms in a bi-level way. For the first level, we propose a cross-domain batch normalization with the whitening module (xBN) to ensure compact domain-specific features and learn domain-invariant features at the same time. With the domain-specific features from the first level that paves a way, on the second level, we suggest an alternative orthogonal regularizer (OR) that can make the distribution of the feature spectrum more uniform, thus domain-invariant feature redundancy is mitigated. Such a bi-level mechanism greatly reduces the feature redundancy for UDA. To evaluate the efficacy of the proposed bi-level mechanism, we plug those two novel modules (i.e., xBN and OR) into convolutional neural networks (CNNs) to form our UDA model and also conduct the corresponding empirical evaluations on five cross-domain object recognition benchmarks including both classical and large-scale image datasets. Experimental results show that the proposed UDA model could achieve state-of-the-art performance both in quantity and quality. Our source codes will be released after publication.}
}
@article{HU2023109299,
title = {Multi-modal unsupervised domain adaptation for semantic image segmentation},
journal = {Pattern Recognition},
volume = {137},
pages = {109299},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109299},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007786},
author = {Sijie Hu and Fabien Bonardi and Samia Bouchafa and Désiré Sidibé},
keywords = {Unsupervised domain adaptation, Multi-modal learning, Self-supervised learning, Knowledge transfer, Semantic segmentation},
abstract = {We propose a novel multi-modal-based Unsupervised Domain Adaptation (UDA) method for semantic segmentation. Recently, depth has proven to be a relevent property for providing geometric cues to enhance the RGB representation. However, existing UDA methods solely process RGB images or additionally cultivate depth-awareness with an auxiliary depth estimation task. We argue that geometric cues that are crucial to semantic segmentation, such as local shape and relative position, are challenging to recover from an auxiliary depth estimation task with mere color (RGB) information. In this paper, we propose a novel multi-modal UDA method named MMADT, which relies on both RGB and depth images as input. In particular, we design a Depth Fusion Block (DFB) to recalibrate depth information and leverage Depth Adversarial Training (DAT) to bridge the depth discrepancy between the source and target domain. Besides, we propose a self-supervised multi-modal depth estimation assistant network named Geo-Assistant (GA) to align the feature space of RGB and depth and shape the sensitivity of our MMADT to depth information. We experimentally observed significant performance improvement in multiple synthetic to real adaptation benchmarks, i.e., SYNTHIA-to-Cityscapes, GTA5-to-Cityscapes and SELMA-to-Cityscapes. Additionally, our multi-modal UDA scheme is easy to port to other UDA methods with a consistent performance boost.}
}
@article{YANG2023109275,
title = {RESKM: A General Framework to Accelerate Large-Scale Spectral Clustering},
journal = {Pattern Recognition},
volume = {137},
pages = {109275},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109275},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007543},
author = {Geping Yang and Sucheng Deng and Xiang Chen and Can Chen and Yiyang Yang and Zhiguo Gong and Zhifeng Hao},
keywords = {Machine learning, Spectral clustering, Unsupervised learning, Large-scale},
abstract = {Spectral Clustering is an effective preprocessing method in communities for its excellent performance, but its scalability still is a challenge. Many efforts have been made to face this problem, and several solutions are proposed, including Nyström Approximation, Sparse Representation Approximation, etc. However, according to our survey, there is still a large room for improvement. This work thoroughly investigates the factors relevant to large-scale Spectral Clustering and proposes a general framework to accelerate Spectral Clustering by utilizing the Robust and Efficient Spectral k-Means (RESKM). The contributions of RESKM are three folds: (1) a unified framework is proposed for large-scale Spectral Clustering; (2) it consists of four phases, each phase is theoretically analyzed, and the corresponding acceleration is suggested; (3) the majority of the existing large-scale Spectral Clustering methods can be integrated into RESKM and therefore be accelerated. Experiments on datasets with different scalability demonstrate that the robustness and efficiency of RESKM.}
}
@article{BREITENBACH2023109355,
title = {On a method for detecting periods and repeating patterns in time series data with autocorrelation and function approximation},
journal = {Pattern Recognition},
volume = {138},
pages = {109355},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109355},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000560},
author = {Tim Breitenbach and Bartosz Wilkusz and Lauritz Rasbach and Patrick Jahnke},
keywords = {Time series analysis, Time series modelling, Seasonality detection, Period detection, Time series decomposition},
abstract = {Detecting recurrent patterns in time series data is an important capability. The reason is that repeating patterns on the one hand indicate well defined processes that can be further analyzed once detected and on the other hand are a reliable feature to predict future occurrences and adapt accordingly. The challenge in real data to define a period is that a time series is usually also influenced by non-periodic dynamics and noise. In this work, a mathematical framework is proved to define regular patterns. Their properties are used within a suggested algorithm based on the concept of autocorrelation and function approximation to fit a model capturing the periodic part of the time series. Based on that model and a corresponding autocorrelation, a new score is defined to evaluate how well a hypothesized period fits to the time series. This score is particularly useful in a big data scenario where decisions for periodicity are needed to be taken automatically, which is one of the main achievement of the presented work. The period analysis algorithm is applied to data from two different use cases. The first one is a data center scenario where the information of the periodic pattern is used to create a feature that improves a machine learning framework predicting future resource demands. The feature represents the phase of the repeating pattern. In a second scenario, expression data from mice liver cells are investigated concerning periodic rhythms. A Python implementation of the presented algorithm is provided via a github repository under https://github.com/LauritzR/period-detection.}
}
@article{LAZARO2023109303,
title = {Neural network for ordinal classification of imbalanced data by minimizing a Bayesian cost},
journal = {Pattern Recognition},
volume = {137},
pages = {109303},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109303},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000043},
author = {Marcelino Lázaro and Aníbal R. Figueiras-Vidal},
keywords = {Bayes cost, Parzen windows, Ordinal classification, Imbalanced},
abstract = {Ordinal classification of imbalanced data is a challenging problem that appears in many real world applications. The challenge is to simultaneously consider the order of the classes and the class imbalance, which can notably improve the performance metrics. The Bayesian formulation allows to deal with these two characteristics jointly: It takes into account the prior probability of each class and the decision costs, which can be used to include the imbalance and the ordinal information, respectively. We propose to use the Bayesian formulation to train neural networks, which have shown excellent results in many classification tasks. A loss function is proposed to train networks with a single neuron in the output layer and a threshold based decision rule. The loss is an estimate of the Bayesian classification cost, based on the Parzen windows estimator, which is fitted for a thresholded decision. Experiments with several real datasets show that the proposed method provides competitive results in different scenarios, due to its high flexibility to specify the relative importance of the errors in the classification of patterns of different classes, considering the order and independently of the probability of each class.}
}
@article{MIN2023109291,
title = {Hybrid feature enhancement network for few-shot semantic segmentation},
journal = {Pattern Recognition},
volume = {137},
pages = {109291},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109291},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007701},
author = {Hai Min and Yemao Zhang and Yang Zhao and Wei Jia and Yingke Lei and Chunxiao Fan},
keywords = {Semantic segmentation, Few-shot segmentation, Few-shot learning},
abstract = {Although few-shot semantic segmentation methods have been widely studied in computer vision field, it still has room for improvement. In this work, we propose to enrich the feature representation with texture information and assign adaptive weights to losses. Specially, we incorporate the texture information obtained by texture enhance module with layer's features on ResNet, and then get a series of hybrid features. The incorporation of texture information enhances the similarity calculation to make the support set guidance more effective. Besides, the proposed adaptive loss makes the network optimize in a better direction. The experiments testify that the proposed method achieves the better results than that of previous methods on few-shot segmentation dataset such as PASCAL-5i, COCO-20i and FSS-1000.}
}
@article{MARTINS2023109359,
title = {Meta-learning for dynamic tuning of active learning on stream classification},
journal = {Pattern Recognition},
volume = {138},
pages = {109359},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109359},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000602},
author = {Vinicius Eiji Martins and Alberto Cano and Sylvio {Barbon Junior}},
keywords = {Meta-learning, Active learning, Data stream, Concept drift},
abstract = {Supervised data stream learning depends on the incoming sample’s true label to update a classifier’s model. In real life, obtaining the ground truth for each instance is a challenging process; it is highly costly and time consuming. Active Learning has already bridged this gap by finding a reduced set of instances to support the creation of a reliable stream classifier. However, identifying a reduced number of informative instances to support a suitable classifier update and drift adaptation is very tricky. To better adapt to concept drifts using a reduced number of samples, we propose an online tuning of the Uncertainty Sampling threshold using a meta-learning approach. Our approach exploits statistical meta-features from adaptive windows to meta-recommend a suitable threshold to address the trade-off between the number of labelling queries and high accuracy. Experiments exposed that the proposed approach provides the best trade-off between accuracy and query reduction by dynamic tuning the uncertainty threshold using lightweight meta-features.}
}
@article{YU2023109343,
title = {Low-rank tensor recovery via non-convex regularization, structured factorization and spatio-temporal characteristics},
journal = {Pattern Recognition},
volume = {137},
pages = {109343},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109343},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000444},
author = {Quan Yu and Ming Yang},
keywords = {Tensor completion, Tensor robust principle component analysis, Low-rank approximation, Dynamic background, Spatio-temporal characteristics},
abstract = {Recently, the convex low-rank 3rd-order tensor recovery has attracted considerable attention. However, there are some limitations to the convex relaxation approach, which may yield biased estimators. To overcome this disadvantage, we develop a novel non-convex tensor pseudo-norm to replace the weighted sum of the tensor nuclear norm as a tighter rank approximation. Then in tensor robust principle component analysis, we introduce the noise analysis to separate the spare foreground from the dynamic background more accurately. Furthermore, by introducing a spatio-temporal matrix, we can make better use of the inherent spatio-temporal characteristics of the low-rank static background and sparse foreground. Finally, we introduce an incoherent term to constrain the sparse foreground and the dynamic background to improve the separability. Some preliminary numerical examples of color image, video, and face image data sets are presented to illustrate the efficiency of our proposed methods.}
}
@article{MORALES2023109367,
title = {BabyNet: Reconstructing 3D faces of babies from uncalibrated photographs},
journal = {Pattern Recognition},
volume = {139},
pages = {109367},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109367},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000687},
author = {Araceli Morales and Antonia Alomar and Antonio R. Porras and Marius George Linguraru and Gemma Piella and Federico M. Sukno},
keywords = {3D face reconstruction, Graph neural network, Baby model},
abstract = {We present a 3D face reconstruction system that aims at recovering the 3D facial geometry of babies from uncalibrated photographs, BabyNet. Since the 3D facial geometry of babies differs substantially from that of adults, baby-specific facial reconstruction systems are needed. BabyNet consists of two stages: 1) a 3D graph convolutional autoencoder learns a latent space of the baby 3D facial shape; and 2) a 2D encoder that maps photographs to the 3D latent space based on representative features extracted using transfer learning. In this way, using the pre-trained 3D decoder, we can recover a 3D face from 2D images. We evaluate BabyNet and show that 1) methods based on adult datasets cannot model the 3D facial geometry of babies, which proves the need for a baby-specific method, and 2) BabyNet outperforms classical model-fitting methods even when a baby-specific 3D morphable model, such as BabyFM, is used.}
}
@article{PAN2023109316,
title = {Theoretical guarantee for crowdsourcing learning with unsure option},
journal = {Pattern Recognition},
volume = {137},
pages = {109316},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109316},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000171},
author = {Yigong Pan and Ke Tang and Guangzhong Sun},
keywords = {Machine learning, Crowdsourcing learning, Labeling},
abstract = {Crowdsourcing learning, in which labels are collected from multiple workers through crowdsourcing platforms, has attracted much attention during the past decade. This learning paradigm would reduce the labeling cost since crowdsourcing workers may be non-expert and hence less costly. On the other hand, crowdsourcing learning algorithms also suffer from being misled by incorrect labels introduced by imperfect workers. To control such risks, recently, it has been suggested to provide workers an additional unsure option during the labeling process. Although the benefits of the unsure option have been empirically demonstrated, theoretical analysis is still limited. In this article, a theoretical analysis of crowdsourcing learning with the unsure option is presented. Specifically, an upper bound of minimally sufficient number of crowd labels required for learning a probably approximately correct (PAC) classification model with and without the unsure option are given respectively. Next, a condition under which providing (or not providing) an unsure option to workers is derived. Then, the theoretical results are extended to guide non-identical label options (with or without unsure options) to different workers. Last, several useful applications are proposed based on theoretical results.}
}
@article{YIN2023109274,
title = {Hypergraph based semi-supervised symmetric nonnegative matrix factorization for image clustering},
journal = {Pattern Recognition},
volume = {137},
pages = {109274},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109274},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007531},
author = {Jingxing Yin and Siyuan Peng and Zhijing Yang and Badong Chen and Zhiping Lin},
keywords = {Symmetric nonnegative matrix factorization, Hypergraph learning, Semi-supervised learning, Clustering},
abstract = {Semi-supervised symmetric nonnegative matrix factorization (SNMF) has been shown to be a significant method for both linear and nonlinear data clustering applications. Nevertheless, existing SNMF-based methods only adopt a simple graph to construct the similarity matrix, and cannot fully use the limited supervised information for the construction of the similarity matrix. To overcome the drawbacks of previous SNMF-based methods, a new semi-supervised SNMF-based method called hypergraph based semi-supervised SNMF (HSSNMF), is proposed in this paper for image clustering. Specifically, HSSNMF adopts a predefined hypergraph to build a similarity matrix for capturing the high-order relationships of samples. By exploiting a new hypergraph based pairwise constraints propagation (HPCP) algorithm, HSSNMF propagates the pairwise constraints of the limited data points to the entire data points, which can make full use of the limited supervised information and construct a more informative similarity matrix. Using the multiplicative updating algorithm, a discriminative assignment matrix can then be obtained by solving the optimization problem of HSSNMF. Moreover, analyses of the convergence, supervisory information, and computational complexity of HSSNMF are presented. Finally, extensive clustering experiments have been conducted on six real-world image datasets, and the experimental results have demonstrated the superiority of HSSNMF while compared with several state-of-the-art methods.}
}
@article{ROY2023109315,
title = {Multi-scale attention guided pose transfer},
journal = {Pattern Recognition},
volume = {137},
pages = {109315},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109315},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300016X},
author = {Prasun Roy and Saumik Bhattacharya and Subhankar Ghosh and Umapada Pal},
keywords = {Pose transfer, Attention, GAN, DeepFashion},
abstract = {Pose transfer refers to the probabilistic image generation of a person with a previously unseen novel pose from another image of that person having a different pose. Due to potential academic and commercial applications, pose transfer has been extensively studied in recent years. Among the various approaches to the problem, attention guided progressive generation is shown to produce state-of-the-art results in most cases. This paper presents an improved network architecture for pose transfer by introducing attention links at every resolution level of the encoder and decoder. By utilizing such dense multi-scale attention guided approach, we are able to achieve significant improvement over the existing methods both visually and analytically. We conclude our findings with extensive qualitative and quantitative comparisons against several existing methods on the DeepFashion dataset. We also show the generality of the proposed network architecture by extending it to multiple application domains, such as semantic reconstruction, virtual try-on and style manipulation.11For reproducibility, the code implementation and the pre-trained models are available at https://github.com/prasunroy/pose-transfer.}
}
@article{LI2023109356,
title = {Semi-supervised vector-valued learning: Improved bounds and algorithms},
journal = {Pattern Recognition},
volume = {138},
pages = {109356},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109356},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000572},
author = {Jian Li and Yong Liu and Weiping Wang},
keywords = {Vector-valued learning, Semi-supervised learning, Excess risk bound, Local rademacher complexity},
abstract = {Vector-valued learning, where the output space admits a vector-valued structure, is an important problem that covers a broad family of important domains, e.g. multi-task learning and transfer learning. Using local Rademacher complexity and unlabeled data, we derive novel semi-supervised excess risk bounds for general vector-valued learning from both kernel perspective and linear perspective. The derived bounds are much sharper than existing ones and the convergence rates are improved from the square root of labeled sample size to the square root of total sample size or directly dependent on labeled sample size. Motivated by our theoretical analysis, we propose a general semi-supervised algorithm for efficiently learning vector-valued functions, incorporating both local Rademacher complexity and Laplacian regularization. Extensive experimental results illustrate the proposed algorithm significantly outperforms the compared methods, which coincides with our theoretical findings.}
}
@article{SUN2023109304,
title = {Detail enhancement-based vehicle re-identification with orientation-guided re-ranking},
journal = {Pattern Recognition},
volume = {137},
pages = {109304},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109304},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000055},
author = {Ziruo Sun and Xiushan Nie and Xiaopeng Bi and Shaohua Wang and Yilong Yin},
keywords = {Vehicle re-identification, Detail enhancement, Re-ranking},
abstract = {Vehicle re-identification (re-ID) has attracted extensive attention in the field of computer vision owing to the development of smart cities. Two issues remain in the task of vehicle re-ID: minor inter-class differences and extreme orientation variations. To address them, we propose a detailed enhancement-based vehicle re-ID method with orientation-guided re-ranking. In the proposed method, a novel enhancement module using stripe-based partition, is presented to avoid the negative influence of minor inter-class difference. The stripe-based partition subdivides the feature map of middle layers in the network into two stripes, and then the module explores more detailed information for vehicle re-ID. Furthermore, a multilayer feature fusion module is proposed to enhance the feature representation of a vehicle. To address the problem of extreme orientation variation in vehicle re-ID, we propose an orientation-guided re-ranking strategy to re-rank the retrieval result based on the orientation information and query expansion algorithm. This strategy can optimize the ranking of vehicles whose orientation is not similar to the query images in the post-processing stage. Extensive experiments on three public datasets confirm the effectiveness of our method.}
}
@article{YUAN2023109298,
title = {A multi-strategy contrastive learning framework for weakly supervised semantic segmentation},
journal = {Pattern Recognition},
volume = {137},
pages = {109298},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109298},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007774},
author = {Kunhao Yuan and Gerald Schaefer and Yu-Kun Lai and Yifan Wang and Xiyao Liu and Lin Guan and Hui Fang},
keywords = {Weakly supervised learning, Representation learning, Contrastive learning, Semantic segmentation},
abstract = {Weakly supervised semantic segmentation (WSSS) has gained significant popularity as it relies only on weak labels such as image level annotations rather than the pixel level annotations required by supervised semantic segmentation (SSS) methods. Despite drastically reduced annotation costs, typical feature representations learned from WSSS are only representative of some salient parts of objects and less reliable compared to SSS due to the weak guidance during training. In this paper, we propose a novel Multi-Strategy Contrastive Learning (MuSCLe) framework to obtain enhanced feature representations and improve WSSS performance by exploiting similarity and dissimilarity of contrastive sample pairs at image, region, pixel and object boundary levels. Extensive experiments demonstrate the effectiveness of our method and show that MuSCLe outperforms current state-of-the-art methods on the widely used PASCAL VOC 2012 dataset.}
}
@article{WANG2023109273,
title = {An adaptive mutual K-nearest neighbors clustering algorithm based on maximizing mutual information},
journal = {Pattern Recognition},
volume = {137},
pages = {109273},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109273},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200752X},
author = {Yizhang Wang and Wei Pang and Zhixiang Jiao},
keywords = {Mutual K-nearest neighbors, Adaptive clustering, Maximizing mutual information},
abstract = {Clustering based on Mutual K-nearest Neighbors (CMNN) is a classical method of grouping data into different clusters. However, it has two well-known limitations: (1) the clustering results are very much dependent on the parameter k; (2) CMNN assumes that noise points correspond to clusters of small sizes according to the Mutual K-nearest Neighbors (MKNN) criterion, but some data points in small size clusters are wrongly identified as noises. To address these two issues, we propose an adaptive improved CMNN algorithm (AVCMNN), which consists of two parts: (1) improved CMNN algorithm (abbreviated as VCMNN) and (2) adaptive VCMNN algorithm (abbreviated as AVCMNN). Specifically, the first part is VCMNN algorithm, we first reassign the data points in some small-size clusters by a novel voting strategy because some of them are wrongly identified as noise points, and the clustering results are improved. Then, the second part is AVCMNN, we use maximizing mutual information to construct an objective function to optimize the parameters of the proposed method and finally obtain the better parameters values and clustering results. We conduct extensive experiments on twenty datasets, including six synthetic datasets, ten UCI datasets, and four image datasets. The experimental results show that VCMNN and AVCMNN outperforms three classical algorithms (i.e., CMNN, DPC, and DBSCAN) and six state-of-the-art (SOTA) clustering algorithms in most cases.}
}
@article{KIM2023109286,
title = {Generating Transferable Adversarial Examples for Speech Classification},
journal = {Pattern Recognition},
volume = {137},
pages = {109286},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109286},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007658},
author = {Hoki Kim and Jinseong Park and Jaewook Lee},
keywords = {Speech classification, Adversarial attack, Transferability},
abstract = {Despite the success of deep neural networks, the existence of adversarial attacks has revealed the vulnerability of neural networks in terms of security. Adversarial attacks add subtle noise to the original example, resulting in a false prediction. Although adversarial attacks have been mainly studied in the image domain, a recent line of research has discovered that speech classification systems are also exposed to adversarial attacks. By adding inaudible noise, an adversary can deceive speech classification systems and cause fatal issues in various applications, such as speaker identification and command recognition tasks. However, research on the transferability of audio adversarial examples is still limited. Thus, in this study, we first investigate the transferability of audio adversarial examples with different structures and conditions. Through extensive experiments, we discover that the transferability of audio adversarial examples is related to their noise sensitivity. Based on the analyses, we present a new adversarial attack called noise injected attack that generates highly transferable audio adversarial examples by injecting additive noise during the gradient ascent process. Our experimental results demonstrate that the proposed method outperforms other adversarial attacks in terms of transferability.}
}
@article{WANG2023109360,
title = {Versatile recurrent neural network for wide types of video restoration},
journal = {Pattern Recognition},
volume = {138},
pages = {109360},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109360},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000614},
author = {Yadong Wang and Xiangzhi Bai},
keywords = {RNN, Video restoration, Versatile, Efficient},
abstract = {Video shooting of natural scenes often suffers from various serious degradation, such as motion blur, impact of atmospheric turbulence, random noise and resolution reduction, etc. Different from the maturity of image restoration research, video restoration is much more complicated so that it lacks effective general method. Here, we present a versatile recurrent neural network (VRNN) to handle wide types of video degradation and generate stable videos with ideal clarity. We complete the design of VRNN through deducing a general video restoration paradigm that reveals the importance of simultaneously utilizing past and future information for restoring current frame. Specifically, we propose a novel RNN cell in which hidden state flows in bidirections, enriching temporal information contained in the extracted features. Furthermore, a feature fusion module involves temporal and spatial attention processing is designed to refine features of neighbouring frames and help reconstruct current frame. Extensive experiments on well-known public datasets (including four different kinds of video restoration tasks, with a total of 35,666 videos and 515,774 frames) show that the proposed VRNN achieves 1–4 dB of PSNR increasing or several times less of computational complexity in all tasks against state-of-the-art methods, manifesting the versatile and efficient ability of proposed VRNN in wide types of video restoration.}
}
@article{LIU2023109368,
title = {Expression snippet transformer for robust video-based facial expression recognition},
journal = {Pattern Recognition},
volume = {138},
pages = {109368},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109368},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000699},
author = {Yuanyuan Liu and Wenbin Wang and Chuanxu Feng and Haoyu Zhang and Zhe Chen and Yibing Zhan},
keywords = {Video-based facial expression recognition, Snippet-based transformer, Attention-augmented snippet feature extractor, Shuffled snippet order prediction},
abstract = {Although Transformer can be powerful for modeling visual relations and describing complicated patterns, it could still perform unsatisfactorily for video-based facial expression recognition, since the expression movements in a video can be too small to reflect meaningful spatial-temporal relations. To this end, we propose to decompose the modeling of expression movements of a video into the modeling of a series of expression snippets, each of which contains a few frames, and then boost the Transformer’s ability for intra-snippet and inter-snippet visual modeling, respectively, obtaining the Expression snippet Transformer (EST). For intra-snippet modeling, we devise an attention-augmented snippet feature extractor to enhance the encoding of subtle facial movements of each snippet. For inter-snippet modeling, we introduce a shuffled snippet order prediction head and a corresponding loss to improve the modeling of subtle motion changes across subsequent snippets. The EST obtains state-of-the-art performance, demonstrating its superiority to other CNN-based methods. Our code and the trained model are available at https://github.com/DreamMr/EST}
}
@article{FRANCO2023109372,
title = {Under the hood of transformer networks for trajectory forecasting},
journal = {Pattern Recognition},
volume = {138},
pages = {109372},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109372},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000730},
author = {Luca Franco and Leonardo Placidi and Francesco Giuliari and Irtiza Hasan and Marco Cristani and Fabio Galasso},
keywords = {Trajectory forecasting, Human behavior, Transformer networks, BERT, Multi-modal future prediction},
abstract = {Transformer Networks have established themselves as the de-facto state-of-the-art for trajectory forecasting but there is currently no systematic study on their capability to model the motion patterns of people, without interactions with other individuals nor the social context. There is abundant literature on LSTMs, CNNs and GANs on this subject. However methods adopting Transformer techniques achieve great performances by complex models and a clear analysis of their adoption as plain sequence models is missing. This paper proposes the first in-depth study of Transformer Networks (TF) and the Bidirectional Transformers (BERT) for the forecasting of the individual motion of people, without bells and whistles. We conduct an exhaustive evaluation of the input/output representations, problem formulations and sequence modelling, including a novel analysis of their capability to predict multi-modal futures. Out of comparative evaluation on the ETH+UCY benchmark, both TF and BERT are top performers in predicting individual motions and remain within a narrow margin wrt more complex techniques, including both social interactions and scene contexts. Source code will be released for all conducted experiments.}
}
@article{TANG2023109320,
title = {Task-balanced distillation for object detection},
journal = {Pattern Recognition},
volume = {137},
pages = {109320},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109320},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000213},
author = {Ruining Tang and Zhenyu Liu and Yangguang Li and Yiguo Song and Hui Liu and Qide Wang and Jing Shao and Guifang Duan and Jianrong Tan},
keywords = {Object detection, Knowledge distillation, Computer vision},
abstract = {Mainstream object detectors are commonly constituted of two sub-tasks, including classification and regression tasks, implemented by two parallel heads. This classic design paradigm inevitably leads to inconsistent spatial distributions between classification score and localization quality (IOU). Therefore, this paper alleviates this misalignment in the view of knowledge distillation. First, we observe that the massive teacher achieves a higher proportion of harmonious predictions than the lightweight student. Based on this intriguing observation, a novel Harmony Score (HS) is devised to estimate the alignment of classification and regression qualities. HS models the relationship between two sub-tasks and is seen as prior knowledge to promote harmonious predictions for the student. Second, this spatial misalignment will result in inharmonious region selection when distilling features. To alleviate this problem, a novel Task-decoupled Feature Distillation (TFD) is proposed by flexibly balancing the contributions of classification and regression tasks. Eventually, HD and TFD constitute the proposed method, named Task-Balanced Distillation (TBD). Extensive experiments demonstrate the considerable potential and generalization of the proposed method. Notably, when equipped with TBD, the performances of RetinaNet-R18/RetinaNet-R50/Faster-RCNN-R18 can be boosted from 33.2/37.4/34.5 to 37.3/41.2/37.7, outperforming the recent KD-based methods like FRS, FGD, and MGD.}
}
@article{KHAN2023109344,
title = {A High Dynamic Range Imaging Method for Short Exposure Multiview Images},
journal = {Pattern Recognition},
volume = {137},
pages = {109344},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109344},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000456},
author = {Rizwan Khan and You Yang and Kejun Wu and Atif Mehmood and Zahid Hussain Qaisar and Zhonglong Zheng},
keywords = {Multiview camera, Image enhancement, High dynamic range},
abstract = {The restoration and enhancement of multiview low dynamic range (MVLDR) images captured in low lighting conditions is a great challenge. The disparity maps are hardly reliable in practical, real-world scenarios and suffers from holes and artifacts due to large baseline and angle deviation among multiple cameras in low lighting conditions. Furthermore, multiple images with some additional information (e.g., ISO/exposure time, etc.) are required for the radiance map and poses the additional challenges of deghosting to encounter motion artifacts. In this paper, we proposed a method to reconstruct multiview high dynamic range (MVHDR) images from MVLDR images without relying on disparity maps. We detect and accurately match the feature points among the involved input views and gather the brightness information from the neighboring viewpoints to optimize an image restoration function based on input exposure gain to finally generate MVHDR images. Our method is very reliable and suitable for a wide baseline among sparse cameras. The proposed method requires only one image per viewpoint without any additional information and outperforms others.}
}
@article{FANG2023109340,
title = {Robust image clustering via context-aware contrastive graph learning},
journal = {Pattern Recognition},
volume = {138},
pages = {109340},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109340},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000419},
author = {Uno Fang and Jianxin Li and Xuequan Lu and Ajmal Mian and Zhaoquan Gu},
keywords = {Supervised clustering, Graph convolution network, Contrastive graph learning, Graph view generation},
abstract = {Graph convolution networks (GCN) have recently become popular for image clustering. However, existing GCN-based image clustering techniques focus on learning image neighbourhoods which leads to poor reasoning on the cluster boundaries. To address this challenge, we propose a supervised image clustering approach based on contrastive graph learning (CGL). Our method generates an influential graph view (IGV) and a topological graph view (TGV) for each class to represent its global context from different viewpoints. These generated graph views are used to reason the inter-cluster relationships and intra-cluster boundaries from the local context of each node in a contrastive manner. Our method considers each class as a fully connected graph to explore its characteristics and strategically generate directional graph views. This enhances the transferability of the proposed approach to handle data with a similar structure. We conduct extensive experiments on open datasets such as LFW, CASIA-WebFace, and CIFAR-10 and show that our method outperforms state-of-the-art including deep GRAph Contrastive rEpresentation learning (GRACE), GraphCL, and Graph Contrastive Clustering (GCC).}
}
@article{LI2023109297,
title = {Learning depth via leveraging semantics: Self-supervised monocular depth estimation with both implicit and explicit semantic guidance},
journal = {Pattern Recognition},
volume = {137},
pages = {109297},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109297},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007762},
author = {Rui Li and Danna Xue and Shaolin Su and Xiantuo He and Qing Mao and Yu Zhu and Jinqiu Sun and Yanning Zhang},
keywords = {Semantic-guided self-supervised depth estimation, Semantic-aware spatial feature modulation, Semantic-guided ranking loss, Robust point pair sampling, Uncertainty weighting},
abstract = {Self-supervised monocular depth estimation has shown great success in learning depth using only images for supervision. In this paper, we propose to enhance self-supervised depth estimation with semantics and propose a novel learning scheme, which incorporates both implicit and explicit semantic guidances. Specifically, we propose to relate depth distributions to the semantic category information by proposing a Semantic-aware Spatial Feature Modulation (SSFM) scheme, which implicitly modulates the semantic and depth features in a joint learning framework. The modulation parameters are generated from semantic labels to acquire category-level guidance. Meanwhile, a semantic-guided ranking loss is proposed to explicitly constrain the estimated depth borders using the corresponding segmentation labels. To avoid the impact brought by erroneous segmentation labels, both robust sampling strategy and prediction uncertainty weighting are proposed for the ranking loss. Extensive experimental results show that our method produces high-quality depth maps with semantically consistent depth distributions and accurate depth edges, outperforming the state-of-the-art methods by significant margins.}
}
@article{MAHESHWARI2023109341,
title = {DCSNE: Density-based Clustering using Graph Shared Neighbors and Entropy},
journal = {Pattern Recognition},
volume = {137},
pages = {109341},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109341},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000420},
author = {Rashmi Maheshwari and Sraban Kumar Mohanty and Amaresh Chandra Mishra},
keywords = {Density-based clustering, Neighborhood information, Randomness, Shared neighbor, Disorderliness, Entropy},
abstract = {Density-based clustering techniques identify arbitrary shaped clusters in the presence of outliers by capturing the intrinsic distribution of data and separating high and low-density regions based on the neighborhood information. They use global parameters to compute the density distribution of data points, the optimization of which is quite a challenging and time intensive task. The similarity graphs constructed from the data points can easily capture the topology of the density regions without using any user-defined parameters. Moreover, the concept of entropy can be useful to capture the randomness and disorderliness present in highly complex data distributions. Our proposed algorithm makes use of entropy in conjunction with the graph local neighborhood information to compute density distribution of data points. Then, actual clusters are identified using the density distribution and the shared neighbor information between the regions. The experimental results show that the proposed technique outperforms other comparable methods in terms of cluster quality in the presence of noise on diversified gene expression and real datasets.}
}
@article{LEE2023109365,
title = {AFI-GAN: Improving feature interpolation of feature pyramid networks via adversarial training for object detection},
journal = {Pattern Recognition},
volume = {138},
pages = {109365},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109365},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000663},
author = {Seong-Ho Lee and Seung-Hwan Bae},
keywords = {Multi-scale feature representation, Object detection, Adversarial training, Feature up-sampling},
abstract = {Recent convolutional detectors learn strong semantic features by generating and combining multi-scale features via feature interpolation. However, simple interpolation incurs often noisy and blurred features. To resolve this, we propose a novel adversarially-trained interpolator which can substitute for the traditional interpolation effortlessly. In specific, we design AFI-GAN consisting of an AF interpolator and a feature patch discriminator. In addition, we present a progressive adversarial learning and AFI-GAN losses to generate multi-scale features for downstream detection tasks. However, we can also finetune the proposed AFI-GAN with the recent multi-scale detectors without the adversarial learning once a pre-trained AF interpolator is provided. We prove the effectiveness and flexibility of our AF interpolator, and achieve the better box and mask APs by 2.2% and 1.6% on average compared to using other interpolation. Moreover, we achieve an impressive detection score of 57.3% mAP on the MSCOCO dataset. Code is available at https://github.com/inhavl-shlee/AFI-GAN.}
}
@article{ZHANG2023109313,
title = {Weakly-supervised butterfly detection based on saliency map},
journal = {Pattern Recognition},
volume = {138},
pages = {109313},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109313},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000146},
author = {Ting Zhang and Muhammad Waqas and Yu Fang and Zhaoying Liu and Zahid Halim and Yujian Li and Sheng Chen},
keywords = {Butterfly detection, Saliency map, Class activation map, Weakly-supervised object detection},
abstract = {Given the actual needs for detecting multiple features of butterflies in natural ecosystems, this paper proposes a model of weakly-supervised butterfly detection based on a saliency map (WBD-SM) to enhance the accuracy of butterfly detection in the ecological environment as well as to overcome the difficulty of fine annotation. Our proposed model first extracts the features of different scales using the VGG16 without the fully connected layers as the backbone network. Next, the saliency maps of butterfly images are extracted using the deep supervision network with shortcut connections (DSS) used for the butterfly target location. The class activation maps of butterfly images are derived via the adversarial complementary learning (ACoL) network for butterfly target recognition. Then, the saliency and class activation maps are post-processed with conditional random fields, thereby obtaining the refined saliency maps of butterfly objects. Finally, the locations of the butterflies are acquired based on the saliency maps. Experimental results on the 20 categories of butterfly dataset collected in this paper indicate that the WBD-SM achieves a higher recognition accuracy than that of the VGG16 under different division ratios. At the same time, when the training set and test set are 8:2, our WBD-SM attains a 95.67% localization accuracy, which is 9.37% and 11.87% higher than the results of the DSS and ACoL, respectively. Compared with three state-of-the-art fully-supervised object detection networks, RefineDet, YOLOv3 and single-shot detection (SSD), the detection performance of our WBD-SM is better than RefineDet, and YOLOv3, and is almost the same as SSD.}
}
@article{DONG2023109256,
title = {Control Distance IoU and Control Distance IoU Loss for Better Bounding Box Regression},
journal = {Pattern Recognition},
volume = {137},
pages = {109256},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109256},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200735X},
author = {Chen Dong and Miao Duoqian},
keywords = {Computer vision, Object detection, IoU, Loss function},
abstract = {Numerous improvements in feedback mechanisms have contributed to the great progress in object detection. In this paper, we first present an evaluation-feedback module, which consists of an evaluation system and feedback mechanism. Then we analyze and summarize traditional evaluation-feedback modules. We focus on both the evaluation system and the feedback mechanism, and propose Control Distance IoU and Control Distance IoU loss function (CDIoU and CDIoU loss) without increasing parameters in models, which make significant enhancements on several classical and emerging models. Finally, we propose Automatic Ground Truth Clustering (AGTC) and Floating Learning Rate Decay (FLRD) for faster regression in object detection. Experiments show that a coordinated evaluation-feedback module can effectively improve model performance. Both CNN and transformer-based detectors with CDIoU + CDIoU loss, AGTC, and FLRD achieve excellent performances. There are a maximum AP improvement of 2.9%, an average AP of 1.1% improvement on MS COCO, a maximum AP improvement of 8.2%, and an average AP improvement of 3.7% on Visdrone dataset.}
}
@article{SUN2023109285,
title = {Wse-MF: A weighting-based student exercise matrix factorization model},
journal = {Pattern Recognition},
volume = {138},
pages = {109285},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109285},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007646},
author = {Xia Sun and Bo Li and Richard Sutcliffe and Zhizezhang Gao and Wenying Kang and Jun Feng},
keywords = {Educational data mining, Personalized exercise prediction, Matrix factorization},
abstract = {Students who have been taught new ideas need to develop their skills by carrying out further work in their own time. This often consists of a series of exercises which must be completed. While students can choose exercises themselves from online sources, they will learn more quickly and easily if the exercises are specifically tailored to their needs. A good teacher will always aim to do this, but with the large groups of students who typically take advantage of open online courses, it may not be possible. Exercise prediction, working with large-scale matrix data, is a better way to address this challenge, and a key stage within such prediction is to calculate the probability that a student will answer a given question correctly. Therefore, this paper presents a novel approach called Weighting-based Student Exercise Matrix Factorization (Wse-MF) which combines student learning ability and exercise difficulty as prior weights. In order to learn how to complete the matrix, we apply an iterative optimization method that makes the approach practical for large-scale educational deployment. Compared with eight models in cognitive diagnosis and matrix factorization, our research results suggest that Wse-MF significantly outperforms the state-of-the-art on a range of real-world datasets in both prediction quality and time complexity. Moreover, we find that there is an optimal value of the latent factor K (the inner dimension of the factorization) for each dataset, which is related to the relationship between skills and exercises in that dataset. Similarly, the optimal value of hyperparameter c0 is linked to the ratio between exercises and students. Taken as a whole, we demonstrate improvements to matrix factorization within the context of educational data.}
}
@article{MOUSAVI2023109353,
title = {A generalized multi-aspect distance metric for mixed-type data clustering},
journal = {Pattern Recognition},
volume = {138},
pages = {109353},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109353},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000547},
author = {Elahe Mousavi and Mohammadreza Sehhati},
keywords = {Clustering, Mixed data, Ordinal and nominal attribute, Inter-dependency, Intra-attribute information, Mutual information},
abstract = {Distance calculation is straightforward when working with pure categorical or pure numerical data sets. Defining a unified distance to improve the clustering performance for a mixed data set composed of nominal, ordinal, and numerical attributes is very challenging due to the attributes’ different natures. In this study, we proposed a new measure of distance for a mixed-type data set that regards inter-attribute information and intra-attribute information depending on the type of attributes. In this regard, entropy and Jensen–Shannon divergence concepts were used to exploit the inter-attribute information of categorical-categorical and categorical-numerical attributes, respectively. Also, a modified version of Mahalanobis distance was proposed to consider the intra- and inter-attribute information of numerical attributes. We also introduced a unified framework based on mutual information to control attributes’ contribution to distance measurement. The proposed distance in conjunction with spectral clustering was extensively evaluated concerning various categorical, numerical, and mixed-type benchmark data sets, and the results demonstrated the efficacy of the proposed method.}
}
@article{ZENG2023109337,
title = {Beyond OCR + VQA: Towards end-to-end reading and reasoning for robust and accurate textvqa},
journal = {Pattern Recognition},
volume = {138},
pages = {109337},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109337},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000389},
author = {Gangyan Zeng and Yuan Zhang and Yu Zhou and Xiaomeng Yang and Ning Jiang and Guoqing Zhao and Weiping Wang and Xu-Cheng Yin},
keywords = {Textvqa, End-to-end, Scene text reading, Scene text reasoning},
abstract = {Text-based visual question answering (TextVQA), which answers a visual question by considering both visual contents and scene texts, has attracted increasing attention recently. Most existing methods employ an optical character recognition (OCR) module as a pre-processor to read texts, then combine it with a visual question answering (VQA) framework. However, inaccurate OCR results may lead to cumulative error propagation, and the correlation between text reading and text-based reasoning is not fully exploited. In this work, we integrate OCR into the flow of TextVQA, targeting the mutual reinforcement of OCR and VQA tasks. Specifically, a visually enhanced text embedding module is proposed to predict semantic features from the visual information of texts, by which texts can be reasonably understood even without accurate recognition. Further, two elaborate schemes are developed to leverage contextual information in VQA to modify OCR results. The first scheme is a reading modification module that adaptively selects the answer results according to the contexts. Second, we propose an efficient end-to-end text reading and reasoning network, where the downstream VQA signal contributes to the optimization of text reading. Extensive experiments show that our method outperforms existing alternatives in terms of accuracy and robustness, whether ground truth OCR annotations are used or not.}
}
@article{CHEN2023109369,
title = {Unsupervised person re-identification via multi-domain joint learning},
journal = {Pattern Recognition},
volume = {138},
pages = {109369},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109369},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000705},
author = {Feng Chen and Nian Wang and Jun Tang and Pu Yan and Jun Yu},
keywords = {Person re-identification, Data augmentation, Domain adaptation, Unsupervised learning},
abstract = {Deep learning techniques have achieved impressive progress in the task of person re-identification. However, how to generalize a learned model from the source domain to the target domain remains a long-standing challenge. Inspired by the fact that the enrichment of data diversity and the utilization of miscellaneous semantic features can lead to better generalization ability, we design a model that integrates a novel data augmentation method with a multi-label assignment strategy to achieve semantic features decoupling in the source domain. The pre-trained model is employed to extract several kinds of semantic features from the target dataset, and each kind of semantic features is regarded as a specific domain. We then cluster features of each domain and exploit the connection between different clustering results to perform self-distillation for generating more reliable pseudo labels. Finally, the obtained pseudo labels are used to fine-tune the pre-trained model to achieve model transfer from the source domain to the target one. Extensive experiments demonstrate that our approach outperforms some state-of-the-art methods by a clear margin and even surpass some supervised methods. Source code is available at: https://www.github.com/flychen321/MDJL.}
}
@article{XU2023109338,
title = {Computation-Efficient Knowledge Distillation via Uncertainty-Aware Mixup},
journal = {Pattern Recognition},
volume = {138},
pages = {109338},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109338},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000390},
author = {Guodong Xu and Ziwei Liu and Chen Change Loy},
keywords = {Knowledge distillation, Training cost},
abstract = {Knowledge distillation (KD) has emerged as an essential technique not only for model compression, but also other learning tasks such as continual learning. Given the richer application spectrum and potential online usage of KD, knowledge distillation efficiency becomes a pivotal component. In this work, we study this little-explored but important topic. Unlike previous works that focus solely on the accuracy of student network, we attempt to achieve a harder goal – to obtain a performance comparable to conventional KD with a lower computation cost during the transfer. To this end, we present UNcertainty-aware mIXup (UNIX), an effective approach that can reduce transfer cost by 20% to 30% and yet maintain comparable or achieve even better student performance than conventional KD. This is made possible via effective uncertainty sampling and a novel adaptive mixup approach that select informative samples dynamically over ample data and compact knowledge in these samples. We show that our approach inherently performs hard sample mining. We demonstrate the applicability of our approach to improve various existing KD approaches by reducing their queries to a teacher network. Extensive experiments are performed on CIFAR100 and ImageNet. Code and model are available at https://github.com/xuguodong03/UNIXKD.}
}
@article{MA2023109280,
title = {CrossRectify: Leveraging disagreement for semi-supervised object detection},
journal = {Pattern Recognition},
volume = {137},
pages = {109280},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109280},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007592},
author = {Chengcheng Ma and Xingjia Pan and Qixiang Ye and Fan Tang and Weiming Dong and Changsheng Xu},
keywords = {Object detection, Semi-supervised learning, 2D Semi-supervised object detection, 3D Semi-supervised object detection, Self-labeling},
abstract = {Semi-supervised object detection has recently achieved substantial progress. As a mainstream solution, the self-labeling-based methods train the detector on both labeled data and unlabeled data with pseudo labels predicted by the detector itself, but their performances are always limited. Through experimental analysis, we reveal the underlying reason is that the detector is misguided by the incorrect pseudo labels predicted by itself (dubbed self-errors). These self-errors can hurt performance even worse than random-errors, and can be neither discerned nor rectified during the self-labeling process. In this paper, we propose an effective detection framework named CrossRectify, to obtain accurate pseudo labels by simultaneously training two detectors with different initial parameters. Specifically, the proposed approach leverages the disagreements between detectors to discern the self-errors and refines the pseudo label quality by the proposed cross-rectifying mechanism. Extensive experiments show that CrossRectify achieves outperforming performances over various detector structures on 2D and 3D detection benchmarks.}
}
@article{SHIFATERABBI2023109268,
title = {Invariance encoding in sliced-Wasserstein space for image classification with limited training data},
journal = {Pattern Recognition},
volume = {137},
pages = {109268},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109268},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007476},
author = {Mohammad Shifat-E-Rabbi and Yan Zhuang and Shiying Li and Abu Hasnat Mohammad Rubaiyat and Xuwang Yin and Gustavo K. Rohde},
keywords = {R-CDT, Mathematical model, Generative model, Invariance learning},
abstract = {Deep convolutional neural networks (CNNs) are broadly considered to be state-of-the-art generic end-to-end image classification systems. However, they are known to underperform when training data are limited and thus require data augmentation strategies that render the method computationally expensive and not always effective. Rather than using a data augmentation strategy to encode invariances as typically done in machine learning, here we propose to mathematically augment a nearest subspace classification model in sliced-Wasserstein space by exploiting certain mathematical properties of the Radon Cumulative Distribution Transform (R-CDT), a recently introduced image transform. We demonstrate that for a particular type of learning problem, our mathematical solution has advantages over data augmentation with deep CNNs in terms of classification accuracy and computational complexity, and is particularly effective under a limited training data setting. The method is simple, effective, computationally efficient, non-iterative, and requires no parameters to be tuned. Python code implementing our method is available at https://github.com/rohdelab/mathematical_augmentation. Our method is integrated as a part of the software package PyTransKit, which is available at https://github.com/rohdelab/PyTransKit.}
}
@article{WANG2023109296,
title = {Toward a blind image quality evaluator in the wild by learning beyond human opinion scores},
journal = {Pattern Recognition},
volume = {137},
pages = {109296},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109296},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007750},
author = {Zhihua Wang and Zhi-Ri Tang and Jianguo Zhang and Yuming Fang},
keywords = {Blind image quality assessment, Opinion-free, Pseudo binary label, Unsupervised domain adaptation, gMAD competition},
abstract = {Nowadays, most existing blind image quality assessment (BIQA) models inthewild heavily rely on human ratings, which are extraordinarily labor-expensive to collect. Here, we propose an opinion−free BIQA method that learns from multiple annotators to assess the perceptual quality of images captured in the wild. Specifically, we first synthesize distorted images based on the pristine counterparts. We then randomly assemble a set of image pairs from the synthetic images, and use a group of IQA models to assign pseudo-binary labels for each pair indicating which image has higher quality as the supervisory signal. Based on the newly established pseudo-labeled dataset, we train a deep neural network (DNN)-based BIQA model to rank the perceptual quality, optimized for consistency with the binary rank labels. Since there exists domain shift, e.g., distortion shift and content shift, between the synthetic and in-the-wild images, we leverage two ways to alleviate this issue. First, the simulated distortions should be similar to authentic distortions as much as possible. Second, an unsupervised domain adaptation (UDA) module is further applied to encourage learning domain-invariant features between two domains. Extensive experiments demonstrate the effectiveness of our proposed opinion−free BIQA model, yielding SOTA performance in terms of correlation with human opinion scores, as well as gMAD competition. Our code is available at: https://github.com/wangzhihua520/OF_BIQA.}
}
@article{LIU2023109284,
title = {The Performance Index of Convolutional Neural Network-Based Classifiers in Class Imbalance Problem},
journal = {Pattern Recognition},
volume = {137},
pages = {109284},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109284},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007634},
author = {Yanchen Liu and King Wai Chiu Lai},
keywords = {Deep Learning, Convolutional Neural Network, Class Imbalance, Class Balance Index, Model Performance Index},
abstract = {Class imbalance is a common problem in many classification domains. This paper provides an evaluation index and one algorithm for this problem based on binary classification. The Model Performance Index (MPI) is proposed for assessing classifier performance as a new evaluation metric, considering class imbalance impacts. Based on MPI, we investigate algorithms to estimate ideal classifier performance with a fair distribution (1:1), referred to as the Ideal Model Performance Algorithm. Experimentally, compared with traditional metrics, MPI is more sensitive. Specifically, it can detect all types of changes in classifier performances, while others might remain at the same levels. Moreover, for the estimation of classifier performances, the algorithm reaches small differences between predictions and the values observed. Generally, for ideal performances, it achieved error rates of 0.060% - 1.3% for rare class in four experiments, showing a practical value on estimation and representation on the classifier performances.}
}
@article{LIN2023109342,
title = {Multiview Jointly Sparse Discriminant Common Subspace Learning},
journal = {Pattern Recognition},
volume = {138},
pages = {109342},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109342},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000432},
author = {Yiling Lin and Zhihui Lai and Jie Zhou and Jiajun Wen and Heng Kong},
keywords = {Feature extraction, Small-class problem, Multiview classification, Discriminant common-space learning},
abstract = {Multiview data leads to the demand for classifying samples from various views, and the large gap between different views makes the classification task challenging. Recently, researchers have extended linear discriminant analysis (LDA) to multi-view scenarios. However, the extended methods are generally associated with the small-class problem, that is, the projection size is limited by the number of classes. In addition, they are sensitive to variations in images or outliers. To solve these problems, this study proposes a generalized robust multiview discriminant analysis (GRMDA) to obtain a linear transform for each view and for learning multiview jointly sparse discriminant common subspace. GRMDA aims to achieve both maximal between-class and minimal within-class variation for data from multiple views in a common space. Instead of formulating the ratio trace problem, we reformulate GRMDA inspired by maximum margin criterion (MMC) to address the small-class problem. Moreover, the proposed method achieves stronger robustness by reconstructing the within-class and between-class scatter terms from the definition of L2,1 norm. Furthermore, GRMDA ensures joint sparsity using the L2,1 norm-based regularization term. Additionally, we present an iterative algorithm, convergence proof, and complexity analysis. Experiments on six popular databases, that is, COIL100, USPS/MNIST, Extended Yale Face B, AR, BBCSport, and multiple feature datasets, were conducted to evaluate the performance of GRMDA against the state-of-the-art multiview methods. The experimental results demonstrate that the proposed method can achieve a significant performance with strong robustness and fast convergence.}
}
@article{CAO2023109302,
title = {Poincaré Fréchet mean},
journal = {Pattern Recognition},
volume = {137},
pages = {109302},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109302},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000031},
author = {Xiaofeng Cao},
keywords = {Fréchet mean, Hyperbolic geometry, Poincaré model, Minimizing upper bound, ()-approximation},
abstract = {Generalizing the Fréchet mean from the Euclidean metric is not able to properly capture the geometric characteristics of many non-trivial operations, such as the non-dot inner product and non-Euclidean gradients defined on the manifold. One effective solution is to derive its hyperbolic representations in the Poincaré or hyperboloid model. Our goodness-of-fit testing shows that the Poincaré Fréchet mean achieves much lower χ2 power than that of the hyperboloid and typical non-linear kernels with regard to parameter perturbations. However, recent advanced optimization solvers, such as Riemannian gradient descent and minimizing upper bound, may result in imprecise convergences. This paper presents an (1−ϵ)-approximation approach to search a core-set on the Poincaré model, reducing deviations of the Poincaré Fréchet mean to its optimum. A hierarchical splitting algorithm that implicitly explores the hyperbolic representations for an arbitrary manifold is then presented. Experiments show that the (1−ϵ) Poincaré Fréchet mean adopted in hierarchical splitting, achieves better representations than Euclidean, kernel, and Lorentzian Fréchet means in graph and image data.}
}
@article{ZHANG2023109354,
title = {Self-structured pyramid network with parallel spatial-channel attention for change detection in VHR remote sensed imagery},
journal = {Pattern Recognition},
volume = {138},
pages = {109354},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109354},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000559},
author = {Mingyang Zhang and Hanhong Zheng and Maoguo Gong and Yue Wu and Hao Li and Xiangming Jiang},
keywords = {Change detection, VHR remote sensing images, Feature pyramids, Attention mechanisms, Deep learning},
abstract = {Land cover change detection (CD) in very-high-resolution (VHR) images is still impeded by weak pattern separability and land cover complexity. To address these challenges, a self-structured pyramid network (S2PNet) with a parallel spatial-channel attention mechanism (PSAM) and a self-structured feature pyramid (SFP) is proposed for a finer annotation of changed land cover. The proposed PSAM refines the features of different levels in dual-branch coordinated by running parallel without mutual influence for a better recognition of varied objects, which can lead to less incorrectly detected land cover. And the SFP integrates the embedded multi-scale features to acquire an improved cognition over multi-scale objects, which can contribute to a more complete annotation over diverse objects. All-round experiments over several widely used open large-scale VHR CD data sets are carried out, which indicate the efficiency and effectiveness of the proposed method. Related comparisons suggest that the proposed method can achieve higher performance over several existing state-of-the-art CD methods. The source codes will be released at https://github.com/HaiXing-1998/S2PNet-CD.}
}
@article{GONG2023109272,
title = {Improving visual-semantic embeddings by learning semantically-enhanced hard negatives for cross-modal information retrieval},
journal = {Pattern Recognition},
volume = {137},
pages = {109272},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109272},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007518},
author = {Yan Gong and Georgina Cosma},
keywords = {Visual semantic embedding network, Cross-modal, Information retrieval, Hard negatives},
abstract = {Visual Semantic Embedding (VSE) networks aim to extract the semantics of images and their descriptions and embed them into the same latent space for cross-modal information retrieval. Most existing VSE networks are trained by adopting a hard negatives loss function which learns an objective margin between the similarity of relevant and irrelevant image–description embedding pairs. However, the objective margin in the hard negatives loss function is set as a fixed hyperparameter that ignores the semantic differences of the irrelevant image–description pairs. To address the challenge of measuring the optimal similarities between image–description pairs before obtaining the trained VSE networks, this paper presents a novel approach that comprises two main parts: (1) finds the underlying semantics of image descriptions; and (2) proposes a novel semantically-enhanced hard negatives loss function, where the learning objective is dynamically determined based on the optimal similarity scores between irrelevant image–description pairs. Extensive experiments were carried out by integrating the proposed methods into five state-of-the-art VSE networks that were applied to three benchmark datasets for cross-modal information retrieval tasks. The results revealed that the proposed methods achieved the best performance and can also be adopted by existing and future VSE networks.}
}
@article{LV2023109301,
title = {Semi-supervised node classification via fine-grained graph auxiliary augmentation learning},
journal = {Pattern Recognition},
volume = {137},
pages = {109301},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109301},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300002X},
author = {Jia Lv and Kaikai Song and Qiang Ye and Guangjian Tian},
keywords = {Graph neural network, Node classification, Data augmentation, Auxiliary learning},
abstract = {Node classification has become an important research topic in recent years. Since there are always a few training samples, researchers improve the performance by properly leveraging the predictions of unlabeled nodes during training. However, suffering from the model degradation resulted from the accumulative error of pseudo-labels, there is limited improvement. In this paper we present fine-grained Graph Auxiliary aUgmentation (GAU). It trains the primary task together with an automatically created auxiliary task which is a fine-grained node classification task. And an auxiliary augmentation strategy is designed to enlarge the labeled set for the auxiliary task by utilizing the pseudo-labels of the primary task. Comprehensive experiments show that GAU alleviates the sensitivity of the model to the pseudo-label quality, so more unlabeled nodes can participate in the training. From the perspective of co-training, the fine-grained auxiliary task which is trained by much more unlabeled nodes helps to learn better node representations from a different view, thereby boosting the final performance. Extensive experiments verify the superior performance of the GAU on different GNN architectures when compared with other state-of-the-art approaches.}
}
@article{CHOWDHURY2023109314,
title = {Feature weighting in DBSCAN using reverse nearest neighbours},
journal = {Pattern Recognition},
volume = {137},
pages = {109314},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109314},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000158},
author = {Stiphen Chowdhury and Na Helian and Renato {Cordeiro de Amorim}},
keywords = {Density-based clustering, Reverse nearest neighbour, DBSCAN},
abstract = {DBSCAN is arguably the most popular density-based clustering algorithm, and it is capable of recovering non-spherical clusters. One of its main weaknesses is that it treats all features equally. In this paper, we propose a density-based clustering algorithm capable of calculating feature weights representing the degree of relevance of each feature, which takes the density structure of the data into account. First, we improve DBSCAN and introduce a new algorithm called DBSCANR. DBSCANR reduces the number of parameters of DBSCAN to one. Then, a new step is introduced to the clustering process of DBSCANR to iteratively update feature weights based on the current partition of data. The feature weights produced by the weighted version of the new clustering algorithm, W-DBSCANR, measure the relevance of variables in a clustering and can be used in feature selection in data mining applications where large and complex real-world data are often involved. Experimental results on both artificial and real-world data have shown that the new algorithms outperformed various DBSCAN type algorithms in recovering clusters in data.}
}