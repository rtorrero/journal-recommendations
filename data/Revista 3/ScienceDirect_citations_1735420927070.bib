@article{SUN2022108845,
title = {Iterative structure transformation and conditional random field based method for unsupervised multimodal change detection},
journal = {Pattern Recognition},
volume = {131},
pages = {108845},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108845},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003260},
author = {Yuli Sun and Lin Lei and Dongdong Guan and Junzheng Wu and Gangyao Kuang},
keywords = {Unsupervised change detection, KNN graph, Image transformation, Multimodal, Conditional random field},
abstract = {Change detection between heterogeneous images has become an increasingly interesting research topic in remote sensing. The different appearances and statistics of heterogeneous images bring great challenges to this task. In this paper, we propose an unsupervised iterative structure transformation and conditional random field (IST-CRF) based multimodal change detection (MCD) method, combining an imaging modality-invariant based structure transformation method with a random filed framework specifically designed for MCD, to acquire an optimal change map within a global probabilistic model. IST-CRF first constructs graphs to represent the structures of the images, and transforms the heterogeneous images to the same differential domain by using graph based forward and backward structure transformations. Then, the change vectors are calculated to distinguish the changed and unchanged areas. Finally, in order to classify the change vectors and compute the binary change map, a CRF model is designed to fully explore the spectral-spatial information, which incorporates the change information, local spatially-adjacent neighbor information, and global spectrally-similar neighbor information with a random field framework. As the changed samples will influence the structure transformation and reduce the quality of change vectors, we use an iterative framework to propagate the CRF segmentation results back to the structure transformation process that removes the changed samples, and thus improve the accuracy of change detection. Experiments conducted on different real data sets show the effectiveness of IST-CRF. Source code of the proposed method will be made available at https://github.com/yulisun/IST-CRF.}
}
@article{LOPEZLOPEZ2022108885,
title = {Incremental Learning from Low-labelled Stream Data in Open-Set Video Face Recognition},
journal = {Pattern Recognition},
volume = {131},
pages = {108885},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108885},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003661},
author = {Eric Lopez-Lopez and Xose M. Pardo and Carlos V. Regueiro},
keywords = {Open-set face recognition, Incremental Learning, Self-updating, Adaptive biometrics, Video-surveillance},
abstract = {Deep Learning approaches have brought solutions, with impressive performance, to general classification problems where wealthy of annotated data are provided for training. In contrast, less progress has been made in continual learning of a set of non-stationary classes, mainly when applied to unsupervised problems with streaming data. Here, we propose a novel incremental learning approach which combines a deep features encoder with an Open-Set Dynamic Ensembles of SVM, to tackle the problem of identifying individuals of interest (IoI) from streaming face data. From a simple weak classifier trained on a few video-frames, our method can use unsupervised operational data to enhance recognition. Our approach adapts to new patterns avoiding catastrophic forgetting and partially heals itself from miss-adaptation. Besides, to better comply with real world conditions, the system was designed to operate in an open-set setting. Results show a benefit of up to 15% F1-score increase respect to non-adaptive state-of-the-art methods.}
}
@article{ZHANG2022108833,
title = {Visual-to-EEG cross-modal knowledge distillation for continuous emotion recognition},
journal = {Pattern Recognition},
volume = {130},
pages = {108833},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108833},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003144},
author = {Su Zhang and Chuangao Tang and Cuntai Guan},
keywords = {Continuous emotion recognition, Knowledge distillation, Cross-modality},
abstract = {Visual modality is one of the most dominant modalities for current continuous emotion recognition methods. Compared to which the EEG modality is relatively less sound due to its intrinsic limitation such as subject bias and low spatial resolution. This work attempts to improve the continuous prediction of the EEG modality by using the dark knowledge from the visual modality. The teacher model is built by a cascade convolutional neural network - temporal convolutional network (CNN-TCN) architecture, and the student model is built by TCNs. They are fed by video frames and EEG average band power features, respectively. Two data partitioning schemes are employed, i.e., the trial-level random shuffling (TRS) and the leave-one-subject-out (LOSO). The standalone teacher and student can produce continuous prediction superior to the baseline method, and the employment of the visual-to-EEG cross-modal KD further improves the prediction with statistical significance, i.e., p-value <0.01 for TRS and p-value <0.05 for LOSO partitioning. The saliency maps of the trained student model show that the brain areas associated with the active valence state are not located in precise brain areas. Instead, it results from synchronized activity among various brain areas. And the fast beta and gamma waves, with the frequency of 18−30Hz and 30−45Hz, contribute the most to the human emotion process compared to other bands. The code is available at https://github.com/sucv/Visual_to_EEG_Cross_Modal_KD_for_CER.}
}
@article{CHEN2022108849,
title = {Symbolic sequence representation with Markovian state optimization},
journal = {Pattern Recognition},
volume = {131},
pages = {108849},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108849},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003302},
author = {Lifei Chen and Haiyan Wu and Wenxuan Kang and Shengrui Wang},
keywords = {Sequence representation, Hidden Markov model, State clustering, Hierarchical model selection, Activity recognition},
abstract = {Sequence representation, which is aimed at embedding sequentially symbolic data in a real space, is a foundational task in sequence pattern recognition. It is a difficult problem due to the challenges entailed in learning the intrinsic structural features within sequences in small sample size cases, in an unsupervised way. In this paper, we propose to represent each symbolic sequence by its transition probability distribution over discriminating topics, formalized by a set of optimized Hidden Markov Model (HMM) states shared by all sequences. An efficient method, called Markovian state clustering with hierarchical model selection, is proposed to optimize the Markovian states and to adaptively determine the number of topics. The proposed method is experimentally evaluated on human activity recognition and protein recognition, and results obtained demonstrate its effectiveness and efficiency.}
}
@article{ZHU2022108897,
title = {Adaptive aggregation-distillation autoencoder for unsupervised anomaly detection},
journal = {Pattern Recognition},
volume = {131},
pages = {108897},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108897},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003788},
author = {Jiaqi Zhu and Fang Deng and Jiachen Zhao and Jie Chen},
keywords = {Anomaly detection, Aggregation-distillation mechanism, Autoencoders, Unsupervised learning},
abstract = {Anomaly detection (AD) has been receiving great attention as it plays a crucial role in many areas of basic research and industrial applications. However, most existing AD methods not only rely on training on normal data, but also ignore the multi-cluster nature of normal and abnormal patterns. To overcome these limitations, this paper proposes a novel method called Adaptive Aggregation-Distillation AutoEncoder (AADAE) for unsupervised anomaly detection. AADAE is built upon the density-based landmark selection in respect to representing diverse normal patterns. During training, AADAE adaptively updates the location and quantity of landmarks. Then, an aggregation-distillation mechanism is constructed: Firstly, it aggregates the latent representations of normal and anomalous to different landmark-guided regions within the convex polygon with landmarks as vertices, which minimizes the intra-class variation and promotes the separability of normal and abnormal samples. Secondly, the distillation mechanism is applied to obtain reliable detection results when there are anomalies in the training set. The aggregation process motivates AADAE to learn the distribution of multi-cluster normal samples with the help of landmarks, which in turn facilitates the distillation process to differentiate normal from anomalies for training. Extensive empirical studies on ten datasets from different application domains demonstrate the efficiency and generalization ability of the method.}
}
@article{NING2022108873,
title = {HCFNN: High-order coverage function neural network for image classification},
journal = {Pattern Recognition},
volume = {131},
pages = {108873},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108873},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003545},
author = {Xin Ning and Weijuan Tian and Zaiyang Yu and Weijun Li and Xiao Bai and Yuebao Wang},
keywords = {DNNs, Neuron modeling, Heuristic algorithm, Back propagation, Computer vision},
abstract = {Recent advances in deep neural networks (DNNs) have mainly focused on innovations in network architecture and loss function. In this paper, we introduce a flexible high-order coverage function (HCF) neuron model to replace the fully-connected (FC) layers. The approximation theorem and proof for the HCF are also presented to demonstrate its fitting ability. Unlike the FC layers, which cannot handle high-dimensional data well, the HCF utilizes weight coefficients and hyper-parameters to mine underlying geometries with arbitrary shapes in an n-dimensional space. To explore the power and potential of our HCF neuron model, a high-order coverage function neural network (HCFNN) is proposed, which incorporates the HCF neuron as the building block. Moreover, a novel adaptive optimization method for weights and hyper-parameters is designed to achieve effective network learning. Comprehensive experiments on nine datasets in several domains validate the effectiveness and generalizability of the HCF and HCFNN. The proposed method provides a new perspective for further developments in DNNs and ensures wide application in the field of image classification. The source code is available at https://github.com/Tough2011/HCFNet.git}
}
@article{ZHANG2022108821,
title = {LSRML: A latent space regularization based meta-learning framework for MR image segmentation},
journal = {Pattern Recognition},
volume = {130},
pages = {108821},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108821},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003028},
author = {Bo Zhang and Yunpeng Tan and Hui Wang and Zheng Zhang and Xiuzhuang Zhou and Jingyun Wu and Yue Mi and Haiwen Huang and Wendong Wang},
keywords = {Latent space regularization, Meta learning, Domain generalization, Domain discriminator, Multi-source domain adaptation},
abstract = {Data sources for medical image segmentation can be quite extensive, and models trained with data from a source domain may perform poorly on data from the target domain owing to domain shift issues. To overcome the impact of domain shift, we propose a novel meta-learning-based multi-source domain adaptation framework for medical image segmentation. Specifically, we designed a domain discriminator module to produce category prediction over the latent features, and an image reconstruction module to reconstruct the foreground and background of the target domain image separately. Furthermore, we constructed a large-scale multi-modal prostate dataset, which contained 495,902 magnetic resonance images of 419 cases, with prostate and lesion masks, as well as diagnostic descriptions for each patient. We evaluated our proposed method through extensive experiments using the proposed and the benchmark datasets. Experimental results show that our model achieves better segmentation and generalization performance compared to state-of-the-art approaches.}
}
@article{WU2022108884,
title = {Cross-view panorama image synthesis with progressive attention GANs},
journal = {Pattern Recognition},
volume = {131},
pages = {108884},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108884},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200365X},
author = {Songsong Wu and Hao Tang and Xiao-Yuan Jing and Jianjun Qian and Nicu Sebe and Yan Yan and Qinghua Zhang},
keywords = {Progressive attention GANs, Cross-view panorama image synthesis, Cross-stage attention, Orientation-aware data augmentation, Multi-stage image generation},
abstract = {Despite the significant progress of conditional image generation, it remains difficult to synthesize a ground-view panorama image from a top-view aerial image. Among the core challenges are the vast differences in image appearance and resolution between aerial images and panorama images, and the limited aside information available for top-to-ground viewpoint transformation. To address these challenges, we propose a new Progressive Attention Generative Adversarial Network (PAGAN) with two novel components: a multistage progressive generation framework and a cross-stage attention module. In the first stage, an aerial image is fed into a U-Net-like network to generate one local region of the panorama image and its corresponding segmentation map. Then, the synthetic panorama image region is extended and refined through the following generation stages with our proposed cross-stage attention module that passes semantic information forward stage-by-stage. In each of the successive generation stages, the synthetic panorama image and segmentation map are separately fed into an image discriminator and a segmentation discriminator to compute both later real and fake, as well as feature alignment score maps for discrimination. The model is trained with a novel orientation-aware data augmentation strategy based on the geometric relation between aerial and panorama images. Extensive experimental results on two cross-view datasets show that PAGAN generates high-quality panorama images with more convincing details than state-of-the-art methods.}
}
@article{MARCHETTI2022108913,
title = {Score-Oriented Loss (SOL) functions},
journal = {Pattern Recognition},
volume = {132},
pages = {108913},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108913},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003946},
author = {F. Marchetti and S. Guastavino and M. Piana and C. Campi},
keywords = {Supervised machine learning, Binary classification, Loss functions, Skill scores},
abstract = {Loss functions engineering and the assessment of prediction performances are two crucial and intertwined aspects of supervised machine learning. This paper focuses on binary classification to introduce a class of loss functions that are defined on probabilistic confusion matrices and that allow an automatic and a priori maximization of the skill scores. These loss functions are tested in various classification experiments, which show that the probability distribution function associated with the confusion matrices significantly impacts the outcome of the score maximization process, and that the proposed functions are competitive with other state-of-the-art probabilistic losses.}
}
@article{GAO2022108861,
title = {A modified interval type-2 Takagi-Sugeno fuzzy neural network and its convergence analysis},
journal = {Pattern Recognition},
volume = {131},
pages = {108861},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108861},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003429},
author = {Tao Gao and Xiao Bai and Chen Wang and Liang Zhang and Jin Zheng and Jian Wang},
keywords = {IT2 fuzzy model, Fuzzy neural network, Takagi-Sugeno, Conjugate gradient, Convergence},
abstract = {In this paper, to compute the firing strength values of type-2 fuzzy models, a soft version of minimum is presented, which endows the fuzzy model with the ability to solve large dimensional problems. In addition, a conjugate gradient method is borrowed to train the designed interval type-2 Takagi-Sugeno fuzzy model. Compared with the existing gradient-based learning strategy, this scheme can efficiently enhance the fuzzy model performance. Last but not least, convergence analysis for this modified interval type-2 Takagi-Sugeno fuzzy neural network (MIT2TSFNN) is conducted in detail, which proves that the gradient of the error function tends to zero with the iteration increasing (weak convergence) and the sequence of model parameters (weights) convergences to a fixed point (strong convergence). To validate the effectiveness of the proposed MIT2TSFNN and its theoretical results, simulation results of six regression and six classification problems are presented.}
}
@article{QIAN2022108889,
title = {A survey of robust adversarial training in pattern recognition: Fundamental, theory, and methodologies},
journal = {Pattern Recognition},
volume = {131},
pages = {108889},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108889},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003703},
author = {Zhuang Qian and Kaizhu Huang and Qiu-Feng Wang and Xu-Yao Zhang},
keywords = {Adversarial examples, Adversarial training, Robust learning},
abstract = {Deep neural networks have achieved remarkable success in machine learning, computer vision, and pattern recognition in the last few decades. Recent studies, however, show that neural networks (both shallow and deep) may be easily fooled by certain imperceptibly perturbed input samples called adversarial examples. Such security vulnerability has resulted in a large body of research in recent years because real-world threats could be introduced due to the vast applications of neural networks. To address the robustness issue to adversarial examples particularly in pattern recognition, robust adversarial training has become one mainstream. Various ideas, methods, and applications have boomed in the field. Yet, a deep understanding of adversarial training including characteristics, interpretations, theories, and connections among different models has remained elusive. This paper presents a comprehensive survey trying to offer a systematic and structured investigation on robust adversarial training in pattern recognition. We start with fundamentals including definition, notations, and properties of adversarial examples. We then introduce a general theoretical framework with gradient regularization for defending against adversarial samples - robust adversarial training with visualizations and interpretations on why adversarial training can lead to model robustness. Connections will also be established between adversarial training and other traditional learning theories. After that, we summarize, review, and discuss various methodologies with defense/training algorithms in a structured way. Finally, we present analysis, outlook, and remarks on adversarial training.}
}
@article{BESPALOV2022108816,
title = {BRULÈ: Barycenter-Regularized Unsupervised Landmark Extraction},
journal = {Pattern Recognition},
volume = {131},
pages = {108816},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108816},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002977},
author = {Iaroslav Bespalov and Nazar Buzun and Dmitry V. Dylov},
abstract = {Unsupervised retrieval of image features is vital for many computer vision tasks where the annotation is missing or scarce. In this work, we propose a new unsupervised approach to detect the landmarks in images, validating it on the popular task of human face key-points extraction. The method is based on the idea of auto-encoding the wanted landmarks in the latent space while discarding the non-essential information (and effectively preserving the interpretability). The interpretable latent space representation (the bottleneck containing nothing but the wanted key-points) is achieved by a new two-step regularization approach. The first regularization step evaluates transport distance from a given set of landmarks to some average value (the barycenter by Wasserstein distance). The second regularization step controls deviations from the barycenter by applying random geometric deformations synchronously to the initial image and to the encoded landmarks. We demonstrate the effectiveness of the approach both in unsupervised and semi-supervised training scenarios using 300-W, CelebA, and MAFL datasets. The proposed regularization paradigm is shown to prevent overfitting, and the detection quality is shown to improve beyond the state-of-the-art face models.}
}
@article{SHEN2022108828,
title = {Classification for high-dimension low-sample size data},
journal = {Pattern Recognition},
volume = {130},
pages = {108828},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108828},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003090},
author = {Liran Shen and Meng Joo Er and Qingbo Yin},
keywords = {Binary linear classifier, Quadratic programming, Data piling, Covariance matrix},
abstract = {High-dimension and low-sample-size (HDLSS) data sets have posed great challenges to many machine learning methods. To deal with practical HDLSS problems, development of new classification techniques is highly desired. After the cause of the over-fitting phenomenon is identified, a new classification criterion for HDLSS data sets, termed tolerance similarity, is proposed to emphasize maximization of within-class variance on the premise of class separability. Leveraging on this criterion, a novel linear binary classifier, termed No-separated Data Maximum Dispersion classifier (NPDMD), is designed. The main idea of the NPDMD is to spread samples of two classes in a large interval in the respective positive or negative space along the projecting direction when the distance between the projection means for two classes is large enough. The salient features of the proposed NPDMD are: (1) The NPDMD operates well on HDLSS data sets; (2) The NPDMD solves the objective function in the entire feature space to avoid the data-piling phenomenon. (3) The NPDMD leverages on the low-rank property of the covariance matrix for HDLSS data sets to accelerate the computation speed. (4) The NPDMD is suitable for different real-word applications. (5) The NPDMD can be implemented readily using Quadratic Programming. Not only theoretical properties of the NPDMD have been derived, but also a series of evaluations have been conducted on one simulated and six real-world benchmark data sets, including face classification and mRNA classification. Experimental results and comprehensive studies demonstrate the superiority of the NPDMD in terms of correct classification rate, mean within-group correct classification rate and the area under the ROC curve.}
}
@article{SU2022108868,
title = {DSLA: Dynamic smooth label assignment for efficient anchor-free object detection},
journal = {Pattern Recognition},
volume = {131},
pages = {108868},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108868},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003491},
author = {Hu Su and Yonghao He and Rui Jiang and Jiabin Zhang and Wei Zou and Bin Fan},
keywords = {Convolutional neural network, Object detection, Centerness score, Intersection-of-union},
abstract = {Anchor-free detectors basically formulate object detection as dense classification and regression. For popular anchor-free detectors, it is common to introduce an individual prediction branch to estimate the quality of localization. The following inconsistencies are observed when we delve into the practices of classification and quality estimation. Firstly, for some adjacent samples which are assigned completely different labels, the trained model would produce similar classification scores. This violates the training objective and leads to performance degradation. Secondly, it is found that detected bounding boxes with higher confidences contrarily have smaller overlaps with the corresponding ground-truth. Accurately localized bounding boxes would be suppressed by less accurate ones in the Non-Maximum Suppression (NMS) procedure. To address the inconsistency problems, the Dynamic Smooth Label Assignment (DSLA) method is proposed. Based on the concept of centerness originally developed in FCOS, a smooth assignment strategy is proposed. The label is smoothed to a continuous value in [0,1] to make a steady transition between positive and negative samples. Intersection-of-Union (IoU) is predicted dynamically during training and is coupled with the smoothed label. The dynamic smooth label is assigned to supervise the classification branch. Under such supervision, quality estimation branch is naturally merged into the classification branch, which simplifies the architecture of anchor-free detector. Comprehensive experiments are conducted on the MS COCO benchmark. It is demonstrated that, DSLA can significantly boost the detection accuracy by alleviating the above inconsistencies for anchor-free detectors. Our codes are released at https://github.com/YonghaoHe/DSLA.}
}
@article{WANG2022108908,
title = {COVID-19 contact tracking by group activity trajectory recovery over camera networks},
journal = {Pattern Recognition},
volume = {132},
pages = {108908},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108908},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003892},
author = {Chao Wang and XiaoChen Wang and Zhongyuan Wang and WenQian Zhu and Ruimin Hu},
keywords = {Contact tracking, COVID-19, Group activity, Trajectory recovery},
abstract = {Contact tracking plays an important role in the epidemiological investigation of COVID-19, which can effectively reduce the spread of the epidemic. As an excellent alternative method for contact tracking, mobile phone location-based methods are widely used for locating and tracking contacts. However, current inaccurate positioning algorithms that are widely used in contact tracking lead to the inaccurate follow-up of contacts. Aiming to achieve accurate contact tracking for the COVID-19 contact group, we extend the analysis of the GPS data to combine GPS data with video surveillance data and address a novel task named group activity trajectory recovery. Meanwhile, a new dataset called GATR-GPS is constructed to simulate a realistic scenario of COVID-19 contact tracking, and a coordinated optimization algorithm with a spatio-temporal constraint table is further proposed to realize efficient trajectory recovery of pedestrian trajectories. Extensive experiments on the novel collected dataset and commonly used two existing person re-identification datasets are performed, and the results evidently demonstrate that our method achieves competitive results compared to the state-of-the-art methods.}
}
@article{PU2022108832,
title = {Learning a deep dual-level network for robust DeepFake detection},
journal = {Pattern Recognition},
volume = {130},
pages = {108832},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108832},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003132},
author = {Wenbo Pu and Jing Hu and Xin Wang and Yuezun Li and Shu Hu and Bin Zhu and Rui Song and Qi Song and Xi Wu and Siwei Lyu},
keywords = {DeepFake detection, Multitask learning, Imbalanced learning, AUC optimization},
abstract = {Face manipulation techniques, especially DeepFake techniques, are causing severe social concerns and security problems. When faced with skewed data distributions such as those found in the real world, existing DeepFake detection methods exhibit significantly degraded performance, especially the AUC score. In this paper, we focus on DeepFake detection in real-world situations. We propose a dual-level collaborative framework to detect frame-level and video-level forgeries simultaneously with a joint loss function to optimize both the AUC score and error rate at the same time. Our experiments indicate that the AUC loss boosts imbalanced learning performance and outperforms focal loss, a state-of-the-art loss function to address imbalanced data. In addition, our multitask structure enables mutual reinforcement of frame-level and video-level detection and achieves outstanding performance in imbalanced learning. Our proposed method is also more robust to video quality variations and shows better generalization ability in cross-dataset evaluations than existing DeepFake detection methods. Our implementation is available online at https://github.com/PWB97/Deepfake-detection.}
}
@article{LI2022108872,
title = {Alleviating the estimation bias of deep deterministic policy gradient via co-regularization},
journal = {Pattern Recognition},
volume = {131},
pages = {108872},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108872},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003533},
author = {Yao Li and YuHui Wang and YaoZhong Gan and XiaoYang Tan},
keywords = {Reinforcement learning, Overestimation, Underestimation, Co-training, Deterministic policy gradient},
abstract = {The overestimation in Deep Deterministic Policy Gradients (DDPG) caused by value approximation error may result in unstable policy training. Twin Delayed Deep Deterministic Policy Gradient (TD3) addresses the overestimation but suffers from the underestimation. In this paper, we propose a Co-Regularization based Deep Deterministic (CoD2) policy gradient method to mitigate the estimation bias. Two learners characterized by overestimated and underestimated biases are trained with Co-regularization to achieve this goal. The overestimated and underestimated values are updated conservatively in CoD2 for policy evaluation. Experimental results show that our method achieves comparable performance compared with other methods.}
}
@article{ZHU2022108820,
title = {Multi-granularity episodic contrastive learning for few-shot learning},
journal = {Pattern Recognition},
volume = {131},
pages = {108820},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108820},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003016},
author = {Pengfei Zhu and Zhilin Zhu and Yu Wang and Jinglin Zhang and Shuai Zhao},
keywords = {Multi-granularity computing, Episodic contrastive learning, Few-shot learning, Deep learning},
abstract = {Few-shot learning (FSL) aims at fast adaptation to novel classes with few training samples. Among FSL methods, meta-learning and transfer learning-based methods are the most powerful ones. However, most of them rely to some extent on cross-entropy loss, which leads to representations that are overly concerned with the classes already seen, and in turn leads to sub-optimal generalization on novel classes. In this study, we are inspired by meta-learning and transfer learning-based methods and believe good feature representations are vital for FSL. To this end, we propose a new multi-granularity episodic contrastive learning method (MGECL) that introduces contrastive learning into the episode training process. In particular, by enforcing our proposed contrastive loss on both class and instance granularities, the model is able to extract category-independent discriminative patterns and learn richer and more transferable feature representations. Extensive experiments demonstrate that our proposed method achieves state-of-the-art performance on three popular few-shot benchmarks. Our code is available at https://github.com/z1358/MGECL_PR.}
}
@article{WANG2022108867,
title = {Shedding light on images: Multi-level image brightness enhancement guided by arbitrary references},
journal = {Pattern Recognition},
volume = {131},
pages = {108867},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108867},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200348X},
author = {Ya’nan Wang and Zhuqing Jiang and Chang Liu and Kai Li and Aidong Men and Haiying Wang and Xiaobo Chen},
keywords = {Low-light image enhancement, Multi-level mapping, Arbitrary references, Codec network, Decomposition, Concatenation},
abstract = {The non-linearity between human perception and image brightness levels results in different definitions of NORMAL-light. Thus, most existing low-light image enhancement methods which produce one-to-one mapping can not meet the aesthetic demand. Other pioneers enhance low-light images guided by a given value. However, the inherent problem of non-linearity will cause poor usability. To this end, we propose a user-friendly neural network for multi-level low-light image enhancement. Inspired by style transfer, our method decomposes an image into content component feature and luminance component feature in the latent space. Then we enhance the image brightness to different levels by concatenating the content components from low-light images and the luminance components from reference images. The network meets various user requirements by selecting different brightness references. Moreover, information except for brightness is preserved to alleviate color distortion. Extensive experiments demonstrate the superiority of our network against existing methods.}
}
@article{ZENG2022108896,
title = {Multivariate multi-layer classifier},
journal = {Pattern Recognition},
volume = {131},
pages = {108896},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108896},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003776},
author = {Huanze Zeng and Argon Chen},
keywords = {Classification, Classifiers, Multivariate decision tree, Machine learning, Tree construction},
abstract = {The variance-ratio binary multi-layer classifier (VRBMLC) has been recently proposed and shown to outperform conventional binary decision trees (BDTs). Though effective with better interpretability, the VRBMLC generates deep layers of tree nodes as it employs a one-feature-at-a-time binary split at each layer. To further condense the tree depth and enhance the classification performance, this research proposes a multivariate multi-layer classifier that applies a variance-ratio criterion to enable ternary splits of each tree node and that integrates the oblique discriminant hyperplane in the tree node. We benchmark 16 state-of-the-art univariate and multivariate classifiers on 43 publicly available datasets. The results show that the proposed methods greatly simplify the tree structure and yield a significantly higher average accuracy.}
}
@article{LU2022108844,
title = {Locality preserving projection with symmetric graph embedding for unsupervised dimensionality reduction},
journal = {Pattern Recognition},
volume = {131},
pages = {108844},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108844},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003259},
author = {Xiaohuan Lu and Jiang Long and Jie Wen and Lunke Fei and Bob Zhang and Yong Xu},
keywords = {Dimensionality reduction, Feature extraction, Graph embedding, Unsupervised learning},
abstract = {Preserving the intrinsic structure of data is very important for unsupervised dimensionality reduction. For structure preserving, graph embedding technique is widely considered. However, most of the existing unsupervised graph embedding based methods cannot effectively preserve the intrinsic structure of data since these methods either use the constant graph or only explore the geometric structure based on the distance information or representation information. To solve this problem, a novel method, called locality preserving projection with symmetric graph embedding (LPP_SGE), is proposed. LPP_SGE introduces a novel adaptive graph learning model and can obtain the intrinsic graph and projection in a unified framework by fully exploring the representation information and distance information of the original data. Different from the existing works which generally introduce no less than two constraints to capture the representation information and distance information, LPP_SGE can simultaneously capture the above two kinds of structure information in one term. Moreover, LPP_SGE introduces an ‘l2,1’ norm based projection constraint to select the most discriminative features from the complex data for dimensionality reduction, such that the robustness is enhanced. Experimental results on four databases and two kinds of noisy databases show that LPP_SGE performs better than many well-known methods.}
}
@article{LUONG2022108815,
title = {Multi-layer manifold learning for deep non-negative matrix factorization-based multi-view clustering},
journal = {Pattern Recognition},
volume = {131},
pages = {108815},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108815},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002965},
author = {Khanh Luong and Richi Nayak and Thirunavukarasu Balasubramaniam and Md Abul Bashar},
keywords = {Multi-view data/clustering, Manifold learning, Non-negative Matrix Factorization (NMF), Deep Matrix Factorization (DMF), Deep Non-negative Matrix Factorization (Deep-NMF)},
abstract = {Multi-view data clustering based on Non-negative Matrix Factorization (NMF) has been commonly used for pattern recognition by grouping multi-view high-dimensional data by projecting it to a lower-order dimensional space. However, the NMF framework fails to learn the accurate lower-order representation of the input data if it exhibits complex and non-linear relationships. This paper proposes a deep non-negative matrix factorization-based framework for effective multi-view data clustering by uncovering both the non-linear relationships and the intrinsic components of the data. Both the consensus and complementary information present in multiple views are sufficiently learned in the proposed framework with the effective use of constraints such as normalized cut-type and orthogonal. The optimal manifold of multi-view data is effectively incorporated in all layers of the framework. Extensive experimental results show the proposed method outperforms state-of-the-art multi-view matrix factorization-based methods.}
}
@article{ALYASEEN2022108912,
title = {Wrapper feature selection method based differential evolution and extreme learning machine for intrusion detection system},
journal = {Pattern Recognition},
volume = {132},
pages = {108912},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108912},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003934},
author = {Wathiq Laftah Al-Yaseen and Ali Kadhum Idrees and Faezah Hamad Almasoudy},
keywords = {Intrusion detection system (IDS), Feature selection, Differential evolution (DE), Extreme learning machine (ELM), NSL-KDD},
abstract = {The intrusion detection system (IDS) has gained a rapid increase of interest due to its widely recognized potential in various security fields, however, it suffers from several challenges. Different network datasets have several redundant and irrelevant features that affect the decision of the IDS classifier. Therefore, it is essential to decrease these features to improve the system performance. In this paper, an efficient wrapper feature selection method is proposed for improving the performance and decreasing the processing time of the IDS. The proposed approach employs a differential evaluation algorithm to select the useful features whilst the extreme learning machine classifier is applied after feature selection to evaluate the selected features. Many experiments are performed using the full NSL-KDD dataset to evaluate the performance of the proposed method. The results prove that the proposed approach can efficiently reduce the features, increase the accuracy, reduce the false alarm rates, and improve the processing time of the IDS in comparison to other recent related works.}
}
@article{ZHOU2022108860,
title = {Discovering unknowns: Context-enhanced anomaly detection for curiosity-driven autonomous underwater exploration},
journal = {Pattern Recognition},
volume = {131},
pages = {108860},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108860},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003417},
author = {Yang Zhou and Baihua Li and Jiangtao Wang and Emanuele Rocco and Qinggang Meng},
keywords = {Anomaly detection, Learning unknown objects, Deep learning autoencoder, Autonomous underwater robotics},
abstract = {Discovering unknown objects from visual information as curiosity is highly demanded for autonomous exploration in underwater environment. In this research, we propose an end-to-end deep neural network for anomaly detection in the highly dynamic unstructured underwater background faced by a moving robot. A novel patch-level autoencoder combined with a context-enhanced autoregressive network is introduced to differentiate abnormal patterns (unknowns) from normal ones (knowns) in fine-scale regions. The autoencoder and autoregressive network share the same encoder to extract latent features. The autoregressive branch learns semantic dependence based on conditional probability to identify anomaly in a latent feature space. The overall anomaly score is weighted by both image reconstruction loss and feature similarity loss. The model outperforms state-of-the-art anomaly detection, demonstrated on the benchmark dataset CIFAR-10. Average discrimination performance AUROC improved 2.18%, and inception distance between normal and anomalous classes improved 9.33% in Z-score. The network has been tested using three underwater datasets from underwater simulation, a real-world undersea video and public SUIM data. The AUROC accuracy improved 6.36%, 32.45% and 40.17% respectively by using the proposed patch learning paradigm. It is the first report on unknown detection as navigation clues for curiosity-driven autonomous underwater exploration.}
}
@article{LI2022108900,
title = {Adaptive weighted guided image filtering for depth enhancement in shape-from-focus},
journal = {Pattern Recognition},
volume = {131},
pages = {108900},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108900},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003818},
author = {Yuwen Li and Zhengguo Li and Chaobing Zheng and Shiqian Wu},
keywords = {Shape from focus, Depth enhancement, Adaptive weighted guided image filtering, Edge-preserving, Robustness},
abstract = {Existing shape from focus (SFF) techniques cannot preserve depth edges and fine structural details from a sequence of multi-focus images. Moreover, noise in the sequence affects the accuracy of the depth map. In this paper, a novel depth enhancement algorithm for the SFF based on an adaptive weighted guided image filtering (AWGIF) is proposed to address the above issues. The AWGIF is applied to decompose an initial depth map estimated by the traditional SFF into base and detail layers. In order to preserve the edges accurately in the refined depth map, the guidance image is constructed from the sequence, and the coefficient of the AWGIF is utilized to suppress the noise while enhancing the fine depth details. Experiments on real and synthetic objects demonstrate the superiority of our algorithm in terms of anti-noise, and the ability to preserve depth edges and fine structural details w.r.t. existing methods.}
}
@article{HUANG2022108831,
title = {Cyclical Adversarial Attack Pierces Black-box Deep Neural Networks},
journal = {Pattern Recognition},
volume = {131},
pages = {108831},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108831},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003120},
author = {Lifeng Huang and Shuxin Wei and Chengying Gao and Ning Liu},
keywords = {Adversarial example, Transferability, Black-box attack, Defenses},
abstract = {Deep neural networks (DNNs) have shown vulnerability to adversarial attacks. By exploiting the transferability of adversarial examples, attackers can fool models under black-box settings without accessing the underlying information. However, they often exhibit weak performance when transferring to defenses, which may give a false sense of security. In this paper, we propose Cyclical Adversarial Attack (CA2), a general and straightforward method to boost the transferability to break defenders. We first revisit the momentum-based methods from the perspective of optimization and find that they usually suffer from the transferability saturation dilemma. To address this, CA2 performs cyclical optimization algorithm to produce adversarial examples. Unlike the standard momentum policy that accumulates the velocity to continuously update the solution, we divide the generation process into multiple phases and treat the velocity vectors from the previous phase as proper knowledge to guide a new adversarial attack with larger steps. Moreover, CA2 applies a novel and compatible augmentation algorithm at every optimization in a loop manner for enhancing the black-box transferability further, referred to as cyclical augmentation. Extensive experiments conducted on a variety of models not only validate the efficacy of each designed algorithm in CA2, but also illustrate the superiority of our method compared with the state-of-the-art transferable attacks. Our implemental code is publicly available at https://github.com/mesunhlf/CA2.}
}
@article{CHEN2022108827,
title = {GasHis-Transformer: A multi-scale visual transformer approach for gastric histopathological image detection},
journal = {Pattern Recognition},
volume = {130},
pages = {108827},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108827},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003089},
author = {Haoyuan Chen and Chen Li and Ge Wang and Xiaoyan Li and Md {Mamunur Rahaman} and Hongzan Sun and Weiming Hu and Yixin Li and Wanli Liu and Changhao Sun and Shiliang Ai and Marcin Grzegorzek},
keywords = {Gastric histropathological image, Multi-scale visual transformer, Image detection},
abstract = {In this paper, a multi-scale visual transformer model, referred as GasHis-Transformer, is proposed for Gastric Histopathological Image Detection (GHID), which enables the automatic global detection of gastric cancer images. GasHis-Transformer model consists of two key modules designed to extract global and local information using a position-encoded transformer model and a convolutional neural network with local convolution, respectively. A publicly available hematoxylin and eosin (H&E) stained gastric histopathological image dataset is used in the experiment. Furthermore, a Dropconnect based lightweight network is proposed to reduce the model size and training time of GasHis-Transformer for clinical applications with improved confidence. Moreover, a series of contrast and extended experiments verify the robustness, extensibility and stability of GasHis-Transformer. In conclusion, GasHis-Transformer demonstrates high global detection performance and shows its significant potential in GHID task.}
}
@article{ZHUANG2022108907,
title = {Multi-criteria Selection of Rehearsal Samples for Continual Learning},
journal = {Pattern Recognition},
volume = {132},
pages = {108907},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108907},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003880},
author = {Chen Zhuang and Shaoli Huang and Gong Cheng and Jifeng Ning},
keywords = {Continual Learning, Multiple Criteria, Rehersal Method, Learning to learn},
abstract = {Retaining a small subset to replay is a direct and effective way to prevent catastrophic forgetting in continual learning. However, due to data complexity and restricted memory, picking a proper subset for rehearsal is challenging and still being explored. In this work, we present a Multi-criteria Subset Selection approach that can stabilize and advance replay-based continual learning. The method picks rehearsal samples by integrating multiple criteria, including distance to prototype, intra-class cluster variation, and classifier loss. By doing so, it maximizes the comprehensive representation power of the sampled subset by ensuring its representativeness, diversity, and discriminability. We empirically find that singular criteria are likely to fail in particular tasks, while multi-criteria minimizes this risk and stabilizes task training throughout the continual learning process. Moreover, our method improves replay-based methods consistently and achieves state-of-the-art performance on both CIFAR100 and Tiny-Imagenet datasets.}
}
@article{SOUZA2022108895,
title = {High-order conditional mutual information maximization for dealing with high-order dependencies in feature selection},
journal = {Pattern Recognition},
volume = {131},
pages = {108895},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108895},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003764},
author = {Francisco Souza and Cristiano Premebida and Rui Araújo},
keywords = {Feature selection, Mutual information, Information theory, Pattern recognition},
abstract = {This paper presents a novel feature selection method based on the conditional mutual information (CMI). The proposed High Order Conditional Mutual Information Maximization (HOCMIM) method incorporates high order dependencies into the feature selection procedure and has a straightforward interpretation due to its bottom-up derivation. The HOCMIM is derived from the CMI’s chain expansion and expressed as a maximization optimization problem. The maximization problem is solved using a greedy search procedure, which speeds up the entire feature selection process. The experiments are run on a set of benchmark datasets (20 in total). The HOCMIM is compared with eighteen state-of-the-art feature selection algorithms, from the results of two supervised learning classifiers (Support Vector Machine and K-Nearest Neighbor). The HOCMIM achieves the best results in terms of accuracy and shows to be faster than high order feature selection counterparts.}
}
@article{SHU2022108843,
title = {Using global information to refine local patterns for texture representation and classification},
journal = {Pattern Recognition},
volume = {131},
pages = {108843},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108843},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003247},
author = {Xin Shu and Hui Pan and Jinlong Shi and Xiaoning Song and Xiao-Jun Wu},
keywords = {Texture classification, Texture descriptor, Texture representation, Feature pattern refinement, Local binary pattern},
abstract = {Local binary pattern (LBP) and its variants have been successfully applied in texture feature extraction. However, it is hard for most LBP-based methods to effectively describe and distinguish the local neighborhoods with similar structures (that is, the calculated feature patterns are identical) but different contrasts or grayscales. To alleviate such problems, we propose a novel global refined local binary pattern (GRLBP) by analyzing the nature of pixel intensity distribution in local neighborhoods. GRLBP consists of two descriptors called magnitude refined local sign binary pattern (MRLBP_S) and center refined local magnitude binary pattern (CRLBP_M). MRLBP_S distinguishes local neighborhoods with contrast differences by using global magnitude anchors to refine local sign patterns. And CRLBP_M identifies local neighborhoods with grayscale differences by employing global central grayscale anchors to refine local magnitude patterns. Finally, frequency histograms of MRLBP_S and CRLBP_M from each image are cascaded to generate the GRLBP. Extensive experimental results on seven benchmark texture databases: Outex, CUReT, KTH-TIPS, UMD, UIUC, KTH-T2b, and DTD demonstrate that the proposed GRLBP can represent the detailed information of texture images. Furthermore, compared with state-of-the-art LBP variants, GRLBP has competitive advantages in classification accuracy, feature dimension, and computational complexity, respectively.}
}
@article{KIM2022108871,
title = {2PESNet: Towards online processing of temporal action localization},
journal = {Pattern Recognition},
volume = {131},
pages = {108871},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108871},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003521},
author = {Young Hwi Kim and Seonghyeon Nam and Seon Joo Kim},
keywords = {Online video understanding, Temporal action localization},
abstract = {Existing online video processing methods such as online action detection focus on a frame-level understanding for high responsiveness. However, it has a fundamental limitation in that it lacks instance-level understanding of videos, making it difficult to be applied to higher-level vision tasks. The instance-level action detection, known as Temporal Action Localization (TAL), have limitations when applying to the online settings. In this work, we introduce a new task that aims to detect action instances of videos in an online setting, named Online Temporal Action Localization (OnTAL). To tackle this problem, we propose a 2-Pass End/Start detection Network (2PESNet) that detects action instances by effectively finding the start and end of an action instance. Additionally, we propose a two-stage action end detection method to further improve the performance. Extensive experiments on THUMOS’14 and ActivityNet v1.3 demonstrate that our model is able to take both accuracy and responsiveness when predicting action instances from streaming videos.}
}
@article{LAN2022108819,
title = {Towards lifelong object recognition: A dataset and benchmark},
journal = {Pattern Recognition},
volume = {130},
pages = {108819},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108819},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003004},
author = {Chuanlin Lan and Fan Feng and Qi Liu and Qi She and Qihan Yang and Xinyue Hao and Ivan Mashkin and Ka Shun Kei and Dong Qiang and Vincenzo Lomonaco and Xuesong Shi and Zhengwei Wang and Yao Guo and Yimin Zhang and Fei Qiao and Rosa H.M. Chan},
keywords = {Robotic vision, Continual learning, Lifelong learning, Object recognition},
abstract = {Lifelong learning algorithms aim to enable robots to handle open-set and detrimental conditions, and yet there is a lack of adequate datasets with diverse factors for benchmarking. In this work, we constructed and released a lifelong learning robotic vision dataset, OpenLORIS-Object. This dataset was collected by RGB-D camera capturing dynamic environment in daily life scenarios with diverse factors, including illumination, occlusion, object pixel size and clutter, of quantified difficulty levels. To the best of our knowledge, this is an unique real-world dataset for robotic vision with independent and quantifiable environmental factors, which are currently unaccounted for in other lifelong learning datasets such as CORe50 and NICO. We tested 9 state-of-the-art algorithms with 4 evaluation metrics over the dataset in Domain Incremental Learning, Task Incremental Learning, and Class Incremental Learning scenarios. The results demonstrate that these existing algorithms are insufficient to handle lifelong learning task in dynamic environments. Our dataset and benchmarks are now publicly available at this website.22https://lifelong-robotic-vision.github.io/dataset/object}
}
@article{LI2022108911,
title = {Learning intra-domain style-invariant representation for unsupervised domain adaptation of semantic segmentation},
journal = {Pattern Recognition},
volume = {132},
pages = {108911},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108911},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003922},
author = {Zongyao Li and Ren Togo and Takahiro Ogawa and Miki Haseyama},
keywords = {Style-invariant representation, Self-ensembling, Domain adaptation},
abstract = {In this paper, we aim to tackle the problem of unsupervised domain adaptation (UDA) of semantic segmentation and improve the UDA performance with a novel conception of learning intra-domain style-invariant representation. Previous UDA methods focused on reducing the inter-domain inconsistency between the source domain and the target domain. However, due to the different data distributions of the two domains, reducing the inter-domain inconsistency cannot ensure the generalization ability of the trained model in the target domain. Therefore, to improve the UDA performance, we take into consideration the intra-domain diversity of the target domain for the first time in studies on UDA and aim to train the model to generalize well to the diverse intra-domain styles. To achieve this, we propose a self-ensembling method to learn the intra-domain style-invariant representation and we introduce a semantic-aware multimodal image-to-image translation model to obtain images with diversified intra-domain styles. Our method achieves state-of-the-art performance on two synthetic-to-real adaptation benchmarks, and we demonstrate the effectiveness of our method by conducting extensive experiments.}
}
@article{AO2022108859,
title = {Cross-modal prototype learning for zero-shot handwritten character recognition},
journal = {Pattern Recognition},
volume = {131},
pages = {108859},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108859},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003405},
author = {Xiang Ao and Xu-Yao Zhang and Cheng-Lin Liu},
keywords = {Online handwriting, Offline handwriting, Printed character, Zero-shot, Prototype, Cross-modality},
abstract = {Traditional methods of handwritten character recognition rely on extensive labeled data. However, humans can generalize to unseen handwritten characters by watching a few printed examples in textbooks. To simulate this ability, we propose a cross-modal prototype learning method (CMPL) to realize zero-shot recognition. For each character class, a prototype is generated by mapping the printed character into a deep neural network feature space. For unseen character class, its prototype can be directly produced from a printed character sample, therefore, not requiring any handwritten samples to realize class-incremental learning. Specifically, CMPL considers different modalities simultaneously - online handwritten trajectories, offline handwritten images, and auxiliary printed character images. The joint learning of the above modalities is achieved through sharing printed prototypes between online and offline data. In zero-shot inference, we feed CMPL the printed samples to obtain corresponding class prototypes, and then the unseen handwritten character can be recognized by the nearest prototype. Our experimental results demonstrate that CMPL outperforms the state-of-the-art methods in both online and offline zero-shot handwritten Chinese character recognition. Moreover, we also show the cross-domain generalization of CMPL from two perspectives: cross-language and modern-to-ancient handwritten character recognition, focusing on the transferability between different languages and different styles (i.e., modern and historical handwritings).}
}
@article{KV2022108883,
title = {On the role of question encoder sequence model in robust visual question answering},
journal = {Pattern Recognition},
volume = {131},
pages = {108883},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108883},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003648},
author = {Gouthaman KV and Anurag Mittal},
keywords = {Visual question answering, Out-of-distribution performance, Gated recurrent unit, Transformer, Graph attention network},
abstract = {Generalizing beyond the experiences has a significant role in developing robust and practical machine learning systems. It has been shown that current Visual Question Answering (VQA) models are over-dependent on the language-priors (spurious correlations between question-types and their most frequent answers) from the train set and pose poor performance on Out-of-Distribution (OOD) test sets. This conduct negatively affects the robustness of VQA models and restricts them from being utilized in real-world situations. This paper shows that the sequence model architecture used in the question-encoder has a significant role in the OOD performance of VQA models. To demonstrate this, we performed a detailed analysis of various existing RNN-based and Transformer-based question-encoders, and along, we proposed a novel Graph attention network (GAT)-based question-encoder. Our study found that a better choice of sequence model in the question-encoder reduces the over-fit to language biases and improves OOD performance in VQA even without using any additional relatively complex bias-mitigation approaches.}
}
@article{WANG2022108870,
title = {Data-attention-YOLO (DAY): A comprehensive framework for mesoscale eddy identification},
journal = {Pattern Recognition},
volume = {131},
pages = {108870},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108870},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200351X},
author = {Xinning Wang and Xuegong Wang and Chong Li and Yuben Zhao and Peng Ren},
keywords = {Mesoscale eddy identification, Attention mechanism, Data-attention-based YOLO, One-stage detection},
abstract = {The accurate mesoscale eddy identification methods with deep learning framework depend on either single eddy characteristic from altimeter missions or multi-step eddy examination strategies, disregarding those indistinguishable features from multiple eddy data integration. In this article, we first propose a data-attention-based YOLO (DAY) to precisely recognize mesoscale eddies in the South China Sea (SCS), which can hierarchically unite multiple eddy attributes and efficiently predict eddies with one-step strategy involving detection and classification. It consists of two main components: heterogeneous eddy data integration module and dynamic attention detecting module for eddy identification. The data integration component empirically transforms the field of multi-source eddy data and propagates eddy labels through automatic labeling method, which sustains a good supply for our dynamic attention-base detecting network. To thoroughly identify mesoscale eddies based on spatio-temporal patterns, DAY efficiently learns the characteristics of mesoscale eddies with an improved one-step identification YOLO network. The comparative evaluation results demonstrate that DAY achieves 54% performance improvement over the state-of-the-art methods on single gray SLA data and outperforms two-stage detecting technique Faster R-CNN by 51%.}
}
@article{MO2022108899,
title = {Dimension-aware attention for efficient mobile networks},
journal = {Pattern Recognition},
volume = {131},
pages = {108899},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108899},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003806},
author = {Rongyun Mo and Shenqi Lai and Yan Yan and Zhenhua Chai and Xiaolin Wei},
keywords = {Efficient mobile networks, Attention mechanism, Feature enhancement, Multi-branch factorization, Multi-dimensional information},
abstract = {Recently, attention mechanisms have shown great potential in improving the performance of mobile networks. Typically, they involve 2D symmetric convolution operations or generate 2D attention maps. However, such manners usually introduce high computational cost and large memory consumption, increasing the computational burden of mobile networks. To address this problem, we propose a novel lightweight attention mechanism, called Dimension-Aware Attention (DAA) block, by modeling the intra-dependencies of each dimension of the input feature map. Specifically, we factorize the channel and spatial attention by three parallel feature vector encoding branches, where stacked 1D asymmetric convolution operations can be naturally leveraged to capture large receptive fields. In this way, channel-aware, horizontal-aware, and vertical-aware attention vectors are extracted to effectively encode multi-dimensional information and greatly reduce the computational complexity of mobile networks. Experiments on multiple vision tasks demonstrate that our DAA block achieves better accuracy against state-of-the-art attention mechanisms with much lower computational operations. Our code is available at https://github.com/rymo96/DAANet.}
}
@article{SHEN2022108909,
title = {Joint operation and attention block search for lightweight image restoration},
journal = {Pattern Recognition},
volume = {132},
pages = {108909},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108909},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003909},
author = {Hao Shen and Zhong-Qiu Zhao and Wenrui Liao and Weidong Tian and De-Shuang Huang},
keywords = {Image restoration, Neural architecture search, Attention mechanism},
abstract = {Recently, block-based design methods have shown effectiveness in image restoration tasks, which are usually designed in a handcrafted manner and have computation and memory consumption challenges in practice. In this paper, we propose a joint operation and attention block search algorithm for image restoration, which focuses on searching for optimal combinations of operation blocks and attention blocks. Specifically, we first construct two search spaces: operation block search space and attention block search space. The former is used to explore the suitable operation of each layer and aims to construct a lightweight and effective operation search module (OSM). The latter is applied to discover the optimal connection of various attention mechanisms and aims to enhance the feature expression. The searched structure is called the attention search module (ASM). Then we combine OSM and ASM to construct a joint search module (JSM), which serves as the basic module to build the final network. Moreover, we propose a cross-scale fusion module (CSFM) to effectively integrate multiple hierarchical features from JSMs, which helps to mine feature corrections of intermediate layers. Extensive experiments on image super-resolution, gray image denoising, and JPEG image deblocking tasks demonstrate that our proposed network can achieve competitive performance. The source code is available on https://github.com/it-hao/JSNet.}
}
@article{KARUNANAYAKE2022108838,
title = {Artificial life for segmentation of fusion ultrasound images of breast abnormalities},
journal = {Pattern Recognition},
volume = {131},
pages = {108838},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108838},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003193},
author = {Nalan Karunanayake and Wanrudee Lohitvisate and Stanislav S. Makhanov},
keywords = {Artificial life, Fusion image, Medical image segmentation, Genetic algorithm, Ultrasound images, Breast cancer},
abstract = {Segmentation of cancerous tumors in ultrasound (US) images of human organs is one of the critical problems in medical imaging. The US images are characterized by low contrast, irregular shapes, high levels of speckle-noise and acoustic shadows, making it difficult to segment the tumor. Yet, US imaging is considered one of the most inexpensive and safe imaging tests available to detect cancer in its early stages. However, an automatic segmentation method applicable to all types of US imagery does not exist. This paper proposes a novel segmentation method that combines image fusion, artificial life (AL) and a genetic algorithm (GA). The new algorithm has been applied to US images of breast cancer. The method is based on tracing agents (TA), which are artificial organisms with memory and the ability to communicate. They live inside a fusion image generated from the US and the elastography (EL) images. The TA can recognize the patterns of strong edges and boundary gaps allowing to outline the tumor. The new model has been tested against six types of segmentation models, i.e., machine learning, active contours, level set models, superpixel models, edge linking models and selected hybrid methods. The experiments include 16 state-of-the-art methods, which outperform 69 recent and classical segmentation routines. The tests were run on 395 breast cancer images from http://onlinemedicalimages.com and https://www.ultrasoundcases.info/. TA training employs a GA. The model has been verified on “hard” cases (complex shapes, boundary leakage, and noisy edge maps). The proposed algorithm produces more accurate results than the reference methods on high complexity images. A video demo of the algorithm is at http://shorturl.at/htBW9.}
}
@article{SHI2022108923,
title = {Robust convolutional neural networks against adversarial attacks on medical images},
journal = {Pattern Recognition},
volume = {132},
pages = {108923},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108923},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004046},
author = {Xiaoshuang Shi and Yifan Peng and Qingyu Chen and Tiarnan Keenan and Alisa T. Thavikulwat and Sungwon Lee and Yuxing Tang and Emily Y. Chew and Ronald M. Summers and Zhiyong Lu},
keywords = {CNNs, Adversarial examples, Sparsity denoising},
abstract = {Convolutional neural networks (CNNs) have been widely applied to medical images. However, medical images are vulnerable to adversarial attacks by perturbations that are undetectable to human experts. This poses significant security risks and challenges to CNN-based applications in clinic practice. In this work, we quantify the scale of adversarial perturbation imperceptible to clinical practitioners and investigate the cause of the vulnerability in CNNs. Specifically, we discover that noise (i.e., irrelevant or corrupted discriminative information) in medical images might be a key contributor to performance deterioration of CNNs against adversarial perturbations, as noisy features are learned unconsciously by CNNs in feature representations and magnified by adversarial perturbations. In response, we propose a novel defense method by embedding sparsity denoising operators in CNNs for improved robustness. Tested with various state-of-the-art attacking methods on two distinct medical image modalities, we demonstrate that the proposed method can successfully defend against those unnoticeable adversarial attacks by retaining as much as over 90% of its original performance. We believe our findings are critical for improving and deploying CNN-based medical applications in real-world scenarios.}
}
@article{SHUANG2022108878,
title = {Comprehensive-perception dynamic reasoning for visual question answering},
journal = {Pattern Recognition},
volume = {131},
pages = {108878},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108878},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003594},
author = {Kai Shuang and Jinyu Guo and Zihan Wang},
keywords = {Cross-modal information fusion, Visual question answering, Comprehensive perception, Relational reasoning},
abstract = {The goal of Visual Question Answering (VQA) is to answer questions based on an image. In the VQA task, reasoning plays an important role in dealing with relations because this task has a high requirement for modeling complex features. In most existing models, the features are only extracted and integrated between adjacent layers. This pattern arguably affects the integrity of information interaction during reasoning. In this paper, we propose a comprehensive-perception dynamic reasoning (CPDR) model to utilize the cross-layer object features for multi-step compound reasoning. It calculates the interactions among the object features from all previous layers and integrates these interactions to generate new object features, iteratively. Finally, the object features of all layers will be used for the final prediction. Empirical results show that our model achieves superior performance among VQA models which are not VLP-based, and incorporating the CPDR module into the VLP models brings considerable performance improvements.}
}
@article{SHARMA2022108826,
title = {Covid-MANet: Multi-task attention network for explainable diagnosis and severity assessment of COVID-19 from CXR images},
journal = {Pattern Recognition},
volume = {131},
pages = {108826},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108826},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003077},
author = {Ajay Sharma and Pramod Kumar Mishra},
keywords = {Covid-19, Lung segmentation, Infection segmentation, Chest X-ray, Deep learning, Transfer learning, Explainable AI},
abstract = {The devastating outbreak of Coronavirus Disease (COVID-19) cases in early 2020 led the world to face health crises. Subsequently, the exponential reproduction rate of COVID-19 disease can only be reduced by early diagnosis of COVID-19 infection cases correctly. The initial research findings reported that radiological examinations using CT and CXR modality have successfully reduced false negatives by RT-PCR test. This research study aims to develop an explainable diagnosis system for the detection and infection region quantification of COVID-19 disease. The existing research studies successfully explored deep learning approaches with higher performance measures but lacked generalization and interpretability for COVID-19 diagnosis. In this study, we address these issues by the Covid-MANet network, an automated end-to-end multi-task attention network that works for 5 classes in three stages for COVID-19 infection screening. The first stage of the Covid-MANet network localizes attention of the model to the relevant lungs region for disease recognition. The second stage of the Covid-MANet network differentiates COVID-19 cases from bacterial pneumonia, viral pneumonia, normal and tuberculosis cases, respectively. To improve the interpretation and explainability, three experiments have been conducted in exploration of the most coherent and appropriate classification approach. Moreover, the multi-scale attention model MA-DenseNet201 proposed for the classification of COVID-19 cases. The final stage of the Covid-MANet network quantifies the proportion of infection and severity of COVID-19 in the lungs. The COVID-19 cases are graded into more specific severity levels such as mild, moderate, severe, and critical as per the score assigned by the RALE scoring system. The MA-DenseNet201 classification model outperforms eight state-of-the-art CNN models, in terms of sensitivity and interpretation with lung localization network. The COVID-19 infection segmentation by UNet with DenseNet121 encoder achieves dice score of 86.15% outperforming UNet, UNet++, AttentionUNet, R2UNet, with VGG16, ResNet50 and DenseNet201 encoder. The proposed network not only classifies images based on the predicted label but also highlights the infection by segmentation/localization of model-focused regions to support explainable decisions. MA-DenseNet201 model with a segmentation-based cropping approach achieves maximum interpretation of 96% with COVID-19 sensitivity of 97.75%. Finally, based on class-varied sensitivity analysis Covid-MANet ensemble network of MA-DenseNet201, ResNet50 and MobileNet achieve 95.05% accuracy and 98.75% COVID-19 sensitivity. The proposed model is externally validated on an unseen dataset, yields 98.17% COVID-19 sensitivity.}
}
@article{GAO2022108866,
title = {Exploiting key points supervision and grouped feature fusion for multiview pedestrian detection},
journal = {Pattern Recognition},
volume = {131},
pages = {108866},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108866},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003478},
author = {Xin Gao and Yijin Xiong and Guoying Zhang and Hui Deng and Kangkang Kou},
keywords = {Multiview aggregation, Pedestrian detection, Key points, Grouped feature fusion},
abstract = {Multiview pedestrian detection detects pedestrians based on the perception of the same environment from multiple perspectives. This task requires feature extraction in a single view with occlusion and aggregation of multiview information. However, existing research is limited by the local occlusion and the multiview feature stitching method, which cannot perform multiview aggregation efficiently. This paper introduces a network that utilizes key points supervision and grouped feature fusion to address these challenges. It uses key points to regress pedestrians in a single view, and augments the pedestrian consistency information in overlapping views by a grouped feature fusion module. Specifically, the proposed key points supervision effectively alleviates false negatives due to occlusion, and the grouped feature fusion module enhances pedestrian location features by computing the similarity and spatial correlation of overlapping views after single view projection to the ground plane, thereby reducing target ambiguity. Quantitative and qualitative results show that the proposed method can reduce false negatives and false positives in multiview pedestrian detection and achieve efficient multiview feature aggregation. Compared to state-of-the-art methods, the proposed model achieves superior performance, achieving the highest MODA of 92.4 and 93.9 on Wildtrack and MultiviewX datasets, respectively. We believe, to the best of our knowledge, that this approach offers a new optimization idea for multiview aggregation.}
}
@article{KIM2022108894,
title = {Conditional motion in-betweening},
journal = {Pattern Recognition},
volume = {132},
pages = {108894},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108894},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003752},
author = {Jihoon Kim and Taehyun Byun and Seungyoun Shin and Jungdam Won and Sungjoon Choi},
keywords = {Motion in-betweening, Conditional motion generation, Generative model, Motion data augmentation},
abstract = {Motion in-betweening (MIB) is a process of generating intermediate skeletal movement between the given start and target poses while preserving the naturalness of the motion, such as periodic footstep motion while walking. Although state-of-the-art MIB methods are capable of producing plausible motions given sparse key-poses, they often lack the controllability to generate motions satisfying the semantic contexts required in practical applications. We focus on the method that can handle pose or semantic conditioned MIB tasks using a unified model. We also present a motion augmentation method to improve the quality of pose-conditioned motion generation via defining a distribution over smooth trajectories. Our proposed method outperforms the existing state-of-the-art MIB method in pose prediction errors while providing additional controllability. Our code and results are available on our project web page: https://jihoonerd.github.io/Conditional-Motion-In-Betweening.}
}
@article{LIU2022108842,
title = {Hiding multiple images into a single image via joint compressive autoencoders},
journal = {Pattern Recognition},
volume = {131},
pages = {108842},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108842},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003235},
author = {Xiyao Liu and Ziping Ma and Zhihong Chen and Fangfang Li and Ming Jiang and Gerald Schaefer and Hui Fang},
keywords = {Image hiding, Neural networks, Deep learning, Compressive autoencoder},
abstract = {Interest in image hiding has been continually growing. Recently, deep learning-based image hiding approaches improve the hidden capacity significantly. However, the major challenges of the existing methods are that they are difficult to balance between the errors of the modified cover image and those of the recovered secret image. To solve this problem, in this paper, we develop an image hiding algorithm based on a joint compressive autoencoder framework. Further, we propose a novel strategy to enlarge the hidden capacity, i.e., hiding multi-images in one container image. Specifically, our approach provides an extremely high image hidden capacity coupled with small reconstruction errors of the secret image. More importantly, we tackle the trade-off problem of earlier approaches by mapping the image representations in the latent spaces of the joint compressive autoencoder models, leading to both high visual quality of the container image and low reconstruction error the secret image. In an extensive set of experiments, we confirm our proposed approach to outperform several state-of-the-art image hiding methods, yielding high imperceptibility and steganalysis resistance of the container images with high recovery quality of the secret images, while improving the image hidden capacity significantly (four times higher than full-image hiding capacity).}
}
@article{BRAGANTINI2022108882,
title = {Rethinking interactive image segmentation: Feature space annotation},
journal = {Pattern Recognition},
volume = {131},
pages = {108882},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108882},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003636},
author = {Jordão Bragantini and Alexandre X. Falcão and Laurent Najman},
keywords = {Interactive image segmentation, Data annotation, Interactive machine learning, Feature space annotation},
abstract = {Despite the progress of interactive image segmentation methods, high-quality pixel-level annotation is still time-consuming and laborious — a bottleneck for several deep learning applications. We take a step back to propose interactive and simultaneous segment annotation from multiple images guided by feature space projection. This strategy is in stark contrast to existing interactive segmentation methodologies, which perform annotation in the image domain. We show that feature space annotation achieves competitive results with state-of-the-art methods in foreground segmentation datasets: iCoSeg, DAVIS, and Rooftop. Moreover, in the semantic segmentation context, it achieves 91.5% accuracy in the Cityscapes dataset, being 74.75 times faster than the original annotation procedure. Further, our contribution sheds light on a novel direction for interactive image annotation that can be integrated with existing methodologies. The supplementary material presents video demonstrations. Code available at https://github.com/LIDS-UNICAMP/rethinking-interactive-image-segmentation.}
}
@article{WU2022108830,
title = {Heterogeneous representation learning and matching for few-shot relation prediction},
journal = {Pattern Recognition},
volume = {131},
pages = {108830},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108830},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003119},
author = {Tao Wu and Hongyu Ma and Chao Wang and Shaojie Qiao and Liang Zhang and Shui Yu},
keywords = {Knowledge graphs, Few-shot learning, Relation prediction, Representation learning, Convolutional network},
abstract = {The recent explosive development of knowledge graphs (KGs) in artificial intelligence tasks coupled with incomplete or partial information has triggered considerable research interest in relation prediction. However, many challenges still remain unsolved: (i) the previous relation prediction methods require a significant amount of training instances (i.e., head-tail entity pairs) for every relation, which is infeasible in practical scenarios; and (ii) the representation learning of entities and relations always assumes that all local neighbors and their features contribute equally to the embedding, not sufficiently considering the heterogeneity of the information; and (iii) the state-of-the-art methods usually require a lot of training time, resulting in a high cost in real-world applications. To overcome these challenges, we propose a heterogeneous representation learning and matching approach, Multi-metric Feature Extraction Network (MFEN for short), for few-shot relation prediction in KGs. Our method focuses on knowledge graphs to sufficiently explore the topological structure and node content in graphs. Rather than taking the average of the embeddings of all relational neighbors, a heterogeneity-aware representation learning method is proposed to generate high-expressive embeddings, which capture the heterogenous roles of the relational neighbors of given entity and all of their features via a convolutional encoder. To learn the expressive representations efficiently, a single-layer CNN architecture with multi-scale filters is devised. In addition, multiple heuristic metrics are combined to efficiently improve the accuracy of similarity calculation. The proposed MFEN model is evaluated on two representative benchmark datasets NELL and Wiki. Extensive experiments have demonstrated that our method gets more than 5% accuracy improvement and three times speedup to state-of-the-art models. Code is available on https://github.com/summer-funny/MFEN.}
}
@article{YANG2022108910,
title = {Tree-based data augmentation and mutual learning for offline handwritten mathematical expression recognition},
journal = {Pattern Recognition},
volume = {132},
pages = {108910},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108910},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003910},
author = {Chen Yang and Jun Du and Jianshu Zhang and Changjie Wu and Mingjun Chen and JiaJia Wu},
keywords = {Tree-based data augmentation, Tree-based mutual learning, Encoder-decoder, Offline handwritten mathematical expression recognition},
abstract = {Recently, thanks to the successful application of the attention-based encoder-decoder framework, handwritten mathematical expression recognition (HMER) has achieved significant improvement. However, HMER is still a challenging task in the handwriting recognition area, which suffers from the ambiguity of handwritten symbols, the two-dimensional structure of mathematical expressions, and the lack of labeled data. In this paper, we attempt to improve the recognition performance and generalization ability of the existing state-of-the-art method from two perspectives: data augmentation and model design. We first propose a tree-based multi-level (including symbol level, sub-expression level, and image level) data augmentation strategy, which can generate many synthetic images. Then, we present a novel encoder-decoder hybrid model via tree-based mutual learning to fully utilize the complementarity between tree decoder and string decoder. Benefitting from our data augmentation strategy, we achieve 58.47%/57.82%/62.67% and 74.45% expression recognition accuracy respectively on the CROHME14/16/19 competition datasets and the OffRaSHME20 competition dataset. Moreover, tree-based data augmentation is a key technology to our champion system for the OffRaSHME20 competition. Our tree-based mutual learning method further improves the recognition accuracy to 61.63%/59.81%/64.38% and 75.68% on these datasets. Further quantitative and qualitative analyses also demonstrate the effectiveness and robustness of our proposed methods.}
}
@article{HU2022108906,
title = {Feature Nonlinear Transformation Non-Negative Matrix Factorization with Kullback-Leibler Divergence},
journal = {Pattern Recognition},
volume = {132},
pages = {108906},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108906},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003879},
author = {Lirui Hu and Ning Wu and Xiao Li},
keywords = {Non-negative matrix factorization, Nonlinear transformation, Feature extraction, Object recognition, Clustering, Kullback-Leibler divergence},
abstract = {This paper introduces a Feature Nonlinear Transformation Non-Negative Matrix Factorization with Kullback-Leibler Divergence (FNTNMF-KLD) for extracting the nonlinear features of a matrix in standard NMF. This method uses a nonlinear transformation to act on the feature matrix for constructing a NMF model based on the objective function of Kullback-Leibler Divergence, and the Taylor series expansion and the Newton iteration formula of solving root are used to obtain the iterative update rules of the basis matrix and the feature matrix. Experimental results show that the proposed method obtains the nonlinear features of data matrix in a more efficient way. In object recognition and clustering tasks, better accuracy can be achieved over some typical NMF methods.}
}
@article{SHI2022108837,
title = {Multimodal channel-wise attention transformer inspired by multisensory integration mechanisms of the brain},
journal = {Pattern Recognition},
volume = {130},
pages = {108837},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108837},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003181},
author = {Qianqian Shi and Junsong Fan and Zuoren Wang and Zhaoxiang Zhang},
keywords = {Multisensory integration, Top-down attention, Multimodal transformer, Fine-grained bird recognition, Emotion recognition},
abstract = {Multisensory integration has attracted intense studies for decades. How to combine visual and auditory information to optimize perception and decision-making is a key question in neuroscience as well as machine learning. Inspired by the mechanisms of multisensory integration in the brain, we propose a multimodal channel-wise attention transformer (MCAT) that performs reliability-weighted integration and revises the weights allocation according to a top-down attention-like mechanism. We apply MCAT on EF-LSTM neural networks for a fine-grained video bird recognition task, and on MulT neural networks for an emotion recognition task. The performance of both models is improved remarkably. Ablation study shows that the attention mechanism is indispensable for effective multisensory integration. Moreover, we found that cross-modal integration models are in accordance with the law of inverse effectiveness of multisensory integration in the brain, which reveals that our model may have mechanisms similar to those in the brain. Taken together, the results demonstrate that the brain-inspired MCAT block is effective for improving multisensory integration, providing useful clues for designing new algorithms and understanding multisensory integration in the brain.}
}
@article{ZHANG2022108877,
title = {SO-softmax loss for discriminable embedding learning in CNNs},
journal = {Pattern Recognition},
volume = {131},
pages = {108877},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108877},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003582},
author = {Qiang Zhang and Jibin Yang and Xiongwei Zhang and Tieyong Cao},
keywords = {Convolutional neural networks, Cosine similarity, Cross entropy loss, Quadratic transformation, Embedding learning, Softmax},
abstract = {Convolutional neural networks (CNNs)-based classifiers, trained with the softmax cross-entropy loss, have achieved remarkable success in learning embeddings for pattern recognition. The cosine similarity-based softmax variants further improve the performance by focusing on optimizing the angles between embeddings and class weights. However, embeddings learned by these variants still have significant intra-class variances since these methods only optimize the relative differences between intra- and inter-class cosine similarities. To simultaneously optimize intra- and inter-class cosine similarities, this paper proposes a cosine Similarity Optimization-based softmax (SO-softmax) loss, which is based on a generalized softmax loss formulation that combines both similarities. The proposed loss constrains the intra-class (positive) and inter-class (negative) cosine similarity by quadratic transformations, thus making the embedding representation more compact within classes and more distinguishable between classes. It is verified theoretically that SO-softmax loss can optimize both the similarities simultaneously. Thorough experiments are conducted on typical audio classification, image classification, face verification, image retrieval, and person re-identification tasks, and the results show that SO-softmax loss outperforms the state-of-the-art loss functions in CNNs-based frameworks.}
}
@article{PEI2022108825,
title = {Multi-scale attention-based pseudo-3D convolution neural network for Alzheimer’s disease diagnosis using structural MRI},
journal = {Pattern Recognition},
volume = {131},
pages = {108825},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108825},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003065},
author = {Zhao Pei and Zhiyang Wan and Yanning Zhang and Miao Wang and Chengcai Leng and Yee-Hong Yang},
keywords = {Diagnosis of Alzheimer’s disease, Pseudo-3D, Attention mechanism, Multi-scale, Joint loss function},
abstract = {Recently, deep learning based Computer-Aided Diagnosis methods have been widely utilized due to their highly effective diagnosis of patients. Although Convolutional Neural Networks (CNNs) are capable of extracting the latent structural characteristics of dementia and of capturing the changes of brain anatomy in Magnetic Resonance Imaging (MRI) scans, the high-dimensional input to a deep CNN usually makes the network difficult to train, and affects its diagnostic accuracy. In this paper, a novel method called the hierarchical pseudo-3D convolution neural network based on a kernel attention mechanism with a new global context block, which is abbreviated as “PKG-Net”, is proposed to accurately predict Alzheimer’s disease even when the input features are complex. Specifically, the proposed network first extracts multi-scale features from pre-processed images. Second, the attention mechanism and global context blocks are applied to combine features from different layers to hierarchically transform the MRI into more compact high-level features. Then, a joint loss function is used to train the proposed network to generate more distinguishing features, which improve the generalization performance of the network. In addition, we combine our method with different architectures. Extensive experiments are conducted to analyze the performance of the PKG-Net with different hyper-parameters and architectures. Finally, in order to verify the effectiveness of our method on Alzheimer’s disease diagnosis, we carry out extensive experiments on the ADNI dataset, and compare the results of our method with that of existing methods in terms of accuracy, recall and precision. Furthermore, our network can fully take advantage of the deep 3D convolutional neural network for automatic feature extraction and representation, and thus can avoid the limitation of low processing efficiency caused by the preprocessing procedure in which a specific area needs to be annotated in advance. Finally, we evaluate our proposed framework using two public datasets, ADNI-1 and ADNI-2, and the experimental results show that our proposed framework can achieve superior performance over state-of-the-art approaches.}
}
@article{HU2022108893,
title = {Learning deep morphological networks with neural architecture search},
journal = {Pattern Recognition},
volume = {131},
pages = {108893},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108893},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003740},
author = {Yufei Hu and Nacim Belkhir and Jesus Angulo and Angela Yao and Gianni Franchi},
keywords = {Mathematical morphology, Deep learning, Architecture search, Edge detection, Semantic segmentation},
abstract = {Deep Neural Networks (DNNs) are generated by sequentially performing linear and non-linear processes. The combination of linear and non-linear procedures is critical for generating a sufficiently deep feature space. Most non-linear operators are derivations of activation functions or pooling functions. Mathematical morphology is a branch of mathematics that provides non-linear operators for various image processing problems. This paper investigates the utility of integrating these operations into an end-to-end deep learning framework. DNNs are designed to acquire a realistic representation for a particular job. Morphological operators give topological descriptors that convey salient information about the shapes of objects depicted in images. We propose a method based on meta-learning to incorporate morphological operators into DNNs. The learned architecture demonstrates how our novel morphological operations significantly increase DNN performance on various tasks, including picture classification, edge detection, and semantic segmentation. Our codes are available at https://nao-morpho.github.io/.}
}
@article{2024110163,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {147},
pages = {110163},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(23)00860-9},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008609}
}
@article{WANG2022108841,
title = {An entity-weights-based convolutional neural network for large-sale complex knowledge embedding},
journal = {Pattern Recognition},
volume = {131},
pages = {108841},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108841},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003223},
author = {Zhengdi Wang and Lvqing Yang and Zhenfeng Lei and Anwar {Ul Haq} and Defu Zhang and Shuangyuan Yang and Akindipe Olusegun Francis},
keywords = {Graph-based finance, Representation learning, Complete incidence matrix, Convolutional neural network, Matrix factorization},
abstract = {Knowledge graph (KG) has increasingly been seen as a significant resource in financial applications (e.g., risk control, auditing and anti-fraud). However, there are few prior studies that focus on multi-relational circles, extracting additional information under the completed KG and selecting similarity measures for knowledge representation. In this paper, we introduce multi-relational circles and propose a novel embedding model, which considers entity weights calculated by PageRank algorithm to improve TransE method. In order to extract additional information, we use entity weights to convert embeddings into an on-map mining problem, and propose a model called CNNe based on entity weights and a convolutional neural network with three hidden layers, which converts vectors of entities, entity weights and relationships into matrices to perform link prediction in the same way as image processing. With the help of ten different similarity measures, it is demonstrated that the choice of distance measure greatly effect the results of the translation embedding models. Moreover, we propose two embedding methods, sMFE and tMFE, to enhance the results using matrix factorization. The complete incidence matrix is first applied to knowledge embedding, which contains the most comprehensive topological properties of the graph. Experimental results on standard benchmark datasets demonstrate that the proposed models are effective. In particular, CNNe achieves a mean rank of 166 less than the baseline method and an improvement of 2.1% on the proportion of correct entities ranked in the top ten on YAGO3-10 dataset.}
}
@article{YANG2022108836,
title = {To Actively Initialize Active Learning},
journal = {Pattern Recognition},
volume = {131},
pages = {108836},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108836},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200317X},
author = {Yazhou Yang and Marco Loog},
keywords = {active learning, active initialization, nearest neighbor criterion, minimum nearest neighbor distance},
abstract = {Though much effort has been spent on designing new active learning algorithms, little attention has been paid to the initialization problem of active learning, i.e., how to find a set of labeled samples which contains at least one instance per category. This work identifies the initialization of active learning as a separate and novel research problem, reviews existing methods that can be adapted to be used for this task and, in addition, proposes a new active initialization criterion: the Nearest Neighbor Criterion. Experiments on 16 benchmark datasets verify that the novel method often finds an initialization set with fewer queried samples than other methods do.}
}
@article{LIN2022108917,
title = {BSCA-Net: Bit Slicing Context Attention network for polyp segmentation},
journal = {Pattern Recognition},
volume = {132},
pages = {108917},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108917},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003983},
author = {Yi Lin and Jichun Wu and Guobao Xiao and Junwen Guo and Geng Chen and Jiayi Ma},
keywords = {Medical image segmentation, Polyp segmentation, Colonoscopy, Attention mechanism},
abstract = {In this paper, we propose a novel Bit-Slicing Context Attention Network (BSCA-Net), an end-to-end network, to improve the extraction ability of boundary information for polyp segmentation. The core of BSCA-Net is a new Bit Slice Context Attention (BSCA) module, which exploits the bit-plane slicing information to effectively extract the boundary information between polyps and the surrounding tissue. In addition, we design a novel Split-Squeeze-Bottleneck-Union (SSBU) module, to exploit the geometrical information from different aspects. Also, based on SSBU, we propose an multipath concat attention decoder (MCAD) and an multipath attention concat encoder (MACE), to further improve the network performance for polyp segmentation. Finally, by combining BSCA, SSBU, MCAD and MACE, the proposed BSCA-Net is able to effectively suppress noises in feature maps, and simultaneously improve the ability of feature expression in different levels, for polyp segmentation. Empirical experiments on five benchmark datasets (Kvasir, CVC-ClinicDB, ETIS, CVC-ColonDB and CVC-300) demonstrate the superior of the proposed BSCA-Net over existing cutting-edge methods.}
}
@article{WU2022108865,
title = {Inter-Attribute awareness for pedestrian attribute recognition},
journal = {Pattern Recognition},
volume = {131},
pages = {108865},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108865},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003466},
author = {Junyi Wu and Yan Huang and Zhipeng Gao and Yating Hong and Jianqiang Zhao and Xinsheng Du},
keywords = {Pedestrian attribute recognition, Inter-Attribute awareness, Vector-Neuron capsules},
abstract = {The task of pedestrian attribute recognition (PAR) is to distinguish a series of person semantic attributes. Generally, existing methods adopt multi-label classification algorithms to tackle the PAR task by utilizing multiple attribute labels. Despite remarkable progress, this kind of method normally ignores relations between different attributes. In order to be aware of relations between attributes, we propose an inter-attribute aware network via vector-neuron capsule for PAR (IAA-Caps). Our IAA-Caps method replaces traditional one-dimensional scalar neurons with two-dimensional vector-neuron capsules by embedding them in IAA-Caps. Specifically, during IAA-Caps training, one dimension in capsules is used to recognize different attributes, and the other dimension is used to strengthen the relations of different attributes. Through considering inter-attribute relations, compared with previous methods that use a heavyweight backbone (e.g., ResNet50 or BN-Inception), a more lightweight backbone (i.e., OSNet) can be adopted in our proposed IAA-Caps to achieve better performance. Experiments are conducted on several PAR benchmark datasets, including PETA, PA-100K, RAPv1, and RAPv2, demonstrating the effectiveness of the proposed IAA-Caps. In addition, experiments also show that the proposed method can improve the performance of PAR on different backbones, showing its generalization ability.}
}
@article{PARK2022108905,
title = {Wasserstein approximate bayesian computation for visual tracking},
journal = {Pattern Recognition},
volume = {131},
pages = {108905},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108905},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003867},
author = {Jinhee Park and Junseok Kwon},
abstract = {In this study, we present novel visual tracking methods based on the Wasserstein approximate Bayesian computation (ABC). For visual tracking, the proposed Wasserstein ABC (WABC) method approximates the likelihood within the Wasserstein space more accurately than the conventional ABC methods by directly measuring the discrepancy between the likelihood distributions. To encode the temporal dependency among time-series likelihood distributions, we extend the WABC method to the time-series WABC (TWABC) method. Subsequently, the proposed Hilbert TWABC (HTWABC) method reduces the computational costs caused by the TWABC method while substituting the original Wasserstein distance with the Hilbert distance. Experimental results demonstrate that the proposed visual trackers outperform other state-of-the-art visual tracking methods quantitatively. Moreover, ablation studies verify the effectiveness of individual components consisting of the proposed method (e.g., the Wasserstein distance, curve matching, and Hilbert metric).}
}
@article{WU2022108881,
title = {Complementarity-aware cross-modal feature fusion network for RGB-T semantic segmentation},
journal = {Pattern Recognition},
volume = {131},
pages = {108881},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108881},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003624},
author = {Wei Wu and Tao Chu and Qiong Liu},
keywords = {RGB-T, Cross-modal fusion, Multi-supervision, Semantic segmentation},
abstract = {RGB-T semantic segmentation has attracted growing attention because it makes a model robust towards challenging illumination. Most existing methods fuse RGB and thermal information in an equal manner along spatial dimensions, which results in feature redundancy and affects the discriminability of cross-modal features. In this paper, we propose a Complementarity-aware Cross-modal Feature Fusion Network (CCFFNet) including a Complementarity-Aware Encoder (CAE) and a Three-Path Fusion and Supervision (TPFS). The CAE, which consists of cascaded cross-modal fusion modules, can select complementary information from RGB and thermal features via a novel gate and fuse them by a channel-wise weighting mechanism. TPFS not only iteratively performs Three-Path Fusion (TPF) to further enhance cross-modal features, but also supervise the training of CCFFNet along three branches by Three-Supervision (TS). Extensive experiments are carried out and the results demonstrate that our model outperforms the state-of-the-art models by at least 1.6% mIoU on MFNet dataset and 2.9% mIoU on PST900 dataset, respectively. And a single-modality-based model can be easily applied to multi-modal semantic segmentation when plugging our CAE.}
}
@article{LIU2022108829,
title = {CVM-Cervix: A hybrid cervical Pap-smear image classification framework using CNN, visual transformer and multilayer perceptron},
journal = {Pattern Recognition},
volume = {130},
pages = {108829},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108829},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003107},
author = {Wanli Liu and Chen Li and Ning Xu and Tao Jiang and Md Mamunur Rahaman and Hongzan Sun and Xiangchen Wu and Weiming Hu and Haoyuan Chen and Changhao Sun and Yudong Yao and Marcin Grzegorzek},
keywords = {Convolutional neural network, Visual transformer, Multilayer perceptron, Cervical cell classification, Pap smear, Image classification},
abstract = {Cervical cancer is the seventh most common cancer among all the cancers worldwide and the fourth most common cancer among women. Cervical cytopathology image classification is an important method to diagnose cervical cancer. However, manual inspection is very troublesome, and experts are prone to make mistakes. The emergence of the automatic computer-aided diagnosis system solves this problem. This paper proposes a framework called CVM-Cervix based on deep learning to perform cervical cell classification tasks. It can analyze pap slides quickly and accurately. CVM-Cervix first proposes a Convolutional Neural Network module and a Visual Transformer module for local and global feature extraction respectively, then a Multilayer Perceptron module is designed to fuse the local and global features for the final classification. Experimental results show the effectiveness and potential of the proposed CVM-Cervix in the field of cervical Pap smear image classification. In addition, according to the practical needs of clinical work, we perform a lightweight post-processing to compress the model.}
}
@article{LU2022108869,
title = {A novel part-level feature extraction method for fine-grained vehicle recognition},
journal = {Pattern Recognition},
volume = {131},
pages = {108869},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108869},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003508},
author = {Lei Lu and Ping Wang and Yijie Cao},
keywords = {Fine-grained recognition, Part-level feature extraction, Feature grouping, Feature fusion},
abstract = {In this paper, we propose a novel part-level feature extraction method to enhance the discriminative ability of deep convolutional features for the task of fine-grained vehicle recognition. Generally, the challenges for fine-grained vehicle recognition are mainly caused by the subtle visual differences between part regions of vehicles. Therefore, it is essential to extract discriminative features from part regions. Many existing methods, especially deep convolutional neural networks (D-CNNs), tend to detect the discriminative part regions explicitly or learn the part information implicitly through network restructuring and neglect the abundant part-level information contained in the high-level features generated by CNNs. In light of this, we propose a simple and effective part-level feature extraction method to enhance the representation of part-level features within the global features of target object generated by the backbone networks. The proposed method is built on the deep convolutional layers from which the discriminative part features could be integrated and extracted accordingly. More specifically, a basic feature grouping module is adopted to integrate the feature maps of deep convolutional layers into groups in each of which the related discriminative parts are assembled. The feature grouping process is performed in a multi-stage manner to ensure the integration process. Then a fusion module follows to model the coarse-to-fine relationship of the part features and further ensure the integrity and effectiveness of the part features. We conduct comparison experiments on public datasets, and the results show that the proposed method achieves comparable performance with state-of-the-art algorithms.}
}
@article{YIN2024110217,
title = {Corrigendum to “GITGAN: Generative inter-subject transfer for EEG motor imagery analysis” [Pattern Recognition 146 (2024) 110015]},
journal = {Pattern Recognition},
volume = {148},
pages = {110217},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110217},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009147},
author = {Kang Yin and Elissa Yanting Lim and Seong-Whan Lee}
}
@article{SHU2022108921,
title = {Privileged multi-task learning for attribute-aware aesthetic assessment},
journal = {Pattern Recognition},
volume = {132},
pages = {108921},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108921},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004022},
author = {Yangyang Shu and Qian Li and Lingqiao Liu and Guandong Xu},
keywords = {Aesthetic assessment, Privileged information, Multi-task learning},
abstract = {Aesthetic attributes are crucial for aesthetics because they explicitly present some photo quality cues that a human expert might use to evaluate a photo’s aesthetic quality. However, the aesthetic attributes have not been largely and sufficiently exploited for photo aesthetic assessment. In this paper, we propose a novel approach to photo aesthetic assessment with the help of aesthetic attributes. The aesthetic attributes are used as privileged information (PI), which is often available during training phase but unavailable in prediction phase due to the high collection expense. The proposed framework consists of a deep multi-task network as generator and a fully connected network as discriminator. Deep multi-task network learns the aesthetic attributes and score simultaneously to capture their dependencies and extract better feature representations. Specifically, we use ranking constraint in the label space, similarity constraint and prior probabilities loss in the privileged information space to make the output of multi-task network converge to that of ground truth. Adversarial loss is used to identify and distinguish the predicted privileged information of a deep multi-task network from the ground truth PI distribution. Experimental results on two benchmark databases demonstrate the superiority of the proposed method to state-of-the-art.}
}
@article{LIU2022108848,
title = {Learning multimodal relationship interaction for visual relationship detection},
journal = {Pattern Recognition},
volume = {132},
pages = {108848},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108848},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003296},
author = {Zhixuan Liu and Wei-Shi Zheng},
keywords = {Visual relationship detection, Scene graph generation, Relationship context, Multimodal relationship interaction},
abstract = {Visual relationship detection aims to recognize visual relationships in scenes as triplets 〈subject-predicate-object〉. Previous works have shown remarkable progress by introducing multimodal features, external linguistics, scene context, etc. Due to the loss of informative multimodal hyper-relations (i.e. relations of relationships), the meaningful contexts of relationships are not fully captured yet, which limits the reasoning ability. In this work, we propose a Multimodal Similarity Guided Relationship Interaction Network (MSGRIN) to explicitly model the relations of relationships in graph neural network paradigm. In a visual scene, the MSGRIN takes the visual relationships as nodes to construct an adaptive graph and enhances deep message passing by introducing Entity Appearance Reconstruction, Entity Relevance Filtering and Multimodal Similarity Attention. We have conducted extensive experiments on two datasets: Visual Relationship Detection (VRD) and Visual Genome (VG). The evaluation results demonstrate that the proposed MSGRIN has empirically performed more effectively overall.}
}
@article{YU2022108876,
title = {A novel explainable neural network for Alzheimer’s disease diagnosis},
journal = {Pattern Recognition},
volume = {131},
pages = {108876},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108876},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003570},
author = {Lu Yu and Wei Xiang and Juan Fang and Yi-Ping {Phoebe Chen} and Ruifeng Zhu},
keywords = {Explainable neural networks, XAI, High-resolution heatmap, MRI},
abstract = {Visual classification for medical images has been dominated by convolutional neural networks (CNNs) for years. Though they have shown great performance on accuracy, some of them provide decisions that are hard to explain while others encode information from irrelevant or noisy regions. In this work, we try to close this gap by proposing an explainable framework which consists of a predictor and an explainable tool, so as to provide accurate diagnoses with intuitive visualization maps and prediction basis. Specifically, the predictor is designed by applying attention mechanisms to multi-scale features so as to learn and discover class discriminative latent representations that are close to each brain volume’s label. Meanwhile, to explain our predictor, we propose the novel explainable tool which includes a high-resolution visualization method and a prediction-basis creation and retrieval module. The former effectively integrates the feature maps of intermediate layers as well as the last convolutional layer, which surpasses state-of-the-art visualization approaches in producing high-resolution representations with more accurate localization of discriminative areas. While the latter provides prediction basis evidence via retrieved volumes with similar latent representations which are accessible to neurologists. Extensive experiments show that the proposed framework achieves higher level of accuracy and explainability over other state-of-the-art solutions. More importantly, it localizes crucial brain areas with clearer boundaries, less noises, which matches background knowledge in the neuroscience literature.}
}
@article{HU2022108824,
title = {Model scheduling and sample selection for ensemble adversarial example attacks},
journal = {Pattern Recognition},
volume = {130},
pages = {108824},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108824},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003053},
author = {Zichao Hu and Heng Li and Liheng Yuan and Zhang Cheng and Wei Yuan and Ming Zhu},
keywords = {Adversarial example, Black-box attack, Model scheduling, Sample selection},
abstract = {Adversarial examples refer to the malicious inputs that can mislead deep neural networks (DNNs) to falsely classify them. In practice, some adversarial examples are transferable and hence can deceive different target models. In multi-stage ensemble adversarial example attacks, adversaries can generate strongly transferable adversarial examples through iteratively perturbing legitimate examples to attack well-trained source models in a white-box manner. Limited by computational and memory resources (e.g., GPU memory), however, adversaries cannot handle all models and all legitimate examples at a time. This brings an important but never studied research issue: how to optimally schedule source models and appropriately select samples to improve adversarial example transferability and reduce unnecessary computational overheads? To shed light on this problem, we develop a novel multi-stage ensemble adversarial example attack method based on our proposed strategies of model scheduling and sample selection. The first strategy schedules source models to be attacked in every stage, based on the criteria of decision boundary similarity and model diversity. The second selects input samples to be handled by ensemble attacks, according to their sensitivity level for adversarial perturbations. To our knowledge, we are the first to study model scheduling and sample selection for multi-stage ensemble attacks. We conduct extensive experiments on three datasets with a variety of source and target models. Experiments show that our model scheduling based ensemble attack outperforms the all-model ensemble attack and the state-of-the-art ensemble attacks SCES, SMBEA and EnsembleFool in transferability. Moreover, our sample selection strategy improves attack success rate by about 138%.}
}
@article{YANG2022108916,
title = {Multi-feature sparse similar representation for person identification},
journal = {Pattern Recognition},
volume = {132},
pages = {108916},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108916},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003971},
author = {Meng Yang and Lei Liao and Kangyin Ke and Guangwei Gao},
keywords = {Multi-feature, Person identification, Sparse representation},
abstract = {Person identification with a single feature (e.g., face recognition, speaker verification, person re-identification, etc.) has been studied extensively for many years, while few works focus on multi-feature person identification. Though promising performance has been achieved by only using the information of facial images, voice, or pedestrian appearance, it is still challenging to recognize a person with only a single feature in some situations (e.g., a person at a distance or occluded by other objects, and a partial person out of view). In this paper, we present a multi-feature sparse similar representation (MFSSR) method to effectively fuse face features, body features, and global image features for the task of person identification. In MFSSR, we designed a reconstructed deep spatial feature for representing the appearance of human body by using the spatial correlation coding of partial deep spatial features. Then we presented a multi-feature sparse similar representation model for jointly using different features, e.g., face, body, and the global image. Besides, considering that the coding coefficients associated with good samples but not outliers should be more similar among different features, we jointly represent different features by imposing a weighted ℓ1-norm distance regularization, instead of the conventional ℓ2-norm regularization, on the coefficients. Experimental results on several multi-feature person identification databases have clearly shown the superior performance of the proposed model.}
}
@article{REN2022108864,
title = {DARTSRepair: Core-failure-set guided DARTS for network robustness to common corruptions},
journal = {Pattern Recognition},
volume = {131},
pages = {108864},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108864},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003454},
author = {Xuhong Ren and Jianlang Chen and Felix Juefei-Xu and Wanli Xue and Qing Guo and Lei Ma and Jianjun Zhao and Shengyong Chen},
keywords = {Network architecture search, Core-failure-set selection, Robustness enhancement, Differentiable architecture search},
abstract = {Network architecture search (NAS), in particular the differentiable architecture search (DARTS) method, has shown a great power to learn excellent model architectures on the specific dataset of interest. In contrast to using a fixed dataset, in this work, we focus on a different but important scenario for NAS: how to refine a deployed network’s model architecture to enhance its robustness with the guidance of a few collected and misclassified examples that are degraded by some real-world unknown corruptions having a specific pattern (e.g., noise, blur, etc..). To this end, we first conduct an empirical study to validate that the model architectures can be definitely related to the corruption patterns. Surprisingly, by just adding a few corrupted and misclassified examples (e.g., 103 examples) to the clean training dataset (e.g., 5.0×104 examples), we can refine the model architecture and enhance the robustness significantly. To make it more practical, the key problem, i.e., how to select the proper failure examples for the effective NAS guidance, should be carefully investigated. Then, we propose a novel core-failure-set guided DARTS that embeds a K-center-greedy algorithm for DARTS to select suitable corrupted failure examples to refine the model architecture. We use our method for DARTS-refined DNNs on the clean as well as 15 corruptions with the guidance of four specific real-world corruptions. Compared with the state-of-the-art NAS as well as data-augmentation-based enhancement methods, our final method can achieve higher accuracy on both corrupted datasets and the original clean dataset. On some of the corruption patterns, we can achieve as high as over 45% absolute accuracy improvements.}
}
@article{LIANG2022108840,
title = {Video summarization with a convolutional attentive adversarial network},
journal = {Pattern Recognition},
volume = {131},
pages = {108840},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108840},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003211},
author = {Guoqiang Liang and Yanbing Lv and Shucheng Li and Shizhou Zhang and Yanning Zhang},
keywords = {Video summarization, Generative adversarial network, Self attention},
abstract = {With the explosive growth of video data, video summarization, which attempts to seek the minimum subset of frames while still conveying the main story, has become one of the hottest topics. Nowadays, substantial achievements have been made by supervised learning techniques, especially after the emergence of deep learning. However, it is extremely expensive and difficult to construct a large-scale video summarization dataset through human annotation. To address this problem, we propose a convolutional attentive adversarial network (CAAN), whose key idea is to build a deep summarizer in an unsupervised way. Upon the generative adversarial network, our overall framework consists of a generator and a discriminator. The former predicts importance scores for all the frames of a video while the latter tries to distinguish the score-weighted frame features from original frame features. To capture the global and local temporal relationship of video frames, the generator employs a fully convolutional sequence network to build global representation of a video, and an attention-based network to predict normalized importance scores. To optimize the parameters, our objective function is composed of three loss functions, which can guide the frame-level importance score prediction collaboratively. To validate this proposed method, we have conducted extensive experiments on two public benchmarks SumMe and TVSum. The results show the superiority of our proposed method against other state-of-the-art unsupervised approaches. Our method even outperforms some published supervised approaches.}
}
@article{PANG2022108888,
title = {Fast algorithms for incremental and decremental semi-supervised discriminant analysis},
journal = {Pattern Recognition},
volume = {131},
pages = {108888},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108888},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003697},
author = {Wenrao Pang and Gang Wu},
keywords = {Dimensionality reduction, Semi-supervised discriminant analysis, Incremental learning, Decremental learning, Modified total scatter matrix},
abstract = {Incremental and decremental problems are challenging tasks in semi-supervised learning. The incremental semi-supervised discriminant analysis (ISSDA) method proposed by Dhamecha et al. is an efficient method for incremental semi-supervised learning. However, one deficiency of the ISSDA method is that the total scatter matrix remains unchanged during incremental learning, which is impractical in practice. On the other hand, there may be a series of incorrectly artificial labeling in the public data set, and it is interesting to consider the decremental problem in semi-supervised learning. To the best of our knowledge, however, there are few decremental algorithms for semi-supervised discriminant analysis. The contributions of this work are as follows. First, a new incremental semi-supervised discriminant analysis method is proposed, in which we consider updating the total scatter matrix and the between-class scatter matrix simultaneously when new samples are added. Second, we show how to solve the large eigenproblem of the updated total scatter matrix efficiently. Third, we propose two decremental algorithms for semi-supervised discriminant analysis. Numerical experiments demonstrate the superiority of the proposed algorithms over many state-of-the-art algorithms for semi-supervised discriminant analysis.}
}
@article{ZHAO2022108880,
title = {Self-guided information for few-shot classification},
journal = {Pattern Recognition},
volume = {131},
pages = {108880},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108880},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003612},
author = {Zhineng Zhao and Qifan Liu and Wenming Cao and Deliang Lian and Zhihai He},
keywords = {Few-shot classification, Graph convolution network, Self-guided information},
abstract = {Few-shot classification aims to identify novel categories using only a few labeled samples. Generally, the metric-based few-shot classification methods compare the feature embedding of Query samples (unlabeled samples) with Support samples (labeled samples) in a metric algorithm to predict which category the Query sample belongs to. Obtaining a good feature embedding for each sample in the feature extraction stage can improve the classification accuracy in the metric stage. Based on this, we design the Self-Guided Information Convolution (SGI-Conv), an improved convolution structure, which utilizes the high-level features to guide the network to extract the required discriminative features. To effectively utilize the feature embeddings of samples, we divide the metric network into multiple blocks and build a multi-layer graph convolutional network by sharing adjacent matrices. The multi-layer structure enhances the aggregation ability of graph convolution. Extensive experiments on multiple benchmark datasets demonstrate that our method has achieved competitive results on the few-shot classification tasks.}
}
@article{YAN2022108904,
title = {Effective full-scale detection for salient object based on condensing-and-filtering network},
journal = {Pattern Recognition},
volume = {131},
pages = {108904},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108904},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003855},
author = {Xinyu Yan and Meijun Sun and Yahong Han and Zheng Wang and Qi Tian},
keywords = {Salient object detection, Neural networks, Full-scale feature extraction, Multi-level feature fusion},
abstract = {With the development of deep learning, salient object detection methods have made great progress. However, there are still two challenges: 1) The lack of rich features extracted from multiple perspectives at different encoder levels results in the omission of salient objects with varying scales. 2) The ineffective fusion of multi-level features during decoding dilutes the saliency features, which destroys the purity of the predicted maps. In this paper, we design a Condensing-and-Filtering Network (CFNet), in which a saliency pyramid condensing module (SPCM) and a saliency filtering module (SFM) are proposed to solve the above two problems respectively. Specifically, SPCM introduces pyramid convolution as the basic unit to condense full-scale features from global and local perspectives at each level of the encoder. SFM is equipped with an ingenious ‘funnel’ structure to effectively filter multi-level features and supplement details, which makes the fusion of features more robust. The two modules complement each other, so that the full-scale features can be used effectively to predict salient objects. Extensive experimental results on five benchmark datasets demonstrate that our method performs favourably against the state-of-the-art approaches, and also shows superiority in terms of speed (16.18ms) and FLOPs (21.19G). Meanwhile, we extend our CFNet to the task of RGB-D salient object detection and achieve better results, which further demonstrate its effectiveness. The code will be made available.}
}
@article{TAN2022108839,
title = {Semi-supervised partial multi-label classification via consistency learning},
journal = {Pattern Recognition},
volume = {131},
pages = {108839},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108839},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200320X},
author = {Anhui Tan and Jiye Liang and Wei-Zhi Wu and Jia Zhang},
keywords = {Semi-supervised partial multi-label learning, Label correlation, HSIC},
abstract = {Partial multi-label learning refers to the problem that each instance is associated with a candidate label set involving both relevant and noisy labels. Existing solutions mainly focus on label disambiguation, while ignoring the negative effect of the inconsistency between feature information and label information. Specifically, the existence of completely unlabeled instances makes the estimation of label co-occurrence difficult. To tackle these problems, we propose a novel framework for partial multi-label learning in semi-supervised scenarios by solving the inconsistency between features and labels. In the first stage, the label-level correlation matrix on both labeled and unlabeled instances is derived via Hilbert-Schmidt Independence Criterion (HSIC). The correlation matrix can characterize the label correlation of labeled instances and can propagate the label correlation of unlabeled instances. In the second stage, the proposed framework achieves the training of feature mapping, the recovery of ground-truth labels, and the alleviation of noisy labels in a mutually beneficial manner, and develops an alternative optimization procedure to optimize them. In addition, a nonlinear version is extended by using kernel trick. Experimental studies demonstrate that the proposed methods can achieve competitive superiority against existing well-established methods.}
}
@article{WANG2022108892,
title = {Improving deep learning on point cloud by maximizing mutual information across layers},
journal = {Pattern Recognition},
volume = {131},
pages = {108892},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108892},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003739},
author = {Di Wang and Lulu Tang and Xu Wang and Luqing Luo and Zhi-Xin Yang},
keywords = {Deep learning, 3D vision, Point clouds, Mutual information},
abstract = {It is a fundamental and vital task to enhance the perception capability of the point cloud learning network in 3D machine vision applications. Most existing methods utilize feature fusion and geometric transformation to improve point cloud learning without paying enough attention to mining further intrinsic information across multiple network layers. Motivated to improve consistency between hierarchical features and strengthen the perception capability of the point cloud network, we propose exploring whether maximizing the mutual information (MI) across shallow and deep layers is beneficial to improve representation learning on point clouds. A novel design of Maximizing Mutual Information (MMI) Module is proposed, which assists the training process of the main network to capture discriminative features of the input point clouds. Specifically, the MMI-based loss function is employed to constrain the differences of semantic information in two hierarchical features extracted from the shallow and deep layers of the network. Extensive experiments show that our method is generally applicable to point cloud tasks, including classification, shape retrieval, indoor scene segmentation, 3D object detection, and completion, and illustrate the efficacy of our proposed method and its advantages over existing ones. Our source code is available at https://github.com/wendydidi/MMI.git.}
}
@article{GUO2022108928,
title = {Metric learning via perturbing hard-to-classify instances},
journal = {Pattern Recognition},
volume = {132},
pages = {108928},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108928},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004095},
author = {Xinyao Guo and Wei Wei and Jianqing Liang and Chuangyin Dang and Jiye Liang},
keywords = {Metric learning, Hard-to-classify instances, Instance perturbation, Alternating minimization},
abstract = {Constraint selection is an effective means to alleviate the problem of a massive amount of constraints in metric learning. However, it is difficult to find and deal with all association constraints with the same hard-to-classify instance (i.e., an instance surrounded by dissimilar instances), negatively affecting metric learning algorithms. To address this problem, we propose a new metric learning algorithm from the perspective of selecting instances, Metric Learning via Perturbing of Hard-to-classify Instances (ML-PHI), which directly perturbs the hard-to-classify instances to reduce over-fitting for the hard-to-classify instances. ML-PHI perturbs hard-to-classify instances to be closer to similar instances while keeping the positions of the remaining instances as constant as possible. As a result, the negative impacts of hard-to-classify instances are effectively reduced. We have conducted extensive experiments on real data sets, and the results show that ML-PHI is effective and outperforms state-of-the-art methods.}
}
@article{YANG2022108823,
title = {Asymmetric cross–modal hashing with high–level semantic similarity},
journal = {Pattern Recognition},
volume = {130},
pages = {108823},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108823},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003041},
author = {Fan Yang and Yufeng Liu and Xiaojian Ding and Fumin Ma and Jie Cao},
keywords = {Cross-modal retrieval, Hashing, Similarity search, Supervised, Optimization},
abstract = {Cross-modal hashing aims at using modality content to retrieve semantically relevant objects of different modalities, so cross-modal retrieval has attracted much attention. To effectively exploit the discriminative label information and retain more semantic information in the process of hash learning, we propose a novel cross-modal hashing method, named high-level semantic similarity analysis hashing (HSSAH) for cross-modal retrieval. To reduce time complexity and enhance discriminant ability in hash codes, HSSAH constructs an asymmetric high-level semantic similarity learning framework to replace the binary semantic similarity matrix. Moreover, the developed HSSAH is a two-stage approach, and a semantic-enhanced scheme is proposed in the second stage, which fully leverages the label information to gain more powerful hash functions. We conducted comprehensive experiments on three benchmark datasets to evaluate the performance of HSSAH. Experimental results show that HSSAH can achieve significantly better retrieval precision and outperforms several state-of-the-art approaches.}
}
@article{MARCHETTI2022108920,
title = {Local-to-Global Support Vector Machines (LGSVMs)},
journal = {Pattern Recognition},
volume = {132},
pages = {108920},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108920},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004010},
author = {F. Marchetti and E. Perracchione},
keywords = {Local-to-global support vector machines, Partition of unity, Supervised classification, Kernel models},
abstract = {For supervised classification tasks that involve a large number of instances, we propose and study a new efficient tool, namely the Local-to-Global Support Vector Machine (LGSVM) method. Its background somehow lies in the framework of approximation theory and of local kernel-based models, such as the Partition of Unity (PU) method. Indeed, even if the latter needs to be accurately tailored for classification tasks, such as allowing the use of the cosine semi-metric for defining the patches, the LGSVM is a global method constructed by gluing together the local SVM contributions via compactly supported weights. When the number of instances grows, such a construction of a global classifier enables us to significantly reduce the usually high complexity cost of SVMs. This claim is supported by a theoretical analysis of the LGSVM and of its complexity as well as by extensive numerical experiments carried out by considering benchmark datasets.}
}
@article{FUJITAKE2022108847,
title = {Temporal feature enhancement network with external memory for live-stream video object detection},
journal = {Pattern Recognition},
volume = {131},
pages = {108847},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108847},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003284},
author = {Masato Fujitake and Akihiro Sugimoto},
keywords = {Video object detection, Video analysis, Object detection},
abstract = {This paper proposes a method exploiting temporal context with an attention mechanism for detecting objects in real-time in a live streaming video. Video object detection is challenging and essential in practical applications such as robotics, smartphones, and surveillance cameras. Although methods have been proposed to improve the accuracy or run-time speed by exploiting temporal information, the trade-off between them tends to be ignored. We thus focus on the trade-off between accuracy and speed, and propose a method to improve the accuracy by aggregating the past information from a lightweight feature extractor with an attention mechanism. Evaluations on the UA-DETRAC and ImageNet VID datasets demonstrate our model’s superior performance to state-of-the-art methods on live streaming real-time object detection.}
}
@article{HUANG2022108818,
title = {Hippocampus-heuristic character recognition network for zero-shot learning in Chinese character recognition},
journal = {Pattern Recognition},
volume = {130},
pages = {108818},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108818},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002990},
author = {Guanjie Huang and Xiangyu Luo and Shaowei Wang and Tianlong Gu and Kaile Su},
keywords = {Chinese character recognition, Hippocampus thinking, Radical analysis, Zero-shot learning, Label embedding},
abstract = {The recognition of Chinese characters has always been a challenging task due to their huge variety and complex structures. The current radical-based methods fail to recognize Chinese characters without learning all of their radicals in the training stage. To this end, we propose a novel Hippocampus-heuristic Character Recognition Network (HCRN), which can recognize unseen Chinese characters only by training part of radicals. More specifically, the network architecture of HCRN is a new pseudo-siamese network designed by us, which can learn features from pairs of input samples and use them to predict unseen characters. The experimental results on the recognition of printed and handwritten characters show that HCRN is robust and effective on zero/few-shot learning tasks. For the printed characters, the mean accuracy of HCRN outperforms the state-of-the-art approach by 23.93% on recognizing unseen characters. For the handwritten characters, HCRN improves the mean accuracy by 11.25% on recognizing unseen characters.}
}
@article{TU2022108887,
title = {DFR-ST: Discriminative feature representation with spatio-temporal cues for vehicle re-identification},
journal = {Pattern Recognition},
volume = {131},
pages = {108887},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108887},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003685},
author = {Jingzheng Tu and Cailian Chen and Xiaolin Huang and Jianping He and Xinping Guan},
keywords = {Vehicle re-identification, Computer vision, Deep learning, Attention mechanism, Video surveillance},
abstract = {Vehicle re-identification (re-ID) aims to discover and match the target vehicles from a gallery image set taken by different cameras on a wide range of road networks. It is crucial for lots of applications such as security surveillance and traffic management. The remarkably similar appearances of distinct vehicles and the significant changes in viewpoints and illumination conditions pose grand challenges to vehicle re-ID. Conventional solutions focus on designing global visual appearances without sufficient consideration of vehicles’ spatio-temporal relationships in different images. This paper proposes a discriminative feature representation with spatio-temporal clues (DFR-ST) for vehicle re-ID. It is capable of building robust features in the embedding space by involving appearance and spatio-temporal information. The proposed DFR-ST constructs an appearance model for a multi-grained visual representation by a two-stream architecture and a spatio-temporal metric to provide complementary information based on this multi-modal information. Experimental results on four public datasets demonstrate DFR-ST outperforms the state-of-the-art methods, which validates the effectiveness of the proposed method.}
}
@article{MEI2022108835,
title = {Spatial feature mapping for 6DoF object pose estimation},
journal = {Pattern Recognition},
volume = {131},
pages = {108835},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108835},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003168},
author = {Jianhan Mei and Xudong Jiang and Henghui Ding},
keywords = {6D Pose estimation, Rotation symmetry, Spherical convolution, Graph convolutional network},
abstract = {This work aims to estimate 6Dof (6D) object pose in background clutter. Considering the strong occlusion and background noise, we propose to utilize the spatial structure for better tackling this challenging task. Observing that the 3D mesh can be naturally abstracted by a graph, we build the graph using 3D points as vertices and mesh connections as edges. We construct the corresponding mapping from 2D image features to 3D points for filling the graph and fusion of the 2D and 3D features. Afterward, a Graph Convolutional Network (GCN) is applied to help the feature exchange among objects’ points in 3D space. To address the problem of rotation symmetry ambiguity for objects, a spherical convolution is utilized and the spherical features are combined with the convolutional features that are mapped to the graph. Predefined 3D keypoints are voted and the 6DoF pose is obtained via the fitting optimization. Two scenarios of inference, one with the depth information and the other without it are discussed. Tested on the datasets of YCB-Video and LINEMOD, the experiments demonstrate the effectiveness of our proposed method.}
}
@article{YANG2022108863,
title = {Multi-View correlation distillation for incremental object detection},
journal = {Pattern Recognition},
volume = {131},
pages = {108863},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108863},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003442},
author = {Dongbao Yang and Yu Zhou and Aoting Zhang and Xurui Sun and Dayan Wu and Weiping Wang and Qixiang Ye},
keywords = {Object detection, Incremental learning, Catastrophic forgetting, Knowledge distillation},
abstract = {In real applications, new object classes often emerge after the detection model has been trained on a prepared dataset with fixed classes. Fine-tuning the old model with only new data will lead to a well-known phenomenon of catastrophic forgetting, which severely degrades the performance of modern object detectors. Due to the storage burden, data privacy and time consumption, sometimes it is impractical to train the model from scratch with all data of both old and new classes. In this paper, we propose a novel Multi-View Correlation Distillation (MVCD) based incremental object detection method, which explores the intra-feature correlations in the feature space of the object detector. To better transfer the knowledge learned from the old classes and maintain the ability to learn new classes, we select the sample-specific discriminative features from channel-wise, point-wise and instance-wise views. Meanwhile, the correlation distillation losses on the selective features are designed to regularize the learning of the incremental object detector. A new metric named Stability-Plasticity-mAP (SPmAP) is proposed to evaluate the incremental learning performance as a complementary metric to mAP, which integrates the metrics for the stability on old classes and the plasticity on new classes in incremental object detection. The extensive experiments conducted on VOC2007 and COCO demonstrate that MVCD achieves a better trade-off between stability and plasticity than state-of-the-art first-order distillation-based incremental object detection methods.}
}
@article{PATIL2022108822,
title = {Dual-frame spatio-temporal feature modulation for video enhancement},
journal = {Pattern Recognition},
volume = {130},
pages = {108822},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108822},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200303X},
author = {Prashant W. Patil and Sunil Gupta and Santu Rana and Svetha Venkatesh},
keywords = {Multi-frame features, Spatio-temporal feature modulation, Recurrent feature sharing, Multi-weather video enhancement},
abstract = {Current video enhancement approaches have achieved good performance in specific rainy, hazy, foggy, and snowy weather conditions. However, they currently suffer from two important limitations. First, they can only handle degradation caused by single weather. Second, they use large, complex models with 10–50 millions of parameters needing high computing resources. As video enhancement is a pre-processing step for applications like video surveillance, traffic monitoring, autonomous driving, etc., it is necessary to have a lightweight enhancement module. Therefore, we propose a dual-frame spatio-temporal feature modulation architecture to handle the degradation caused by diverse weather conditions. The proposed architecture combines the concept of spatio-temporal multi-resolution feature modulation with a multi-receptive parallel encoders and domain-based feature filtering modules to learn domain-specific features. Further, the architecture provides temporal consistency with recurrent feature merging, achieved by providing feedback of the previous frame output. The indoor (REVIDE, NYUDepth), synthetically generated outdoor weather degraded video de-hazing, and de-raining with veiling effect databases are used for experimentation. Also, the performance of the proposed method is analyzed for night-time de-hazing and de-raining with veiling effect weather conditions. Experimental results show the superior performance of our framework compared to existing state-of-the-art methods used for video de-hazing (indoor/outdoor) and de-raining with veiling effect weather conditions. The code is available at https://github.com/pwp1208/PR2022}
}
@article{WANG2022108903,
title = {Multiple geometry representations for 6D object pose estimation in occluded or truncated scenes},
journal = {Pattern Recognition},
volume = {132},
pages = {108903},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108903},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003843},
author = {Jichun Wang and Lemiao Qiu and Guodong Yi and Shuyou Zhang and Yang Wang},
keywords = {Neural network, Pose estimation, Keypoints, Edge vectors, Symmetry correspondences},
abstract = {Deep learning-based 6D object pose estimation methods from a single RGBD image have recently received increasing attention because of their powerful representation learning capabilities. These methods, however, cannot handle severe occlusion and truncation. In this paper, we present a novel 6D object pose estimation method based on multiple geometry representations. Specifically, we introduce a network to fuse the appearance and geometry features extracted from input color and depth images. Then, we utilize these per-point fusion features to estimate keypoint offsets, edge vectors, and dense symmetry correspondences in the canonical coordinate system. Finally, a two-stage pose regression module is applied to compute the 6D pose of an object. Relative to the unitary 3D keypoint-based strategy, such combination of multiple geometry representations provides sufficient and diverse information, especially for occluded or truncated scenes. To show the robustness to occlusion and truncation of the proposed method, we conduct comparative experiments on the Occlusion LineMOD, Truncation LineMOD, and T-LESS datasets. Results reveal that the proposed method outperforms state-of-the-art techniques by a large margin.}
}
@article{SHI2022108879,
title = {Weighting and pruning based ensemble deep random vector functional link network for tabular data classification},
journal = {Pattern Recognition},
volume = {132},
pages = {108879},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108879},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003600},
author = {Qiushi Shi and Minghui Hu and Ponnuthurai Nagaratnam Suganthan and Rakesh Katuwal},
keywords = {Ensemble deep random vector functional link (edRVFL), Weighting methods, Pruning, UCI classification datasets},
abstract = {In this paper, we first integrate normalization to the Ensemble Deep Random Vector Functional Link network (edRVFL). This re-normalization step can help the network avoid divergence of the hidden features. Then, we propose novel variants of the edRVFL network. Weighted edRVFL (WedRVFL) uses weighting methods to give training samples different weights in different layers according to how the samples were classified confidently in the previous layer thereby increasing the ensemble’s diversity and accuracy. Furthermore, a pruning-based edRVFL (PedRVFL) has also been proposed. We prune some inferior neurons based on their importance for classification before generating the next hidden layer. Through this method, we ensure that the randomly generated inferior features will not propagate to deeper layers. Subsequently, the combination of weighting and pruning, called Weighting and Pruning based Ensemble Deep Random Vector Functional Link Network (WPedRVFL), is proposed. We compare their performances with other state-of-the-art classification methods on 24 tabular UCI classification datasets. The experimental results illustrate the superior performance of our proposed methods.}
}
@article{LI2022108875,
title = {Clustering experience replay for the effective exploitation in reinforcement learning},
journal = {Pattern Recognition},
volume = {131},
pages = {108875},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108875},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003569},
author = {Min Li and Tianyi Huang and William Zhu},
keywords = {Reinforcement learning, Clustering, Experience replay, Exploitation efficiency, Time division},
abstract = {Reinforcement learning is a useful tool for training an agent to effectively achieve the desired goal in the sequential decision-making problem. It trains the agent to make decision by exploiting the experience in the transitions resulting from the different decisions. To exploit this experience, most reinforcement learning methods replay the explored transitions by uniform sampling. But in this way, it is easy to ignore the last explored transitions. Another way to exploit this experience defines the priority of each transition by the estimation error in training and then replays the transitions according to their priorities. But it only updates the priorities of the transitions replayed at the current training time step, thus the transitions with low priorities will be ignored. In this paper, we propose a clustering experience replay, called CER, to effectively exploit the experience hidden in all explored transitions in the current training. CER clusters and replays the transitions by a divide-and-conquer framework based on time division as follows. Firstly, it divides the whole training process into several periods. Secondly, at the end of each period, it uses k-means to cluster the transitions explored in this period. Finally, it constructs a conditional probability density function to ensure that all kinds of transitions will be sufficiently replayed in the current training. We construct a new method, TD3_CER, to implement our clustering experience replay on TD3. Through the theoretical analysis and experiments, we illustrate that our TD3_CER is more effective than the existing reinforcement learning methods. The source code can be downloaded from https://github.com/grcai/CER-Master.}
}
@article{GAJAMANNAGE2022108891,
title = {Reconstruction of fragmented trajectories of collective motion using Hadamard deep autoencoders},
journal = {Pattern Recognition},
volume = {131},
pages = {108891},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108891},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003727},
author = {Kelum Gajamannage and Yonggi Park and Randy Paffenroth and Anura P. Jayasumana},
keywords = {Multi-object tracking, Collective motion, Deep autoencoders, Hadamard product, Self-propelled particles},
abstract = {Learning dynamics of collectively moving agents such as fish or humans is an essential task in research. Due to phenomena such as occlusion or change of illumination, the multi-object methods tracking such dynamics may lose the tracks of the agents which may result in fragmentations of trajectories. Here, we present an extended deep autoencoder (DA) that we train only on the fully observed segments of the trajectories by defining its loss function as the Hadamard product of a binary indicator matrix with the absolute difference between the outputs and the labels. The trajectory matrix of the agents practicing collective motion is low-rank due to mutual interactions and dependencies between the agents that we utilize as the underlying pattern that our Hadamard deep autoencoder (HDA) codes during its training. The performance of this HDA is compared with that of a low-rank matrix completion scheme in the context of fragmented trajectory reconstruction.}
}
@article{FUCHS2022108846,
title = {A novel way to formalize stable graph cores by using matching-graphs},
journal = {Pattern Recognition},
volume = {131},
pages = {108846},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108846},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003272},
author = {Mathias Fuchs and Kaspar Riesen},
keywords = {Graph matching, Matching-graphs, Graph edit distance, Structural pattern recognition},
abstract = {The increasing amount of data available and the rate at which it is collected leads to rapid developments of systems for intelligent information processing and pattern recognition. Often the underlying data is inherently complex, making it difficult to represent it by linear, vectorial data structures. This is where graphs offer a versatile alternative for formal data representation. Actually, quite an amount of graph-based methods for pattern recognition has been proposed. A considerable part of these methods rely on graph matching. In the present paper, we propose a novel encoding of specific graph matching information. The basic idea is to formalize the stable cores of individual classes of graphs – discovered during intra-class matchings – by means of so called matching-graphs. We evaluate the benefit of these matching-graphs by researching two classification approaches that rely on this novel data structure. The first approach is a distance based classifier focusing on the matching-graphs during dissimilarity computation. For the second approach, we propose to use sets of matching-graphs to embed input graphs into a vector space. The basic idea is to produce hundreds of matching-graphs first, and then represent each graph g as a vector that shows the occurrence of, or the distance to, each matching-graph. In a thorough experimental evaluation on seven real world data sets we empirically confirm that our novel approaches are able to improve the classification accuracy of systems that rely on comparable information as well as state-of-the-art methods.}
}
@article{CHEN2022108862,
title = {Online Adaptive Kernel Learning with Random Features for Large-scale Nonlinear Classification},
journal = {Pattern Recognition},
volume = {131},
pages = {108862},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108862},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003430},
author = {Yingying Chen and Xiaowei Yang},
keywords = {Large-scale, Nonlinear classification, Online learning, Random feature map},
abstract = {In the field of support vector machines, online random feature map algorithms are very important methods for large-scale nonlinear classification problems. At present, the existing methods have the following shortcomings: (1) If only the hyperplane vector is updated during learning while the random feature components are fixed, there is no guarantee that these online methods can adapt to the change of data distribution shape when the data is coming one by one. (2) When the kernel is selected improperly, the samples mapped to an inappropriate space may not be well classified. In order to overcome these shortcomings, considering the fact that iteratively updating random feature components can make data better fit in the current space and lead to the flexible adjustment of the kernel function, random features based online adaptive kernel learning (RF-OAK) is proposed for large-scale nonlinear classification problems. Theoretical analysis of the proposed algorithm is also provided. The experimental results and the Wilcoxon signed-ranks test show that in terms of test accuracy, the proposed method is significantly better than the state-of-the-art online feature mapping classification methods. Compared with the deep learning algorithms, the training time of RF-OAK is shorter. In terms of test accuracy, RF-OAK is better than online algorithm and comparable with offline algorithms.}
}
@article{SONG2022108858,
title = {Multi-feature deep information bottleneck network for breast cancer classification in contrast enhanced spectral mammography},
journal = {Pattern Recognition},
volume = {131},
pages = {108858},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108858},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003399},
author = {Jingqi Song and Yuanjie Zheng and Jing Wang and Muhammad Zakir Ullah and Xuecheng Li and Zhenxing Zou and Guocheng Ding},
keywords = {Contrast enhanced spectral mammography, Classification, Deep learning, Multi-feature, Information bottleneck},
abstract = {There is considerable variation in the size, shape and location of tumours, which makes it challenging for radiologists to diagnose breast cancer. Automated diagnosis of breast cancer from Contrast Enhanced Spectral Mammography (CESM) can support clinical decision making. However, existing methods fail to obtain an effective representation of the CESM and ignore the relationships between images. In this paper, we investigated for the first time a novel and flexible multimodal representation learning method, multi-feature deep information bottleneck (MDIB), for breast cancer classification in CESM. Specifically, the method incorporated an information bottleneck (IB)-based module to learn the prominent representation that provide concise input while informative for the classification. In addition, we creatively extended IB theory to multi-feature IB, which facilitates the learning of relevant features for classification between CESM images. To validate our method, experiments were conducted on our private and public datasets. The classification results of our method were also compared with those of state-of-the-art methods. The experiment results proved the effectiveness and the efficiency of the proposed method. We release our code at https://github.com/sjq5263/MDIB-for-CESM-classification.}
}
@article{DONG2022108886,
title = {Deep rank hashing network for cancellable face identification},
journal = {Pattern Recognition},
volume = {131},
pages = {108886},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108886},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003673},
author = {Xingbo Dong and Sangrae Cho and Youngsam Kim and Soohyung Kim and Andrew Beng Jin Teoh},
keywords = {Cancellable biometrics, Deep learning, Face biometrics, Hashing, Identification},
abstract = {Cancellable biometrics (CB) is one of the major approaches for biometric template protection. However, almost all the prior arts are designed to work under verification (one-to-one matching). This paper proposes a deep learning-based cancellable biometric scheme for face identification (one-to-many matching). Our scheme comprises two key ingredients: a deep rank hashing (DRH) network and a cancellable identification scheme. The DRH network transforms a raw face image into discriminative yet compact face hash codes based upon the nonlinear subspace ranking notion. The network is designed to be trained for both identification and hashing goals with their respective rich identity-related and rank hashing relevant loss functions. A modified softmax function is utilized to alleviate the hashing quantization error, and a regularization term is designed to encourage hash code balance. The hash code is binarized, compressed, and secured with the randomized lookup table function. Unlike prior CB schemes that require two input factors for verification, the proposed scheme demands no additional input except face images during identification, yet the face template is replaceable whenever needed based upon a one-time XOR cipher notion. The proposed scheme is evaluated on five public unconstrained face datasets in terms of verification, closed-set and open-set identification performance accuracy, computation cost, template protection criteria, and security.}
}
@article{BAI2022108834,
title = {Practical protection against video data leakage via universal adversarial head},
journal = {Pattern Recognition},
volume = {131},
pages = {108834},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108834},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003156},
author = {Jiawang Bai and Bin Chen and Kuofeng Gao and Xuan Wang and Shu-Tao Xia},
keywords = {Privacy protection, Video retrieval, Deep hashing, Adversarial attack},
abstract = {While online video sharing becomes more popular, it also causes unconscious leakage of personal information in the video retrieval systems like deep hashing. A snoop can collect more users’ private information from the video database by querying similar videos. This paper focuses on bypassing the deep video hashing based retrieval to prevent information from being maliciously collected. We propose universal adversarial head (UAH), which crafts adversarial query videos by prepending the original videos with a sequence of adversarial frames to perturb the normal hash codes in the Hamming space. This adversarial head can be generated only with a few natural videos, and mislead the retrieval system to return irrelevant videos when it is applied to most query videos. Furthermore, to obey the principle of information protection, we expand the proposed method to a data-free paradigm to generate the UAH, without access to users’ original videos. Extensive experiments demonstrate the effectiveness of our method in misleading deep video hashing under both white-box and black-box settings.}
}
@article{NASIRI2022108805,
title = {Multiple-solutions RANSAC for finding axes of symmetry in fragments of objects},
journal = {Pattern Recognition},
volume = {131},
pages = {108805},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108805},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002862},
author = {Seyed-Mahdi Nasiri and Reshad Hosseini and Hadi Moradi},
keywords = {Symmetry axis, Multiple-solutions RANSAC, 3D Reconstruction},
abstract = {The problem of “finding best lines passing through a set of straight lines” has appeared in applications such as archaeological pottery analysis, precision manufacturing, and 3D modelling. In these applications, an instance of this problem is finding the symmetry axis of a symmetrical object from a set of its surface normal lines. We show that the mentioned instance of the problem may have two meaningful local minima, one of which is the symmetry axis, a fact that has been neglected in the literature. A multiple-solutions RANSAC algorithm is proposed for finding initial estimates of both local minima in the presence of outliers. Then, a coordinate-descent algorithm is presented that starts from these initial estimates and finds the local minima of the problem. The proposed coordinate-descent method does not involve any line search procedure, and its convergence is guaranteed. We also provide a proof for the rate of the convergence.}
}
@article{YANG2022108874,
title = {Learning deep feature correspondence for unsupervised anomaly detection and segmentation},
journal = {Pattern Recognition},
volume = {132},
pages = {108874},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108874},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003557},
author = {Jie Yang and Yong Shi and Zhiquan Qi},
keywords = {Anomaly detection, Anomaly segmentation, Feature correspondence, Dual network},
abstract = {Developing machine learning models that can detect and localize the unexpected or anomalous structures within images is very important for numerous computer vision tasks, such as the defect inspection of manufactured products. However, it is challenging especially when there are few or even no anomalous image samples available. In this paper, we propose an unsupervised mechanism, i.e. deep feature correspondence (DFC), which can be effectively leveraged to detect and segment out the anomalies in images solely with the prior knowledge from anomaly-free samples. We develop our DFC in an asymmetric dual network framework that consists of a generic feature extraction network and an elaborated feature estimation network, and detect the possible anomalies within images by modeling and evaluating the associated deep feature correspondence between the two dual network branches. Furthermore, to improve the robustness of the DFC and further boost the detection performance, we specifically propose a self-feature enhancement (SFE) strategy and a multi-context residual learning (MCRL) network module. Extensive experiments have been carried out to validate the effectiveness of our DFC and the proposed SFE and MCRL. Our approach is very effective for detecting and segmenting the anomalies that appear in confined local regions of images, especially the industrial anomalies. It advances the state-of-the-art performances on the benchmark dataset – MVTec AD. Besides, when applied to a real industrial inspection scene, it outperforms the comparatives by a large margin.}
}
@article{YUAN2022108902,
title = {Cubic-cross convolutional attention and count prior embedding for smoke segmentation},
journal = {Pattern Recognition},
volume = {131},
pages = {108902},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108902},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003831},
author = {Feiniu Yuan and Zeshu Dong and Lin Zhang and Xue Xia and Jinting Shi},
keywords = {Smoke segmentation, Information embedding, Cubic-cross convolutional attention, Count prior attention},
abstract = {It is very challenging to accurately segment smoke images because smoke has some adverse properties, such as semi-transparency and blurry boundary. Aiming at solving these problems, we first fuse convolutional results along different axes to equivalently produce a cubic-cross convolutional kernel, which enlarges receptive fields at affordable computational costs for capturing long-range dependency of smoke pixels, and then we propose a Cubic-cross Convolutional Attention (CCA). To embed global category information, we propose a count prior structure to model and supervise the count of smoke pixels. To ensure the network can correctly extract a count prior map, we impose a regression loss on the count prior map and corresponding ideal count map directly calculated from its ground truth. Then we multiply the reshaped input by the count prior map to produce a Count Prior Attention (CPA) map, which is upsampled to generate the final output. A cross entropy loss is used to supervise the final segmentation. Finally, we use ResNet50 for feature encoding, and stack CCA and CPA together to propose a Cubic-cross convolutional attention and Count prior Embedding Network (CCENet) for smoke segmentation. Experiments on both synthetic and real smoke datasets show that our method outperforms existing state-of-the-art methods.}
}
@article{DADSETAN2022108919,
title = {Deep learning of longitudinal mammogram examinations for breast cancer risk prediction},
journal = {Pattern Recognition},
volume = {132},
pages = {108919},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108919},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004009},
author = {Saba Dadsetan and Dooman Arefan and Wendie A. Berg and Margarita L. Zuley and Jules H. Sumkin and Shandong Wu},
keywords = {Breast cancer, Risk prediction, Deep learning, Digital mammogram, Longitudinal data},
abstract = {Information in digital mammogram images has been shown to be associated with the risk of developing breast cancer. Longitudinal breast cancer screening mammogram examinations may carry spatiotemporal information that can enhance breast cancer risk prediction. No deep learning models have been designed to capture such spatiotemporal information over multiple examinations to predict the risk. In this study, we propose a novel deep learning structure, LRP-NET, to capture the spatiotemporal changes of breast tissue over multiple negative/benign screening mammogram examinations to predict near-term breast cancer risk in a case-control setting. Specifically, LRP-NET is designed based on clinical knowledge to capture the imaging changes of bilateral breast tissue over four sequential mammogram examinations. We evaluate our proposed model with two ablation studies and compare it to three models/settings, including 1) a “loose” model without explicitly capturing the spatiotemporal changes over longitudinal examinations, 2) LRP-NET but using a varying number (i.e., 1 and 3) of sequential examinations, and 3) a previous model that uses only a single mammogram examination. On a case-control cohort of 200 patients, each with four examinations, our experiments on a total of 3200 images show that the LRP-NET model outperforms the compared models/settings.}
}
@article{PENG2022108890,
title = {H-ProMed: Ultrasound image segmentation based on the evolutionary neural network and an improved principal curve},
journal = {Pattern Recognition},
volume = {131},
pages = {108890},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108890},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003715},
author = {Tao Peng and Jing Zhao and Yidong Gu and Caishan Wang and Yiyun Wu and Xiuxiu Cheng and Jing Cai},
keywords = {Accurate prostate segmentation, Transrectal ultrasound, Principal curve, Optimized closed polygonal segment method, Evolutionary neural network, Interpretable mathematical model},
abstract = {The purpose of this work is to develop a method for accurate and robust prostate segmentation in transrectal ultrasound (TRUS) images. These images are difficult to segment due to missing/ambiguous boundary between the prostate and neighboring structures, the presence of shadow artifacts, as well as the large variability in prostate shapes. This paper develops a novel hybrid method for TRUS prostate segmentation by combining an improved principal curve-based method with an evolutionary neural network; the former for achieving the data sequences while and the latter for improving the smoothness of the prostate contour. Both qualitative and quantitative experimental results showed that our proposed method achieved superior segmentation accuracy and robustness as compared to state-of-the-art methods. The average Dice similarity coefficient (DSC), Jaccard similarity coefficient (Ω), and accuracy (ACC) of prostate contours against ground-truths were 96.8%, 95.7%, and 96.4%, and the DSC of around 92% and 95% for other deep learning and hybrid methods, respectively.}
}
@article{HUANG2022108817,
title = {Efficient federated multi-view learning},
journal = {Pattern Recognition},
volume = {131},
pages = {108817},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108817},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002989},
author = {Shudong Huang and Wei Shi and Zenglin Xu and Ivor W. Tsang and Jiancheng Lv},
keywords = {Federated learning, Multi-view learning, Matrix factorization, Clustering},
abstract = {Multi-view learning aims to explore a global common structure shared by different views collected from multiple individual sources. The nascent field of federated learning tries to learn a global model over distributed networks of devices. This paper shows that multi-view learning is naturally suited to address the feature heterogeneity of the federated setting. We propose a novel model, namely robust federated multi-view learning (FedMVL), which is considered in the following formulation: given a dataset with M views, it is required to train machine learning models while the M views are distributed across M devices or nodes. Considering the unique challenges like stragglers and fault tolerance in federated setting, we derive an iterative federated optimization algorithm that allows each node with the flexibility to approximately address its subproblem. To the best of our knowledge, our model for the first time considers the issues including high communication cost, fault tolerance, and stragglers for distributed multi-view learning. The proposed model also achieves encouraging performance on clustering task compared to closely related methods, as we illustrate through simulations on several real-world datasets.}
}
@article{LI2022108914,
title = {Weighted 3D volume reconstruction from series of slice data using a modified Allen–Cahn equation},
journal = {Pattern Recognition},
volume = {132},
pages = {108914},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108914},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003958},
author = {Yibao Li and Xin Song and Soobin Kwak and Junseok Kim},
keywords = {Shape transformation, 3D volume reconstruction, Allen–Cahn equation},
abstract = {In this study, we develop a fast and accurate computational method for a weighted three-dimensional (3D) volume reconstruction from a series of slice data using a phase-field model. The proposed method is based on a modified Allen–Cahn (AC) equation with a fidelity term. The algorithm automatically generates the necessary slices between the given slices by solving the governing equation. To reconstruct a 3D volume, we first set a source slice and target slice. Next, we set the source slice as the initial condition and the target slice as the fidelity function. Finally, we retain the numerical solutions during an evolution as intermediate slices between the source and target slices. There are two criteria for choosing the intermediate slice: One is based on the area of the symmetric difference between the phase-field solution and the target and the other is based on the change of the phase-field solution relative to the area of the target. We use the weighted average of the two criteria. To validate the efficiency and accuracy of the proposed numerical algorithm, several computational experiments are conducted. Computational test results confirm the superior performance of the proposed algorithm.}
}