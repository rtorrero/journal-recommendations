@article{ZHANG2024110111,
title = {Vital information is only worth one thumbnail: Towards efficient human pose estimation},
journal = {Pattern Recognition},
volume = {147},
pages = {110111},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110111},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008087},
author = {Zian Zhang and Yongqiang Zhang and Yin Zhang and Rui Tian and Mingli Ding},
keywords = {Human pose estimation, Small-size input, Knowledge distillation, Network compression and acceleration},
abstract = {In pursuit of impressive performance, existing DCNN-based approaches of human pose estimation usually use massive networks and large-size images to train a deep model. When applying these deep based methods in real-time systems, current works try to compress the deep network by reducing the number of layers and channels, but such approaches are complex and poorly generalized since they require elaborate design of small-scale network structures. Based on the fact that large-size images contain redundant information, in this paper, we explore the influence of image-size on system complexity and propose a novel framework called ThumbPose to accelerate and compress deep models by inferring on thumbnail representations in the task of human pose estimation. In our framework, we first propose a style supervised online downscaler to reduce an input image into a thumbnail image. Furthermore, a training strategy of dual-branch auto-encoding is designed to obtain effective and accurate thumbnail representation in a knowledge distillation manner, which is further used to maintain the performance of thumbnail images as the original-size input images. For heat-map based human pose estimation, ThumbPose is an orthogonal and implementation-friendly method, that can not only compress and accelerate the inference network but also obtain an image downscaler in a supervised manner that can be used in other high-level tasks (e.g. detection, segmentation, etc. in practical applications). Extensive experiments on MS COCO dataset demonstrate the effectiveness of our proposed method, and ThumbPose achieves superior performance (＋ 1.3% AP and ＋ 0.7% AR) with negligible additional cost (<0.2 GFLOPs) compared to previous state-of-the-art methods when using small-size images as inputs. Moreover, experiments on MPII show that our model achieves higher accuracy (＋ 0.2% Mean@0.5) with minimal computation (2.5 GFLOPs) compared to superior lightweight models obtained by the network compression methods.}
}
@article{ALAMINOS2024110139,
title = {Hybrid ARMA-GARCH-Neural Networks for intraday strategy exploration in high-frequency trading},
journal = {Pattern Recognition},
volume = {148},
pages = {110139},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110139},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008361},
author = {David Alaminos and M. Belén Salas and Antonio Partal-Ureña},
keywords = {High-frequency, Intraday trading, Defence stock prices, FOREX markets, Neural networks, Autoregressive moving average, Generalized autoregressive conditional heteroskedasticity, Quantum computing},
abstract = {The frequency of armed conflicts increased during the last 20 years. The problems of the emergence of military disputes, not only concern social parameters, but also economic and financial dimensions. This study examines the potential impact of global geopolitical events on the stock market prices of the Dow Jones U.S. Aerospace & Defense Index and Foreign Exchange (FOREX) markets movements. We analyse whether defence stocks and exchange rate perform similarly during military incidents or geopolitical crises. We built an Autoregressive Moving Average Model with a Generalized Autoregressive Conditional Heteroskedasticity process (ARMA-GARCH) with the machine learning methods of Neural Networks, Deep Recurrent Convolutional Neural Networks, Deep Neural Decision Trees, Quantum Neural Networks, and Quantum Recurrent Neural Networks, aimed at detecting intraday patterns for forecasting defence stock market and FOREX markets disturbances in a market microstructure framework. The empirical results provide preliminary findings on the foreseeability of market disturbances and small differences are observed before and during geopolitical events. Additionally, we confirm the effectiveness of the hybrid model ARMA-GARCH with the machine learning approaches, being ARMA-GARCH-Quantum Recurrent Neural Network the technique that achieves the best accuracy results. Our work has a large potential impact on investment market agents and portfolio managers, as shocks from geopolitical events could provide a new methodology to support the decision-making process for trading in High-Frequency Trading.}
}
@article{HAJIVEISEH2024110179,
title = {Deep asymmetric nonnegative matrix factorization for graph clustering},
journal = {Pattern Recognition},
volume = {148},
pages = {110179},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110179},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008762},
author = {Akram Hajiveiseh and Seyed Amjad Seyedi and Fardin {Akhlaghian Tab}},
keywords = {Nonnegative matrix factorization, Deep learning, Graph clustering, Directed graph},
abstract = {Graph clustering is a fundamental technique in machine learning that has widespread applications in various fields. Deep Nonnegative Matrix Factorization (DNMF) was recently emerged to cope with the extraction of several layers of features, and it has been demonstrated to achieve remarkable results on unsupervised tasks. While DNMF has been applied for analyzing graphs, the effectiveness of the current DNMF approaches for graph clustering is generally unsatisfactory: these methods are intrinsically data representation models, and their objective functions do not capture cluster structures, also ignores direction which is crucial in the directed graph clustering problems. To overcome these downsides, this paper proposes a graph-specific DNMF model based on the Asymmetric NMF which can handle undirected and directed graphs. Inspired by hierarchical graph clustering and graph summarization approaches, the Deep Asymmetric Nonnegative Matrix Factorization (DAsNMF) is introduced for the directed graph clustering problem. In a pseudo-hierarchical clustering setting, DAsNMF decomposes the input graph to extract low-level to high-level node representations and graph representations (summarized graphs). In addition, the asymmetric cosine and PageRank-based similarities are imposed on the proposed model to preserve the local and global graph structures. The learning process is formulated as a unified optimization problem to jointly train representation learning model and clustering model. The extensive experimental studies validate the effectiveness of the proposed method on directed graphs.}
}
@article{LIU2024110127,
title = {A lightweight unsupervised adversarial detector based on autoencoder and isolation forest},
journal = {Pattern Recognition},
volume = {147},
pages = {110127},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110127},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008245},
author = {Hui Liu and Bo Zhao and Jiabao Guo and Kehuan Zhang and Peng Liu},
keywords = {Deep neural networks, Adversarial examples, Adversarial detection, Autoencoder, Isolation forest},
abstract = {Although deep neural networks (DNNs) have performed well on many perceptual tasks, they are vulnerable to adversarial examples that are generated by adding slight but maliciously crafted perturbations to benign images. Adversarial detection is an important technique for identifying adversarial examples before they are entered into target DNNs. Previous studies that were performed to detect adversarial examples either targeted specific attacks or required expensive computation. Designing a lightweight unsupervised detector is still a challenging problem. In this paper, we propose an AutoEncoder-based Adversarial Examples (AEAE) detector that can guard DNN models by detecting adversarial examples with low computation in an unsupervised manner. The AEAE includes only a shallow autoencoder that performs two roles. First, a well-trained autoencoder has learned the manifold of benign examples. This autoencoder can produce a large reconstruction error for adversarial images with large perturbations, so we can detect significantly perturbed adversarial examples based on the reconstruction error. Second, the autoencoder can filter out small noises and change the DNN’s prediction on adversarial examples with small perturbations. It helps to detect slightly perturbed adversarial examples based on the prediction distance. To cover these two cases, we utilize the reconstruction error and prediction distance from benign images to construct a two-tuple feature set and train an adversarial detector using the isolation forest algorithm. We show empirically that AEAE is an unsupervised and inexpensive detector against most state-of-the-art attacks. Through the detection in these two cases, there is nowhere to hide adversarial examples.}
}
@article{YANG2024110082,
title = {Geometric-inspired graph-based Incomplete Multi-view Clustering},
journal = {Pattern Recognition},
volume = {147},
pages = {110082},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110082},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007793},
author = {Zequn Yang and Han Zhang and Yake Wei and Zheng Wang and Feiping Nie and Di Hu},
keywords = {Multi-view clustering, Incomplete view, Weight aggregation, Geometric analysis, Graph-based clustering},
abstract = {Multi-view clustering methods group data into different clusters by discovering the consensus in heterogeneous sources, which however becomes difficult when partial views of real-world data are missing. Consequently, reducing the impact of missing views and leveraging available views are the key concerns for the Incomplete Multi-view Clustering (IMvC) problem. In this research, we take an innovative, geometry-based perspective to investigate the IMvC problem under a commonly-used weight aggregation framework. We conduct a geometric analysis to understand how missing views shift the aggregation solution from the one achieved with full views, subsequently impacting the clustering result. Drawing from our analysis, we introduce a weight reallocation approach that minimizes the shift and approximates the full-view solution by reallocating the factual weight of each available view. Furthermore, we address the IMvC problem by using our reallocation method on a graph aggregation algorithm to obtain reliable clusters. Our extensive experiments demonstrate that our proposed approach outperforms previous IMvC methods, reporting superior results on four datasets with three metrics. Especially, on the Caltech101-7 dataset with 40 percent missing data, our method achieves an accuracy of 0.686, which significantly outperforms the results of other comparison methods that are no larger than 0.662. Further, our method can be used as a flexible plugin to improve other weight aggregation algorithms. The source code of this work is publicly available at https://github.com/bjlfzs/Geometric-Inspired-Graph-based-Incomplete-Multi-view-Clustering.}
}
@article{WANG2024110183,
title = {Unsupervised feature selection by learning exponential weights},
journal = {Pattern Recognition},
volume = {148},
pages = {110183},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110183},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008804},
author = {Chenchen Wang and Jun Wang and Zhichen Gu and Jin-Mao Wei and Jian Liu},
keywords = {Unsupervised feature selection, Sparse regression, Local structure learning, Global information preservation},
abstract = {Unsupervised feature selection has gained considerable attention for extracting valuable features from unlabeled datasets. Existing approaches typically rely on sparse mapping matrices to preserve local neighborhood structures. However, this strategy favors large-weight features, potentially overlooking smaller yet valuable ones and distorting data distribution and feature structure. Besides, some methods focus on local structure information, failing to explore global information. To address these limitations, we introduce an exponential weighting mechanism to induce a rational feature distribution and explore data structure in the feature subspace. Specifically, we propose a unified framework incorporating local structure learning and exponentially weighted sparse regression for optimal feature combinations, preserving global and local information. Experimental results demonstrate the superiority of our approach over existing unsupervised feature selection methods.}
}
@article{ZHAO2024110195,
title = {SSIR: Spatial shuffle multi-head self-attention for Single Image Super-Resolution},
journal = {Pattern Recognition},
volume = {148},
pages = {110195},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110195},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008920},
author = {Liangliang Zhao and Junyu Gao and Donghu Deng and Xuelong Li},
keywords = {Single Image Super-Resolution, Long-range attention, Vision transformer},
abstract = {Benefiting from the development of deep convolutional neural networks, CNN-based single-image super-resolution methods have achieved remarkable reconstruction results. However, the limited perceptual field of the convolutional kernel and the use of static weights in the inference process limit the performance of CNN-based methods. Recently, a few vision transformer-based image super-resolution methods have achieved excellent performance compared to CNN-based methods. These methods contain many parameters and require vast amounts of GPU memory for training. In this paper, we propose a spatial shuffle multi-head self-attention for single-image super-resolution that can significantly model long-range pixel dependencies without additional computational consumption. A local perception module is also proposed to combine convolutional neural networks’ local connectivity and translational invariance. Reconstruction results on five popular benchmarks show that the proposed method outperforms existing methods in both reconstruction accuracy and visual performance. The proposed method matches the performance of transformed-based methods but requires an inferior number of transformer blocks, which reduces the number of parameters by 40%, GPU memory by 30%, and inference time by 30% compared to transformer-based methods.}
}
@article{LIU2024110151,
title = {PA-Pose: Partial point cloud fusion based on reliable alignment for 6D pose tracking},
journal = {Pattern Recognition},
volume = {148},
pages = {110151},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110151},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008488},
author = {Zhenyu Liu and Qide Wang and Daxin Liu and Jianrong Tan},
keywords = {3D point cloud, Deep learning, Pose tracking, Feature fusion},
abstract = {Learning-based 6-DOF (6D) pose tracking, serving as a basis for most real-time applications such as augmented reality and robot manipulation, receives attention transiting from 2D to 3D vision, with the popularity of depth sensors. However, the irregular nature of 3D point clouds challenges this task, especially since the lack of explicit alignments hinders the interaction and fusion between the observed point clouds. Therefore, this paper proposes a novel approach named PA-Pose to achieve 6D pose tracking in point clouds. It takes the forward-predicted dense correspondences within an overlap as reliable alignments, to guide the feature fusion of the partial-to-partial point clouds. Then, the relative transformation pose of adjacent observations is continuously regressed from the point-wisely fused features by confidence scoring, avoiding non-differentiable pose fitting. In addition, a shifted point convolution (SPConv) operation is introduced in the fusion process, to further promote the local context interaction of the observed point cloud pair in the expanded alignment field. Extensive experiments on two benchmark datasets (YCB-Video and YCBInEOAT) demonstrate that our method achieves state-of-the-art performance. Even though only 3D point clouds are taken as input, our PA-Pose is still competitive with those methods fully utilizing RGB-D information in the single view. Finally, experiments in the real scene for tracking industrial objects also validates the effectiveness of the proposed method.}
}
@article{LIN2024110143,
title = {A coarse-to-fine pattern parser for mitigating the issue of drastic imbalance in pixel distribution},
journal = {Pattern Recognition},
volume = {148},
pages = {110143},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110143},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008403},
author = {Zhongqi Lin and Xudong Jiang and Zengwei Zheng},
keywords = {Pattern parsing, Unbalanced distribution of pixels, Graph attention, Capsule network},
abstract = {The significance of minute semantic components such as eyes and eyebrows tends to be overshadowed by larger components like skin and background, leading to inadequate graph attention from the model. Recent cropping-and-segmenting strategies comprise distinct stages and do not involve extensive interactions, thereby preventing joint optimization for collaborative perception. To mitigate this flaw, a coarse-to-fine pattern parsing network (CtFPPN) is proposed based on the capsule network (CapsNet). The CtFPPN incorporates a coarse-grained parser module, which generates binary coarse-scaled masks for larger components, and a fine-grained parser module, which performs fine-scaled parsing for smaller components using the coarse contexts as references. To establish a connection between two parsers, the discretization attention fragmentation mechanism (DAFM) is customized. The fine-grained parser has the option to aggregate projections from non-spatially-fixed collections of the coarse-grained parser, thereby embodying the principle of "coarse-to-fine" of CtFPPN. Under the premise of the existence of imbalanced pixel distribution, quantitative and ablation experiments of face and human parsing demonstrate the superiority of CtFPPN over the state-of-the-arts. Notably, CtFPPN excels in mitigating the pixel imbalance issues and accurately defining fine-scaled semantic boundaries of minuscule components.}
}
@article{SCHULTZ2024110138,
title = {ConvGeN: A convex space learning approach for deep-generative oversampling and imbalanced classification of small tabular datasets},
journal = {Pattern Recognition},
volume = {147},
pages = {110138},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110138},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300835X},
author = {Kristian Schultz and Saptarshi Bej and Waldemar Hahn and Markus Wolfien and Prashant Srivastava and Olaf Wolkenhauer},
keywords = {Imbalanced data, Convex space learning, LoRAS, GAN, Tabular data},
abstract = {Oversampling is commonly used to improve classifier performance for small tabular imbalanced datasets. State-of-the-art linear interpolation approaches can be used to generate synthetic samples from the convex space of the minority class. Generative networks are common deep learning approaches for synthetic sample generation. However, their scope on synthetic tabular data generation in the context of imbalanced classification is not adequately explored. In this article, we show that existing deep generative models perform poorly compared to linear interpolation-based approaches for imbalanced classification problems on small tabular datasets. To overcome this, we propose a deep generative model, ConvGeN that combines the idea of convex space learning with deep generative models. ConvGeN learns coefficients for the convex combinations of the minority class samples, such that the synthetic data is distinct enough from the majority class. Our benchmarking experiments demonstrate that our proposed model ConvGeN improves imbalanced classification on such small datasets, as compared to existing deep generative models, while being on par with the existing linear interpolation approaches. Moreover, we discuss how our model can be used for synthetic tabular data generation in general, even outside the scope of data imbalance, and thus improves the overall applicability of convex space learning.}
}
@article{XING2024110191,
title = {Bounded exponential loss function based AdaBoost ensemble of OCSVMs},
journal = {Pattern Recognition},
volume = {148},
pages = {110191},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110191},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008889},
author = {Hong-Jie Xing and Wei-Tao Liu and Xi-Zhao Wang},
keywords = {One-class classification, AdaBoost, Exponential loss function, One-class support vector machine, Outliers},
abstract = {As a commonly used ensemble method, AdaBoost has drawn much consideration in the field of machine learning. However, AdaBoost is highly sensitive to outliers. The performance of AdaBoost may be greatly deteriorated when the training samples are polluted by outliers. For binary and multi-class classifications, there have emerged many approaches to improving the robustness of AdaBoost against outliers. Unfortunately, there are too few researches on enhancing the robustness of AdaBoost against outliers in the case of one-class classification. In this study, the exponential loss function of AdaBoost is replaced by a more robust one to improve the anti-outlier ability of the conventional AdaBoost based ensemble of one-class support vector machines (OCSVMs). Furthermore, based on the redesigned loss function, the update formulae for the weights of base classifiers and the probability distribution of training samples are reformulated towards the AdaBoost ensemble of OCSVMs. The empirical error upper bound is derived from the theoretical viewpoint. Experimental outcomes upon the artificial and benchmark data sets show that the presented ensemble approach is more robust against outliers than its related methods.}
}
@article{ZHANG2024110167,
title = {Anomaly detection via gating highway connection for retinal fundus images},
journal = {Pattern Recognition},
volume = {148},
pages = {110167},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110167},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008646},
author = {Wentian Zhang and Haozhe Liu and Jinheng Xie and Yawen Huang and Yu Zhang and Yuexiang Li and Raghavendra Ramachandra and Yefeng Zheng},
keywords = {Anomaly detection, Feature prediction, Fundus image, Skip connection},
abstract = {Since the labels for medical images are challenging to collect in real scenarios, especially for rare diseases, fully supervised methods cannot achieve robust performance for clinical anomaly detection. Recent research tried to tackle this problem by training the anomaly detection framework using only normal data. Reconstruction-based methods, e.g., auto-encoder, achieved impressive performances in the anomaly detection task. However, most existing methods adopted the straightforward backbone architecture (i.e., encoder-and-decoder) for image reconstruction. The design of a skip connection, which can directly transfer information between the encoder and decoder, is rarely used. Since the existing U-Net has demonstrated the effectiveness of skip connections for image reconstruction tasks, in this paper, we first use the dynamic gating strategy to achieve the usage of skip connections in existing reconstruction-based anomaly detection methods and then propose a novel gating highway connection module to adaptively integrate skip connections into the framework and boost its anomaly detection performance, namely GatingAno. Furthermore, we formulate an auxiliary task, namely histograms of oriented gradients (HOG) prediction, to encourage the framework to exploit contextual information from fundus images in a self-driven manner, which increases the robustness of feature representation extracted from the healthy samples. Last but not least, to improve the model generalization for anomalous data, we introduce an adversarial strategy for the training of our multi-task framework. Experimental results on the publicly available datasets, i.e., IDRiD and ADAM, validate the superiority of our method for detecting abnormalities in retinal fundus images. The source code is available at https://github.com/WentianZhang-ML/GatingAno.}
}
@article{ZOU2024110115,
title = {UniG-Encoder: A universal feature encoder for graph and hypergraph node classification},
journal = {Pattern Recognition},
volume = {147},
pages = {110115},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110115},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008129},
author = {Minhao Zou and Zhongxue Gan and Yutong Wang and Junheng Zhang and Dongyan Sui and Chun Guan and Siyang Leng},
keywords = {Graph and hypergraph, Representation learning, Homophily and heterophily, Node classification, Feature projection},
abstract = {Despite the decent performance and fruitful applications of Graph Neural Networks (GNNs), Hypergraph Neural Networks (HGNNs), and their well-designed variants, on some commonly used benchmark graphs and hypergraphs, they are outperformed by even a simple Multi-Layer Perceptron. This observation motivates a reexamination of the design paradigm of the current GNNs and HGNNs and poses challenges of extracting graph features effectively. In this work, a universal feature encoder for both graph and hypergraph representation learning is designed, called UniG-Encoder. The architecture starts with a forward transformation of the topological relationships of connected nodes into edge or hyperedge features via a normalized projection matrix. The resulting edge/hyperedge features, together with the original node features, are fed into a neural network. The encoded node embeddings are then derived from the reversed transformation, described by the transpose of the projection matrix, of the network’s output, which can be further used for tasks such as node classification. The designed projection matrix, encoding the graph features, is intuitive and interpretable. The proposed architecture, in contrast to the traditional spectral-based and/or message passing approaches, simultaneously and comprehensively exploits the node features and graph/hypergraph topologies in an efficient and unified manner, covering both heterophilic and homophilic graphs. Furthermore, a variant version, UniG-Encoder II, is devised to leverage multi-hop node information. Extensive experiments are conducted and demonstrate the superior performance of the proposed framework on twelve representative hypergraph datasets and six real-world graph datasets, compared to the state-of-the-art methods. Our implementation is available online at https://github.com/MinhZou/UniG-Encoder.}
}
@article{HUANG2024110086,
title = {Joint representation learning for text and 3D point cloud},
journal = {Pattern Recognition},
volume = {147},
pages = {110086},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110086},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007835},
author = {Rui Huang and Xuran Pan and Henry Zheng and Haojun Jiang and Zhifeng Xie and Cheng Wu and Shiji Song and Gao Huang},
keywords = {Point cloud, Multi-modal learning, Representation learning},
abstract = {Recent advancements in vision-language pre-training (e.g., CLIP) have enabled 2D vision models to benefit from language supervision. However, the joint representation learning of 3D point cloud with text remains under-explored due to challenges in acquiring 3D-Text data pairs. Prior works propose to project point clouds into 2D depth maps and directly use CLIP, while they sacrifice 3D structural information, limiting its applicability. In this paper, we put forward Text4Point, a novel framework to construct language-guided 3D models for dense prediction tasks. Text4Point utilizes 2D images as a bridge to connect the point cloud and language modalities. It follows a pre-training and fine-tuning paradigm. During pre-training, we leverage dense contrastive learning to align the image and point cloud representations using the readily available RGB-D data. Together with the well-aligned image and text features achieved by CLIP, the point cloud features are implicitly aligned with the text embeddings. Further, we propose a Text Querying Module to integrate language information into 3D representation learning by querying text embeddings with point cloud features. For fine-tuning, the model learns 3D representations under informative language guidance without 2D images. Extensive experiments demonstrate consistent improvement on various dense prediction tasks with Text4Point.}
}
@article{LUO2024110192,
title = {A full-scale hierarchical encoder-decoder network with cascading edge-prior for infrared and visible image fusion},
journal = {Pattern Recognition},
volume = {148},
pages = {110192},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110192},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008890},
author = {Xiaoqing Luo and Juan Wang and Zhancheng Zhang and Xiao-jun Wu},
keywords = {Full-scale, Long-range, Progressive semantic, Edge-prior, Encoder-decoder, Infrared and visible image fusion},
abstract = {Most existing deep learning-based infrared and visible image fusion methods always fail to consider the full-scale long-range correlation and the prior knowledge, resulting in the fused images with low-contrast salient objects and blurred edge details. To overcome these drawbacks, a full-scale hierarchical encoder-decoder network with cascading edge-prior for infrared and visible image fusion is proposed. First, a top-down encoder extracts the hierarchical representations from source image. Then, to inject edge priors into the network and capture the progressive semantic correlations, a triple fusion mechanism is proposed including edge image fusion based on maximum fusion strategy, single-scale shallow layer fusion and full-scale semantic layer fusion based on dual-attention fusion (DAF) strategy. The fused full-scale semantic features (F2SF) are obtained by capturing the long-range affinities of the full-scale. At the same time, a cascading edge-prior branch (CEPB) is designed to embed the fused edge knowledge into fused single-scale shallow features, jointly guiding the decoder to focus on abundant details layer-by-layer on the basis of F2SF, thus recovering the edge and texture details of the fused image well. Finally, a novel loss function consisting of SSIM, intensity and edge loss is constructed to further maintain the network with better edge representation and reconstruction capability. Compared with existing state-of-the-art fusion methods, the proposed method has better performance in terms of both visual evaluation and objective evaluation on public datasets. The source code is available at https://github.com/lxq-jnu/FSFusion.}
}
@article{LIU2024110140,
title = {CS-net: Conv-simpleformer network for agricultural image segmentation},
journal = {Pattern Recognition},
volume = {147},
pages = {110140},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110140},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008373},
author = {Lei Liu and Guorun Li and Yuefeng Du and Xiaoyu Li and Xiuheng Wu and Zhi Qiao and Tianyi Wang},
keywords = {Semantic segmentation, CS-net, Agricultural image, CNNs, Transformers, Simple-attention},
abstract = {Agricultural image segmentation needs to catch up to the development speed of deep learning, and the explosive computational overhead and limited high-quality labeled datasets are the main reasons preventing the application of Transformers to agricultural image segmentation. This study proposes a Simple-Attention Block (SIAB) using channel-by-channel and spatial convolutional computation, whose computational complexity is linearly correlated with the input image size. Then, we design a Simpleformer by cascading SIAB and FFN to reshape the Transformer architecture. Further, the fusion of CNN and Simpleformer constructs a dataset quality-independent agricultural image segmentation model (CS-Net). Finally, we evaluate CS-Net on four datasets, and compared with the state-of-the-art models, CS-Net has more advantageous inference speed and segmentation accuracy, which pushes the development of Transformers in the field of agricultural image processing. Additionally, we explore the reasons for the Transformers’ performance collapse for agricultural applications, providing research scholars with a theoretical foundation for related issues.}
}
@article{DU2024110116,
title = {JoyPose: Jointly learning evolutionary data augmentation and anatomy-aware global–local representation for 3D human pose estimation},
journal = {Pattern Recognition},
volume = {147},
pages = {110116},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110116},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008130},
author = {Songlin Du and Zhiwei Yuan and Peifu Lai and Takeshi Ikenaga},
keywords = {3D human pose estimation, Evolutionary data augmentation, Global–local representation, Anatomy-awareness, Joint optimization},
abstract = {Video-based 3D human pose estimation is an important yet challenging task for many human-involved pattern recognition systems. Existing deep learning-based 3D human pose estimation methods are faced with the problems of lacking large-scale training data and lacking effective solutions to represent the complicated human body structure. To this end, this paper proposes a jointly learning framework entitled JoyPose that simultaneously leverages both human pose data augmentation and human pose estimation. In particular, JoyPose consists of an evolutionary data augmentation module and an anatomy-aware global–local pose feature representation module for 3D human pose estimation. The evolution for data augmentation is guided by a reinforcement learning strategy in a probabilistic way according to pose estimation loss. The anatomy-aware global–local pose feature representation module separately captures global features and local features according to anatomical and kinematic patterns observed from pose estimation errors across different human joints. The performance of the final human pose estimation is leveraged by both data augmentation and anatomy-aware global–local feature representation. Extensive experiments on three real-world datasets demonstrate the superiority and robustness against state-of-the-art methods.}
}
@article{XIAN2024110112,
title = {Feature fusion method based on spiking neural convolutional network for edge detection},
journal = {Pattern Recognition},
volume = {147},
pages = {110112},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110112},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008099},
author = {Ronghao Xian and Xin Xiong and Hong Peng and Jun Wang and Antonio Ramírez {de Arellano Marrero} and Qian Yang},
keywords = {Edge detection, Feature fusion, Nonlinear spiking neural P systems, NSNP-type neuron model},
abstract = {NSNP-type neuron is a new type of neuron model inspired by nonlinear spiking mechanisms in nonlinear spiking neural P systems. In order to address the loss problem of edge detail information in edge detection methods based on deep learning, we propose a feature fusion method based on NSNP-type neurons. The architecture of this feature fusion method consists of two modules: feature extraction module and feature fusion module. In particular, the feature fusion module is composed of convolutional blocks constructed by NSNP-type neurons for multi-level feature fusions, and CoT blocks with Transformer style is introduced to extract rich contextual information from low-level features and high-level features. To fuse multi-level features and preserve contextual information, we design a new loss function that not only preserves feature prediction loss and fusion loss, but also considers contour-related and texture-related information. The proposed method is evaluated on BSDS500 and NYUDv2 data sets and compare it with 9 baseline methods and 12 CNN-based methods, and we achieve ODS of 0.808 and OIS of 0.827 on BSDS500. The comparison results demonstrate the advantages of the proposed method for edge detection. The source code is available at https://github.com/xhuph66/FF-CNSNP-master.}
}
@article{YE2024110156,
title = {Self-Supervised Adversarial Variational Learning},
journal = {Pattern Recognition},
volume = {148},
pages = {110156},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110156},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008531},
author = {Fei Ye and Adrian. G. Bors},
keywords = {Self-supervised learning, Variational Autoencoders (VAE), Generative Adversarial Nets (GAN), Representation learning, Mutual information},
abstract = {A natural approach for representation learning is to combine the inference mechanisms of VAEs and the generative abilities of GANs, within a new model, namely VAEGAN. Most existing VAEGAN models would jointly train the generator and inference modules, which has limitations when learning representations generated by a pre-trained GAN model without data. In this paper, we develop a novel hybrid model, called the Self-Supervised Adversarial Variational Learning (SS-AVL) which introduces a two-step optimization procedure training separately the generator and the inference model. The primary advantage of SS-AVL over existing VAEGAN models is that SS-AVL optimizes the inference models in a self-supervised learning manner where the samples used for training the inference models are drawn from the generator distribution instead of using real samples. This can allow SS-AVL to learn representations from arbitrary GAN models without using real data. Additionally, we employ information maximization into the context of increasing the maximum likelihood, which encourages SS-AVL to learn meaningful latent representations. We perform extensive experiments to demonstrate the effectiveness of the proposed SS-AVL model.}
}
@article{LIU2024110087,
title = {Spatio-temporal human action localization in indoor surveillances},
journal = {Pattern Recognition},
volume = {147},
pages = {110087},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110087},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007847},
author = {Zihao Liu and Danfeng Yan and Yuanqiang Cai and Yan Song},
keywords = {Video analysis, Spatio-temporal action localization dataset, Real-world indoor surveillance},
abstract = {Spatio-temporal action localization is a crucial and challenging task in the field of video understanding. Existing benchmarks for spatio-temporal action detection are limited by factors such as incomplete annotations, high-level non-universal actions, and uncommon scenarios. To address these limitations and facilitate research in real-world security applications, we introduce a novel human-centric dataset for spatio-temporal localization of atomic actions in indoor surveillance settings, termed as HIA (Human-centric Indoor Actions). The HIA dataset is constructed by selecting 30 atomic action classes, compiling 100 surveillance videos, and annotating 219,225 frames with 370,937 bounding boxes. The primary characteristics of HIA include (1) accurate spatio-temporal annotations for atomic actions, (2) human-centric annotations at the frame level, (3) temporal linking of persons across discontinuous tracks, and (4) utilization of indoor surveillance videos. Our HIA, with its realistic settings in indoor surveillance scenes and comprehensive annotations, presents a valuable and novel challenge to the spatio-temporal action localization domain. To establish a benchmark, we evaluate various methods and provide an in-depth analysis of the HIA dataset. The HIA dataset will be made available soon, and we anticipate that it will serve as a standard and practical benchmark for the research community.}
}
@article{WANG2024110180,
title = {Oracle character recognition using unsupervised discriminative consistency network},
journal = {Pattern Recognition},
volume = {148},
pages = {110180},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110180},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008774},
author = {Mei Wang and Weihong Deng and Sen Su},
keywords = {Oracle character recognition, Unsupervised domain adaptation, Self-training, Consistency regularization},
abstract = {Ancient history relies on the study of ancient characters. However, real-world scanned oracle characters are difficult to collect and annotate, posing a major obstacle for oracle character recognition (OrCR). Besides, serious abrasion and inter-class similarity also make OrCR more challenging. In this paper, we propose a novel unsupervised domain adaptation method for OrCR, which enables to transfer knowledge from labeled handprinted oracle characters to unlabeled scanned data. We leverage pseudo-labeling to incorporate the semantic information into adaptation and constrain augmentation consistency to make the predictions of scanned samples consistent under different perturbations, leading to the model robustness to abrasion, stain and distortion. Simultaneously, an unsupervised transition loss is proposed to learn more discriminative features on the scanned domain by optimizing both between-class and within-class transition probability. Extensive experiments show that our approach achieves state-of-the-art result on Oracle-241 dataset and substantially outperforms the recently proposed structure-texture separation network by 15.1%.}
}
@article{XU2024110128,
title = {A unified and efficient semi-supervised learning framework for stereo matching},
journal = {Pattern Recognition},
volume = {147},
pages = {110128},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110128},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008257},
author = {Fudong Xu and Lin Wang and Huibin Li},
keywords = {Stereo matching, Semi-supervised learning, Pseudo labeling},
abstract = {Recently, stereo matching algorithms have made tremendous progress in terms of both accuracy and efficiency. However, it remains a great challenge to train a practical model due to the scarcity of ground truth disparity. In this paper, we propose a Semi-supervised Stereo Matching Framework (SSMF), i.e., a continuous self-training pipeline involving both teacher model and student model. The proposed framework combines the consistency regularization with the entropy minimization to effectively utilize the unlabeled data in large quantities. To enhance the quality of pseudo disparities: (1) we introduce an additional branch in stereo models to regress confidence map and perform adaptive selection; (2) we inject strong data augmentations on unlabeled data to achieve the prediction consistency; (3) we propose a novel loss function to utilize pseudo disparities of different confidence levels. Comprehensive experimental results show that the proposed framework enables to largely improve the disparity accuracy and robustness. Moreover, it also demonstrates competitive performance in cross-domain scenarios. Among all published methods as of August 2023, it achieves 1st on KITTI 2012 benchmark and 4th on KITTI 2015 benchmark. Code is available at https://github.com/Twil-7/semi-supervised-stereo-matching.}
}
@article{ZHONG2024110168,
title = {ICLR: Instance Credibility-Based Label Refinement for label noisy person re-identification},
journal = {Pattern Recognition},
volume = {148},
pages = {110168},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110168},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008658},
author = {Xian Zhong and Xiyu Han and Xuemei Jia and Wenxin Huang and Wenxuan Liu and Shuaipeng Su and Xiaohan Yu and Mang Ye},
keywords = {Person re-identification, Label noise, Label-Incredibility Optimization, Incredible Instance Re-Weight},
abstract = {Person re-identification (Re-ID) has demonstrated remarkable performance when trained on accurately annotated data. However, in practical applications, the presence of annotation errors is unavoidable, which can undermine the accuracy and robustness of the Re-ID model training. To address the adverse impacts of label noise, especially in scenarios with limited training samples for each identity (ID), a common approach is to utilize all the available sample labels. Unfortunately, these labels contain incorrect labels, leading to the model being influenced by noise and compromising its performance. In this paper, we propose an Instance Credibility-based Label Refinement and Re-weighting (ICLR) framework to exploit partially credible labels to refine and re-weight incredible labels effectively. Specifically, the Label-Incredibility Optimization (LIO) module is proposed to optimize incredible labels before model training, which partitions the samples into credible and incredible samples and propagates credible labels to others. Furthermore, we design an Incredible Instance Re-weight (I2R) strategy, aiming to emphasize instances that contribute more significantly and dynamically adjust the weight of each instance. The proposed method seamlessly reinforces accuracy without requiring additional information or discarding any samples. Extensive experimental results conducted on Market-1501 and Duke-MTMC datasets demonstrate the effectiveness of our proposed method, leading to a substantial improvement in performance under both random noise and pattern noise settings. Code will be available at https://github.com/whut16/ReID-Label-Noise.}
}
@article{BAO2024110144,
title = {Dynamic Graph Contrastive Learning via Maximize Temporal Consistency},
journal = {Pattern Recognition},
volume = {148},
pages = {110144},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110144},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008415},
author = {Peng Bao and Jianian Li and Rong Yan and Zhongyi Liu},
keywords = {Contrastive learning, Dynamic graph, Temporal information},
abstract = {Graph contrastive learning (GCL) is one of the most powerful self-supervised representation learning frameworks. Existing GCL methods have achieved impressive performance. However, it is still challenging to capture the evolution of nodes or edges, where the interaction of nodes or edges is stable in a short time but changeable at long time intervals. Therefore, it is crucial to capture the temporal consistency in dynamic graph. In this paper, we propose a novel Dynamic Graph Contrastive Learning framework, DyGCL, which learns node representation by maximizing the temporal consistency in a short time and discriminating the non-consistency in a long term. More specifically, DyGCL consists of two parts: GCL Trainer and Auxiliary Trainer. GCL Trainer focus on distinguishing temporal consistency and non-consistency. And the Auxiliary Trainer aims to improve the generalization ability with less labeled data as auxiliary supervision. Finally, we demonstrate the effectiveness and superiority of DyGCL by applying it to three datasets.}
}
@article{SONG2024110071,
title = {Dynamic attention augmented graph network for video accident anticipation},
journal = {Pattern Recognition},
volume = {147},
pages = {110071},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110071},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007689},
author = {Wenfeng Song and Shuai Li and Tao Chang and Ke Xie and Aimin Hao and Hong Qin},
keywords = {Accident anticipation, Dynamic attention, Graph network, Global context},
abstract = {Accident anticipation (or the prediction of abnormal events in general) aims to forecast accidents before they occur by assessing risks based on the preceding frames in videos. The risk assessment heavily relies on understanding the semantics of the scene context and predicting the interactions among the involved subjects. Indeed, the comprehensive utilization of spatial relationships among the subjects of immediate interest in a single frame and temporal dependencies across consecutive frames is crucial for video accident anticipation. To address this challenge, we propose a novel approach called Dynamic Attention Augmented Graph Network (DAA-GNN), which leverages underlying spatial cues and models’ relationships among detected subjects of immediate interest. Specifically, our approach employs a graph neural network that is enhanced by global context clues, allowing effective message propagation and the discovery of interactions among the subjects of interest in the scene. The DAA-GNN includes a temporal attention module designed to identify long-term dependencies along the temporal axis, contributing to an end-to-end deep network solution for accurate accident anticipation. We extensively evaluate our method on the publicly available Dashcam Accident Dataset (DAD) and Epic Fail (EF) datasets, by conducting comprehensive experiments to assess its performance. The results unequivocally demonstrate that our method outperforms the state-of-the-art accident anticipation methods. Our source code and datasets are available at https://github.com/ZxyLinkstart/DAA-GNN.}
}
@article{TANG2024110155,
title = {Offline handwritten mathematical expression recognition with graph encoder and transformer decoder},
journal = {Pattern Recognition},
volume = {148},
pages = {110155},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110155},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300852X},
author = {Jia-Man Tang and Hong-Yu Guo and Jin-Wen Wu and Fei Yin and Lin-Lin Huang},
keywords = {Handwritten mathematical expression recognition, Symbol detection, Graph Neural Network, Transformer},
abstract = {Handwritten mathematical expression recognition (HMER) has attracted extensive attention. Despite the significant progress achieved in recent years attributed to the development of deep learning approaches, HMER remains a challenge due to the complex spatial structure and variable writing styles. Encoder–decoder models with attention mechanism, which treats HMER as an image-to-sequence (i.e. LaTeX) generation task, have boosted the accuracy, but suffer from low interpretability in that the symbols are not segmented explicitly. Symbol segmentation is desired for facilitating post-processing and human interaction in real applications. In this paper, we formulate the mathematical expression as a graph and propose a Graph-Encoder-Transformer-Decoder (GETD) approach for HMER. For constructing the graph from input image, candidate symbols are first detected using an object detector and represented as the nodes of a graph, called symbol graph, and the edges of the graph encodes the between-symbol relationship. The spatial information is aggregated in a graph neural network (GNN), and a Transformer-based decoder is used to identify the symbol classes and structure from the graph. Experiments on public datasets demonstrate that our GETD model achieves competitive expression recognition performance while offering good interpretability compared with previous methods.}
}
@article{CAI2024110109,
title = {Attention Cycle-consistent universal network for More Universal Domain Adaptation},
journal = {Pattern Recognition},
volume = {147},
pages = {110109},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110109},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008063},
author = {Ziyun Cai and Yawen Huang and Tengfei Zhang and Xiao-Yuan Jing and Yefeng Zheng and Ling Shao},
keywords = {Domain adaptation, RGB-D data, Visual categorization, Unequal category},
abstract = {Existing Universal Domain Adaptation (UniDA) approaches can handle various domain adaptation (DA) tasks, which need no prior information about the category overlap across target and source domains. However, traditional UniDA scenario cannot fully cover every DA scenario, e.g., Multi-Source DA is absent. Therefore, aiming to simultaneously handle more DA scenarios in nature, we propose the More Universal Domain Adaptation (MUniDA) task. There are three challenges in MUniDA: (i) Category shift between source and target domains; (ii) Domain shift, especially the domain shift among multiple modalities in the source, which is ignored by the current UniDA approaches; (iii) How to recognize common categories across domains? We propose a more universally applicable DA approach that can tackle above challenges without any modification called Attention Cycle-consistent Universal Network (A-CycleUN). We show through extensive experiments on several benchmarks that A-CycleUN works stably and outperforms baselines across different MUniDA settings.}
}
@article{KIM2024110201,
title = {LC-MSM: Language-Conditioned Masked Segmentation Model for unsupervised domain adaptation},
journal = {Pattern Recognition},
volume = {148},
pages = {110201},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110201},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008981},
author = {Young-Eun Kim and Yu-Won Lee and Seong-Whan Lee},
keywords = {Unsupervised domain adaptation, Semantic segmentation, Text-image correlation},
abstract = {Unsupervised domain adaptation (UDA) is an important research topic in semantic segmentation tasks, wherein pixel-wise annotations are often difficult to collect in a test environment due to their high labeling costs. Previous UDA-based studies trained their segmentation networks using labeled synthetic data and unlabeled realistic data as source and target domains, respectively. However, they often fail to distinguish semantically similar classes, such as person vs. rider and road vs. sidewalk, because these classes are prone to confusion in domain-shifted environments. In this paper, we introduce a Language-Conditioned Masked Segmentation Model (LC-MSM), which is a new framework for the joint learning of context relations and domain-agnostic information for domain-adaptive semantic segmentation. Specifically, we reconstruct semantic labels with masked image conditions on the generalized text embeddings of the corresponding semantic class from OpenCLIP, which contains domain-invariant knowledge from large-scale data. To this end, we correlate the generalized text embeddings onto the per-pixel image feature of a masked image that learned the spatial context to further append domain-agnostic language information to the semantic decoder. This facilitates the generalization of our model to the target domain via the learning of context information within individual training instances, while considering cross-domain representations spanning the entire dataset. LC-MSM achieves an unprecedented UDA performance of 71.8 and 62.8 mIoU on GTA→Cityscapes and SYNTHIA→Cityscapes, respectively, which corresponds to an improvement of +3.5 and +1.9 percent points over the baseline method.}
}
@article{ZHAO2024110125,
title = {Incremental feature selection for dynamic incomplete data using sub-tolerance relations},
journal = {Pattern Recognition},
volume = {148},
pages = {110125},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110125},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008221},
author = {Jie Zhao and Yun Ling and Faliang Huang and Jiahai Wang and Eric W.K. See-To},
keywords = {Incremental feature selection, Tolerance rough set, Sub-tolerance relation, Significance measure},
abstract = {Tolerance Rough Set (TRS) theory is commonly employed for feature selection with incomplete data. However, TRS has limitations such as ignoring uncertainty, which often leads to the inclusion of redundant features and diminished classification accuracy. To address these limitations, we propose an extension called Subrelation Tolerance Class (STC). STC decomposes the tolerance relation into two subrelations, enabling a two-stage certainty measurement. This approach progressively filters out certain regions, thereby reducing computational space requirements, and introduces a new significance measure that considers both certain and uncertain information. Leveraging STC and our proposed measure, we develop an incremental feature selection algorithm capable of handling incomplete streaming data. We conduct experiments on real-world datasets and compare the performance with existing algorithms to validate the superiority of our method. The experimental results show that our algorithm reduces the execution time by over 89.78% compared to the baselines while maintaining the classification accuracy.}
}
@article{ZHAO2024110149,
title = {Coresets for fast causal discovery with the additive noise model},
journal = {Pattern Recognition},
volume = {148},
pages = {110149},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110149},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008464},
author = {Boxiang Zhao and Shuliang Wang and Lianhua Chi and Hanning Yuan and Ye Yuan and Qi Li and Jing Geng and Shao-Liang Zhang},
keywords = {Causal discovery, Coresets, Functional causal model, Additive noise model, Big data},
abstract = {Causal discovery reveals the true causal relationships behind data and discovering causal relationships from observed data is a particularly challenging problem, especially in large-scale datasets. The functional causal model is an effective method for causal discovery, but its time efficiency cannot be guaranteed. How to efficiently apply it to massive data still needs to be solved. In this paper, we propose a coreset construction for the additive noise model to accelerate causal discovery. According to the asymmetry characteristic of causality, samples were assigned different weights to construct the coreset. With the constructed coreset, we propose a Fast causal discovery algorithm based on the Additive Noise Model (FANM) to improve the time efficiency of the functional causal model while ensuring the result performance of causal discovery. Experiments on synthetic data and real-world data show that our proposed algorithm is much more time-efficient than the methods based on the functional causal model, and the runtime of FANM remains consistent as sample size increases while maintaining or exceeding the accuracy of the original nonlinear additive noise model.}
}
@article{HAN2024110096,
title = {F-SCP: An automatic prompt generation method for specific classes based on visual language pre-training models},
journal = {Pattern Recognition},
volume = {147},
pages = {110096},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110096},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007938},
author = {Baihong Han and Xiaoyan Jiang and Zhijun Fang and Hamido Fujita and Yongbin Gao},
keywords = {Multi-modal, Vision language model, Prompt tuning, Large-scale pre-training model},
abstract = {The zero-shot classification performance of large-scale vision-language pre-training models (e.g., CLIP, BLIP and ALIGN) can be enhanced by incorporating a prompt (e.g., “a photo of a [CLASS]”) before the class words. Modifying the prompt slightly can have significant effect on the classification outcomes of these models. Thus, it is crucial to include an appropriate prompt tailored to the classes. However, manual prompt design is labor-intensive and necessitates domain-specific expertise. The CoOp (Context Optimization) converts hand-crafted prompt templates into learnable word vectors to automatically generate prompts, resulting in substantial improvements for CLIP. However, CoOp exhibited significant variation in classification performance across different classes. Although CoOp-CSC (Class-Specific Context) has a separate prompt for each class, only shows some advantages on fine-grained datasets. In this paper, we propose a novel automatic prompt generation method called F-SCP (Filter-based Specific Class Prompt), which distinguishes itself from the CoOp-UC (Unified Context) model and the CoOp-CSC model. Our approach focuses on prompt generation for low-accuracy classes and similar classes. We add the Filter and SCP modules to the prompt generation architecture. The Filter module selects the poorly classified classes, and then reproduce the prompts through the SCP (Specific Class Prompt) module to replace the prompts of specific classes. Experimental results on six multi-domain datasets shows the superiority of our approach over the state-of-the-art methods. Particularly, the improvement in accuracy for the specific classes mentioned above is significant. For instance, compared with CoOp-UC on the OxfordPets dataset, the low-accuracy classes, such as, Class21 and Class26, are improved by 18% and 12%, respectively.}
}
@article{ZHAO2024110193,
title = {Deep federated learning hybrid optimization model based on encrypted aligned data},
journal = {Pattern Recognition},
volume = {148},
pages = {110193},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110193},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008907},
author = {Zhongnan Zhao and Xiaoliang Liang and Hai Huang and Kun Wang},
keywords = {Federated learning, Gaussian Mixture Model, Variational AutoEncoder, Encrypted aligned data, Privacy protection},
abstract = {Federated learning can achieve multi-party data-collaborative applications while safeguarding personal privacy. However, the process often leads to a decline in the quality of sample data due to a substantial amount of missing encrypted aligned data, and there is a lack of research on how to improve the model learning effect by increasing the number of samples of encrypted aligned data in federated learning. Therefore, this paper integrates the functional characteristics of deep learning models and proposes a Variational AutoEncoder Gaussian Mixture Model Clustering Vertical Federated Learning Model (VAEGMMC-VFL), which leverages the feature extraction capability of the autoencoder and the clustering and pattern discovery capabilities of Gaussian mixture clustering on diverse datasets to further explore a large number of potentially usable samples. Firstly, the Variational AutoEncoder is used to achieve dimensionality reduction and sample feature reconstruction of high-dimensional data samples. Subsequently, Gaussian mixture clustering is further employed to partition the dataset into multiple potential Gaussian-distributed clusters and filter the sample data using thresholding. Additionally, the paper introduces a labeled sample attribute value finding algorithm to fill in attribute values for encrypted unaligned samples that meet the requirements, allowing for the full recovery of encrypted unaligned data. In the experimental section, the paper selects four sets of datasets from different industries and compares the proposed method with three federated learning clustering methods in terms of clustering loss, reconstruction loss, and other metrics. Tests on precision, accuracy, recall, ROC curve, and F1-score indicate that the proposed method outperforms similar approaches.}
}
@article{DERE2024110068,
title = {Conditional reiterative High-Fidelity GAN inversion for image editing},
journal = {Pattern Recognition},
volume = {147},
pages = {110068},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110068},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007653},
author = {Vedant Vasant Dere and Amita Shinde and Prachi Vast},
keywords = {GAN, Image editing, Image restoration, High fidelity, StyleGAN},
abstract = {Our work introduces a conditional reiteration mechanism for High-Fidelity GAN (Generative Adversarial Networks) inversion (HFGI), preserving image-specific details (like background, appearance, etc.) for both normal and out-of-domain images (e.g. heavy makeup faces). The HFGI encoder’s single-stage conditional latent maps result in blurry regions in restored images and loss of detailed information during editing. To address this, we proposed a reiterative conditional latents method that restores image-specific details sharply. The process involves two stages of iterations, reconstructing the image in the first stage, and refining image-specific details using conditional latent codes in the second stage. Our model successfully inverts out-of-domain images while preserving all details and supports InterfaceGAN, GANspace, and StyleClip for editing. We compare our approach with state-of-the-art GAN inversion methods on FFHQ (Flickr-Faces-HQ) Dataset, demonstrating significant improvements in inversion and editing quality.}
}
@article{RAHIMZADEHARASHLOO2024110189,
title = {Large-margin multiple kernel ℓp-SVDD using Frank–Wolfe algorithm for novelty detection},
journal = {Pattern Recognition},
volume = {148},
pages = {110189},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110189},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008865},
author = {Shervin {Rahimzadeh Arashloo}},
keywords = {ℓ-SVDD, Large-margin learning, Convex optimisation, Frank–Wolfe algorithm, Multiple kernel learning, Imbalanced classification, Novelty detection},
abstract = {Using a variable ℓp≥1-norm penalty on the slacks, the recently introduced ℓp-norm Support Vector Data Description (ℓp-SVDD) method has improved the performance in novelty detection over the baseline approach, sometimes remarkably. This work extends this modelling formalism in multiple aspects. First, a large-margin extension of the ℓp-SVDD method is formulated to enhance generalisation capability by maximising the margin between the positive and negative samples. Second, based on the Frank–Wolfe algorithm, an efficient yet effective method with predictable accuracy is presented to optimise the convex objective function in the proposed method. Finally, it is illustrated that the proposed approach can effectively benefit from a multiple kernel learning scheme to achieve state-of-the-art performance. The proposed method is theoretically analysed using Rademacher complexities to link its classification error probability to the margin and experimentally evaluated on several datasets to demonstrate its merits against existing methods.}
}
@article{YADAV2024110137,
title = {Robust multi-scale weighting-based edge-smoothing filter for single image dehazing},
journal = {Pattern Recognition},
volume = {149},
pages = {110137},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110137},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008348},
author = {Sumit Kr. Yadav and Kishor Sarawadekar},
keywords = {Dark channel prior, Edge-preservation, Guided image filter, Transmission map, Halo artifacts, Gradient domain},
abstract = {The guided image filter (GIF) and weighted guided image filter (WGIF) are local linear model-based good edge-preserving filters. However, due to fixed regularization parameter, they suffer from halo artifacts (morphological artifacts) in the sharp regions. To overcome this issue, a robust multi-scale weighting-based edge-smoothing filter (RMWEF) for single image dehazing is proposed in this paper. It removes morphological artifacts and over-smoothness strongly and preserves edge information precisely in both flat and sharp regions. The proposed dehaze method has four-steps. First, initial transmission map and atmospheric map are estimated by using a novel dark channel prior (DCP) method. Then, the morphological artifacts of initial transmission map are reduced by using non-local haze line averaging (NL-HLA) method. In the third step, transmission map is refined by using the proposed RMWEF. Finally, the haze free image is restored. Theoretical and experimental analysis proves that the proposed algorithm produce effective dehaze results quicker than the existing methods.}
}
@article{RAMCHANDRAN2024110113,
title = {Learning conditional variational autoencoders with missing covariates},
journal = {Pattern Recognition},
volume = {147},
pages = {110113},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110113},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008105},
author = {Siddharth Ramchandran and Gleb Tikhonov and Otto Lönnroth and Pekka Tiikkainen and Harri Lähdesmäki},
keywords = {Variational autoencoders, Gaussian process, Conditional VAEs, Missing value imputation},
abstract = {Conditional variational autoencoders (CVAEs) are versatile deep latent variable models that extend the standard VAE framework by conditioning the generative model with auxiliary covariates. The original CVAE model assumes that the data samples are independent, whereas more recent conditional VAE models, such as the Gaussian process (GP) prior VAEs, can account for complex correlation structures across all data samples. While several methods have been proposed to learn standard VAEs from partially observed datasets, these methods fall short for conditional VAEs. In this work, we propose a method to learn conditional VAEs from datasets in which auxiliary covariates can contain missing values as well. The proposed method augments the conditional VAEs with a prior distribution for the missing covariates and estimates their posterior using amortised variational inference. At training time, our method accounts for the uncertainty associated with the missing covariates while simultaneously maximising the evidence lower bound. We develop computationally efficient methods to learn CVAEs and GP prior VAEs that are compatible with mini-batching. Our experiments on simulated datasets as well as on real-world biomedical datasets show that the proposed method outperforms previous methods in learning conditional VAEs from non-temporal, temporal, and longitudinal datasets.}
}
@article{PENG2024110092,
title = {Region-adaptive and context-complementary cross modulation for RGB-T semantic segmentation},
journal = {Pattern Recognition},
volume = {147},
pages = {110092},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110092},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007896},
author = {Fengguang Peng and Zihan Ding and Ziming Chen and Gang Wang and Tianrui Hui and Si Liu and Hang Shi},
keywords = {RGB-Thermal, Semantic segmentation, Region-Adaptive Channel Modulation, Context-Complementary Spatial Modulation},
abstract = {RGB-Thermal (RGB-T) semantic segmentation is an emerging task aiming to improve the robustness of segmentation methods under extreme imaging conditions with the aid of thermal infrared modality. Foreground–background distinguishment and complementary information mining are two key challenges of this task. Recent methods use naive channel attention and cross-attention to tackle these challenges, but they still struggle with a sub-optimal solution where salient foreground features and noisy background ones might be equally modulated without distinction. The quadratic computational overhead of cross-attention also blocks its application on high-resolution features. Moreover, lacking complementary information mining in the encoding phase hinders the comprehensive scene encoding as well. To alleviate these limitations, we propose a cross modulation process with two collaborative components. The first Region-Adaptive Channel Modulation (RACM) module conducts channel attention at a fine-grained region level where foreground and background regions can be modulated differently in each channel. The second Context-Complementary Spatial Modulation (CCSM) module mines and transfers complementary information between the two modalities early in the encoding phase. Experiments show that our method achieves state-of-the-art performances on current RGB-T segmentation benchmarks.}
}
@article{NG2024110124,
title = {When IC meets text: Towards a rich annotated integrated circuit text dataset},
journal = {Pattern Recognition},
volume = {147},
pages = {110124},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110124},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300821X},
author = {Chun Chet Ng and Che-Tsung Lin and Zhi Qin Tan and Xinyu Wang and Jie Long Kew and Chee Seng Chan and Christopher Zach},
keywords = {Attribute-guided curriculum learning, Optical character recognition, Integrated circuit text dataset},
abstract = {Automated Optical Inspection (AOI) is a process that uses cameras to autonomously scan printed circuit boards for quality control. Text is often printed on chip components, and it is crucial that this text is correctly recognized during AOI, as it contains valuable information. In this paper, we introduce ICText, the largest dataset for text detection and recognition on integrated circuits. Uniquely, it includes labels for character quality attributes such as low contrast, blurry, and broken. While loss-reweighting and Curriculum Learning (CL) have been proposed to improve object detector performance by balancing positive and negative samples and gradually training the model from easy to hard samples, these methods have had limited success with one-stage object detectors commonly used in industry. To address this, we propose Attribute-Guided Curriculum Learning (AGCL), which leverages the labeled character quality attributes in ICText. Our extensive experiments demonstrate that AGCL can be applied to different detectors in a plug-and-play fashion to achieve higher Average Precision (AP), significantly outperforming existing methods on ICText without any additional computational overhead during inference. Furthermore, we show that AGCL is also effective on the generic object detection dataset Pascal VOC. Our code and dataset will be publicly available at https://github.com/chunchet-ng/ICText-AGCL.}
}
@article{CHEN2024110084,
title = {MPCCT: Multimodal vision-language learning paradigm with context-based compact Transformer},
journal = {Pattern Recognition},
volume = {147},
pages = {110084},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110084},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007811},
author = {Chongqing Chen and Dezhi Han and Chin-Chen Chang},
keywords = {Multimodal vision-language paradigms, High-dependency modeling, Visual question answering (VQA), Visual grounding (VG), Logical relationship reasoning},
abstract = {Transformer and its variants have become the preferred option for multimodal vision-language paradigms. However, they struggle with tasks that demand high-dependency modeling and reasoning, like visual question answering (VQA) and visual grounding (VG). For this, we propose a general scheme called MPCCT, which: (1) incorporates designed textual global-context information to facilitate precise computation of dependency relationships between language tokens in the language encoder; (2) dynamically modulates and filters image features using optimized textual global-context information, combined with designed spatial context information, to further enhance the dependency modeling of image tokens and the model’s reasoning ability; (3) reasonably align the language sequence containing textual global-context information with the image sequence information modulated by spatial position information. To validate MPCCT, we conducted extensive experiments on five benchmark datasets in VQA and VG, achieving new SOTA performance on multiple benchmarks, especially 73.71% on VQA-v2 and 99.15% on CLEVR. The code is available at https://github.com/RainyMoo/myvqa.}
}
@article{PENG2024110153,
title = {Graph meets probabilistic generation model: A new perspective for graph disentanglement},
journal = {Pattern Recognition},
volume = {148},
pages = {110153},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110153},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008506},
author = {Zouzhang Peng and Shuai Zheng and Zhenfeng Zhu and Zhizhe Liu and Jian Cheng and Honghui Dong and Yao Zhao},
keywords = {Graph representation learning, Graph disentanglement, Probabilistic generation model},
abstract = {Different from the existing graph disentanglement neural networks, we interpret the graph entanglement under a probabilistic generation framework in this paper. With this foundation, a Mixed Probabilistic Generation Model induced Graph Disentanglement Network (MPGD) is proposed. Considering the disentangled components corresponding to different factors as obeying specific distributions, a generalized probabilistic aggregation scheme among components is deduced theoretically. As a key part of the mixed probabilistic generative model, we provide a solution for estimating the mixture probabilities using self-attention and an in-depth analysis of its close connection with the classical EM parameter estimation method. Meanwhile, a way of probabilistic aggregation is formulated to obtain the node representation in embedding space. In addition, the prior mixture probabilities are formulated as an auxiliary factor-aware representation to facilitate the twin-branch prediction. A variety of experiments show that MPGD achieves more competitive performance than some existing state-of-the-art methods while having ideal disentangling effects. The code implementation is available in https://github.com/GiorgioPeng/MPGD.}
}
@article{WANG2024110110,
title = {HyRSM++: Hybrid relation guided temporal set matching for few-shot action recognition},
journal = {Pattern Recognition},
volume = {147},
pages = {110110},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110110},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008075},
author = {Xiang Wang and Shiwei Zhang and Zhiwu Qing and Zhengrong Zuo and Changxin Gao and Rong Jin and Nong Sang},
keywords = {Few-shot action recognition, Set matching, Semi-supervised few-shot action recognition, Unsupervised few-shot action recognition},
abstract = {Few-shot action recognition is a challenging but practical problem aiming to learn a model that can be easily adapted to identify new action categories with only a few labeled samples. However, existing attempts still suffer from two drawbacks: (i) learning individual features without considering the entire task may result in limited representation capability, and (ii) existing alignment strategies are sensitive to noises and misaligned instances. To handle the two limitations, we propose a novel Hybrid Relation guided temporal Set Matching (HyRSM++) approach for few-shot action recognition. The core idea of HyRSM++ is to integrate all videos within the task to learn discriminative representations and involve a robust matching technique. To be specific, HyRSM++ consists of two key components, a hybrid relation module and a temporal set matching metric. Given the basic representations from the feature extractor, the hybrid relation module is introduced to fully exploit associated relations within and cross videos in an episodic task and thus can learn task-specific embeddings. Subsequently, in the temporal set matching metric, we carry out the distance measure between query and support videos from a set matching perspective and design a bidirectional Mean Hausdorff Metric to improve the resilience to misaligned instances. Furthermore, we extend the proposed HyRSM++ to deal with the more challenging semi-supervised few-shot action recognition and unsupervised few-shot action recognition tasks. Experimental results on multiple benchmarks demonstrate that our method consistently outperforms existing methods and achieves state-of-the-art performance under various few-shot settings. The source code is available at https://github.com/alibaba-mmai-research/HyRSMPlusPlus.}
}
@article{MBOUOPDA2024110121,
title = {Scalable and accurate subsequence transform for time series classification},
journal = {Pattern Recognition},
volume = {147},
pages = {110121},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110121},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300818X},
author = {Michael Franklin Mbouopda and Engelbert {Mephu Nguifo}},
keywords = {Time series, Classification, Shapelet, Scalability, Interpretability},
abstract = {Time series classification using phase-independent subsequences called shapelets is one of the best approaches in the state of the art. This approach is especially characterized by its interpretable property and its fast prediction time. However, given a dataset of n time series of length at most m, learning shapelets requires a computation time of O(n2m4) which is too high for practical datasets. In this paper, we exploit the fact that shapelets are shared by the members of the same class to propose the SAST (Scalable and Accurate Subsequence Transform) algorithm which has a time complexity of O(nm3). SAST is accurate, interpretable and does not learn redundant shapelets. The experiments we conducted on the UCR archive datasets showed that SAST is more accurate than the state of the art Shapelet Transform algorithm while being significantly more scalable.}
}
@article{LIU2024110081,
title = {Learning implicit labeling-importance and label correlation for multi-label feature selection with streaming labels},
journal = {Pattern Recognition},
volume = {147},
pages = {110081},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110081},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007781},
author = {Jinghua Liu and Wei Wei and Yaojin Lin and Lijie Yang and Hongbo Zhang},
keywords = {Multi-label feature selection, Streaming labels, Labeling-importance, Label correlation},
abstract = {Multi-label feature selection plays an increasingly important role in alleviating the high dimensionality of multi-label learning tasks. Most extant methods posit that the learning task is performed in an environment where the label space is statically known. In reality, however, the environment is open and the labels may arrive dynamically, which is coined as streaming labels. Streaming labels-based multi-label feature selection suffers from many challenges derived from label space: (1) The label space expands dynamically; (2) Newly arrived labels exhibit complex relationships, often involving label correlation and labeling-importance. To cope with this challenge, in this paper, an intuitive yet effective algorithm named LLSL, i.e. learning implicit labeling-importance and label correlation for multi-label feature selection with streaming labels, is proposed. To be specific, the implicit labeling-importance with respect to streaming labels is firstly formalized by conducting the nearest neighbor reconstruction on feature space. Secondly, label correlation is seamlessly integrated into the objective function of feature relevance by designing the feature relevance influence factor. Based on the above, we build a feature conversion, which can realize the fusion of label-specific features for each streaming label. Finally, extensive experiments conducted on fifteen benchmark datasets provide clear evidence that LLSL has superior performance compared to three established streaming label-based MFS algorithms and seven static label space-based MFS algorithms.}
}
@article{SU2024110150,
title = {Nonmonotone variable projection algorithms for matrix decomposition with missing data},
journal = {Pattern Recognition},
volume = {148},
pages = {110150},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110150},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008476},
author = {Xiang-xiang Su and Min Gan and Guang-yong Chen and Lin Yang and Jun-wei Jin},
keywords = {Low-rank matrix factorization, Missing data, Damped wiberg method, Variable projection},
abstract = {This paper investigates algorithms for matrix factorization when some or many components are missing, a problem that arises frequently in computer vision and pattern recognition. We demonstrate that the Jacobian used in the damped Wiberg (DW) method is exactly the same as that of Kaufman’s simplified variable projection (VP) algorithm. Our analysis provides a novel perspective on the efficiency of VP algorithms by improving the strong convexity of the approximate function. To enhance numerical stability, we set a lower bound on the damping parameter instead of adding a null space like the DW algorithm. Another challenge of low-rank matrix decomposition with missing data is the existence of many sharp local minima, which are often distributed in narrow valleys of the landscape of objection functions. Falling into such minima tends to result in poor reconstruction results. To address this issue, we design a non-monotonic VP algorithm, which can facilitate the algorithm to escape from sharp minima and converge to flatter minima. Numerical experiments confirm the effectiveness and efficiency of the proposed nonmonotone VP algorithm.}
}
@article{LONG2024110178,
title = {Interpretable multidisease diagnosis and label noise detection based on a matching network and self-paced learning},
journal = {Pattern Recognition},
volume = {148},
pages = {110178},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110178},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008750},
author = {Jiawei Long and Jiangtao Ren},
keywords = {Multidisease diagnosis, Electronic medical record, Label noise detection, Self-paced learning, Interpretability of deep learning},
abstract = {With the extensive use of information systems in hospitals, a large quantity of electronic medical record data has been accumulated, which makes it possible to train clinical decision support systems based on the data. However, electronic medical records are written by doctors of different levels, which easily introduces label noise into the datasets. The lack of interpretability of current auxiliary diagnosis methods is another problem. To address these challenges, we introduce a matching network based on medical guidelines and build an auxiliary diagnosis model based on self-paced learning. The matching network based on guidelines can provide medical knowledge beyond medical records and a certain degree of interpretability. Additionally, self-paced learning can help the model identify the label noise and prevent the model from being misled. The experiments show that our method outperforms the baselines in a Chinese medical multi-disease diagnosis dataset and the MIMIC-III dataset and has good performance in the label noise detection task.}
}
@article{JI2024110126,
title = {Structure correspondence searching of CAD model using local feature-based description and indexing},
journal = {Pattern Recognition},
volume = {148},
pages = {110126},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110126},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008233},
author = {Baoning Ji and Jie Zhang and Yuan Li and Wenbin Tang},
keywords = {CAD model, Structure correspondence, Model segmentation, Local feature, Indexing mechanism},
abstract = {The CAD model retrieval has played a significant role in various applications, including product development and knowledge mining. However, most existing retrieval methods compare 3D shape similarity from a global perspective, while detecting similar structures automatically for CAD models remains a challenging problem. Consequently, this study proposes a structure correspondence searching framework for CAD models to address the issues. According to the boundary representation (B-rep) information, the proposed method first segments a CAD model into a set of local features denoted as structural cells. Then, the descriptor of each structural cell is extracted using a weighted shape distribution vector and neighbor set. In order to speed up the matching of structural cells, an indexing and filtering mechanism is constructed based on the shape clustering and topological analysis. The matched structural cells determine the boundary of similar structures. Finally, similarity measurement is conducted to generate a ranking list by analyzing the quality of the matched structural cells. The rationality and efficiency of the proposed approach are demonstrated via an analysis of experimental results.}
}
@article{RIBERO2024110122,
title = {Reducing communication in federated learning via efficient client sampling},
journal = {Pattern Recognition},
volume = {148},
pages = {110122},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110122},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008191},
author = {Mónica Ribero and Haris Vikalo},
keywords = {Federated learning, Machine learning, Distributed optimization},
abstract = {Federated learning (FL) ameliorates privacy concerns in settings where a central server coordinates learning from data distributed across many clients; rather than sharing the data, the clients train locally and report the models they learn to the server. Aggregation of local models requires communicating massive amounts of information between the clients and the server, consuming network bandwidth. We propose a novel framework for updating the global model in communication-constrained FL systems by requesting input only from the clients with informative updates, and estimating the local updates that are not communicated. Specifically, describing the progression of the model’s weights by an Ornstein–Uhlenbeck process allows us to develop sampling strategy for selecting a subset of clients with significant weight updates; model updates of the clients not selected for communication are replaced by their estimates. We test this policy on realistic federated benchmark datasets and show that the proposed framework provides up to 50% reduction in communication while maintaining competitive or achieving superior performance compared to baselines. The proposed method represents a new line of strategies for communication-efficient FL that is orthogonal to the existing user-driven techniques, such as compression, thus complementing rather than aiming to replace those existing methods.}
}
@article{MA2024110154,
title = {Discriminative multi-label feature selection with adaptive graph diffusion},
journal = {Pattern Recognition},
volume = {148},
pages = {110154},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110154},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008518},
author = {Jiajun Ma and Fei Xu and Xiaofeng Rong},
keywords = {Multi-label learning, Feature selection, Adaptive graph diffusion, Sparse regularization},
abstract = {Feature selection can alleviate the problem of the curse of dimensionality by selecting more discriminative features, which plays an important role in multi-label learning. Recently, embedded feature selection methods have received increasing attentions. However, most existing methods learn the low-dimensional embeddings under the guidance of the local structure between the original instance pairs, thereby ignoring the high-order structure between instances and being sensitive to noise in the original features. To address these issues, we propose a feature selection method named discriminative multi-label feature selection with adaptive graph diffusion (MFS-AGD). Specifically, we first construct a graph embedding learning framework equipped with adaptive graph diffusion to uncover a latent subspace that preserves the higher-order structure information between four tuples. Then, the Hilbert–Schmidt independence criterion (HSIC) is incorporated into the embedding learning framework to ensure the maximum dependency between the latent representation and labels. Benefiting from the interactive optimization of the feature selection matrix, latent representation and similarity graph, the selected features can accurately explore the higher-order structural and supervised information of data. By further considering the correlation between labels, MFS-AG is extended to a more discriminative version,i.e., LMFS-AG. Extensive experimental results on various benchmark data sets validate the advantages of the proposed MFS-AGD and LMFS-AGD methods.}
}
@article{YANG2024110073,
title = {Residual shape adaptive dense-nested Unet: Redesign the long lateral skip connections for metal surface tiny defect inspection},
journal = {Pattern Recognition},
volume = {147},
pages = {110073},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110073},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007707},
author = {Benyi Yang and Zhenyu Liu and Guifang Duan and Jianrong Tan},
keywords = {Machine vision, Metal surface defect inspection, Dense-nested Unet, Residual Shape Adaptive module},
abstract = {The state-of-the-art metal surface defect inspection methods have two problems: (1) they are sensitive to tiny defects because of their extreme small sizes, and (2) they cannot accurately locate the random appeared defects whose semantic relationship with the background context is weak. To solve these problems, Residual Shape Adaptive Dense-nested Unet, a pixel-based defect inspection method is proposed, to obtain the exact shape and location of the defect, by (1) assembling different depth Unet branches with dense skip connections as the feature extractor to combine multi-semantic level visual features; (2) adding Residual Shape Adaptive modules on the dense skip connections to help the model locate the defect regions; and (3) introducing the multi-branch training method which enables model pruning to reduce redundant parameters and accelerate the inspection speed. Experiments are conducted and demonstrated that the Residual Shape Adaptive Dense-nested Unet achieves the best performance among the state-of-the-art defect inspection methods.}
}
@article{DAVASHI2024110166,
title = {IME: Efficient list-based method for incremental mining of maximal erasable patterns},
journal = {Pattern Recognition},
volume = {148},
pages = {110166},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110166},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008634},
author = {Razieh Davashi},
keywords = {Erasable pattern mining, Maximal erasable patterns, Incremental mining, Dynamic data},
abstract = {Erasable pattern mining can help factories facing a financial crisis increase productivity by identifying and eliminating unprofitable products. The Flag-GenMax-EI algorithm extracts Maximal Erasable Itemsets (MEIs); however, it does not support dynamic data. In practice, many applications create databases incrementally. Using the Flag-GenMax-EI algorithm to mine maximal erasable patterns from incremental databases is clearly very costly because it must be run each time. In this paper, an efficient method called IME is proposed for incremental mining of maximal erasable patterns. IMEI-List and IMEP-List are two new data structures introduced by the proposed method. These lists allow the algorithm to update all tree nodes without rescanning the updated database (original database + new database) and recreating the nodes. This is the first study of incremental mining of maximal erasable patterns. Extensive experimental results on dense and sparse incremental data show that the proposed algorithm improves scalability. It extracts MEIs much faster than the Flag-GenMax-EI algorithm in different modes of database update.}
}
@article{REN2024110114,
title = {Self-supervised video distortion correction algorithm based on iterative optimization},
journal = {Pattern Recognition},
volume = {148},
pages = {110114},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110114},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008117},
author = {Zhihao Ren and Ya Su},
keywords = {Distortion rectification, Iterative optimization, Frames alignment, Affine transformation},
abstract = {Wide-angle video frames typically contain more information, but they also exhibit distortion that degrades the visual quality, especially at the edges. To eliminate this distortion from videos, we propose a self-supervised iterative optimization method in this paper. Specifically, we construct a motion parameter estimation model utilizing two consecutive distorted frames, where motion parameters comprise affine transform and distortion parameters. We apply the Gauss–Newton algorithm to minimize the sum-of-squares error between frames and update parameters. Treating inter-frame motion as undistort-affine-distort transformations, frame alignment is achieved by continuously adjusting transform parameters. Ultimately, frames are corrected using the converged parameters. We generated a synthetic dataset with various distortion parameters for evaluation. Experiments demonstrate superior performance versus state-of-the-art methods on synthetic and real wide-angle videos. Our algorithm also achieves higher parameter estimation accuracy without sacrificing efficiency.}
}
@article{CROOK2024110080,
title = {A linear transportation Lp distance for pattern recognition},
journal = {Pattern Recognition},
volume = {147},
pages = {110080},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110080},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300777X},
author = {Oliver M. Crook and Mihai Cucuringu and Tim Hurst and Carola-Bibiane Schönlieb and Matthew Thorpe and Konstantinos C. Zygalakis},
keywords = {Optimal transport, Linear embedding, Multi-channelled signals},
abstract = {The transportation Lp distance, denoted TLp, has been proposed as a generalisation of Wasserstein Wp distances motivated by the property that it can be applied directly to colour or multi-channelled images, as well as multivariate time-series without normalisation or mass constraints. Both TLp and Wp assign a cost based on the transport distance (i.e. the “Lagrangian” model), the key difference between the distances is that TLp interprets the signal as a function whilst Wp interprets the signal as a measure. Both distances are powerful tools in modelling data with spatial or temporal perturbations. However, their computational cost can make them infeasible to apply to even moderate pattern recognition tasks. The linear Wasserstein distance was proposed as a method for projecting signals into a Euclidean space where the Euclidean distance is approximately the Wasserstein distance (more formally, this is a projection on to the tangent manifold). We propose linear versions of the TLp distance (LTLp) and we show significant improvement over the linear Wp distance on signal processing tasks, whilst being several orders of magnitude faster to compute than the TLp distance.}
}
@article{GAO2024110190,
title = {TSVT: Token Sparsification Vision Transformer for robust RGB-D salient object detection},
journal = {Pattern Recognition},
volume = {148},
pages = {110190},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110190},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008877},
author = {Lina Gao and Bing Liu and Ping Fu and Mingzhu Xu},
keywords = {Salient object detection, RGB-D image, Self-attention mechanism, Vision transformer, Token sparsification},
abstract = {Visual transformer-based salient object detection (SOD) models have attracted increasing research attention. However, the existing transformer-based RGB-D SOD models usually operate on the full token sequences of RGB-D images and use an equal tokenization process to treat appearance and depth modalities, which leads to limited feature richness and inefficiency. To address these limitations, we present a novel token sparsification vision transformer architecture for RGB-D SOD, named TSVT, that explicitly extracts global-local multi-modality features with sparse tokens. The TSVT is an asymmetric encoder–decoder architecture with a dynamic sparse token encoder that adaptively selects and operates on sparse tokens, along with an multiple cascade aggregation decoder (MCAD) that predicts saliency results. Furthermore, we deeply investigate the differences and similarities between the appearance and depth modalities and develop an interactive diversity fusion module (IDFM) to integrate each pair of multi-modality tokens in different stages. Finally, to comprehensively evaluate the performance of the proposed model, we conduct extensive experiments on seven standard RGB-D SOD benchmarks in terms of five evaluation metrics. The experimental results reveal that the proposed model is more robust and effective than fifteen existing RGB-D SOD models. Moreover, the complexity of our model with the sparsification module is more than two times lower than that of the variant model without the dynamic sparse token module (DSTM).}
}
@article{WANG2024110085,
title = {Feature specific progressive improvement for salient object detection},
journal = {Pattern Recognition},
volume = {147},
pages = {110085},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110085},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007823},
author = {Xianheng Wang and Zhaobin Liu and Veronica Liesaputra and Zhiyi Huang},
keywords = {Salient object detection, Fully convolutional neural network, Level-specific feature extraction, Progressive refinement of saliency},
abstract = {Benefiting from deep learning, Salient Object Detection (SOD) has made much progress. However, most existing methods adopt the same strategy to extract salient cues from different feature levels without fully considering their differences in the feature extraction stage and/or suffer from the accumulation of noise and dilution of spatial details in the feature fusion stage. These two problems hinder the further improvement in performance. In this paper, we propose an effective SOD model, PiNet, which can address the above problems via two novel mechanisms in the network: level-specific feature extraction and progressive refinement of saliency. We have designed the customized feature extraction components for each level of features—enabling us to extract better saliency cues from multi-level features. The saliency feature refinement in the branches follows a coarse-to-fine process, where the refined features progressively contain more location cues, internal and boundary details. Through short connections, the extracted saliency cues in different branches are selectively transmitted and integrated, which well mitigates the accumulation of noisy information and the dilution of detailed information. By using four different backbones, we verify our model has good adaptability and can make accurate saliency predictions under different pretrained models. Extensive experiments on five public datasets demonstrate that PiNet outperforms 19 state-of-the-art (SOTA) methods in SOD, with its small model size (56.1 MB) and fast inference speed (47 FPS).}
}
@article{LIU2024110131,
title = {Transformer-based stroke relation encoding for online handwriting and sketches},
journal = {Pattern Recognition},
volume = {148},
pages = {110131},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110131},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008282},
author = {Jing-Yu Liu and Yan-Ming Zhang and Fei Yin and Cheng-Lin Liu},
keywords = {Online stroke classification, Handwritten document analysis, Diagram recognition, Sketch semantic segmentation, Position encoding in transformer},
abstract = {Stroke classification for online handwriting and sketches, aimed at grouping strokes into different semantic categories, has drawn considerable attention due to its wide applications. This task is challenging since individual strokes look similar and are easily confused with each other. The key is to consider both the individual strokes and the contextual information jointly for making prediction. Previous methods are insufficient in modeling and exploiting complex contextual information of strokes. To overcome this limitation, we propose a Transformer-based model for Online Handwriting and Sketches (T-OHS), with novel relation encoding schemes to take advantage of temporal and spatial information in stroke sequence. Particularly, we introduce a coarse-to-fine hierarchical encoding approach based on the polar coordinate system for precisely modeling spatial relations between strokes. Experiments on three types of handwriting data, including online handwritten documents, diagrams, and sketches, demonstrate that our method is universal and provides state-of-the-art performance.}
}
@article{CASTRO2024110171,
title = {AttenGait: Gait recognition with attention and rich modalities},
journal = {Pattern Recognition},
volume = {148},
pages = {110171},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110171},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008683},
author = {Francisco M. Castro and Rubén Delgado-Escaño and Ruber Hernández-García and Manuel J. Marín-Jiménez and Nicolás Guil},
keywords = {Gait, Optical flow, Deep learning, Attention, Biometrics},
abstract = {Current gait recognition systems employ different types of manual attention mechanisms, like horizontal cropping of the input data to guide the training process and extract useful gait signatures for people identification. Typically, these techniques are applied using silhouettes as input, which limits the learning capabilities of the models. Thus, due to the limited information provided by silhouettes, state-of-the-art gait recognition approaches must use very simple and manually designed mechanisms, in contrast to approaches proposed for other topics such as action recognition. To tackle this problem, we propose AttenGait, a novel model for gait recognition equipped with trainable attention mechanisms that automatically discover interesting areas of the input data. AttenGait can be used with any kind of informative modalities, such as optical flow, obtaining state-of-the-art results thanks to the richer information contained in those modalities. We evaluate AttenGait on two public datasets for gait recognition: CASIA-B and GREW; improving the previous state-of-the-art results on them, obtaining 95.8% and 70.7% average accuracy, respectively. Code will be available at https://github.com/fmcp/attengait.}
}
@article{HE2024110119,
title = {Adversarial and focused training of abnormal videos for weakly-supervised anomaly detection},
journal = {Pattern Recognition},
volume = {147},
pages = {110119},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110119},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008166},
author = {Ping He and Fan Zhang and Gang Li and Huibin Li},
keywords = {Inter-video data imbalance problem, Weakly-supervised video anomaly detection, Adversarial training, Focused training},
abstract = {Due to the sparsity and scarcity of abnormal events, intra-video and inter-video data imbalance problems are fundamental issues for the weakly supervised video anomaly detection (WS-VAD) task. Many previous works have made great progress in the intra-video data imbalance problem while lacking attention to the inter-video case. However, we find that when reducing the number of abnormal videos used for training, the performance of some existing state-of-the-art WS-VAD methods will be decreased. To alleviate this problem, we propose a novel solution by adversarial and focused training (AFT) of abnormal videos. Specifically, our solution consists of two modules. One is a data-based adversarial training (AT) module that performs data augmentation through latent space-based adversarial sample generation of abnormal videos, and the other is a model-based focused training (FT) module that focuses on the cost-sensitive loss of abnormal videos. Once the whole pipeline has been trained, a score-level late fusion strategy is employed to combine the abnormal scores of both adversarial training and focused training modules in the testing phase. The effectiveness of the proposed approach is demonstrated on UCF-Crime, ShanghaiTech, XD-Violence, and UCSD Peds datasets in both the inter-video data imbalanced experimental setting and the original experimental setting. The source code is available at: https://github.com/Destind/AFT_codes.}
}
@article{LI2024110147,
title = {From patch, sample to domain: Capture geometric structures for few-shot learning},
journal = {Pattern Recognition},
volume = {148},
pages = {110147},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110147},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008440},
author = {Qiaonan Li and Guihua Wen and Pei Yang},
keywords = {Cross-domain, Few-shot learning, Optimal transport},
abstract = {Few-shot learning aims to recognize novel concepts with only few samples by using prior knowledge learned from the seen concepts. In this paper, we address the problem of few-shot learning under domain shifts. Traditional few-shot learning methods are not directly applicable to cross-domain scenarios due to the large discrepancy of feature distributions across domains. To this end, we propose a novel Hierarchical Optimal Transport network with Attention (HOTA) for cross-domain few-shot learning. The underlying idea is to learn the transferable and discriminative embeddings by taking advantage of the hierarchical geometric structures among image data, ranging from patch, sample to domain. The HOTA framework utilizes a hierarchical optimal transport network to smooth the domain shifts by domain alignment while enhancing the discrimination and the transferability of the embeddings by aligning the patches of images. To further enhance the transferability, HOTA conducts a mix-up data augmentation based on cross-domain attention to capture the relationships of samples in different domains. The extensive experiments on a variety of few-shot benchmark scenarios demonstrate that HOTA outperforms the state-of-the-art methods under both supervised and unsupervised conditions.}
}
@article{DAI2024110094,
title = {OAMatcher: An overlapping areas-based network with label credibility for robust and accurate feature matching},
journal = {Pattern Recognition},
volume = {147},
pages = {110094},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110094},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007914},
author = {Kun Dai and Tao Xie and Ke Wang and Zhiqiang Jiang and Ruifeng Li and Lijun Zhao},
keywords = {Local feature matching, Transformer},
abstract = {Local feature matching involves establishing accurate pixel-wise correspondences between an image pair, which is a critical component in several visual applications (e.g., visual localization). Recently, detector-free techniques have realized excellent performance in this task. However, existing methods tend to focus on the entire image without prioritizing overlapping regions, resulting in undesirable interference from non-overlapping areas during the descriptors enhancement process. Moreover, these approaches neglect unreliable ground-truth matching labels triggered by measurement noise in datasets, leading to sub-optimal network optimization. In this study, we develop a novel overlapping areas-based network OAMatcher to resolve these issues. For the first issue, OAMatcher employs an overlapping regions perception block (ORPB) that captures the overlapping areas of image pairs to filter out plentiful mismatches and circumvent interference from non-overlapping regions during descriptors enhancement process. Specifically, the ORPB first enhances the descriptors of all keypoints to mimic the human behaviour of scrutinizing entire images back and forth at the start of feature matching. Subsequently, the ORPB introduces an overlapping regions extraction block (OREB) that captures the keypoints within overlapping zones to mimic the humans behaviour of shifting the focus from the whole images to co-visible areas. After OREB, ORPB performs descriptors enhancement exclusively among the keypoints within these co-visible regions, ensuring minimal disturbances from non-overlapping areas. In addition, the ORPB confines the predicted matches strictly to co-visible regions, thus efficiently filtering out a significant number of mismatches in non-overlapping zones. For the second issue, OAMatcher proposes a labels weighting algorithm (LWA) that predicts the label credibility for ground-truth matching labels. LWA assigns low credibility to unreliable labels and utilizes the credibility to weight loss, effectively diminishing the influence of unreliable labels. Extensive experiments show that OAMatcher delivers excellent results for homography estimation, pose estimation, and visual localization tasks.}
}
@article{NOOR2024110135,
title = {H-CapsNet: A capsule network for hierarchical image classification},
journal = {Pattern Recognition},
volume = {147},
pages = {110135},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110135},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008324},
author = {Khondaker Tasrif Noor and Antonio Robles-Kelly},
keywords = {Capsule networks, Hierarchical image classification, Convolutional neural networks, Deep learning},
abstract = {In this paper, we present H-CapsNet, a capsule network for hierarchical image classification. Our network makes use of the natural capacity of CapsNets (capsule networks) to capture hierarchical relationships. Thus, our network is such that each multi-layer capsule network accounts for each of the class hierarchies using dedicated capsules. Further, we make use of a modified hinge loss that enforces consistency amongst the hierarchies involved. We also present a strategy to dynamically adjust the training parameters to achieve a better balance between the class hierarchies under consideration. We have performed experiments using several widely available datasets and compared them against several alternatives. In our experiments, H-CapsNet delivers a margin of improvement over competing hierarchical classification networks elsewhere in the literature.}
}
@article{CHEN2024110090,
title = {Gait feature learning via spatio-temporal two-branch networks},
journal = {Pattern Recognition},
volume = {147},
pages = {110090},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110090},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007872},
author = {Yifan Chen and Xuelong Li},
keywords = {Gait recognition, Spatio-temporal gait feature, Convolutional neural networks},
abstract = {Gait recognition has become a mainstream technology for identification due to its ability to capture gait features over long distances without subject cooperation and resistance to camouflage. However, current gait recognition methods face challenges as they use a single network to extract both temporal and spatial features from gait sequences. This approach imposes a heavy burden on the network, resulting in reduced extraction efficiency. To solve this problem, we propose a two-branch network to extract the spatio-temporal features of gait sequences. One branch primarily focuses on spatial feature extraction, while the other concentrates on temporal feature extraction. This design can make one branch focus on a specific task, leading to significant performance improvements. For temporal feature extraction, we propose the Global Temporal Information Extraction Network (GTIEN). GTIEN extracts temporal features of gait sequences by sequentially exploring the relationship between adjacent gait silhouettes from pixel and block levels. For spatial feature extraction, we introduce the Selective Horizontal Pyramid Convolution Network (SHPCN). SHPCN explores the multi-granularity features of gait silhouettes from global and local perspectives and assigns them appropriate weights according to their importance. By reasonably combining the temporal features extracted from GTIEN and spatial features extracted from SHPCN, we can effectively learn the spatial–temporal information of the gait sequences. Extensive experiments on CASIA-B and OUMVLP demonstrate that our method has better performance than some state-of-the-art methods.}
}
@article{GONZALEZSABBAGH2024110159,
title = {DGD-cGAN: A dual generator for image dewatering and restoration},
journal = {Pattern Recognition},
volume = {148},
pages = {110159},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110159},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008567},
author = {Salma Gonzalez-Sabbagh and Antonio Robles-Kelly and Shang Gao},
keywords = {Underwater image restoration, Generative adversarial network, Deep learning},
abstract = {Underwater images are usually covered with a blue–greenish colour cast, making them distorted, blurry or low in contrast. This phenomenon occurs due to the light attenuation given by the scattering and absorption in the water column. In this paper we present an image dewatering approach motivated upon the observation that the image formation model can be used to drive the learning process by constraining the loss function and making used of paired data. To this end, we employ a conditional generative adversarial network (cGAN) with two generators. Our Dual Generator Dewatering cGAN (DGD-cGAN) removes the haze and colour cast induced by the water column and restores the true colours of underwater scenes whereby the effects of various attenuation and scattering phenomena that occur in underwater images are tackled by the two generators. The first generator takes as input the underwater image and predicts the dewatered scene, while the second generator learns the underwater image formation process by implementing a custom loss function based upon the transmission and the veiling light components of the image formation model. Extensive experiments show that DGD-cGAN consistently delivers a margin of improvement as compared with state-of-the-art methods on several widely available datasets.}
}
@article{WANG2024110123,
title = {Kinship similarity for open sets},
journal = {Pattern Recognition},
volume = {148},
pages = {110123},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110123},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008208},
author = {Wei Wang and Shaodi You and Sezer Karaoglu and Theo Gevers},
keywords = {Kinship similarity measurement, Kinship recognition, Open set},
abstract = {The aim of image-based kinship recognition methods is to determine the genetic relationship between people from their face images. Kinship recognition and related methods have many applications in computer vision. Most of the methods are very meaningful but are focused on closed sets. However, for real-life scenarios, kinship recognition is an open set problem. In contrast to previous methods, this paper considers kinship recognition for open sets. Further, the aim is to determine family relationships and their corresponding degrees of kinship. Our method is pairwise-based and is able to exploit mutual information from positive pairs. Large scale experiments and ablation studies show that our method (1) reaches SOTA performance on the FIW dataset, (2) is able to properly separate kinship categories using pairwise similarity and (3) generates uniform similarity distributions.}
}
@article{ZHAO2024110132,
title = {HairManip: High quality hair manipulation via hair element disentangling},
journal = {Pattern Recognition},
volume = {147},
pages = {110132},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110132},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008294},
author = {Huihuang Zhao and Lin Zhang and Paul L. Rosin and Yu-Kun Lai and Yaonan Wang},
keywords = {Hair editing networks, Image generation, Hair manipulation, Generative adversarial networks, Deep learning},
abstract = {Hair editing is challenging due to the complexity and variety of hair materials and shapes. Existing methods employ reference images or user-painted masks to edit hair and have achieved promising results. However, discrepancies in color and shape between the source and target hair can occasionally result in unrealistic results. Therefore, we propose a new hair editing method named HairManip, which decouples the hair information from the input source image into shape and color components. We then train hairstyle and hair color editing sub-networks to handle this complex information independently. To further enhance editing efficiency and accuracy, we introduce a latent code preprocessing module that effectively extracts meaningful features from hair regions, thereby improving the model’s editing capabilities. The experimental results demonstrate that our method achieves significant results in editing accuracy and authenticity, thanks to the carefully designed network structure and loss functions. Code can be found at https://github.com/Zlin0530/HairManip.}
}
@article{SLIMANI2024110108,
title = {RoCNet++: Triangle-based descriptor for accurate and robust point cloud registration},
journal = {Pattern Recognition},
volume = {147},
pages = {110108},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110108},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008051},
author = {Karim Slimani and Catherine Achard and Brahim Tamadazte},
keywords = {Point cloud learning, Registration, Geometric descriptor, Attention mechanism, Pose estimation},
abstract = {This paper introduces RoCNet++, a point cloud registration method with two main contributions, one concerning the design of a robust descriptor and another concerning the estimation of the rigid transformation. First, to robustly capture the local geometric properties of the surface, i.e., each point is characterized by all the triangles formed by itself and its nearest neighbours in the 3D point cloud. The idea is to assist the learning of the descriptor by introducing a priori information about interesting geometric properties such as the invariance of triangle angles under rigid transformations. This local triangle-based descriptor is integrated into the recently developed RoCNet architecture for estimating the correspondences between source and target point clouds. We then introduce the Farthest Sampling-guided Registration (FSR), which relies on successive farthest point samplings to estimate the global rigid transformation between 3D point clouds. The new proposed architecture RoCNet++ has been evaluated in different configurations: clean, noisy and partial data on both synthetic and real databases such as ModelNet40, KITTI, and 3DMatch. RoCNet++ shows improved performances on these benchmark datasets in favourable and unfavourable conditions. Furthermore, both the local triangle-based descriptor and the Farthest Sampling-guided Registration (FSR) can be used in other registration algorithms.}
}
@article{ZHU2024110148,
title = {Improved channel attention methods via hierarchical pooling and reducing information loss},
journal = {Pattern Recognition},
volume = {148},
pages = {110148},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110148},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008452},
author = {Meng Zhu and Weidong Min and Junwei Han and Qing Han and Shimiao Cui},
keywords = {Convolutional neural networks, Channel attention, Pooling, Reducing information loss, Information encoding},
abstract = {Channel attention has been demonstrated to improve performance of convolutional neural networks. Most existing channel attention methods lower channel dimension for reducing computational complexity. However, the dimension reduction causes information loss, thus resulting in performance loss. To alleviate the paradox of complexity and performance trade-off, we propose two novel channel attention methods named Grouping-Shuffle-Aggregation Channel Attention (GSACA) method and Mixed Encoding Channel Attention (MECA) method, respectively. Our GSACA method partitions channel variables into several groups and performs the independent matrix multiplication without the dimension reduction to each group. Our GSACA method enables interaction between all groups using a ”channel shuffle” operator. After these, our GSACA method performs the independent matrix multiplication each group again and aggregates all channel correlations. Our MECA method encodes channel information through dual path architectures to benefit from both path topology where one uses the multilayer perception with dimension reduction to encode channel information and the other uses channel information encoding method without dimension reduction. Furthermore, a novel pooling operator named hierarchical pooling is presented and applied to our GSACA and MECA methods. The experimental results showed that our GSACA method almost consistently outperformed most existing channel attention methods and that our MECA method consistently outperformed the existing channel attention methods.}
}
@article{XIE2024110172,
title = {GhostFormer: Efficiently amalgamated CNN-transformer architecture for object detection},
journal = {Pattern Recognition},
volume = {148},
pages = {110172},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110172},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008695},
author = {Xin Xie and Dengquan Wu and Mingye Xie and Zixi Li},
keywords = {Object detection, Lightweight network design, Feature extraction, CNN-transformer},
abstract = {The lightweight network model has gradually evolved into an important research direction in object detection. Network lightweight design has a variety of research methods, such as quantization, knowledge distillation, and neural architecture search. However, these methods either fail to break through the performance bottleneck of the model itself or require massive training costs. In order to solve these problems, a new object detection model based on CNN-Transformer hybrid feature extraction network called GhostFormer is proposed from the perspective of lightweight network structure design. GhostFormer makes full use of the advantages of local modeling of CNN and global modeling of Transformer, not only effectively reducing the complexity of the convolution model but also breaking through the limitation of Transformer’s lack of inductive bias. Finally, better transfer results are obtained in downstream tasks. Experiments show that the model is less than half as computationally expensive as YOLOv7 on the Pascal VOC dataset, with only about 3 % mAP@0.5 loss, and 9.7% mAP@0.5:0.95 improvement on the MS COCO dataset compared with GhostNet.}
}
@article{SHAMSOLMOALI2024110120,
title = {Distance-based Weighted Transformer Network for image completion},
journal = {Pattern Recognition},
volume = {147},
pages = {110120},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110120},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008178},
author = {Pourya Shamsolmoali and Masoumeh Zareapoor and Huiyu Zhou and Xuelong Li and Yue Lu},
keywords = {Generative network, Attention network, Image completion},
abstract = {The challenge of image generation has been effectively modeled as a problem of structure priors or transformation. However, existing models have unsatisfactory performance in understanding the global input image structures because of particular inherent features (for example, local inductive prior). Recent studies have shown that self-attention is an efficient modeling technique for image completion problems. In this paper, we propose a new architecture that relies on Distance-based Weighted Transformer (DWT) to better understand the relationships between an image’s components. In our model, we leverage the strengths of both Convolutional Neural Networks (CNNs) and DWT blocks to enhance the image completion process. Specifically, CNNs are used to augment the local texture information of coarse priors and DWT blocks are used to recover certain coarse textures and coherent visual structures. Unlike current approaches that generally use CNNs to create feature maps, we use the DWT to encode global dependencies and compute distance-based weighted feature maps, which substantially minimizes the problem of visual ambiguities. Meanwhile, to better produce repeated textures, we introduce Residual Fast Fourier Convolution (Res-FFC) blocks to combine the encoder’s skip features with the coarse features provided by our generator. Furthermore, a simple yet effective technique is proposed to normalize the non-zero values of convolutions, and fine-tune the network layers for regularization of the gradient norms to provide an efficient training stabilizer. Extensive quantitative and qualitative experiments on three challenging datasets demonstrate the superiority of our proposed model compared to existing approaches.}
}
@article{YAN2024110095,
title = {KGSR: A kernel guided network for real-world blind super-resolution},
journal = {Pattern Recognition},
volume = {147},
pages = {110095},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110095},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007926},
author = {Qingsen Yan and Axi Niu and Chaoqun Wang and Wei Dong and Marcin Woźniak and Yanning Zhang},
keywords = {Blind super-resolution, Kernel estimation, Discriminator, Unsupervised learning, Non-ideal degradation},
abstract = {In recent years, deep learning-based methods have emerged as dominant players in the field of super-resolution (SR), owing to their exceptional reconstruction performance. The primary driver of their effectiveness lies in their utilization of extensive sets of paired low-resolution and high-resolution images for training deep learning models. This training enables the models to effectively replicate the intricate mapping relationship between low-resolution and high-resolution images. Nevertheless, at present, acquiring a sufficient quantity of such image pairs that satisfy the requirements remains a formidable obstacle. Therefore, in order to break the restriction of limited training sets, self-supervised learning has been introduced to train a model for each low-quality image, without requiring pairwise ground-truths. However, they generally presuppose the generation of low-resolution (LR) images from their high-resolution (HR) counterparts using a pre-defined kernel, such as Bicubic downscaling. Such an assumption is seldom valid for real-world LR images, where degradation processes in practical applications are diverse, intricate, and often undisclosed. Therefore, when the presumed downscaling kernel does not match the actual one, the outcomes of state-of-the-art approaches degrade substantially. In this paper, we introduce KGSR, a kernel-guided network for addressing real-world blind SR, effectively avoiding requiring large training image pairs and transforming the blind image super-resolution problem into a supervised learning and non-blind scenario. Specifically, KGSR trains two networks, namely Upscaling and Downscaling, utilizing only patches extracted from the input test image. On one hand, owing to the cross-scale recurrence property of the SR kernel within a single image, the Downscaling network acquires knowledge of the image-specific degradation process through a generative adversarial network. Consequently, the Downscaling network is capable of generating a downsampled version of the LR test image even when the acquisition process is unknown or less than ideal. Additionally, we employ a dedicated discriminator to compel the Downscaling network to prioritize the characterization of kernel orientations. Conversely, a precise blur kernel has the potential to yield superior performance. Guided by the accurate image-specific SR kernel acquired from the Downscaling network and the downsampled LR input, the Upscaling network is capable of producing a high-quality HR image from the LR input. Within the Upscaling network, we additionally introduce an effective module for harnessing the acquired image-specific SR kernel. KGSR operates as a fully unsupervised approach, yet it can concurrently produce both the image-specific SR kernel and high-quality HR images. Comprehensive experiments conducted on standard benchmarks validate the efficacy of the proposed approach compared to state-of-the-art methodologies. Moreover, the suggested method can deliver visually appealing SR outcomes while exhibiting shorter processing times when applied to real-world LR images.}
}
@article{TIAN2024110091,
title = {Learning a target-dependent classifier for cross-domain semantic segmentation: Fine-tuning versus meta-learning},
journal = {Pattern Recognition},
volume = {147},
pages = {110091},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110091},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007884},
author = {Haitao Tian and Shiru Qu and Pierre Payeur},
keywords = {Domain adaptation, Semantic segmentation, Unsupervised learning, Meta-learning},
abstract = {Recently proposed domain adaptation arts have dominated the field of cross-domain semantic segmentation by operating domain manifolds alignment and learning an optimal joint hypothesis (joint-domain classifier) for both source and target domains. However, a joint-domain classifier can still violate the cluster assumption in the target domain in case domain manifolds are not fully aligned after domain adaptation. In this work, we raise the intractability of perfect domain alignment and turn to exploit a novel hypothesis: a target-dependent classifier, to efficiently adapt to the target domain clusters even given a certain degree of domain misalignment. Specifically, we first propose an unsupervised fine-tuning strategy, which optimizes the joint hypothesis of vanilla domain adaptation into a target-dependent hypothesis to better fit with the target domain clusters. Second, we connect the “learning to learn” concept of meta-learning with pixel-wise domain adaptation, which serves as a reliable hypothesis initialization, providing an alternative solution to learning a more generalized target-dependent classifier. The proposed learning method is general to conventional domain adaptation models. In experiments, we recycle the pre-trained conventional DA models and learn target-dependent classifiers with the proposed method. Experimental results on synthetic-to-real adaptation and cross-city adaptation benchmarks demonstrate that the target-dependent classifier leads over state-of-the-art performance.}
}
@article{BAIK2024110107,
title = {DBN-Mix: Training dual branch network using bilateral mixup augmentation for long-tailed visual recognition},
journal = {Pattern Recognition},
volume = {147},
pages = {110107},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110107},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300804X},
author = {Jae Soon Baik and In Young Yoon and Jun Won Choi},
keywords = {Long-tailed visual recognition, Class imbalance, Image classification, Mixup augmentation, Temperature scaling},
abstract = {There is growing interest in the challenging visual perception task of learning from long-tailed class distributions. The extreme class imbalance in the training dataset biases the model to prefer recognizing majority class data over minority class data. Furthermore, the lack of diversity in minority class samples makes it difficult to find a good representation. In this paper, we propose an effective data augmentation method, referred to as bilateral mixup augmentation, which can improve the performance of long-tailed visual recognition. The bilateral mixup augmentation combines two samples generated by a uniform sampler and a re-balanced sampler and augments the training dataset to enhance the representation learning for minority classes. We also reduce the classifier bias using class-wise temperature scaling, which scales the logits differently per class in the training phase. We apply both ideas to the dual-branch network (DBN) framework, presenting a new model, named dual-branch network with bilateral mixup (DBN-Mix). Experiments on popular long-tailed visual recognition datasets show that DBN-Mix improves performance significantly over baseline and that the proposed method achieves state-of-the-art performance in some categories of benchmarks.}
}
@article{MOON2024110067,
title = {RoMP-transformer: Rotational bounding box with multi-level feature pyramid transformer for object detection},
journal = {Pattern Recognition},
volume = {147},
pages = {110067},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110067},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007641},
author = {Joonhyeok Moon and Munsu Jeon and Siheon Jeong and Ki-Yong Oh},
keywords = {Autonomous flight and surveillance, Artificial intelligence for object detection, Multi-level feature pyramid transformer, Multi-level multi-scale feature map, Pyramid vision transformer},
abstract = {This study proposes rotational bounding box with a multi-level feature pyramid transformer (RoMP-Transformer)—a fast and accurate one-stage deep neural network for object detection. The proposed RoMP-Transformer exhibits three characteristics. First, a rotational bounding box is utilized to minimize the effect of the background during the construction of feature maps, enhancing the robustness of the RoMP-Transformer. Second, the RoMP-Transformer employs a multi-level feature pyramid transformer by combining a multi-level feature pyramid network with a pyramid vision-transformer, effectively extracting high-quality features and achieving high accuracy. Third, the RoMP-Transformer executes bounding box optimization by minimizing the optimal intersection of union (IoU) loss by considering both the modified SKEW IoU and distance IoU. The modified SKEW IoU significantly accelerates the calculation, and the fused IoU calculation method improves prediction accuracy. Further, Bayesian optimization and weight lightening with half-tensor are performed to optimize the performance of the RoMP-Transformer for real-time applications. Experiments on three image sets—one on power transmission facilities, MSRA-TD500, and DOTA-v1.0—demonstrate that the proposed RoMP-Transformer outperforms other state-of-the-art neural networks in object detection in terms of accuracy, robustness, and calculation speed. Systematic analysis also reveals that the methods utilized by the RoMP-Transformer optimize object detection performance. The proposed architecture is expected to inspire further study of deep neural networks for object detection in real-world applications.}
}
@article{QIU2024110188,
title = {Multi-grained clip focus for skeleton-based action recognition},
journal = {Pattern Recognition},
volume = {148},
pages = {110188},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110188},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008853},
author = {Helei Qiu and Biao Hou},
keywords = {Action recognition, Skeleton, Multi-grain, Self-attention},
abstract = {Joint-level and part-level information are crucial for modeling actions with different granularity. In addition, the relevant information on different joints between consecutive frames is very useful for skeleton-based action recognition. To effectively capture the action information, a new multi-grained clip focus network (MGCF-Net) is proposed. Firstly, the skeleton sequence is divided into multiple clips, each containing several consecutive frames. According to the structure of the human body, each clip is divided into several tuples. Then an intra-clip attention module is proposed to capture intra-clip action information. Specifically, multi-head self-attention is divided into two parts, obtaining relevant information at the joint and part levels, and integrating the information captured from these two parts to obtain multi-grained contextual features. In addition, an inter-clip focus module is used to capture the key information of several consecutive sub-actions, which will help to distinguish similar actions. On two large-scale benchmarks for skeleton-based action recognition, our method achieves the most advanced performance, and its effectiveness has been verified.}
}
@article{BREGER2024110136,
title = {visClust: A visual clustering algorithm based on orthogonal projections},
journal = {Pattern Recognition},
volume = {148},
pages = {110136},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110136},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008336},
author = {Anna Breger and Clemens Karner and Martin Ehler},
keywords = {Clustering, Data mining, Stiefel manifold, Lower dimensional data representations, Projections},
abstract = {We present a novel clustering algorithm, visClust, that is based on lower dimensional data representations and visual interpretation. Thereto, we design a transformation that allows the data to be represented by a binary integer array enabling the use of image processing methods to select a partition. Qualitative and quantitative analyses measured in accuracy and an adjusted Rand-Index show that the algorithm performs well while requiring low runtime as well and RAM. We compare the results to 6 state-of-the-art algorithms with available code, confirming the quality of visClust by superior performance in most experiments. Moreover, the algorithm asks for just one obligatory input parameter while allowing optimization via optional parameters. The code is made available on GitHub and straightforward to use.}
}
@article{XIE2024110176,
title = {NODE-ImgNet: A PDE-informed effective and robust model for image denoising},
journal = {Pattern Recognition},
volume = {148},
pages = {110176},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110176},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008737},
author = {Xinheng Xie and Yue Wu and Hao Ni and Cuiyu He},
keywords = {Image denoising, NODE network, PDE learning},
abstract = {Inspired by the traditional partial differential equation (PDE) approach for image denoising, we propose a novel neural network architecture, referred as NODE-ImgNet, that combines neural ordinary differential equations (NODEs) with convolutional neural network (CNN) blocks. NODE-ImgNet is intrinsically a PDE model, where the dynamic system is learned implicitly without the explicit specification of the PDE. This naturally circumvents the typical issues associated with introducing artifacts during the learning process. By invoking such a NODE structure, which can also be viewed as a continuous variant of a residual network (ResNet) and inherits its advantage in image denoising, our model achieves enhanced accuracy and parameter efficiency. In particular, our model exhibits consistent effectiveness in different scenarios, including denoising gray and color images perturbed by Gaussian noise, as well as real-noisy images, and demonstrates superiority in learning from small image datasets.}
}
@article{YIN2024110117,
title = {MSA-GCN: Multiscale Adaptive Graph Convolution Network for gait emotion recognition},
journal = {Pattern Recognition},
volume = {147},
pages = {110117},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110117},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008142},
author = {Yunfei Yin and Li Jing and Faliang Huang and Guangchao Yang and Zhuowei Wang},
keywords = {Emotion recognition, Gait emotion recognition, Graph convolutional network, Multiscale mapping},
abstract = {Gait emotion recognition plays a crucial role in the intelligent system. Most existing approaches identify emotions by focusing on local actions over time. However, some valuable observational facts that the effective distances of different emotions in the time domain are different, and the local actions during walking are quite similar, are put aside in those methods. And this ignorance often ends up impairing performance of emotion recognition. To address the issues, a novel model, named MSA-GCN (MultiScale Adaptive Graph Convolution Network), is proposed to utilize the valuable observational knowledge for improving emotion recognition performance. In the proposed model, an adaptive spatio-temporal graph convolution is designed to dynamically select convolution kernels to learn the spatio-temporal features of different emotions. Moreover, a Cross-Scale Mapping Interaction mechanism (CSMI) is proposed to construct an adaptive adjacency matrix for high-quality aggregation of the multiscale information. Extensive experimental results on public datasets indicate that, compared with the state-of-the-art methods, the proposed approach achieves better performance in terms of emotion recognition accuracy, and shows the proposed approach is promising.}
}
@article{CHEN2024110072,
title = {A general elevating framework for label noise filters},
journal = {Pattern Recognition},
volume = {147},
pages = {110072},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110072},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007690},
author = {Qingqiang Chen and Gaoxia Jiang and Fuyuan Cao and Changqian Men and Wenjian Wang},
keywords = {Classification, Label noise, Noise filter, Sample reduction},
abstract = {In real applications, label noise has a great influence on data modeling. As one kind of label noise treatment method, noise filter has attracted extensive attention recently. The existing filters perform well in dealing with label noise completely at random (NCAR) but poorly in dealing with the label noise in the form of clusters (LLC). Besides, the existing filters may over remove the samples located at the classification boundaries, thereby affecting the generalization performance of classifiers. To fill these gaps, we propose a general elevating framework for label noise filters. The core idea of the framework is to improve the filtering performance of the existing filters by sample reduction. Specifically, since most of the existing filters are based on classifier prediction, and the complexity of samples at the boundary will affect the prediction performance of classifiers. Therefore, reducing the complexity of the boundary sample is very helpful to improve the performance of the filters. To this end, we propose a sample reduction method, which can not only reduce the complexity of the sample at the boundary but also convert LLC to NCAR, to get some representative samples. Next, the filter based on classifier prediction is employed to recognize the noisy representative samples. Finally, the noisy labeled samples in the given data set are found according to the identified noisy representatives. Furthermore, through empirical analysis, we found that compared with some classical metrics for evaluating the performance of noise filters, classification accuracy is more suitable to measure the performance of filters. Exhaustive experiments testify the validity of the framework, and the experimental results demonstrate that the performance of our framework is especially outstanding for LLC treatment.}
}
@article{FAN2024110141,
title = {Random epipolar constraint loss functions for supervised optical flow estimation},
journal = {Pattern Recognition},
volume = {148},
pages = {110141},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110141},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008385},
author = {Zhengyuan Fan and Zemin Cai},
keywords = {Optical flow estimation, Epipolar geometry, Lightweight neural network},
abstract = {The majority of supervised models estimate optical flow through minimizing the numerical difference between the predicted flow and the ground truth, resulting in the loss of positional and geometric characteristics of the calculated flow fields. In addition, these models require a large number of parameters and high computational cost when computing optical flow. To address these issues, this paper presents a novel loss function and a lightweight framework for optical flow estimation. The proposed loss function, called the random epipolar constraint loss function (RECLoss), incorporates epipolar geometry into supervised optimization to transform the numerical difference into geometry constraint. The RECLoss can make the optical flow estimation models more effective and enhance their generalization abilities. Moreover, the design of RECLoss is more interpretable and the estimated optical flow fields from RECLoss have clearly defined mathematical meanings. A lightweight recurrent neural network for optical flow estimation (LRFlow) that balances computational cost and estimation accuracy is also proposed. The LRFlow, containing only 3.0M parameters, consists of a feature extractor, a correlation matching module, and an iterative update unit. The proposed lightweight network achieves state-of-the-art results compared to all other lightweight networks on the challenging MPI-Sintel and KITTI2015 datasets. The effectiveness of RECLoss in improving the accuracy of LRFlow and other state-of-the-art methods such as RAFT and GMA has been validated through extensive experiments. The source code of the project are available at https://github.com/Eryo-iPython/RECLoss.}
}
@article{ZHAN2024110152,
title = {YOLOPX: Anchor-free multi-task learning network for panoptic driving perception},
journal = {Pattern Recognition},
volume = {148},
pages = {110152},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110152},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300849X},
author = {Jiao Zhan and Yarong Luo and Chi Guo and Yejun Wu and Jiawei Meng and Jingnan Liu},
keywords = {Multi-task learning, Panoptic driving perception, Autonomous driving, Anchor-free},
abstract = {Panoptic driving perception encompasses traffic object detection, drivable area segmentation, and lane detection. Existing methods typically utilize anchor-based multi-task learning networks to complete this task. While these methods yield promising results, they suffer from the inherent limitations of anchor-based detectors. In this paper, we propose YOLOPX, a simple and efficient anchor-free multi-task learning network for panoptic driving perception. To the best of our knowledge, this is the first work to employ the anchor-free detection head in panoptic driving perception. This anchor-free manner simplifies training by avoiding anchor-related heuristic tuning, and enhances the adaptability and scalability of our multi-task learning network. In addition, YOLOPX incorporates a novel lane detection head that combines multi-scale high-resolution features and long-distance contextual dependencies to improve segmentation performance. Beyond structure optimization, we propose optimization improvements to enhance network training, enabling our multi-task learning network to achieve optimal performance through simple end-to-end training. Experimental results on the challenging BDD100K dataset demonstrate the state-of-the-art (SOTA) performance of YOLOPX: it achieves 93.7% recall and 83.3% mAP50 on traffic object detection, 93.2% mIoU on drivable area segmentation, and 88.6% accuracy and 27.2% IoU on lane detection. Moreover, YOLOPX has faster inference speed compared to the lightweight network YOLOP. Consequently, YOLOPX is a powerful solution for panoptic driving perception problems. The code is available at https://github.com/jiaoZ7688/YOLOPX.}
}
@article{LI2024110133,
title = {Joint Feature Generation and Open-set Prototype Learning for generalized zero-shot open-set classification},
journal = {Pattern Recognition},
volume = {147},
pages = {110133},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110133},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008300},
author = {Xiao Li and Min Fang and Zhibo Zhai},
keywords = {Generalized zero-shot open-set classification, Feature generation, Open-set prototype learning, Intra-class compactness loss, Inter-class dispersion loss},
abstract = {In generalized zero-shot classification, test samples can belong to either seen or unseen classes. However, in real-world situations, there may be many open-set samples in the test set where neither visual nor semantic representations of the classes are provided. The new problem is defined as generalized zero-shot open-set classification (GZSOSC). The purpose is to tell whether an instance belongs to which seen or unseen classes, or to reject an instance if it belongs to the open-set classes. To address this problem, we propose a novel method called Joint Feature Generation and Open-Set Prototype Learning (JFGOPL) for GZSOSC tasks. JFGOPL is presented to combine GAN training with open-set prototype learning, where the former generates high-quality unseen and open-set samples and the latter learns some open-set prototypes. Specifically, a novel GAN training strategy is proposed, where an intra-class compactness loss and an inter-class dispersion loss are proposed to ensure the discrimination of the generated samples and to make the learned embedding network less susceptible to the domain shift problem. Furthermore, open-set prototypes are derived by projecting confident open-set samples into the semantic space using the updated embedding network. Experiments on widely used benchmarks demonstrate the superiority of JFGOPL over existing methods for tackling the challenging GZSOSC problem.}
}
@article{DING2024110088,
title = {A cascaded framework with cross-modality transfer learning for whole heart segmentation},
journal = {Pattern Recognition},
volume = {147},
pages = {110088},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110088},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007859},
author = {Yi Ding and Dan Mu and Jiaqi Zhang and Zhen Qin and Li You and Zhiguang Qin and Yingkun Guo},
keywords = {Whole heart segmentation, Modality transfer, Generative adversarial training, Unify data distribution, Attention mechanism},
abstract = {Automatic and accurate segmentation of the whole heart structure from 3D cardiac images plays an important role in helping physicians diagnose and treat cardiovascular disease. However, the time-consuming and laborious manual labeling of the heart images results in the inefficiency of utilizing the existing CT or MRI for training the deep learning network, which decrease the accuracy of whole heart segmentation. However, multi-modality data contains multi-level information of cardiac images due to different imaging mechanisms, which is beneficial to improve the segmentation accuracy. Therefore, this paper proposes a cascaded framework with cross-modality transfer learning for whole heart segmentation (CM-TranCaF), which consists of three key modules: modality transfer network (MTN), U-shaped multi-attention network (MAUNet) and spatial configuration network (SCN). In MTN, MRI images are transferred from MRI domain to CT domain, to increase the data volume by adopting the idea of adversarial training. The MAUNet is designed based on UNet, while the attention gates (AGs) are integrated into the skip connection to reduce the weight of background pixels. Moreover, to solve the problem of boundary blur, the position attention block (PAB) is also integrated into the bottom layer to aggregate similar features. Finally, the SCN is used to finetune the segmentation results by utilizing the anatomical information between different cardiac substructures. By evaluating the proposed method on the dataset of the MM-WHS challenge, CM-TranCaF achieves a Dice score of 91.1% on the testing dataset. The extensive experimental results prove the effectiveness of the proposed method compared to other state-of-the-art methods.}
}
@article{ISHIKAWA2024110181,
title = {Japanese historical character recognition by focusing on character parts},
journal = {Pattern Recognition},
volume = {148},
pages = {110181},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110181},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008786},
author = {Takuru Ishikawa and Tomo Miyazaki and Shinichiro Omachi},
keywords = {Historical document analysis, Japanese historical character, Learning character parts, Few-shot, Zero-shot recognition},
abstract = {Japanese historical documents provide valuable information. Character recognition is a critical technology for the digitalization of historical documents. Sample imbalance is a significant obstacle in recognizing Japanese historical characters, kuzushiji. Thousands of kuzushiji only have less than a few samples. Thus, recognition performance deteriorates greatly in kuzushiji with a few samples. In this study, we propose a framework for transferring knowledge of character parts from font to kuzushiji. The pretraining learns character parts from synthesized font images. However, fine-tuning to kuzushiji is more complex. We propose calculating a mean squared error loss between feature vectors of kuzushiji and font images, resulting in consistent feature vectors in kuzushiji and font. Consequently, we can perform zero-shot recognition for kuzushiji using the font images of zero-sampled kuzushiji. The experimental results show that the proposed method recognized zero-sampled kuzushiji at approximately 48% accuracy. Consequently, we significantly expand the number of recognizable kuzushiji.}
}
@article{HU2024110129,
title = {Disturbance rejection with compensation on features},
journal = {Pattern Recognition},
volume = {147},
pages = {110129},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110129},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008269},
author = {Xiaobo Hu and Jianbo Su and Jun Zhang},
keywords = {Compensation, Disturbance rejection, Modeling error, Pattern recognition},
abstract = {In pattern recognition tasks, the information from system input is modeled through a series of nonlinear operations, which include but not limited to feature extraction, regression, and classification. Both theoretically and practically, these operations are inevitably subject to internal modeling error and external disturbance, resulting at a performance challenge. Those state-of-the-art methods, e.g. Convolutional Neural Network and Transformer, still display significant instabilities and failures under practical applications, so comes a lack of generalization. Consequently, the more robust pattern recognition methods and related theories still merit a further study. This paper firstly reviews those state-of-the-art technologies in the field. The bottleneck of performances in those latest researches is associated with a lack of disturbance estimation and corresponding compensation. Therefore, the implications of disturbance rejection in pattern recognition field are further discussed from a control point of view. Then, the open problems are summarized. Ultimately, a discussion of the potential solutions, which is related to the application of compensation on features, is given to highlight the future study. Through the systematic review in this paper, the disturbance rejection in pattern recognition is developed into a control problem. Hopefully, more effective control technologies for the compensation on features can be used to improve the robustness of pattern recognition theoretically and practically.}
}
@article{CHEN2024110157,
title = {Dual subspace manifold learning based on GCN for intensity-invariant facial expression recognition},
journal = {Pattern Recognition},
volume = {148},
pages = {110157},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110157},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008543},
author = {Jingying Chen and Jinxin Shi and Ruyi Xu},
keywords = {Graph convolutional network, Semi-supervised learning, Intensity-invariant representation, Manifold learning, Facial expression recognition},
abstract = {Facial expression recognition (FER) is one of the most important computer vision tasks for understanding human inner emotions. However, the poor generation ability of the FER model limits its applicability due to tremendous intraclass variation. Especially for expressions of varying intensities, the appearance differences among weak expressions are subtle, which makes FER tasks challenging. In response to these issues, this paper presents a dual subspace manifold learning method based on a graph convolutional network (GCN) for intensity-invariant FER tasks. Our method treats the target task as a node classification problem and learns the manifold representation using two subspace analysis methods: locality preserving projection (LPP) and peak-piloted locality preserving projection (PLPP). Inspired by the classic LPP, which maintains local similarity among data, this paper introduces a novel PLPP that maintains the locality between peak expressions and non-peak expressions to enhance the representation of weak expressions. This paper also reports two subspace fusion methods, one based on a weighted adjacency matrix and another on a self-attention mechanism, that combine the LPP and PLPP to further improve FER performance. The second method achieves a recognition accuracy of 93.83% on the CK+, 74.86% on the Oulu-CASIA and 75.37% on the MMI for weak expressions, outperforming state-of-the-art methods.}
}
@article{LIU2024110076,
title = {Exploiting sublimated deep features for image retrieval},
journal = {Pattern Recognition},
volume = {147},
pages = {110076},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110076},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007732},
author = {Guang-Hai Liu and Zuo-Yong Li and Jing-Yu Yang and David Zhang},
keywords = {Image retrieval, Deep feature, Orientation-selective mechanism, Sublimated deep feature histogram, Gain whitening learning},
abstract = {Deep learning techniques can be used to describe image content, which is a good way to reduce the semantic gap between low-level and high-level features. However, convolutional neural networks (CNNs) exhibit texture bias and largely ignore global object shape. This does not conform to human perception and can fail to gain the advantages of both low-level features and deep features. To address this problem, in this work, the main focus was shifted from using simple deep features to the use of sublimated deep features, which incorporate global object shape and color features. Along these lines, in this work, a novel image retrieval method named the sublimated deep feature histogram (SDFH) was proposed. The main highlights are: 1) An effect orientation feature was introduced, namely, orientation-selective feature, to mimic the orientation-selection mechanism. This provides a good representation of global object shape and reduces the side-effects of texture bias. 2) A new concept was developed, namely color perceptual feature, to address the shortcoming of deep features—that they discard color features. This includes color cues in the deep features and provides a more discriminating representation. 3) The orientation selective and color perception mechanisms were effectively mimicked to provide a compact yet efficient representation, and propose an effective transfer learning method called gain whitening learning. By carrying out comparative experiments, it was demonstrated that sublimated deep features can provide highly competitive retrieval performance (in terms of mean average precision) using a pre-trained CNN model applied to well-known benchmark datasets. Our results provide new insights into image retrieval based on the mechanisms of the primary visual cortex (V1). Furthermore, the method is more in line with human perception than other methods.}
}
@article{YU2024110145,
title = {Contrasting augmented features for domain adaptation with limited target domain data},
journal = {Pattern Recognition},
volume = {148},
pages = {110145},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110145},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008427},
author = {Xi Yu and Xiang Gu and Jian Sun},
keywords = {Domain adaptation, Limited target domain data, Contrasting augmented features},
abstract = {Domain adaptation aims to alleviate distribution gaps between source and target domains. However, when the available target domain data are scarce for training, learning generalizable representations for domain adaptation is challenging. We propose a novel approach, dubbed Contrasting Augmented Features (CAF), to tackle the challenge of insufficient target domain data for domain adaptation, by generating and contrasting augmented features. We introduce a semantic feature generator to generate augmented features by replacing the instance-level feature statistics of one domain with another domain. With the augmented features, we further design the reweighted instance contrastive loss and category contrastive loss to improve feature discrimination and align feature distributions of source and target domains. CAF can be applied to few-shot domain adaptation and unsupervised domain adaptation with limited unlabeled target domain data. Despite its simplicity, extensive experiments show promising results for both applications. In addition, experiments demonstrate that CAF is more robust to the number of target domain data and also effective in vanilla unsupervised domain adaptation setting with full target domain data.}
}
@article{JIANG2024110173,
title = {Joint recognition of basic and compound facial expressions by mining latent soft labels},
journal = {Pattern Recognition},
volume = {148},
pages = {110173},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110173},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008701},
author = {Jing Jiang and Mei Wang and Bo Xiao and Jiani Hu and Weihong Deng},
keywords = {Facial expression recognition, Basic expressions, Compound expressions, Soft label, Expression correlation},
abstract = {Previous works on facial expression recognition focus on basic emotions, while ignoring more complex compound expressions. However, both basic and compound emotions appear in the real-world environment. In this work, we aim to jointly recognize basic and compound expressions. Aiming at the Basic-Compound Facial Expression Recognition (BC-FER) task, we illustrate that traditional hard label training is not ideal due to great label dependencies. Therefore, we propose an expression soft label mining (ESLM) method to improve the performance. On the one hand, an iterated soft label mining (ISLM) algorithm assisted by teacher–student network is proposed to make the network generate soft targets automatically for learning. On the other hand, to explicitly leverage prior knowledge of label correlations, we propose an expression correlation score learning (ECSL) loss to regularize the predicted distributions. Extensive experimental results on CFEE, RAF-DB, and EmotioNet show that our method achieves state-of-the-art performance on BC-FER task.}
}
@article{CHEN2024110142,
title = {Global routing between capsules},
journal = {Pattern Recognition},
volume = {148},
pages = {110142},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110142},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008397},
author = {Ran Chen and Hao Shen and Zhong-Qiu Zhao and Yi Yang and Zhao Zhang},
keywords = {CapsNet, Global routing, Multi-branch, Straight-through-routing},
abstract = {Current convolutional neural networks (CNNs) lack viewpoint equivariance. Hence, they perform poorly when dealing with viewpoints unseen during training procedures. CNN achieves invariance via pooling operations in image classification tasks. However, the pooling operation does not necessarily improve viewpoint generalization, rather relying on more data to achieve viewpoint equivariance. Capsule network (CapsNet) is proposed to tackle this issue, but it is inefficient and inaccurate when applied to complex datasets. We propose a novel CapsNet architecture called Global Routing CapsNet (GR-CapsNet) to solve this problem. Specifically, colored background in the input image can generate invalid background voting capsules to reduce the performance of CapsNet. Therefore, we first construct a dynamic linear unit (DLU), which avoids the generation of invalid background voting capsules. Then we present two extra learnable units: frequency domain unit (FDU) and spatial unit (SPU). The former is used to capture finer features in the frequency domain and aims to improve classification performance on complex datasets. The latter is applied to construct the spatial relationship between the voting capsules and component capsules and aims to enhance robustness to affine transformation. Finally, we propose a global routing mechanism to simplify the routing process for CapsNet, which obtains more feature information to improve the performance of CapsNet. Extensive experiments on nine datasets show that our method obtains better robustness and generalization and achieves SOTA performance compared to other related methods. And it has fewer the number of parameters and GPU memory consumption than these related methods. The source code is available on https://github.com/cwpl/GR-CapsNet.}
}
@article{ZHAO2024110170,
title = {VPCFormer: A transformer-based multi-view finger vein recognition model and a new benchmark},
journal = {Pattern Recognition},
volume = {148},
pages = {110170},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110170},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008671},
author = {Pengyang Zhao and Yizhuo Song and Siqi Wang and Jing-Hao Xue and Shuping Zhao and Qingmin Liao and Wenming Yang},
keywords = {Database, Multi-view finger vein recognition, Transformer, Attention mechanism},
abstract = {In the past decade, finger vein authentication garners significant interest. However, most existing databases and algorithms predominantly focused on single-view finger vein recognition. The current projection of vein patterns actually maps a 3D network topology into a 2D plane, which inevitably leads to 3D feature loss and topological ambiguity in 2D images. Additionally, single-view based methods are sensitive to finger rotation and translation in practical applications. So far, there are currently few dedicated studies and public databases on multi-view finger vein recognition. To address these issues, we first establish a benchmark for future research by constructing the multi-view finger vein database, named Tsinghua Multi-View Finger Vein-3 Views (THUMVFV-3V) Database , which is collected over two sessions. THUMVFV-3V provides three types of Regions of Interest (ROIs) and includes unified preprocessing operations, catering to the majority of existing methods. Furthermore, we propose a novel Transformer-based model named Vein Pattern Constrained Transformer (VPCFormer) for multi-view finger vein recognition, primarily composed of multiple Vein Pattern Constrained Encoders (VPC-Encoders) and Neighborhood-Perspective Modules (NPMs). Specifically, the VPC-Encoder incorporates a novel Vein Pattern Attention Module (VPAM) and an Integrative Feed-Forward Network (IFFN). Motivated by the fact that the strong correlations veins exhibit across different views, we devise the VPAM. Assisted by a vein mask, VPAM is meticulously designed to exclusively extract intra- and inter-view dependencies between vein patterns. Further, we propose IFFN to efficiently aggregate the preceding attention and contextual information of VPAM. In addition, the NPM is utilized to capture the correlations within a single view, enhancing the final multi-view finger vein representation. Extensive experiments demonstrate the superiority of our VPCFormer. The THUMVFV-3V database is available at https://github.com/Pengyang233/THUMVFV-3V-Database.}
}
@article{WANG2024110118,
title = {PWDformer: Deformable transformer for long-term series forecasting},
journal = {Pattern Recognition},
volume = {147},
pages = {110118},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110118},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008154},
author = {Zheng Wang and Haowei Ran and Jinchang Ren and Meijun Sun},
keywords = {Long-term forecasting, Time series forecasting, Deep learning, Transformer},
abstract = {Long-term forecasting is of paramount importance in numerous scenarios, including predicting future energy, water, and food consumption. For instance, extreme weather events and natural disasters can profoundly impact infrastructure operations and pose severe safety concerns. Traditional CNN-based models often struggle to capture long-distance dependencies effectively. In contrast, Transformers-based models have shown significant promise in long-term forecasting. This paper investigates the long-term forecasting problem and identifies a common limitation in existing Transformer-based models: they tend to reduce computational complexity at the expense of time information aggregation capability. Moreover, the order of time series plays a crucial role in accurate predictions, but current Transformer-based models lack sensitivity to time series order, rendering them unreasonable. To address these issues, we propose a novel Deformable-Local (DL) aggregation mechanism. This mechanism enhances the model’s ability to aggregate time information and allows the model to adaptively adjust the size of the time aggregation window. Consequently, the model can discern more complex time patterns, leading to more accurate predictions. Additionally, our model incorporates a Frequency Selection module to reinforce effective features and reduce noise. Furthermore, we introduce Position Weights to mitigate the order-insensitivity problem present in existing methods. In extensive evaluations of long-term forecasting tasks, we conducted benchmark tests on six datasets covering various practical applications, including energy, traffic, economics, weather, and disease. Our method achieved state-of-the-art (SOTA) results, demonstrating significant improvements. For instance, on the ETT dataset, our model achieved an average MSE improvement of approximately 19% and an average MAE improvement of around 27%. Remarkably, for predicted lengths of 96 and 192, we achieved outstanding MSE and MAE improvements of 32.1% and 30.9%, respectively.}
}
@article{HOU2024110089,
title = {View-coherent correlation consistency for semi-supervised semantic segmentation},
journal = {Pattern Recognition},
volume = {147},
pages = {110089},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110089},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007860},
author = {Yunzhong Hou and Stephen Gould and Liang Zheng},
keywords = {Semi-supervised learning, Semantic segmentation, Contrastive learning, Data augmentation},
abstract = {Semi-supervised semantic segmentation needs rich and robust supervision for unlabeled data. However, promoting or punishing feature similarities with vanilla contrastive learning can be unreliable for semi-supervised semantic segmentation: pixel pairs are assigned as either positive or negative based on noisy pseudo labels, and both reliable and wrongly-assigned pairs receive uniform penalties. To address this issue, we propose correlation consistency learning, which leverages rich pairwise relationships in self-correlation matrices and matches them to the similarities between soft pseudo labels to provide robust supervision. Unlike vanilla contrastive learning, our approach prioritizes pairs with highly confident pseudo labels and applies weaker penalties for pairs that are less confident. We also introduce a strong semi-supervised learning pipeline that applies data augmentation in a view-coherent manner: even under complex augmentation strategies, for each pixel, a match can be found in different augmentation views. The novelties of the proposed method are the correlation consistency loss and the view-coherent data augmentation, and their combination gives us the view-coherent correlation consistency (VC3) system, which achieves state-of-the-art results in several semi-supervised settings on two datasets.}
}
@article{ECHEBERRIABARRIO2024110130,
title = {Topological safeguard for evasion attack interpreting the neural networks’ behavior},
journal = {Pattern Recognition},
volume = {147},
pages = {110130},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110130},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008270},
author = {Xabier Echeberria-Barrio and Amaia Gil-Lerchundi and Iñigo Mendialdua and Raul Orduna-Urrutia},
keywords = {Artificial neural network interpretability, Artificial neural network cybersecurity, Adversarial learning, Evasion attack, Artificial neural network countermeasure},
abstract = {In the last years, Deep Learning technology has been proposed in different fields, bringing many advances in each of them, but raising new threats in these solutions regarding cybersecurity. Those implemented models have brought several vulnerabilities associated with Deep Learning technology. Moreover, those allow taking advantage of the implemented model, obtaining private information, and even modifying the model’s decision-making. Therefore, interest in studying those vulnerabilities/attacks and designing defenses to avoid or fight them is gaining prominence among researchers. In particular, the widely known evasion attack is being analyzed by researchers; thus, several defenses to avoid such a threat can be found in the literature. Since the presentation of the L-BFG algorithm, this threat concerns the research community. However, it continues developing new and ingenious countermeasures since there is no perfect defense for all the known evasion algorithms. In this work, a novel detector of evasion attacks is developed. It focuses on the information on the activations of the neurons given by the model when an input sample is injected. Moreover, it pays attention to the topology of the targeted deep learning model to analyze the activations according to which neurons are connecting. This approach is motivated from the observation that the literature shows that the targeted model’s topology contains essential information about if the evasion attack occurs. For this purpose, a huge data preprocessing is required to introduce all this information in the detector, which uses the Graph Convolutional Neural Network (GCN) technology. Thus, it understands the topology of the target model, obtaining promising results and improving the outcomes presented in the literature related to similar defenses.}
}
@article{LIU2024110093,
title = {Residual Deformable Convolution for better image de-weathering},
journal = {Pattern Recognition},
volume = {147},
pages = {110093},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110093},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007902},
author = {Huikai Liu and Ao Zhang and Wenqian Zhu and Bin Fu and Bingjian Ding and Shengwu Xiong},
keywords = {Image de-weathering, Deep learning, Image restoration, Autoencoder, Multi-patch skip-forward, Residual deformable convolution},
abstract = {Adverse weather conditions pose great challenges to computer vision tasks like detection, segmentation, tracing et, al. in the wild. Image de-weathering aiming at removing weather degradations from images/videos has hence accumulated huge popularity as a significant component of image restoration. A large number of SOTA de-weathering methods are based on the autoencoder architecture for its excellent generalization and high computational efficiency. However, for most of these models, parts of high-frequency information are inevitably lost in the downsampling process in the encoders, while degraded features are unable to be effectively inhibited in the upsampling modules in the decoders, largely limiting the restoration performance. In this paper, we propose a multi-patch skip-forward structure for the encoder to deliver fine-grain features from shallow layers to deep layers, and provide more detailed semantics for feature embedding. For the decoding part, the Residual Deformable Convolutional module is developed to dynamically recover the degradation with spatial attention, achieving high-quality pixel-wise reconstruction. Extensive experiments show that our model outperforms many recently proposed state-of-the-art works on both specific-task de-weathering, such as de-raining, de-snowing, and all-task de-weathering. The source code is available at https://github.com/IntelligentDrivingCoding/DeformDeweatherNet.}
}
@article{ZHANG2024110158,
title = {Re-abstraction and perturbing support pair network for few-shot fine-grained image classification},
journal = {Pattern Recognition},
volume = {148},
pages = {110158},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110158},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008555},
author = {Weichuan Zhang and Yali Zhao and Yongsheng Gao and Changming Sun},
keywords = {Few-shot fine-grained image classification (FSFGIC), Re-abstraction and perturbing support pair network (RaPSPNet), Feature re-abstraction embedding (FRaE) module, Perturbing support pair (PSP) based similarity measure module},
abstract = {The goal of few-shot fine-grained image classification (FSFGIC) is to distinguish subordinate-level categories with subtle visual differences such as the species of bird and models of car with only a few samples. In this work, we argue that a designed network that has the ability to better distinguish feature descriptors of different categories will effectively improve the performance of FSFGIC. We propose a re-abstraction and perturbing support pair network (RaPSPNet) for FSFGIC. Specifically, we first design a feature re-abstraction embedding (FRaE) module which can not only effectively amplify the difference between the feature information from different categories but also better extract the feature information from images. Furthermore, a novel perturbing support pair (PSP) based similarity measure module is designed which evaluates the relationships of feature information among a query image and two different categories of support images (a support pair) at the same time for guiding the designed FRaE module to find salient feature information from the same category of query and support images and find distinguishable feature information from the different categories of query and support images. Extensive experiments on FSFGIC tasks demonstrate the superiority of the proposed methods over state-of-the-art benchmarks.}
}
@article{LIANG2024110134,
title = {Linearized alternating direction method of multipliers for elastic-net support vector machines},
journal = {Pattern Recognition},
volume = {148},
pages = {110134},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110134},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008312},
author = {Rongmei Liang and Xiaofei Wu and Zhimin Zhang},
keywords = {Convex optimization, Linearized ADMM, Elastic-net, Support vector machines},
abstract = {In many high-dimensional datasets, the phenomenon that features are relevant often occurs. Elastic-net regularization is widely used in support vector machines (SVMs) because it can automatically perform feature selection and encourage highly correlated features to be selected or removed together. Recently, some effective algorithms have been proposed to solve the elastic-net SVMs with different convex loss functions, such as hinge, squared hinge, huberized hinge, pinball and huberized pinball. In this paper, we develop a linearized alternating direction method of multipliers (LADMM) algorithm to solve above elastic-net SVMs. In addition, our algorithm can be applied to solve some new elastic-net SVMs such as elastic-net least squares SVM. Compared with some existing algorithms, our algorithm has comparable or better performances in terms of computational cost and accuracy. Under mild conditions, we prove the convergence and derive convergence rate of our algorithm. Furthermore, numerical experiments on synthetic and real datasets demonstrate the feasibility and validity of the proposed algorithm.}
}
@article{BU2024110182,
title = {Improving Augmentation Consistency for Graph Contrastive Learning},
journal = {Pattern Recognition},
volume = {148},
pages = {110182},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110182},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008798},
author = {Weixin Bu and Xiaofeng Cao and Yizhen Zheng and Shirui Pan},
keywords = {Graph contrastive learning, Augmentation consistency},
abstract = {Graph contrastive learning (GCL) enhances unsupervised graph representation by generating different contrastive views, in which properties of augmented nodes are required to be aligned with their anchors. However, we find that in some existing GCL methods, it is hard to inherit semantic and structural properties of graphs from anchor views due to inconsistent augmentation schemes, which may hurt node consistency in augmented views. In this paper, we present ConGCL to improve node consistency and enhance node classification. Specifically, we first consider context entailment, which integrates the semantic and structural properties to better mine the underlying consistency relationships of nodes. Beneficial from this, we then design a novel consistency improvement loss to maintain augmentation consistency agreement among positive node pairs under stochastic augmentation schemes. To investigate the effectiveness of ConGCL on improving augmentation consistency and enhancing node classification, we conduct empirical study and extensive experiments on benchmark datasets. The code is available at: https://github.com/brysonwx/ConGCL.}
}
@article{LIU2024110174,
title = {Confusable facial expression recognition with geometry-aware conditional network},
journal = {Pattern Recognition},
volume = {148},
pages = {110174},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110174},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008713},
author = {Tong Liu and Jing Li and Jia Wu and Bo Du and Jun Wan and Jun Chang},
keywords = {Confusable facial expression recognition, Facial image synthesis, Long-range dependencies, Geometry-aware conditional network},
abstract = {Many facial expression recognition (FER) methods have now achieved satisfactory results. However, some facial expressions have similar muscle deformation, making them easy to confuse. These confusable facial expressions are a key challenge to accurately recognizing facial expressions. In addition, most current FER methods rely on the convolution operation, but convolution is a building block that processes one local neighborhood at a time; thus, it fails to capture the geometric patterns that are important for facial muscle deformation. To address this issue, considering the problems of pose variations and insufficient training data, this paper proposes a geometry-aware conditional network (GACN) that captures long-range dependencies for simultaneous pose-invariant facial expression editing and geometry-aware FER. Specifically, the GACN can complete a pose-invariant image editing task with long-range dependency by introducing conditional self-attention operations to a generative adversarial network. Moreover, the GACN presents non-local operations as building blocks of the classifier to capture the texture and geometry patterns simultaneously. Finally, these two tasks can further boost each other’s performances through our GACN, and confusable facial expressions can be effectively distinguished. And we overcome the effect of pose variations while expanding and enriching the training set. Our proposed algorithm is evaluated on both the in-the-lab and in-the-wild datasets and outperforms the state-of-the-art methods.}
}
@article{TANG2024110169,
title = {MI3C: Mining intra- and inter-image context for person search},
journal = {Pattern Recognition},
volume = {148},
pages = {110169},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110169},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300866X},
author = {Zongheng Tang and Yulu Gao and Tianrui Hui and Fengguang Peng and Si Liu},
keywords = {Person-search, Person re-identification, Deep neural networks},
abstract = {Person search aims to localize the queried person from a gallery of uncropped, realistic images. Unlike re-identification (Re-ID), person search deals with the entire scene image containing rich and diverse visual context information. However, existing works mainly focus on the person’s appearance while ignoring other essential intra- and inter-image context information. To comprehensively leverage the intra- and inter-image context, we propose a unified framework termed MI3C including the Intra-image Multi-View Context network (IMVC) and the Inter-image Group Context Ranking algorithm (IGCR). Concretely, the IMVC integrates the features from the scene, surrounding, instance, and part views collaboratively to generate the final ID feature for person search. Furthermore, the IGCR algorithm employs group matching results between query and gallery image pairs to measure the holistic image matching similarity, which is adopted as part of the sorting metric to yield a more robust ranking among the whole gallery. Extensive experiments on two popular person search benchmarks demonstrate that by mining intra- and inter-image context, our method outperforms previous state-of-the-art methods by conspicuous margins. Specifically, we achieve 96.7% mAP and 97.1% top-1 accuracy on the CUHK-SYSU dataset, 55.6% mAP, and 90.8% top-1 accuracy on the PRW dataset.}
}
@article{ZHU2024110077,
title = {Deepfake detection via inter-frame inconsistency recomposition and enhancement},
journal = {Pattern Recognition},
volume = {147},
pages = {110077},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110077},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323007744},
author = {Chuntao Zhu and Bolin Zhang and Qilin Yin and Chengxi Yin and Wei Lu},
keywords = {Video forensics, Deepfake detection, Inter-frame inconsistency, Image recomposition, Multi-level features},
abstract = {Due to the remarkable progress in face manipulation technology, malicious applications of these technologies may pose a great threat to the social stability. Therefore, it is essential to carry out the research of deepfake detection. In this paper, we assumed that the illumination on frames that skip a certain space is basically consistent in real videos, but tends to be inconsistent in fake videos. From this point, a network which contains a learnable Image Decomposition Module (IDM) and multi-level feature enhancement is proposed. IDM decomposes frames into illumination and reflection, and frame recomposition is followed to highlight the frame-level illumination inconsistency. Multi-level feature enhancement is proposed to enhance the illumination inconsistency at feature level. In addition, considering the computational complexity and human vision perception mechanism, we train the network in logarithm domain. Experimental results show that the proposed method is effective and superior compared with other state-of-the-art deepfake detection methods on mainstream deepfake datasets.}
}
@article{POSADAMORENO2024110146,
title = {ECLAD: Extracting Concepts with Local Aggregated Descriptors},
journal = {Pattern Recognition},
volume = {147},
pages = {110146},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110146},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008439},
author = {Andrés Felipe Posada-Moreno and Nikita Surya and Sebastian Trimpe},
keywords = {Concept extraction, Explainable artificial intelligence, Convolutional neural networks},
abstract = {Convolutional neural networks (CNNs) are increasingly being used in critical systems, where robustness and alignment are crucial. In this context, the field of explainable artificial intelligence has proposed the generation of high-level explanations of the prediction process of CNNs through concept extraction. While these methods can detect whether or not a concept is present in an image, they are unable to determine its location. What is more, a fair comparison of such approaches is difficult due to a lack of proper validation procedures. To address these issues, we propose a novel method for automatic concept extraction and localization based on representations obtained through pixel-wise aggregations of CNN activation maps. Further, we introduce a process for the quantitative comparison and validation of concept-extraction techniques based on synthetic datasets with pixel-wise annotations of their main components, mitigating possible confirmation biases induced by human visual inspection. Extensive experimentation on both synthetic and real-world datasets demonstrates that our method outperforms state-of-the-art alternatives.}
}