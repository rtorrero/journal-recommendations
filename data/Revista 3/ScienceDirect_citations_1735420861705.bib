@article{XU2023109811,
title = {Cross-Domain Few-Shot classification via class-shared and class-specific dictionaries},
journal = {Pattern Recognition},
volume = {144},
pages = {109811},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109811},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005095},
author = {Renjie Xu and Lei Xing and Baodi Liu and Dapeng Tao and Weijia Cao and Weifeng Liu},
keywords = {Few-shot learning, Dictionary learning, Cross-Domain, Collaborative representation},
abstract = {In Cross-Domain Few-Shot Classification, researchers mainly utilize models which trained with source domain tasks to adapt to the target domain with very few samples, thus causing serious class-difference-caused domain differences. Although researchers have proposed methods to minimize the domain differences, the existing methods have the following drawbacks: 1) most models do not utilize the common knowledge between the source and target domains, and 2) require additional labeled samples from the target domain for finetuning or domain alignment, which is hard to obtain in reality. To address the problem mentioned above, we propose a class-shared and class-specific dictionaries (CSCSD) learning method. To make better utilization of the common knowledge, we apply a class-shared dictionary which is learned to represent the generality of source and target domain. Moreover, class-specific dictionaries are applied to represent the class-specific knowledge that can’t be represented in the class-shared dictionary. Furthermore, unlike most other models, our CSCSD does not require additional target domain samples to meta-train or finetune. With the dictionaries, CSCSD can obtain more distinguishable collaborative representations of samples with the origin representations extracted with the model. To evaluate the effectiveness of CSCSD, we utilize larger datasets, e.g., MiniImageNet and TieredImageNet as source domains and fine-grained datasets, e.g., CUB, Cars, Places, and Plantae as target domains. With our CSCSD, the Cross-Domain Few-Shot accuracy exceeds most domain adaptive Few-Shot which utilizes additional training set in target domains.}
}
@article{MIN2023109803,
title = {Optimality in high-dimensional tensor discriminant analysis},
journal = {Pattern Recognition},
volume = {143},
pages = {109803},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109803},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005010},
author = {Keqian Min and Qing Mai and Junge Li},
keywords = {Discriminant analysis, Minimax optimality, Tensor},
abstract = {Tensor discriminant analysis is an important topic in tensor data analysis. However, given the many proposals for tensor discriminant analysis methods, there lacks a systematic theoretical study, especially concerning optimality. We fill this gap by providing the minimax lower bounds for the estimation and prediction errors under the tensor discriminant analysis model coupled with the sparsity assumption. We further show that one existing high-dimensional tensor discriminant analysis estimator has matching upper bounds, and is thus optimal. Our results apply to tensors with arbitrary orders and ultra-high dimensions. If one focuses on one-way tensors (i.e., vectors), our results further provide strong theoretical justifications for several popular sparse linear discriminant analysis methods. Numerical studies are also presented to support our theoretical results.}
}
@article{ZHOU2023109827,
title = {Feature fusion network for long-tailed visual recognition},
journal = {Pattern Recognition},
volume = {144},
pages = {109827},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109827},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005253},
author = {Xuesong Zhou and Junhai Zhai and Yang Cao},
keywords = {Long-tailed learning, Head and tail classes, Feature representations, Feature fusion network},
abstract = {Deep learning has achieved remarkable success in recent years; however, deep learning methods face significant challenges on long-tailed datasets, which are prevalent in real-world scenarios. In a long-tailed dataset, there are many more samples in the head classes than in the tail classes, and this class imbalance makes it difficult to learn a good feature representation for both head and tail classes simultaneously, particularly when using a single-stage method. Although the existing two-stage methods can alleviate the problem of single-stage methods not performing well on the tail classes by classifier retraining in the second stage, this does not resolve the problem of insufficient learning of head and tail features. Thus, in this paper, we propose a two-stage feature fusion network (FFN). The proposed FFN addresses this issue using one network for the head classes and another network for the tail classes, each of which is trained with a different loss function. This allows the feature learning module to effectively distinguish between the head and tail classes in the embedding space. The classifier learning module fuses the features obtained from the feature learning module, and the classifier is fine-tuned to classify the input images. Different from traditional two-stage methods, the proposed utilizes different loss functions for the head and tail classes; thus, the classifier can achieve balanced results between the head and tail classes. We conduct extensive experiments on three benchmark datasets comparing the proposed FFN with six state-of-the-art methods including two baseline methods, the experimental results demonstrate that the FFN achieves significant improvement on all three benchmark datasets. The code is publicly available at https://github.com/zxsong999/Feature-Fusion-Network.pytorch.}
}
@article{LIU2023109774,
title = {WSDS-GAN: A weak-strong dual supervised learning method for underwater image enhancement},
journal = {Pattern Recognition},
volume = {143},
pages = {109774},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109774},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004727},
author = {Qiong Liu and Qi Zhang and Wei Liu and Wenbai Chen and Xinwang Liu and Xiangke Wang},
keywords = {Underwater image enhancement, Two-stage learning, Deep learning, CycleGAN},
abstract = {Underwater Image Enhancement (UIE) is a crucial preprocessing step for underwater vision tasks. Addressing the challenge of training supervised deep learning models on large, diverse datasets while learning the intrinsic degradation factors of underwater images is essential for improving model generalization performance. In this paper, we propose a Weak-Strong Dual Supervised Generative Adversarial Network (WSDS-GAN) for UIE. During the first weakly supervised learning phase, unpaired images, consisting of degraded underwater images and clear in-air images, are used to train the model with the goal of recovering color, brightness, and content. In the second strongly supervised learning phase, a limited number of paired images are fed into the model to further train the image detail recovery generator. Comprehensive experiments on public datasets and self-photographed images demonstrate the effectiveness of our proposed method over existing state-of-the-art methods, both qualitatively and quantitatively. Additionally, we show that our method significantly enhances image details to support subsequent underwater vision tasks.}
}
@article{SANG2023109746,
title = {Reward shaping with hierarchical graph topology},
journal = {Pattern Recognition},
volume = {143},
pages = {109746},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109746},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004442},
author = {Jianghui Sang and Yongli Wang and Weiping Ding and Zaki Ahmadkhan and Lin Xu},
keywords = {Reinforcement learning, Reward shaping, Probability graph, Markov decision process},
abstract = {Reward shaping using GCNs is a popular research area in reinforcement learning. However, it is difficult to shape potential functions for complicated tasks. In this paper, we develop Reward Shaping with Hierarchical Graph Topology (HGT). HGT propagates information about the rewards through the message passing mechanism, which can be used as potential functions for reward shaping. We describe reinforcement learning by a probability graph model. Then we generate a underlying graph with each state is a node and edges represent transition probabilities between states. In order to prominently shape potential functions for complex environments, HGT divides the underlying graph constructed from states into multiple subgraphs. Since these subgraphs provide a representation of multiple logical relationships between states in the Markov decision process, the aggregation process rich correlation information between nodes, which makes the propagated messages more powerful. When compared to cutting-edge RL techniques, HGT achieves faster learning rates in experiments on Atari and Mujoco tasks.}
}
@article{HAO2023109843,
title = {Contrastive Generative Network with Recursive-Loop for 3D point cloud generalized zero-shot classification},
journal = {Pattern Recognition},
volume = {144},
pages = {109843},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109843},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005411},
author = {Yun Hao and Yukun Su and Guosheng Lin and Hanjing Su and Qingyao Wu},
keywords = {3D point cloud, Generalized zero-shot, Contrastive learning, Recursive-loop},
abstract = {Generalized Zero-Shot Learning (GZSL) aims to recognize objects from both seen and unseen categories by transferring semantic knowledge and merely utilizing seen class data for training. Recent feature generation methods in the 2D image domain have made great progress. However, very little is known about its usefulness in 3D point cloud zero-shot learning. This work aims to facilitate research on 3D point cloud generalized zero-shot learning. Different from previous works, we focus on synthesizing the more high-level discriminative point cloud features. To this end, we design a representation enhancement strategy to generate the features. Specifically, we propose a Contrastive Generative Network with Recursive-Loop, termed as CGRL, which can be leveraged to enlarge the inter-class distances and narrow the intra-class gaps. By applying the contrastive representations to the generative model in a recursive-loop form, it can provide the self-guidance for the generator recurrently, which can help yield more discriminative features and train a better classifier. To validate the effectiveness of the proposed method, extensive experiments are conducted on three benchmarks, including ModelNet40, McGill, and ScanObjectNN. Experimental evaluations demonstrate the superiority of our approach and it can outperform the state-of-the-arts by a large margin. Code is available at https://github.com/photon-git/CGRL}
}
@article{REZAEI2023109815,
title = {Mixed data clustering based on a number of similar features},
journal = {Pattern Recognition},
volume = {143},
pages = {109815},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109815},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005137},
author = {Hamid Rezaei and Negin Daneshpour},
keywords = {Clustering, Mixed data, Data object distance, Similarity, Cluster center},
abstract = {Finding the degree of similarity measurement is one of the challenges of mixed data clustering. In this article, it has been tried to design a more efficient method by innovating in three important parts of clustering. In the part of the general method, for assigning data objects to the cluster, in addition to the distance, attention is paid to the “number of similar features”. Compared to assigning each object to a cluster, in cases where the distances are equal or close, the cluster center with the highest number of features similar to the given objects will be appropriate. This method is more accurate than the Hamming distance. To determine the cluster centers, instead of random selection, a more suitable object is identified with a distance-based method. In accuracy in three datasets, the proposed algorithm has performed at least two percent better than the other algorithms.}
}
@article{SHAO2023109781,
title = {Conditional pseudo-supervised contrast for data-Free knowledge distillation},
journal = {Pattern Recognition},
volume = {143},
pages = {109781},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109781},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300479X},
author = {Renrong Shao and Wei Zhang and Jun Wang},
keywords = {Model compression, Knowledge distillation, Representation learning, Contrastive learning, Privacy protection},
abstract = {Data-free knowledge distillation (DFKD) is an effective manner to solve model compression and transmission restrictions while retaining privacy protection, which has attracted extensive attention in recent years. Currently, the majority of existing methods utilize a generator to synthesize images to support the distillation. Although the current methods have achieved great success, there are still many issues to be explored. Firstly, the outstanding performance of supervised learning in deep learning drives us to explore a pseudo-supervised paradigm on DFKD. Secondly, current synthesized methods cannot distinguish the distributions of different categories of samples, thus producing ambiguous samples that may lead to an incorrect evaluation by the teacher. Besides, current methods cannot optimize the category-wise diversity samples, which will hinder the student model learning from diverse samples and further achieving better performance. In this paper, to address the above limitations, we propose a novel learning paradigm, i.e., conditional pseudo-supervised contrast for data-free knowledge distillation (CPSC-DFKD). The primary innovations of CPSC-DFKD are: (1) introducing a conditional generative adversarial network to synthesize category-specific diverse images for pseudo-supervised learning, (2) improving the modules of the generator to distinguish the distributions of different categories, and (3) proposing pseudo-supervised contrastive learning based on teacher and student views to enhance diversity. Comprehensive experiments on three commonly-used datasets validate the performance lift of both the student and generator brought by CPSC-DFKD. The code is available at https://github.com/RoryShao/CPSC-DFKD.git}
}
@article{ZHANG2023109762,
title = {An enhanced noise-tolerant hashing for drone object detection},
journal = {Pattern Recognition},
volume = {143},
pages = {109762},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109762},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004600},
author = {Luming Zhang and Guifeng Wang and Ming Chen and Fuji Ren and Ling Shao},
keywords = {Multiple attributes, Attributes fusion, Noise-tolerant, Deep hashing, Drone, Matrix factorization},
abstract = {Drone, a.k.a. Unmanned aerial vehicle (UAV), has been pervasively applied in geological hazard monitoring, smart agriculture, and urban planning in the past decade. In this work, we fuse multiple attributes into a noise-tolerant hashing framework that can detect objects from drone pictures extremely fast. Our method can intrinsically and flexibly encode various topological structures from each target object, based on which multi-scale objects can be discovered in a view- and altitude-invariant way. Moreover, by leveraging lF and l1 norms collaboratively, the calculated hash codes are robust to low quality drone pictures and noisy semantic labels. More specifically, for each drone-borne picture, we extract visually/semantically salient object parts inside it. To characterize their topological structure, we construct a graphlet by linking the spatially adjacent object patches into a small graph. Subsequently, a binary matrix factorization (MF) is designed to hierarchically exploit the semantics of these graphlets, wherein three attributes: i) deep binary hash codes learning, ii) contaminated pictures/labels denoising, and iii) adaptive data graph updating are seamlessly incorporated. Such multi-attribute binary MF can be solved iteratively, and in turn each graphlet is transformed into the binary hash codes. Finally, the hash codes corresponding to graphlets within each drone photo are utilized for ranking-based object discovery. Comprehensive experiments on the DAC-SDC, MOHR, and our self-compiled data set have demonstrated the competitively speed and accuracy of our method. As a byproduct, we employ an elaborately-designed FPGA architecture to calculate our hash codes. On average, a 57 frames per second (fps) object detection speed is achieved on 4K drone videos (without temporal modeling).}
}
@article{YANG2023109786,
title = {Improved polar complex exponential transform for robust local image description},
journal = {Pattern Recognition},
volume = {143},
pages = {109786},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109786},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004843},
author = {Zhanlong Yang and Linzhi Yang and Geng Chen and Pew-Thian Yap},
keywords = {Image description, Local image descriptor, Polar complex exponential transform, Phase information},
abstract = {Image description via robust local descriptors plays a vital role in a large number of image representation and matching applications. In this paper, we propose a novel distinctive local image descriptor that is based on the phase and amplitude information of Polar Complex Exponential Transform (PCET). The proposed descriptor, called IPCET (Improved PCET), is robust to the common photometric transformations (e.g., illumination, noise, JPEG compression, and blur) and geometric transformations (e.g., scaling, rotation, translation, and significant affine distortion). We perform extensive experiments to compare our IPCET descriptor with six most cutting-edge region descriptors (i.e., SIFT, Zernike Moment, GLOH, PCA-SIFT, SURF, and ORB). Experimental results demonstrate that our IPCET descriptor outperforms cutting-edge moment-based descriptors.}
}
@article{BOURSINOS2023109734,
title = {Efficient probability intervals for classification using inductive venn predictors},
journal = {Pattern Recognition},
volume = {143},
pages = {109734},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109734},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004326},
author = {Dimitrios Boursinos and Xenofon Koutsoukos},
keywords = {Deep neural networks, Assurance monitoring, Inductive Venn predictors, Probability intervals},
abstract = {Learning enabled components are frequently used by autonomous systems and it is common for deep neural networks to be integrated in such systems for their ability to learn complex, non-linear data patterns and make accurate predictions in dynamic environments. However, their large number of parameters and their use as black boxes introduce risks as the confidence in each prediction is unknown and output values like softmax scores are not usually well-calibrated. Different frameworks have been proposed to compute accurate confidence measures along with the predictions but at the same time introduce a number of limitations like execution time overhead or inability to be used with high-dimensional data. In this paper, we use the Inductive Venn Predictors framework for computing probability intervals regarding the correctness of each prediction in real-time. We propose taxonomies based on distance metric learning to compute informative probability intervals in applications involving high-dimensional inputs. By assigning pseudo-labels to unlabeled input data during system deployment we further improve the efficiency of the computed probability intervals. Empirical evaluation on image classification and botnet attacks detection in Internet-of-Things (IoT) applications demonstrates improved accuracy and calibration. The proposed method is computationally efficient, and therefore, can be used in real-time. The code is available at https://github.com/dboursinos/Efficient-Probability-Intervals-Classification-Inductive-Venn-Predictors.}
}
@article{KANG2023109840,
title = {Structure-preserving image translation for multi-source medical image domain adaptation},
journal = {Pattern Recognition},
volume = {144},
pages = {109840},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109840},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005381},
author = {Myeongkyun Kang and Philip Chikontwe and Dongkyu Won and Miguel Luna and Sang Hyun Park},
keywords = {Domain adaptation, Data augmentation, Mutual information, Segmentation, Unpaired image translation},
abstract = {Domain adaptation is an important task for medical image analysis to improve generalization on datasets collected from diverse institutes using different scanners and protocols. For images with visible domain shift, using image translation models is an intuitive and effective way to perform domain adaptation, but the structure of the generated image may often be distorted when large content discrepancies between domains exist; resulting in poor downstream task performance. To address this, we propose a novel image translation model that disentangles structure and texture to only transfer the latter by using mutual information and texture co-occurrence losses. We translate source domain images to the target domain and employ the generated results as augmented samples for domain adaptation segmentation training. We evaluate our method on three public segmentation datasets: MMWHS, Fundus, and Prostate datasets acquired from diverse institutes. Experimental results show that a segmentation model trained using the augmented images from our approach outperforms state-of-the-art domain adaptation, image translation, and domain generalization methods.}
}
@article{CUI2023109759,
title = {Temporal-Relational hypergraph tri-Attention networks for stock trend prediction},
journal = {Pattern Recognition},
volume = {143},
pages = {109759},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109759},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004570},
author = {Chaoran Cui and Xiaojie Li and Chunyun Zhang and Weili Guan and Meng Wang},
keywords = {Stock trend prediction, Stock investment simulation, Hypergraph convolutional networks, Attention mechanism},
abstract = {Predicting the future price trends of stocks is a challenging yet intriguing problem given its critical role to help investors make profitable decisions. In this paper, we present a collaborative temporal-relational modeling framework for end-to-end stock trend prediction. Different from existing studies relying on the pairwise correlations between stocks, we argue that stocks are naturally connected as a collective group, and introduce two heterogeneous hypergraphs to separately characterize the stock group-wise relationships of industry-belonging and fund-holding. A novel hypergraph tri-attention network (HGTAN) is proposed to augment the hypergraph convolutional networks with a hierarchical organization of intra-hyperedge, inter-hyperedge, and inter-hypergraph attention modules. In this manner, HGTAN adaptively determines the importance of nodes, hyperedges, and hypergraphs during the information propagation among stocks, so that the potential synergies between stock movements can be fully exploited. Experimental evaluation and investment simulation on real-world stock data demonstrate the effectiveness of our approach.}
}
@article{ELAMOURI2023109804,
title = {Constrained DTW preserving shapelets for explainable time-series clustering},
journal = {Pattern Recognition},
volume = {143},
pages = {109804},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109804},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005022},
author = {Hussein El Amouri and Thomas Lampert and Pierre Gançarski and Clément Mallet},
keywords = {Shapelets, Semi-supervised learning, Constrained clustering, Time-series, Representation learning},
abstract = {The analysis of time series is becoming ever more popular due to the proliferation of sensors. A well-known similarity measure for time-series is Dynamic Time Warping (DTW), which does not respect the axioms of a metric. These, however, can be reintroduced through Learning DTW-Preserving Shapelets (LDPS). This article extends LDPS and presents constrained DTW-preserving shapelets (CDPS). CDPS directs the time-series representation to captures the user’s interpretation of the data by considering a limited amount of user knowledge in the from of must-link- cannot link constraints. Subsequently, unconstrained algorithms can be used to generate a clustering that respects the constraints without explicit knowledge of them. Out-of-sample data can be transformed into this space, overcoming the limitations of traditional transductive constrained-clustering algorithms. Furthermore, several Shapelet Cluster Explanation (SCE) approaches are proposed that explain the clustering and can simplify the representation while preserving clustering performance. State-of-the-art performance is demonstrated on multiple time-series datasets and an open-source implementation will be made publicly available upon acceptance.}
}
@article{LEE2023109731,
title = {Resampling approach for one-Class classification},
journal = {Pattern Recognition},
volume = {143},
pages = {109731},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109731},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004296},
author = {Hae-Hwan Lee and Seunghwan Park and Jongho Im},
keywords = {One-class classification, Data-driven approach, Oversampling, Calibration},
abstract = {The performance of a classification model depends significantly on the degree to which the support of each data class overlaps. Successfully distinguishing between classes is difficult if the support is similar. In the one-class classification (OCC) problem, wherein the data comprise only a single class, the classifier performance is significantly degraded if the population support of each class is similar. In this study, we propose a resampling algorithm that enhances classifier performance by utilizing the macro information that is most easily obtainable in these two problem situations. The algorithm aims to improve classifier performance by reprocessing the given data into data with mitigated class imbalance through raking and sampling techniques. This performance improvement is demonstrated by comparing representative classifiers used in the existing OCC problem with traditional binary classifier models, which are unavailable on a single-class dataset.}
}
@article{XU2023109787,
title = {Conditional Independence Induced Unsupervised Domain Adaptation},
journal = {Pattern Recognition},
volume = {143},
pages = {109787},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109787},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004855},
author = {Xiao-Lin Xu and Geng-Xin Xu and Chuan-Xian Ren and Dao-Qing Dai and Hong Yan},
keywords = {Domain Adaptation, Discriminant Analysis, Feature Learning, Conditional Independence, Classification},
abstract = {Learning domain-adaptive features is important to tackle the dataset bias problem, where data distributions in the labeled source domain and the unlabeled target domain can be different. The critical issue is to identify and then reduce the redundant information including class-irrelevant and domain-specific features. In this paper, a conditional independence induced unsupervised domain adaptation (CIDA) method is proposed to tackle the challenges. It aims to find the low-dimensional and transferable feature representation of each observation, namely the latent variable in the domain-adaptive subspace. Technically, two mutual information terms are optimized at the same time. One is the mutual information between the latent variable and the class label, and the other is the mutual information between the latent variable and the domain label. Note that the key module can be approximately reformulated as a conditional independence/dependence based optimization problem, and thus, it has a probabilistic interpretation with the Gaussian process. Temporary labels of the target samples and the model parameters are alternatively optimized. The objective function can be incorporated with deep network architectures, and the algorithm is implemented iteratively in an end-to-end manner. Extensive experiments are conducted on several benchmark datasets, and the results show effectiveness of CIDA.}
}
@article{YANG2023109722,
title = {Preferred vector machine for forest fire detection},
journal = {Pattern Recognition},
volume = {143},
pages = {109722},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109722},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300420X},
author = {Xubing Yang and Zhichun Hua and Li Zhang and Xijian Fan and Fuquan Zhang and Qiaolin Ye and Liyong Fu},
keywords = {Forest fire detection, Fire detection rate, Error warning rate, SVM, Dual representation},
abstract = {Machine learning-based fire detection/recognition is very popular in forest-monitoring systems. However, without considering the prior knowledge, e.g., equal attention on both classes of the fire and non-fire samples, fire miss-detected phenomena frequently appeared in the current methods. In this work, considering model’s interpretability and the limited data for model-training, we propose a novel pixel-precision method, termed as PreVM (Preferred Vector Machine). To guarantee high fire detection rate under precise control, a new L0 norm constraint is introduced to the fire class. Computationally, instead of the traditional L1 re-weighted techniques in L0 norm approximation, this L0 constraint can be converted into linear inequality and incorporated into the process of parameter selection. To further speed up model-training and reduce error warning rate, we also present a kernel-based L1 norm PreVM (L1-PreVM). Theoretically, we firstly prove the existence of dual representation for the general Lp (p≥1) norm regularization problems in RKHS (Reproducing Kernel Hilbert Space). Then, we provide a mathematical evidence for L1 norm kernelization to conquer the case when feature samples do not appear in pairs. The work also includes an extensive experimentation on the real forest fire images and videos. Compared with the-state-of-art methods, the results show that our PreVM is capable of simultaneously achieving higher fire detection rates and lower error warning rates, and L1-PreVM is also superior in real-time detection.}
}
@article{DELGADOSANTOS2023109798,
title = {Exploring transformers for behavioural biometrics: A case study in gait recognition},
journal = {Pattern Recognition},
volume = {143},
pages = {109798},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109798},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300496X},
author = {Paula Delgado-Santos and Ruben Tolosana and Richard Guest and Farzin Deravi and Ruben Vera-Rodriguez},
keywords = {Biometrics, Behavioural biometrics, Gait recognition, Deep learning, Transformers, Mobile devices},
abstract = {Biometrics on mobile devices has attracted a lot of attention in recent years as it is considered a user-friendly authentication method. This interest has also been motivated by the success of Deep Learning (DL). Architectures based on Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) have established convenience for the task, improving the performance and robustness in comparison to traditional machine learning techniques. However, some aspects must still be revisited and improved. To the best of our knowledge, this is the first article that explores and proposes a novel gait biometric recognition systems based on Transformers, which currently obtain state-of-the-art performance in many applications. Several state-of-the-art architectures (Vanilla, Informer, Autoformer, Block-Recurrent Transformer, and THAT) are considered in the experimental framework. In addition, new Transformer configurations are proposed to further increase the performance. Experiments are carried out using the two popular public databases: whuGAIT and OU-ISIR. The results achieved prove the high ability of the proposed Transformer, outperforming state-of-the-art CNN and RNN architectures.}
}
@article{WANG2023109775,
title = {Learning pixel-adaptive weights for portrait photo retouching},
journal = {Pattern Recognition},
volume = {143},
pages = {109775},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109775},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004739},
author = {Binglu Wang and Chengzhe Lu and Dawei Yan and Yongqiang Zhao and Ning Li and Xuelong Li},
keywords = {Portrait photo retouching, 3D Lookup table, Visual attention},
abstract = {The lookup table-based methods achieve promising retouching performance by learning image-adaptive weights to combine 3-dimensional lookup tables (3D LUTs) and conducting pixel-to-pixel color transformation. However, this paradigm ignores the local context cues and applies the same transformation to portrait pixels and background pixels that exhibit the same raw RGB values. In contrast, an expert usually conducts different operations to adjust the color temperatures, tones of portrait regions, and background regions. This inspires us to model local context cues to improve the retouching quality explicitly.Thus, the center pixel of an image patch is first retouched by predicting pixel-adaptive lookup table weights. To modulate the influence of neighboring pixels, as neighboring pixels exhibit different affinities to the center pixel, a local attention mask is estimated. Then, the quality of the local attention mask is further improved by applying supervision, which is based on the affinity map calculated by the ground-truth portrait mask. For group-level consistency, we propose to directly constrain the variance of mean color components in the Lab space. Extensive experiments on the PPR10K dataset demonstrate the effectiveness of the proposed method, the retouching performance on high-resolution photos is improved by over 0.5dB in terms of PSNR, and the group-level inconsistency is reduced by 2.1.}
}
@article{XIAO2023109828,
title = {Revisiting the transferability of adversarial examples via source-agnostic adversarial feature inducing method},
journal = {Pattern Recognition},
volume = {144},
pages = {109828},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109828},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005265},
author = {Yatie Xiao and Jizhe Zhou and Kongyang Chen and Zhenbang Liu},
keywords = {Adversarial attack, Transferability, Feature inducing, Diversity},
abstract = {Though deep neural networks (DNNs) have revealed their extraordinary performance in the fields of computer vision, it is evident that the vulnerability of DNNs to adversarial attacks with crafted human-imperceptible perturbations. Most existing adversarial attacks draw their attention to invading target deep task models by enhancing input-diagnostic features via image rotation, warp, or transformation to improve adversarial transferability. Such manners pay close concentration to operation on original inputs regardless of the properties from different source information. Research has inspired us to consider utilizing source-agnostic information and integrating generated features with raw inputs to enrich adversarial properties. For such needs, we propose a simple and flexible adversarial attack method with source-agnostic Feature Inducing Method (FIM) for improving the transferability of adversarial examples (AEs). FIM first focuses on generating perturbed features by imitating diverse patterns from multi-domain sources. Instead of exploiting the original inputs’ diversity, such proposed work gains the various properties by random feature imitation referring to different source distributions. By optimizing the generated features with norm bounds, FIM then integrates original inputs with imitative features. Such manner can diverse row positive class-general features, which reduce the capability of class-specific patterns on cross-model transferability. Based on the crafted property, FIM employs the adaptive gradient-based strategy on such information to generate perturbations, which helps to decrease probability dropping into local optimal when searching for the decision boundary of source and target models. We conduct detailed experiments to evaluate the performance of our proposed approach with existing baselines on three public datasets. The experimental results reveal the better performance of the proposed works on fooling source and target task models leading to a considerable margin in most adversarial scenarios. We further investigate adversarial attacks on adversarial defense models (with adversarial training and trades). Such a proposed attack strategy achieves better attack quality by a margin over 3.00% on CIFAR10 and reduces the robust accuracy of adversarially trained models by a large margin near 9.00% on MNIST. Furthermore, we exploit the performance of the proposed attack strategy applied to feature-level adversarial domains and conduct evaluations to demonstrate its adversarial feasibility in integrating with various attack mechanisms, which gains better adversarial effectiveness over 20.00% than the base attacks on studied deep task models.}
}
@article{YI2023109799,
title = {Graph classification via discriminative edge feature learning},
journal = {Pattern Recognition},
volume = {143},
pages = {109799},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109799},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004971},
author = {Yang Yi and Xuequan Lu and Shang Gao and Antonio Robles-Kelly and Yuejie Zhang},
keywords = {GCNNs, Graph construction, Graph datasets, Graph classification},
abstract = {Spectral graph convolutional neural networks (GCNNs) have been producing encouraging results in graph classification tasks. However, most spectral GCNNs utilize fixed graphs when aggregating node features while omitting edge feature learning and failing to get an optimal graph structure. Moreover, many existing graph datasets do not provide initialized edge features, further restraining the ability of learning edge features via spectral GCNNs. In this paper, we try to address this issue by designing an edge feature scheme and an add-on layer between every two stacked graph convolution layers in spectral GCNN. Both are lightweight while effective in filling the gap between edge feature learning and performance enhancement of graph classification. The edge feature scheme makes edge features adapt to node representations at different spectral graph convolution layers. The add-on layer helps adjust the edge features to an optimal graph structure. To test the effectiveness of our method, we take Euclidean positions as initial node features and extract graphs with semantic information from point cloud objects. The node features of our extracted graphs are more scalable for edge feature learning than most existing graph datasets (in one-hot encoded label format). Three new graph datasets are constructed based on ModelNet40, ModelNet10 and ShapeNet Part datasets. Experimental results show that our method outperforms state-of-the-art graph classification methods on the new datasets. Our code and the constructed graph datasets will be released to the community.}
}
@article{DING2023109747,
title = {Deep forest auto-Encoder for resource-Centric attributes graph embedding},
journal = {Pattern Recognition},
volume = {143},
pages = {109747},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109747},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004454},
author = {Yan Ding and Yujuan Zhai and Ming Hu and Jia Zhao},
keywords = {Graph embedding, Deep random forest, Deep auto-encoder, Self-attention},
abstract = {Graph embedding is an important technique used for representing graph structure data that preserves intrinsic features in a low-dimensional space suitable for graph-based applications. Graphs containing node attributes and weighted links are commonly employed to model various real-world problems and issues in computer science. In recent years, a hot research topic has been the exploitation of diverse information, including node attributes and topological semantic information, in graph embedding. However, due to limitations in deep learning based on neural networks, such information has not been fully utilized nor adequately integrated in existing models, leaving graph embedding unsatisfactory, especially for large resource graphs (e.g., knowledge graphs and task interaction graphs). In this study, we introduce a resource-centric graph embedding approach based on deep random forests learning, which reconstructs graphs using a deep autoencoder to achieve high effectiveness. To accomplish this, our approach employs three key components. The first component is a preprocessor driven by graph similarity, alongside modularity and self-attention modules, to comprehensively integrate graph representation. The second component utilizes local graph information structures to enhance the raw graph. Finally, we integrate diverse information using multi-grained scanning and dual-level cascade forests in the deep learning extractor and generator, ultimately producing the final graph embedding. Experimental results on seven real-world scenarios show that our approach outperforms state-of-the-art embedding methods.}
}
@article{HUA2023109844,
title = {Deep fidelity in DNN watermarking: A study of backdoor watermarking for classification models},
journal = {Pattern Recognition},
volume = {144},
pages = {109844},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109844},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005423},
author = {Guang Hua and Andrew Beng Jin Teoh},
keywords = {Deep fidelity, Backdoor watermarking, Backdoor fidelity, Deep learning security, Neural network watermarking, Intellectual property protection, Ownership verification},
abstract = {Backdoor watermarking is a promising paradigm to protect the copyright of deep neural network (DNN) models. In the existing works on this subject, researchers have intensively focused on watermarking robustness, while the concept of fidelity, which is concerned with the preservation of the model’s original functionality, has received less attention. In this paper, focusing on deep image classification models, we show that the existing shared notion of the sole measurement of learning accuracy is inadequate to characterize backdoor fidelity. Meanwhile, we show that the analogous concept of embedding distortion in multimedia watermarking, interpreted as the total weight loss (TWL) in DNN backdoor watermarking, is also problematic for fidelity measurement. To address this challenge, we propose the concept of deep fidelity, which states that the backdoor watermarked DNN model should preserve both the feature representation and decision boundary of the unwatermarked host model. To achieve deep fidelity, we propose two loss functions termed penultimate feature loss (PFL) and softmax probability-distribution loss (SPL) to preserve feature representation, while the decision boundary is preserved by the proposed fix last layer (FixLL) treatment, inspired by the recent discovery that deep learning with a fixed classifier causes no loss of learning accuracy. With the above designs, both embedding from scratch and fine-tuning strategies are implemented to evaluate the deep fidelity of backdoor embedding, whose advantages over the existing methods are verified via experiments using ResNet18 for MNIST and CIFAR-10 classifications, and wide residual network (i.e., WRN28_10) for CIFAR-100 task. PyTorch codes are available at https://github.com/ghua-ac/dnn_watermark.}
}
@article{CAI2023109771,
title = {Domain embedding transfer for unequal RGB-D image recognition},
journal = {Pattern Recognition},
volume = {143},
pages = {109771},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109771},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004697},
author = {Ziyun Cai and Xiao-Yuan Jing and Ling Shao},
keywords = {Domain adaptation, RGB-D data, Visual categorization, Unequal category},
abstract = {Most recent unsupervised domain adaptation (UDA) approaches concentrate on single RGB source to single RGB target task. They have to face the real-world scenario, where the source domain can be collected from multiple modalities, e.g., RGB data and depth data. Our work focuses on a more practical and challenging scenario which recognizes RGB images by learning from RGB-D data under the label inequality scenario. We are confronted with three challenges: multiple modalities in the source domain, domain shifting problem and unequal label numbers. To address the aforementioned settings, a novel method, referred to as Domain depth Embedding Transfer (DdET) is proposed, which takes advantage of the depth data in the source domain and handles the domain distribution mismatch under label inequality scenario simultaneously. We conduct comprehensive experiments on five cross domain image classification tasks and observe that DdET can perform favorably against state-of-the-art methods, especially under label inequality scenario.}
}
@article{JI2023109816,
title = {Paired contrastive feature for highly reliable offline signature verification},
journal = {Pattern Recognition},
volume = {144},
pages = {109816},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109816},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005149},
author = {Xiaotong ji and Daiki Suehiro and Seiichi Uchida},
keywords = {Writer-independent signature verification, Skilled forgery, Offline signature verification, Paired contrastive feature, Learning with rejection, Top-rank learning},
abstract = {Signature verification requires high reliability. Especially in the writer-independent scenario with the skilled-forgery-only condition, achieving high reliability is challenging but very important. In this paper, we propose to apply two machine learning frameworks, learning with rejection and top-rank learning, to this task because they can suppress ambiguous results and thus give only reliable verification results. Since those frameworks accept a single input, we transform a pair of genuine and query signatures into a single feature vector, called Paired Contrastive Feature (PCF). PCF internally represents similarity (or discrepancy) between the two paired signatures; thus, reliable machine learning frameworks can make reliable decisions using PCF. Through experiments on three public signature datasets in the offline skilled-forgery-only writer-independent scenario, we evaluate and validate the effectiveness and reliability of the proposed models by comparing their performance with a state-of-the-art model.}
}
@article{ZHANG2023109763,
title = {Switching clusters’ synchronization for discrete space-time complex dynamical networks via boundary feedback controls},
journal = {Pattern Recognition},
volume = {143},
pages = {109763},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109763},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004612},
author = {Tianwei Zhang and Zhouhong Li},
keywords = {Complex networks, Space-time discretization, Switching system, Wirtinger’s inequality, Cluster synchronization},
abstract = {Unlike the existing literatures that consider only discrete-time networks, this paper explores the double effects of both discrete time and discrete spatial diffusions in a switching complex dynamical networks. By means of the knowledge of clusters controls, a clusters synchronous frame of space-time discrete switching complex networks with boundary feedback controller is newly proposed and established. With the helps of some indispensable vector-valued sequence inequalities and Lyapunov function with switching signals and clusters’ information, the boundary feedback controllers are designed to synchronize space-time discrete switching complex networks coupled with nodes’ states or spatial diffusions in the form of clusters. Additionally, a realizable computer algorithm is given to make the derived results of this paper easier to enforce. The current work is pioneering in consideration of discrete spatial diffusions and provides a theoretical and practical basis for future research in this regard.}
}
@article{WANG2023109809,
title = {Information-diffused graph tracking with linear complexity},
journal = {Pattern Recognition},
volume = {143},
pages = {109809},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109809},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005071},
author = {Zhixing Wang and Jinzhen Yao and Chuanming Tang and Jianlin Zhang and Qiliang Bao and Zhenming Peng},
keywords = {Siamese tracker, Graph neural network, Attention mechanism, Linear attention},
abstract = {Mainstream tracking approaches have achieved remarkable performance by adopting transformer structures. However, transformer structures’ inherent design of dot-product with softmax normalization incurs quadratic computation complexity regarding sequence length. This issue is further complicated when vision tasks employ softmax attention, as sequence length scales with the square of images’ sizes. Even though sparse attention and low-rank decomposition can alleviate over-inflated computation, it is still laborious to balance trackers’ accuracy, computation cost, and inference speed. To tackle the above problems, we propose an Information-Diffused Graph tracking pipeline with linear complexity (IDGtrack). As the feature constraint relationship in the physical world is an important cue for vision tasks, graph modules are constructed with information-diffused adjacency matrices to substitute softmax attention, which is not only efficient for linear computations but also maintains the non-negativity and global distribution of the attention matrix. Distinct from traditional linear attention methods exclusive to self-attention, a self-integrated and cross-context graph module with linear complexity is explored where a complete bipartite graph is established between the target and search region, facilitating a comprehensive perception of local and background information. Extensive experiments are conducted on public tracking benchmarks, demonstrating that our method achieves state-of-the-art (SOTA) performance with 111 FPS on GPU RTX3090.}
}
@article{LI2023109849,
title = {Few pixels attacks with generative model},
journal = {Pattern Recognition},
volume = {144},
pages = {109849},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109849},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005472},
author = {Yang Li and Quan Pan and Zhaowen Feng and Erik Cambria},
keywords = {Neural network vulnerability, Adversarial attack, Few pixels attacks, Generative attack},
abstract = {Adversarial attacks have attracted much attention in recent years, and a number of works have demonstrated the effectiveness of attacks on the entire image at perturbation generation. However, in practice, specially designed perturbation of the entire image is impractical. Some work has crafted adversarial samples with a few scrambled pixels by advanced search, e.g., SparseFool, OnePixel, etc., but they take more time to find such pixels that can be perturbed. Therefore, to construct the adversarial samples with few pixels perturbed in an end-to-end way, we propose a new framework, in which a dual-decoder VAE for perturbations finding is designed. To make adversarial learning more effective, we proposed a new version of the adversarial loss by considering the generalization. To evaluate the sophistication of the proposed framework, we compared it with more than a dozen existing related attack methods. The effectiveness and efficiency of the proposed framework are verified from the extensive experimental results. The validity of the model structure is also validated by the ablation study.}
}
@article{ZHU2023109772,
title = {Tri-HGNN: Learning triple policies fused hierarchical graph neural networks for pedestrian trajectory prediction},
journal = {Pattern Recognition},
volume = {143},
pages = {109772},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109772},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004703},
author = {Wenjun Zhu and Yanghong Liu and Peng Wang and Mengyi Zhang and Tian Wang and Yang Yi},
keywords = {Trajectory prediction, Hierarchical policy, Graph neural networks,},
abstract = {In complex and dynamic urban traffic scenarios, the accurate trajectory prediction of surrounding pedestrians with interactive behaviors plays a vital role in the self-driving system. Intrinsic factors and extrinsic factors will inevitably influence the pedestrians trajectory. Intrinsic factors such as pedestrians diversified intentions bring rich and diverse multi-modal future possibilities. Besides, extrinsic factors affecting the future trajectory are accompanied by context semantics such as interactions among pedestrians. However, most of the existing methods discuss two problems (interaction and intention) separately. Considering both two factors impact the trajectory of pedestrians, a Triple Policies Fused Hierarchical Graph Neural Networks (Tri-HGNN) is proposed to model spatial and temporal interactions and intentions among the whole scene of pedestrians at each time step and predict the multiple future trajectories. Tri-HGNN contains three different policies: (i) Extrinsic-level policy is used to extract spatial nodes embedding from the interaction graph of pedestrian trajectories by using the Graph Attention Network. (ii) Intrinsic-level policy adopts the Graph Convolutional Network to infer the human intention for more accurate prediction. Moreover, human intention is influenced by the intrinsic interaction generated among pedestrians, so we fuse the interaction features to grasp the influence of the extrinsic interaction. (iii) Basic-level policy then integrates the heuristic information obtained from other two policies and concatenates it with historical trajectories to make multiple predictions through Temporal Convolution Network. Experimental results show that our model improves performance compared with state-of-the-art methods on the ETH/UCY and SDD benchmarks.}
}
@article{ZHAO2023109796,
title = {APUNet: Attention-guided upsampling network for sparse and non-uniform point cloud},
journal = {Pattern Recognition},
volume = {143},
pages = {109796},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109796},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004946},
author = {Tianming Zhao and Linfeng Li and Tian Tian and Jiayi Ma and Jinwen Tian},
keywords = {Point cloud upsampling, Distance prior, DisTransformer, Attention mechanism, Feature prediction},
abstract = {Point cloud upsampling is a basic low-level task, that is important for improving the quality of a point cloud. However, existing point cloud upsampling methods perform poorly on sparse and non-uniform point clouds, due to that they fail to fully model the relationship between points. To address this issue, in this paper, we propose an attention-guided network called APUNet to exploit the correlation between points, which can perform unsampling for sparse and non-uniform point cloud. In particular, we first propose a feature extraction unit, DisTransformer, which can effectively model the relationship between points by introducing a distance prior to the attention mechanism. We also design a point cloud feature extraction network based on DisTransformer. By computing the correlation between patches and the correlation between points, we fuse the global and local features to better model the correlation of the whole object. Furthermore, we propose a feature prediction module based on attention mechanisms that avoids generating clustered points by transforming the point cloud expansion task into a point cloud prediction task. Qualitative and quantitative experiments reveal the superiority of our method compared to the current state-of-the-art methods. Compared with other point cloud upsampling methods, APUNet can much better upsample non-uniform and extremely sparse point clouds.}
}
@article{JI2023109744,
title = {Tri-objective optimization-based cascade ensemble pruning for deep forest},
journal = {Pattern Recognition},
volume = {143},
pages = {109744},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109744},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004429},
author = {Junzhong Ji and Junwei Li},
keywords = {Ensemble learning, Ensemble pruning, Deep forest, Multi-objective optimization, Coupled diversity},
abstract = {Deep forest is a new multi-layer ensemble model, where the high time costs and storage requirements inhibit its large-scale application. However, current deep forest pruning methods used to alleviate these drawbacks do not consider its cascade coupling characteristics. Therefore, we propose a tri-objective optimization-based cascade ensemble pruning (TOOCEP) algorithm for it. Concretely, we first present a tri-objective optimization-based single-layer pruning (TOOSLP) method to prune its single-layer by simultaneously optimizing three objectives, namely accuracy, independent diversity, and coupled diversity. Particularly, the coupled diversity is designed for deep forest to deal with the coupling relationships between its adjacent layers. Then, we perform TOOSLP in a cascade framework to prune the deep forest layer-by-layer. Experimental results on 15 UCI datasets show that TOOCEP outperforms several state-of-the-art methods in accuracy and pruned rate, which significantly reduces the storage space and accelerate the prediction speed of deep forest.}
}
@article{ZHAO2023109808,
title = {A fast stereo matching network based on temporal attention and 2D convolution},
journal = {Pattern Recognition},
volume = {144},
pages = {109808},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109808},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300506X},
author = {Youchen Zhao and Hua Zhong and Boyuan Jia and Haixiong Li},
keywords = {Stereo matching, Temporal attention, 2D Convolution, Edge cues, Pyramid cost volume},
abstract = {We propose a fast stereo matching network based on temporal attention and 2D convolution (TANet). Due to the high similarity of the disparity between consecutive frames in an image sequence, we propose a temporal attention (TA) module that uses the disparity map of the previous frame to guide the disparity search range in the current frame, thus significantly improving the efficiency of disparity calculation in the cost volume module. Additionally, we propose a hierarchical cost construction and 2D convolution aggregation module that constructs a pyramid cost volume by fusing edge cues to establish detail constraints. This overcomes the problem of difficult convergence caused by information loss when replacing 3D convolution with 2D convolution. Experimental results show that the TA module effectively optimizes the cost volume and, together with 2D convolution, improves the computational speed. Compared with state-of-the-art algorithms, TANet achieves a speedup of nearly 4x, with a running time of 0.061s, and reduces the parameter count by nearly half while decreasing accuracy by 1.1%. Code is available at https://github.com/Y0uchenZ/TANet.}
}
@article{LIN2023109813,
title = {A self-adaptive soft-recoding strategy for performance improvement of error-correcting output codes},
journal = {Pattern Recognition},
volume = {143},
pages = {109813},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109813},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005113},
author = {Guangyi Lin and Jie Gao and Nan Zeng and Yong Xu and Kunhong Liu and Beizhan Wang and Junfeng Yao and Qingqiang Wu},
keywords = {Error-correcting output code, Soft-recoding, Ternary coding, Regression, Multiclass classification},
abstract = {The technique of error-correcting output codes (ECOC) has been proven to be of high discriminative ability in many classification applications. However, most algorithms on the ECOC were designed based on the binary or ternary codes (referred to as the hard codes), which might fail to precisely correct errors in dealing with tough tasks. In this study, a Soft-Recoding strategy based on a self-adaptive algorithm is proposed, which replaces the traditional hard codes with the real-value elements to better fit the output distribution of the base learners. This is achieved by minimizing the ratio of two distances: the distance of the output vector to the ground-truth class, and the average distance of the output vector to the remaining classes. Extensive experiments using five different hard ECOC algorithms and the corresponding softened versions on twenty datasets with diversified numbers of features and classes confirm the effectiveness of our Soft-Recoding strategy in promoting the performance of the original ECOC algorithms. Our source code and additional results are available at: github.com/MLDMXM2017/SA-soft-recoding.}
}
@article{HUANG2023109837,
title = {Learning consistent region features for lifelong person re-identification},
journal = {Pattern Recognition},
volume = {144},
pages = {109837},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109837},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005356},
author = {Jinze Huang and Xiaohan Yu and Dong An and Yaoguang Wei and Xiao Bai and Jin Zheng and Chen Wang and Jun Zhou},
keywords = {Person re-identification, Lifelong learning, Feature sharing},
abstract = {The lifelong person re-identification (LRe-ID) model retrieves a person across multiple cameras in continuous data streams and learns new coming datasets incrementally. However, there are two well-known challenges: catastrophic forgetting and generalization loss, which arise due to parameter updates and domain shifts. While there has been encouraging progress in balancing these challenges, few existing methods have addressed them from a unified feature perspective. Inspired by the Complementary Learning Systems theory, an effective framework is proposed to share consistent features and extract discriminative features for each sample. Specifically, this framework consists of Property Region Features, Feature Adaption and Feature Perspicacity. Property Region Features are the parametric representation of consistent region features, Feature Adaption and Feature Perspicacity are responsible for diversity features generation and discriminative features extraction, respectively. Moreover, a cascade knowledge distillation structure is introduced to guarantee Property Feature consistency, and correspondingly, a weighted distillation loss function is designed to prevent generalization loss on current domains caused by overlapping historical knowledge. Extensive experiments conducted on twelve Re-ID datasets, including both rehearsal and no-rehearsal settings, clearly validate the superiority of our method over state-of-the-art competitors, with significantly improved performance. The source code will be released on https://github.com/whisperH/ConRFL/.}
}
@article{CHEN2023109780,
title = {Data-free quantization via mixed-precision compensation without fine-tuning},
journal = {Pattern Recognition},
volume = {143},
pages = {109780},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109780},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004788},
author = {Jun Chen and Shipeng Bai and Tianxin Huang and Mengmeng Wang and Guanzhong Tian and Yong Liu},
keywords = {Neural network compression, Date-free quantization},
abstract = {Neural network quantization is a very promising solution in the field of model compression, but its resulting accuracy highly depends on a training/fine-tuning process and requires the original data. This not only brings heavy computation and time costs but also is not conducive to privacy and sensitive information protection. Therefore, a few recent works are starting to focus on data-free quantization. However, data-free quantization does not perform well while dealing with ultra-low precision quantization. Although researchers utilize generative methods of synthetic data to address this problem partially, data synthesis needs to take a lot of computation and time. In this paper, we propose a data-free mixed-precision compensation (DF-MPC) method to recover the performance of an ultra-low precision quantized model without any data and fine-tuning process. By assuming the quantized error caused by a low-precision quantized layer can be restored via the reconstruction of a high-precision quantized layer, we mathematically formulate the reconstruction loss between the pre-trained full-precision model and its layer-wise mixed-precision quantized model. Based on our formulation, we theoretically deduce the closed-form solution by minimizing the reconstruction loss of the feature maps. Since DF-MPC does not require any original/synthetic data, it is a more efficient method to approximate the full-precision model. Experimentally, our DF-MPC is able to achieve higher accuracy for an ultra-low precision quantized model compared to the recent methods without any data and fine-tuning process.}
}
@article{LIANG2023109810,
title = {Variational Bayesian deep network for blind Poisson denoising},
journal = {Pattern Recognition},
volume = {143},
pages = {109810},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109810},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005083},
author = {Hao Liang and Rui Liu and Zhongyuan Wang and Jiayi Ma and Xin Tian},
keywords = {Blind poisson denoising, Variational bayesian, Parameter estimation},
abstract = {Deep learning-based approaches have recently achieved considerable results in Poisson denoising under low-light conditions. However, most existing methods mainly focus on the network architecture design, which lacks physical interpretability and thus unsuitable for blind denoising in real environments with unknown levels of noises. To address this issue, we propose a variational Bayesian deep network for blind Poisson denoising (VBDNet). We mainly consider an approximate posterior form for the noise variance in a variational Bayesian framework and utilize a neural network to parameterize the variance of Poisson noise. For network design, VBDNet is divided into two sub-networks. The noise estimation sub-network is responsible for the Bayesian inference. This network improves the blind denoising ability of the subsequent denoising sub-network by learning Poisson noise characteristics under different noise levels in the training process. A network of U-Net structures implements the denoising sub-network for noise removal. By combining the advantage of Bayesian inference (noise estimation sub-network) and deep learning (denoising sub-network), VBDNet outperforms other state-of-the-art methods on both synthetic and natural data. The code and details are available at https://github.com/HLImg/VBDNet.}
}
@article{SUN2023109729,
title = {Matching based on variance minimization of component distances using edges of free-form surfaces},
journal = {Pattern Recognition},
volume = {143},
pages = {109729},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109729},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004272},
author = {Jingyu Sun and Yadong Gong and Jibin Zhao and Huan Zhang and Liya Jin},
keywords = {Normal and tangential distance, Variance minimization, Edge of point cloud, Allowance distribution, Fine registration},
abstract = {The basis for guidance in the field of automated robot processing is modeling by visual scanning. Matching algorithms are the real link that can be made between the ideal model and the subsequent robot processing. The matching algorithm that meets the machining requirements plays a pivotal role in the entire process, which provides the exact location of design models and measurement data. In order to meet the requirement of making the machining allowance uniform, a fine registration method which considers the variance minimization of the normal and tangential distances between the edge neighbors of the scattered point cloud is proposed. The edge neighbors are achieved by local growing of edge seeds based on the minimization of energy of supervoxel, which can represent the model more accurate than edge points extracted directly. The edge neighbor points are used to participate in the calculation, and the objective function of minimizing the variance of the two-way distance with the introduction of weight coefficients is proposed to constrain the iterative process. The effect of the distance in two directions on the result is analyzed, to determine the appropriate weight coefficients so that the matching calculation converges quickly and accurately. In comparison with other classical and state-of-the-art matching methods, the method in this paper performs well in terms of solution efficiency and accuracy of results. Moreover, the ability of this paper’s method to resist Gaussian noise is investigated, and it is found that this paper’s method has good robustness when σ is less than 1 for Gaussian noise. Ultimately, a uniformly distributed residual model is obtained to provide a visually guided basis for subsequent machining.}
}
@article{TANG2023109826,
title = {AC2AS: Activation Consistency Coupled ANN-SNN framework for fast and memory-efficient SNN training},
journal = {Pattern Recognition},
volume = {144},
pages = {109826},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109826},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005241},
author = {Jianxiong Tang and Jian-Huang Lai and Xiaohua Xie and Lingxiao Yang and Wei-Shi Zheng},
keywords = {Spiking neural networks, Deep learning, Supervised learning, Image classification},
abstract = {Spiking neural networks are efficient computation models for low-power environments. Spike-based BP algorithms and ANN-to-SNN (ANN2SNN) conversions are successful techniques for SNN training. Nevertheless, the spike-base BP training is slow and requires large memory costs, while ANN2SNN needs many inference steps to obtain good performance. In this paper, we propose an Activation Consistency Coupled ANN-SNN (AC2AS) framework to train the SNN in a fast and memory-efficient way. The AC2AS consists of two components: (a) a weight-shared architecture between ANN and SNN and (b) spiking mapping units. Firstly, the architecture trains the weight-shared parameters on the ANN branch, resulting in fast training and low memory costs for SNN. Secondly, the spiking mapping units are designed to ensure that the activation values of the ANN are the spiking features. As a result, the activation consistency is guaranteed, and the classification error of the SNN can be optimized by training the ANN branch. Besides, we design an adaptive threshold adjustment (ATA) algorithm to decrease the firing of noisy spikes. Experiment results show that our AC2AS-based models perform well on the benchmark datasets (CIFAR10, CIFAR100, and Tiny-ImageNet). Moreover, the AC2AS achieves comparable accuracy under 0.625× time steps, 0.377× training time, 0.27× GPU memory costs, and 0.33× spike activities of the Spike-based BP model. The code is available at https://github.com/TJXTT/AC2ASNN.}
}
@article{SCABINI2023109802,
title = {RADAM: Texture recognition through randomized aggregated encoding of deep activation maps},
journal = {Pattern Recognition},
volume = {143},
pages = {109802},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109802},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005009},
author = {Leonardo Scabini and Kallil M. Zielinski and Lucas C. Ribas and Wesley N. Gonçalves and Bernard {De Baets} and Odemir M. Bruno},
keywords = {Texture analysis, Randomized neural networks, Transfer learning, Convolutional networks, Feature extraction},
abstract = {Texture analysis is a classical yet challenging task in computer vision for which deep neural networks are actively being applied. Most approaches are based on building feature aggregation modules around a pre-trained backbone and then fine-tuning the new architecture on specific texture recognition tasks. Here we propose a new method named Random encoding of Aggregated Deep Activation Maps (RADAM) which extracts rich texture representations without ever changing the backbone. The technique consists of encoding the output at different depths of a pre-trained deep convolutional network using a Randomized Autoencoder (RAE). The RAE is trained locally to each image using a closed-form solution, and its decoder weights are used to compose a 1-dimensional texture representation that is fed into a linear SVM. This means that no fine-tuning or backpropagation is needed for the backbone. We explore RADAM on several texture benchmarks and achieve state-of-the-art results with different computational budgets. Our results suggest that pre-trained backbones may not require additional fine-tuning for texture recognition if their learned representations are better encoded.}
}
@article{ZHANG2023109773,
title = {Crop classification based on multi-temporal PolSAR images with a single tensor network},
journal = {Pattern Recognition},
volume = {143},
pages = {109773},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109773},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004715},
author = {Wei-Tao Zhang and Lu Liu and Yv Bai and Yi-Bang Li and Jiao Guo},
keywords = {Polarimetric synthetic aperture radar (PolSAR), Crop classification, Tensor affine transformation network (TATN)},
abstract = {Accurate and reliable discrimination of crop categories is a significant data source for agricultural monitoring and food security evaluation research. The convolutional neural network (CNN) model is one of the most popular classifiers for crop discrimination based on polarimetric synthetic aperture radar (PolSAR) data. However, it is better to avoid directly using large amounts of raw features that extracted from PolSAR data for CNN models because of the “dimension disaster” problem caused by multiple periods and various feature extraction schemes. Consequently, an extra feature compression model has to be incorporated to mitigate the “dimension disaster” problem. However, ill coupling of the two models may result in degraded classification performance, thus the combination of two models has to be further optimized. In this paper, we propose a novel single tensor affine transformation network (TATN) for crop classification using multi-temporal PolSAR data, where the input sample of the network is a higher order tensor formed by raw features, and the hidden layers of the network adopt the tensor affine transformation rather than convolution to extract discriminative features for classification. Since the tensor affine transformation preserves the structural information of the original input tensor samples, the TATN is expected to achieve a higher crop classification accuracy. Moreover, the TATN holds less amount of parameters than most of deep learning models, which enables to avoid the extra feature compression procedure. The experimental results validate the merits of our model.}
}
@article{DUQUEDOMINGO2023109797,
title = {One Shot Learning with class partitioning and cross validation voting (CP-CVV)},
journal = {Pattern Recognition},
volume = {143},
pages = {109797},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109797},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004958},
author = {Jaime Duque-Domingo and Roberto Medina Aparicio and Luis Miguel González Rodrigo},
keywords = {One Shot Learning, CP-CVV, Siamese, ConvNeXt, ViT, Regnet, Wide ResNet, ResNeXt, CVV, CNN, Ensemble models, Voting, Grocery product classification, CIFARFS},
abstract = {One Shot Learning includes all those techniques that make it possible to classify images using a single image per category. One of its possible applications is the identification of food products. For a grocery store, it is interesting to record a single image of each product and be able to recognise it again from other images, such as photos taken by customers. Within deep learning, Siamese neural networks are able to verify whether two images belong to the same category or not. In this paper, a new Siamese network training technique, called CP-CVV, is presented. It uses the combination of different models trained with different classes. The separation of validation classes has been done in such a way that each of the combined models is different in order to avoid overfitting with respect to the validation. Unlike normal training, the test images belong to classes that have not previously been used in training, allowing the model to work on new categories, of which only one image exists. Different backbones have been evaluated in the Siamese composition, but also the integration of multiple models with different backbones. The results show that the model improves on previous works and allows the classification problem to be solved, an additional step towards the use of Siamese networks. To the best of our knowledge, there is no existing work that has proposed integrating Siamese neural networks using a class-based validation set separation technique so as to be better at generalising for unknown classes. Additionally, we have applied Cross-Validation-Voting with ConvNeXt to improve the existing classification results of a well-known Grocery Store Dataset.}
}
@article{WANG2023109784,
title = {Improving point cloud classification and segmentation via parametric veronese mapping},
journal = {Pattern Recognition},
volume = {144},
pages = {109784},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109784},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300482X},
author = {Ruibin Wang and Xianghua Ying and Bowei Xing and Xin Tong and Taiyan Chen and Jinfa Yang and Yongjie Shi},
keywords = {3D Point cloud, Deep learning, Non-linear space mapping, Shape classification, Part segmentation, Semantic segmentation},
abstract = {Deep learning based 3D point cloud classification and segmentation has achieved remarkable success. Existing methods are usually implemented in the original space with 3D coordinates as inputs. However, we find that point networks taking only information of first-order coordinates hardly learn geometric features of higher order, such as point cloud normals or poses. In this study, we propose to map the input point clouds into a non-linear space to facilitate networks learning and leveraging high-order features. Firstly, we design the Parametric Veronese Mapping (PVM) function which automatically learns to map point clouds into a non-linear space. As a result, the mapped point clouds are enriched with high-order elements and maintain the basic point set properties as in the original 3D space. We can then exploit existing networks to learn high-order features from mapped point clouds. Secondly, we contribute a two-stage transformation learning module that modifies the previous one-stage module to better leverage high-order features for aligning point clouds in the projective space. Finally, an interaction module is designed to learn more discriminative features by aggregating information from both the original and projective space. Extensive experiments demonstrate that our method successfully improves the ability of most existing networks to learn high-order features and thus contributing to more accurate classification and segmentation. Moreover, the resulting models show stronger robustness to affine transformations and real-world perturbations.}
}
@article{WANG2023109745,
title = {Uncovering Hidden Vulnerabilities in Convolutional Neural Networks through Graph-based Adversarial Robustness Evaluation},
journal = {Pattern Recognition},
volume = {143},
pages = {109745},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109745},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004430},
author = {Ke Wang and Zicong Chen and Xilin Dang and Xuan Fan and Xuming Han and Chien-Ming Chen and Weiping Ding and Siu-Ming Yiu and Jian Weng},
keywords = {Graph of patterns, Graph distance algorithm, Adversarial robustness, Interpretable graph-based systems, Convolutional neural networks},
abstract = {Convolutional neural networks (CNNs) are widely used for image classification, but their vulnerability to adversarial attacks poses challenges to their reliability and security. However, current adversarial robustness (AR) measures lack a theoretical foundation, limiting the insight into the decision process. To address this issue, we propose a new AR evaluation framework based on Graph of Patterns (GoPs) models and graph distance algorithms. Our approach provides a fine-grained analysis of AR from three perspectives, providing targeted insight into the vulnerability of CNNs. Compared to current standards, our approach is theoretically grounded and allows fine-tuning of model components without repeated attempts and validation. Our experimental results demonstrate its effectiveness in uncovering hidden vulnerabilities in CNNs and providing actionable approaches to improve their AR. Our GoPs modeling approach and graph distance algorithms can be extended to apply to other graph machine learning tasks such as Metric Learning on multi-relational graphs. Overall, our framework represents significant progress in AR evaluation, providing a more interpretable, targeted, and efficient approach to assess CNN robustness in complex graph-based systems.}
}
@article{GIANNOULIS2023109814,
title = {DITAN: A deep-learning domain agnostic framework for detection and interpretation of temporally-based multivariate ANomalies},
journal = {Pattern Recognition},
volume = {143},
pages = {109814},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109814},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005125},
author = {Michail Giannoulis and Andrew Harris and Vincent Barra},
keywords = {Multivariate time series, Anomaly detection, Neural networks, Generic normality feature learning, Predictability modeling},
abstract = {We present DITAN, a novel unsupervised domain-agnostic framework for detecting and interpreting temporal-based anomalies. It is based on an encoder-decoder architecture with both implicit/explicit attention and adjustable layers/units for predicting normality as regular patterns in sequential data. A two-stage thresholding methodology with built-in pruning is used to detect anomalies, while root cause and similarities are interpreted in data and units space. Our approach is designed to intersect the 9 fundamental characteristics extracted from the union of related works. We demonstrate the DITAN modules on real-world datasets of 6 multivariate time series contaminated by point and contextual temporal-based anomalies at a varying duration. Experiments show a dominant predictability power of DITAN against the originally proposed models. DITAN is able to determine critical regions and thus identify anomalous events similarly well. Informative similarities between anomalous records are interpreted, since almost all similarities in units space are also verified in data space.}
}
@article{HUANG2023109838,
title = {EEG-based classification combining Bayesian convolutional neural networks with recurrence plot for motor movement/imagery},
journal = {Pattern Recognition},
volume = {144},
pages = {109838},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109838},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005368},
author = {Wenqie Huang and Guanghui Yan and Wenwen Chang and Yuchan Zhang and Yueting Yuan},
keywords = {Electroencephalogram (EEG), Real execution, Motor imagery (MI), Deep learning (DL), Bayesian convolutional neural networks (BCNNs), Recurrence plots (RPs), Feature extraction, Classification},
abstract = {Electroencephalogram (EEG)-based Motor imagery (MI) is a key topic in the brain-computer interface (BCI). The EEG-based real execution and motor imagery multi-class classification tasks are also crucial, but only a few kinds of literature research it. In addition, classification accuracy still has room for improvement, and the inter-individual variability problems in BCI applications need to be solved. To address these issues, we developed a novel model (RP-BCNNs) that combines the recurrence plot (RP) and Bayesian Convolutional Neural Networks (BCNNs). First, we employ an RP computation for preprocessed EEG signals of each channel and merge all RPs of all channels into one based on the weighted average method. Then, we feed the RP features into BCNNs to classify 2-class, 3-class, 4-class, and 5-class on real/imaginary movements classification tasks. The results show that the RP-BCNNs model outperforms the state-of-the-art methods, achieving average accuracies of 92.86%, 94.12%, 91.37%, 92.61% for real movements and 94.07%, 93.77%, 90.54%, 91.85% for imaginary movements. Our findings suggest that combining complex network methods with deep learning can improve the classification performance of EEG-based BCI systems (e.g., motor imagery, emotion recognition, and epileptic seizure classification).}
}
@article{ZHENG2023109825,
title = {Adaptive local adversarial attacks on 3D point clouds},
journal = {Pattern Recognition},
volume = {144},
pages = {109825},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109825},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300523X},
author = {Shijun Zheng and Weiquan Liu and Siqi Shen and Yu Zang and Chenglu Wen and Ming Cheng and Cheng Wang},
keywords = {Point clouds, Adversarial attack, Salient regions, Adversarial examples},
abstract = {Modern artificial intelligence systems rely heavily on deep learning techniques. However, deep neural networks are easily disturbed by adversarial objects. Adversarial examples are beneficial to improve the robustness of the 3D neural network model and enhance the stability of the artificial intelligence system. At present, most 3D adversarial attack methods perturb the entire point cloud to generate adversarial examples, which results in high perturbation costs and low operability in the physical world. In this paper, we propose an adaptive local adversarial attack method (AL-Adv) on 3D point clouds to generate adversarial point clouds. First, we analyze the vulnerability of the 3D network model and extract the salient regions of the input point cloud, namely the vulnerable regions. Second, we propose an adaptive gradient attack algorithm that targets salient regions. The proposed attack algorithm adaptively assigns different disturbances in different directions of the three-dimensional coordinates of the point cloud. Experimental results show that our proposed AL-Adv method achieves a higher attack success rate than the global attack method. Specifically, the adversarial examples generated by AL-Adv demonstrate good imperceptibility and small generation costs.}
}
@article{PENG2023109785,
title = {MSINet: Mining scale information from digital surface models for semantic segmentation of aerial images},
journal = {Pattern Recognition},
volume = {143},
pages = {109785},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109785},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004831},
author = {Chengli Peng and Haifeng Li and Chao Tao and Yansheng Li and Jiayi Ma},
keywords = {Semantic segmentation, Aerial image, Multi-scale, Digital surface model},
abstract = {Compared with other kinds of images, aerial images have more obvious object scale distinction and larger resolution, which results in that the whole scale information of aerial images can hardly be explored. To address this difficulty, we develop a novel network based on the digital surface models (DSMs) of aerial images in this paper. The proposed network termed MSINet can efficiently mine scale information through the DSMs from two aspects. Firstly, we propose an interpolation pyramid algorithm to encode the scale information from the DSMs and hence provide a scale prior information to the normal segmentation network. The interpolation pyramid algorithm implements interpolation operations with different scales on the DSMs and detects the pixel value change after the interpolation operations. Objects with different scales will express diverse changes, which provides useful information to encode their scale information. Secondly, aiming to address the problem that the DSMs contain a large amount of noise in the boundary part, a spatial information enhancement module and a mutual-guidance module are developed in this paper. These two modules can fix the misleading guidance information caused by the noise in the boundary part of the DSMs and hence achieve more accurate scale information inserting. The extensive experimental results prove that our methods can outperform other competitors in terms of qualitative and quantitative performance.}
}
@article{LI2023109831,
title = {Semi-supervised transfer learning with hierarchical self-regularization},
journal = {Pattern Recognition},
volume = {144},
pages = {109831},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109831},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005290},
author = {Xingjian Li and Abulikemu Abuduweili and Humphrey Shi and Pengkun Yang and Dejing Dou and Haoyi Xiong and Chengzhong Xu},
keywords = {Semi-supervised learning, Transfer learning, Fine-tuning, Deep learning, Hierarchical consistency, Adaptive sample selection},
abstract = {Both semi-supervised learning and transfer learning aim at lowering the annotation burden for training models. However, such two tasks are usually studied separately, i.e. most semi-supervised learning algorithms train models from scratch while transfer learning assumes pre-trained models as the initialization. In this work, we focus on a previously-less-concerned setting that further reduces the annotation efforts through incorporating both semi-supervised and transfer learning, where specifically a pre-trained source model is used as the initialization of semi-supervised learning. As those powerful pre-trained models are ubiquitously available nowadays and can considerably benefit various down-streaming tasks, such a setting is relevant to real-world applications yet challenging to design effective algorithms. Aiming at enabling transfer learning under semi-supervised settings, we propose a hierarchical self-regularization mechanism to exploit unlabeled samples more effectively, where a novel self-regularizer has been introduced to incorporate both individual-level and population-level regularization terms. The former term employs self-distillation to regularize learned deep features for each individual sample, and the latter one enforces self-consistency on feature distributions between labeled and unlabeled samples. Samples involved in both regularizers are weighted by an adaptive strategy, where self-regularization effects of both terms are adaptively controlled by the confidence of every sample. To validate our algorithm, exhaustive experiments have been conducted on diverse datasets such as CIFAR-10 for general object recognition, CUB-200-2011/MIT-indoor-67 for fine-grained classification and MURA for medical image classification. Compared with state-of-the-art semi-supervised learning methods including Pseudo Label, Mean Teacher, MixMatch and FixMatch, our algorithm demonstrates two advantages: first of all, the proposed approach adopts a new point of view to tackle problems caused by inadequate supervision and achieves very competitive results; then, it is complementary to these state-of-the-art methods and thus can be combined with them to get additional improvements. Furthermore, our method can also be applied to fully supervised transfer learning and self-supervised learning. We have published our code at https://github.com/SHI-Labs/Semi-Supervised-Transfer-Learning.}
}
@article{XU2023109819,
title = {Haar wavelet downsampling: A simple but effective downsampling module for semantic segmentation},
journal = {Pattern Recognition},
volume = {143},
pages = {109819},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109819},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005174},
author = {Guoping Xu and Wentao Liao and Xuan Zhang and Chang Li and Xinwei He and Xinglong Wu},
keywords = {Semantic segmentation, Downsampling, Haar wavelet, Information entropy},
abstract = {Downsampling operations such as max pooling or strided convolution are ubiquitously utilized in Convolutional Neural Networks (CNNs) to aggregate local features, enlarge receptive field, and minimize computational overhead. However, for a semantic segmentation task, pooling features over the local neighbourhood may result in the loss of important spatial information, which is conducive for pixel-wise predictions. To address this issue, we introduce a simple yet effective pooling operation called the Haar Wavelet-based Downsampling (HWD) module. This module can be easily integrated into CNNs to enhance the performance of semantic segmentation models. The core idea of HWD is to apply Haar wavelet transform for reducing the spatial resolution of feature maps while preserving as much information as possible. Furthermore, to investigate the benefits of HWD, we propose a novel metric, named as feature entropy index (FEI), which measures the degree of information uncertainty after downsampling in CNNs. Specifically, the FEI can be used to indicate the ability of downsampling methods to preserve essential information in semantic segmentation. Our comprehensive experiments demonstrate that the proposed HWD module could (1) effectively improve the segmentation performance across different modality image datasets with various CNN architectures, and (2) efficiently reduce information uncertainty compared to the conventional downsampling methods. Our implementation are available at https://github.com/apple1986/HWD.}
}
@article{HUANG2023109794,
title = {Graph-based learning of nonlinear physiological interactions for classification of emotions},
journal = {Pattern Recognition},
volume = {143},
pages = {109794},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109794},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004922},
author = {Huiyu Huang and Miaolin Fan and Chun-An Chou},
keywords = {Emotion recognition, Network physiology, Multimodal physiological signals, Dynamical interactions, Graph mining},
abstract = {Emotion recognition has been drawing the attention of researchers and practitioners in recent years. While various research studies show successful applications to recognize and distinguish emotions based on physiological responses using machine learning techniques, less research to date is focused on the network properties of physiological interactions under different emotional states. To this end, we propose a multi-modal graph learning framework to quantify the interactions among physiological systems and present representative networks associated with emotional states. More specifically, we introduce a novel information-theoretic-based time delay stability to quantify complex interactions between physiological modalities. We test our quantification approach on three publicly available benchmark databases for emotion recognition and demonstrate the comparative performances of measuring the interactions of physiological systems in response to emotional states. Finally, we present the visualization of multi-modal physiological network topology, which may be useful for emotional interpretations in practice.}
}
@article{GOYAL2023109742,
title = {Kinship verification using multi-level dictionary pair learning for multiple resolution images},
journal = {Pattern Recognition},
volume = {143},
pages = {109742},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109742},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004405},
author = {Aarti Goyal and Toshanlal Meenpal},
keywords = {Dictionary learning, Multiple resolution images, Multi-level representation, Kinship verification},
abstract = {Kinship verification using facial images is gaining substantial attention by computer vision researchers. The real challenge in kinship verification is to effectively represent the discriminative features to ease the differences between kinship image pairs. Further, existing kinship methods only focus on a single resolution, and ignore the variability of resolutions in practical scenarios. To address these issues, we propose a multi-level dictionary pair learning (MLDPL) method to learn dictionary pairs by incorporating multiple resolution images for kinship verification. We learn dictionary pairs jointly by transforming discriminative features of image pairs into different coding coefficients in the same space, thereby reducing the differences between them. Further, multiple resolution images are incorporated into dictionary pair learning to effectively deal with resolution variations in kinship verification. Extensive experiments are performed on different kinship datasets to validate the efficacy of proposed MLDPL method. Experimental results show that MLDPL achieves competitive performance on all kinship datasets.}
}
@article{JI2023109766,
title = {Higher-order memory guided temporal random walk for dynamic heterogeneous network embedding},
journal = {Pattern Recognition},
volume = {143},
pages = {109766},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109766},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004648},
author = {Cheng Ji and Tao Zhao and Qingyun Sun and Xingcheng Fu and Jianxin Li},
keywords = {Higher order, Dynamic network, Heterogeneous network},
abstract = {Network embedding (NE) aims at learning node embeddings via structure-based sampling. However, there are complex patterns in network structure (heterogeneity, higher-order dependence, dynamics) in the real world. The existing methods suffer from high dependence and constraints on manually designed higher-order structures and loss of fine-grained temporal information. To solve the above challenges, we propose a novel higher-order memory guided temporal random walk for dynamic heterogeneous network embedding (HoMo-DyHNE). The proposed model is a two-stage architecture consisting of a meta-structure-independent random walk algorithm namely HoMo-TRW with transition vectors and higher-order memory, and a Hawkes-based featured Skip-gram (HFSG) incorporating a multivariate Hawkes point process to measure the history-current association intensity. Extensive experiments demonstrate the superior effectiveness of our proposed method.}
}
@article{MEHBOOB2023109782,
title = {An encoded histogram of ridge bifurcations and contours for fingerprint presentation attack detection},
journal = {Pattern Recognition},
volume = {143},
pages = {109782},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109782},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004806},
author = {Rubab Mehboob and Hassan Dawood and Hussain Dawood},
keywords = {Biometric systems, fingerprint liveness detection, Fingerprint texture, Live fingerprints, Spoof fingerprints, Minutiae-based features, Presentation attack detection, Liveness detection, Ridge bifurcations, Ridge contours},
abstract = {In recent years, the exponential growth of internet technologies has made personal authentication an integral part of security applications. The fingerprint-based biometric systems are essentially used to safeguard the users' privacy and confidentiality. However, such systems are prone to spoof attacks by artificial replicas of the fingerprints. This paper presents an improved feature extractor called BiRi-PAD (Encoded Histogram of Ridge Bifurcations and Contours for fingerprint Presentation Attack Detection) that aims to enhance the accuracy of live fingerprint detection. The feature extraction process of the proposed BiRi-PAD consists of four steps. First, the fingerprint image undergoes a process of extracting 2-channel ridge contour maps (2-RC maps). The first channel of 2-RC maps consists of ridge contours extracted by a set of derivative filters in the spatial domain whereas the second channel of 2-RC maps consists of the ridge contours extracted by the maximum moments based on phase congruency in the frequency domain. Further, minutiae-based feature information i.e., ridge bifurcations are extracted by the minimum moments based on phase congruency covariance. Moreover, a fusion equation is proposed to integrate 2-RC maps and bifurcations into a single feature map. Second, an improved Comprehensive Local Phase Quantization (CLPQ) based on the well-known feature descriptor Rotation Invariant Local Phase Quantization (LPQri) is proposed to extract the phase information of ridges. CLPQ extracts the orientation of the ridges by using the complex parts of the significant frequency components of LPQri and monogenic filters. Third, the proposed BiRi-PAD quantizes the 2-RC maps into pre-determined intervals. Finally, both 2-RC maps and CLPQ features are integrated to generate a feature vector of a single fingerprint image. Performance evaluations of BiRi-PAD are conducted on three publicly available benchmarks from the LivDet competition, namely LivDet 2013, 2011, and 2015. Experimental evaluations demonstrate that the proposed BiRi-PAD achieves significant reductions in average rates compared to state-of-the-art techniques of fingerprint liveness detection. Specifically, on LivDet 2013, LivDet 2011, and LivDet 2015, the average rates are reduced to 1.92%, 4.39%, and 4.55%, respectively.}
}
@article{QIAN2023109790,
title = {Knowledge transfer evolutionary search for lightweight neural architecture with dynamic inference},
journal = {Pattern Recognition},
volume = {143},
pages = {109790},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109790},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004880},
author = {Xiaoxue Qian and Fang Liu and Licheng Jiao and Xiangrong Zhang and Xinyan Huang and Shuo Li and Puhua Chen and Xu Liu},
keywords = {Neural architecture search (NAS), Knowledge transfer, Dynamic inference, Image classification},
abstract = {Relying on the availability of massive labeled samples, most neural architecture search (NAS) methods focus on searching large and complex models; and adopt fixed structures and parameters at the inference stage. Few approaches automatically design lightweight networks for label-limited tasks and further consider the inference differences between inputs. To address these issues, we introduce evolutionary computation (EC) and attention mechanism and propose a knowledge transfer evolutionary search for lightweight neural architecture with dynamic inference, then verify it using synthetic aperture radar (SAR) images. SAR image classification is a typical label-limited task due to the inherent imaging mechanism of SAR. We design the EC-based architecture search and attention-based dynamic inference for SAR image scene classification. Specifically, we build a SAR-tailored search space, explore topology pruning-based mutation operators to search lightweight architectures, and further design a dynamic Ridgelet convolution capable of adaptive reasoning to enhance the representation ability of searched lightweight networks. Moreover, we propose a knowledge transfer training strategy and hybrid evaluation criteria to ensure searching quickly and robustly. Experimental results show that the proposed method can search for superior neural architectures, thus improving the classification performance of SAR images.}
}
@article{CHEN2023109738,
title = {Integrating topology beyond descriptions for zero-shot learning},
journal = {Pattern Recognition},
volume = {143},
pages = {109738},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109738},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004363},
author = {Ziyi Chen and Yutong Gao and Congyan Lang and Lili Wei and Yidong Li and Hongzhe Liu and Fayao Liu},
keywords = {Zero-shot learning, Topology mining, Image classification},
abstract = {Zero-shot learning (ZSL) aims to discriminate object categories through the identification of their attributes and has received much attention for its capability to predict unseen categories without collecting training data. Recently, excellent works have been devoted to optimizing the model inference by mining the topology among categories/attributes, which proves that the topology learning is beneficial and important for ZSL. However, existing works focus almost exclusively on the construction of semantic topological knowledge with textual descriptions, which, though effective, still suffer from two deficiencies: first, the semantic gap between modalities makes it difficult for the category attributes to accurately describe the corresponding visual characters, resulting in the topology constructed in the semantic modality being distorted in the visual modality; second, it is difficult for one to enumerate all the attributes hidden in images, resulting in an incomplete topology mined only from the defined attributes. Therefore, we propose a Cross-Modality Topology Propagation Matcher (CTPM) to construct a more complete topology system by collaborative mining of topological knowledge in both the visual and semantic modalities. We stand at the dataset level to construct sample-based visual topological knowledge based on the global image features to preserve the integrity of visual information. Meanwhile, we exploit the matching relationship between visual and semantic modalities to make topological knowledge propagate effectively across modalities, and fully enjoy the benefits of multi-modality topological knowledge in category/attribute reasoning. We validate the effectiveness of our CTPM through extensive experiments and achieve state-of-the-art performance on four ZSL datasets.}
}
@article{TERROSOSAENZ2023109807,
title = {Music Mobility Patterns: How Songs Propagate Around The World Through Spotify},
journal = {Pattern Recognition},
volume = {143},
pages = {109807},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109807},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005058},
author = {Fernando Terroso-Saenz and Jesús Soto and Andres Muñoz},
keywords = {Music Streaming Services, Mobility Patterns, Spotify, Prediction},
abstract = {Nowadays, music streaming services allow users to get instant access to an unprecedented amount of music of any type. This entails that songs, including potential new hits, can be discovered by listeners in any part of the globe and their propagation can be tracked through these streaming services. In this context, the present work focuses on recognizing the mobility patterns that songs follow in such a propagation among different countries of the world. To this end, this work defines a novel mechanism to uncover such mobility patterns of music from a directed-graph structure where nodes are countries and each edge reflects a frequent propagation of songs between pairs of countries. The resulting patterns reflect strong correlations with the migratory flows and the cultural and social similarities among regions. For instance, a propagation pattern was observed among North European countries with a good command of English. From such patterns, potential predictors for anticipating the mobility of a song are discussed. The results of this work can be beneficial for record companies and artists in their marketing campaigns for album releases and the organization of concerts and festivals.}
}
@article{MEHRJARDI2023109778,
title = {A survey on deep learning-based image forgery detection},
journal = {Pattern Recognition},
volume = {144},
pages = {109778},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109778},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004764},
author = {Fatemeh Zare Mehrjardi and Ali Mohammad Latif and Mohsen Sardari Zarchi and Razieh Sheikhpour},
keywords = {Forgery detection, Deep learning, Inpainting, Copy move, Splicing, Tampered image, CNN, RNN, R-CNN, Auto-Encoder},
abstract = {Image is known as one of the communication tools between humans. With the development and availability of digital devices such as cameras and cell phones, taking images has become easy anywhere. Images are used in many medical, forensic medicine, and judiciary applications. Sometimes images are used as evidence, so the authenticity and reliability of digital images are increasingly important. Some people manipulate images by adding or deleting parts of an image, which makes the image invalid. Therefore, image forgery detection and localization are important. The development of image editing tools has made this issue an important problem in the field of computer vision. In recent years, many different algorithms have been proposed to detect forgery in the image and pixel levels. All these algorithms are categorized into two main methods: traditional and deep-learning methods. The deep learning method is one of the important branches of artificial intelligence science. This method has become one of the most popular methods in most computer vision problems due to the automatic identification and prediction process and robustness against geometric transformations and post-processing operations. In this study, a comprehensive review of image forgery types, benchmark datasets, evaluation metrics in forgery detection, traditional forgery detection methods, discovering the weaknesses and limitations of traditional methods, forgery detection with deep learning methods, and the performance of this method is presented. According to the expansion of deep-learning methods and their successful performance in most computer vision problems, our main focus in this study is forgery detection based on deep-learning methods. This survey can be helpful for a researcher to obtain a deep background in the forgery detection field.}
}
@article{JIA2023109823,
title = {A reflectance re-weighted Retinex model for non-uniform and low-light image enhancement},
journal = {Pattern Recognition},
volume = {144},
pages = {109823},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109823},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005216},
author = {Fan Jia and Hok Shing Wong and Tiange Wang and Tieyong Zeng},
keywords = {Image enhancement, Variational method, Retinex model, Non-uniform enhancement},
abstract = {Image enhancement is a fundamental low-level task of significant importance that can directly affect high-level image processing tasks. Although various methods have been proposed to enhance images, the effectiveness of current methods deteriorates significantly under non-uniform lighting. Since the brightness may vary dramatically in different regions of real-world photos, current methods hardly achieve a good balance between enhancing low-light regions and retaining normal-light regions in the same image. Consequently, either the low-light regions are under-enhanced or the normal-light regions are over-enhanced, while at the same time, color distortion and artifacts are frequently found. To overcome this shortcoming, we propose a robust Retinex-based model with reflectance map re-weighting that can improve the brightness level of the low-light image and re-balance the brightness concurrently. We introduce an alternating scheme to solve our proposed model, in which the illumination map, reflectance map, and weighting map are updated iteratively. By utilizing the regularization terms, the noise is well-suppressed during the process. An initialization scheme for the weighting map is also proposed to make our model adaptable to a wide range of light conditions. To the best of our knowledge, we are the first to propose a variational model with an explicitly constructed re-weighting prior and the associated weighing map concept for the reflectance map. It can estimate the reflectance map, suppress noise, and re-balance the brightness simultaneously. A series of experimental results on a variety of popular datasets demonstrate the efficacy of our method and its superiority in enhancing real low-light images when compared to other state-of-the-art methods.}
}
@article{YANG2023109847,
title = {AdvMask: A sparse adversarial attack-based data augmentation method for image classification},
journal = {Pattern Recognition},
volume = {144},
pages = {109847},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109847},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005459},
author = {Suorong Yang and Jinqiao Li and Tianyue Zhang and Jian Zhao and Furao Shen},
keywords = {Data augmentation, Image classification, Sparse adversarial attack, Generalization},
abstract = {Data augmentation has been an essential technique for improving the generalization ability of deep neural networks in image classification tasks. However, intensive changes in appearance and different degrees of occlusion in images are the key factors that severely affect the generalization ability of image classification models. Therefore, in order to enhance the generalization performance and robustness of deep models, data augmentation approaches by providing models with more diverse training data in various scenarios are widely applied. Although many existing data augmentation methods simulate occlusion in the augmented images to enhance the generalization of models, these methods randomly delete some areas in images without considering the semantic information of images. In this work, we propose a novel data augmentation method named AdvMask for image classification based on sparse adversarial attack techniques. AdvMask first identifies the key points that have the greatest influence on the classification results via a proposed end-to-end sparse adversarial attack module. During the data augmentation process, AdvMask efficiently generates diverse augmented data with structured occlusions based on the key points. By doing so, AdvMask can force deep models to seek other relevant content while the most discriminative content is hidden. Extensive experimental results on various benchmark datasets and deep models demonstrate that our proposed method can effectively improve the generalization performance of deep models and significantly outperforms previous data augmentation methods. Code for reproducing our results is available at https://github.com/Jackbrocp/AdvMask.}
}
@article{KIM2023109751,
title = {A discriminative SPD feature learning approach on Riemannian manifolds for EEG classification},
journal = {Pattern Recognition},
volume = {143},
pages = {109751},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109751},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004491},
author = {Byung Hyung Kim and Jin Woo Choi and Honggu Lee and Sungho Jo},
keywords = {Discriminative, EEG, Non-stationary, SPD Matrix, Riemannian, Barycenter},
abstract = {Covariance matrix learning methods have become popular for many classification tasks owing to their ability to capture interesting structures in non-linear data while respecting the Riemannian geometry of the underlying symmetric positive definite (SPD) manifolds. Several deep learning architectures applied to these matrix learning methods have recently been proposed in classification tasks by learning discriminative Euclidean-based embeddings. In this paper, we propose a new Riemannian-based deep learning network to generate more discriminative features for electroencephalogram (EEG) classification. Our key innovation lies in learning the Riemannian barycenter for each class within a Riemannian geometric space. The proposed model normalizes the distribution of SPD matrices and learns the center of each class to penalize the distances between the matrix and the corresponding class centers. As a result, our framework can further simultaneously reduce the intra-class distances, enlarge the inter-class distances for the learned features, and consistently outperform other state-of-the-art methods on three widely used EEG datasets and the data from our stress-induced experiment in virtual reality. Experimental results demonstrate the superiority of the proposed framework for learning the non-stationary nature of EEG signals due to the robustness of the covariance descriptor and the benefits of considering the barycenters on the Riemannian geometry.}
}
@article{MOHAMUD2023109848,
title = {Encoder–decoder cycle for visual question answering based on perception-action cycle},
journal = {Pattern Recognition},
volume = {144},
pages = {109848},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109848},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005460},
author = {Safaa Abdullahi Moallim Mohamud and Amin Jalali and Minho Lee},
keywords = {Visual question answering, Vision language tasks, Multi-modality fusion, Attention, Bilinear fusion, Brain-inspired frameworks},
abstract = {In this study, we propose a novel encoder–decoder cycle (EDC) framework inspired by the human learning process called the perception-action cycle to tackle challenging problems such as visual question answering (VQA) and visual relationship detection (VRD). EDC considers the understanding of the visual features of an image as perception and the act of answering the question regarding that image as an action. In the perception-action cycle, information is primarily collected from the environment and then passed to sensory structures in the brain to form an understanding of the environment. Acquired knowledge is then passed to motor structures to perform an action on the environment. Next, sensory structures perceive the altered environment and improve their understanding of the surrounding world. This process of understanding the environment, performing an action correspondingly, and then re-evaluating the initial understanding occurs cyclically in human life. EDC initially mimics this mechanism of introspection by comprehending and refining visual features to acquire the proper knowledge for answering the question. Subsequently, it decodes visual and language features into answer features, feeding them back cyclically to the encoder. In the VRD task, EDC decodes visual features to generate predicate features. We evaluate the proposed framework on the TDIUC, VQA 2.0, and VRD datasets, which outperforms the state-of-the-art models on the TDIUC and VRD datasets.}
}
@article{XING2023109820,
title = {Contrastive deep support vector data description},
journal = {Pattern Recognition},
volume = {143},
pages = {109820},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109820},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005186},
author = {Hong-Jie Xing and Ping-Ping Zhang},
keywords = {Deep support vector data description, Contrastive learning, Anomaly detection, One-class classification, Hypersphere collapse},
abstract = {In comparison with support vector data description (SVDD), deep SVDD (DSVDD) is more suitable for dealing with large-scale data sets. DSVDD uses mapping network to replace the role of kernel mapping in SVDD. Moreover, the objective of DSVDD is to simultaneously learn the optimal connection weights of mapping network and the minimum volume of hypersphere. To further improve the performance of DSVDD for tackling large-scale data sets and obtain the discriminative features of the given samples in a self-supervised learning manner, contrastive DSVDD (CDSVDD) is proposed in this study. In the pre-training phase of CDSVDD, the contrastive loss and the rotation prediction loss are jointly minimized to achieve the optimal feature representations. Furthermore, the learned feature representations are utilized to determine the hypersphere center. In the training phase of CDSVDD, the distances between the obtained feature representations and the hypersphere center together with the contrastive loss are simultaneously minimized to derive the optimal network connection weights, the minimum volume of hypersphere and the optimal feature representations. In addition, CDSVDD can efficiently solve the hypersphere collapse problem of DSVDD. The ablation study on CDSVDD verifies that compared with the case of determining the hypersphere center by the feature representations of the original samples, the hypersphere center determined by the feature representations of the augmented samples makes CDSVDD achieve better hypersphere boundary and more compact feature representations. Experimental results on the four benchmark data sets demonstrate that the proposed CDSVDD acquires better detection performance in comparison with its six pertinent methods.}
}
@article{WANG2023109795,
title = {Hyperspectral anomaly detection based on variational background inference and generative adversarial network},
journal = {Pattern Recognition},
volume = {143},
pages = {109795},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109795},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004934},
author = {Zhiwei Wang and Xue Wang and Kun Tan and Bo Han and Jianwei Ding and Zhaoxian Liu},
keywords = {Background distribution characteristics, GAN, Hyperspectral anomaly detection},
abstract = {Hyperspectral anomaly detection is aimed at detecting targets with significant spectral differences from their surroundings. Recently, deep generative models have been applied to anomaly detections, while the existing generative adversarial network (GAN)-based methods have difficulty in accurately modeling the background and achieving spectrum reconstruction. In this article, a hyperspectral anomaly detection network based on variational background inference and generative adversarial framework (VBIGAN-AD) is proposed. The proposed VBIGAN model can learn the background distribution characteristics of HSIs and enhance the detection performance by the use of reconstruction errors. Specifically, the VBIGAN framework consists of sample and latent GANs, which establishes the relationship between data samples and latent samples through two sub-networks to capture the data distribution. Furthermore, the variational inference method is introduced and the hyperspectral background distribution can be converged to a multivariate normal distribution. To accurately learn the background distribution characteristics and reconstruct the background spectra, the coupling loss is conducted by enforcing feature match in the two discriminators on the basis of composite loss, and the results show that the additional loss can promote the detection performance. As a result, the reconstruction errors generated by the VBIGAN-AD method is utilized to detect abnormal targets. The experiments conducted on five datasets proved the robustness and applicability of the proposed VBIGAN-AD method.}
}
@article{MIAO2023109743,
title = {SMPR: Single-stage multi-person pose regression},
journal = {Pattern Recognition},
volume = {143},
pages = {109743},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109743},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004417},
author = {Huixin Miao and Junqi Lin and Junjie Cao and Xiaoguang He and Zhixun Su and Risheng Liu},
keywords = {Pose estimation, Dense prediction, Instance-aware keypoints, Pose scoring},
abstract = {Existing multi-person pose estimators can be roughly divided into two-stage approaches (top-down and bottom-up approaches) and one-stage approaches. The two-stage methods either suffer high computational redundancy for additional person detectors or group keypoints heuristically after predicting all the instance-free keypoints. The recently proposed single-stage methods do not rely on the above two extra stages but have lower performance than the latest bottom-up approaches. In this work, a novel single-stage multi-person pose regression, termed SMPR, is presented. It follows the paradigm of dense prediction and predicts instance-aware keypoints from every location. Besides feature aggregation, we propose better strategies to define positive pose hypotheses for training which all play an important role in dense pose estimation. The network also learns the scores of estimated poses. The pose scoring strategy further improves the pose estimation performance by prioritizing superior poses during non-maximum suppression (NMS). We show that our method not only outperforms existing single-stage methods but also be competitive with the latest bottom-up methods, with 70.2 AP and 77.5 AP75 on the COCO test-dev pose benchmark. The code is available at https://github.com/cmdi-dlut/SMPR .}
}
@article{WU2023109783,
title = {An efficient EM algorithm for two-layer mixture model of gaussian process functional regressions},
journal = {Pattern Recognition},
volume = {143},
pages = {109783},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109783},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004818},
author = {Di Wu and Yurong Xie and Zhe Qiang},
keywords = {Mixture of gaussian processes, Hierarchical mixture of experts, Classification EM algorithm, Computational efficiency, Local maximum problem, Curve clustering},
abstract = {The mixture of Gaussian processes is effective for regression, but it cannot handle the non-stationary curve clustering problem well. The two-layer mixture of Gaussian process functional regressions (TMGPFR) model was established to deal with this problem. In this paper, we first propose the classification EM (CEM) algorithm to solve that the optimization algorithm is inefficient for TMGPFRs, and then propose the deterministic annealing CEM algorithm for TMGPFRs to overcome the local maximum problem of the CEM algorithm. Lastly, experiments are conducted on synthetic and real-world data sets, and the results show that our proposed algorithms are more effective than the compared algorithms on curve clustering and regression.}
}
@article{HUANG2023109715,
title = {Longitudinal prediction of postnatal brain magnetic resonance images via a metamorphic generative adversarial network},
journal = {Pattern Recognition},
volume = {143},
pages = {109715},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109715},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004132},
author = {Yunzhi Huang and Sahar Ahmad and Luyi Han and Shuai Wang and Zhengwang Wu and Weili Lin and Gang Li and Li Wang and Pew-Thian Yap},
keywords = {Infant brain MRI, Longitudinal prediction, Metamorphic GAN},
abstract = {Missing scans are inevitable in longitudinal studies due to either subject dropouts or failed scans. In this paper, we propose a deep learning framework to predict missing scans from acquired scans, catering to longitudinal infant studies. Prediction of infant brain MRI is challenging owing to the rapid contrast and structural changes particularly during the first year of life. We introduce a trustworthy metamorphic generative adversarial network (MGAN) for translating infant brain MRI from one time point to another. MGAN has three key features: (i) Image translation leveraging spatial and frequency information for detail-preserving mapping; (ii) Quality-guided learning strategy that focuses attention on challenging regions. (iii) Multi-scale hybrid loss function that improves translation of image contents. Experimental results indicate that MGAN outperforms existing GANs by accurately predicting both tissue contrasts and anatomical details.}
}
@article{VINDAS2023109812,
title = {Guided deep embedded clustering regularization for multifeature medical signal classification},
journal = {Pattern Recognition},
volume = {143},
pages = {109812},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109812},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005101},
author = {Yamil Vindas and Emmanuel Roux and Blaise Kévin Guépié and Marilys Almar and Philippe Delachartre},
keywords = {Multifeature learning, Deep regularization, Guided training, Signal classification, Transcranial doppler},
abstract = {Medical signal classification often focuses on one representation (raw signal or time frequency). In that context, recent works have shown the value of exploiting different representations simultaneously. We propose a regularized end-to-end trained model for classification in a medical context exploiting both the raw signal and a time-frequency representation (TFR). First, a 2D convolutional neural network (CNN) encoder and a 1D CNN-transformer encoder start by extracting embedded representations from the TFR and the raw signal, respectively. Then, the obtained embeddings are fused to form a common latent space that is used for classification. We propose to guide the training of each encoder by applying two iterated losses. Moreover, we propose to regularize the fused common latent space using deep embedded clustering. Extensive experiments on three medical datasets and ablation studies show the adaptability and good performance of our method for medical signal classification. Our method makes it possible to improve the classification performance from 4% to 12% MCC on a transcranial Doppler dataset, when compared with single-feature counterparts, while giving more stable models. The code is available at: https://github.com/gdec-submission/gdec/.}
}
@article{KALE2023109791,
title = {Face age synthesis: A review on datasets, methods, and open research areas},
journal = {Pattern Recognition},
volume = {143},
pages = {109791},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109791},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004892},
author = {Ayşe Kale and Oğuz Altun},
keywords = {Age progression, Age regression, Face aging, GANs},
abstract = {Face age synthesis is the determination of how a person looks in the future or the past by reconstructing their facial image. Determining the change in the human face over the years is a critical process for cross-age face recognition systems in forensic issues such as finding missing people and fugitive criminals. Therefore, it is a subject that has attracted attention in recent years. With the implementation of deep learning methods, better quality and photo-realistic images began to be produced. However, researchers continue to improve both aging accuracy and identity preservation requirements. We group the studies in the literature under two categories: classical methods and deep learning methods. We review both categories in the methods used, evaluation methods, and databases.}
}
@article{ZHAO2023109836,
title = {Deep multi-view spectral clustering via ensemble},
journal = {Pattern Recognition},
volume = {144},
pages = {109836},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109836},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005344},
author = {Mingyu Zhao and Weidong Yang and Feiping Nie},
keywords = {Spectral embedding, Multi-view clustering, Ensemble clustering, Graph reconstruction},
abstract = {Graph-based methods have achieved great success in multi-view clustering. However, existing graph-based models generally utilize shallow and linear embedding functions to obtain the common spectral embedding for clustering assignments. In addition, the fusion similarity graphs from multiple views are generally obtained by a simple weighted-sum rule. To this end, we propose a novel deep multi-view spectral clustering via ensemble model (DMCE), which applies ensemble clustering to fuse the similarity graphs from different views. On this basis, we employ the graph auto-encoder to learn the common spectral embedding, which can be regarded as the indicator matrix directly. Moreover, a unified optimization framework is designed to update the variables in the proposed DMCE, which consists of graph reconstruction loss, orthogonal loss, and graph contrastive learning loss. Extensive experiments on six real-world benchmark datasets have demonstrated the effectiveness of our model compared with the state-of-the-art multi-view clustering methods.}
}
@article{SHI2023109750,
title = {Source-free and black-box domain adaptation via distributionally adversarial training},
journal = {Pattern Recognition},
volume = {143},
pages = {109750},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109750},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300448X},
author = {Yucheng Shi and Kunhong Wu and Yahong Han and Yunfeng Shao and Bingshuai Li and Fei Wu},
keywords = {Source-free unsupervised domain adaptation, Distributionally adversarial training, Data and model privacy, Black-box probe},
abstract = {Source-free unsupervised domain adaptation is one class of practical deep learning methods which generalize in the target domain without transferring data from source domain. However, existing source-free domain adaptation methods rely on source model transferring. In many data-critical scenarios, the transferred source models may suffer from membership inference attacks and expose private data. In this paper, we aim to overcome a more practical and challenging setting where the source models cannot be transferred to the target domain. The source models are considered as queryable black-box models which only output hard labels. We use public third-party data to probe the source model and obtain supervision information, dispensing with transferring source model. To fill the gap between third-party data and target data, we further propose Distributionally Adversarial Training (DAT) to align the distribution of third-party data with target data, gain more informative query results and improve the data efficiency. We call this new framework Black-box Probe Domain Adaptation (BPDA) which adopts query mechanism and DAT to probe and refine supervision information. Experimental results on several domain adaptation datasets demonstrate the practicability and data efficiency of BPDA in query-only and source-free unsupervised domain adaptation.}
}
@article{MAO2023109779,
title = {Multi-proxy feature learning for robust fine-grained visual recognition},
journal = {Pattern Recognition},
volume = {143},
pages = {109779},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109779},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004776},
author = {Shunan Mao and Yaowei Wang and Xiaoyu Wang and Shiliang Zhang},
keywords = {Fine-grained visual recognition, Noisy label, Long tail, Proxy learning},
abstract = {Visual representation for fine-grained visual recognition can be learned by mandatorily enforcing all samples of the same category into a uniform representation. This strict training objective performs well under closed-set setting but is not applicable to data in the wild containing noisy annotations and long-tailed distributions, e.g., it may lead to a feature space biased to head categories. This paper tackles this challenge by pursuing a more balanced and discriminative feature space by first retaining intra-class variances to isolate noises, then eliminating intra-class variances to improve the visual recognition performance. We propose the Compact Memory Updater to maintain a memory bank, which memorizes proxy features to represent multiple typical appearances of each category in the training set. The Proxy-based Feature Enhancement hence leverages proxy features to ensure samples of the same category have similar features. Iteratively running those two modules boosts the robustness and discriminative power of the learnt representation, hence facilitates various fine-grained visual recognition tasks including person re-identification (re-id), image classification and retrieval. Extensive experiments on noisy and long-tailed training sets show this Multi-Proxy Feature Learning (MPFL) framework achieves promising performance. For instance on a training set with 90% one-shot categories, MPFL outperforms the recent long-tailed person re-id method LEAP-AF by 16.9% in rank-1 accuracy.}
}
@article{LIU2023109727,
title = {Trigonometric projection statistics histograms for 3D local feature representation and shape description},
journal = {Pattern Recognition},
volume = {143},
pages = {109727},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109727},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004259},
author = {Xingsheng Liu and Anhu Li and Jianfeng Sun and Zhiyong Lu},
keywords = {3D feature descriptor, Local reference frame, Trigonometric projection mechanism, Object recognition, Shape registration},
abstract = {Feature representation as a significant approach to three-dimensional (3D) shape description has been widely employed in computer vision. However, most existing methods are suffering from the emerging challenges for descriptiveness, robustness and efficiency. This paper presents a novel feature descriptor named trigonometric projection statistics histograms (TPSH). By constructing the repeatable local reference frame based on a multi-attribute weighting strategy, TPSH can address many prevailing nuisances such as noise, occlusion and varying resolution. The trigonometric projection mechanism is originally proposed for TPSH generation, which combines two perspective views to encode both spatial distribution and geometrical measurements from local shape into statistics histograms. The experimental evaluation on public datasets proves that TPSH outperforms state-of-the-art methods in descriptiveness and robustness while maintaining storage compactness and computational efficiency. It is demonstrated that TPSH can not only be suited for 3D object recognition and shape registration, but also generalized across various acquisition devices, data modalities and application scenarios.}
}
@article{ZHAO2023109824,
title = {A balanced random learning strategy for CNN based Landsat image segmentation under imbalanced and noisy labels},
journal = {Pattern Recognition},
volume = {144},
pages = {109824},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109824},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005228},
author = {Xuemei Zhao and Yong Cheng and Luo Liang and Haijian Wang and Xingyu Gao and Jun Wu},
keywords = {Landsat image segmentation, Noisy labels, Confidence interval, Random learning, Multi-layer features},
abstract = {Landsat image segmentation is important for obtaining large-scale land cover maps. The accuracy of CNN-based Landsat image segmentation highly depends on the quantity and quality of the training samples. However, enough accurate labels for Landsat images are difficult to access. Fortunately, traditional classifier induced segmentation results can be considered as an alternative, although they are noisy and unbalanced to a certain extent. To resist noisy labels and alleviate the impact of imbalanced samples, this paper proposes a confidence interval based balanced random learning strategy. Firstly, a confidence interval-based mask is employed to control the random learning rate of the network from the entire noisy training set. Then, the multi-layer feature maps of CNN are fully utilized to compensate for the information loss in random learning, in which down-sampled labels are used to decrease the uncertainty brought by up-sampling CNN feature maps. In addition, considering the corruption of noisy labels on different classes, a balanced random learning with different confidence levels is performed on each class to further improve the learning ability of CNN. Experimental results on two widely used backbones, namely VGGNet and ResNet, demonstrate that the proposed balanced random learning strategy can effectively improve the performance of CNN under imbalanced and noisy labels, which can be improved by 3.41%.}
}
@article{WANG2023109817,
title = {Robust table structure recognition with dynamic queries enhanced detection transformer},
journal = {Pattern Recognition},
volume = {144},
pages = {109817},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109817},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005150},
author = {Jiawei Wang and Weihong Lin and Chixiang Ma and Mingze Li and Zheng Sun and Lei Sun and Qiang Huo},
keywords = {Table structure recognition, Separation line regression, Two-stage DETR, Dynamic query},
abstract = {We present a new table structure recognition (TSR) approach, called TSRFormer, to robustly recognize the structures of complex tables with geometrical distortions from various table images. Unlike previous methods, we formulate table separation line prediction as a line regression problem instead of an image segmentation problem and propose a new two-stage dynamic queries enhanced DETR based separation line regression approach, named DQ-DETR, to predict separation lines from table images directly. Compared to Vallina DETR, we propose three improvements in DQ-DETR to make the two-stage DETR framework work efficiently and effectively for the separation line prediction task: 1) A new query design, named Dynamic Query, to decouple single line query into separable point queries which could intuitively improve the localization accuracy for regression tasks; 2) A dynamic queries based progressive line regression approach to progressively regressing points on the line which further enhances localization accuracy for distorted tables; 3) A prior-enhanced matching strategy to solve the slow convergence issue of DETR. After separation line prediction, a simple relation network based cell merging module is used to recover spanning cells. With these new techniques, our TSRFormer achieves state-of-the-art performance on several benchmark datasets, including SciTSR, PubTabNet, WTW, FinTabNet, and cTDaR TrackB2-Modern. Furthermore, we have validated the robustness and high localization accuracy of our approach to tables with complex structures, borderless cells, large blank spaces, empty or spanning cells as well as distorted or even curved shapes on a more challenging real-world in-house dataset.}
}
@article{LI2023109841,
title = {A multi-grained unsupervised domain adaptation approach for semantic segmentation},
journal = {Pattern Recognition},
volume = {144},
pages = {109841},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109841},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005393},
author = {Luyang Li and Tai Ma and Yue Lu and Qingli Li and Lianghua He and Ying Wen},
keywords = {Domain adaptation, Unsupervised semantic segmentation, Neural network},
abstract = {When transferring knowledge between different datasets, domain mismatch greatly hinders model’s performance. So domain adaption has been brought up to tackle the problem. Traditional methods focusing either on global or local alignment play a limited role in improving model’s performance. In this paper, we propose a multi-grained unsupervised domain adaptation approach (Muda) for semantic segmentation. Muda aims to enforce multi-grained semantic consistency between domains by aligning domains at both global and category level. Specifically, coarse-grained adaptation uses global adversarial learning on an image translation model and a main segmentation model, which respectively attempts to eliminate appearance differences and to get similar segmentation maps from two domains. While fine-grained adaptation employs an auxiliary model to adapt category information to refine pseudo labels of target data. Experiments and ablation studies are conducted on two synthetic-to-real benchmarks: GTA5 → Cityscapes and SYNTHIA → Cityscapes, which show that our model outperforms the state-of-the-art methods.}
}
@article{HU2023109760,
title = {Attention‐guided evolutionary attack with elastic‐net regularization on face recognition},
journal = {Pattern Recognition},
volume = {143},
pages = {109760},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109760},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004582},
author = {Cong Hu and Yuanbo Li and Zhenhua Feng and Xiaojun Wu},
keywords = {Face recognition, Convolutional neural networks, Adversarial examples, Evolutionary attack, Attention mechanisms},
abstract = {In recent years, face recognition has achieved promising results along with the development of advanced Deep Neural Networks (DNNs). The existing face recognition systems are vulnerable to adversarial examples, which brings potential security risks. Evolutionary Attack (EA) has been successfully used to fool face recognition by inducing a minimum perturbation to a face image with few queries. However, EA employs the global information of face images but ignores their local characteristics. In addition, restricting the ℓ2-norm of adversarial perturbations hinders the diversity of adversarial perturbations. To solve the above problems, we propose Attention-guided Evolutionary Attack with Elastic-Net Regularization (ERAEA) for attacking face recognition. ERAEA extracts local facial characteristics by attention mechanism, effectively improving the attack effect and image perception quality. In particular, ERAEA adopts an attention mechanism to guide evolutionary direction, which operates on the covariance matrix as it contains crucial information about the evolutionary path. Furthermore, we design an adaptive elastic-net regularization to diversify the adversarial perturbation, accelerating the optimization performance. Extensive experiments obtained on three benchmarks demonstrate that our proposed method achieves better perturbation norm than the state-of-the-art methods with limited queries on face recognition and generates adversarial face images with higher perceptual quality. Besides, ERAEA requires fewer queries to achieve a fixed adversarial perturbation norm.}
}
@article{DIALLO2023109764,
title = {Auto-attention mechanism for multi-view deep embedding clustering},
journal = {Pattern Recognition},
volume = {143},
pages = {109764},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109764},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004624},
author = {Bassoma Diallo and Jie Hu and Tianrui Li and Ghufran Ahmad Khan and Xinyan Liang and Hongjun Wang},
keywords = {Deep embedding clustering, Deep multi-view clustering, Multi-view autoencoder, Auto-attention},
abstract = {In several fields, deep learning has achieved tremendous success. Multi-view learning is a workable method for handling data from several sources. For clustering multi-view data, deep learning and multi-view learning are excellent options. However, a persistent challenge is a need for the current deep learning approach to independently drive divergent neural networks for different perspectives while working with multi-view data. The current methods use the number of viewpoints to calculate neural network statistics. Consequently, as the number of views rises, it results in a considerable calculation. Furthermore, they vainly try to unite various viewpoints at the training. Incorporating a triple fusion technique, this research suggests an innovative multi-view deep embedding clustering (MDEC) model. The suggested model can jointly acquire the specific knowledge in each view as well as the information fragment of the collective views. The main goal of the MDEC is to lower the errors made when learning the features of each view and correlating data from many views. To address the optimization problem, the MDEC model advises a suitable iterative updating approach. In testing modern deep learning and non-deep learning algorithms, the experimental study on small and large-scale multi-view data shows encouraging results for the MDEC model. In multi-view clustering, this work demonstrates the benefit of the deep learning-based approach over the non-ones. However, future work will address a variety of issues related to MDEC including the speed.}
}
@article{KOU2023109788,
title = {Infrared small target segmentation networks: A survey},
journal = {Pattern Recognition},
volume = {143},
pages = {109788},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109788},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004867},
author = {Renke Kou and Chunping Wang and Zhenming Peng and Zhihe Zhao and Yaohong Chen and Jinhui Han and Fuyu Huang and Ying Yu and Qiang Fu},
keywords = {Infrared small target, Characteristic analysis, Segmentation network, Deep learning, Collaborative technology, Data-driven, False alarm, Missed detection},
abstract = {Fast and robust small target detection is one of the key technologies in the infrared (IR) search and tracking systems. With the development of deep learning, there are many data-driven IR small target segmentation algorithms, but they have not been extensively surveyed; we believe our proposed survey is the first to systematically survey them. Focusing on IR small target segmentation tasks, we summarized 7 characteristics of IR small targets, 3 feature extraction methods, 8 design strategies, 30 segmentation networks, 8 loss functions, and 13 evaluation indexes. Then, the accuracy, robustness, and computational complexities of 18 segmentation networks on 5 public datasets were compared and analyzed. Finally, we have discussed the existing problems and future trends in the field of IR small target detection. The proposed survey is a valuable reference for both beginners adapting to current trends in IR small target detection and researchers already experienced in this field.}
}
@article{DING2023109833,
title = {Graph clustering network with structure embedding enhanced},
journal = {Pattern Recognition},
volume = {144},
pages = {109833},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109833},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005319},
author = {Shifei Ding and Benyu Wu and Xiao Xu and Lili Guo and Ling Ding},
keywords = {Graph machine learning, Graph Neural Network, Deep clustering, Self-supervised learning},
abstract = {Recently, deep clustering utilizing Graph Neural Networks has shown good performance in the graph clustering. However, the structure information of graph was underused in existing deep clustering methods. Particularly, the lack of concern on mining different types structure information simultaneously. To tackle with the problem, this paper proposes a Graph Clustering Network with Structure Embedding Enhanced (GC-SEE) which extracts nodes importance-based and attributes importance-based structure information via a feature attention fusion graph convolution module and a graph attention encoder module respectively. Additionally, it captures different orders-based structure information through multi-scale feature fusion. Finally, a self-supervised learning module has been designed to integrate different types structure information and guide the updates of the GC-SEE. The comprehensive experiments on benchmark datasets commonly used demonstrate the superiority of the GC-SEE. The results showcase the effectiveness of the GC-SEE in exploiting multiple types of structure for deep clustering.}
}
@article{NOVOAPARADELA2023109805,
title = {Fast deep autoencoder for federated learning},
journal = {Pattern Recognition},
volume = {143},
pages = {109805},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109805},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005034},
author = {David Novoa-Paradela and Oscar Fontenla-Romero and Bertha Guijarro-Berdiñas},
keywords = {Deep autoencoder, Anomaly detection, Federated learning, Edge computing, Machine learning},
abstract = {This paper presents a novel, fast and privacy preserving implementation of deep autoencoders. DAEF (Deep AutoEncoder for Federated learning), unlike traditional neural networks, trains a deep autoencoder network in a non-iterative way, which drastically reduces training time. Training can be performed incrementally, in parallel and distributed and, thanks to its mathematical formulation, the information to be exchanged does not endanger the privacy of the training data. The method has been evaluated and compared with other state-of-the-art autoencoders, showing interesting results in terms of accuracy, speed and use of available resources. This makes DAEF a valid method for edge computing and federated learning, in addition to other classic machine learning scenarios.}
}
@article{LI2023109829,
title = {Compositional clustering: Applications to multi-label object recognition and speaker identification},
journal = {Pattern Recognition},
volume = {144},
pages = {109829},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109829},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005277},
author = {Zeqian Li and Xinlu He and Jacob Whitehill},
keywords = {Clustering algorithms, Compositional learning, Few-shot learning, Embedding models, Speaker diarization, Affinity propagation},
abstract = {We consider a novel clustering task in which clusters can have compositional relationships, e.g., one cluster contains images of rectangles, one contains images of circles, and a third (compositional) cluster contains images with both objects. In contrast to hierarchical clustering in which a parent cluster represents the intersection of properties of the child clusters, our problem is about finding compositional clusters that represent the union of the properties of the constituent clusters. This task is motivated by recently developed few-shot learning and embedding models (Alfassy et al., 2019; Li et al., 2021) that can distinguish the label sets, not just the individual labels, assigned to the examples. We propose three new algorithms – Compositional Affinity Propagation (CAP), Compositional k-means (CKM), and Greedy Compositional Reassignment (GCR) – that can partition examples into coherent groups and infer the compositional structure among them. We show promising results, compared to popular algorithms such as Gaussian mixtures, Fuzzy c-means, and Agglomerative Clustering, on the OmniGlot and LibriSpeech datasets. Our work has applications to open-world multi-label object recognition and speaker identification & diarization with simultaneous speech from multiple speakers.}
}
@article{HUA2023109719,
title = {Dynamic scene deblurring with continuous cross-layer attention transmission},
journal = {Pattern Recognition},
volume = {143},
pages = {109719},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109719},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300417X},
author = {Xia Hua and Mingxin Li and Junxiong Fei and Jianguo Liu and Yu Shi and Hanyu Hong},
keywords = {Image deblurring, Attention mechanism, Deep convolutional neural networks},
abstract = {The deep convolutional neural networks (CNNs) using attention mechanism have achieved great success for dynamic scene deblurring. In most of these networks, only the features refined by the attention maps can be passed to the next layer and the attention maps of different layers are separated from each other, which does not make full use of the attention information from different layers in the CNN. To address this problem, we introduce a new continuous cross-layer attention transmission (CCLAT) mechanism that can exploit hierarchical attention information from all the convolutional layers. Based on the CCLAT mechanism, we use a very simple attention module to construct a novel residual dense attention fusion block (RDAFB). In RDAFB, the attention maps inferred from the outputs of the preceding RDAFB and each layer are directly connected to the subsequent ones, leading to a CCLAT mechanism. Taking RDAFB as the building block, we design an effective architecture for dynamic scene deblurring named RDAFNet. The experiments on benchmark datasets show that the proposed model outperforms the state-of-the-art deblurring approaches, and demonstrate the effectiveness of CCLAT mechanism. The source code is available on: https://github.com/xjmz6/RDAFNet.}
}
@article{LUO2023109776,
title = {Classification of tumor in one single ultrasound image via a novel multi-view learning strategy},
journal = {Pattern Recognition},
volume = {143},
pages = {109776},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109776},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004740},
author = {Yaozhong Luo and Qinghua Huang and Longzhong Liu},
keywords = {Image classification, Deep learning, Multi-view learning, Breast cancer recognition},
abstract = {Computer-aided diagnosis (CAD) technology has been widely used in the early diagnosis of breast cancer. Nowadays, most of the existing breast ultrasound classification methods need to crop a tumor-centered image (TCI) on each image as the input of the system. These methods ignore the fact that the tumor as well as its surrounding tissues can actually be viewed from multiple aspects, and it is difficult to extract multi-resolution information applying only a single view image. In addition, the current methods do not effectively extract fine-grained features, and subtle details play an important role in breast classification. In our research, we propose a novel strategy to generate multi-resolution TCIs in a single ultrasound image, resulting in a multi-data-input learning task. Hence, a conventional single image based learning task is converted into a multi-view learning task, and an improved combined style fusion method suitable for a deep network is proposed, which integrates the advantage of the decision-based and feature-based methods to fuse the information of different views. At the same time, we first attempt to introduce the fine-grained classification method into breast classifications and capture the pairwise correlation between feature channels at each position to extract subtle information. The comparative experimental results show that our method can effectively improve the classification performance and achieves the best results in five metrics.}
}
@article{SHENG2023109724,
title = {Modeling global distribution for federated learning with label distribution skew},
journal = {Pattern Recognition},
volume = {143},
pages = {109724},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109724},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004223},
author = {Tao Sheng and Chengchao Shen and Yuan Liu and Yeyu Ou and Zhe Qu and Yixiong Liang and Jianxin Wang},
keywords = {Federated learning, Label distribution skew, Generative adversarial network, Non-Independent and identically distributed},
abstract = {Federated learning achieves joint training of deep models by connecting decentralized datasources, which can significantly mitigate the risk of privacy leakage. However, in a more general case, the distributions of labels among clients are different, called “label distribution skew”. Directly applying conventional federated learning without consideration of label distribution skew issue significantly hurts the performance of the global model. To this end, we propose a novel federated learning method, named FedMGD, to alleviate the performance degradation caused by the label distribution skew issue. It introduces a global Generative Adversarial Network to model the global data distribution without access to local datasets, so the global model can be trained using the global information of data distribution without privacy leakage. The experimental results demonstrate that our proposed method significantly outperforms the state-of-the-art on several public benchmarks. Code is available at https://www.github.com/Sheng-T/FedMGD.}
}
@article{WOO2023109800,
title = {MKConv: Multidimensional feature representation for point cloud analysis},
journal = {Pattern Recognition},
volume = {143},
pages = {109800},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109800},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004983},
author = {Sungmin Woo and Dogyoon Lee and Sangwon Hwang and Woo Jin Kim and Sangyoun Lee},
keywords = {Point cloud, Feature learning, Convolutional neural network, 3D vision},
abstract = {Despite the remarkable success of deep learning, an optimal convolution operation on point clouds remains elusive owing to their irregular data structure. Existing methods mainly focus on designing an effective continuous kernel function that can handle an arbitrary point in continuous space. Various approaches exhibiting high performance have been proposed, but we observe that the standard pointwise feature is represented by 1D channels and can become more informative when its representation involves additional spatial feature dimensions. In this paper, we present Multidimensional Kernel Convolution (MKConv), a novel convolution operator that learns to transform the point feature representation from a vector to a multidimensional matrix. Unlike standard point convolution, MKConv proceeds via two steps. (i) It first activates the spatial dimensions of local feature representation by exploiting multidimensional kernel weights. These spatially expanded features can represent their embedded information through spatial correlation as well as channel correlation in feature space, carrying more detailed local structure information. (ii) Then, discrete convolutions are applied to the multidimensional features which can be regarded as a grid-structured matrix. In this way, we can utilize the discrete convolutions for point cloud data without voxelization that suffers from information loss. Furthermore, we propose a spatial attention module, Multidimensional Local Attention (MLA), to provide comprehensive structure awareness within the local point set by reweighting the spatial feature dimensions. We demonstrate that MKConv has excellent applicability to point cloud processing tasks including object classification, object part segmentation, and scene semantic segmentation with superior results.}
}
@article{ABBAD2023109748,
title = {Exploring multivariate generalized gamma manifold for color texture retrieval},
journal = {Pattern Recognition},
volume = {143},
pages = {109748},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109748},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004466},
author = {Zakariae Abbad and Ahmed Drissi {El Maliani} and Said Ouatik {El Alaoui} and Mohammed {El Hassouni} and Mohamed Tahar Kadaoui Abbassi},
keywords = {Color texture retrieval, Gaussian copula, Geodesic distance, Manifolds, Wavelet transforms, Graph theory},
abstract = {This work proposes a novel method for color-textured image retrieval on a Multivariate Generalized Gamma Distribution manifold (MGΓD). Thanks to the Gaussian copula theory, we define the expression of MGΓD, which efficiently models the statistical dependence structure between dual-tree complex wavelet transform (DTCWT) of the color components. The major contribution of this paper is to provide a geometric perspective to the MGΓD by treating it as a Riemannian manifold while proposing the geodesic distance (GD) as a measure of Riemannian similarity on it. Based on information geometry tools, we conduct a geometrical study of the MGΓD manifold, allowing us to derive two suitable approximations of the GD. The experiments are performed on five well-known color texture databases, considering the content-based image retrieval (CBIR) framework and using the RGB color space. The obtained results demonstrate the efficiency of the geometric interpretation through the proposed GD as a natural and intuitive similarity measure on the studied statistical manifold.}
}
@article{LU2023109818,
title = {Neighborhood overlap-aware heterogeneous hypergraph neural network for link prediction},
journal = {Pattern Recognition},
volume = {144},
pages = {109818},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109818},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005162},
author = {Yifan Lu and Mengzhou Gao and Huan Liu and Zehao Liu and Wei Yu and Xiaoming Li and Pengfei Jiao},
keywords = {Heterogeneous graph, Structural information learning, Complex semantics, Link prediction},
abstract = {In real world, a large number of networks are heterogeneous, containing different types of semantics and connections. Existing studies typically only consider lower-order pairwise relations rather than higher-order group interactions. Furthermore, they tend to focus more on node attributes rather than graph structural information. This results models failing to maintain graph topology effectively, which reduces the effectiveness on link prediction. To address these limitations, we propose Neighborhood Overlap-aware Heterogeneous hypergraph neural network (NOH) that learns useful structural information from the heterogeneous graph and estimates overlapped neighborhood for link prediction. Our model fuses the heterogeneity of graphs with structural information so that the model maintains both lower-order pairwise relations and higher-order complex semantics. Our extensive experiments on four real-world datasets show that NOH consistently achieves state-of-the-art performance on link prediction.}
}
@article{YU2023109792,
title = {Adapt-Infomap: Face clustering with adaptive graph refinement in infomap},
journal = {Pattern Recognition},
volume = {143},
pages = {109792},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109792},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004909},
author = {Xiaotian Yu and Yifan Yang and Aibo Wang and Ling Xing and Haokui Zhang and Hanling Yi and Guangming Lu and Xiaoyu Wang},
keywords = {Face clustering, Map equation, Graph partitioning},
abstract = {Face clustering is a critical task in computer vision due to the increasing number of applications such as augmented reality or photo album management. The primary challenge in this task arises from the imperfections in image feature representations. Given image features extracted from an existing pre-trained representation model, it remains an unresolved problem that how to leverage the inherent characteristics of similarities among unlabelled images to improve the clustering performance. In order to solve face clustering in an unsupervised manner, we develop an effective and robust framework named as Adapt-Infomap. First, we reformulate face clustering as a process of non-overlapping community detection. Specially, Adapt-Infomap achieves face clustering by minimizing the entropy of information flows (also known as the map equation) on an affinity graph of images. Since the affinity graph of images might contain noisy edges, we develop an outlier detection strategy in Adapt-Infomap to adaptively refine the affinity graph. Experiments with ablation studies demonstrate that Adapt-Infomap significantly outperforms existing methods and achieves new state-of-the-arts on three popular large-scale datasets for face clustering, e.g., an absolute improvement of more than 10% and 3% comparing with prior unsupervised and supervised methods respectively in terms of average of Pairwise F-score.}
}
@article{KONG2023109793,
title = {FGBC: Flexible graph-based balanced classifier for class-imbalanced semi-supervised learning},
journal = {Pattern Recognition},
volume = {143},
pages = {109793},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109793},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004910},
author = {Xiangyuan Kong and Xiang Wei and Xiaoyu Liu and Jingjie Wang and Weiwei Xing and Wei Lu},
keywords = {Semi-supervised learning, Class-imbalanced learning, Graph network, Label propagation, MixUp},
abstract = {Semi-supervised learning (SSL) has witnessed resounding success in many standard class-balanced benchmark datasets. However, real-world data often exhibit class-imbalanced distributions, which poses significant challenges for existing SSL algorithms. In general, fully supervised models trained on a class-imbalanced dataset are biased toward the majority classes, and this issue becomes more severe for class-imbalanced semi-supervised learning (CISSL) conditions. To address this issue, we put forward a novel CISSL framework dubbed FGBC by introducing a flexible graph-based balanced classifier with three innovations. Specifically, because the propagation of label information becomes difficult for tail classes, we propose a graph-based classifier head attached to the representation layer of the existing SSL framework for efficient pseudo-label propagation. Then, by considering that the learning status of different classes in CISSL may vary, we introduce a flexible threshold adjustment in pseudo-labeling to further select balanced samples to participate in training. Furthermore, to alleviate the risk of overfitting tail classes, we devised a class-aware feature MixUp (CFM) augmentation algorithm, which can further enhance the features of each class by considering their class sizes. Experimental results demonstrate that FGBC achieves state-of-the-art performance on datasets from CIFAR-10/100, SVHN and Small ImageNet-127 under various levels of CISSL conditions.}
}
@article{SHAO2023109765,
title = {Video anomaly detection with NTCN-ML: A novel TCN for multi-instance learning},
journal = {Pattern Recognition},
volume = {143},
pages = {109765},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109765},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004636},
author = {Wenhao Shao and Ruliang Xiao and Praboda Rajapaksha and Mengzhu Wang and Noel Crespi and Zhigang Luo and Roberto Minerva},
keywords = {Video process, Pattern recognition, Anomaly detection, Feature extraction, Temporal convolutional network, Deep learning},
abstract = {A key challenge in video anomaly detection is the identification of rare abnormal patterns in the positive instances as they exhibit only a small variation compared to normal patterns, and they are largely biased by the dominant negative instances. To address this issue, we propose a weakly supervised video anomaly detection model called NTCN-ML - Novel Temporal Convolutional Network Multi-Instance Learning Model. The NTCN-ML model extracts temporal representations of video data to construct a time-series pattern to optimize the multi-instance learning process. The model examines the correlation between positive and negative samples in the multi-instance learning process to balance the feature association between rare positive and negative instances. The video anomaly detection with the NTCN-ML model achieved 95.3% and 85.1% accuracy for UCF-Crime and ShanghaiTech datasets, respectively, and outperformed the baseline models.}
}
@article{ZHAO2023109789,
title = {AGMN: Association graph-based graph matching network for coronary artery semantic labeling on invasive coronary angiograms},
journal = {Pattern Recognition},
volume = {143},
pages = {109789},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109789},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004879},
author = {Chen Zhao and Zhihui Xu and Jingfeng Jiang and Michele Esposito and Drew Pienta and Guang-Uei Hung and Weihua Zhou},
keywords = {Coronary artery disease, Coronary arterial anatomy, Semantic labeling, Graph matching network},
abstract = {Semantic labeling of coronary arterial segments in invasive coronary angiography (ICA) is important for automated assessment and report generation of coronary artery stenosis in computer-aided coronary artery disease (CAD) diagnosis. However, separating and identifying individual coronary arterial segments is challenging because morphological similarities of different branches on the coronary arterial tree and human-to-human variabilities exist. Inspired by the training procedure of interventional cardiologists for interpreting the structure of coronary arteries, we propose an association graph-based graph matching network (AGMN) for coronary arterial semantic labeling. We first extract the vascular tree from invasive coronary angiography (ICA) and convert it into multiple individual graphs. Then, an association graph is constructed from two individual graphs where each vertex represents the relationship between two arterial segments. Thus, we convert the arterial segment labeling task into a vertex classification task; ultimately, the semantic artery labeling becomes equivalent to identifying the artery-to-artery correspondence on graphs. More specifically, the AGMN extracts the vertex features by the embedding module using the association graph, aggregates the features from adjacent vertices and edges by graph convolution network, and decodes the features to generate the semantic mappings between arteries. By learning the mapping of arterial branches between two individual graphs, the unlabeled arterial segments are classified by the labeled segments to achieve semantic labeling. A dataset containing 263 ICAs was employed to train and validate the proposed model, and a five-fold cross-validation scheme was performed. Our AGMN model achieved an average accuracy of 0.8264, an average precision of 0.8276, an average recall of 0.8264, and an average F1-score of 0.8262, which significantly outperformed existing coronary artery semantic labeling methods. In conclusion, we have developed and validated a new algorithm with high accuracy, interpretability, and robustness for coronary artery semantic labeling on ICAs.}
}
@article{TITO2023109834,
title = {Hierarchical multimodal transformers for Multipage DocVQA},
journal = {Pattern Recognition},
volume = {144},
pages = {109834},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109834},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005320},
author = {Rubèn Tito and Dimosthenis Karatzas and Ernest Valveny},
keywords = {Multipage document Visual Question Answering, Document Visual Question Answering, Multipage documents, Document Intelligence},
abstract = {Existing work on DocVQA only considers single-page documents. However, in real applications documents are mostly composed of multiple pages that should be processed altogether. In this work, we propose a new multimodal hierarchical method Hi-VT5, that overcomes the limitations of current methods to process long multipage documents. In contrast to previous hierarchical methods that focus on different semantic granularity (He et al., 2021) or different subtasks (Zhou et al., 2022) used in image classification. Our method is a hierarchical transformer architecture where the encoder learns to summarize the most relevant information of every page and then, the decoder uses this summarized representation to generate the final answer, following a bottom-up approach. Moreover, due to the lack of multipage DocVQA datasets, we also introduce MP-DocVQA, an extension of SP-DocVQA where questions are posed over multipage documents instead of single pages. Through extensive experimentation, we demonstrate that Hi-VT5 is able, in a single stage, to answer the questions and provide the page that contains the answer, which can be used as a kind of explainability measure.}
}
@article{TAO2023109761,
title = {Learning discriminative feature representation with pixel-level supervision for forest smoke recognition},
journal = {Pattern Recognition},
volume = {143},
pages = {109761},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109761},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004594},
author = {Huanjie Tao and Qianyue Duan and Minghao Lu and Zhenwu Hu},
keywords = {Deep neural network, Component separation, Forest smoke recognition, Supervision information},
abstract = {Existing vision-based smoke recognition methods still face the issues of low detection rates and high false alarm rates in complex scenes. One reason is that they label light smoke and heavy smoke as the same value, which ignores the differences in multiple attribute information involved in the smoke imaging process. To solve this issue, this paper presents a pixel-level supervision neural network (PSNet) to learn discriminative feature representations for forest smoke recognition. First, the pixel-level supervision information, including the background component, smoke component, fusion ratio, and class information, is cooperatively considered to effectively guide the model training process. To avoid negative transfer caused by the asynchronous optimization of shared layer parameters and achieve synchronous minimization of each loss term, a regularization term based on the smoke imaging principle and a weight dynamic updating method are proposed to balance the weight coefficients of different loss terms. Second, a detail-difference-aware module (DDAM) based on a detail-difference-aware block (DDAB) and a spatial attention block (SAB) is proposed to distinguish smoke and smoke-like targets by fusing xy-shared convolution and z-shared convolution, which adaptively allocates the weights over different positions to prioritize the most informative visual elements in the spatial domain. Third, an attention-based feature separation module (AFSM) is proposed to relieve mutual interference in extracting background features and smoke features by designing component interaction attention (CIA), background component attention (BCA), smoke component attention (SCA), and enhanced residual blocks (ERBs), which can guide the interaction and separation process of background information and smoke information to enhance the discriminative spatial features and suppress interference features. ERB effectively eliminates noise and enhances smoke edge information based on median filters. Finally, to further enhance the feature representation capability, a multiconnection aggregation method (MCAM) is proposed by fully aggregating local and global features simultaneously. Extensive experiments show that our method achieves better performance than existing smoke recognition methods. Extensive experiments show that our PSNet achieves better performance than existing smoke recognition methods. For smoke recognition, our PSNet achieves a 96.95% detection rate, 3.02% false alarm rate, and 0.9694 F1-score. The average calculation time for each image is only 0.0195. For smoke component separation, our PSNet also achieves 0.0014 on evaluation criteria mean square error between predicted smoke component images and labelled smoke component images. These key experimental results are better than those of previous methods.}
}
@article{DAI2023109806,
title = {KD-Former: Kinematic and dynamic coupled transformer network for 3D human motion prediction},
journal = {Pattern Recognition},
volume = {143},
pages = {109806},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109806},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005046},
author = {Ju Dai and Hao Li and Rui Zeng and Junxuan Bai and Feng Zhou and Junjun Pan},
keywords = {Human motion prediction, Motion kinematics, Motion dynamics, Transformer},
abstract = {Recent studies have made remarkable progress on 3D human motion prediction by describing motion with kinematic knowledge. However, kinematics only considers the 3D positions or rotations of human skeletons, failing to reveal the physical characteristics of human motion. Motion dynamics reflects the forces between joints, explicitly encoding the skeleton topology, whereas rarely exploited in motion prediction. In this paper, we propose the Kinematic and Dynamic coupled transFormer (KD-Former), which incorporates dynamics with kinematics, to learn powerful features for high-fidelity motion prediction. Specifically, We first formulate a reduced-order dynamic model of human body to calculate the forces of all joints. Then we construct a non-autoregressive encoder-decoder framework based on the transformer structure. The encoder involves a kinematic encoder and a dynamic encoder, which are respectively responsible for extracting the kinematic and dynamic features for given history sequences via a spatial transformer and a temporal transformer. Future query sequences are decoded in parallel in the decoder by leveraging the encoded kinematic and dynamic information of history sequences. Experiments on Human3.6M and CMU MoCap benchmarks verify the effectiveness and superiority of our method. Code will be available at: https://github.com/wslh852/KD-Former.git.}
}
@article{WANG2023109830,
title = {CrowdMLP: Weakly-supervised crowd counting via multi-granularity MLP},
journal = {Pattern Recognition},
volume = {144},
pages = {109830},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109830},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005289},
author = {Mingjie Wang and Jun Zhou and Hao Cai and Minglun Gong},
keywords = {Weakly-supervised learning, Crowd counting, Multi-granularity MLP, Self-supervised proxy task},
abstract = {Currently, state-of-the-art crowd counting algorithms rely excessively on location-level annotations, which are burdensome to acquire. When only weak supervisory signals at the count level are available, it is arduous and error-prone to regress total counts due to the lack of explicit spatial constraints. To address this issue, we propose a novel and efficient counter, CrowdMLP, which explores the modelling of global dependencies of embeddings and regresses total counts by designing a multi-granularity MLP regressor. Specifically, a locally-focused pre-trained frontend is used to extract crude feature maps with intrinsic spatial cues, preventing the model from collapsing into trivial outcomes. The crude embeddings, along with the raw crowd scenes, are tokenized at different granularity levels. Next, the multi-granularity MLP mixes tokens at the dimensions of cardinality, channel, and spatial for mining global information. We also propose an effective proxy task called Split-Counting to overcome the limited samples and the lack of spatial hints in a self-supervised manner. Extensive experiments demonstrate that CrowdMLP significantly outperforms existing weakly-supervised counting algorithms and performs better than state-of-the-art location-level supervised approaches.}
}
@article{TAKHANOV2023109777,
title = {Autoencoders for a manifold learning problem with a jacobian rank constraint},
journal = {Pattern Recognition},
volume = {143},
pages = {109777},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109777},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004752},
author = {Rustem Takhanov and Y. Sultan Abylkairov and Maxat Tezekbayev},
keywords = {Manifold learning, Dimensionality reduction, Alternating algorithm, Ky fan antinorm, Autoencoders, Rank constraints},
abstract = {We formulate the manifold learning problem as the problem of finding an operator that maps any point to a close neighbor that lies on a “hidden” k-dimensional manifold. We call this operator the correcting function. Under this formulation, autoencoders can be viewed as a tool to approximate the correcting function. Given an autoencoder whose Jacobian has rank k, we deduce from the classical Constant Rank Theorem that its range has a structure of a k-dimensional manifold. A k-dimensionality of the range can be forced by the architecture of an autoencoder (by fixing the dimension of the code space), or alternatively, by an additional constraint that the rank of the autoencoder mapping is not greater than k. This constraint is included in the objective function as a new term, namely a squared Ky-Fan k-antinorm of the Jacobian function. We claim that this constraint is a factor that effectively reduces the dimension of the range of an autoencoder, additionally to the reduction defined by the architecture. We also add a new curvature term into the objective. To conclude, we experimentally compare our approach with the CAE+H method on synthetic and real-world datasets.}
}
@article{LIU2023109822,
title = {Unpaired image super-resolution using a lightweight invertible neural network},
journal = {Pattern Recognition},
volume = {144},
pages = {109822},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109822},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005204},
author = {Huan Liu and Mingwen Shao and Yuanjian Qiao and Yecong Wan and Deyu Meng},
keywords = {Image super-resolution, Unpaired SR, Image degradation, Invertible neural network, Generative adversarial network},
abstract = {Unpaired image super-resolution (SR) has recently attracted considerable attention in the unsupervised SR community. In contrast to supervised SR, existing unpaired SR methods inevitably resort to the generative adversarial network (GAN) to explore data distribution on the given HR and unpaired LR dataset. Nevertheless, predominant strategies often strive for sophisticated network structures or training pipelines, making them intractable to apply in real-world scenarios. In this work, a lightweight invertible neural network (INN) is proposed for unpaired SR to alleviate this limitation. Specifically, we regard image degradation and SR as a pair of mutually-inverse tasks and replace the two generators in one-stage GAN with INN. Due to the information lossless nature of INN, it is impossible to generate noise in vain during image degradation. We thus design a simple noise injection network to induce realistic noise, thereby simulating real LR images. To further maintain the stability and realism of the noise, we propose to extract the noise prior from the real-world LR image. With extracted noise prior as input, our noise injection network can narrow the gap between the generated noise and the real one, thereby encouraging the degraded images to match the real-world LR domain. Extensive experiments demonstrate that our method achieves comparable performance with other SOTA methods in quantitative and qualitative evaluations while enjoying faster speed and much smaller parameters.}
}
@article{ZHANG2023109801,
title = {Construction of a feature enhancement network for small object detection},
journal = {Pattern Recognition},
volume = {143},
pages = {109801},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109801},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004995},
author = {Hongyun Zhang and Miao Li and Duoqian Miao and Witold Pedrycz and Zhaoguo Wang and Minghui Jiang},
keywords = {Collision detection, Granular computing, High-Resolution block, FENet, HR-FPN, Small object detection,},
abstract = {Limited by the size, location, number of samples and other factors of the small object itself, the small object is usually insufficient, which degrades the performance of the small object detection algorithms. To address this issue, we construct a novel Feature Enhancement Network (FENet) to improve the performance of small object detection. Firstly, an improved data augmentation method based on collision detection and spatial context extension (CDCI) is proposed to effectively expand the possibility of small object detection. Then, based on the idea of Granular Computing, a multi-granular deformable convolution network is constructed to acquire the offset feature representation at the different granularity levels. Finally, we design a high-resolution block (HR block) and build High-Resolution Block-based Feature Pyramid by parallel embedding HR block in FPN (HR-FPN) to make full use different granularity and resolution features. By above strategies, FENet can acquire sufficient feature information of small objects. In this paper, we firstly applied the multi-granularity deformable convolution to feature extraction of small objects. Meanwhile, a new feature fusion module is constructed by optimizing feature pyramid to maintain the detailed features and enrich the semantic information of small objects. Experiments show that FENet achieves excellent performance compared with performance of other methods when applied to the publicly available COCO dataset, VisDrone dataset and TinyPerson dataset. The code is available at https://github.com/cowarder/FENet.}
}
@article{CSIMOES2023109749,
title = {Gaussian kernel fuzzy c-means with width parameter computation and regularization},
journal = {Pattern Recognition},
volume = {143},
pages = {109749},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109749},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004478},
author = {Eduardo {C． Simões} and Francisco de A. {T． de Carvalho}},
keywords = {Gaussian kernel fuzzy clustering, Kernelization of the metric, Width parameter, Entropy regularization},
abstract = {The conventional Gaussian kernel fuzzy c-means clustering algorithms require selecting the width hyper-parameter, which is data-dependent and fixed for the entire execution. Not only that, but these parameters are the same for every dataset variable. Therefore, the variables have the same importance in the clustering task, including irrelevant variables. This paper proposes a Gaussian kernel fuzzy c-means with kernelization of the metric and automated computation of width parameters. These width parameters change at each iteration of the algorithm and vary from each variable and from each cluster. Thus, this algorithm can re-scale the variables differently, thus highlighting those that are relevant to the clustering task. Fuzzy clustering algorithms with regularization have become popular due to their high performance in large-scale data clustering, robustness for initialization, and low computational complexity. Because the width parameters of the variables can also be controlled by entropy, this paper also proposes Gaussian kernel fuzzy c-means algorithms with kernelization of the metric and automated computation of width parameters through entropy regularization. To demonstrate their usefulness, the proposed algorithms are compared with the conventional KFCM-K algorithm and previous algorithms that automatically compute the width parameter of the Gaussian kernel.}
}
@article{HUANG2023109736,
title = {Single-particle reconstruction in cryo-EM based on three-dimensional weighted nuclear norm minimization},
journal = {Pattern Recognition},
volume = {143},
pages = {109736},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109736},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300434X},
author = {Chaoyan Huang and Tingting Wu and Juncheng Li and Bin Dong and Tieyong Zeng},
keywords = {Single-particle reconstruction, Cryogenic electron microscopy, Forward-backward splitting algorithm, Three-dimensional weighted nuclear norm minimization},
abstract = {Single-particle reconstruction (SPR) in cryogenic electron microscopy (cryo-EM) aims at aligning and averaging two-dimensional micrographs to reconstruct a three-dimensional particle. How to reconstruct micrographs from heavy noise is a crucial point for achieving better micrograph quality, and thus many methods focus on noise removal. However, new problems such as over-smoothing often occur in their results due to failure in handling heavy noise well. This paper proposes a three-dimensional weighted nuclear norm minimization (3DWNNM) model for SPR in the cryo-EM task to address these issues. Specifically, we design a minimization solver based on the forward-backward splitting algorithm to tackle our model efficiently. Under certain conditions, this solution has an energy-decaying feature and performs exceptionally well in reconstruction. Numerical experiments fully demonstrate the effectiveness and the robustness of the proposed method.}
}