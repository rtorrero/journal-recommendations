@article{VELDEN2024110547,
title = {A general framework for implementing distances for categorical variables},
journal = {Pattern Recognition},
volume = {153},
pages = {110547},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110547},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400298X},
author = {Michel van de Velden and Alfonso Iodice D’Enza and Angelos Markos and Carlo Cavicchia},
keywords = {Categorical data, Distance, Cluster analysis, Classification, K-NN},
abstract = {The degree to which objects differ from each other with respect to observations on a set of variables, plays an important role in many statistical methods. Many data analysis methods require a quantification of differences in the observed values which we can call distances. An appropriate definition of a distance depends on the nature of the data and the problem at hand. For distances between numerical variables, there exist many definitions that depend on the size of the observed differences. For categorical data, the definition of a distance is more complex as there is no straightforward quantification of the size of the observed differences. In this paper, we introduce a flexible framework for efficiently computing distances between categorical variables, supporting existing and new formulations tailored to specific contexts. In supervised classification, it enhances performance by integrating relationships between response and predictor variables. This framework allows measuring differences among objects across diverse data types and domains.}
}
@article{WANG2024110621,
title = {A multi-resolution self-supervised learning framework for semantic segmentation in histopathology},
journal = {Pattern Recognition},
volume = {155},
pages = {110621},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110621},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003728},
author = {Hao Wang and Euijoon Ahn and Jinman Kim},
keywords = {Multi-resolution histopathology learning, Self-supervised learning, Semantic segmentation},
abstract = {Modern whole slide imaging technique together with supervised deep learning approaches have been advancing the field of histopathology, enabling accurate analysis of tissues. These approaches use whole slide images (WSIs) at various resolutions, utilising low-resolution WSIs to identify regions of interest in the tissue and high-resolution for detailed analysis of cellular structures. Due to the labour-intensive process of annotating gigapixels WSIs, accurate analysis of WSIs remains challenging for supervised approaches. Self-supervised learning (SSL) has emerged as an approach to build efficient and robust models using unlabelled data. It has been successfully used to pre-train models to learn meaningful image features which are then fine-tuned with downstream tasks for improved performance compared to training models from scratch. Yet, existing SSL methods optimised for WSI are unable to leverage the multi-resolutions and instead, work only in an individual resolution neglecting the hierarchical structure of multi-resolution inputs. This limitation prevents from the effective utilisation of complementary information between different resolutions, hampering discriminative WSI representation learning. In this paper we propose a Multi-resolution SSL Framework for WSI semantic segmentation (MSF-WSI) that effectively learns histopathological features. Our MSF-WSI learns complementary information from multiple WSI resolutions during the pre-training stage; this contrasts with existing works that only learn between the resolutions at the fine-tuning stage. Our pre-training initialises the model with a comprehensive understanding of multi-resolution features which can lead to improved performance in the subsequent tasks. To achieve this, we introduced a novel Context-Target Fusion Module (CTFM) and a masked jigsaw pretext task to facilitate the learning of multi-resolution features. Additionally, we designed Dense SimSiam Learning (DSL) strategy to maximise the similarities of image features from early model layers to enable discriminative learned representations. We evaluated our method using three public datasets on breast and liver cancer segmentation tasks. Our experiment results demonstrated that our MSF-WSI surpassed the accuracy of other state-of-the-art methods in downstream fine-tuning and semi-supervised settings.}
}
@article{LI2024110536,
title = {Decoupled and boosted learning for skeleton-based dynamic hand gesture recognition},
journal = {Pattern Recognition},
volume = {153},
pages = {110536},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110536},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002875},
author = {Yangke Li and Guangshun Wei and Christian Desrosiers and Yuanfeng Zhou},
keywords = {Hand gesture recognition, Decoupled skeleton representation, Boosted learning, Multi-scale features, Attention mechanism},
abstract = {With the development of cost-effective depth sensors, skeleton-based dynamic hand gesture recognition has made significant progress. Existing methods mostly utilize a single model to learn all spatial–temporal features. Meanwhile, they cannot effectively boost key features and make use of multi-scale features. In this paper, we propose a lightweight dual-stream framework, which consists of a temporal mutual boosted stream (TMB-Stream) and a spatial self-boosted stream (SSB-Stream). In the TMB-Stream, we design a hybrid attention module (HAM) to boost important motion features from temporal sequences, which is composed of a multi-scale multi-head attention module (MMAM) and a spatial–temporal attention module (STAM). In the SSB-Stream, we present a self-boosted learning manner to promote the performance of the spatial stream. Specifically, we design a multi-scale auto-encoder (MAE), which can use limited skeleton data to extract and boost spatial latent features by minimizing the gap between original and reconstructed skeleton images. In addition, we propose a multi-scale fusion module (MFM) to effectively fuse multi-scale features in stages. Experimental results show that our lightweight framework yields satisfactory performance on SHREC’17 Track and DHG-14/28 datasets, as well as very competitive performance on FPHA dataset.}
}
@article{CHEN2024110649,
title = {Underwater object detection in noisy imbalanced datasets},
journal = {Pattern Recognition},
volume = {155},
pages = {110649},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110649},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400400X},
author = {Long Chen and Tengyue Li and Andy Zhou and Shengke Wang and Junyu Dong and Huiyu Zhou},
keywords = {Underwater object detection, Imbalanced detection, Noise removal, Factor-agnostic gradient re-weighting},
abstract = {Class imbalance occurs in the datasets with a disproportionate ratio of observations. The class imbalance problem drives the detection and classification systems to be more biased towards the over-represented classes while the under-represented classes may not receive sufficient learning. Previous works often deploy distribution based re-balancing approaches to address this problem. However, these established techniques do not work properly for underwater object detection where label noise commonly exists. In our experiments, we observe that the imbalanced detection problem may be caused by imbalance data distributions or label noise. To deal with these challenges, we first propose a noise removal (NR) algorithm to remove label noise in the datasets, and then propose a factor-agnostic gradient re-weighting algorithm (FAGR) to address the imbalanced detection problem. FAGR provides a rebalanced gradient to each class, which encourages the detection network to treat all the classes equally whilst minimising the detection discrepancy. Our proposed NR+FAGR framework achieves state-of-the-art (SOAT) performance on three underwater object datasets due to its high capacity in handling the class imbalance and noise issues. The source code will be made available at: https://github.com/IanDragon.}
}
@article{LIU2024110633,
title = {DSFusion: Infrared and visible image fusion method combining detail and scene information},
journal = {Pattern Recognition},
volume = {154},
pages = {110633},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110633},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003844},
author = {Kuizhuang Liu and Min Li and Cheng Chen and Chengwei Rao and Enguang Zuo and Yunling Wang and Ziwei Yan and Bo Wang and Chen Chen and Xiaoyi Lv},
keywords = {Image fusion, Local attention mechanism, Improved channel attention mechanism, Loss function},
abstract = {The goal of image fusion is to combine the complementary features of two images to generate an information-rich fused image. However, taking into account both detail and scene information is difficult for existing image fusion algorithms. Therefore, we propose an infrared and visible image fusion method combining details and scene information (DSFusion). Specifically, we first design a local attention module (LAM), which performs feature extraction on the source image from multiple perspectives in order to better preserve minute detail information. Moreover, in order to distinguish and highlight the differences between the two modal images, we improved the channel attention module. Finally, we design a new loss function that can effectively balance the detail and scene information of the fusion image. Extensive testing on publicly available datasets demonstrates that DSFusion surpasses state of the art in both qualitative and quantitative evaluation. Furthermore, promising outcomes have been obtained in generalization experiments by directly expanding the trained model to other datasets, indicating the model’s excellent generalization capability. The code is available at https://github.com/LKZ1584905069/DSFusion.}
}
@article{HAN2024110581,
title = {Ensemble filter-transfer learning algorithm},
journal = {Pattern Recognition},
volume = {154},
pages = {110581},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110581},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003327},
author = {Honggui Han and Mengmeng Li and Hongyan Yang and Xiaolong Wu and Huayun Han},
keywords = {Transfer learning, Knowledge filter algorithm, Ensemble transfer mechanism, Model prediction},
abstract = {Transfer learning algorithms are capable to apply previously learned knowledge in source domain, which alleviates much expensive efforts of knowledge recollection in target domain. But the knowledge in source domain is always imperfect due to redundant or contaminated information. To solve this problem, an ensemble filter-transfer learning (EFTL) algorithm based on the source knowledge reconstruction is proposed in this paper. First, a knowledge partition strategy based on model is developed to classify the source knowledge into different types. Then, the positive knowledge can be identified, which contributes to target domain with a rejection of the negative transfer. Second, a knowledge filter algorithm is introduced to filter out the redundant information in non-positive knowledge. Then, the non-positive knowledge can be reconstructed by this algorithm to prevent the loss of available information. Third, an ensemble transfer mechanism is established to realize the synchronous transfer of omnidirectional knowledge for the target domain. Finally, comparative experiments on model prediction in practical applications are provided to illustrate the dependability of EFTL.}
}
@article{MAO2024110645,
title = {Contrastive cross-modal clustering with twin network},
journal = {Pattern Recognition},
volume = {155},
pages = {110645},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110645},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003960},
author = {Yiqiao Mao and Xiaoqiang Yan and Shizhe Hu and Yangdong Ye},
keywords = {Cross-modal clustering, Correlation information, Contrastive learning, Twin network},
abstract = {Cross-modal clustering (CMC) methods explore the correlation information between multiple modalities to improve clustering performance. However, the obvious differences between heterogeneous modalities make it difficult to obtain the correlation information directly. In this paper, we propose a novel Contrastive Cross-modal Clustering with Twin Network (3CTnet) for CMC, which contrasts the differences of multiple modalities to fully mine the correlation information. The 3CTnet contains two modal-special encoders and an attention-based correlation propagate module (CPM). First, the modal-special encoders are trained by pseudo-labels to learn the clustering structure and feature of single modality. Then we contrast the clustering structures and features of different modalities to explore the inter-cluster and inter-feature correlation information simultaneously. Finally, the CPM is designed to propagate the learned correlation information among modal-special encoders to further optimize the learning of features and clustering structures. The experiments show that 3CTnet outperforms the state-of-the-art CMC methods on six large datasets.}
}
@article{LIN2024110593,
title = {Deep graph layer information mining convolutional network},
journal = {Pattern Recognition},
volume = {154},
pages = {110593},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110593},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003443},
author = {Guangfeng Lin and Wenchao Wei and Xiaobing Kang and Kaiyang Liao and Erhu Zhang},
keywords = {Deep learning, Graph convolutional neural network, Graph learning, Hierarchical structure},
abstract = {Graph convolution network is a powerful method of deep learning of graph structure data. Existing methods usually adjust the neighborhood information aggregation mode or optimize the graph topology layer by layer for improving the graph convolution network. However, these methods seldom consider the discriminative information about hierarchical characteristics nodes (some special nodes only can be correctly classified in one layer and are the misclassification nodes in the other layers of deep graph convolutional networks) in the different layers for complementing the neighborhood topology information. To further find these information, a deep graph layer information mining convolutional network (GLIM) can alternately measure the neighborhood ranking information on topology structure and update the residual identity mapping node information on the different layers for enhancing the model classification performance. Moreover, GLIM can construct a unified framework with the various hyper-parameters for the different graph learning method based on graph convolution network. Experiments show GLIM outperforms the state-of-the-art methods for semi-supervised node classification in three cite datasets (Cora, CiteSeer,and PubMed) and three image datasets (MNIST, Cifar10 and Cifar100).}
}
@article{LIU2024110588,
title = {Prototype learning based generic multiple object tracking via point-to-box supervision},
journal = {Pattern Recognition},
volume = {154},
pages = {110588},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110588},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400339X},
author = {Wenxi Liu and Yuhao Lin and Qi Li and Yinhua She and Yuanlong Yu and Jia Pan and Jason Gu},
keywords = {Generic multiple object tracking, Multiple object tracking, Prototype learning, Object detection, Deep learning},
abstract = {Generic multiple object tracking aims to recover the trajectories for generic moving objects of the same category. This task relies on the ability of effectively extracting representative features of the target objects. To this end, we propose a novel prototype learning based model, PLGMOT, that can explore the template features of an exemplar object and extend to more objects to acquire their prototype. Their prototype features can be continuously updated during the video, in favor of generalization to all the target objects with different appearances. More importantly, on the public benchmark GMOT-40, our method achieves more than 14% advantage over the state-of-the-art methods, with less than 0.5% of the training data that is not even completely annotated in the form of bounding boxes, thanks to our proposed point-to-box label refinement training algorithm and hierarchical motion-aware association algorithm.}
}
@article{YANG2024110569,
title = {LDDMM-Face: Large deformation diffeomorphic metric learning for cross-annotation face alignment},
journal = {Pattern Recognition},
volume = {154},
pages = {110569},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110569},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003200},
author = {Huilin Yang and Junyan Lyu and Pujin Cheng and Roger Tam and Xiaoying Tang},
keywords = {Face alignment, Facial landmarks, Diffeomorphic mapping, Deep learning},
abstract = {We propose an innovative, flexible, and consistent cross-annotation face alignment framework, LDDMM-Face, the key contribution of which is a deformation layer that naturally embeds facial geometry in a diffeomorphic way. This enables and solves cross-annotation face alignment tasks that were impossible in the existing works. Instead of predicting facial landmarks via a heatmap or coordinate regression, we formulate the face alignment task in a diffeomorphic registration manner and predict momenta that uniquely parameterize the deformation between the initial boundary and true boundary. We then perform large deformation diffeomorphic metric mapping (LDDMM) simultaneously for curve and landmark to localize the facial landmarks. The novel embedding of LDDMM into a deep network allows LDDMM-Face to consistently annotate facial landmarks without ambiguity and flexibly handle various annotation schemes, and can even predict dense annotations from sparse ones. To the best of our knowledge, this is the first study to leverage learning-based diffeomorphic mapping for face alignment. Our method can be easily integrated into various face alignment networks. We extensively evaluate LDDMM-Face on five benchmark datasets: 300W, WFLW, HELEN, COFW-68, and AFLW. LDDMM-Face distinguishes itself with outstanding performance when dealing with within-dataset cross-annotation learning (sparse-to-dense) and cross-dataset learning (different training and testing datasets). In addition, LDDMM-Face shows promising results on the most challenging task of cross-dataset cross-annotation learning (different training and testing datasets with different annotations). Our codes are available at https://github.com/CRazorback/LDDMM-Face.}
}
@article{AKHTER2024110548,
title = {Low-cost orthogonal basis-core extraction for classification and reconstruction using tensor ring},
journal = {Pattern Recognition},
volume = {154},
pages = {110548},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110548},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002991},
author = {Suravi Akhter and Muhammad Mahbub Alam and Md. Shariful Islam and M. Arshad Momen and Md. Shariful Islam and Mohammad Shoyaib},
keywords = {Feature extraction, Classification, OTR, Reconstruction, Reshaping of matrices},
abstract = {Tensor based methods have gained popularity for being able to represent multi-aspect real world data in a lower dimensional space. Among them, methods with orthogonal factors perform relatively better in classification. However, most of them cannot handle higher order data. Recently, Tensor Ring (TR) based methods are proposed to combat with the higher order issue more effectively focusing on both classification and reconstruction. A TR-based method with orthogonal cores performs reasonably well for a given error. However, its computational complexity is very high and might produce extra features. To solve these issues, in this paper, we propose a method named as Orthogonal basis-core extraction using Tensor Ring (OTR) that can facilitate better discrimination and reconstruction at a lower cost. To maintain the ring property, we also show, theoretically, that reshaping of the product of semi-orthogonal reshaped cores remains semi-orthogonal. Rigorous experiments over eighteen benchmark datasets from different fields demonstrate the superiority of OTR over state-of-the-art methods in terms of classification and reconstruction.}
}
@article{KONG2024110617,
title = {Robust feature selection via central point link information and sparse latent representation},
journal = {Pattern Recognition},
volume = {154},
pages = {110617},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110617},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003686},
author = {Jiarui Kong and Ronghua Shang and Weitong Zhang and Chao Wang and Songhua Xu},
keywords = {Center matrix, Link information, Sparse latent representation, Unsupervised feature selection, Dual graph structure},
abstract = {Before conducting unsupervised feature selection, it is usually assumed that these data are independent of each other. On the contrary, real data will influence each other. Therefore, traditional feature selection methods may lose information related to each other between data. This can lead to inaccurately generated pseudo-label information and may result in poor feature selection results. To find solutions to this issue, this paper proposes robust feature selection via central point link information and sparse latent representation (CPSLR). Firstly, structure a link graph by calculating the center matrix to store the distance information from the sample to the center point. If two samples have similar distances to the center point, it can be determined that they belong to the same class. Therefore, the similarity between samples is preserved, and more accurate pseudo-label information is obtained. Secondly, CPSLR uses data graph and link graph to form a dual graph structure. It can not only retain the link information between samples but also retain the manifold structures of the samples. Then, CPSLR saves the interconnection contents between samples by sparse latent representation. That is, the constraint l2,1-norm is exerted on the expression of latent representation, and sparse non-redundant interconnection information is preserved. And by combining central point link information with sparse latent representation makes the interconnections between data reserved more comprehensive. That is to say, the pseudo-labels obtained are more like the real labels of the classes. Finally, CPSLR constrains the feature transformation matrix by l2,1/2-norm constraint so as to select robust and sparse features. CPSLR uses l2,1/2-norm constraint to assure that the feature transformation matrix is sparse, selecting more discriminative features, thereby obtaining the feature selection that can improve its efficiency. The experiments demonstrate that the clustering result of CPSLR outperform six classical or latest compared algorithms on eight datasets.}
}
@article{YUAN2024110565,
title = {TSAR-MVS: Textureless-aware segmentation and correlative refinement guided multi-view stereo},
journal = {Pattern Recognition},
volume = {154},
pages = {110565},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110565},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003169},
author = {Zhenlong Yuan and Jiakai Cao and Zhaoqi Wang and Zhaoxin Li},
keywords = {Multi-view stereo, 3D reconstruction, Filtering, Superpixel, Segmentation},
abstract = {The reconstruction of textureless areas has long been a challenging problem in MVS due to lack of reliable pixel correspondences between images. In this paper, we propose the Textureless-aware Segmentation And Correlative Refinement guided Multi-View Stereo (TSAR-MVS), a novel method that effectively tackles challenges posed by textureless areas in 3D reconstruction through filtering, refinement and segmentation. First, we implement the joint hypothesis filtering, a technique that merges a confidence estimator with a disparity discontinuity detector to eliminate incorrect depth estimations. Second, to spread the pixels with confident depth, we introduce an iterative correlation refinement strategy that leverages RANSAC to generate 3D planes based on superpixels, succeeded by a weighted median filter for broadening the influence of accurately determined pixels. Finally, we present a textureless-aware segmentation method that leverages edge detection and line detection for accurately identify large textureless regions for further depth completion. Experiments on ETH3D, Tanks & Temples and Strecha datasets demonstrate the superior performance and strong generalization capability of our proposed method.}
}
@article{YAO2024110577,
title = {A recurrent graph neural network for inductive representation learning on dynamic graphs},
journal = {Pattern Recognition},
volume = {154},
pages = {110577},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110577},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003285},
author = {Hong-Yu Yao and Chun-Yang Zhang and Zhi-Liang Yao and C.L. Philip Chen and Junfeng Hu},
keywords = {Recurrent graph neural networks, Dynamic networks, Graph representation learning, Inductive learning, Unsupervised learning},
abstract = {Graph representation learning has recently garnered significant attention due to its wide applications in graph analysis tasks. It is well-known that real-world networks are dynamic, with edges and nodes evolving over time. This presents unique challenges that are distinct from those of static networks. However, most graph representation learning methods are either designed for static graphs, or address only partial challenges associated with dynamic graphs. They overlook the intricate interplay between topology and temporality in the evolution of dynamic graphs and the complexity of sequence modeling. Therefore, we propose a new dynamic graph representation learning model, called as R-GraphSAGE, which takes comprehensive considerations for embedding dynamic graphs. By incorporating a recurrent structure into GraphSAGE, the proposed R-GraphSAGE explores structural and temporal patterns integrally to capture more fine-grained evolving patterns of dynamic graphs. Additionally, it offers a lightweight architecture to decrease the computational costs for handling snapshot sequences, achieving a balance between performance and complexity. Moreover, it can inductively process the addition of new nodes and adapt to the situations without labels and node attributes. The performance of the proposed R-GraphSAGE is evaluated across various downstream tasks with both synthetic and real-world networks. The experimental results demonstrate that it outperforms state-of-the-art baselines by a significant margin in most cases.}
}
@article{ZHU2024110553,
title = {Brain tumor segmentation in MRI with multi-modality spatial information enhancement and boundary shape correction},
journal = {Pattern Recognition},
volume = {153},
pages = {110553},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110553},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003042},
author = {Zhiqin Zhu and Ziyu Wang and Guanqiu Qi and Neal Mazur and Pan Yang and Yu Liu},
keywords = {Brain tumor segmentation, Multi-modality MRI, Spatial information enhancement, Boundary shape correction},
abstract = {Brain tumor segmentation is currently of a priori guiding significance in medical research and clinical diagnosis. Brain tumor segmentation techniques can accurately partition different tumor areas on multi-modality images captured by magnetic resonance imaging (MRI). Due to the unpredictable pathological process of brain tumor generation and growth, brain tumor images often show irregular shapes and uneven internal gray levels. Existing neural network-based segmentation methods with an encoding/decoding structure can perform image segmentation to some extent. However, they ignore issues such as differences in multi-modality information, loss of spatial information, and under-utilization of boundary information, thereby limiting the further improvement of segmentation accuracy. This paper proposes a multimodal spatial information enhancement and boundary shape correction method consisting of a modality information extraction (MIE) module, a spatial information enhancement (SIE) module, and a boundary shape correction (BSC) module. The above three modules act on the input, backbone, and loss functions of deep convolutional networks (DCNN), respectively, and compose an end-to-end 3D brain tumor segmentation model. The three proposed modules can solve the low utilization rate of effective modality information, the insufficient spatial information acquisition ability, and the improper segmentation of key boundary positions can be solved. The proposed method was validated on BraTS2017, 2018, and 2019 datasets. Comparative experimental results confirmed the effectiveness and superiority of the proposed method over state-of-the-art segmentation methods.}
}
@article{YU2024110650,
title = {Degradation-removed multiscale fusion for low-light salient object detection},
journal = {Pattern Recognition},
volume = {155},
pages = {110650},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110650},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004011},
author = {Nana Yu and Jie Wang and Hong Shi and Zihao Zhang and Yahong Han},
keywords = {Salient object detection, Low-light, Image enhancement, Retinex decomposition, Multiscale adaptive fusion},
abstract = {Low-light is practical in real-life applications and leads to decreased performance of visual perception. For example, the challenges significantly burden the applications of salient object detection (SOD). Existing methods primarily focus on SOD under normal-light conditions. Although some forward-looking work has attempted to address the problem, the model design fails to directly target the physical degradation factor of darkness. Introducing extra sensors (e.g., depth or infrared) may supplement more necessary saliency information, however, acquisition costs and computational overhead will be involved. To improve the SOD under low-light condition, we devise a new image enhancement method and integrate it into the SOD network to form a new learning framework. Specifically, we employ Retinex-guided self-enhancement in combination with multiscale cross-channel detection, effectively mitigating the influence of factors such as image dark degradation and low contrast. This approach enhances the detection performance without incurring additional costs. Additionally, to promote efforts towards this task, we construct a comprehensive low-light SOD dataset benchmark, named YLLSOD. Finally, we conduct extensive comparative experiments between our proposed method and the state-of-the-art single-modal methods, validating the competitiveness of our approach. Comparative experiments with some representative bi-modal methods further illustrate the advantages of our proposed method. Our new dataset will be available at https://github.com/ynn1030/YLLSOD.}
}
@article{YU2024110552,
title = {Robformer: A robust decomposition transformer for long-term time series forecasting},
journal = {Pattern Recognition},
volume = {153},
pages = {110552},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110552},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003030},
author = {Yang Yu and Ruizhe Ma and Zongmin Ma},
keywords = {Long-term time series forecasting, Transformer, Time series decomposition},
abstract = {Transformer-based forecasting methods have been widely applied to forecast long-term multivariate time series, which achieves significant improvements on extending the forecasting time. However, their performance can degenerate terribly when abrupt trend shift and seasonal fluctuation arise in long-term time series. Hence, we identify two bottlenecks of previous Transformers architecture: (1) the robustless decomposition module and (2) trend shifting problem. These result in a different distribution between the trend prediction and ground truth in the long-term multivariate series forecasting. Towards these bottlenecks, we design Robformer as a novel decomposition-based Transformer, which consists of three new inner module to enhance the predictability of Transformers. Concretely, we renew the decomposition module and add a seasonal component adjustment module to tackle the unstationarized series. Further, we propose a novel inner trend forecasting architecture inspired by polynomial fitting method, which outperforms previous design in accuracy and robustness. Our empirical studies show that Robformer can achieve 17% and 10% relative improvements than state-of-the-art Autoformer and FEDformer baselines under the fair long-term multivariate setting on six benchmarks, covering five mainstream time series forecasting applications: energy, economics, traffic, weather, and disease. The code will be released upon publication.}
}
@article{ZHANG2024110591,
title = {Matrix normal PCA for interpretable dimension reduction and graphical noise modeling},
journal = {Pattern Recognition},
volume = {154},
pages = {110591},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110591},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400342X},
author = {Chihao Zhang and Kuo Gai and Shihua Zhang},
keywords = {Principal component analysis, Dimension reduction, Matrix normal distribution, Sparse inverse covariance, Graphical noise modeling},
abstract = {Principal component analysis (PCA) is one of the most widely used dimension reduction and multivariate statistical techniques. From a probabilistic perspective, PCA seeks a low-dimensional representation of data in the presence of independent identical Gaussian noise. Probabilistic PCA (PPCA) and its variants have been extensively studied for decades. Most of them assume the underlying noise follows a certain independent identical distribution. However, the noise in the real world is usually complicated and structured. To address this challenge, some variants of PCA for non-IID data have been proposed. However, most of the existing methods only assume that the noise is correlated in the feature space while there may exist two-way structured noise. To this end, we propose a powerful and intuitive PCA method (MN-PCA) through modeling the graphical noise by the matrix normal distribution, which enables us to explore the structure of noise in both the feature space and the sample space. MN-PCA obtains a low-rank representation of data and the structure of noise simultaneously. And it can be explained as approximating data over the generalized Mahalanobis distance. We first solve this model by a standard approach – maximizing the regularized likelihood – and then develop a novel algorithm that exploits the Wasserstein distance, which is more robust. Extensive experiments on various data demonstrate their effectiveness.}
}
@article{YANG2024110644,
title = {A safe screening rule with bi-level optimization of ν support vector machine},
journal = {Pattern Recognition},
volume = {155},
pages = {110644},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110644},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003959},
author = {Zhiji Yang and Wanyi Chen and Huan Zhang and Yitian Xu and Lei Shi and Jianhua Zhao},
abstract = {Support vector machine (SVM) has achieved many successes in machine learning, especially for a small sample problem. As a famous extension of the traditional SVM, the ν support vector machine (ν-SVM) has shown outstanding performance due to its great model interpretability. However, it still faces challenges in training overhead for large-scale problems. To address this issue, we propose a safe screening rule with bi-level optimization for ν-SVM (SRBO-ν-SVM) which can screen out inactive samples before training and reduce the computational cost without sacrificing the prediction accuracy. Our SRBO-ν-SVM is strictly deduced by integrating the Karush–Kuhn–Tucker (KKT) conditions, the variational inequalities of convex problems and the ν-property. Furthermore, we develop an efficient dual coordinate descent method (DCDM) to further improve computational speed. Finally, a unified framework for SRBO is proposed to accelerate many SVM-type models, and it is successfully applied to one-class SVM. Experimental results on 6 artificial data sets and 30 benchmark data sets have verified the effectiveness and safety of our proposed methods in supervised and unsupervised tasks.}
}
@article{OUYANG2024110620,
title = {MixingMask: A contour-aware approach for joint object detection and instance segmentation},
journal = {Pattern Recognition},
volume = {155},
pages = {110620},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110620},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003716},
author = {Wenzhe Ouyang and Zenglin Xu and Jing Xu and Qifan Wang and Yong Xu},
keywords = {Object detection, Instance segmentation, Vision perception, Mixing, Contour regression},
abstract = {Remarkable achievements have been made in object detection and segmentation tasks. However, there remains a noticeable scarcity of methodologies that can achieve satisfactory results in both tasks simultaneously. To address this problem, we present a solution called MixingMask, in which we have a key insight to provide specific attention to boundary features by leveraging contour-based segmentation methods. Specifically, our approach commences with a novel contour deformation module that employs mixing operations coupled with the proposed adaptive feature sampling. Successively, the contours are encoded using a decoupled vector for bounding box regression, thereby effectively associating the contour shape with its scale and position. Lastly, we incorporate the proposed contour regression module into the baseline method to achieve specialized attention to boundary features. Such a design not only successfully remedies the prevailing disregard towards boundary features but also forms an implicit liaison between object detection and instance segmentation tasks. Comprehensive experimental assessments validate the superior performance of the proposed method in both object detection and instance segmentation tasks.}
}
@article{YIN2024110580,
title = {Feature selection for multilabel classification with missing labels via multi-scale fusion fuzzy uncertainty measures},
journal = {Pattern Recognition},
volume = {154},
pages = {110580},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110580},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003315},
author = {Tengyu Yin and Hongmei Chen and Zhihong Wang and Keyu Liu and Zhong Yuan and Shi-Jinn Horng and Tianrui Li},
keywords = {Multi-scale fuzzy rough sets, Multilabel feature selection, Missing labels, Uncertainty measures},
abstract = {Numerous high-dimension multilabel data are generated, posing a challenge for multilabel learning. Building effective learning models with discriminative features is essential to improve the performance of multilabel learning. Multilabel feature selection can filter out the discriminative features according to their contribution to classification. However, ambiguity, uncertainty, and missing labels coexist in real-life multilabel data, which brings adverse effects to multilabel feature selection. The multi-scale fuzzy rough set gives an effective way to mine intrinsic knowledge hidden in uncertain data. This paper first extends the multi-scale learning to multilabel data with missing labels and proposes a feature selection method for multilabel classification with missing labels via multi-scale fusion fuzzy uncertainty measures called FSMML. The missing label space construction and feature evaluation metric are carefully investigated in the framework of multi-scale learning. A multilabel multi-scale learning strategy is formalized with the fuzzy granularity cognitive mechanism as the core, and the multi-scale fusion fuzzy label learning is given to reconstruct the missing label space. Then, a novel multilabel multi-scale fuzzy rough sets with missing labels is developed, and the significance of each scale is quantified. Moreover, some multi-scale fusion fuzzy uncertainty measures are defined by capturing the sample fuzzy similarity in the feature and reconstructed label spaces. Accordingly, the relevance between features and label set and the interactivity and redundancy between features in feature evaluation are discussed. Finally, FSMML chooses high-quality features to maximize relevance and interactivity and minimize redundancy. Extensive experiments demonstrate the effectiveness of FSMML on fifteen datasets with missing labels.}
}
@article{TANG2024110604,
title = {Joint-individual fusion structure with fusion attention module for multi-modal skin cancer classification},
journal = {Pattern Recognition},
volume = {154},
pages = {110604},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110604},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003558},
author = {Peng Tang and Xintong Yan and Yang Nan and Xiaobin Hu and Bjoern H. Menze and Sebastian Krammer and Tobias Lasser},
keywords = {Skin cancer classification, Joint-individual fusion structure, Multi-modal fusion attention, Dermatological image and metadata},
abstract = {Many convolutional neural network (CNN) based approaches for skin cancer classification primarily rely on dermatological images, yielding commendable results in classification accuracy. However, leveraging patient metadata, a crucial source of clinical information for dermatologists, can further enhance accuracy. Current methodologies predominantly employ basic joint fusion structures (FS) and fusion modules (FMs) for multi-modal classification, leaving room for advancement in enhancing accuracy through exploration of more sophisticated FS and FM architectures. Thus, this paper introduces a novel fusion method that integrates dermatological images (dermoscopy images or clinical images) with patient metadata for skin cancer classification, focusing on enhancing FS and FM components. Initially, we propose a joint-individual fusion (JIF) structure that simultaneously learns shared features across multi-modality data while preserving specific characteristics. Subsequently, we introduce a multi-modal fusion attention (MMFA) module designed to amplify the most relevant image and metadata features through a combination of self and mutual attention mechanisms, thereby bolstering the decision-making pipeline. Our study compares the efficacy of the proposed JIF-MMFA method with other state-of-the-art fusion techniques across three distinct public datasets. Results demonstrate that the JIF-MMFA method consistently enhances classification outcomes across various CNN backbones, outperforming alternative fusion methodologies on all three datasets. These findings underscore the effectiveness and robustness of our proposed approach in skin cancer classification.}
}
@article{DIAO2024110564,
title = {Understanding the vulnerability of skeleton-based Human Activity Recognition via black-box attack},
journal = {Pattern Recognition},
volume = {153},
pages = {110564},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110564},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003157},
author = {Yunfeng Diao and He Wang and Tianjia Shao and Yongliang Yang and Kun Zhou and David Hogg and Meng Wang},
keywords = {Black-box attack, Skeletal action recognition, Adversarial robustness, On-manifold adversarial samples},
abstract = {Human Activity Recognition (HAR) has been employed in a wide range of applications, e.g. self-driving cars, where safety and lives are at stake. Recently, the robustness of skeleton-based HAR methods have been questioned due to their vulnerability to adversarial attacks. However, the proposed attacks require the full-knowledge of the attacked classifier, which is overly restrictive. In this paper, we show such threats indeed exist, even when the attacker only has access to the input/output of the model. To this end, we propose the very first black-box adversarial attack approach in skeleton-based HAR called BASAR. BASAR explores the interplay between the classification boundary and the natural motion manifold. To our best knowledge, this is the first time data manifold is introduced in adversarial attacks on time series. Via BASAR, we find on-manifold adversarial samples are extremely deceitful and rather common in skeletal motions, in contrast to the common belief that adversarial samples only exist off-manifold. Through exhaustive evaluation, we show that BASAR can deliver successful attacks across classifiers, datasets, and attack modes. By attack, BASAR helps identify the potential causes of the model vulnerability and provides insights on possible improvements. Finally, to mitigate the newly identified threat, we propose a new adversarial training approach by leveraging the sophisticated distributions of on/off-manifold adversarial samples, called mixed manifold-based adversarial training (MMAT). MMAT can successfully help defend against adversarial attacks without compromising classification accuracy.}
}
@article{WANG2024110592,
title = {Joint learning of latent subspace and structured graph for multi-view clustering},
journal = {Pattern Recognition},
volume = {154},
pages = {110592},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110592},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003431},
author = {Yinuo Wang and Yu Guo and Zheng Wang and Fei Wang},
keywords = {Multi-view clustering, Latent space, Subspace clustering, Graph learning},
abstract = {Most existing multi-view clustering methods rely solely on subspace clustering or graph-based clustering. Subspace clustering reduces the redundant information in high dimensional data, but it neglects the intrinsic structural dependencies among samples. Graph-based clustering can model the similarity among samples but tends to suffer from redundant information. In this paper, a novel framework jointing subspace learning and structured graph learning for multi-view clustering (SSMC) is proposed, which benefits from the merits of both subspace learning and structured graph learning. SSMC utilizes graph regularized subspace learning to obtain low dimensional consensus features, where the embedded features are ensured to have maximized correlation to reduce the redundant information, and the graph regularization forces embedded features to preserve their sample similarities. Meanwhile, an adaptive structured graph is learned based on the consensus features in the embedded feature space, avoiding the curse of dimensionality in the graph learning procedure. A rank constraint forces the learned graph to have exactly the same number of connected components as the number of clusters, to obtain a more reliable structured graph. Moreover, an effective algorithm is proposed to optimize the SSMC, where the graph regularized subspace learning part and the structured graph learning part are jointly optimized in a mutual reinforcement manner. The experimental results on real-world benchmark datasets show that the SSMC outperforms the state-of-the-arts in multi-view clustering tasks.}
}
@article{PILAVCI2024110628,
title = {Graph domain adaptation with localized graph signal representations},
journal = {Pattern Recognition},
volume = {155},
pages = {110628},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110628},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003790},
author = {Yusuf Yiğit Pilavcı and Eylem Tuğçe Güneyi and Cemil Cengiz and Elif Vural},
keywords = {Domain adaptation, Spectral graph theory, Graph signal processing, Spectral graph wavelets, Graph Laplacian},
abstract = {In this paper we propose a domain adaptation algorithm designed for graph domains. Given a source graph with many labeled nodes and a target graph with few or no labeled nodes, we aim to estimate the target labels by making use of the similarity between the characteristics of the variation of the label functions on the two graphs. Our assumption about the source and the target domains is that the local behavior of the label function, such as its spread and speed of variation on the graph, bears resemblance between the two graphs. We estimate the unknown target labels by solving an optimization problem where the label information is transferred from the source graph to the target graph based on the prior that the projections of the label functions onto localized graph bases be similar between the source and the target graphs. In order to efficiently capture the local variation of the label functions on the graphs, spectral graph wavelets are used as the graph bases. Experimentation on various data sets shows that the proposed method yields quite satisfactory classification accuracy compared to reference domain adaptation methods.}
}
@article{WANG2024110576,
title = {OA-Pose: Occlusion-aware monocular 6-DoF object pose estimation under geometry alignment for robot manipulation},
journal = {Pattern Recognition},
volume = {154},
pages = {110576},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110576},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003273},
author = {Jikun Wang and Luqing Luo and Weixiang Liang and Zhi-Xin Yang},
keywords = {Object pose estimation, Occlusion scene, Deep learning, Dense correspondence, Robot manipulation},
abstract = {Object pose estimation is the fundamental technology of robot manipulation systems. Recently, various learning-based monocular pose estimation methods have achieved outstanding performance by establishing sparse/dense 2D–3D correspondences. However, in cluttered environments, occlusion has been a challenging problem for pose estimation due to limited information provided by visible parts. In this work, we propose an efficient occlusion-aware monocular pose estimation method, called OA-Pose, to learn geometric feature information of occluded objects from cluttered scenes. Our framework takes RGB images as input and generates 2D–3D correspondences of visible and invisible parts based on the proposed Occlusion-aware Geometry Alignment Module. Extensive experiments show that our method is superior and competitive with state-of-the-art on multiple public datasets. We also conduct grasping experiments with different degrees of object occlusion, demonstrating the usability of our algorithm to deploy on robots in unstructured environments.}
}
@article{JU2024110567,
title = {Focus on informative graphs! Semi-supervised active learning for graph-level classification},
journal = {Pattern Recognition},
volume = {153},
pages = {110567},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110567},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003182},
author = {Wei Ju and Zhengyang Mao and Ziyue Qiao and Yifang Qin and Siyu Yi and Zhiping Xiao and Xiao Luo and Yanjie Fu and Ming Zhang},
keywords = {Graph classification, Graph neural networks, Semi-supervised learning, Active learning},
abstract = {Graph-level classification is a critical problem in social analysis and bioinformatics. Since annotated labels are typically costly, we intend to study this challenging task in semi-supervised scenarios with limited budgets. Inspired by the fact that active learning is capable of interactively querying an oracle to annotate a small number of informative examples in the unlabeled dataset, we develop a novel Semi-supervised active learning framework termed GraphSpa for graph-level classification. To make the most of labeling budgets, we propose an effective unlabeled data selection strategy that takes both local similarity and global semantic structure into account. Specifically, we first construct an adaptive queue with labeled samples and select informative samples that have a low degree of similarity to the queue using the Min-Max principle from the local view. Further, we introduce class prototypes and select samples with a large predictive loss discrepancy from the global view. To harness the full potential of unlabeled data, we develop a semi-supervised active learning framework on the basis of our fusion selection strategy coupled with graph contrastive learning during active learning. The effectiveness of our GraphSpa is validated against state-of-the-art methods through experimental results on diverse real-world benchmark datasets.}
}
@article{HUA2024110541,
title = {Re-decoupling the classification branch in object detectors for few-class scenes},
journal = {Pattern Recognition},
volume = {153},
pages = {110541},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110541},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002929},
author = {Jie Hua and Zhongyuan Wang and Qin Zou and Jinsheng Xiao and Xin Tian and Yufei Zhang},
keywords = {Feature degradation, Re-decoupling, Few-class scenes, Object detection, Mutual exclusion constraint},
abstract = {Few-class object detection is a critical task in numerous scenes, such as autonomous driving and intelligent surveillance. The current researches mainly focus on the correlation or decoupling between classification and regression subtasks in object detection. However, they rarely take advantage of the potential of re-decoupling the classification subtask. In this paper, we propose to re-decouple the classification branch in object detection for few-class scenes by reducing multi-classification features to multiple binary-classification features. Since multi-classification losses cannot supervise the network to learn decoupled binary-classification features, we introduce a single-class loss to supervise decoupled multiple binary-classification branches. In particular, we propose a basic feature degradation head (FD-Head) structure that decouples the classification branches and applies binary-classification loss to encourage each branch to learn only the degraded single-class features. In addition, based on the mutual exclusion between classes, we propose a mutual exclusion constraint (FD-Head-M) module to constrain the scores of all classes, promoting the detector performance. Finally, we replace the original convolution with more powerful feature extraction modules to form the enhanced FD-Head (FD-Head-E). Notably, our method can be used as a universal module and embedded into the existing object detectors to boost their performance. When applying our method to typical object detectors, it experimentally achieves performance gains of 1.2–2.2%, 1.7–2.5% on the KITTI-3, SeaShips datasets respectively. When using ResNet50 as the backbone network, our method gains an accuracy of 45.9% on the MS COCO dataset.}
}
@article{CHEN2024110622,
title = {Improving CNN-based semantic segmentation on structurally similar data using contrastive graph convolutional networks},
journal = {Pattern Recognition},
volume = {155},
pages = {110622},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110622},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400373X},
author = {Ling Chen and Zedong Tang and Hao Li},
keywords = {Semantic segmentation, Structural similarity, Graph network, Contrastive learning},
abstract = {Structurally similar data exist in most practical semantic segmentation applications. For example, objects can appear identical or positionally similar in many images, such as video frames. Objects with structural similarity in data samples can confuse deep neural networks (DNNs) in semantic segmentation applications. These challenges often lead to lower pixel classification accuracy of natural object segmentation. This study proposes a novel approach (S2-GCN) that enhances CNN-based semantic segmentation for structurally similar data using a contrastive graph convolutional network (GCN). By selecting specific label pairs and developing a customized GCN branch parallel to an encoder-decoder backbone, our method significantly improves accuracy, IoU, and F1-score, by up to 8 %, as demonstrated through an extensive evaluation of five datasets. Our findings show that the proposed method effectively addresses the structural similarity problem of CNN-based semantic segmentation and can be applied to a wide range of practical applications.}
}
@article{MOAYED2024110530,
title = {Deep and wide nonnegative matrix factorization with embedded regularization},
journal = {Pattern Recognition},
volume = {153},
pages = {110530},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110530},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002814},
author = {Hojjat Moayed and Eghbal G. Mansoori},
keywords = {Feature extraction, Deep learning, Nonnegative matrix factorization, Channel augmentation, Regularization},
abstract = {End-to-end learning is an advanced framework in deep learning. It combines feature extraction, followed by pattern recognition (classification, clustering, etc.) in a unified learning structure. However, these deep networks face several challenges such as overfitting, vanishing gradient, computational complexity, information loss in layers, and weak robustness to noisy data/features. To address these challenges, this paper presents Deep and Wide Nonnegative Matrix Factorization (DWNMF) with embedded regularization for the feature extraction stage of the end-to-end models. DWNMF aims to identify more robust features while preventing overfitting via embedding regularization. For this purpose, DWNMF integrates input data with its noisy versions as diverse augmented channels. Then, the features in all channels are extracted in parallel using distinct network branches. The parameters of this model learn the intrinsic hierarchical features in the channels of complex data objects. Finally, the extracted features in different channels are aggregated in a single feature space to perform the classification task. To embed regularization in the DWNMF model, some NMF neurons in the layers are substituted by random neurons to increase the stability and robustness of the extracted features. Experimental results confirm that the DWNMF model extracts more robust features, prevents overfitting, and achieves better classification accuracy compared to state-of-the-art methods.}
}
@article{ZHANG2024110559,
title = {Ta-Adapter: Enhancing few-shot CLIP with task-aware encoders},
journal = {Pattern Recognition},
volume = {153},
pages = {110559},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110559},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003108},
author = {Wenbo Zhang and Yifan Zhang and Yuyang Deng and Wenlong Zhang and Jianfeng Lin and Binqiang Huang and Jinlu Zhang and Wenhao Yu},
keywords = {Contrastive language-image pre-training, Adapter, Prompt learning, Transfer learning},
abstract = {Contrastive Language-Image Pre-training (CLIP) has shown impressive zero-shot transfer capabilities, but its potential for specific downstream tasks is not fully utilized. To further enhance CLIP’s few-shot capability for specific datasets, some subsequent works have been proposed, such as methods based on lightweight adapters and prompt learning. However, since CLIP is pretrained on a diverse collection of image and text pairs sourced from the internet, it is difficult to sufficiently tune models to specific datasets using only lightweight adaptions. In this paper, we argue that largely modifying the internal representations within CLIP’s encoders can yield better results on downstream datasets. In this work, we introduce Ta-Adapter, a method that equips both the visual and textual encoders of CLIP with task-specific prompts. These prompts are generated using a collaborative prompt learning approach, which allows the encoders to produce representations that are better aligned with specific downstream datasets. Then, we initialize an adapter module using the optimized features generated by the task-aware visual encoder for further feature alignment, and this module can also be further fine-tuned. Our extensive experiments on image classification datasets show that compared to the state-of-the-art few-shot methods Tip-Adapter-F and MaPLe, our model exhibits good performance and obtains an average absolute gain of 2.04% and 1.62% on 11 different image recognition datasets, respectively. In conclusion, this work presents a unique and effective approach to unlocking the full potential of CLIP’s few-shot learning capabilities.}
}
@article{LI2024110647,
title = {Multi-granularity relationship reasoning network for high-fidelity 3D shape reconstruction},
journal = {Pattern Recognition},
volume = {155},
pages = {110647},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110647},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003984},
author = {Lei Li and Zhiyuan Zhou and Suping Wu and Pan Li and Boyang Zhang},
keywords = {3D reconstruction, Multi-granularity, Cycle loss, High-fidelity},
abstract = {Monocular image-based 3D reconstruction is widely used in virtual reality, augmented reality, and autonomous driving, which benefits from the rapid development of deep learning approaches. Most of the available methods focused on reconstructing the overall shape of the object while ignoring some fine-grained details. Moreover, these methods make it hard to exactly reconstruct complex topological structures. In this paper, we propose a multi-granularity relationship reasoning network (MGRRNet), which aims to recover 3D shapes with high fidelity and rich details via the relationship reasoning between different granularity information. Specifically, our model captures the discriminative and detailed features at different granularities for extracting attentional regions. Then we perform the relationship reasoning between different granularities to reinforce the multi-granularity consistency and inter-granularity correlation. By doing this, our network is able to achieve robust feature representation and fine reconstruction. During the learning process, we jointly optimize procedures of different granularity feature representations via a sequence of inter-granularity cycle loss iterations. Extensive experimental results on two publicly available datasets justify that our approach achieves competitive performance compared to the state-of-the-art methods. Codes and all resources will be publicly available at https://github.com/Ray-tju/MGRRNet.}
}
@article{LI2024110595,
title = {Influence maximization for heterogeneous networks based on self-supervised clustered heterogeneous graph transformer},
journal = {Pattern Recognition},
volume = {154},
pages = {110595},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110595},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003467},
author = {Ying Li and Linlin Li and Xiangyu Liu and Yijun Liu and Qianqian Li},
keywords = {Influence maximization, Heterogeneous network, Clustering information, Heterogeneous graph transformer},
abstract = {Influence maximization (IM) has drawn significant attention in recent years. Most existing IM methods primarily focus on homogeneous networks, and do not take into account the heterogeneity and the attributes of different types of nodes in heterogeneous networks. However, heterogeneous networks are ubiquitous in real world, encompassing rich semantics and complex structural information. Additionally, the clustering characteristics inherent in a network have a critical and substantial impact on the process of information diffusion, which is often overlooked in IM models designed for heterogeneous networks. To address the challenges posed by the heterogeneity and clustering structure in heterogeneous networks, we propose a novel deep learning framework based on a self-supervised clustered heterogeneous graph transformer for IM in heterogeneous networks, which we have named SCHGT-IM. SCHGT-IM aggregates the heterogeneity and clustering information in heterogeneous networks and incorporates a clustered cascade (CC) model as an information diffusion model to enhance the realism of simulations. We evaluate the performance of SCHGT-IM in comparison with that of state-of-the-art IM models using three academic heterogeneous networks extracted from the DBLP dataset. The experimental results on influence spread demonstrate that SCHGT-IM is superior to fourteen state-of-the-art algorithms and is highly effective in selecting influential seed nodes of different types from heterogeneous networks.}
}
@article{BATTISTELLA2024110623,
title = {GHOST: Graph-based higher-order similarity transformation for classification},
journal = {Pattern Recognition},
volume = {155},
pages = {110623},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110623},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003741},
author = {Enzo Battistella and Maria Vakalopoulou and Nikos Paragios and Éric Deutsch},
keywords = {Feature selection, Distance learning, Higher-order},
abstract = {Exploring and identifying a good feature representation to describe high-dimensional datasets is a challenge of prime importance. However, plenty of feature selection techniques and distance metrics exist, which entails an intricacy for identifying the one best suited to the task. This paper provides an algorithm to design high-order distance metrics over a sparse selection of features dedicated to classification. Our approach is based on Conditional Random Field (CRF) energy minimization and Dual Decomposition, which allow efficiency and great flexibility in the considered features. The optimization technique ensures the tractability of high-dimensionality problems using hundreds of features and samples. Our approach is evaluated on synthetic data as well as on Covid-19 patient stratification. Comparisons with state-of-the-art baselines and our proposed method on different classification results prove the learned metric’s relevance.}
}
@article{YVINEC2024110571,
title = {PIPE: Parallelized inference through ensembling of residual quantization expansions},
journal = {Pattern Recognition},
volume = {154},
pages = {110571},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110571},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003224},
author = {Edouard Yvinec and Arnaud Dapogny and Kevin Bailly},
keywords = {Quantization, Deep learning, LLM, Ensemble, Efficient inference},
abstract = {Deep neural networks (DNNs) are ubiquitous in computer vision and natural language processing, but suffer from high inference cost. This problem can be addressed by quantization, which consists in converting floating point operations into a lower bit-width format. With the growing concerns on privacy rights, we focus our efforts on data-free methods. However, such techniques suffer from their lack of adaptability to the target devices, as a hardware typically only support specific bit widths. Thus, to adapt to a variety of devices, a quantization method shall be flexible enough to find good accuracy v.s. speed trade-offs for every bit width and target device. To achieve this, we propose PIPE, a quantization method that leverages residual error expansion, along with group sparsity and an ensemble approximation for better parallelization. PIPEis backed off by strong theoretical guarantees and achieves superior performance on every benchmarked application (from vision to NLP tasks), architecture (ConvNets, transformers) and bit-width (from int8 to ternary quantization).}
}
@article{JIA2024110583,
title = {Discriminative label correlation based robust structure learning for multi-label feature selection},
journal = {Pattern Recognition},
volume = {154},
pages = {110583},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110583},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003340},
author = {Qingwei Jia and Tingquan Deng and Yan Wang and Changzhong Wang},
keywords = {Multi-label learning, Feature selection, Feature redundancy, Discriminative label correlation},
abstract = {Feature selection is a key technique to tackle the curse of dimensionality in multi-label learning. Lots of embedded multi-label feature selection methods have been developed. However, they face challenges in identifying and excluding redundant features. To address these issues, this paper proposes a multi-label feature selection method that combines robust structural learning and discriminative label regularization. The proposed method starts from the feature space rather than data space, motivated by the principle that redundant features have high similarity or strong correlation. To exclude redundant features, a regularization on the feature selection matrix is designed by combining ℓ2,1-norm penalty with inner products of feature weight vectors. This regularization can help to learn a robust structure in the feature selection matrix. Meanwhile, both of the similarity and dissimilarity of labels of instances are involved in exploring discriminative label correlations. Extensive experiments verified the effectiveness of the proposed model for feature selection.}
}
@article{CHEN2024110607,
title = {Adaptive propagation deep graph neural networks},
journal = {Pattern Recognition},
volume = {154},
pages = {110607},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110607},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003583},
author = {Wei Chen and Wenxu Yan and Wenyuan Wang},
keywords = {Graph neural network, Adaptive propagation combinations, Subjective and objective information, Aggregation weights, Computational costs},
abstract = {Graph neural networks (GNNs) with adaptive propagation combinations represent a specialized deep learning paradigm, engineered to capture complex nodal interconnections within graph data. The primary challenge of this model lies in distilling and representing features extracted over varying nodal distances. This paper delves into an array of adaptive propagation strategies, with a focus on the influence of nodal distances and information aggregation on model efficacy. Our investigation identifies a critical performance drop in scenarios featuring overly brief propagation paths or an insufficient number of layers. Addressing this, we propose an innovative adaptive propagation technique in deep graph neural networks, named AP-DGNN, aimed at reconstructing high-order graph convolutional neural networks (GCNs). The AP-DGNN model assigns unique aggregation combination weights to each node and category, culminating in a final model representation through a process of weighted aggregation. Notably, these weights are capable of assimilating both subjective and objective information characteristics within the network. To substantiate our model’s effectiveness and scalability, we employed often-used benchmark datasets for experimental validation. A notable aspect of our AP-DGNN model is its minimal training parameter requirement and reduced computational demand. Furthermore, we demonstrate the model’s enhanced performance, which remains consistent across various hyperparameter configurations. This aspect was rigorously tested under diverse hyperparameter settings. Our findings contribute significantly to the evolution of graph neural networks, potentially revolutionizing their application across multiple domains. The research presented herein not only advances the understanding of GNNs but also paves the way for their robust application in varied scenarios. Codes are available at https://github.com/CW112/AP_DGNN.}
}
@article{CAO2024110555,
title = {CAST: Cross-Modal Retrieval and Visual Conditioning for image captioning},
journal = {Pattern Recognition},
volume = {153},
pages = {110555},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110555},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003066},
author = {Shan Cao and Gaoyun An and Yigang Cen and Zhaoqilin Yang and Weisi Lin},
keywords = {Image captioning, Image–text retriever, Image & memory comprehender, Dual attention decoder},
abstract = {Image captioning requires not only accurate recognition of objects and corresponding relationships, but also full comprehension of the scene information. However, existing models suffer from partial understanding and object hallucination. In this paper, a Cross-modal retrievAl and viSual condiTioning model (CAST) is proposed to address the above issues for image captioning with three key modules: an image–text retriever, an image & memory comprehender and a dual attention decoder. Aiming at a comprehensive understanding, we propose to exploit cross-modal retrieval to mimic human cognition, i.e., to trigger retrieval of contextual information (called episodic memory) about a specific event. Specifically, the image–text retriever searches the top n relevant sentences which serve as episodic memory for each input image. Then the image & memory comprehender encodes an input image and enriches episodic memory by self-attention and relevance attention respectively, which can encourage CAST to comprehend the scene thoroughly and support decoding more effectively. Finally, such image representation and memory are integrated into our dual attention decoder, which performs visual conditioning by re-weighting image and text features to alleviate object hallucination. Extensive experiments are conducted on MS COCO and Flickr30k datasets, which demonstrate that our CAST achieves state-of-the-art performance. Our model also has a promising performance even in low-resource scenarios (i.e. 0.1%, 0.5% and 1% of MS COCO training set).}
}
@article{RIBAS2024110566,
title = {Learning a complex network representation for shape classification},
journal = {Pattern Recognition},
volume = {154},
pages = {110566},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110566},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003170},
author = {Lucas C. Ribas and Odemir M. Bruno},
keywords = {Shape representation, Complex network, Neural network},
abstract = {Shape contour is a key low-level characteristic, making shape description an important aspect in many computer vision problems, with several challenges such as variations in scale, rotation, and noise. In this paper, we introduce an approach for shape analysis and classification from binary images based on representations learned by applying Randomized Neural Networks (RNNs) on feature maps derived from a Complex Network (CN) framework. Our approach models the contour in a complex network and computes their topological measures using a dynamic evolution strategy. This evolution of the CN provides significant information into the physical aspects of the shape’s contour. Therefore, we propose embedding the topological measures computed from the dynamics of the CN evolution into a matrix representation, which we have named the Topological Feature Map (TFM). Then, we employ the RNN to learn representations from the TFM through a sliding window strategy. The proposed representation is formed by the learned weights between the hidden and output layers of the RNN. Our experimental results show performance improvements in shape classification using the proposed method across two generic shape datasets. We also applied our approach to the recognition of plant leaves, achieving high performance in this challenging task. Furthermore, the proposed approach has demonstrated robustness to noise and invariance to transformations in scale and orientation of the shapes.}
}
@article{TABERNIK2024110540,
title = {Dense center-direction regression for object counting and localization with point supervision},
journal = {Pattern Recognition},
volume = {153},
pages = {110540},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110540},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002917},
author = {Domen Tabernik and Jon Muhovič and Danijel Skočaj},
keywords = {Point-supervision, Object counting, Object localization, Center-point prediction, Center-direction regression, CeDiRNet},
abstract = {Object counting and localization problems are commonly addressed with point-supervised learning, which allows the use of less labor-intensive point annotations. However, learning based on point annotations poses challenges due to the high imbalance between the sets of annotated and unannotated pixels, which is often treated with Gaussian smoothing of point annotations and focal loss. However, these approaches still focus on the pixels in the immediate vicinity of the point annotations and exploit the rest of the data only indirectly. In this work, we propose a novel approach termed CeDiRNet for point-supervised learning that uses a dense regression of directions pointing towards the nearest object centers, i.e. center-directions. This provides greater support for each center point arising from many surrounding pixels pointing towards the object center. We propose a formulation of center-directions that allows the problem to be split into the domain-specific dense regression of center-directions and the final localization task based on a small, lightweight, and domain-agnostic localization network that can be trained with synthetic data completely independent of the target domain. We demonstrate the performance of the proposed method on six different datasets for object counting and localization and show that it outperforms the existing state-of-the-art methods. The code is accessible on GitHub at https://github.com/vicoslab/CeDiRNet.git.}
}
@article{XIE2024110605,
title = {Non-convex tensorial multi-view clustering by integrating ℓ1-based sliced-Laplacian regularization and ℓ2,p-sparsity},
journal = {Pattern Recognition},
volume = {154},
pages = {110605},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110605},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400356X},
author = {Deyan Xie and Ming Yang and Quanxue Gao and Wei Song},
keywords = {Multi-view subspace clustering, t-SVD},
abstract = {Consider the recent upswing in interest around multi-view clustering procedures. Such methods aim to boost clustering efficiency by leveraging information from numerous perspectives. Much research has been devoted to tensorial representation to exploit high-order correlations underlying disparate views while preserving the local geometric structure inside each view. Our research introduces a novel multi-view clustering approach. This approach creates a 3rd-order tensor, assimilating features from all perspectives. We use the t-product in the tensor space to generate the self-representation tensor from the tensorial data. We incorporate the ℓ1-based sliced-Laplacian regularization to increase our model’s resilience and introduce a fresh column-wise sparse norm: the ℓ2,p-norm with 0<p<1. This norm displays attributes of invariance, continuity, and differentiability. We present a closed-form answer to the ℓ2,p-regularized shrinkage problem, broadening its relevance to other generalized problems. Simultaneously, we propose a tensorial arctan-function as an improved surrogate for the tensor rank. This function has proven more proficient at assessing consistency across multiple viewpoints. By integrating these two components, we formulate an effective algorithm that refines our suggested model, ensuring that the constructed sequence gravitates toward the stationary KKT point. Our team conducts extensive experiments on various datasets to evaluate our model’s effectiveness, spanning diverse situations and scales. Results from these experiments emphasize that our approach establishes a novel performance standard.}
}
@article{GHOSH2024110634,
title = {Linear Centroid Encoder for Supervised Principal Component Analysis},
journal = {Pattern Recognition},
volume = {155},
pages = {110634},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110634},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003856},
author = {Tomojit Ghosh and Michael Kirby},
keywords = {Supervised Linear Centroid-Encoder, Centroid-Encoder, Principal component analysis (PCA), Supervised PCA, Linear dimensionality reduction, Supervised dimensionality reduction},
abstract = {We propose a new supervised dimensionality reduction technique called Supervised Linear Centroid-Encoder (SLCE), a linear counterpart of the nonlinear Centroid-Encoder (CE) (Ghosh and Kirby, 2022). SLCE works by mapping the samples of a class to its class centroid using a linear transformation. The transformation is a projection that reconstructs a point such that its distance from the corresponding class centroid, i.e., centroid-reconstruction loss, is minimized in the ambient space. We derive a closed-form solution using an eigendecomposition of a symmetric matrix. We did a detailed analysis and presented some crucial mathematical properties of the proposed approach. We establish a connection between the eigenvalues and the centroid-reconstruction loss. In contrast to Principal Component Analysis (PCA) which reconstructs a sample in the ambient space, the transformation of SLCE uses the instances of a class to rebuild the corresponding class centroid. Therefore the proposed method can be considered a form of supervised PCA. Experimental results show the performance advantage of SLCE over other supervised methods.}
}
@article{ZHANG2024110582,
title = {CRTrack: Learning Correlation-Refine network for visual object tracking},
journal = {Pattern Recognition},
volume = {154},
pages = {110582},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110582},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003339},
author = {Wenkang Zhang and Fei Xie and Tianyang Xu and Jiang Zhai and Wankou Yang},
keywords = {Visual tracking, Cross-correlation, Channel & spatial refine, Siamese network},
abstract = {Conducting reliable feature interaction plays a critical role in the visual tracking community, especially in recent dominated Siamese-based tracking paradigm. In general, there are two primary approaches for fusing representations from template and search area in the Siamese setting, i.e., cross-correlation and transformer modeling. The former provides a straightforward interaction solution, which may have limitations in handling complex scenarios, such as appearance variations and occlusion. While the latter offers an effective interaction mechanism, albeit with higher computation complexity and model cost. In contrast to traditional Siamese-based trackers which rely on two mentioned feature cross-correlation operators, this paper proposes a novel Correlation-Refine network to address the issue of lacking semantic information caused by local linear matching in correlation, from both spatial and channel perspectives. Correlation-Refine network (named CR) is solely built on top of fully convolutional layers, without employing intricate transformer mechanisms or complex methods to fuse features from multiple scales. Moreover, we present a concise yet effective convolutional tracking framework based on the correlation-refine network. CR network can increase the discriminative ability of semantic information in a coarse-to-fine manner: it gradually learns the semantic features of the target to be tracked and suppresses interference from similar objects by stacking multiple CR layers. Extensive experiments and comparisons with recent competitive trackers in challenging large-scale benchmarks demonstrate that, our tracker outperforms all previous convolutional trackers and has competitive results with transformer-based method. The code will be made available.}
}
@article{JIANG2024110537,
title = {Self-attention empowered graph convolutional network for structure learning and node embedding},
journal = {Pattern Recognition},
volume = {153},
pages = {110537},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110537},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002887},
author = {Mengying Jiang and Guizhong Liu and Yuanchao Su and Xinliang Wu},
keywords = {Representation learning, Heterophily, Structure learning, Graph neural networks},
abstract = {In representation learning on graph-structured data, many popular graph neural networks (GNNs) fail to capture long-range dependencies, leading to performance degradation. Furthermore, this weakness is magnified when the concerned graph is characterized by heterophily (low homophily). To solve this issue, this paper proposes a novel graph representation learning framework called the graph convolutional network with self-attention (GCN-SA). The proposed scheme exhibits an exceptional generalization capability in node-level representation learning. The proposed GCN-SA contains two enhancements corresponding to edges and node features. For edges, we utilize a self-attention mechanism to design a stable and effective graph-structure-learning module that can capture the internal correlation between any pair of nodes. This graph-structure-learning module can identify reliable neighbors for each node from the entire graph. Regarding the node features, we modify the transformer block to enable it to assist the GCN in integrating valuable information from the entire graph. These two enhancements work in distinct ways to help our GCN-SA capture long-range dependencies, enabling it to perform representation learning on graphs with varying levels of homophily. The experimental results on real-world benchmark datasets demonstrate the effectiveness of the proposed GCN-SA. Compared to other outstanding GNN counterparts, the proposed GCN-SA is competitive. The source code is available at https://github.com/mengyingjiang/GCN-SA.}
}
@article{WANG2024110570,
title = {OpenInst: A simple query-based method for open-world instance segmentation},
journal = {Pattern Recognition},
volume = {153},
pages = {110570},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110570},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003212},
author = {Cheng Wang and Guoli Wang and Qian Zhang and Peng Guo and Wenyu Liu and Xinggang Wang},
keywords = {Open-world instance segmentation, Object localization network, Query-based detector},
abstract = {Open-world instance segmentation has recently gained significant popularity due to its importance in many real-world applications, such as autonomous driving, robot perception, and remote sensing. However, previous methods have either produced unsatisfactory results or relied on complex systems and paradigms. We wonder if there is a simple way to obtain state-of-the-art results. Fortunately, we have identified two observations that help us achieve the best of both worlds: (1) query-based methods demonstrate superiority over dense proposal-based methods in open-world instance segmentation, and (2) learning localization cues is sufficient for open-world instance segmentation. Based on these observations, we propose a simple query-based method named OpenInst for open-world instance segmentation. OpenInst leverages advanced query-based methods like QueryInst and focuses on learning localization cues. Notably, OpenInst is an extremely simple and straightforward framework without any auxiliary modules or post-processing, yet achieves state-of-the-art results on multiple benchmarks. Specifically, in the COCO→UVO scenario, OpenInst achieves a mask Average Recall (AR) of 53.3, outperforming the previous best methods by 2.0 AR with a simpler structure. We hope that OpenInst can serve as a solid baseline for future research in this area. The source codes are available at https://github.com/hustvl/OpenInst.}
}
@article{ZHANG2024110618,
title = {Self-ensembling depth completion via density-aware consistency},
journal = {Pattern Recognition},
volume = {154},
pages = {110618},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110618},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003698},
author = {Xuanmeng Zhang and Zhedong Zheng and Minyue Jiang and Xiaoqing Ye},
keywords = {Depth completion, Semi-supervised learning, Density-aware consistency, Uncertainty estimation},
abstract = {Depth completion can predict a dense depth map by taking a sparse depth map and the aligned RGB image as input, but the acquisition of ground truth annotations is labor-intensive and non-scalable. Therefore, we resort to semi-supervised learning, where we only need to annotate a few images and leverage massive unlabeled data without ground truth labels to facilitate model learning. In this paper, we propose SEED, a SElf-Ensembling Depth completion framework to enhance the generalization of the model on unlabeled data. Specifically, SEED contains a pair of the teacher and student models, which are given high-density and low-density sparse depth maps as input respectively. The main idea underpinning SEED is to enforce the density-aware consistency by encouraging consistent prediction across different-density input depth maps. One empirical challenge is that the pseudo-depth labels produced by the teacher model inevitably contain wrong depth values, which would mislead the convergence of the student model. To resist the noisy labels, we propose an automatic method to measure the reliability of the generated pseudo-depth labels adaptively. By leveraging the discrepancy of prediction distributions, we model the pixel-wise uncertainty map as the prediction variance and rectify the training process from noisy labels explicitly. To our knowledge, we are among the early semi-supervised attempts on the depth completion task. Extensive experiments on both outdoor and indoor datasets demonstrate that SEED consistently improves the performance of the baseline model by a large margin and even is on par with several fully-supervised methods.}
}
@article{LIU2024110646,
title = {A multi-modal extraction integrated model for neuropsychiatric disorders classification},
journal = {Pattern Recognition},
volume = {155},
pages = {110646},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110646},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003972},
author = {Liangliang Liu and Zhihong Liu and Jing Chang and Xue Xu},
keywords = {Integrated model, Neuropsychiatric disorder, Multi-scale, Interpretability, Classification},
abstract = {Convolutional neural networks (CNNs) provide high-precision automatic classification of neuropsychiatric disorders based on images. However, the “black box” nature leads to poor interpretability of CNN. This study constructs an integrated model for neuropsychiatric disorders classification from multi-modal data. The proposed model consists of a novel multi-scale image features extraction neural network (MSFM) and a XGBoost. The proposed MSFM extracts the pixel context semantic information from fMRI images with different scales, which employs token and channel-mixing strategy to enhance the information communication between context semantic information. XGBoost is used to extract phenotypic feature from phenotypic records. Based on the integration of phenotypic and image features, a comparative interpretable classification of mental disorders can be achieved. The overall accuracy, sensitivity, and recall of the binary classification (healthy controls & neuropsychiatric disorders) of the integrated model are 90.23%, 91.08%, and 89.33%, respectively. The visualization of image features and the phenotypic features present consistency in the brain regions, increasing the interpretability of the MSFM. Especially, through visual statistical analysis of the test set, it was found that there are differences in the distribution of ADHD, BD, and SD in the brain regions. Our solution may provide psychiatrists with ideas for comparative examinations and diagnosis.}
}
@article{SHI2024110594,
title = {Self-supervised feature-gate coupling for dynamic network pruning},
journal = {Pattern Recognition},
volume = {154},
pages = {110594},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110594},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003455},
author = {Mengnan Shi and Chang Liu and Jianbin Jiao and Qixiang Ye},
keywords = {Contrastive self-supervised learning (CSL), Dynamic network pruning (DNP), Feature-gate coupling, Instance neighborhood relationship},
abstract = {Gating modules have been widely explored in dynamic network pruning (DNP) to reduce the run-time computational cost of deep neural networks while keeping the features representative. Despite the substantial progress, existing methods remain ignoring the consistency between feature and gate distributions, which may lead to distortion of gated features. In this paper, we propose a feature-gate coupling (FGC) approach aiming to align distributions of features and gates. FGC is a plug-and-play module that consists of two steps carried out in an iterative self-supervised manner. In the first step, FGC utilizes the k-Nearest Neighbor algorithm in the feature space to explore instance neighborhood relationships, which are treated as self-supervisory signals. In the second step, FGC exploits contrastive self-supervised learning (CSL) to regularize gating modules, leading to the alignment of instance neighborhood relationships within the feature and gate spaces. Experimental results validate that the proposed FGC method improves the baseline approach with significant margins, outperforming state-of-the-art methods with a better accuracy-computation trade-off. Code is publicly available at github.com/smn2010/FGC-PR.}
}
@article{DENG2024110606,
title = {Module-based graph pooling for graph classification},
journal = {Pattern Recognition},
volume = {154},
pages = {110606},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110606},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003571},
author = {Sucheng Deng and Geping Yang and Yiyang Yang and Zhiguo Gong and Can Chen and Xiang Chen and Zhifeng Hao},
keywords = {Graph neural network, Graph classification, Graph pooling},
abstract = {Graph Neural Network (GNN) models are recently proposed to process the graph-structured data for the learning tasks on graphs, e.g., node classification, link prediction, and so on. This work focuses on the graph classification task, aiming to obtain the graph representation and predict the class label for a graph. Existing works proposed applying graph pooling to obtain graph embedding but still suffer from several issues. First, node embeddings are generated according to the topological information of the whole graph, but ignoring the local isomorphic substructures commonly seen in bioinformatics and chemistry. Another limitation arises when aggregating node embeddings. The hard assignment obtained through clustering algorithms, which rely on preset and fixed parameters instead of considering the graph’s properties adaptively, restricts the flexibility in handling graphs of varying scales. To address the above problems, a module-based graph pooling framework (MGPool) is proposed in this work. Inspired by the rules of bioinformatics, MGPool assumes that a graph consists of multiple modules (also known as sub structures), which are identified based on the natural organization of the graph rather than the hard allocation of nodes. Benefiting from the hypothesis, MGPool generates node embeddings from graph-view and module-view, which is capable to capture global graph information and local isomorphic information respectively. Then module-level pooling is used to capture the intra-module information, while the inter-module information in terms of the correlation between modules is obtained through graph-level pooling. Finally, an entropy-based weighting mechanism is proposed to adjust the modules’ weights for the graph aggregation. Experiments conducted on bioinformatics benchmark datasets demonstrate the effectiveness of MGPool by outperforming other state-of-the-art graph pooling methods. For social network datasets, MGPool also provides competitive performance. Moreover, the visualization of module entropy weights is given to reveal the interpretability of the model.}
}
@article{YU2024110554,
title = {A novel non-pretrained deep supervision network for polyp segmentation},
journal = {Pattern Recognition},
volume = {154},
pages = {110554},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110554},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003054},
author = {Zhenni Yu and Li Zhao and Tangfei Liao and Xiaoqin Zhang and Geng Chen and Guobao Xiao},
keywords = {Polyp segmentation, Deep supervision, Attention mechanism, Global context relation},
abstract = {In this paper, we propose a non-pretrained deep supervision network (NPD-Net) for polyp segmentation. Unlike previous deep supervision networks that rely on ground truth (GT) or pre-training with GT to supervise deep features(the prediction maps from decoder), we propose a novel deep supervision strategy that directly utilizes the GT encoder (that encodes GT to get its maps) after initialization to mitigate overfitting and enhance generalization ability without pre-training, in other words, a non-pretrained. This strategy makes up the gap of directly using GT for deep supervision while mitigates the risk of overfitting due to leverage the well-train pre-trained weights on a small polyp datasets. In addition, we introduce a simple and efficient parallel dual attention module (PDA) to enhance the global modeling ability. PDA executes spatial and channel attention in parallel, and adopts implicit positional encoding and transpose operation to reduce computational complexity. Finally, NPD-Net is able to effectively supervise deep features, expand the range of context information acquisition and improve segmentation performance, particularly in terms of generalization ability. Our experimental results on five benchmark datasets demonstrate that NPD-Net outperforms other state-of-the-art methods. The code will be available at https://github.com/guobaoxiao/NPD-Net.}
}
@article{LUO2024110578,
title = {NCART: Neural Classification and Regression Tree for tabular data},
journal = {Pattern Recognition},
volume = {154},
pages = {110578},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110578},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003297},
author = {Jiaqi Luo and Shixin Xu},
keywords = {Tabular data, Neural networks, Interpretability, Classification and Regression Tree},
abstract = {Deep learning models have become popular in the analysis of tabular data, as they address the limitations of decision trees and enable valuable applications like semi-supervised learning, online learning, and transfer learning. However, these deep-learning approaches often encounter a trade-off. On one hand, they can be computationally demanding when dealing with large-scale or high-dimensional datasets. On the other hand, they may lack interpretability and may not be suitable for small-scale datasets. In this study, we propose a novel interpretable neural network called Neural Classification and Regression Tree (NCART) to overcome these challenges. NCART is a modified version of Residual Networks that replaces fully-connected layers with multiple differentiable oblivious decision trees. By integrating decision trees into the architecture, NCART maintains its interpretability while benefiting from the end-to-end capabilities of neural networks. The simplicity of the NCART architecture makes it well-suited for datasets of varying sizes and reduces computational costs compared to state-of-the-art deep learning models. Extensive numerical experiments demonstrate the superior performance of NCART compared to existing deep learning models, establishing it as a strong competitor to tree-based models. The code is available at https://github.com/Luojiaqimath/NCART.}
}
@article{HEIDARI2024110639,
title = {A novel K-means and K-medoids algorithms for clustering non-spherical-shape clusters non-sensitive to outliers},
journal = {Pattern Recognition},
volume = {155},
pages = {110639},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110639},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400390X},
author = {J. Heidari and N. Daneshpour and A. Zangeneh},
keywords = {Initial centers, Number of clusters, Overlap space, Non-spherical},
abstract = {Determination of the optimal number of clusters, the random selection of the initial centers, the non-detection of non-spherical clusters, and the negative impact of outliers are the main challenges of the K-means algorithm. In this paper, to tackle these issues three simple and intelligent algorithms are proposed by changing the structure of the K-means and K-medoids algorithms. The difference between these algorithms is in the selection of the initial centers and the stop condition. A method has been proposed to obtain the overlap space between the clusters. Using this method, a modified K-means algorithm is developed for the clustering of non-spherical data. These algorithms are designed in a way that they are not sensitive to outliers and can identify clusters having non-spherical shapes. The performance of the proposed methods is illustrated by applying the proposed algorithms to the different data sets and by comparing the results of the algorithms with other methods.}
}
@article{YANG2024110641,
title = {Few-shot intent detection with self-supervised pretraining and prototype-aware attention},
journal = {Pattern Recognition},
volume = {155},
pages = {110641},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110641},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003923},
author = {Shun Yang and YaJun Du and Xin Zheng and XianYong Li and XiaoLiang Chen and YanLi Li and ChunZhi Xie},
keywords = {Few-shot learning, Intent detection, Prototype generation, Self-supervised learning, Contrastive learning},
abstract = {Few-shot intent detection is a more challenging application. However, traditional prototypical networks based on averaging often suffer from issues such as missing key information, poor generalization capabilities. In previous work, using three-dimensional convolutional neural networks (3DCNN) to generate prototype representations faces challenges with long-distance dependencies. Furthermore, a pretrained encoder’s performance in a specific domain is often suboptimal because its knowledge of the specific domain is fragmented. Therefore, in this paper, we propose a simple yet effective two-stage learning strategy to address these issues. In the first stage, we propose a self-supervised multi-task pretraining (SMTP) strategy. SMTP utilizes unlabeled data from the current domain to help the pretrained encoder learn the semantic information of the text and implicitly distinguish semantically similar text representations without using any labels. SMTP aims to enhance the representation capability of the pretrained encoder in a specific domain. In the second stage, we propose a prototype-aware attention (PaAT) model to generate prototype representations of the same class. PaAT generates prototype representations by calculating the attention between class texts, which can effectively solve the long-distance dependence problem of 3DCNN. PaAT is a siamese architecture that can simultaneously generate prototype representations and sentence-level representations of unseen data. In addition, to prevent overfitting in few-shot learning, we introduce an unsupervised contrastive regularization term to constrain PaAT. Our method achieves state-of-the-art performance on four public datasets.11Our code is available: https://github.com/YS19999/SMTP-PaAT. .}
}
@article{XIE2024110589,
title = {3D surface segmentation from point clouds via quadric fits based on DBSCAN clustering},
journal = {Pattern Recognition},
volume = {154},
pages = {110589},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110589},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003406},
author = {Tingting Xie and Hui Chen and Wanquan Liu and Rongyu Zhou and Qilin Li},
keywords = {3D point clouds, Surface extraction, DBSCAN, Quadric surface},
abstract = {Extracting surfaces from 3D point clouds is significant in reconstructing and transforming these discrete points into their corresponding models. Scanned point clouds are often accompanied by noise, and the existing methods mainly rely on local feature similarities for surface extractions. Errors in estimating the feature information may lead to incorrect surface detection. In this paper we propose a surface extraction and boundary detection method based on clustering technique. The method can be described in three steps: In the first step, a normal correction is carried out using the information from the neighborhood of points with sharp features. The second step is to cluster the points that meet the coplanar condition of the local quadric surface (LQS). In the third step, surface merger is performed by merging the local surfaces satisfying the merging conditions. Experimental validation is carried out to determine the effectiveness of the proposed method. The experimental results show improved surface extraction accuracy of the proposed method in comparison to RANSAC, RG, LCCP, C2NO and HT methods.}
}
@article{LI2024110625,
title = {Toward a deeper understanding: RetNet viewed through Convolution},
journal = {Pattern Recognition},
volume = {155},
pages = {110625},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110625},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003765},
author = {Chenghao Li and Chaoning Zhang},
keywords = {Convolutional neural network, Vision transformer, RetNet},
abstract = {The success of Vision Transformer (ViT) has been widely reported on a wide range of image recognition tasks. ViT can learn global dependencies superior to CNN, yet CNN’s inherent locality can substitute for expensive training resources. Recently, the outstanding performance of RetNet in the field of language modeling has garnered attention, surpassing that of the Transformer with explicit local modeling, shifting researchers’ focus toward Transformers in the CV field. This paper investigates the effectiveness of RetNet from a CNN perspective and presents a variant of RetNet tailored to the visual domain. Similar to RetNet we improves ViT’s local modeling by applying a weight mask on the original self-attention matrix. A straightforward way to locally adapt the self-attention matrix can be realized by an Element-wise Learnable Mask (ELM), for which our preliminary results show promising results. However, the Element-wise Learnable Mask not only induces a non-trivial additional parameter overhead but also increases the optimization complexity. To this end, this work proposes a novel Gaussian mixture mask (GMM) in which one mask only has two learnable parameters and it can be conveniently used in any ViT variants whose attention mechanism allows the use of masks. Experimental results on multiple small datasets demonstrate that the effectiveness of our proposed Gaussian mask for boosting ViTs for free (almost zero additional parameter or computation cost). Our code is publicly available at https://github.com/CatworldLee/Gaussian-Mixture-Mask-Attention.}
}
@article{HAN2024110573,
title = {UVMO: Deep unsupervised visual reconstruction-based multimodal-assisted odometry},
journal = {Pattern Recognition},
volume = {153},
pages = {110573},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110573},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003248},
author = {Songrui Han and Mingchi Li and Hongying Tang and Yaozhe Song and Guanjun Tong},
keywords = {Visual odometry, Pose estimation, Visual reconstruction, Multimodal assisted, Triple-modal fusion, Image-based mask},
abstract = {In recent years, unsupervised visual odometry (VO) based on visual reconstruction has attracted lots of attention due to its end-to-end pose estimation approach and the advantage of not requiring real labels for training. Unsupervised VO inputs monocular video frames into a pose estimation network to output the predicted poses, and optimizes the pose prediction by minimizing visual reconstruction loss with epipolar geometry constraint. However, lack of depth information and complex environments such as rapid turns and uneven lighting in monocular video frames can result in insufficient visual information for pose estimation. Additionally, dynamic objects and discontinuous occlusions in monocular video frames can introduce inappropriate errors in visual reconstruction. In this paper, an Unsupervised V isual reconstruction-based Multimodal-assisted Odometry (UVMO) is proposed. UVMO leverages inertial and lidar information to complement visual information to acquire more accurate pose estimation. Specifically, a triple-modal fusion strategy called SMPF is proposed to conduct a more comprehensive and stable fusion of the three modalities’ data. Additionally, an image-based mask is introduced to filter out the dynamic occlusion regions in video frames, improving the accuracy of visual reconstruction. To the best of our knowledge, this paper is the first to propose a pure deep learning-based visual-inertial-lidar odometry. Experiments show that UVMO achieves state-of-the-art performance among pure deep learning-based unsupervised odometry.}
}
@article{LIU2024110636,
title = {Many birds, one stone: Medical image segmentation with multiple partially labeled datasets},
journal = {Pattern Recognition},
volume = {155},
pages = {110636},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110636},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400387X},
author = {Qing Liu and Hailong Zeng and Zhaodong Sun and Xiaobai Li and Guoying Zhao and Yixiong Liang},
keywords = {Partially supervised learning, Self-training, Medical image segmentation, Cross-task attention},
abstract = {Medical image segmentation is fundamental in the field of medical image analysis and has wide clinical applications in disease diagnosis and surgical planning etc. Current prevalent solution is to train a deep network in a fully supervised way with a large-scale fully labeled dataset. However, due to the high labor cost and requirement on medical expertise, such dataset is always absent. Instead, there are multiple partially labeled datasets which are originally established for specific purposes. To make full use of these partially labeled datasets, we propose a novel partially supervised segmentation network, named PSSNet, which consists of a task-specific feature learning network followed by a cross-task attention module (xTA) to exploit task dependencies to enhance task-specific features. To solve the challenges raised by unlabeled classes and domain shift across datasets, we propose an adversarial self-training strategy. We conduct experiments on two medical image segmentation tasks. One is the fine-grained fundus image segmentation aiming to simultaneously segment four-class lesions, OD and OC, and vessels. Validation on seven datasets demonstrates that our PSSNet performs the best among three baselines and three state-of-the-arts. The other is the multiple abdominal organ segmentation in CT images. Our PSSNet is trained on three partially labeled datasets, i.e., LiTS, KiTS and Spleen. Validation on one fully labeled dataset, i.e., BTCV, demonstrates that our PSSNet achieves better performances than four state-of-the-arts. The code is publicly available at https://github.com/CVIU-CSU/PSSNet.}
}
@article{WANG2024110544,
title = {Sparse and robust support vector machine with capped squared loss for large-scale pattern classification},
journal = {Pattern Recognition},
volume = {153},
pages = {110544},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110544},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002954},
author = {Huajun Wang and Hongwei Zhang and Wenqian Li},
keywords = {Capped squared loss, Fast algorithm, Support vectors, Low computational complexity, Working set},
abstract = {Support vector machine (SVM), being considered one of the most efficient tools for classification, has received widespread attention in various fields. However, its performance is hindered when dealing with large-scale pattern classification tasks due to high memory requirements and running very slow. To address this challenge, we construct a novel sparse and robust SVM based on our newly proposed capped squared loss (named as Lcsl-SVM). To solve Lcsl-SVM, we first focus on establishing optimality theory of Lcsl-SVM via our defined proximal stationary point, which is convenient for us to efficiently characterize the Lcsl support vectors of Lcsl-SVM. We subsequently demonstrate that the Lcsl support vectors comprise merely a minor fraction of entire training data. This observation leads us to introduce the concept of the working set. Furthermore, we design a novel subspace fast algorithm with working set (named as Lcsl-ADMM) for solving Lcsl-SVM, which is proven that Lcsl-ADMM has both global convergence and relatively low computational complexity. Finally, numerical experiments show that Lcsl-ADMM has excellent performances in terms of getting the best classification accuracy, using the shortest time and presenting the smallest numbers of support vectors when solving large-scale pattern classification problems.}
}
@article{JANG2024110560,
title = {Synthetic unknown class learning for learning unknowns},
journal = {Pattern Recognition},
volume = {153},
pages = {110560},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110560},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400311X},
author = {Jaeyeon Jang},
keywords = {Open set recognition, Overgeneralization, Knowledge distillation, Generative adversarial learning, Unknown},
abstract = {This paper addresses the open set recognition (OSR) problem, where the goal is to correctly classify samples of known classes while detecting unknown samples to reject. In the OSR problem, “unknown” is assumed to have infinite possibilities because we have no knowledge about unknowns until they emerge. Intuitively, the more an OSR system explores the possibilities of unknowns, the more likely it is to detect unknowns. Even though several generative OSR models have been proposed to explore more by generating synthetic samples and learning them as unknowns, the generated samples are limited to a small subspace of the known classes. Thus, this paper proposes a novel synthetic unknown class learning method that constantly generates unknown-like samples while maintaining diversity between the generated samples. By learning the unknown-like samples and known samples in an alternating manner, the proposed method can not only experience diverse synthetic unknowns but also reduce overgeneralization with respect to known classes. Experiments on several benchmark datasets show that the proposed method significantly outperforms other state-of-the-art approaches by generating diverse realistic unknown samples.}
}
@article{CUEVASVELASQUEZ2024110601,
title = {Efficient multi-task progressive learning for semantic segmentation and disparity estimation},
journal = {Pattern Recognition},
volume = {154},
pages = {110601},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110601},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003522},
author = {Hanz Cuevas-Velasquez and Alejandro Galán-Cuenca and Robert B. Fisher and Antonio Javier Gallego},
keywords = {Computer vision, Stereo vision, Semantic segmentation, Joint learning, 3D modeling, Multi-task, Disparity estimation},
abstract = {Scene understanding is an important area in robotics and autonomous driving. To accomplish these tasks, the 3D structures in the scene have to be inferred to know what the objects and their locations are. To this end, semantic segmentation and disparity estimation networks are typically used, but running them individually is inefficient since they require high-performance resources. A possible solution is to learn both tasks together using a multi-task approach. Some current methods address this problem by learning semantic segmentation and monocular depth together. However, monocular depth estimation from single images is an ill-posed problem. A better solution is to estimate the disparity between two stereo images and take advantage of this additional information to improve the segmentation. This work proposes an efficient multi-task method that jointly learns disparity and semantic segmentation. Employing a Siamese backbone architecture for multi-scale feature extraction, the method integrates specialized branches for disparity estimation and coarse and refined segmentations, leveraging progressive task-specific feature sharing and attention mechanisms to enhance accuracy for solving both tasks concurrently. The proposal achieves state-of-the-art results for joint segmentation and disparity estimation on three distinct datasets: Cityscapes, TrimBot2020 Garden, and S-ROSeS, using only 1/3 of the parameters of previous approaches.}
}
@article{LEE2024110549,
title = {Observation weights matching approach for causal inference},
journal = {Pattern Recognition},
volume = {154},
pages = {110549},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110549},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003005},
author = {Kangbok Lee and Sumin Han and Hyeoncheol Baik and Yeasung Jeong and Young Woong Park},
keywords = {Ensemble learning, LogitBoost, AdaBoost, Overlapping regions, Observation weights, Propensity score matching},
abstract = {This study introduces a novel method integrating pattern recognition models with causal inference methodologies to adeptly identify and manage overlapping regions between treatment and control groups. Our approach, Observation Weights Matching (OWM), addresses the intrinsic challenges in observational studies—specifically, the fixed sample size and the lack of complete overlap in pretreatment variables. Through ensemble learning, OWM effectively retains examples within these critical overlapping regions, systematically generating weighted data distributions that aid in the precise identification of these instances. By prioritizing hard-to-classify observations and employing a novel metric of critical values for matched samples, our approach optimizes matching performance and provides greater robustness in causal analysis. Through empirical and simulation studies, we demonstrate OWM's notable advantage over traditional matching methods, enhancing causal inference in observational research. Furthermore, we show that OWM provides richer balance scores than propensity scores, ensuring unbiased estimations and advancing the field significantly.}
}
@article{AKHTAR2024110637,
title = {Advancing Supervised Learning with the Wave Loss Function: A Robust and Smooth Approach},
journal = {Pattern Recognition},
volume = {155},
pages = {110637},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110637},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003881},
author = {Mushir Akhtar and M. Tanveer and Mohd. Arshad},
keywords = {Supervised learning, Pattern classification, Loss function, Support vector machine, Twin support vector machine, Wave loss function, Adam algorithm, Alzheimer’s disease},
abstract = {Loss function plays a vital role in supervised learning frameworks. The selection of the appropriate loss function holds the potential to have a substantial impact on the proficiency attained by the acquired model. The training of supervised learning algorithms inherently adheres to predetermined loss functions during the optimization process. In this paper, we present a novel contribution to the realm of supervised machine learning: an asymmetric loss function named wave loss. It exhibits robustness against outliers, insensitivity to noise, boundedness, and a crucial smoothness property. Theoretically, we establish that the proposed wave loss function manifests the essential characteristic of being classification-calibrated. Leveraging this breakthrough, we incorporate the proposed wave loss function into the least squares setting of support vector machines (SVM) and twin support vector machines (TSVM), resulting in two robust and smooth models termed as Wave-SVM and Wave-TSVM, respectively. To address the optimization problem inherent in Wave-SVM, we utilize the adaptive moment estimation (Adam) algorithm, which confers multiple benefits, including the incorporation of adaptive learning rates, efficient memory utilization, and faster convergence during training. It is noteworthy that this paper marks the first instance of Adam’s application to solve an SVM model. Further, we devise an iterative algorithm to solve the optimization problems of Wave-TSVM. To empirically showcase the effectiveness of the proposed Wave-SVM and Wave-TSVM, we evaluate them on benchmark UCI and KEEL datasets (with and without feature noise) from diverse domains. Moreover, to exemplify the applicability of Wave-SVM in the biomedical domain, we evaluate it on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset. The experimental outcomes unequivocally reveal the prowess of Wave-SVM and Wave-TSVM in achieving superior prediction accuracy against the baseline models. The source codes of the proposed models are publicly available at https://github.com/mtanveer1/Wave-SVM.}
}
@article{MA2024110585,
title = {LGNet: Local and global point dependency network for 3D object detection},
journal = {Pattern Recognition},
volume = {154},
pages = {110585},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110585},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003364},
author = {Jianwei Ma and Yan Huang and Cheng Qian and Jian Kang and Jiang Liu and Hui Zhang and Wei Hong},
keywords = {3D object detection, Local and global point dependency, Local point-graph pooling module, Global point-aware module, Autonomous driving},
abstract = {3D object detection is a challenging task in autonomous driving industry scenarios. Many pre-existing methods employ the set-abstraction operation for generating key-point representations, which, however, cannot learn the long-range context dependency properly. In addition, the pooling operator, which only focuses on maximum channel response, is adopted to aggregate features of neighbor points without semantic information. To fix these issues, we propose LGNet, a new framework that simultaneously captures local and global point dependencies for enhancing 3D object detection. Specifically, we first introduce a new local point-graph pooling module to compute point-to-point correlations in a local region and aggregate features from neighboring points. To further capture the long-range dependency in a global context, we devised a global point-aware module to integrate local and global features at higher resolution. Experiments on the KITTI 3D detection dataset and Waymo Open Dataset benchmark show that LGNet achieves state-of-the-art performance in multiple classes. We will upload the code on https://github.com/MWPony/LGNet.}
}
@article{LIU2024110561,
title = {Transductive zero-shot learning with generative model-driven structure alignment},
journal = {Pattern Recognition},
volume = {153},
pages = {110561},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110561},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003121},
author = {Yang Liu and Keda Tao and Tianhui Tian and Xinbo Gao and Jungong Han and Ling Shao},
keywords = {Domain shift, Transductive zero-shot learning, Structure alignment},
abstract = {Zero-shot learning (ZSL) facilitates the transfer of knowledge from seen to unseen categories through high-dimensional vectors that capture both known and unknown class names. However it encounters challenges with domain shift arising from a lack of sufficient labeled data. Although transductive zero-shot learning (TZSL) addresses this bias by including samples from unseen classes, it still faces obstacles in enhancing TZSL performance. In this study, We introduce the Structure Alignment Variational Autoencoder Generative Adversarial Network (SA-VAEGAN), a novel approach that enhances the alignment between visual and auxiliary spaces. We delved into the underlying causes of domain shift and introduced a structural alignment (SA) strategy to tackle these challenges. The SA model thoroughly accounts for both inter-class and intra-class dynamics, designed to leverage the model’s comprehension of high-level semantic relations to disambiguate confusion among similar classes and mitigate intra-class confusion by penalizing atypical visual samples within classes. Assessed across four benchmark datasets, SA-VAEGAN has established a new performance standard, underscoring its efficiency in addressing the domain shift challenge within TZSL tasks, and achieving high accuracy.}
}
@article{SONG2024110568,
title = {Quality-aware blind image motion deblurring},
journal = {Pattern Recognition},
volume = {153},
pages = {110568},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110568},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003194},
author = {Tianshu Song and Leida Li and Jinjian Wu and Weisheng Dong and Deqiang Cheng},
keywords = {Image quality assessment, Motion deblurring, Knowledge embedding, Generalization},
abstract = {Recent mapping-based motion deblurring methods lack the regularization of prior knowledge, resulting in an over-reliance on the training data and limited generalization ability. As deblurring aims to improve image quality, we quantitatively analyze and further discover the strong correlation between image quality and sharpness. Motivated by the above facts and notable accomplishments of recent no-reference image quality assessment (NR-IQA), we present a novel framework that incorporates quality knowledge into mapping-based deblurring models. Specifically, we extract quality-aware features from NR-IQA models as prior knowledge, and subsequently propose a prediction-based strategy and an encoder-reuse strategy to integrate knowledge into the encoder and decoder, respectively. After training, the model can simultaneously deblur images and predict quality features, indicating that it has grasped the knowledge and validating the effectiveness of the proposed embedding strategies. Extensive experimental results show that embedding quality knowledge consistently improves model performance and the model achieves state-of-the-art intra/cross-dataset results. Code and pre-trained models are available at https://github.com/esnthere/QAMD.}
}
@article{DING2024110599,
title = {Cross-contrast mutual fusion network for joint MRI reconstruction and super-resolution},
journal = {Pattern Recognition},
volume = {154},
pages = {110599},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110599},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003509},
author = {Yue Ding and Tao Zhou and Lei Xiang and Ye Wu},
keywords = {Magnetic resonance imaging, MRI reconstruction, Super-resolution, Multi-task learning},
abstract = {Magnetic Resonance Imaging (MRI) is a widely used medical imaging technique that has become an essential tool for diagnosing various diseases and visualizing internal structures and tissues in the human body. MRI reconstruction and super-resolution are two techniques that can enhance image quality and accelerate the imaging process. However, current methods perform these tasks independently and fail to consider the correlations between them. Additionally, multi-contrast SR methods typically concatenate features from different contrasts without considering their correlation. In this paper, we propose a novel Cross-contrast Mutual Fusion Network (CMF-Net) that performs joint MRI reconstruction and super-resolution by enabling mutual propagation of feature representations between the two tasks. The CMF-Net framework consists of two stages: the first stage focuses on fusing multi-contrast features, while the second stage aims to learn task-specific information for joint MRI reconstruction and super-resolution. We propose a Multi-contrast Feature Aggregation (MFA) module to facilitate the integration of multi-contrast features. This module captures multi-scale information from auxiliary contrast to enhance the feature representation’s capability. Furthermore, a Multi-task Mutual Fusion (MMF) module is presented to integrate task-specific features, which explores the correlation between the two tasks to improve MR super-resolution performance. We evaluate the proposed CMF-Net approach on two public MR datasets. Quantitative and qualitative results demonstrate that our CMF-Net outperforms other state-of-the-art methods.}
}
@article{JIAN2024110596,
title = {Light dual hypergraph convolution for collaborative filtering},
journal = {Pattern Recognition},
volume = {154},
pages = {110596},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110596},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003479},
author = {Meng Jian and Langchen Lang and Jingjing Guo and Zun Li and Tuo Wang and Lifang Wu},
keywords = {Collaborative filtering, Hypergraph, Graph convolution, Personalized recommendation, User interest},
abstract = {Recommender systems filter information to meet users’ personalized interests actively. Existing graph-based models typically extract users’ interests from a heterogeneous interaction graph. They do not distinguish learning between users and items, ignoring the heterogeneous property. In addition, the interaction sparsity and long-tail bias issues still limit the recommendation performance significantly. Fortunately, hidden homogeneous correlations that have a considerable volume can entangle abundant CF signals. In this paper, we propose a light dual hypergraph convolution (LDHC) for collaborative filtering, which designs a hypergraph to involve heterogeneous and homogeneous correlations with more CF signals confronting the challenges. Over the integrated hypergraph, a two-level interest propagation is performed within the heterogeneous interaction graph and between the homogeneous user/item graphs to model users’ interests, where learning on users and items is distinguished and collaborated by the homogeneous propagation. Specifically, hypergraph convolution is lightened by removing unnecessary parameters to propagate users’ interests. Extensive experiments on publicly available datasets demonstrate that the proposed LDHC outperforms the state-of-the-art baselines.}
}
@article{LIU2024110640,
title = {Few-shot image classification via hybrid representation},
journal = {Pattern Recognition},
volume = {155},
pages = {110640},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110640},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003911},
author = {Bao-Di Liu and Shuai Shao and Chunyan Zhao and Lei Xing and Weifeng Liu and Weijia Cao and Yicong Zhou},
keywords = {Few-shot image classification, Specific representation, Shared representation},
abstract = {Few-shot image classification aims to learn an embedding model on the base datasets and design a base learner to recognize novel categories. The few-shot image classification framework is a two-phase process. First, the pre-train phase utilizes the base data to train a CNN-based feature extractor. Next, in the meta-test phase, the frozen feature extractor is applied to novel data with categories different from the base data. A base learner is then designed for recognition. Several simple base learners, including nearest neighbor, support vector machine, and logistic regression classifiers, have been recently introduced for few-shot learning tasks. However, these base learners are separately designed to consider specific representations (e.g., the class center) or shared representations (e.g., the boundaries). This paper mainly focuses on exploring the representation-residual base learners, which aim to represent a query sample with the support set and predict the query sample’s label based on the minimal residual error. We first introduce two representation-residual base learners: a specific representation base learner and a shared representation base learner. Then, we propose a novel hybrid representation base learner that combines both base learners to generate competitive representation. Additionally, we extend our approach by incorporating a self-training framework to utilize the query data fully. We evaluate our proposed method on several benchmark few-shot image classification datasets, such as miniImageNet, tieredImageNet, CIFAR-FS, FC100, and CUB datasets. The experimental results indicate that our proposed approach shows a significant performance improvement.}
}
@article{ZHANG2024110503,
title = {Blessing few-shot segmentation via semi-supervised learning with noisy support images},
journal = {Pattern Recognition},
volume = {154},
pages = {110503},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110503},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002541},
author = {Runtong Zhang and Hongyuan Zhu and Hanwang Zhang and Chen Gong and Joey Tianyi Zhou and Fanman Meng},
keywords = {Few-shot segmentation, Semi-supervised learning, Noisy images, Causal inference},
abstract = {Mainstream few-shot segmentation methods meet performance bottleneck due to the data scarcity of novel classes with insufficient intra-class variations, which results in a biased model primarily favoring the base classes. Fortunately, owing to the evolution of the Internet, an extensive repository of unlabeled images has become accessible from diverse sources such as search engines and publicly available datasets. However, such unlabeled images are not a free lunch. There are noisy inter-class and intra-class samples causing severe feature bias and performance degradation. Therefore, we propose a semi-supervised few-shot segmentation framework named F4S, which incorporates a ranking algorithm designed to eliminate noisy samples and select superior pseudo-labeled images, thereby fostering the improvement of few-shot segmentation within a semi-supervised paradigm. The proposed F4S framework can not only enrich the intra-class variations of novel classes during the test phase, but also enhance meta-learning of the network during the training phase. Furthermore, it can be readily implemented with ease on any off-the-shelf few-shot segmentation methods. Additionally, based on a Structural Causal Model (SCM), we further theoretically explain why the proposed method can solve the noise problem: the severe noise effects are removed by cutting off the backdoor path between pseudo labels and noisy support images via causal intervention. On PASCAL-5i and COCO-20i datasets, we show that the proposed F4S can boost various popular few-shot segmentation methods to new state-of-the-art performances.}
}
@article{WANG2024110624,
title = {Rethinking local-to-global representation learning for rotation-invariant point cloud analysis},
journal = {Pattern Recognition},
volume = {154},
pages = {110624},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110624},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003753},
author = {Zhaoxuan Wang and Yunlong Yu and Xianzhi Li},
keywords = {Point cloud, Rotation invariance, Local-to-global, Deep learning},
abstract = {Point cloud analysis has drawn much attention in recent years, whereas most existing point-based deep networks ignore the rotation-invariant property of the encoded features, which leads to poor performance given 3D shapes with arbitrary rotation. In this paper, we propose a novel rotation-invariant method that embeds both distinctive local and global rotation-invariant information. Specifically, we design a two-branch network that separately extracts purely local and global rotation-invariant features. In the global branch, we leverage canonical transformation to extract global representations, while in the local branch, we utilize hand-crafted geometric features (e.g., relative distances and angles) to embed local representations. To fuse the features from distinct branches, we introduce an attention-based fusion module to adaptively integrate the local-to-global representation by considering the geometry contexts of each point. Particularly, different from existing rotation-invariant works, we further introduce a self-attention unit into the global branch for embedding non-local information and also insert multiple fusion modules into the local branch to emphasize the global features. Extensive experiments on standard benchmarks show that our method achieves consistent and competitive performance on various downstream tasks, and also the best performance on the shape classification task on the ModelNet40 dataset with a 0.8% accuracy gain, compared to state-of-the-art methods. The code and pre-trained models are available at https://github.com/CentauriStar/Rotation-Invariant-Point-Cloud-Analysis.}
}
@article{LIU2024110572,
title = {Towards robust image matching in low-luminance environments: Self-supervised keypoint detection and descriptor-free cross-fusion matching},
journal = {Pattern Recognition},
volume = {153},
pages = {110572},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110572},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003236},
author = {Sikang Liu and Yida Wei and Zhichao Wen and Xueli Guo and Zhigang Tu and You Li},
keywords = {Keypoint detection, Feature matching, Structure from motion, Self-supervised, Cross-fusion, Transformer},
abstract = {Image keypoint detection and feature matching are fundamental steps in computer vision tasks. However, variations in environment, time, and viewpoint pose a challenge to the stability of image keypoint detection and matching. Most traditional and deep learning-based methods cannot accurately and efficiently extract highly repeatable keypoints and robust match pairs in low-luminance environments. Therefore, we propose a two-step ‘detection + matching’ framework, which consists of deep neural networks in each step. Firstly, we design a self-supervised robust keypoint detection network, which utilizes multi-scale, multi-angle, and multi-luminance transformation techniques to create pseudo-labeled datasets to improve the model’s keypoint detection repeatability and luminance invariance. Secondly, we propose a descriptor-free cross-fusion matching network, which uses the cross-fusion attention mechanism to establish connections between keypoint-centered image patches and converts the feature-matching task into an image patch assignment task to improve the accuracy and efficiency of matching. Thirdly, the proposed framework is used to replace traditional SIFT in SfM. Experimental results on testing datasets show that the self-supervised robust keypoint detection network achieves higher keypoint repeatability in low luminance environments compared to SIFT, ORB, LIFT, and Superpoint. The descriptor-free cross-fusion matching network’s mean matching accuracy and efficiency are higher than the mainstream Superglue algorithm. Also, SfM achieves better performance regarding the number of sparse point clouds and accuracy.}
}
@article{CHEN2024110600,
title = {Collaborative compensative transformer network for salient object detection},
journal = {Pattern Recognition},
volume = {154},
pages = {110600},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110600},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003510},
author = {Jun Chen and Heye Zhang and Mingming Gong and Zhifan Gao},
keywords = {Salient object detection, Collaborative relation transformer, Collaborative embedding},
abstract = {Salient object detection (SOD) is of high significance for various computer vision applications but is a challenging task due to the complicated scenes in real-world images. Most state-of-the-art SOD methods aim to build long-range dependency for improving global contrast modeling in complicated scenes. However, most of them suffer from the prior assumption of treating image patches as visual tokens for building long-range dependency. This is because this assumption leads to localizing salient regions with uncertain boundaries due to the lost object structure information. In this paper, to address this issue, we re-construct the prior assumption of treating both patches and superpixels as visual tokens for building long-range dependency, which takes into account the properties of superpixels and patches in preserving detailed structural-aware information and local context information, respectively. Based on the re-constructed prior assumption, we propose a Collaborative Compensative Transformer Network (CCTNet) for the SOD task. CCTNet firstly alternates the computation within the same kind of vision tokens and among different vision tokens to build their dependencies. By this means, the relationship between multi-level global context and detailed structure representation can be explicitly modeled for consistent semantic and object structure understanding. Then, CCTNet performs feature joint decoding for SOD by fusing the complementary global context and detailed structure for locating objects with certain boundaries. Extensive experiments were conducted to validate the effectiveness of the proposed modules. Furthermore, the experiments on ten benchmark datasets demonstrated the state-of-the-art performance of CCTNet on both RGB and RGB-D SOD.}
}
@article{GAO2024110556,
title = {CSTrans: Correlation-guided Self-Activation Transformer for Counting Everything},
journal = {Pattern Recognition},
volume = {153},
pages = {110556},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110556},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003078},
author = {Bin-Bin Gao and Zhongyi Huang},
keywords = {Few-shot counting, Local dependency, Counting everything, Vision transformer},
abstract = {Counting everything, also named few-shot counting, requires a model to be able to count objects with any novel (unseen) category giving few exemplar boxes. However, the existing few-shot counting methods are sub-optimal due to weak feature representation, such as the correlation between the exemplar patch and query feature, and contextual dependencies in density map prediction. In this paper, we propose a very simple but effective method, CSTrans, consisting of a Correlation-guided Self-Activation (CSA) module and a Local Dependency Transformer (LDT) module, to mitigate the above two issues, respectively. The CSA utilizes the correlation map to activate the semantic features and suppress the noisy influence of the query features, aiming at mining the potential relation while enriching correlation representation. Furthermore, the LDT incorporates a Transformer to explore local contextual dependencies and predict the density map. Our method achieves competitive performance on FSC-147 and CARPK datasets. We hope its simple implementation and superior performance can serve as a new and strong baseline for few-shot counting tasks and attract more interest in designing simple but effective models in future studies. Our code for CSTrans is available at https://github.com/gaobb/CSTrans.}
}
@article{ZHU2024110584,
title = {Emotion-aware hierarchical interaction network for multimodal image aesthetics assessment},
journal = {Pattern Recognition},
volume = {154},
pages = {110584},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110584},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003352},
author = {Tong Zhu and Leida Li and Pengfei Chen and Jinjian Wu and Yuzhe Yang and Yaqian Li},
keywords = {Image aesthetics assessment, Multimodal learning, Image emotion analysis},
abstract = {Image aesthetics assessment (IAA) has attracted increasing attention recently but is still challenging due to its high abstraction and complexity. Intuitively, image emotion and aesthetics are both human subjective feelings evoked by visual content. Previous researches have demonstrated that image aesthetics has intrinsic relationships with emotion. In fact, human emotional experience has potential impact on the perception of image aesthetics. Therefore, emotion information can be exploited to enhance aesthetic representation learning. Inspired by this, this paper presents an Emotion-Aware Hierarchical Interaction NETwork (EAHI-NET) for multimodal image aesthetics assessment, which explores both intra-modal and inter-modal interactions between aesthetics and emotions hierarchically. Specifically, we first propose the intra-modal emotion-aesthetics interaction module to obtain emotion-enhanced visual and textual aesthetic representations respectively. Then we propose the inter-modal feature enhancement to obtain the cross-modal aesthetic and emotion features. Finally, we design the inter-modal emotion-aesthetics interaction module to further investigate the cross-modal interplay between aesthetics and emotion, based on which hierarchical feature representations are achieved for multimodal IAA. The extensive experiments show that the proposed method can outperform the state-of-the-arts on multimodal AVA and Photo.net datasets.}
}
@article{GILRODRIGUEZ2024110575,
title = {Color matching in the wild},
journal = {Pattern Recognition},
volume = {154},
pages = {110575},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110575},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003261},
author = {Raquel {Gil Rodríguez} and Javier Vazquez-Corral and Marcelo Bertalmío and Graham D. Finlayson},
keywords = {Color stabilization, Color matching, Logarithmic encoded images, Gamma-corrected images, HDR encoding},
abstract = {We present a method that, given two different views of the same scene taken by two cameras with unknown settings and internal parameters, corrects the colors of one of the images making it look as if it was captured under the other camera settings. Our method is able to deal with any standard non-linear encoded images (gamma-corrected, logarithmic-encoded, or any other) without requiring any previous knowledge of the encoding. To this end, our method makes use of two important observations. First, the camera imaging pipeline from RAW to sRGB can be well approximated by considering just a per-pixel shading and a color transformation matrix, and second, for correcting the images we only need to estimate a single matrix –that will contain information from both of the original images– and an approximation of the shading term (that emulates the non-linearity). Our proposed method is fast and the results have no spurious artifacts. The method outperforms the state-of-the-art when compared with other methods that do not require knowledge of the encoding used. It is also able to compete with –and even surpass in some cases– methods that consider information about image encoding.}
}
@article{ZHANG2024110630,
title = {TCFAP-Net: Transformer-based Cross-feature Fusion and Adaptive Perception Network for large-scale point cloud semantic segmentation},
journal = {Pattern Recognition},
volume = {154},
pages = {110630},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110630},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003819},
author = {Jianjun Zhang and Zhipeng Jiang and Qinjun Qiu and Zheng Liu},
keywords = {Transformer, Attention, Semantic segmentation, Point cloud scenarios},
abstract = {Point cloud semantic segmentation is an ingredient in understanding real-world scenes. Most existing approaches perform poorly on scene boundaries and struggle with recognizing objects of different scales. In this paper, we propose a novel framework that incorporates Transformer into the U-Net architecture for inferring pointwise semantics. Specifically, the Transformer-based cross-feature fusion module is designed first to employ geometric and semantic information to learn feature offsets to overcome the border ambiguity of segmentation results, and then it utilizes the Transformer to learn cross-feature enhanced and fused encoder features. Additionally, to facilitate the overall network’s structure-to-detail perception capabilities, the adaptive perception module is designed, which employs cross-attention to adaptively allocate weights to encoder features at varying resolutions, establishing long-range contextual dependencies. Ablation studies validate the individual contributions of our module design choices. Compared with the existing competitive methods, our approach achieves state-of-the-art performance and exhibits superior results on benchmarks. Code is available at https://github.com/xiluo-cug/TCFAP-Net.}
}
@article{WANG2024110619,
title = {Nonconvex submodule clustering via joint sliced sparse gradient and cluster-aware approach},
journal = {Pattern Recognition},
volume = {154},
pages = {110619},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110619},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003704},
author = {Jingyu Wang and Tingquan Deng and Ming Yang},
keywords = {Tensor based sparse representation, Submodule clustering, Tensor schatten -norm, Union of free submodules, ℓnorm, SSG, Cluster-aware},
abstract = {Most existing subspace clustering methods preprocess image data by converting them into vectors, which lacks exploration of the spatial structure of high-dimensional data. Therefore, we proposes a nonconvex submodule clustering model (NSSGCA) via joint sliced sparse gradient and cluster-aware approach. NSSGCA arranges each 2D image as lateral slices of a 3rd-order tensor, and utilizes the t-product under the model of the union of free submodules to represent 3rd-order tensor samples, thereby exploring the latent spatial structure of samples. To more accurately approximate tensor rank, a nonconvex Schatten p-norm constraint is imposed on the rotated representation tensor. Under the submodule framework, a consistent gradient matrix is derived based on the δ-nearest neighbor adjacency graph to construct sliced sparse gradient (SSG) regularization, which is more conducive to clustering tasks. NSSGCA learns representation tensor with clearer block-like structure based on ℓq norm and cluster-aware attention mechanism. The convergence of the constructed sequence to the stationary Karush–Kuhn–Tucker (KKT) point is proven. Experimental results on real-world image datasets confirm the effectiveness of NSSGCA.}
}
@article{HU2024110546,
title = {An efficient training-from-scratch framework with BN-based structural compressor},
journal = {Pattern Recognition},
volume = {153},
pages = {110546},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110546},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002978},
author = {Fuyi Hu and Jin Zhang and Song Gao and Yu Lin and Wei Zhou and Ruxin Wang},
keywords = {Channel pruning, Model compression, Knowledge distillation, Convolutional Neural Network (CNN)},
abstract = {Channel pruning is an effective way of compressing convolutional neural networks (CNNs) under constrained resources. The current pruning methods follow a progressive pretrain-prune-finetune pipeline, which is inefficient and computationally expensive. In this paper, we bypass the pretrain-prune-finetune pipeline and propose a novel and efficient model training framework based on online channel pruning, which automatically produces a compact well-performed sub-network in one training-from-scratch pass under a given budget condition. Specifically, we introduce a novel BN-based indicator and a sparsity regularization strategy in the early training stage to iteratively and greedily shrink the model layers, which encourages a high-quality architecture with low channel redundancy. To ensure training stability and promote the generalization ability of the resultant pruned network, we also skillfully incorporate a simple self-distillation framework into our training and pruning pipeline. Extensive experiments indicate that our method can effectively achieve competitive performance on the image classification task compared with the state-of-the-arts.}
}
@article{XIAO2024110533,
title = {Unified multi-level neighbor clustering for Source-Free Unsupervised Domain Adaptation},
journal = {Pattern Recognition},
volume = {153},
pages = {110533},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110533},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400284X},
author = {Yuzhe Xiao and Guangyi Xiao and Hao Chen},
keywords = {Pattern recognition, Computer vision, Unsupervised Domain Adaptation},
abstract = {Source-Free Unsupervised Domain Adaptation (SFUDA) transfers knowledge from the source domain to the target domain with using the source domain model instead of the source data, as the source data cannot be accessed in data privacy scenarios. Our method is based on a clustering assumption: although there is a domain shift, target data with similar semantic still form a cluster in the source feature space. We identify two levels of clustering. One is class-level neighbor clustering: data with the same label tend to form a large cluster. The other is instance-level neighbor clustering: data and its neighbors tend to share the same label. Previous methods only consider one level, and we consider that both are complementary. We propose a new SFUDA method called unified multi-level neighbor clustering to address class and instance consistency in a complementary way. Our proposed method achieves competitive results on three domain adaptation benchmark datasets.}
}
@article{YU2024110643,
title = {Discovering attention-guided cross-modality correlation for visible–infrared person re-identification},
journal = {Pattern Recognition},
volume = {155},
pages = {110643},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110643},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003947},
author = {Hao Yu and Xu Cheng and Kevin Ho Man Cheng and Wei Peng and Zitong Yu and Guoying Zhao},
keywords = {Visible–infrared person re-identification, Cross-modality, Attention mechanism, Modality discrepancies},
abstract = {Visible–infrared person re-identification (VI Re-ID) is an essential and challenging task. Existing studies mainly focus on learning the unified modality-invariant representations directly from visible and infrared images. However, it is hard to obtain the identity-aware patterns due to the co-existence of inter- and intra-modality discrepancies. In this paper, we propose a novel attention-guided cross-modality correlation method (AGCC) to achieve the modality-invariant and identity-discriminative representations for visible–infrared person Re-ID. Specifically, we introduce a modality-aware attention (MAA) mechanism to model the inter- and intra-modality variations, which generates attention masks of two modalities for preserving the most significant region and obtaining the discriminative patterns in each identity. Further, we present an attention-guided channel and spatial correlation scheme (AGCSC) to establish the attention-guided cross-modality correlation, which can bridge the gap between inter- and intra-modalities. Moreover, a novel joint-modality learning head (JMLH) is developed to promote the metric and mutual learning from both feature distribution and classification logit levels. Extensive experiments on two public SYSU-MM01 and RegDB datasets demonstrate the remarkable superiority of our method over the state of the arts. The implementation codes will be made available soon.}
}
@article{ZHOU2024110603,
title = {A high-precision ellipse detection method based on quadrant representation and top-down fitting},
journal = {Pattern Recognition},
volume = {154},
pages = {110603},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110603},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003546},
author = {Hongxia Zhou and Lixin Han and Shaojun Zhu and Hong Yan},
keywords = {Ellipse detection, Elliptical arcs, Block sequences, Quadrant representation, Top-down ellipse fitting},
abstract = {Ellipse detection is a basic task in many computer-vision related problems. While widely studied in recent years, accurate and efficient detection in real-world images is still a challenge. In this paper, a novel ellipse detector, with high accuracy and efficiency, is proposed. The detector models edge by block sequences, and extracts a set of elliptical arcs, which are classified into four sets. Then top-down ellipse fitting strategy that also makes the method able to detect small and flat ellipses is designed. A two-level validation process is used to select highly probable potential ellipses, especially for fragmented ellipses. Experiments on four synthetic datasets show that the proposed method performs far better than existing methods. In images with severe cluttering and occlusion, the F-measure can still be around 0.9. On four real image datasets the proposed method achieves better F-measure scores with competitive speed than state-of-the-art techniques.}
}
@article{LI2024110551,
title = {Distributed statistical learning algorithm for nonlinear regression with autoregressive errors},
journal = {Pattern Recognition},
volume = {153},
pages = {110551},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110551},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003029},
author = {Shaomin Li and Xiaofei Sun and Kangning Wang},
keywords = {Big data, Nonlinear regression, Distributed learning, Autoregressive errors},
abstract = {The growing size of modern data brings challenges to statistical learning, and substantial distributed algorithms have been proposed. However, most of them need the homogeneity assumption that the distribution of the local data is the same as that of the global data. This is seldom in practice, and the learning performance deteriorates seriously if this assumption is not satisfied. Moreover, they are only for independent data, and cannot incorporate the serial correlations between data. To solve these issues, we propose a novel distributed statistical learning framework for the nonlinear regression with autoregressive errors, which realizes communication-efficient distributed optimization, and overcomes the homogeneity assumption. The theoretical results also guarantee that the new distributed framework is equivalent to the global one. Numerical experiments also illustrate the good performance of the new method.}
}
@article{LIU2024110631,
title = {Contrastive visual clustering for improving instance-level contrastive learning as a plugin},
journal = {Pattern Recognition},
volume = {154},
pages = {110631},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110631},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003820},
author = {Yue Liu and Xiangzhen Zan and Xianbin Li and Wenbin Liu and Gang Fang},
keywords = {Self-supervised learning, Contrastive learning, Deep clustering},
abstract = {Contrastive learning has achieved remarkable success in computer vision, however it is built on instance-level discrimination which leaves the valuable intra-class correlation in dataset unexploited. Current semantic clustering methods are proven to be helpful but they would suffer from the error accumulated in the iteration process without ground-truth guidance. In an attempt to remedy the clustering error accumulation when utilizing intra-class correlation for contrastive learning, we propose an online Contrastive Visual Clustering (CVC) method with two actions: gathering instances with highly similar feature embeddings, and penalizing instances being clustered with low confidence. CVC can integrate with not only contrastive learning but also arbitrary self-supervised learning frameworks simply as a plugin. Under various experiment settings, we show that CVC improves the linear classification performance by a large margin for models pre-trained with self-supervised representation learning, in both image and video scenarios. The code is available at https://github.com/yliu1229/CVC.}
}
@article{WANG2024110579,
title = {WBNet: Weakly-supervised salient object detection via scribble and pseudo-background priors},
journal = {Pattern Recognition},
volume = {154},
pages = {110579},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110579},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003303},
author = {Yi Wang and Ruili Wang and Xiangjian He and Chi Lin and Tianzhu Wang and Qi Jia and Xin Fan},
keywords = {Weakly supervision, Salient object detection, Neural networks, Transformer, Pseudo labels, Scribble labels},
abstract = {Weakly supervised salient object detection (WSOD) methods endeavor to boost sparse labels to get more salient cues in various ways. Among them, an effective approach is using pseudo labels from multiple unsupervised self-learning methods, but inaccurate and inconsistent pseudo labels could ultimately lead to detection performance degradation. To tackle this problem, we develop a new multi-source WSOD framework, WBNet, that can effectively utilize pseudo-background (non-salient region) labels combined with scribble labels to obtain more accurate salient features. We first design a comprehensive salient pseudo-mask generator from multiple self-learning features. Then, we pioneer the exploration of generating salient pseudo-labels via point-prompted and box-prompted Segment Anything Models (SAM). Then, WBNet leverages a pixel-level Feature Aggregation Module (FAM), a mask-level Transformer-decoder (TFD), and an auxiliary Boundary Prediction Module (EPM) with a hybrid loss function to handle complex saliency detection tasks. Comprehensively evaluated with state-of-the-art methods on five widely used datasets, the proposed method significantly improves saliency detection performance. The code and results are publicly available at https://github.com/yiwangtz/WBNet.}
}
@article{CHU2024110587,
title = {Joint Variational Inference Network for domain generalization},
journal = {Pattern Recognition},
volume = {154},
pages = {110587},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110587},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003388},
author = {Jun-Zheng Chu and Bin Pan and Xia Xu and Tian-Yang Shi and Zhen-Wei Shi and Tao Li},
keywords = {Domain generalization, Deep learning, Variational Bayesian, Transfer learning},
abstract = {Domain generalization is a machine learning task that involves training a model on a set of domains with the goal of achieving high performance on unseen domains. While most domain generalization methods focus on extracting domain-invariant features, they may ignore domain-specific information beyond object styles which is label-relevant for classification. In this paper, we first propose a two-step data generation process in which the domain label and observed data are sampled from two distributions sequentially, and accordingly develop an end-to-end Joint Variational Inference Network (JVINet) for domain generalization. JVINet is a framework that admits a variational-based discriminative structure, which the domain-specific latent variable is learned and integrated to enhance the feature for classification. When testing on unseen target domains, the potential beneficial domain information is hence utilized to improve generalization ability. The overall objective is to optimize the variational lower bound of the conditional joint likelihood functions for both class and domain labels. We provide theoretical proof that JVINet can achieve an optimal lower bound using a variational-based discriminative approach. To evaluate the effectiveness of our method, we compare it with state-of-the-art methods on both simulated and real-world datasets.}
}
@article{DING2024110615,
title = {C2FResMorph: A high-performance framework for unsupervised 2D medical image registration},
journal = {Pattern Recognition},
volume = {154},
pages = {110615},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110615},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003662},
author = {Yi Ding and Junjian Bu and Zhen Qin and Li You and Mingsheng Cao and Zhiguang Qin and Minghui Pang},
keywords = {Image registration, Convolutional neural networks, Multi-head self-attention, Unsupervised learning},
abstract = {Deformable medical image registration is an important precursor task for surgical automation, while enhancing the registration performance of 2D medical images remains a challenging work. Existing methods primarily minimize the similarity loss between image pairs as the main optimization objective, leading to limited registration accuracy and a lack of pixel matching. Moreover, the scarcity of informative features in 2D images often results in overfitting on the training set, hampering generalization. To address these issues, we propose C2FResMorph, a learning-based deformable registration algorithm specifically designed for 2D medical images. C2FResMorph employs a two-stage framework that improves registration accuracy and preserves topology during deformation in a coarse-to-fine manner. Inside the framework, by leveraging the convolutional neural network’s locality and the multi-head self-attention mechanism’s globality, a ResMorph registration network is designed. Additionally, the integration of residual image knowledge addresses deformation folding in 2D image registration, enhancing the preservation of local structures and improving generalization. Experimental evaluations on three datasets demonstrate that C2FResMorph outperforms existing learning-based methods in terms of accuracy, generalization ability for 2D medical image registration, and also retains the efficiency advantages.}
}
@article{FAN2024110563,
title = {Bidirectional image denoising with blurred image feature},
journal = {Pattern Recognition},
volume = {153},
pages = {110563},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110563},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003145},
author = {Linwei Fan and Xiaoyu Yan and Huiyu Li and Yongxia Zhang and Hui Liu and Caiming Zhang},
keywords = {Image denoising, CNN, Deep learning, Blurred image feature, Bidirectional denoising process},
abstract = {Image denoising remains a classical yet still challenging problem, because the noise can destroy details and cause severe information loss. In recent years, various well-designed CNN-based methods have been extensively applied in image denoising because of the strong learning ability. However, most of them share an unidirectional procedure, which directly learns a mapping from noisy input to a clean image without focusing on the over-smoothed state of the denoising process, limiting the richness of extracted features. Different from previous works, we propose a blurred image feature guided CNN (BFCNN) network that implements a novel blurring-adjusting strategy to address the complex denoising problem via two stages. In stage 1, we build a blurring module (BM) to capture over-smoothed features from noisy observations and generate the blurred image restoration, which is a less informative version of the clean image. Furthermore, a multi-level concatenating module (CM) and an adjusting module (AM) are then designed to recover more detailed information in stage 2. These two modules are jointly designed to restore a properly-smoothed image from the over-smoothed blurred image and the given under-smoothed noisy image. Comparing to the traditional denoising process, the proposed blurring-adjusting strategy produces a precise denoised image more efficiently by converting the unidirectional denoising process into a bidirectional denoising process. To our knowledge, this is the first study that utilizes the over-smoothed image to address the denoising problem. Extensive experiments demonstrate the superiority of our BFCNN with more accurate reconstruction quality and achieve competitive quantitative results among current CNN-based methods. This research will release the code upon acceptance.}
}
@article{HUANG2024110574,
title = {Masked face recognition using domain adaptation},
journal = {Pattern Recognition},
volume = {153},
pages = {110574},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110574},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400325X},
author = {Yu-Chieh Huang and David Akas Bedjo Rahardjo and Ren-Hau Shiue and Homer H. Chen},
keywords = {Face recognition, Masked face, Domain adaptation},
abstract = {Wearing facial masks has become a must in our daily life due to the global COVID-19 pandemic. However, the performance of a face recognition system is severely degraded due to the fact that the face images in the gallery are unmasked faces while the probe face images captured by the camera are masked faces, making the probe face images different from gallery face images in the activated region and the distribution domain. In this paper, we propose a novel face recognition system to address the issue. The system is integrated with a domain adaptation layer and a feature refinement layer. The feature refinement layer is based on the structure of the self-attention mechanism to align activated regions of unmasked faces with those of masked faces. The domain adaptation layer works by adapting the system from the unmasked face domain to the synthetically masked face domain and the real- world masked face domain. The system is tested on real-world data through face verification and face identification. The face verification accuracy is improved by 6.83% for the RMFD_FV dataset and 4.2% for the MFR2 dataset, and the face identification accuracy is improved by 15.43% for the MFRFI dataset.}
}
@article{XIE2024110598,
title = {Consistent graph learning for multi-view spectral clustering},
journal = {Pattern Recognition},
volume = {154},
pages = {110598},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110598},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003492},
author = {Deyan Xie and Quanxue Gao and Yougang Zhao and Fan Yang and Wei Song},
keywords = {Spectral clustering, Multi-view clustering, Tensor learning, Graph learning},
abstract = {Given the heterogeneous information of multiple views and the possible noise embedded in multi-view data, it is difficult to directly learn a consistent representation from multiple graphs to depict the intrinsic structure of all views. We propose a consistent graph learning method by exploiting both the high-order correlations underlying multiple views and the global structure of each single view. First, we calculate a transition probability matrix from each view. Second, a tensor is constructed by stacking each transition matrix as its frontal slice and then decomposed into the latent and error tensors. For the latent tensor, after rotated, a weighted tensor nuclear norm is used to encourage the rotate tensor to fully exploit the high-order correlations underlying multiple views. Furthermore, each frontal slice of the latent tensor is regularized by the nuclear norm to capture the global structure of each single view, and is restricted by probability constraints. Besides, we adopt the Frobenius-norm-based regularization to directly learn a common affinity matrix from the latent tensor. The established model is readily optimized by the alternating Lagrangian method. Extensive experiments on six real world datasets demonstrate that our method outperforms state-of-the-art methods.}
}
@article{GAO2024110558,
title = {Collaborative brightening and amplification of low-light imagery via bi-level adversarial learning},
journal = {Pattern Recognition},
volume = {154},
pages = {110558},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110558},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003091},
author = {Jiaxin Gao and Yaohua Liu and Ziyu Yue and Xin Fan and Risheng Liu},
keywords = {Low-light image, Bi-level optimization, Image enhancement, Deep learning},
abstract = {Poor light conditions constrain the high pursuit of clarity and visible quality of photography especially smartphone devices. Admittedly, existing specific image processing methods, whether super-resolution methods or low-light enhancement methods, can hardly simultaneously enhance the resolution and brightness of low-light images at the same time. This paper dedicates a specialized enhancer with a dual-path modulated-interactive structure to recover high-quality sharp images in conditions of near absence of light, dubbed CollaBA, which learns the direct mapping from low-resolution dark-light images to their high-resolution normal sharp version. Specifically, we construct the generative modulation prior, serving as illuminance attention information, to regulate the exposure level of the neighborhood range. In addition, we construct an interactive degradation removal branch that progressively embeds the generated intrinsic prior to recover high-frequency detail and contrast at the feature level. We also introduce a multi-substrate up-scaler to integrate multi-scale sampling features, effectively addressing artifact-related problems. Rather than adopting the naive time-consuming learning strategy, we design a novel bi-level implicit adversarial learning mechanism as our fast training strategy. Extensive experiments on benchmark datasets — demonstrate our model’s wide-ranging applicability in various ultra-low-light scenarios, across 8 key performance metrics with significant improvements, notably achieving a 35.8% improvement in LPIPS and a 23.1% increase in RMSE. The code will be available at https://github.com/moriyaya/CollaBA.}
}
@article{XIA2024110629,
title = {MetaTKG++: Learning evolving factor enhanced meta-knowledge for temporal knowledge graph reasoning},
journal = {Pattern Recognition},
volume = {155},
pages = {110629},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110629},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003807},
author = {Yuwei Xia and Mengqi Zhang and Qiang Liu and Liang Wang and Shu Wu and Xiaoyu Zhang and Liang Wang},
keywords = {Knowledge extraction, Temporal knowledge graph, Meta-learning, Evolution pattern},
abstract = {Reasoning over Temporal Knowledge Graphs (TKGs) aims to predict future facts based on the given history. One of the key challenges for prediction is to analyze the evolution process of facts. Most existing works focus on exploring evolutionary information in history to obtain effective temporal embeddings for entities and relations, but they ignore the variation in evolution patterns of facts caused by numerous diverse entities and latent evolving factors, which makes them struggle to adapt to future data with different evolution patterns. Moreover, new entities continue to emerge along with the evolution of facts over time. Since existing models highly rely on historical information to learn embeddings for entities, they perform poorly on such entities with little historical information. To tackle these issues, we propose a novel evolving factor enhanced temporal meta-learner framework for TKG reasoning, MetaTKG++ for brevity. Specifically, we first propose a temporal meta-learner which regards TKG reasoning as many temporal meta-tasks for training. From the training process of each meta-task, the obtained meta-knowledge can guide backbones to adapt to future data exhibiting various evolution patterns and to effectively learn entities with little historical information. Then, we design an Evolving Factor Learning module, which aims to assist backbones in learning evolution patterns by modeling latent evolving factors. Meanwhile, during the training process with the proposed meta-learner, the learnable evolving factor can enhance the meta-knowledge with providing more comprehensive information on learning evolution patterns. Extensive experiments on five widely-used datasets and four backbones demonstrate that our method can greatly improve the performance on TKG prediction.}
}
@article{XU2024110590,
title = {An efficient blur kernel estimation method for blind image Super-Resolution},
journal = {Pattern Recognition},
volume = {154},
pages = {110590},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110590},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003418},
author = {Yimin Xu and Nanxi Gao and Fei Chao and Rongrong Ji},
keywords = {Blind super-resolution reconstruction, Efficient inference, Kernel detection and reconstruction},
abstract = {Many existing space-variant blind image Super-Resolution (SR) methods require the generation of blur kernels for all input pixels. However, the existing methods overlook the possibility of similar blur kernel patterns among adjacent pixels, leading to spatial incoherence and wasteful computational resources. In this study, we introduce an efficient two-stage method for estimating blur kernels. Instead of generating pixel-wise kernels, a limited set of kernels at pixels with distinct information are generated first and the remaining kernels are reconstructed based on this initial set. This novel method, referred to as Anchor Detection and Kernel Reconstruction, comprises two main components: an Anchor Detection Module (ADM) and a Kernel Reconstruction Module (KRM). The objective of ADM is to identify pixels in a given Low-Resolution image that contain rich information, which are called anchors. The corresponding blur kernels, denoted as anchor kernels, are then generated for these identified pixels using a complete backbone network. The remaining blur kernels are reconstructed using KRM with a lightweight interpolation method based on the anchor kernels to enhance spatial consistency among the reconstructed pixels. Extensive experiments demonstrate that the proposed ADKR method maintains comparable performance while estimating only 50% of blur kernels with a full backbone network, reaching approximately 20% reduction in FLOPs. The code has been made available at https://github.com/xuyimin0926/ADKR.}
}
@article{ZHU2024110545,
title = {DynamicKD: An effective knowledge distillation via dynamic entropy correction-based distillation for gap optimizing},
journal = {Pattern Recognition},
volume = {153},
pages = {110545},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110545},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324002966},
author = {Songling Zhu and Ronghua Shang and Bo Yuan and Weitong Zhang and Wenjie Li and Yangyang Li and Licheng Jiao},
keywords = {Convolutional neural networks, Knowledge distillation, CNN compression, CNN acceleration},
abstract = {The knowledge distillation uses a high-performance teacher network to guide the student network. However, the performance gap between the teacher and student networks can affect the student’s training. This paper proposes a novel knowledge distillation algorithm based on dynamic entropy correction, which adjusts the student instead of the teacher to reduce the gap. Firstly, the effect of changing the output entropy (short for output information entropy) on the distillation loss in the student is analyzed in theory. This paper shows that correcting the output entropy can reduce the gap. Then, a knowledge distillation algorithm based on dynamic entropy correction is created, which can correct the output entropy in real-time with an entropy controller updated dynamically by the distillation loss. The proposed algorithm is validated on the CIFAR100, ImageNet, and PASCAL VOC 2007. The comparison with various state-of-the-art distillation algorithms shows impressive results, especially in the experiment on the CIFAR100 regarding teacher–student pair resnet32x4–resnet8x4. The proposed algorithm raises 2.64 points over the traditional distillation algorithm and 0.87 points over the state-of-the-art algorithm CRD in classification accuracy, demonstrating its effectiveness and efficiency.}
}
@article{YOO2024110602,
title = {Looking beyond input frames: Self-supervised adaptation for video super-resolution},
journal = {Pattern Recognition},
volume = {154},
pages = {110602},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110602},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003534},
author = {Jinsu Yoo and Jihoon Nam and Sungyong Baik and Tae Hyun Kim},
keywords = {Video super-resolution, Test-time adaptation, Knowledge distillation, Patch-recurrence},
abstract = {Recent test-time adaptive video super-resolution (VSR) methods have elevated the performance by exploiting the self-similar patches within the low-resolution (LR) frames to adapt to given input frames. However, the LR frames contain a limited amount of such patches, limiting the performance of such adaptation methods, especially for a challenging scale factor (e.g., ×4). In this work, we propose to explore beyond the input LR frames. In particular, we observe that a greater amount of self-similar patches across various scales can be found from estimated high-resolution (HR) frames (i.e., initially restored frames) produced by a pre-trained VSR network. Upon the observation, we propose a new self-supervision test-time adaptation approach via self-distillation to exploit such rich amount of self-similar patches from initially restored frames. Specifically, we perform self-distillation by exploiting multi-scale relationship: distilling knowledge from larger patches to smaller ones with similar semantics. Our framework is flexible and effective as the knowledge can be distilled either from the network itself or the larger one. Furthermore, our framework demonstrates the robustness, being able to recover from undesirable artifacts present in initially restored frames. Extensive evaluation with various VSR networks on numerous datasets reveals that our algorithm consistently improves the restoration quality by a large margin without ground-truth HR video frames. Code is available at: https://github.com/jinsuyoo/bissa.}
}
@article{QIU2024110550,
title = {Video anomaly detection guided by clustering learning},
journal = {Pattern Recognition},
volume = {153},
pages = {110550},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110550},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003017},
author = {Shaoming Qiu and Jingfeng Ye and Jiancheng Zhao and Lei He and Liangyu Liu and Bicong E. and Xinchen Huang},
keywords = {Video anomaly detection, Spatial–temporal cascade auto-encoder, Clustering learning, Memory-guided},
abstract = {With the fuzzy boundary between normal and abnormal video data, which cannot be well distinguished by most methods, anomaly detection in video requires better characterization of the data. First we give a convolution-enhanced self-attentive video auto-encoder based on the U-Net architecture, which can extract richer image features. Secondly we design a dual-scale feature clustering structure for this encoder, which simultaneously compresses the channel and spatial structure features of the image to represent the features to obtain good coding characteristics and expand the boundary between normal and abnormal data. We also verify that our approach is equivalent to a class of auto-encoders for memory-guided learning. Finally, in the reconstruction task, since video auto-encoders are capable of triggering temporal time leakage phenomena that can lead to network performance degradation, we propose an anomaly score computation paradigm for video auto-encoders that utilizes the average frame anomaly score of a video clip to compute the first frame anomaly score in that video clip. Extensive experiments on three benchmark datasets show that our method outperforms most existing methods on large datasets with complex patterns. The code will be published at the following link: Anomaly-detection-guided-by-clustering-learning}
}
@article{WANG2024110638,
title = {Reconstruction flow recurrent network for compressed video quality enhancement},
journal = {Pattern Recognition},
volume = {155},
pages = {110638},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110638},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003893},
author = {Zhengning Wang and Xuhang Liu and Chuan Wang and Ting Jiang and Tianjiao Zeng and Zhenni Zeng and Guoqing Wang and Shuaicheng Liu},
keywords = {Video coding, Video quality enhancement, Flow reconstruction},
abstract = {We present a reconstruction flow for the task of compressed video quality enhancement (VQE). Compressed videos often suffer from various coding artifacts, such as blocking and blurring, especially under low bit-rate. VQE aims to suppress these artifacts to improve the visual quality. Frame similarity can be utilized to enhance low-quality frames given their neighboring high-quality frames, for which motion estimation becomes important. Previous approaches often calculate optical flow for the motion compensation. On the other hand, video coding contains a rich set of block motion vectors, forming a coding flow, which may or may not correspond to the scene motion, but to places that deliver the minimum compression error. In contrast, such a valuable coding flow has always been ignored in VQE previously. In this work, we combine these two motion sources into a new flow, namely reconstruction flow, for the purpose of high-quality VQE. Specifically, we estimate optical flows from RGB frames and extract coding flows from coding streams, which are then merged by a fusion module to generate reconstruction flow. Besides, our network is built upon a recurrent network to utilize global temporal information. The deep features are warped according to the reconstruction flow and fed into the subsequent reconstruction module with spatial-variant kernel attention. Our method is evaluated on the leading MFQE2.0 dataset, which demonstrates superior performances when compared to the existing state-of-the-art methods.}
}
@article{WANG2024110586,
title = {Multiscale collaborative representation for face recognition via class-information fusion},
journal = {Pattern Recognition},
volume = {154},
pages = {110586},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110586},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003376},
author = {Changzhong Wang and Shibing Pei and Xiang Lv and Weiping Ding},
keywords = {Face recognition, Collaborative representation, Multi-scale patch, Information fusion, Class information},
abstract = {One of the most challenging issues in face recognition is having only a limited number of training images. Multiscale patch collaborative representation (MSPCRC) is an effective approach to address this problem. However, the existing MSPCRC methods only defined a single patch-scale weight vector for all classes to indicate the importance of different patch scales, ignoring the role of class information when fusing multiscale recognition results. In this work, we consider the effect of class information on face recognition and propose a novel multiscale collaborative-representation face recognition method. Specifically, we first construct the multiscale decision matrices of image subsets from different classes according to patch collaborative representation, and define a patch-scale weight vector for each class to describe the importance of different patch scales. Each element in a scale-weight vector represents the weight value of a certain scale in the corresponding class. Then, we construct the respective optimization objective function for each class, which takes into account the classification information both from the same class and from different classes. Finally, we propose a multiscale fusion-recognition output rule based on the patch-weight vectors. Experimental results demonstrate that the proposed method enhances classification accuracy by approximately 2% to 5% across multiple datasets, surpassing the majority of the competing methods.}
}
@article{WANG2024110614,
title = {MLENet: Multi-Level Extraction Network for video action recognition},
journal = {Pattern Recognition},
volume = {154},
pages = {110614},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110614},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003650},
author = {Fan Wang and Xinke Li and Han Xiong and Haofan Mo and Yongming Li},
keywords = {Action recognition, Spatio-temporal, Temporal feature refinement extraction module, Motion information, Optical flow guided feature},
abstract = {Human action recognition is a well-established task in the field of computer vision. However, accurately representing spatio-temporal information remains a challenge due to the complex interplay between human actions, video timing, and scene changes. To address this challenge and improve the efficiency of temporal modeling in videos, we propose MLENet, a novel approach that eliminates contextual data and eliminates the need for laborious optical flow extraction.MLENet incorporates a Temporal Feature Refinement Extraction Module (TFREM) that utilizes Optical Flow Guided Features to enhance attention to local deep detail information. This refinement process significantly enhances the network’s capacity for feature learning and expression. Moreover, MLENet is designed to be trained end-to-end, facilitating seamless integration into existing frameworks. Additionally, our model adopts a temporal segmentation structure for sampling, effectively reducing redundant information and improving computational efficiency. Compared to existing video-based action recognition models that require optical flow or other modalities, MLENet achieves substantial performance enhancements while requiring fewer inputs. We validate the effectiveness of our proposed approach on benchmark datasets, including Something-Something V1&V2, UCF-101, and HMDB-51, where MLENet consistently outperforms state-of-the-art models.}
}
@article{HUANG2024110562,
title = {Granular3D: Delving into multi-granularity 3D scene graph prediction},
journal = {Pattern Recognition},
volume = {153},
pages = {110562},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110562},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003133},
author = {Kaixiang Huang and Jingru Yang and Jin Wang and Shengfeng He and Zhan Wang and Haiyan He and Qifeng Zhang and Guodong Lu},
keywords = {3D point cloud, 3D semantic scene graph prediction, Multi-granularity, Gather point transformer},
abstract = {This paper addresses the significant challenges in 3D Semantic Scene Graph (3DSSG) prediction, essential for understanding complex 3D environments. Traditional approaches, primarily using PointNet and Graph Convolutional Networks, struggle with effectively extracting multi-grained features from intricate 3D scenes, largely due to a focus on global scene processing and single-scale feature extraction. To overcome these limitations, we introduce Granular3D, a novel approach that shifts the focus towards multi-granularity analysis by predicting relation triplets from specific sub-scenes. One key is the Adaptive Instance Enveloping Method (AIEM), which establishes an approximate envelope structure around irregular instances, providing shape-adaptive local point cloud sampling, thereby comprehensively covering the contextual environments of instances. Moreover, Granular3D incorporates a Hierarchical Dual-Stage Network (HDSN), which differentiates and processes features of instances and their pairs at varying scales, leading to a targeted prediction of instance categories and their relationships. To advance the perception of sub-scene in HDSN, we design a Gather Point Transformer structure (GaPT) that enables the combinatorial interaction of local information from multiple point cloud sets, achieving a more comprehensive local contextual feature extraction. Extensive evaluations on the challenging 3DSSG benchmark demonstrate that our methods provide substantial improvements, establishing a new state-of-the-art in 3DSSG prediction, boosting the top-50 triplet accuracy by ＋2.8%.}
}
@article{FLORIS2024110557,
title = {Composite convolution: A flexible operator for deep learning on 3D point clouds},
journal = {Pattern Recognition},
volume = {153},
pages = {110557},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110557},
url = {https://www.sciencedirect.com/science/article/pii/S003132032400308X},
author = {Alberto Floris and Luca Frittoli and Diego Carrera and Giacomo Boracchi},
keywords = {3D point clouds, Deep learning, Convolution, Anomaly detection},
abstract = {Deep neural networks require specific layers to process point clouds, as the scattered and irregular location of 3D points prevents the use of conventional convolutional filters. We introduce the composite layer, a flexible and general alternative to the existing convolutional operators that process 3D point clouds. We design our composite layer to extract and compress the spatial information from the 3D coordinates of points and then combine this with the feature vectors. Compared to mainstream point-convolutional layers such as ConvPoint and KPConv, our composite layer guarantees greater flexibility in network design and provides an additional form of regularization. To demonstrate the generality of our composite layers, we define both a convolutional composite layer and an aggregate version that combines spatial information and features in a nonlinear manner, and we use these layers to implement CompositeNets. Our experiments on synthetic and real-world datasets show that, in both classification, segmentation, and anomaly detection, our CompositeNets outperform ConvPoint, which uses the same sequential architecture, and achieve similar results as KPConv, which has a deeper, residual architecture. Moreover, our CompositeNets achieve state-of-the-art performance in anomaly detection on point clouds. Our code is publicly available at https://github.com/sirolf-otrebla/CompositeNet.}
}