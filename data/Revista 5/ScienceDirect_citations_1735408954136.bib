@article{SUN2022108845,
title = {Iterative structure transformation and conditional random field based method for unsupervised multimodal change detection},
journal = {Pattern Recognition},
volume = {131},
pages = {108845},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108845},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003260},
author = {Yuli Sun and Lin Lei and Dongdong Guan and Junzheng Wu and Gangyao Kuang},
keywords = {Unsupervised change detection, KNN graph, Image transformation, Multimodal, Conditional random field},
abstract = {Change detection between heterogeneous images has become an increasingly interesting research topic in remote sensing. The different appearances and statistics of heterogeneous images bring great challenges to this task. In this paper, we propose an unsupervised iterative structure transformation and conditional random field (IST-CRF) based multimodal change detection (MCD) method, combining an imaging modality-invariant based structure transformation method with a random filed framework specifically designed for MCD, to acquire an optimal change map within a global probabilistic model. IST-CRF first constructs graphs to represent the structures of the images, and transforms the heterogeneous images to the same differential domain by using graph based forward and backward structure transformations. Then, the change vectors are calculated to distinguish the changed and unchanged areas. Finally, in order to classify the change vectors and compute the binary change map, a CRF model is designed to fully explore the spectral-spatial information, which incorporates the change information, local spatially-adjacent neighbor information, and global spectrally-similar neighbor information with a random field framework. As the changed samples will influence the structure transformation and reduce the quality of change vectors, we use an iterative framework to propagate the CRF segmentation results back to the structure transformation process that removes the changed samples, and thus improve the accuracy of change detection. Experiments conducted on different real data sets show the effectiveness of IST-CRF. Source code of the proposed method will be made available at https://github.com/yulisun/IST-CRF.}
}
@article{ZHANG2022108877,
title = {SO-softmax loss for discriminable embedding learning in CNNs},
journal = {Pattern Recognition},
volume = {131},
pages = {108877},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108877},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003582},
author = {Qiang Zhang and Jibin Yang and Xiongwei Zhang and Tieyong Cao},
keywords = {Convolutional neural networks, Cosine similarity, Cross entropy loss, Quadratic transformation, Embedding learning, Softmax},
abstract = {Convolutional neural networks (CNNs)-based classifiers, trained with the softmax cross-entropy loss, have achieved remarkable success in learning embeddings for pattern recognition. The cosine similarity-based softmax variants further improve the performance by focusing on optimizing the angles between embeddings and class weights. However, embeddings learned by these variants still have significant intra-class variances since these methods only optimize the relative differences between intra- and inter-class cosine similarities. To simultaneously optimize intra- and inter-class cosine similarities, this paper proposes a cosine Similarity Optimization-based softmax (SO-softmax) loss, which is based on a generalized softmax loss formulation that combines both similarities. The proposed loss constrains the intra-class (positive) and inter-class (negative) cosine similarity by quadratic transformations, thus making the embedding representation more compact within classes and more distinguishable between classes. It is verified theoretically that SO-softmax loss can optimize both the similarities simultaneously. Thorough experiments are conducted on typical audio classification, image classification, face verification, image retrieval, and person re-identification tasks, and the results show that SO-softmax loss outperforms the state-of-the-art loss functions in CNNs-based frameworks.}
}
@article{PEI2022108825,
title = {Multi-scale attention-based pseudo-3D convolution neural network for Alzheimer’s disease diagnosis using structural MRI},
journal = {Pattern Recognition},
volume = {131},
pages = {108825},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108825},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003065},
author = {Zhao Pei and Zhiyang Wan and Yanning Zhang and Miao Wang and Chengcai Leng and Yee-Hong Yang},
keywords = {Diagnosis of Alzheimer’s disease, Pseudo-3D, Attention mechanism, Multi-scale, Joint loss function},
abstract = {Recently, deep learning based Computer-Aided Diagnosis methods have been widely utilized due to their highly effective diagnosis of patients. Although Convolutional Neural Networks (CNNs) are capable of extracting the latent structural characteristics of dementia and of capturing the changes of brain anatomy in Magnetic Resonance Imaging (MRI) scans, the high-dimensional input to a deep CNN usually makes the network difficult to train, and affects its diagnostic accuracy. In this paper, a novel method called the hierarchical pseudo-3D convolution neural network based on a kernel attention mechanism with a new global context block, which is abbreviated as “PKG-Net”, is proposed to accurately predict Alzheimer’s disease even when the input features are complex. Specifically, the proposed network first extracts multi-scale features from pre-processed images. Second, the attention mechanism and global context blocks are applied to combine features from different layers to hierarchically transform the MRI into more compact high-level features. Then, a joint loss function is used to train the proposed network to generate more distinguishing features, which improve the generalization performance of the network. In addition, we combine our method with different architectures. Extensive experiments are conducted to analyze the performance of the PKG-Net with different hyper-parameters and architectures. Finally, in order to verify the effectiveness of our method on Alzheimer’s disease diagnosis, we carry out extensive experiments on the ADNI dataset, and compare the results of our method with that of existing methods in terms of accuracy, recall and precision. Furthermore, our network can fully take advantage of the deep 3D convolutional neural network for automatic feature extraction and representation, and thus can avoid the limitation of low processing efficiency caused by the preprocessing procedure in which a specific area needs to be annotated in advance. Finally, we evaluate our proposed framework using two public datasets, ADNI-1 and ADNI-2, and the experimental results show that our proposed framework can achieve superior performance over state-of-the-art approaches.}
}
@article{LOPEZLOPEZ2022108885,
title = {Incremental Learning from Low-labelled Stream Data in Open-Set Video Face Recognition},
journal = {Pattern Recognition},
volume = {131},
pages = {108885},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108885},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003661},
author = {Eric Lopez-Lopez and Xose M. Pardo and Carlos V. Regueiro},
keywords = {Open-set face recognition, Incremental Learning, Self-updating, Adaptive biometrics, Video-surveillance},
abstract = {Deep Learning approaches have brought solutions, with impressive performance, to general classification problems where wealthy of annotated data are provided for training. In contrast, less progress has been made in continual learning of a set of non-stationary classes, mainly when applied to unsupervised problems with streaming data. Here, we propose a novel incremental learning approach which combines a deep features encoder with an Open-Set Dynamic Ensembles of SVM, to tackle the problem of identifying individuals of interest (IoI) from streaming face data. From a simple weak classifier trained on a few video-frames, our method can use unsupervised operational data to enhance recognition. Our approach adapts to new patterns avoiding catastrophic forgetting and partially heals itself from miss-adaptation. Besides, to better comply with real world conditions, the system was designed to operate in an open-set setting. Results show a benefit of up to 15% F1-score increase respect to non-adaptive state-of-the-art methods.}
}
@article{HU2022108893,
title = {Learning deep morphological networks with neural architecture search},
journal = {Pattern Recognition},
volume = {131},
pages = {108893},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108893},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003740},
author = {Yufei Hu and Nacim Belkhir and Jesus Angulo and Angela Yao and Gianni Franchi},
keywords = {Mathematical morphology, Deep learning, Architecture search, Edge detection, Semantic segmentation},
abstract = {Deep Neural Networks (DNNs) are generated by sequentially performing linear and non-linear processes. The combination of linear and non-linear procedures is critical for generating a sufficiently deep feature space. Most non-linear operators are derivations of activation functions or pooling functions. Mathematical morphology is a branch of mathematics that provides non-linear operators for various image processing problems. This paper investigates the utility of integrating these operations into an end-to-end deep learning framework. DNNs are designed to acquire a realistic representation for a particular job. Morphological operators give topological descriptors that convey salient information about the shapes of objects depicted in images. We propose a method based on meta-learning to incorporate morphological operators into DNNs. The learned architecture demonstrates how our novel morphological operations significantly increase DNN performance on various tasks, including picture classification, edge detection, and semantic segmentation. Our codes are available at https://nao-morpho.github.io/.}
}
@article{WANG2022108841,
title = {An entity-weights-based convolutional neural network for large-sale complex knowledge embedding},
journal = {Pattern Recognition},
volume = {131},
pages = {108841},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108841},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003223},
author = {Zhengdi Wang and Lvqing Yang and Zhenfeng Lei and Anwar {Ul Haq} and Defu Zhang and Shuangyuan Yang and Akindipe Olusegun Francis},
keywords = {Graph-based finance, Representation learning, Complete incidence matrix, Convolutional neural network, Matrix factorization},
abstract = {Knowledge graph (KG) has increasingly been seen as a significant resource in financial applications (e.g., risk control, auditing and anti-fraud). However, there are few prior studies that focus on multi-relational circles, extracting additional information under the completed KG and selecting similarity measures for knowledge representation. In this paper, we introduce multi-relational circles and propose a novel embedding model, which considers entity weights calculated by PageRank algorithm to improve TransE method. In order to extract additional information, we use entity weights to convert embeddings into an on-map mining problem, and propose a model called CNNe based on entity weights and a convolutional neural network with three hidden layers, which converts vectors of entities, entity weights and relationships into matrices to perform link prediction in the same way as image processing. With the help of ten different similarity measures, it is demonstrated that the choice of distance measure greatly effect the results of the translation embedding models. Moreover, we propose two embedding methods, sMFE and tMFE, to enhance the results using matrix factorization. The complete incidence matrix is first applied to knowledge embedding, which contains the most comprehensive topological properties of the graph. Experimental results on standard benchmark datasets demonstrate that the proposed models are effective. In particular, CNNe achieves a mean rank of 166 less than the baseline method and an improvement of 2.1% on the proportion of correct entities ranked in the top ten on YAGO3-10 dataset.}
}
@article{YANG2022108836,
title = {To Actively Initialize Active Learning},
journal = {Pattern Recognition},
volume = {131},
pages = {108836},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108836},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200317X},
author = {Yazhou Yang and Marco Loog},
keywords = {active learning, active initialization, nearest neighbor criterion, minimum nearest neighbor distance},
abstract = {Though much effort has been spent on designing new active learning algorithms, little attention has been paid to the initialization problem of active learning, i.e., how to find a set of labeled samples which contains at least one instance per category. This work identifies the initialization of active learning as a separate and novel research problem, reviews existing methods that can be adapted to be used for this task and, in addition, proposes a new active initialization criterion: the Nearest Neighbor Criterion. Experiments on 16 benchmark datasets verify that the novel method often finds an initialization set with fewer queried samples than other methods do.}
}
@article{CHEN2022108849,
title = {Symbolic sequence representation with Markovian state optimization},
journal = {Pattern Recognition},
volume = {131},
pages = {108849},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108849},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003302},
author = {Lifei Chen and Haiyan Wu and Wenxuan Kang and Shengrui Wang},
keywords = {Sequence representation, Hidden Markov model, State clustering, Hierarchical model selection, Activity recognition},
abstract = {Sequence representation, which is aimed at embedding sequentially symbolic data in a real space, is a foundational task in sequence pattern recognition. It is a difficult problem due to the challenges entailed in learning the intrinsic structural features within sequences in small sample size cases, in an unsupervised way. In this paper, we propose to represent each symbolic sequence by its transition probability distribution over discriminating topics, formalized by a set of optimized Hidden Markov Model (HMM) states shared by all sequences. An efficient method, called Markovian state clustering with hierarchical model selection, is proposed to optimize the Markovian states and to adaptively determine the number of topics. The proposed method is experimentally evaluated on human activity recognition and protein recognition, and results obtained demonstrate its effectiveness and efficiency.}
}
@article{WU2022108865,
title = {Inter-Attribute awareness for pedestrian attribute recognition},
journal = {Pattern Recognition},
volume = {131},
pages = {108865},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108865},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003466},
author = {Junyi Wu and Yan Huang and Zhipeng Gao and Yating Hong and Jianqiang Zhao and Xinsheng Du},
keywords = {Pedestrian attribute recognition, Inter-Attribute awareness, Vector-Neuron capsules},
abstract = {The task of pedestrian attribute recognition (PAR) is to distinguish a series of person semantic attributes. Generally, existing methods adopt multi-label classification algorithms to tackle the PAR task by utilizing multiple attribute labels. Despite remarkable progress, this kind of method normally ignores relations between different attributes. In order to be aware of relations between attributes, we propose an inter-attribute aware network via vector-neuron capsule for PAR (IAA-Caps). Our IAA-Caps method replaces traditional one-dimensional scalar neurons with two-dimensional vector-neuron capsules by embedding them in IAA-Caps. Specifically, during IAA-Caps training, one dimension in capsules is used to recognize different attributes, and the other dimension is used to strengthen the relations of different attributes. Through considering inter-attribute relations, compared with previous methods that use a heavyweight backbone (e.g., ResNet50 or BN-Inception), a more lightweight backbone (i.e., OSNet) can be adopted in our proposed IAA-Caps to achieve better performance. Experiments are conducted on several PAR benchmark datasets, including PETA, PA-100K, RAPv1, and RAPv2, demonstrating the effectiveness of the proposed IAA-Caps. In addition, experiments also show that the proposed method can improve the performance of PAR on different backbones, showing its generalization ability.}
}
@article{ZHU2022108897,
title = {Adaptive aggregation-distillation autoencoder for unsupervised anomaly detection},
journal = {Pattern Recognition},
volume = {131},
pages = {108897},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108897},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003788},
author = {Jiaqi Zhu and Fang Deng and Jiachen Zhao and Jie Chen},
keywords = {Anomaly detection, Aggregation-distillation mechanism, Autoencoders, Unsupervised learning},
abstract = {Anomaly detection (AD) has been receiving great attention as it plays a crucial role in many areas of basic research and industrial applications. However, most existing AD methods not only rely on training on normal data, but also ignore the multi-cluster nature of normal and abnormal patterns. To overcome these limitations, this paper proposes a novel method called Adaptive Aggregation-Distillation AutoEncoder (AADAE) for unsupervised anomaly detection. AADAE is built upon the density-based landmark selection in respect to representing diverse normal patterns. During training, AADAE adaptively updates the location and quantity of landmarks. Then, an aggregation-distillation mechanism is constructed: Firstly, it aggregates the latent representations of normal and anomalous to different landmark-guided regions within the convex polygon with landmarks as vertices, which minimizes the intra-class variation and promotes the separability of normal and abnormal samples. Secondly, the distillation mechanism is applied to obtain reliable detection results when there are anomalies in the training set. The aggregation process motivates AADAE to learn the distribution of multi-cluster normal samples with the help of landmarks, which in turn facilitates the distillation process to differentiate normal from anomalies for training. Extensive empirical studies on ten datasets from different application domains demonstrate the efficiency and generalization ability of the method.}
}
@article{NING2022108873,
title = {HCFNN: High-order coverage function neural network for image classification},
journal = {Pattern Recognition},
volume = {131},
pages = {108873},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108873},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003545},
author = {Xin Ning and Weijuan Tian and Zaiyang Yu and Weijun Li and Xiao Bai and Yuebao Wang},
keywords = {DNNs, Neuron modeling, Heuristic algorithm, Back propagation, Computer vision},
abstract = {Recent advances in deep neural networks (DNNs) have mainly focused on innovations in network architecture and loss function. In this paper, we introduce a flexible high-order coverage function (HCF) neuron model to replace the fully-connected (FC) layers. The approximation theorem and proof for the HCF are also presented to demonstrate its fitting ability. Unlike the FC layers, which cannot handle high-dimensional data well, the HCF utilizes weight coefficients and hyper-parameters to mine underlying geometries with arbitrary shapes in an n-dimensional space. To explore the power and potential of our HCF neuron model, a high-order coverage function neural network (HCFNN) is proposed, which incorporates the HCF neuron as the building block. Moreover, a novel adaptive optimization method for weights and hyper-parameters is designed to achieve effective network learning. Comprehensive experiments on nine datasets in several domains validate the effectiveness and generalizability of the HCF and HCFNN. The proposed method provides a new perspective for further developments in DNNs and ensures wide application in the field of image classification. The source code is available at https://github.com/Tough2011/HCFNet.git}
}
@article{PARK2022108905,
title = {Wasserstein approximate bayesian computation for visual tracking},
journal = {Pattern Recognition},
volume = {131},
pages = {108905},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108905},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003867},
author = {Jinhee Park and Junseok Kwon},
abstract = {In this study, we present novel visual tracking methods based on the Wasserstein approximate Bayesian computation (ABC). For visual tracking, the proposed Wasserstein ABC (WABC) method approximates the likelihood within the Wasserstein space more accurately than the conventional ABC methods by directly measuring the discrepancy between the likelihood distributions. To encode the temporal dependency among time-series likelihood distributions, we extend the WABC method to the time-series WABC (TWABC) method. Subsequently, the proposed Hilbert TWABC (HTWABC) method reduces the computational costs caused by the TWABC method while substituting the original Wasserstein distance with the Hilbert distance. Experimental results demonstrate that the proposed visual trackers outperform other state-of-the-art visual tracking methods quantitatively. Moreover, ablation studies verify the effectiveness of individual components consisting of the proposed method (e.g., the Wasserstein distance, curve matching, and Hilbert metric).}
}
@article{WU2022108884,
title = {Cross-view panorama image synthesis with progressive attention GANs},
journal = {Pattern Recognition},
volume = {131},
pages = {108884},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108884},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200365X},
author = {Songsong Wu and Hao Tang and Xiao-Yuan Jing and Jianjun Qian and Nicu Sebe and Yan Yan and Qinghua Zhang},
keywords = {Progressive attention GANs, Cross-view panorama image synthesis, Cross-stage attention, Orientation-aware data augmentation, Multi-stage image generation},
abstract = {Despite the significant progress of conditional image generation, it remains difficult to synthesize a ground-view panorama image from a top-view aerial image. Among the core challenges are the vast differences in image appearance and resolution between aerial images and panorama images, and the limited aside information available for top-to-ground viewpoint transformation. To address these challenges, we propose a new Progressive Attention Generative Adversarial Network (PAGAN) with two novel components: a multistage progressive generation framework and a cross-stage attention module. In the first stage, an aerial image is fed into a U-Net-like network to generate one local region of the panorama image and its corresponding segmentation map. Then, the synthetic panorama image region is extended and refined through the following generation stages with our proposed cross-stage attention module that passes semantic information forward stage-by-stage. In each of the successive generation stages, the synthetic panorama image and segmentation map are separately fed into an image discriminator and a segmentation discriminator to compute both later real and fake, as well as feature alignment score maps for discrimination. The model is trained with a novel orientation-aware data augmentation strategy based on the geometric relation between aerial and panorama images. Extensive experimental results on two cross-view datasets show that PAGAN generates high-quality panorama images with more convincing details than state-of-the-art methods.}
}
@article{WU2022108881,
title = {Complementarity-aware cross-modal feature fusion network for RGB-T semantic segmentation},
journal = {Pattern Recognition},
volume = {131},
pages = {108881},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108881},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003624},
author = {Wei Wu and Tao Chu and Qiong Liu},
keywords = {RGB-T, Cross-modal fusion, Multi-supervision, Semantic segmentation},
abstract = {RGB-T semantic segmentation has attracted growing attention because it makes a model robust towards challenging illumination. Most existing methods fuse RGB and thermal information in an equal manner along spatial dimensions, which results in feature redundancy and affects the discriminability of cross-modal features. In this paper, we propose a Complementarity-aware Cross-modal Feature Fusion Network (CCFFNet) including a Complementarity-Aware Encoder (CAE) and a Three-Path Fusion and Supervision (TPFS). The CAE, which consists of cascaded cross-modal fusion modules, can select complementary information from RGB and thermal features via a novel gate and fuse them by a channel-wise weighting mechanism. TPFS not only iteratively performs Three-Path Fusion (TPF) to further enhance cross-modal features, but also supervise the training of CCFFNet along three branches by Three-Supervision (TS). Extensive experiments are carried out and the results demonstrate that our model outperforms the state-of-the-art models by at least 1.6% mIoU on MFNet dataset and 2.9% mIoU on PST900 dataset, respectively. And a single-modality-based model can be easily applied to multi-modal semantic segmentation when plugging our CAE.}
}
@article{GAO2022108861,
title = {A modified interval type-2 Takagi-Sugeno fuzzy neural network and its convergence analysis},
journal = {Pattern Recognition},
volume = {131},
pages = {108861},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108861},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003429},
author = {Tao Gao and Xiao Bai and Chen Wang and Liang Zhang and Jin Zheng and Jian Wang},
keywords = {IT2 fuzzy model, Fuzzy neural network, Takagi-Sugeno, Conjugate gradient, Convergence},
abstract = {In this paper, to compute the firing strength values of type-2 fuzzy models, a soft version of minimum is presented, which endows the fuzzy model with the ability to solve large dimensional problems. In addition, a conjugate gradient method is borrowed to train the designed interval type-2 Takagi-Sugeno fuzzy model. Compared with the existing gradient-based learning strategy, this scheme can efficiently enhance the fuzzy model performance. Last but not least, convergence analysis for this modified interval type-2 Takagi-Sugeno fuzzy neural network (MIT2TSFNN) is conducted in detail, which proves that the gradient of the error function tends to zero with the iteration increasing (weak convergence) and the sequence of model parameters (weights) convergences to a fixed point (strong convergence). To validate the effectiveness of the proposed MIT2TSFNN and its theoretical results, simulation results of six regression and six classification problems are presented.}
}
@article{QIAN2022108889,
title = {A survey of robust adversarial training in pattern recognition: Fundamental, theory, and methodologies},
journal = {Pattern Recognition},
volume = {131},
pages = {108889},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108889},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003703},
author = {Zhuang Qian and Kaizhu Huang and Qiu-Feng Wang and Xu-Yao Zhang},
keywords = {Adversarial examples, Adversarial training, Robust learning},
abstract = {Deep neural networks have achieved remarkable success in machine learning, computer vision, and pattern recognition in the last few decades. Recent studies, however, show that neural networks (both shallow and deep) may be easily fooled by certain imperceptibly perturbed input samples called adversarial examples. Such security vulnerability has resulted in a large body of research in recent years because real-world threats could be introduced due to the vast applications of neural networks. To address the robustness issue to adversarial examples particularly in pattern recognition, robust adversarial training has become one mainstream. Various ideas, methods, and applications have boomed in the field. Yet, a deep understanding of adversarial training including characteristics, interpretations, theories, and connections among different models has remained elusive. This paper presents a comprehensive survey trying to offer a systematic and structured investigation on robust adversarial training in pattern recognition. We start with fundamentals including definition, notations, and properties of adversarial examples. We then introduce a general theoretical framework with gradient regularization for defending against adversarial samples - robust adversarial training with visualizations and interpretations on why adversarial training can lead to model robustness. Connections will also be established between adversarial training and other traditional learning theories. After that, we summarize, review, and discuss various methodologies with defense/training algorithms in a structured way. Finally, we present analysis, outlook, and remarks on adversarial training.}
}
@article{BESPALOV2022108816,
title = {BRULÈ: Barycenter-Regularized Unsupervised Landmark Extraction},
journal = {Pattern Recognition},
volume = {131},
pages = {108816},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108816},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002977},
author = {Iaroslav Bespalov and Nazar Buzun and Dmitry V. Dylov},
abstract = {Unsupervised retrieval of image features is vital for many computer vision tasks where the annotation is missing or scarce. In this work, we propose a new unsupervised approach to detect the landmarks in images, validating it on the popular task of human face key-points extraction. The method is based on the idea of auto-encoding the wanted landmarks in the latent space while discarding the non-essential information (and effectively preserving the interpretability). The interpretable latent space representation (the bottleneck containing nothing but the wanted key-points) is achieved by a new two-step regularization approach. The first regularization step evaluates transport distance from a given set of landmarks to some average value (the barycenter by Wasserstein distance). The second regularization step controls deviations from the barycenter by applying random geometric deformations synchronously to the initial image and to the encoded landmarks. We demonstrate the effectiveness of the approach both in unsupervised and semi-supervised training scenarios using 300-W, CelebA, and MAFL datasets. The proposed regularization paradigm is shown to prevent overfitting, and the detection quality is shown to improve beyond the state-of-the-art face models.}
}
@article{LU2022108869,
title = {A novel part-level feature extraction method for fine-grained vehicle recognition},
journal = {Pattern Recognition},
volume = {131},
pages = {108869},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108869},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003508},
author = {Lei Lu and Ping Wang and Yijie Cao},
keywords = {Fine-grained recognition, Part-level feature extraction, Feature grouping, Feature fusion},
abstract = {In this paper, we propose a novel part-level feature extraction method to enhance the discriminative ability of deep convolutional features for the task of fine-grained vehicle recognition. Generally, the challenges for fine-grained vehicle recognition are mainly caused by the subtle visual differences between part regions of vehicles. Therefore, it is essential to extract discriminative features from part regions. Many existing methods, especially deep convolutional neural networks (D-CNNs), tend to detect the discriminative part regions explicitly or learn the part information implicitly through network restructuring and neglect the abundant part-level information contained in the high-level features generated by CNNs. In light of this, we propose a simple and effective part-level feature extraction method to enhance the representation of part-level features within the global features of target object generated by the backbone networks. The proposed method is built on the deep convolutional layers from which the discriminative part features could be integrated and extracted accordingly. More specifically, a basic feature grouping module is adopted to integrate the feature maps of deep convolutional layers into groups in each of which the related discriminative parts are assembled. The feature grouping process is performed in a multi-stage manner to ensure the integration process. Then a fusion module follows to model the coarse-to-fine relationship of the part features and further ensure the integrity and effectiveness of the part features. We conduct comparison experiments on public datasets, and the results show that the proposed method achieves comparable performance with state-of-the-art algorithms.}
}
@article{SU2022108868,
title = {DSLA: Dynamic smooth label assignment for efficient anchor-free object detection},
journal = {Pattern Recognition},
volume = {131},
pages = {108868},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108868},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003491},
author = {Hu Su and Yonghao He and Rui Jiang and Jiabin Zhang and Wei Zou and Bin Fan},
keywords = {Convolutional neural network, Object detection, Centerness score, Intersection-of-union},
abstract = {Anchor-free detectors basically formulate object detection as dense classification and regression. For popular anchor-free detectors, it is common to introduce an individual prediction branch to estimate the quality of localization. The following inconsistencies are observed when we delve into the practices of classification and quality estimation. Firstly, for some adjacent samples which are assigned completely different labels, the trained model would produce similar classification scores. This violates the training objective and leads to performance degradation. Secondly, it is found that detected bounding boxes with higher confidences contrarily have smaller overlaps with the corresponding ground-truth. Accurately localized bounding boxes would be suppressed by less accurate ones in the Non-Maximum Suppression (NMS) procedure. To address the inconsistency problems, the Dynamic Smooth Label Assignment (DSLA) method is proposed. Based on the concept of centerness originally developed in FCOS, a smooth assignment strategy is proposed. The label is smoothed to a continuous value in [0,1] to make a steady transition between positive and negative samples. Intersection-of-Union (IoU) is predicted dynamically during training and is coupled with the smoothed label. The dynamic smooth label is assigned to supervise the classification branch. Under such supervision, quality estimation branch is naturally merged into the classification branch, which simplifies the architecture of anchor-free detector. Comprehensive experiments are conducted on the MS COCO benchmark. It is demonstrated that, DSLA can significantly boost the detection accuracy by alleviating the above inconsistencies for anchor-free detectors. Our codes are released at https://github.com/YonghaoHe/DSLA.}
}
@article{YU2022108876,
title = {A novel explainable neural network for Alzheimer’s disease diagnosis},
journal = {Pattern Recognition},
volume = {131},
pages = {108876},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108876},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003570},
author = {Lu Yu and Wei Xiang and Juan Fang and Yi-Ping {Phoebe Chen} and Ruifeng Zhu},
keywords = {Explainable neural networks, XAI, High-resolution heatmap, MRI},
abstract = {Visual classification for medical images has been dominated by convolutional neural networks (CNNs) for years. Though they have shown great performance on accuracy, some of them provide decisions that are hard to explain while others encode information from irrelevant or noisy regions. In this work, we try to close this gap by proposing an explainable framework which consists of a predictor and an explainable tool, so as to provide accurate diagnoses with intuitive visualization maps and prediction basis. Specifically, the predictor is designed by applying attention mechanisms to multi-scale features so as to learn and discover class discriminative latent representations that are close to each brain volume’s label. Meanwhile, to explain our predictor, we propose the novel explainable tool which includes a high-resolution visualization method and a prediction-basis creation and retrieval module. The former effectively integrates the feature maps of intermediate layers as well as the last convolutional layer, which surpasses state-of-the-art visualization approaches in producing high-resolution representations with more accurate localization of discriminative areas. While the latter provides prediction basis evidence via retrieved volumes with similar latent representations which are accessible to neurologists. Extensive experiments show that the proposed framework achieves higher level of accuracy and explainability over other state-of-the-art solutions. More importantly, it localizes crucial brain areas with clearer boundaries, less noises, which matches background knowledge in the neuroscience literature.}
}
@article{2022108938,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {131},
pages = {108938},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(22)00418-6},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004186}
}
@article{REN2022108864,
title = {DARTSRepair: Core-failure-set guided DARTS for network robustness to common corruptions},
journal = {Pattern Recognition},
volume = {131},
pages = {108864},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108864},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003454},
author = {Xuhong Ren and Jianlang Chen and Felix Juefei-Xu and Wanli Xue and Qing Guo and Lei Ma and Jianjun Zhao and Shengyong Chen},
keywords = {Network architecture search, Core-failure-set selection, Robustness enhancement, Differentiable architecture search},
abstract = {Network architecture search (NAS), in particular the differentiable architecture search (DARTS) method, has shown a great power to learn excellent model architectures on the specific dataset of interest. In contrast to using a fixed dataset, in this work, we focus on a different but important scenario for NAS: how to refine a deployed network’s model architecture to enhance its robustness with the guidance of a few collected and misclassified examples that are degraded by some real-world unknown corruptions having a specific pattern (e.g., noise, blur, etc..). To this end, we first conduct an empirical study to validate that the model architectures can be definitely related to the corruption patterns. Surprisingly, by just adding a few corrupted and misclassified examples (e.g., 103 examples) to the clean training dataset (e.g., 5.0×104 examples), we can refine the model architecture and enhance the robustness significantly. To make it more practical, the key problem, i.e., how to select the proper failure examples for the effective NAS guidance, should be carefully investigated. Then, we propose a novel core-failure-set guided DARTS that embeds a K-center-greedy algorithm for DARTS to select suitable corrupted failure examples to refine the model architecture. We use our method for DARTS-refined DNNs on the clean as well as 15 corruptions with the guidance of four specific real-world corruptions. Compared with the state-of-the-art NAS as well as data-augmentation-based enhancement methods, our final method can achieve higher accuracy on both corrupted datasets and the original clean dataset. On some of the corruption patterns, we can achieve as high as over 45% absolute accuracy improvements.}
}
@article{LIANG2022108840,
title = {Video summarization with a convolutional attentive adversarial network},
journal = {Pattern Recognition},
volume = {131},
pages = {108840},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108840},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003211},
author = {Guoqiang Liang and Yanbing Lv and Shucheng Li and Shizhou Zhang and Yanning Zhang},
keywords = {Video summarization, Generative adversarial network, Self attention},
abstract = {With the explosive growth of video data, video summarization, which attempts to seek the minimum subset of frames while still conveying the main story, has become one of the hottest topics. Nowadays, substantial achievements have been made by supervised learning techniques, especially after the emergence of deep learning. However, it is extremely expensive and difficult to construct a large-scale video summarization dataset through human annotation. To address this problem, we propose a convolutional attentive adversarial network (CAAN), whose key idea is to build a deep summarizer in an unsupervised way. Upon the generative adversarial network, our overall framework consists of a generator and a discriminator. The former predicts importance scores for all the frames of a video while the latter tries to distinguish the score-weighted frame features from original frame features. To capture the global and local temporal relationship of video frames, the generator employs a fully convolutional sequence network to build global representation of a video, and an attention-based network to predict normalized importance scores. To optimize the parameters, our objective function is composed of three loss functions, which can guide the frame-level importance score prediction collaboratively. To validate this proposed method, we have conducted extensive experiments on two public benchmarks SumMe and TVSum. The results show the superiority of our proposed method against other state-of-the-art unsupervised approaches. Our method even outperforms some published supervised approaches.}
}
@article{LI2022108872,
title = {Alleviating the estimation bias of deep deterministic policy gradient via co-regularization},
journal = {Pattern Recognition},
volume = {131},
pages = {108872},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108872},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003533},
author = {Yao Li and YuHui Wang and YaoZhong Gan and XiaoYang Tan},
keywords = {Reinforcement learning, Overestimation, Underestimation, Co-training, Deterministic policy gradient},
abstract = {The overestimation in Deep Deterministic Policy Gradients (DDPG) caused by value approximation error may result in unstable policy training. Twin Delayed Deep Deterministic Policy Gradient (TD3) addresses the overestimation but suffers from the underestimation. In this paper, we propose a Co-Regularization based Deep Deterministic (CoD2) policy gradient method to mitigate the estimation bias. Two learners characterized by overestimated and underestimated biases are trained with Co-regularization to achieve this goal. The overestimated and underestimated values are updated conservatively in CoD2 for policy evaluation. Experimental results show that our method achieves comparable performance compared with other methods.}
}
@article{ZHU2022108820,
title = {Multi-granularity episodic contrastive learning for few-shot learning},
journal = {Pattern Recognition},
volume = {131},
pages = {108820},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108820},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003016},
author = {Pengfei Zhu and Zhilin Zhu and Yu Wang and Jinglin Zhang and Shuai Zhao},
keywords = {Multi-granularity computing, Episodic contrastive learning, Few-shot learning, Deep learning},
abstract = {Few-shot learning (FSL) aims at fast adaptation to novel classes with few training samples. Among FSL methods, meta-learning and transfer learning-based methods are the most powerful ones. However, most of them rely to some extent on cross-entropy loss, which leads to representations that are overly concerned with the classes already seen, and in turn leads to sub-optimal generalization on novel classes. In this study, we are inspired by meta-learning and transfer learning-based methods and believe good feature representations are vital for FSL. To this end, we propose a new multi-granularity episodic contrastive learning method (MGECL) that introduces contrastive learning into the episode training process. In particular, by enforcing our proposed contrastive loss on both class and instance granularities, the model is able to extract category-independent discriminative patterns and learn richer and more transferable feature representations. Extensive experiments demonstrate that our proposed method achieves state-of-the-art performance on three popular few-shot benchmarks. Our code is available at https://github.com/z1358/MGECL_PR.}
}
@article{PANG2022108888,
title = {Fast algorithms for incremental and decremental semi-supervised discriminant analysis},
journal = {Pattern Recognition},
volume = {131},
pages = {108888},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108888},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003697},
author = {Wenrao Pang and Gang Wu},
keywords = {Dimensionality reduction, Semi-supervised discriminant analysis, Incremental learning, Decremental learning, Modified total scatter matrix},
abstract = {Incremental and decremental problems are challenging tasks in semi-supervised learning. The incremental semi-supervised discriminant analysis (ISSDA) method proposed by Dhamecha et al. is an efficient method for incremental semi-supervised learning. However, one deficiency of the ISSDA method is that the total scatter matrix remains unchanged during incremental learning, which is impractical in practice. On the other hand, there may be a series of incorrectly artificial labeling in the public data set, and it is interesting to consider the decremental problem in semi-supervised learning. To the best of our knowledge, however, there are few decremental algorithms for semi-supervised discriminant analysis. The contributions of this work are as follows. First, a new incremental semi-supervised discriminant analysis method is proposed, in which we consider updating the total scatter matrix and the between-class scatter matrix simultaneously when new samples are added. Second, we show how to solve the large eigenproblem of the updated total scatter matrix efficiently. Third, we propose two decremental algorithms for semi-supervised discriminant analysis. Numerical experiments demonstrate the superiority of the proposed algorithms over many state-of-the-art algorithms for semi-supervised discriminant analysis.}
}
@article{WANG2022108867,
title = {Shedding light on images: Multi-level image brightness enhancement guided by arbitrary references},
journal = {Pattern Recognition},
volume = {131},
pages = {108867},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108867},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200348X},
author = {Ya’nan Wang and Zhuqing Jiang and Chang Liu and Kai Li and Aidong Men and Haiying Wang and Xiaobo Chen},
keywords = {Low-light image enhancement, Multi-level mapping, Arbitrary references, Codec network, Decomposition, Concatenation},
abstract = {The non-linearity between human perception and image brightness levels results in different definitions of NORMAL-light. Thus, most existing low-light image enhancement methods which produce one-to-one mapping can not meet the aesthetic demand. Other pioneers enhance low-light images guided by a given value. However, the inherent problem of non-linearity will cause poor usability. To this end, we propose a user-friendly neural network for multi-level low-light image enhancement. Inspired by style transfer, our method decomposes an image into content component feature and luminance component feature in the latent space. Then we enhance the image brightness to different levels by concatenating the content components from low-light images and the luminance components from reference images. The network meets various user requirements by selecting different brightness references. Moreover, information except for brightness is preserved to alleviate color distortion. Extensive experiments demonstrate the superiority of our network against existing methods.}
}
@article{ZHAO2022108880,
title = {Self-guided information for few-shot classification},
journal = {Pattern Recognition},
volume = {131},
pages = {108880},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108880},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003612},
author = {Zhineng Zhao and Qifan Liu and Wenming Cao and Deliang Lian and Zhihai He},
keywords = {Few-shot classification, Graph convolution network, Self-guided information},
abstract = {Few-shot classification aims to identify novel categories using only a few labeled samples. Generally, the metric-based few-shot classification methods compare the feature embedding of Query samples (unlabeled samples) with Support samples (labeled samples) in a metric algorithm to predict which category the Query sample belongs to. Obtaining a good feature embedding for each sample in the feature extraction stage can improve the classification accuracy in the metric stage. Based on this, we design the Self-Guided Information Convolution (SGI-Conv), an improved convolution structure, which utilizes the high-level features to guide the network to extract the required discriminative features. To effectively utilize the feature embeddings of samples, we divide the metric network into multiple blocks and build a multi-layer graph convolutional network by sharing adjacent matrices. The multi-layer structure enhances the aggregation ability of graph convolution. Extensive experiments on multiple benchmark datasets demonstrate that our method has achieved competitive results on the few-shot classification tasks.}
}
@article{ZENG2022108896,
title = {Multivariate multi-layer classifier},
journal = {Pattern Recognition},
volume = {131},
pages = {108896},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108896},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003776},
author = {Huanze Zeng and Argon Chen},
keywords = {Classification, Classifiers, Multivariate decision tree, Machine learning, Tree construction},
abstract = {The variance-ratio binary multi-layer classifier (VRBMLC) has been recently proposed and shown to outperform conventional binary decision trees (BDTs). Though effective with better interpretability, the VRBMLC generates deep layers of tree nodes as it employs a one-feature-at-a-time binary split at each layer. To further condense the tree depth and enhance the classification performance, this research proposes a multivariate multi-layer classifier that applies a variance-ratio criterion to enable ternary splits of each tree node and that integrates the oblique discriminant hyperplane in the tree node. We benchmark 16 state-of-the-art univariate and multivariate classifiers on 43 publicly available datasets. The results show that the proposed methods greatly simplify the tree structure and yield a significantly higher average accuracy.}
}
@article{LU2022108844,
title = {Locality preserving projection with symmetric graph embedding for unsupervised dimensionality reduction},
journal = {Pattern Recognition},
volume = {131},
pages = {108844},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108844},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003259},
author = {Xiaohuan Lu and Jiang Long and Jie Wen and Lunke Fei and Bob Zhang and Yong Xu},
keywords = {Dimensionality reduction, Feature extraction, Graph embedding, Unsupervised learning},
abstract = {Preserving the intrinsic structure of data is very important for unsupervised dimensionality reduction. For structure preserving, graph embedding technique is widely considered. However, most of the existing unsupervised graph embedding based methods cannot effectively preserve the intrinsic structure of data since these methods either use the constant graph or only explore the geometric structure based on the distance information or representation information. To solve this problem, a novel method, called locality preserving projection with symmetric graph embedding (LPP_SGE), is proposed. LPP_SGE introduces a novel adaptive graph learning model and can obtain the intrinsic graph and projection in a unified framework by fully exploring the representation information and distance information of the original data. Different from the existing works which generally introduce no less than two constraints to capture the representation information and distance information, LPP_SGE can simultaneously capture the above two kinds of structure information in one term. Moreover, LPP_SGE introduces an ‘l2,1’ norm based projection constraint to select the most discriminative features from the complex data for dimensionality reduction, such that the robustness is enhanced. Experimental results on four databases and two kinds of noisy databases show that LPP_SGE performs better than many well-known methods.}
}
@article{LUONG2022108815,
title = {Multi-layer manifold learning for deep non-negative matrix factorization-based multi-view clustering},
journal = {Pattern Recognition},
volume = {131},
pages = {108815},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108815},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002965},
author = {Khanh Luong and Richi Nayak and Thirunavukarasu Balasubramaniam and Md Abul Bashar},
keywords = {Multi-view data/clustering, Manifold learning, Non-negative Matrix Factorization (NMF), Deep Matrix Factorization (DMF), Deep Non-negative Matrix Factorization (Deep-NMF)},
abstract = {Multi-view data clustering based on Non-negative Matrix Factorization (NMF) has been commonly used for pattern recognition by grouping multi-view high-dimensional data by projecting it to a lower-order dimensional space. However, the NMF framework fails to learn the accurate lower-order representation of the input data if it exhibits complex and non-linear relationships. This paper proposes a deep non-negative matrix factorization-based framework for effective multi-view data clustering by uncovering both the non-linear relationships and the intrinsic components of the data. Both the consensus and complementary information present in multiple views are sufficiently learned in the proposed framework with the effective use of constraints such as normalized cut-type and orthogonal. The optimal manifold of multi-view data is effectively incorporated in all layers of the framework. Extensive experimental results show the proposed method outperforms state-of-the-art multi-view matrix factorization-based methods.}
}
@article{YAN2022108904,
title = {Effective full-scale detection for salient object based on condensing-and-filtering network},
journal = {Pattern Recognition},
volume = {131},
pages = {108904},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108904},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003855},
author = {Xinyu Yan and Meijun Sun and Yahong Han and Zheng Wang and Qi Tian},
keywords = {Salient object detection, Neural networks, Full-scale feature extraction, Multi-level feature fusion},
abstract = {With the development of deep learning, salient object detection methods have made great progress. However, there are still two challenges: 1) The lack of rich features extracted from multiple perspectives at different encoder levels results in the omission of salient objects with varying scales. 2) The ineffective fusion of multi-level features during decoding dilutes the saliency features, which destroys the purity of the predicted maps. In this paper, we design a Condensing-and-Filtering Network (CFNet), in which a saliency pyramid condensing module (SPCM) and a saliency filtering module (SFM) are proposed to solve the above two problems respectively. Specifically, SPCM introduces pyramid convolution as the basic unit to condense full-scale features from global and local perspectives at each level of the encoder. SFM is equipped with an ingenious ‘funnel’ structure to effectively filter multi-level features and supplement details, which makes the fusion of features more robust. The two modules complement each other, so that the full-scale features can be used effectively to predict salient objects. Extensive experimental results on five benchmark datasets demonstrate that our method performs favourably against the state-of-the-art approaches, and also shows superiority in terms of speed (16.18ms) and FLOPs (21.19G). Meanwhile, we extend our CFNet to the task of RGB-D salient object detection and achieve better results, which further demonstrate its effectiveness. The code will be made available.}
}
@article{ZHOU2022108860,
title = {Discovering unknowns: Context-enhanced anomaly detection for curiosity-driven autonomous underwater exploration},
journal = {Pattern Recognition},
volume = {131},
pages = {108860},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108860},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003417},
author = {Yang Zhou and Baihua Li and Jiangtao Wang and Emanuele Rocco and Qinggang Meng},
keywords = {Anomaly detection, Learning unknown objects, Deep learning autoencoder, Autonomous underwater robotics},
abstract = {Discovering unknown objects from visual information as curiosity is highly demanded for autonomous exploration in underwater environment. In this research, we propose an end-to-end deep neural network for anomaly detection in the highly dynamic unstructured underwater background faced by a moving robot. A novel patch-level autoencoder combined with a context-enhanced autoregressive network is introduced to differentiate abnormal patterns (unknowns) from normal ones (knowns) in fine-scale regions. The autoencoder and autoregressive network share the same encoder to extract latent features. The autoregressive branch learns semantic dependence based on conditional probability to identify anomaly in a latent feature space. The overall anomaly score is weighted by both image reconstruction loss and feature similarity loss. The model outperforms state-of-the-art anomaly detection, demonstrated on the benchmark dataset CIFAR-10. Average discrimination performance AUROC improved 2.18%, and inception distance between normal and anomalous classes improved 9.33% in Z-score. The network has been tested using three underwater datasets from underwater simulation, a real-world undersea video and public SUIM data. The AUROC accuracy improved 6.36%, 32.45% and 40.17% respectively by using the proposed patch learning paradigm. It is the first report on unknown detection as navigation clues for curiosity-driven autonomous underwater exploration.}
}
@article{TAN2022108839,
title = {Semi-supervised partial multi-label classification via consistency learning},
journal = {Pattern Recognition},
volume = {131},
pages = {108839},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108839},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200320X},
author = {Anhui Tan and Jiye Liang and Wei-Zhi Wu and Jia Zhang},
keywords = {Semi-supervised partial multi-label learning, Label correlation, HSIC},
abstract = {Partial multi-label learning refers to the problem that each instance is associated with a candidate label set involving both relevant and noisy labels. Existing solutions mainly focus on label disambiguation, while ignoring the negative effect of the inconsistency between feature information and label information. Specifically, the existence of completely unlabeled instances makes the estimation of label co-occurrence difficult. To tackle these problems, we propose a novel framework for partial multi-label learning in semi-supervised scenarios by solving the inconsistency between features and labels. In the first stage, the label-level correlation matrix on both labeled and unlabeled instances is derived via Hilbert-Schmidt Independence Criterion (HSIC). The correlation matrix can characterize the label correlation of labeled instances and can propagate the label correlation of unlabeled instances. In the second stage, the proposed framework achieves the training of feature mapping, the recovery of ground-truth labels, and the alleviation of noisy labels in a mutually beneficial manner, and develops an alternative optimization procedure to optimize them. In addition, a nonlinear version is extended by using kernel trick. Experimental studies demonstrate that the proposed methods can achieve competitive superiority against existing well-established methods.}
}
@article{WANG2022108892,
title = {Improving deep learning on point cloud by maximizing mutual information across layers},
journal = {Pattern Recognition},
volume = {131},
pages = {108892},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108892},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003739},
author = {Di Wang and Lulu Tang and Xu Wang and Luqing Luo and Zhi-Xin Yang},
keywords = {Deep learning, 3D vision, Point clouds, Mutual information},
abstract = {It is a fundamental and vital task to enhance the perception capability of the point cloud learning network in 3D machine vision applications. Most existing methods utilize feature fusion and geometric transformation to improve point cloud learning without paying enough attention to mining further intrinsic information across multiple network layers. Motivated to improve consistency between hierarchical features and strengthen the perception capability of the point cloud network, we propose exploring whether maximizing the mutual information (MI) across shallow and deep layers is beneficial to improve representation learning on point clouds. A novel design of Maximizing Mutual Information (MMI) Module is proposed, which assists the training process of the main network to capture discriminative features of the input point clouds. Specifically, the MMI-based loss function is employed to constrain the differences of semantic information in two hierarchical features extracted from the shallow and deep layers of the network. Extensive experiments show that our method is generally applicable to point cloud tasks, including classification, shape retrieval, indoor scene segmentation, 3D object detection, and completion, and illustrate the efficacy of our proposed method and its advantages over existing ones. Our source code is available at https://github.com/wendydidi/MMI.git.}
}
@article{LI2022108900,
title = {Adaptive weighted guided image filtering for depth enhancement in shape-from-focus},
journal = {Pattern Recognition},
volume = {131},
pages = {108900},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108900},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003818},
author = {Yuwen Li and Zhengguo Li and Chaobing Zheng and Shiqian Wu},
keywords = {Shape from focus, Depth enhancement, Adaptive weighted guided image filtering, Edge-preserving, Robustness},
abstract = {Existing shape from focus (SFF) techniques cannot preserve depth edges and fine structural details from a sequence of multi-focus images. Moreover, noise in the sequence affects the accuracy of the depth map. In this paper, a novel depth enhancement algorithm for the SFF based on an adaptive weighted guided image filtering (AWGIF) is proposed to address the above issues. The AWGIF is applied to decompose an initial depth map estimated by the traditional SFF into base and detail layers. In order to preserve the edges accurately in the refined depth map, the guidance image is constructed from the sequence, and the coefficient of the AWGIF is utilized to suppress the noise while enhancing the fine depth details. Experiments on real and synthetic objects demonstrate the superiority of our algorithm in terms of anti-noise, and the ability to preserve depth edges and fine structural details w.r.t. existing methods.}
}
@article{HUANG2022108831,
title = {Cyclical Adversarial Attack Pierces Black-box Deep Neural Networks},
journal = {Pattern Recognition},
volume = {131},
pages = {108831},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108831},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003120},
author = {Lifeng Huang and Shuxin Wei and Chengying Gao and Ning Liu},
keywords = {Adversarial example, Transferability, Black-box attack, Defenses},
abstract = {Deep neural networks (DNNs) have shown vulnerability to adversarial attacks. By exploiting the transferability of adversarial examples, attackers can fool models under black-box settings without accessing the underlying information. However, they often exhibit weak performance when transferring to defenses, which may give a false sense of security. In this paper, we propose Cyclical Adversarial Attack (CA2), a general and straightforward method to boost the transferability to break defenders. We first revisit the momentum-based methods from the perspective of optimization and find that they usually suffer from the transferability saturation dilemma. To address this, CA2 performs cyclical optimization algorithm to produce adversarial examples. Unlike the standard momentum policy that accumulates the velocity to continuously update the solution, we divide the generation process into multiple phases and treat the velocity vectors from the previous phase as proper knowledge to guide a new adversarial attack with larger steps. Moreover, CA2 applies a novel and compatible augmentation algorithm at every optimization in a loop manner for enhancing the black-box transferability further, referred to as cyclical augmentation. Extensive experiments conducted on a variety of models not only validate the efficacy of each designed algorithm in CA2, but also illustrate the superiority of our method compared with the state-of-the-art transferable attacks. Our implemental code is publicly available at https://github.com/mesunhlf/CA2.}
}
@article{FUJITAKE2022108847,
title = {Temporal feature enhancement network with external memory for live-stream video object detection},
journal = {Pattern Recognition},
volume = {131},
pages = {108847},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108847},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003284},
author = {Masato Fujitake and Akihiro Sugimoto},
keywords = {Video object detection, Video analysis, Object detection},
abstract = {This paper proposes a method exploiting temporal context with an attention mechanism for detecting objects in real-time in a live streaming video. Video object detection is challenging and essential in practical applications such as robotics, smartphones, and surveillance cameras. Although methods have been proposed to improve the accuracy or run-time speed by exploiting temporal information, the trade-off between them tends to be ignored. We thus focus on the trade-off between accuracy and speed, and propose a method to improve the accuracy by aggregating the past information from a lightweight feature extractor with an attention mechanism. Evaluations on the UA-DETRAC and ImageNet VID datasets demonstrate our model’s superior performance to state-of-the-art methods on live streaming real-time object detection.}
}
@article{TU2022108887,
title = {DFR-ST: Discriminative feature representation with spatio-temporal cues for vehicle re-identification},
journal = {Pattern Recognition},
volume = {131},
pages = {108887},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108887},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003685},
author = {Jingzheng Tu and Cailian Chen and Xiaolin Huang and Jianping He and Xinping Guan},
keywords = {Vehicle re-identification, Computer vision, Deep learning, Attention mechanism, Video surveillance},
abstract = {Vehicle re-identification (re-ID) aims to discover and match the target vehicles from a gallery image set taken by different cameras on a wide range of road networks. It is crucial for lots of applications such as security surveillance and traffic management. The remarkably similar appearances of distinct vehicles and the significant changes in viewpoints and illumination conditions pose grand challenges to vehicle re-ID. Conventional solutions focus on designing global visual appearances without sufficient consideration of vehicles’ spatio-temporal relationships in different images. This paper proposes a discriminative feature representation with spatio-temporal clues (DFR-ST) for vehicle re-ID. It is capable of building robust features in the embedding space by involving appearance and spatio-temporal information. The proposed DFR-ST constructs an appearance model for a multi-grained visual representation by a two-stream architecture and a spatio-temporal metric to provide complementary information based on this multi-modal information. Experimental results on four public datasets demonstrate DFR-ST outperforms the state-of-the-art methods, which validates the effectiveness of the proposed method.}
}
@article{MEI2022108835,
title = {Spatial feature mapping for 6DoF object pose estimation},
journal = {Pattern Recognition},
volume = {131},
pages = {108835},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108835},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003168},
author = {Jianhan Mei and Xudong Jiang and Henghui Ding},
keywords = {6D Pose estimation, Rotation symmetry, Spherical convolution, Graph convolutional network},
abstract = {This work aims to estimate 6Dof (6D) object pose in background clutter. Considering the strong occlusion and background noise, we propose to utilize the spatial structure for better tackling this challenging task. Observing that the 3D mesh can be naturally abstracted by a graph, we build the graph using 3D points as vertices and mesh connections as edges. We construct the corresponding mapping from 2D image features to 3D points for filling the graph and fusion of the 2D and 3D features. Afterward, a Graph Convolutional Network (GCN) is applied to help the feature exchange among objects’ points in 3D space. To address the problem of rotation symmetry ambiguity for objects, a spherical convolution is utilized and the spherical features are combined with the convolutional features that are mapped to the graph. Predefined 3D keypoints are voted and the 6DoF pose is obtained via the fitting optimization. Two scenarios of inference, one with the depth information and the other without it are discussed. Tested on the datasets of YCB-Video and LINEMOD, the experiments demonstrate the effectiveness of our proposed method.}
}
@article{YANG2022108863,
title = {Multi-View correlation distillation for incremental object detection},
journal = {Pattern Recognition},
volume = {131},
pages = {108863},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108863},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003442},
author = {Dongbao Yang and Yu Zhou and Aoting Zhang and Xurui Sun and Dayan Wu and Weiping Wang and Qixiang Ye},
keywords = {Object detection, Incremental learning, Catastrophic forgetting, Knowledge distillation},
abstract = {In real applications, new object classes often emerge after the detection model has been trained on a prepared dataset with fixed classes. Fine-tuning the old model with only new data will lead to a well-known phenomenon of catastrophic forgetting, which severely degrades the performance of modern object detectors. Due to the storage burden, data privacy and time consumption, sometimes it is impractical to train the model from scratch with all data of both old and new classes. In this paper, we propose a novel Multi-View Correlation Distillation (MVCD) based incremental object detection method, which explores the intra-feature correlations in the feature space of the object detector. To better transfer the knowledge learned from the old classes and maintain the ability to learn new classes, we select the sample-specific discriminative features from channel-wise, point-wise and instance-wise views. Meanwhile, the correlation distillation losses on the selective features are designed to regularize the learning of the incremental object detector. A new metric named Stability-Plasticity-mAP (SPmAP) is proposed to evaluate the incremental learning performance as a complementary metric to mAP, which integrates the metrics for the stability on old classes and the plasticity on new classes in incremental object detection. The extensive experiments conducted on VOC2007 and COCO demonstrate that MVCD achieves a better trade-off between stability and plasticity than state-of-the-art first-order distillation-based incremental object detection methods.}
}
@article{SOUZA2022108895,
title = {High-order conditional mutual information maximization for dealing with high-order dependencies in feature selection},
journal = {Pattern Recognition},
volume = {131},
pages = {108895},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108895},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003764},
author = {Francisco Souza and Cristiano Premebida and Rui Araújo},
keywords = {Feature selection, Mutual information, Information theory, Pattern recognition},
abstract = {This paper presents a novel feature selection method based on the conditional mutual information (CMI). The proposed High Order Conditional Mutual Information Maximization (HOCMIM) method incorporates high order dependencies into the feature selection procedure and has a straightforward interpretation due to its bottom-up derivation. The HOCMIM is derived from the CMI’s chain expansion and expressed as a maximization optimization problem. The maximization problem is solved using a greedy search procedure, which speeds up the entire feature selection process. The experiments are run on a set of benchmark datasets (20 in total). The HOCMIM is compared with eighteen state-of-the-art feature selection algorithms, from the results of two supervised learning classifiers (Support Vector Machine and K-Nearest Neighbor). The HOCMIM achieves the best results in terms of accuracy and shows to be faster than high order feature selection counterparts.}
}
@article{SHU2022108843,
title = {Using global information to refine local patterns for texture representation and classification},
journal = {Pattern Recognition},
volume = {131},
pages = {108843},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108843},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003247},
author = {Xin Shu and Hui Pan and Jinlong Shi and Xiaoning Song and Xiao-Jun Wu},
keywords = {Texture classification, Texture descriptor, Texture representation, Feature pattern refinement, Local binary pattern},
abstract = {Local binary pattern (LBP) and its variants have been successfully applied in texture feature extraction. However, it is hard for most LBP-based methods to effectively describe and distinguish the local neighborhoods with similar structures (that is, the calculated feature patterns are identical) but different contrasts or grayscales. To alleviate such problems, we propose a novel global refined local binary pattern (GRLBP) by analyzing the nature of pixel intensity distribution in local neighborhoods. GRLBP consists of two descriptors called magnitude refined local sign binary pattern (MRLBP_S) and center refined local magnitude binary pattern (CRLBP_M). MRLBP_S distinguishes local neighborhoods with contrast differences by using global magnitude anchors to refine local sign patterns. And CRLBP_M identifies local neighborhoods with grayscale differences by employing global central grayscale anchors to refine local magnitude patterns. Finally, frequency histograms of MRLBP_S and CRLBP_M from each image are cascaded to generate the GRLBP. Extensive experimental results on seven benchmark texture databases: Outex, CUReT, KTH-TIPS, UMD, UIUC, KTH-T2b, and DTD demonstrate that the proposed GRLBP can represent the detailed information of texture images. Furthermore, compared with state-of-the-art LBP variants, GRLBP has competitive advantages in classification accuracy, feature dimension, and computational complexity, respectively.}
}
@article{KIM2022108871,
title = {2PESNet: Towards online processing of temporal action localization},
journal = {Pattern Recognition},
volume = {131},
pages = {108871},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108871},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003521},
author = {Young Hwi Kim and Seonghyeon Nam and Seon Joo Kim},
keywords = {Online video understanding, Temporal action localization},
abstract = {Existing online video processing methods such as online action detection focus on a frame-level understanding for high responsiveness. However, it has a fundamental limitation in that it lacks instance-level understanding of videos, making it difficult to be applied to higher-level vision tasks. The instance-level action detection, known as Temporal Action Localization (TAL), have limitations when applying to the online settings. In this work, we introduce a new task that aims to detect action instances of videos in an online setting, named Online Temporal Action Localization (OnTAL). To tackle this problem, we propose a 2-Pass End/Start detection Network (2PESNet) that detects action instances by effectively finding the start and end of an action instance. Additionally, we propose a two-stage action end detection method to further improve the performance. Extensive experiments on THUMOS’14 and ActivityNet v1.3 demonstrate that our model is able to take both accuracy and responsiveness when predicting action instances from streaming videos.}
}
@article{AO2022108859,
title = {Cross-modal prototype learning for zero-shot handwritten character recognition},
journal = {Pattern Recognition},
volume = {131},
pages = {108859},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108859},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003405},
author = {Xiang Ao and Xu-Yao Zhang and Cheng-Lin Liu},
keywords = {Online handwriting, Offline handwriting, Printed character, Zero-shot, Prototype, Cross-modality},
abstract = {Traditional methods of handwritten character recognition rely on extensive labeled data. However, humans can generalize to unseen handwritten characters by watching a few printed examples in textbooks. To simulate this ability, we propose a cross-modal prototype learning method (CMPL) to realize zero-shot recognition. For each character class, a prototype is generated by mapping the printed character into a deep neural network feature space. For unseen character class, its prototype can be directly produced from a printed character sample, therefore, not requiring any handwritten samples to realize class-incremental learning. Specifically, CMPL considers different modalities simultaneously - online handwritten trajectories, offline handwritten images, and auxiliary printed character images. The joint learning of the above modalities is achieved through sharing printed prototypes between online and offline data. In zero-shot inference, we feed CMPL the printed samples to obtain corresponding class prototypes, and then the unseen handwritten character can be recognized by the nearest prototype. Our experimental results demonstrate that CMPL outperforms the state-of-the-art methods in both online and offline zero-shot handwritten Chinese character recognition. Moreover, we also show the cross-domain generalization of CMPL from two perspectives: cross-language and modern-to-ancient handwritten character recognition, focusing on the transferability between different languages and different styles (i.e., modern and historical handwritings).}
}
@article{LI2022108875,
title = {Clustering experience replay for the effective exploitation in reinforcement learning},
journal = {Pattern Recognition},
volume = {131},
pages = {108875},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108875},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003569},
author = {Min Li and Tianyi Huang and William Zhu},
keywords = {Reinforcement learning, Clustering, Experience replay, Exploitation efficiency, Time division},
abstract = {Reinforcement learning is a useful tool for training an agent to effectively achieve the desired goal in the sequential decision-making problem. It trains the agent to make decision by exploiting the experience in the transitions resulting from the different decisions. To exploit this experience, most reinforcement learning methods replay the explored transitions by uniform sampling. But in this way, it is easy to ignore the last explored transitions. Another way to exploit this experience defines the priority of each transition by the estimation error in training and then replays the transitions according to their priorities. But it only updates the priorities of the transitions replayed at the current training time step, thus the transitions with low priorities will be ignored. In this paper, we propose a clustering experience replay, called CER, to effectively exploit the experience hidden in all explored transitions in the current training. CER clusters and replays the transitions by a divide-and-conquer framework based on time division as follows. Firstly, it divides the whole training process into several periods. Secondly, at the end of each period, it uses k-means to cluster the transitions explored in this period. Finally, it constructs a conditional probability density function to ensure that all kinds of transitions will be sufficiently replayed in the current training. We construct a new method, TD3_CER, to implement our clustering experience replay on TD3. Through the theoretical analysis and experiments, we illustrate that our TD3_CER is more effective than the existing reinforcement learning methods. The source code can be downloaded from https://github.com/grcai/CER-Master.}
}
@article{WANG2022108814,
title = {YOLO-Anti: YOLO-based counterattack model for unseen congested object detection},
journal = {Pattern Recognition},
volume = {131},
pages = {108814},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108814},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002953},
author = {Kun Wang and Maozhen Liu},
keywords = {Deep learning, Congested and occluded objects, Object detection},
abstract = {Object detection is advancing rapidly with the development of deep learning solutions and big data dimensions. This paper takes the challenging recognition task as the core work and proposes a novel and efficient network framework dedicated to unseen congestion detection. To guarantee the accuracy as well as the speed of inference, the detector utilizes the advanced You Only Look Once v4 (YOLOv4) as the backbone and agglutinates the four proposed strategies, called YOLO-Anti. Our model mainly consists of three modules: First, an adaptive context module similar to valve control is proposed to obtain contextual information that balances foreground and background features. Second, to solve the problem that the imbalance between feature levels weakens the detection performance, a balanced prediction layer method is developed. Finally, we propose an anti-congestion network to selectively expand the local domain to achieve finer-grained detection. Besides, in the training procedure, a designed heterogeneous cross-entropy loss is utilized to strengthen the detector’s discrimination of similar targets in different categories. Extensive experiments were conducted on the PASCAL VOC, COCO, and UA-DETRAC data sets. The state-of-the-art results were achieved on UA-DETRAC and the leading performance on PASCAL VOC and COCO. Also, compared with baseline YOLOv4, the proposed method brings significant accuracy improvement and negligible time consumption.}
}
@article{KV2022108883,
title = {On the role of question encoder sequence model in robust visual question answering},
journal = {Pattern Recognition},
volume = {131},
pages = {108883},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108883},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003648},
author = {Gouthaman KV and Anurag Mittal},
keywords = {Visual question answering, Out-of-distribution performance, Gated recurrent unit, Transformer, Graph attention network},
abstract = {Generalizing beyond the experiences has a significant role in developing robust and practical machine learning systems. It has been shown that current Visual Question Answering (VQA) models are over-dependent on the language-priors (spurious correlations between question-types and their most frequent answers) from the train set and pose poor performance on Out-of-Distribution (OOD) test sets. This conduct negatively affects the robustness of VQA models and restricts them from being utilized in real-world situations. This paper shows that the sequence model architecture used in the question-encoder has a significant role in the OOD performance of VQA models. To demonstrate this, we performed a detailed analysis of various existing RNN-based and Transformer-based question-encoders, and along, we proposed a novel Graph attention network (GAT)-based question-encoder. Our study found that a better choice of sequence model in the question-encoder reduces the over-fit to language biases and improves OOD performance in VQA even without using any additional relatively complex bias-mitigation approaches.}
}
@article{GAJAMANNAGE2022108891,
title = {Reconstruction of fragmented trajectories of collective motion using Hadamard deep autoencoders},
journal = {Pattern Recognition},
volume = {131},
pages = {108891},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108891},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003727},
author = {Kelum Gajamannage and Yonggi Park and Randy Paffenroth and Anura P. Jayasumana},
keywords = {Multi-object tracking, Collective motion, Deep autoencoders, Hadamard product, Self-propelled particles},
abstract = {Learning dynamics of collectively moving agents such as fish or humans is an essential task in research. Due to phenomena such as occlusion or change of illumination, the multi-object methods tracking such dynamics may lose the tracks of the agents which may result in fragmentations of trajectories. Here, we present an extended deep autoencoder (DA) that we train only on the fully observed segments of the trajectories by defining its loss function as the Hadamard product of a binary indicator matrix with the absolute difference between the outputs and the labels. The trajectory matrix of the agents practicing collective motion is low-rank due to mutual interactions and dependencies between the agents that we utilize as the underlying pattern that our Hadamard deep autoencoder (HDA) codes during its training. The performance of this HDA is compared with that of a low-rank matrix completion scheme in the context of fragmented trajectory reconstruction.}
}
@article{WANG2022108870,
title = {Data-attention-YOLO (DAY): A comprehensive framework for mesoscale eddy identification},
journal = {Pattern Recognition},
volume = {131},
pages = {108870},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108870},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200351X},
author = {Xinning Wang and Xuegong Wang and Chong Li and Yuben Zhao and Peng Ren},
keywords = {Mesoscale eddy identification, Attention mechanism, Data-attention-based YOLO, One-stage detection},
abstract = {The accurate mesoscale eddy identification methods with deep learning framework depend on either single eddy characteristic from altimeter missions or multi-step eddy examination strategies, disregarding those indistinguishable features from multiple eddy data integration. In this article, we first propose a data-attention-based YOLO (DAY) to precisely recognize mesoscale eddies in the South China Sea (SCS), which can hierarchically unite multiple eddy attributes and efficiently predict eddies with one-step strategy involving detection and classification. It consists of two main components: heterogeneous eddy data integration module and dynamic attention detecting module for eddy identification. The data integration component empirically transforms the field of multi-source eddy data and propagates eddy labels through automatic labeling method, which sustains a good supply for our dynamic attention-base detecting network. To thoroughly identify mesoscale eddies based on spatio-temporal patterns, DAY efficiently learns the characteristics of mesoscale eddies with an improved one-step identification YOLO network. The comparative evaluation results demonstrate that DAY achieves 54% performance improvement over the state-of-the-art methods on single gray SLA data and outperforms two-stage detecting technique Faster R-CNN by 51%.}
}
@article{MO2022108899,
title = {Dimension-aware attention for efficient mobile networks},
journal = {Pattern Recognition},
volume = {131},
pages = {108899},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108899},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003806},
author = {Rongyun Mo and Shenqi Lai and Yan Yan and Zhenhua Chai and Xiaolin Wei},
keywords = {Efficient mobile networks, Attention mechanism, Feature enhancement, Multi-branch factorization, Multi-dimensional information},
abstract = {Recently, attention mechanisms have shown great potential in improving the performance of mobile networks. Typically, they involve 2D symmetric convolution operations or generate 2D attention maps. However, such manners usually introduce high computational cost and large memory consumption, increasing the computational burden of mobile networks. To address this problem, we propose a novel lightweight attention mechanism, called Dimension-Aware Attention (DAA) block, by modeling the intra-dependencies of each dimension of the input feature map. Specifically, we factorize the channel and spatial attention by three parallel feature vector encoding branches, where stacked 1D asymmetric convolution operations can be naturally leveraged to capture large receptive fields. In this way, channel-aware, horizontal-aware, and vertical-aware attention vectors are extracted to effectively encode multi-dimensional information and greatly reduce the computational complexity of mobile networks. Experiments on multiple vision tasks demonstrate that our DAA block achieves better accuracy against state-of-the-art attention mechanisms with much lower computational operations. Our code is available at https://github.com/rymo96/DAANet.}
}
@article{KARUNANAYAKE2022108838,
title = {Artificial life for segmentation of fusion ultrasound images of breast abnormalities},
journal = {Pattern Recognition},
volume = {131},
pages = {108838},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108838},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003193},
author = {Nalan Karunanayake and Wanrudee Lohitvisate and Stanislav S. Makhanov},
keywords = {Artificial life, Fusion image, Medical image segmentation, Genetic algorithm, Ultrasound images, Breast cancer},
abstract = {Segmentation of cancerous tumors in ultrasound (US) images of human organs is one of the critical problems in medical imaging. The US images are characterized by low contrast, irregular shapes, high levels of speckle-noise and acoustic shadows, making it difficult to segment the tumor. Yet, US imaging is considered one of the most inexpensive and safe imaging tests available to detect cancer in its early stages. However, an automatic segmentation method applicable to all types of US imagery does not exist. This paper proposes a novel segmentation method that combines image fusion, artificial life (AL) and a genetic algorithm (GA). The new algorithm has been applied to US images of breast cancer. The method is based on tracing agents (TA), which are artificial organisms with memory and the ability to communicate. They live inside a fusion image generated from the US and the elastography (EL) images. The TA can recognize the patterns of strong edges and boundary gaps allowing to outline the tumor. The new model has been tested against six types of segmentation models, i.e., machine learning, active contours, level set models, superpixel models, edge linking models and selected hybrid methods. The experiments include 16 state-of-the-art methods, which outperform 69 recent and classical segmentation routines. The tests were run on 395 breast cancer images from http://onlinemedicalimages.com and https://www.ultrasoundcases.info/. TA training employs a GA. The model has been verified on “hard” cases (complex shapes, boundary leakage, and noisy edge maps). The proposed algorithm produces more accurate results than the reference methods on high complexity images. A video demo of the algorithm is at http://shorturl.at/htBW9.}
}
@article{FUCHS2022108846,
title = {A novel way to formalize stable graph cores by using matching-graphs},
journal = {Pattern Recognition},
volume = {131},
pages = {108846},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108846},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003272},
author = {Mathias Fuchs and Kaspar Riesen},
keywords = {Graph matching, Matching-graphs, Graph edit distance, Structural pattern recognition},
abstract = {The increasing amount of data available and the rate at which it is collected leads to rapid developments of systems for intelligent information processing and pattern recognition. Often the underlying data is inherently complex, making it difficult to represent it by linear, vectorial data structures. This is where graphs offer a versatile alternative for formal data representation. Actually, quite an amount of graph-based methods for pattern recognition has been proposed. A considerable part of these methods rely on graph matching. In the present paper, we propose a novel encoding of specific graph matching information. The basic idea is to formalize the stable cores of individual classes of graphs – discovered during intra-class matchings – by means of so called matching-graphs. We evaluate the benefit of these matching-graphs by researching two classification approaches that rely on this novel data structure. The first approach is a distance based classifier focusing on the matching-graphs during dissimilarity computation. For the second approach, we propose to use sets of matching-graphs to embed input graphs into a vector space. The basic idea is to produce hundreds of matching-graphs first, and then represent each graph g as a vector that shows the occurrence of, or the distance to, each matching-graph. In a thorough experimental evaluation on seven real world data sets we empirically confirm that our novel approaches are able to improve the classification accuracy of systems that rely on comparable information as well as state-of-the-art methods.}
}
@article{CHEN2022108862,
title = {Online Adaptive Kernel Learning with Random Features for Large-scale Nonlinear Classification},
journal = {Pattern Recognition},
volume = {131},
pages = {108862},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108862},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003430},
author = {Yingying Chen and Xiaowei Yang},
keywords = {Large-scale, Nonlinear classification, Online learning, Random feature map},
abstract = {In the field of support vector machines, online random feature map algorithms are very important methods for large-scale nonlinear classification problems. At present, the existing methods have the following shortcomings: (1) If only the hyperplane vector is updated during learning while the random feature components are fixed, there is no guarantee that these online methods can adapt to the change of data distribution shape when the data is coming one by one. (2) When the kernel is selected improperly, the samples mapped to an inappropriate space may not be well classified. In order to overcome these shortcomings, considering the fact that iteratively updating random feature components can make data better fit in the current space and lead to the flexible adjustment of the kernel function, random features based online adaptive kernel learning (RF-OAK) is proposed for large-scale nonlinear classification problems. Theoretical analysis of the proposed algorithm is also provided. The experimental results and the Wilcoxon signed-ranks test show that in terms of test accuracy, the proposed method is significantly better than the state-of-the-art online feature mapping classification methods. Compared with the deep learning algorithms, the training time of RF-OAK is shorter. In terms of test accuracy, RF-OAK is better than online algorithm and comparable with offline algorithms.}
}
@article{SHUANG2022108878,
title = {Comprehensive-perception dynamic reasoning for visual question answering},
journal = {Pattern Recognition},
volume = {131},
pages = {108878},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108878},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003594},
author = {Kai Shuang and Jinyu Guo and Zihan Wang},
keywords = {Cross-modal information fusion, Visual question answering, Comprehensive perception, Relational reasoning},
abstract = {The goal of Visual Question Answering (VQA) is to answer questions based on an image. In the VQA task, reasoning plays an important role in dealing with relations because this task has a high requirement for modeling complex features. In most existing models, the features are only extracted and integrated between adjacent layers. This pattern arguably affects the integrity of information interaction during reasoning. In this paper, we propose a comprehensive-perception dynamic reasoning (CPDR) model to utilize the cross-layer object features for multi-step compound reasoning. It calculates the interactions among the object features from all previous layers and integrates these interactions to generate new object features, iteratively. Finally, the object features of all layers will be used for the final prediction. Empirical results show that our model achieves superior performance among VQA models which are not VLP-based, and incorporating the CPDR module into the VLP models brings considerable performance improvements.}
}
@article{SHARMA2022108826,
title = {Covid-MANet: Multi-task attention network for explainable diagnosis and severity assessment of COVID-19 from CXR images},
journal = {Pattern Recognition},
volume = {131},
pages = {108826},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108826},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003077},
author = {Ajay Sharma and Pramod Kumar Mishra},
keywords = {Covid-19, Lung segmentation, Infection segmentation, Chest X-ray, Deep learning, Transfer learning, Explainable AI},
abstract = {The devastating outbreak of Coronavirus Disease (COVID-19) cases in early 2020 led the world to face health crises. Subsequently, the exponential reproduction rate of COVID-19 disease can only be reduced by early diagnosis of COVID-19 infection cases correctly. The initial research findings reported that radiological examinations using CT and CXR modality have successfully reduced false negatives by RT-PCR test. This research study aims to develop an explainable diagnosis system for the detection and infection region quantification of COVID-19 disease. The existing research studies successfully explored deep learning approaches with higher performance measures but lacked generalization and interpretability for COVID-19 diagnosis. In this study, we address these issues by the Covid-MANet network, an automated end-to-end multi-task attention network that works for 5 classes in three stages for COVID-19 infection screening. The first stage of the Covid-MANet network localizes attention of the model to the relevant lungs region for disease recognition. The second stage of the Covid-MANet network differentiates COVID-19 cases from bacterial pneumonia, viral pneumonia, normal and tuberculosis cases, respectively. To improve the interpretation and explainability, three experiments have been conducted in exploration of the most coherent and appropriate classification approach. Moreover, the multi-scale attention model MA-DenseNet201 proposed for the classification of COVID-19 cases. The final stage of the Covid-MANet network quantifies the proportion of infection and severity of COVID-19 in the lungs. The COVID-19 cases are graded into more specific severity levels such as mild, moderate, severe, and critical as per the score assigned by the RALE scoring system. The MA-DenseNet201 classification model outperforms eight state-of-the-art CNN models, in terms of sensitivity and interpretation with lung localization network. The COVID-19 infection segmentation by UNet with DenseNet121 encoder achieves dice score of 86.15% outperforming UNet, UNet++, AttentionUNet, R2UNet, with VGG16, ResNet50 and DenseNet201 encoder. The proposed network not only classifies images based on the predicted label but also highlights the infection by segmentation/localization of model-focused regions to support explainable decisions. MA-DenseNet201 model with a segmentation-based cropping approach achieves maximum interpretation of 96% with COVID-19 sensitivity of 97.75%. Finally, based on class-varied sensitivity analysis Covid-MANet ensemble network of MA-DenseNet201, ResNet50 and MobileNet achieve 95.05% accuracy and 98.75% COVID-19 sensitivity. The proposed model is externally validated on an unseen dataset, yields 98.17% COVID-19 sensitivity.}
}
@article{SONG2022108858,
title = {Multi-feature deep information bottleneck network for breast cancer classification in contrast enhanced spectral mammography},
journal = {Pattern Recognition},
volume = {131},
pages = {108858},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108858},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003399},
author = {Jingqi Song and Yuanjie Zheng and Jing Wang and Muhammad Zakir Ullah and Xuecheng Li and Zhenxing Zou and Guocheng Ding},
keywords = {Contrast enhanced spectral mammography, Classification, Deep learning, Multi-feature, Information bottleneck},
abstract = {There is considerable variation in the size, shape and location of tumours, which makes it challenging for radiologists to diagnose breast cancer. Automated diagnosis of breast cancer from Contrast Enhanced Spectral Mammography (CESM) can support clinical decision making. However, existing methods fail to obtain an effective representation of the CESM and ignore the relationships between images. In this paper, we investigated for the first time a novel and flexible multimodal representation learning method, multi-feature deep information bottleneck (MDIB), for breast cancer classification in CESM. Specifically, the method incorporated an information bottleneck (IB)-based module to learn the prominent representation that provide concise input while informative for the classification. In addition, we creatively extended IB theory to multi-feature IB, which facilitates the learning of relevant features for classification between CESM images. To validate our method, experiments were conducted on our private and public datasets. The classification results of our method were also compared with those of state-of-the-art methods. The experiment results proved the effectiveness and the efficiency of the proposed method. We release our code at https://github.com/sjq5263/MDIB-for-CESM-classification.}
}
@article{DONG2022108886,
title = {Deep rank hashing network for cancellable face identification},
journal = {Pattern Recognition},
volume = {131},
pages = {108886},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108886},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003673},
author = {Xingbo Dong and Sangrae Cho and Youngsam Kim and Soohyung Kim and Andrew Beng Jin Teoh},
keywords = {Cancellable biometrics, Deep learning, Face biometrics, Hashing, Identification},
abstract = {Cancellable biometrics (CB) is one of the major approaches for biometric template protection. However, almost all the prior arts are designed to work under verification (one-to-one matching). This paper proposes a deep learning-based cancellable biometric scheme for face identification (one-to-many matching). Our scheme comprises two key ingredients: a deep rank hashing (DRH) network and a cancellable identification scheme. The DRH network transforms a raw face image into discriminative yet compact face hash codes based upon the nonlinear subspace ranking notion. The network is designed to be trained for both identification and hashing goals with their respective rich identity-related and rank hashing relevant loss functions. A modified softmax function is utilized to alleviate the hashing quantization error, and a regularization term is designed to encourage hash code balance. The hash code is binarized, compressed, and secured with the randomized lookup table function. Unlike prior CB schemes that require two input factors for verification, the proposed scheme demands no additional input except face images during identification, yet the face template is replaceable whenever needed based upon a one-time XOR cipher notion. The proposed scheme is evaluated on five public unconstrained face datasets in terms of verification, closed-set and open-set identification performance accuracy, computation cost, template protection criteria, and security.}
}
@article{BAI2022108834,
title = {Practical protection against video data leakage via universal adversarial head},
journal = {Pattern Recognition},
volume = {131},
pages = {108834},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108834},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003156},
author = {Jiawang Bai and Bin Chen and Kuofeng Gao and Xuan Wang and Shu-Tao Xia},
keywords = {Privacy protection, Video retrieval, Deep hashing, Adversarial attack},
abstract = {While online video sharing becomes more popular, it also causes unconscious leakage of personal information in the video retrieval systems like deep hashing. A snoop can collect more users’ private information from the video database by querying similar videos. This paper focuses on bypassing the deep video hashing based retrieval to prevent information from being maliciously collected. We propose universal adversarial head (UAH), which crafts adversarial query videos by prepending the original videos with a sequence of adversarial frames to perturb the normal hash codes in the Hamming space. This adversarial head can be generated only with a few natural videos, and mislead the retrieval system to return irrelevant videos when it is applied to most query videos. Furthermore, to obey the principle of information protection, we expand the proposed method to a data-free paradigm to generate the UAH, without access to users’ original videos. Extensive experiments demonstrate the effectiveness of our method in misleading deep video hashing under both white-box and black-box settings.}
}
@article{GAO2022108866,
title = {Exploiting key points supervision and grouped feature fusion for multiview pedestrian detection},
journal = {Pattern Recognition},
volume = {131},
pages = {108866},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108866},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003478},
author = {Xin Gao and Yijin Xiong and Guoying Zhang and Hui Deng and Kangkang Kou},
keywords = {Multiview aggregation, Pedestrian detection, Key points, Grouped feature fusion},
abstract = {Multiview pedestrian detection detects pedestrians based on the perception of the same environment from multiple perspectives. This task requires feature extraction in a single view with occlusion and aggregation of multiview information. However, existing research is limited by the local occlusion and the multiview feature stitching method, which cannot perform multiview aggregation efficiently. This paper introduces a network that utilizes key points supervision and grouped feature fusion to address these challenges. It uses key points to regress pedestrians in a single view, and augments the pedestrian consistency information in overlapping views by a grouped feature fusion module. Specifically, the proposed key points supervision effectively alleviates false negatives due to occlusion, and the grouped feature fusion module enhances pedestrian location features by computing the similarity and spatial correlation of overlapping views after single view projection to the ground plane, thereby reducing target ambiguity. Quantitative and qualitative results show that the proposed method can reduce false negatives and false positives in multiview pedestrian detection and achieve efficient multiview feature aggregation. Compared to state-of-the-art methods, the proposed model achieves superior performance, achieving the highest MODA of 92.4 and 93.9 on Wildtrack and MultiviewX datasets, respectively. We believe, to the best of our knowledge, that this approach offers a new optimization idea for multiview aggregation.}
}
@article{NASIRI2022108805,
title = {Multiple-solutions RANSAC for finding axes of symmetry in fragments of objects},
journal = {Pattern Recognition},
volume = {131},
pages = {108805},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108805},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002862},
author = {Seyed-Mahdi Nasiri and Reshad Hosseini and Hadi Moradi},
keywords = {Symmetry axis, Multiple-solutions RANSAC, 3D Reconstruction},
abstract = {The problem of “finding best lines passing through a set of straight lines” has appeared in applications such as archaeological pottery analysis, precision manufacturing, and 3D modelling. In these applications, an instance of this problem is finding the symmetry axis of a symmetrical object from a set of its surface normal lines. We show that the mentioned instance of the problem may have two meaningful local minima, one of which is the symmetry axis, a fact that has been neglected in the literature. A multiple-solutions RANSAC algorithm is proposed for finding initial estimates of both local minima in the presence of outliers. Then, a coordinate-descent algorithm is presented that starts from these initial estimates and finds the local minima of the problem. The proposed coordinate-descent method does not involve any line search procedure, and its convergence is guaranteed. We also provide a proof for the rate of the convergence.}
}
@article{LIU2022108842,
title = {Hiding multiple images into a single image via joint compressive autoencoders},
journal = {Pattern Recognition},
volume = {131},
pages = {108842},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108842},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003235},
author = {Xiyao Liu and Ziping Ma and Zhihong Chen and Fangfang Li and Ming Jiang and Gerald Schaefer and Hui Fang},
keywords = {Image hiding, Neural networks, Deep learning, Compressive autoencoder},
abstract = {Interest in image hiding has been continually growing. Recently, deep learning-based image hiding approaches improve the hidden capacity significantly. However, the major challenges of the existing methods are that they are difficult to balance between the errors of the modified cover image and those of the recovered secret image. To solve this problem, in this paper, we develop an image hiding algorithm based on a joint compressive autoencoder framework. Further, we propose a novel strategy to enlarge the hidden capacity, i.e., hiding multi-images in one container image. Specifically, our approach provides an extremely high image hidden capacity coupled with small reconstruction errors of the secret image. More importantly, we tackle the trade-off problem of earlier approaches by mapping the image representations in the latent spaces of the joint compressive autoencoder models, leading to both high visual quality of the container image and low reconstruction error the secret image. In an extensive set of experiments, we confirm our proposed approach to outperform several state-of-the-art image hiding methods, yielding high imperceptibility and steganalysis resistance of the container images with high recovery quality of the secret images, while improving the image hidden capacity significantly (four times higher than full-image hiding capacity).}
}
@article{YUAN2022108902,
title = {Cubic-cross convolutional attention and count prior embedding for smoke segmentation},
journal = {Pattern Recognition},
volume = {131},
pages = {108902},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108902},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003831},
author = {Feiniu Yuan and Zeshu Dong and Lin Zhang and Xue Xia and Jinting Shi},
keywords = {Smoke segmentation, Information embedding, Cubic-cross convolutional attention, Count prior attention},
abstract = {It is very challenging to accurately segment smoke images because smoke has some adverse properties, such as semi-transparency and blurry boundary. Aiming at solving these problems, we first fuse convolutional results along different axes to equivalently produce a cubic-cross convolutional kernel, which enlarges receptive fields at affordable computational costs for capturing long-range dependency of smoke pixels, and then we propose a Cubic-cross Convolutional Attention (CCA). To embed global category information, we propose a count prior structure to model and supervise the count of smoke pixels. To ensure the network can correctly extract a count prior map, we impose a regression loss on the count prior map and corresponding ideal count map directly calculated from its ground truth. Then we multiply the reshaped input by the count prior map to produce a Count Prior Attention (CPA) map, which is upsampled to generate the final output. A cross entropy loss is used to supervise the final segmentation. Finally, we use ResNet50 for feature encoding, and stack CCA and CPA together to propose a Cubic-cross convolutional attention and Count prior Embedding Network (CCENet) for smoke segmentation. Experiments on both synthetic and real smoke datasets show that our method outperforms existing state-of-the-art methods.}
}
@article{BRAGANTINI2022108882,
title = {Rethinking interactive image segmentation: Feature space annotation},
journal = {Pattern Recognition},
volume = {131},
pages = {108882},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108882},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003636},
author = {Jordão Bragantini and Alexandre X. Falcão and Laurent Najman},
keywords = {Interactive image segmentation, Data annotation, Interactive machine learning, Feature space annotation},
abstract = {Despite the progress of interactive image segmentation methods, high-quality pixel-level annotation is still time-consuming and laborious — a bottleneck for several deep learning applications. We take a step back to propose interactive and simultaneous segment annotation from multiple images guided by feature space projection. This strategy is in stark contrast to existing interactive segmentation methodologies, which perform annotation in the image domain. We show that feature space annotation achieves competitive results with state-of-the-art methods in foreground segmentation datasets: iCoSeg, DAVIS, and Rooftop. Moreover, in the semantic segmentation context, it achieves 91.5% accuracy in the Cityscapes dataset, being 74.75 times faster than the original annotation procedure. Further, our contribution sheds light on a novel direction for interactive image annotation that can be integrated with existing methodologies. The supplementary material presents video demonstrations. Code available at https://github.com/LIDS-UNICAMP/rethinking-interactive-image-segmentation.}
}
@article{WU2022108830,
title = {Heterogeneous representation learning and matching for few-shot relation prediction},
journal = {Pattern Recognition},
volume = {131},
pages = {108830},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108830},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003119},
author = {Tao Wu and Hongyu Ma and Chao Wang and Shaojie Qiao and Liang Zhang and Shui Yu},
keywords = {Knowledge graphs, Few-shot learning, Relation prediction, Representation learning, Convolutional network},
abstract = {The recent explosive development of knowledge graphs (KGs) in artificial intelligence tasks coupled with incomplete or partial information has triggered considerable research interest in relation prediction. However, many challenges still remain unsolved: (i) the previous relation prediction methods require a significant amount of training instances (i.e., head-tail entity pairs) for every relation, which is infeasible in practical scenarios; and (ii) the representation learning of entities and relations always assumes that all local neighbors and their features contribute equally to the embedding, not sufficiently considering the heterogeneity of the information; and (iii) the state-of-the-art methods usually require a lot of training time, resulting in a high cost in real-world applications. To overcome these challenges, we propose a heterogeneous representation learning and matching approach, Multi-metric Feature Extraction Network (MFEN for short), for few-shot relation prediction in KGs. Our method focuses on knowledge graphs to sufficiently explore the topological structure and node content in graphs. Rather than taking the average of the embeddings of all relational neighbors, a heterogeneity-aware representation learning method is proposed to generate high-expressive embeddings, which capture the heterogenous roles of the relational neighbors of given entity and all of their features via a convolutional encoder. To learn the expressive representations efficiently, a single-layer CNN architecture with multi-scale filters is devised. In addition, multiple heuristic metrics are combined to efficiently improve the accuracy of similarity calculation. The proposed MFEN model is evaluated on two representative benchmark datasets NELL and Wiki. Extensive experiments have demonstrated that our method gets more than 5% accuracy improvement and three times speedup to state-of-the-art models. Code is available on https://github.com/summer-funny/MFEN.}
}
@article{PENG2022108890,
title = {H-ProMed: Ultrasound image segmentation based on the evolutionary neural network and an improved principal curve},
journal = {Pattern Recognition},
volume = {131},
pages = {108890},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108890},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003715},
author = {Tao Peng and Jing Zhao and Yidong Gu and Caishan Wang and Yiyun Wu and Xiuxiu Cheng and Jing Cai},
keywords = {Accurate prostate segmentation, Transrectal ultrasound, Principal curve, Optimized closed polygonal segment method, Evolutionary neural network, Interpretable mathematical model},
abstract = {The purpose of this work is to develop a method for accurate and robust prostate segmentation in transrectal ultrasound (TRUS) images. These images are difficult to segment due to missing/ambiguous boundary between the prostate and neighboring structures, the presence of shadow artifacts, as well as the large variability in prostate shapes. This paper develops a novel hybrid method for TRUS prostate segmentation by combining an improved principal curve-based method with an evolutionary neural network; the former for achieving the data sequences while and the latter for improving the smoothness of the prostate contour. Both qualitative and quantitative experimental results showed that our proposed method achieved superior segmentation accuracy and robustness as compared to state-of-the-art methods. The average Dice similarity coefficient (DSC), Jaccard similarity coefficient (Ω), and accuracy (ACC) of prostate contours against ground-truths were 96.8%, 95.7%, and 96.4%, and the DSC of around 92% and 95% for other deep learning and hybrid methods, respectively.}
}
@article{HUANG2022108817,
title = {Efficient federated multi-view learning},
journal = {Pattern Recognition},
volume = {131},
pages = {108817},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108817},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002989},
author = {Shudong Huang and Wei Shi and Zenglin Xu and Ivor W. Tsang and Jiancheng Lv},
keywords = {Federated learning, Multi-view learning, Matrix factorization, Clustering},
abstract = {Multi-view learning aims to explore a global common structure shared by different views collected from multiple individual sources. The nascent field of federated learning tries to learn a global model over distributed networks of devices. This paper shows that multi-view learning is naturally suited to address the feature heterogeneity of the federated setting. We propose a novel model, namely robust federated multi-view learning (FedMVL), which is considered in the following formulation: given a dataset with M views, it is required to train machine learning models while the M views are distributed across M devices or nodes. Considering the unique challenges like stragglers and fault tolerance in federated setting, we derive an iterative federated optimization algorithm that allows each node with the flexibility to approximately address its subproblem. To the best of our knowledge, our model for the first time considers the issues including high communication cost, fault tolerance, and stragglers for distributed multi-view learning. The proposed model also achieves encouraging performance on clustering task compared to closely related methods, as we illustrate through simulations on several real-world datasets.}
}