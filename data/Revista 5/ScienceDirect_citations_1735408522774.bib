@article{XU2023109819,
title = {Haar wavelet downsampling: A simple but effective downsampling module for semantic segmentation},
journal = {Pattern Recognition},
volume = {143},
pages = {109819},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109819},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005174},
author = {Guoping Xu and Wentao Liao and Xuan Zhang and Chang Li and Xinwei He and Xinglong Wu},
keywords = {Semantic segmentation, Downsampling, Haar wavelet, Information entropy},
abstract = {Downsampling operations such as max pooling or strided convolution are ubiquitously utilized in Convolutional Neural Networks (CNNs) to aggregate local features, enlarge receptive field, and minimize computational overhead. However, for a semantic segmentation task, pooling features over the local neighbourhood may result in the loss of important spatial information, which is conducive for pixel-wise predictions. To address this issue, we introduce a simple yet effective pooling operation called the Haar Wavelet-based Downsampling (HWD) module. This module can be easily integrated into CNNs to enhance the performance of semantic segmentation models. The core idea of HWD is to apply Haar wavelet transform for reducing the spatial resolution of feature maps while preserving as much information as possible. Furthermore, to investigate the benefits of HWD, we propose a novel metric, named as feature entropy index (FEI), which measures the degree of information uncertainty after downsampling in CNNs. Specifically, the FEI can be used to indicate the ability of downsampling methods to preserve essential information in semantic segmentation. Our comprehensive experiments demonstrate that the proposed HWD module could (1) effectively improve the segmentation performance across different modality image datasets with various CNN architectures, and (2) efficiently reduce information uncertainty compared to the conventional downsampling methods. Our implementation are available at https://github.com/apple1986/HWD.}
}
@article{MIN2023109803,
title = {Optimality in high-dimensional tensor discriminant analysis},
journal = {Pattern Recognition},
volume = {143},
pages = {109803},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109803},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005010},
author = {Keqian Min and Qing Mai and Junge Li},
keywords = {Discriminant analysis, Minimax optimality, Tensor},
abstract = {Tensor discriminant analysis is an important topic in tensor data analysis. However, given the many proposals for tensor discriminant analysis methods, there lacks a systematic theoretical study, especially concerning optimality. We fill this gap by providing the minimax lower bounds for the estimation and prediction errors under the tensor discriminant analysis model coupled with the sparsity assumption. We further show that one existing high-dimensional tensor discriminant analysis estimator has matching upper bounds, and is thus optimal. Our results apply to tensors with arbitrary orders and ultra-high dimensions. If one focuses on one-way tensors (i.e., vectors), our results further provide strong theoretical justifications for several popular sparse linear discriminant analysis methods. Numerical studies are also presented to support our theoretical results.}
}
@article{HUANG2023109794,
title = {Graph-based learning of nonlinear physiological interactions for classification of emotions},
journal = {Pattern Recognition},
volume = {143},
pages = {109794},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109794},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004922},
author = {Huiyu Huang and Miaolin Fan and Chun-An Chou},
keywords = {Emotion recognition, Network physiology, Multimodal physiological signals, Dynamical interactions, Graph mining},
abstract = {Emotion recognition has been drawing the attention of researchers and practitioners in recent years. While various research studies show successful applications to recognize and distinguish emotions based on physiological responses using machine learning techniques, less research to date is focused on the network properties of physiological interactions under different emotional states. To this end, we propose a multi-modal graph learning framework to quantify the interactions among physiological systems and present representative networks associated with emotional states. More specifically, we introduce a novel information-theoretic-based time delay stability to quantify complex interactions between physiological modalities. We test our quantification approach on three publicly available benchmark databases for emotion recognition and demonstrate the comparative performances of measuring the interactions of physiological systems in response to emotional states. Finally, we present the visualization of multi-modal physiological network topology, which may be useful for emotional interpretations in practice.}
}
@article{GOYAL2023109742,
title = {Kinship verification using multi-level dictionary pair learning for multiple resolution images},
journal = {Pattern Recognition},
volume = {143},
pages = {109742},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109742},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004405},
author = {Aarti Goyal and Toshanlal Meenpal},
keywords = {Dictionary learning, Multiple resolution images, Multi-level representation, Kinship verification},
abstract = {Kinship verification using facial images is gaining substantial attention by computer vision researchers. The real challenge in kinship verification is to effectively represent the discriminative features to ease the differences between kinship image pairs. Further, existing kinship methods only focus on a single resolution, and ignore the variability of resolutions in practical scenarios. To address these issues, we propose a multi-level dictionary pair learning (MLDPL) method to learn dictionary pairs by incorporating multiple resolution images for kinship verification. We learn dictionary pairs jointly by transforming discriminative features of image pairs into different coding coefficients in the same space, thereby reducing the differences between them. Further, multiple resolution images are incorporated into dictionary pair learning to effectively deal with resolution variations in kinship verification. Extensive experiments are performed on different kinship datasets to validate the efficacy of proposed MLDPL method. Experimental results show that MLDPL achieves competitive performance on all kinship datasets.}
}
@article{2023109855,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {143},
pages = {109855},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(23)00553-8},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005538}
}
@article{JI2023109766,
title = {Higher-order memory guided temporal random walk for dynamic heterogeneous network embedding},
journal = {Pattern Recognition},
volume = {143},
pages = {109766},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109766},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004648},
author = {Cheng Ji and Tao Zhao and Qingyun Sun and Xingcheng Fu and Jianxin Li},
keywords = {Higher order, Dynamic network, Heterogeneous network},
abstract = {Network embedding (NE) aims at learning node embeddings via structure-based sampling. However, there are complex patterns in network structure (heterogeneity, higher-order dependence, dynamics) in the real world. The existing methods suffer from high dependence and constraints on manually designed higher-order structures and loss of fine-grained temporal information. To solve the above challenges, we propose a novel higher-order memory guided temporal random walk for dynamic heterogeneous network embedding (HoMo-DyHNE). The proposed model is a two-stage architecture consisting of a meta-structure-independent random walk algorithm namely HoMo-TRW with transition vectors and higher-order memory, and a Hawkes-based featured Skip-gram (HFSG) incorporating a multivariate Hawkes point process to measure the history-current association intensity. Extensive experiments demonstrate the superior effectiveness of our proposed method.}
}
@article{LIU2023109774,
title = {WSDS-GAN: A weak-strong dual supervised learning method for underwater image enhancement},
journal = {Pattern Recognition},
volume = {143},
pages = {109774},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109774},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004727},
author = {Qiong Liu and Qi Zhang and Wei Liu and Wenbai Chen and Xinwang Liu and Xiangke Wang},
keywords = {Underwater image enhancement, Two-stage learning, Deep learning, CycleGAN},
abstract = {Underwater Image Enhancement (UIE) is a crucial preprocessing step for underwater vision tasks. Addressing the challenge of training supervised deep learning models on large, diverse datasets while learning the intrinsic degradation factors of underwater images is essential for improving model generalization performance. In this paper, we propose a Weak-Strong Dual Supervised Generative Adversarial Network (WSDS-GAN) for UIE. During the first weakly supervised learning phase, unpaired images, consisting of degraded underwater images and clear in-air images, are used to train the model with the goal of recovering color, brightness, and content. In the second strongly supervised learning phase, a limited number of paired images are fed into the model to further train the image detail recovery generator. Comprehensive experiments on public datasets and self-photographed images demonstrate the effectiveness of our proposed method over existing state-of-the-art methods, both qualitatively and quantitatively. Additionally, we show that our method significantly enhances image details to support subsequent underwater vision tasks.}
}
@article{MEHBOOB2023109782,
title = {An encoded histogram of ridge bifurcations and contours for fingerprint presentation attack detection},
journal = {Pattern Recognition},
volume = {143},
pages = {109782},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109782},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004806},
author = {Rubab Mehboob and Hassan Dawood and Hussain Dawood},
keywords = {Biometric systems, fingerprint liveness detection, Fingerprint texture, Live fingerprints, Spoof fingerprints, Minutiae-based features, Presentation attack detection, Liveness detection, Ridge bifurcations, Ridge contours},
abstract = {In recent years, the exponential growth of internet technologies has made personal authentication an integral part of security applications. The fingerprint-based biometric systems are essentially used to safeguard the users' privacy and confidentiality. However, such systems are prone to spoof attacks by artificial replicas of the fingerprints. This paper presents an improved feature extractor called BiRi-PAD (Encoded Histogram of Ridge Bifurcations and Contours for fingerprint Presentation Attack Detection) that aims to enhance the accuracy of live fingerprint detection. The feature extraction process of the proposed BiRi-PAD consists of four steps. First, the fingerprint image undergoes a process of extracting 2-channel ridge contour maps (2-RC maps). The first channel of 2-RC maps consists of ridge contours extracted by a set of derivative filters in the spatial domain whereas the second channel of 2-RC maps consists of the ridge contours extracted by the maximum moments based on phase congruency in the frequency domain. Further, minutiae-based feature information i.e., ridge bifurcations are extracted by the minimum moments based on phase congruency covariance. Moreover, a fusion equation is proposed to integrate 2-RC maps and bifurcations into a single feature map. Second, an improved Comprehensive Local Phase Quantization (CLPQ) based on the well-known feature descriptor Rotation Invariant Local Phase Quantization (LPQri) is proposed to extract the phase information of ridges. CLPQ extracts the orientation of the ridges by using the complex parts of the significant frequency components of LPQri and monogenic filters. Third, the proposed BiRi-PAD quantizes the 2-RC maps into pre-determined intervals. Finally, both 2-RC maps and CLPQ features are integrated to generate a feature vector of a single fingerprint image. Performance evaluations of BiRi-PAD are conducted on three publicly available benchmarks from the LivDet competition, namely LivDet 2013, 2011, and 2015. Experimental evaluations demonstrate that the proposed BiRi-PAD achieves significant reductions in average rates compared to state-of-the-art techniques of fingerprint liveness detection. Specifically, on LivDet 2013, LivDet 2011, and LivDet 2015, the average rates are reduced to 1.92%, 4.39%, and 4.55%, respectively.}
}
@article{QIAN2023109790,
title = {Knowledge transfer evolutionary search for lightweight neural architecture with dynamic inference},
journal = {Pattern Recognition},
volume = {143},
pages = {109790},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109790},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004880},
author = {Xiaoxue Qian and Fang Liu and Licheng Jiao and Xiangrong Zhang and Xinyan Huang and Shuo Li and Puhua Chen and Xu Liu},
keywords = {Neural architecture search (NAS), Knowledge transfer, Dynamic inference, Image classification},
abstract = {Relying on the availability of massive labeled samples, most neural architecture search (NAS) methods focus on searching large and complex models; and adopt fixed structures and parameters at the inference stage. Few approaches automatically design lightweight networks for label-limited tasks and further consider the inference differences between inputs. To address these issues, we introduce evolutionary computation (EC) and attention mechanism and propose a knowledge transfer evolutionary search for lightweight neural architecture with dynamic inference, then verify it using synthetic aperture radar (SAR) images. SAR image classification is a typical label-limited task due to the inherent imaging mechanism of SAR. We design the EC-based architecture search and attention-based dynamic inference for SAR image scene classification. Specifically, we build a SAR-tailored search space, explore topology pruning-based mutation operators to search lightweight architectures, and further design a dynamic Ridgelet convolution capable of adaptive reasoning to enhance the representation ability of searched lightweight networks. Moreover, we propose a knowledge transfer training strategy and hybrid evaluation criteria to ensure searching quickly and robustly. Experimental results show that the proposed method can search for superior neural architectures, thus improving the classification performance of SAR images.}
}
@article{CHEN2023109738,
title = {Integrating topology beyond descriptions for zero-shot learning},
journal = {Pattern Recognition},
volume = {143},
pages = {109738},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109738},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004363},
author = {Ziyi Chen and Yutong Gao and Congyan Lang and Lili Wei and Yidong Li and Hongzhe Liu and Fayao Liu},
keywords = {Zero-shot learning, Topology mining, Image classification},
abstract = {Zero-shot learning (ZSL) aims to discriminate object categories through the identification of their attributes and has received much attention for its capability to predict unseen categories without collecting training data. Recently, excellent works have been devoted to optimizing the model inference by mining the topology among categories/attributes, which proves that the topology learning is beneficial and important for ZSL. However, existing works focus almost exclusively on the construction of semantic topological knowledge with textual descriptions, which, though effective, still suffer from two deficiencies: first, the semantic gap between modalities makes it difficult for the category attributes to accurately describe the corresponding visual characters, resulting in the topology constructed in the semantic modality being distorted in the visual modality; second, it is difficult for one to enumerate all the attributes hidden in images, resulting in an incomplete topology mined only from the defined attributes. Therefore, we propose a Cross-Modality Topology Propagation Matcher (CTPM) to construct a more complete topology system by collaborative mining of topological knowledge in both the visual and semantic modalities. We stand at the dataset level to construct sample-based visual topological knowledge based on the global image features to preserve the integrity of visual information. Meanwhile, we exploit the matching relationship between visual and semantic modalities to make topological knowledge propagate effectively across modalities, and fully enjoy the benefits of multi-modality topological knowledge in category/attribute reasoning. We validate the effectiveness of our CTPM through extensive experiments and achieve state-of-the-art performance on four ZSL datasets.}
}
@article{TERROSOSAENZ2023109807,
title = {Music Mobility Patterns: How Songs Propagate Around The World Through Spotify},
journal = {Pattern Recognition},
volume = {143},
pages = {109807},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109807},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005058},
author = {Fernando Terroso-Saenz and Jesús Soto and Andres Muñoz},
keywords = {Music Streaming Services, Mobility Patterns, Spotify, Prediction},
abstract = {Nowadays, music streaming services allow users to get instant access to an unprecedented amount of music of any type. This entails that songs, including potential new hits, can be discovered by listeners in any part of the globe and their propagation can be tracked through these streaming services. In this context, the present work focuses on recognizing the mobility patterns that songs follow in such a propagation among different countries of the world. To this end, this work defines a novel mechanism to uncover such mobility patterns of music from a directed-graph structure where nodes are countries and each edge reflects a frequent propagation of songs between pairs of countries. The resulting patterns reflect strong correlations with the migratory flows and the cultural and social similarities among regions. For instance, a propagation pattern was observed among North European countries with a good command of English. From such patterns, potential predictors for anticipating the mobility of a song are discussed. The results of this work can be beneficial for record companies and artists in their marketing campaigns for album releases and the organization of concerts and festivals.}
}
@article{SANG2023109746,
title = {Reward shaping with hierarchical graph topology},
journal = {Pattern Recognition},
volume = {143},
pages = {109746},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109746},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004442},
author = {Jianghui Sang and Yongli Wang and Weiping Ding and Zaki Ahmadkhan and Lin Xu},
keywords = {Reinforcement learning, Reward shaping, Probability graph, Markov decision process},
abstract = {Reward shaping using GCNs is a popular research area in reinforcement learning. However, it is difficult to shape potential functions for complicated tasks. In this paper, we develop Reward Shaping with Hierarchical Graph Topology (HGT). HGT propagates information about the rewards through the message passing mechanism, which can be used as potential functions for reward shaping. We describe reinforcement learning by a probability graph model. Then we generate a underlying graph with each state is a node and edges represent transition probabilities between states. In order to prominently shape potential functions for complex environments, HGT divides the underlying graph constructed from states into multiple subgraphs. Since these subgraphs provide a representation of multiple logical relationships between states in the Markov decision process, the aggregation process rich correlation information between nodes, which makes the propagated messages more powerful. When compared to cutting-edge RL techniques, HGT achieves faster learning rates in experiments on Atari and Mujoco tasks.}
}
@article{REZAEI2023109815,
title = {Mixed data clustering based on a number of similar features},
journal = {Pattern Recognition},
volume = {143},
pages = {109815},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109815},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005137},
author = {Hamid Rezaei and Negin Daneshpour},
keywords = {Clustering, Mixed data, Data object distance, Similarity, Cluster center},
abstract = {Finding the degree of similarity measurement is one of the challenges of mixed data clustering. In this article, it has been tried to design a more efficient method by innovating in three important parts of clustering. In the part of the general method, for assigning data objects to the cluster, in addition to the distance, attention is paid to the “number of similar features”. Compared to assigning each object to a cluster, in cases where the distances are equal or close, the cluster center with the highest number of features similar to the given objects will be appropriate. This method is more accurate than the Hamming distance. To determine the cluster centers, instead of random selection, a more suitable object is identified with a distance-based method. In accuracy in three datasets, the proposed algorithm has performed at least two percent better than the other algorithms.}
}
@article{SHAO2023109781,
title = {Conditional pseudo-supervised contrast for data-Free knowledge distillation},
journal = {Pattern Recognition},
volume = {143},
pages = {109781},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109781},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300479X},
author = {Renrong Shao and Wei Zhang and Jun Wang},
keywords = {Model compression, Knowledge distillation, Representation learning, Contrastive learning, Privacy protection},
abstract = {Data-free knowledge distillation (DFKD) is an effective manner to solve model compression and transmission restrictions while retaining privacy protection, which has attracted extensive attention in recent years. Currently, the majority of existing methods utilize a generator to synthesize images to support the distillation. Although the current methods have achieved great success, there are still many issues to be explored. Firstly, the outstanding performance of supervised learning in deep learning drives us to explore a pseudo-supervised paradigm on DFKD. Secondly, current synthesized methods cannot distinguish the distributions of different categories of samples, thus producing ambiguous samples that may lead to an incorrect evaluation by the teacher. Besides, current methods cannot optimize the category-wise diversity samples, which will hinder the student model learning from diverse samples and further achieving better performance. In this paper, to address the above limitations, we propose a novel learning paradigm, i.e., conditional pseudo-supervised contrast for data-free knowledge distillation (CPSC-DFKD). The primary innovations of CPSC-DFKD are: (1) introducing a conditional generative adversarial network to synthesize category-specific diverse images for pseudo-supervised learning, (2) improving the modules of the generator to distinguish the distributions of different categories, and (3) proposing pseudo-supervised contrastive learning based on teacher and student views to enhance diversity. Comprehensive experiments on three commonly-used datasets validate the performance lift of both the student and generator brought by CPSC-DFKD. The code is available at https://github.com/RoryShao/CPSC-DFKD.git}
}
@article{ZHANG2023109762,
title = {An enhanced noise-tolerant hashing for drone object detection},
journal = {Pattern Recognition},
volume = {143},
pages = {109762},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109762},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004600},
author = {Luming Zhang and Guifeng Wang and Ming Chen and Fuji Ren and Ling Shao},
keywords = {Multiple attributes, Attributes fusion, Noise-tolerant, Deep hashing, Drone, Matrix factorization},
abstract = {Drone, a.k.a. Unmanned aerial vehicle (UAV), has been pervasively applied in geological hazard monitoring, smart agriculture, and urban planning in the past decade. In this work, we fuse multiple attributes into a noise-tolerant hashing framework that can detect objects from drone pictures extremely fast. Our method can intrinsically and flexibly encode various topological structures from each target object, based on which multi-scale objects can be discovered in a view- and altitude-invariant way. Moreover, by leveraging lF and l1 norms collaboratively, the calculated hash codes are robust to low quality drone pictures and noisy semantic labels. More specifically, for each drone-borne picture, we extract visually/semantically salient object parts inside it. To characterize their topological structure, we construct a graphlet by linking the spatially adjacent object patches into a small graph. Subsequently, a binary matrix factorization (MF) is designed to hierarchically exploit the semantics of these graphlets, wherein three attributes: i) deep binary hash codes learning, ii) contaminated pictures/labels denoising, and iii) adaptive data graph updating are seamlessly incorporated. Such multi-attribute binary MF can be solved iteratively, and in turn each graphlet is transformed into the binary hash codes. Finally, the hash codes corresponding to graphlets within each drone photo are utilized for ranking-based object discovery. Comprehensive experiments on the DAC-SDC, MOHR, and our self-compiled data set have demonstrated the competitively speed and accuracy of our method. As a byproduct, we employ an elaborately-designed FPGA architecture to calculate our hash codes. On average, a 57 frames per second (fps) object detection speed is achieved on 4K drone videos (without temporal modeling).}
}
@article{YANG2023109786,
title = {Improved polar complex exponential transform for robust local image description},
journal = {Pattern Recognition},
volume = {143},
pages = {109786},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109786},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004843},
author = {Zhanlong Yang and Linzhi Yang and Geng Chen and Pew-Thian Yap},
keywords = {Image description, Local image descriptor, Polar complex exponential transform, Phase information},
abstract = {Image description via robust local descriptors plays a vital role in a large number of image representation and matching applications. In this paper, we propose a novel distinctive local image descriptor that is based on the phase and amplitude information of Polar Complex Exponential Transform (PCET). The proposed descriptor, called IPCET (Improved PCET), is robust to the common photometric transformations (e.g., illumination, noise, JPEG compression, and blur) and geometric transformations (e.g., scaling, rotation, translation, and significant affine distortion). We perform extensive experiments to compare our IPCET descriptor with six most cutting-edge region descriptors (i.e., SIFT, Zernike Moment, GLOH, PCA-SIFT, SURF, and ORB). Experimental results demonstrate that our IPCET descriptor outperforms cutting-edge moment-based descriptors.}
}
@article{BOURSINOS2023109734,
title = {Efficient probability intervals for classification using inductive venn predictors},
journal = {Pattern Recognition},
volume = {143},
pages = {109734},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109734},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004326},
author = {Dimitrios Boursinos and Xenofon Koutsoukos},
keywords = {Deep neural networks, Assurance monitoring, Inductive Venn predictors, Probability intervals},
abstract = {Learning enabled components are frequently used by autonomous systems and it is common for deep neural networks to be integrated in such systems for their ability to learn complex, non-linear data patterns and make accurate predictions in dynamic environments. However, their large number of parameters and their use as black boxes introduce risks as the confidence in each prediction is unknown and output values like softmax scores are not usually well-calibrated. Different frameworks have been proposed to compute accurate confidence measures along with the predictions but at the same time introduce a number of limitations like execution time overhead or inability to be used with high-dimensional data. In this paper, we use the Inductive Venn Predictors framework for computing probability intervals regarding the correctness of each prediction in real-time. We propose taxonomies based on distance metric learning to compute informative probability intervals in applications involving high-dimensional inputs. By assigning pseudo-labels to unlabeled input data during system deployment we further improve the efficiency of the computed probability intervals. Empirical evaluation on image classification and botnet attacks detection in Internet-of-Things (IoT) applications demonstrates improved accuracy and calibration. The proposed method is computationally efficient, and therefore, can be used in real-time. The code is available at https://github.com/dboursinos/Efficient-Probability-Intervals-Classification-Inductive-Venn-Predictors.}
}
@article{MA2023109697,
title = {Inter-layer transition in neural architecture search},
journal = {Pattern Recognition},
volume = {143},
pages = {109697},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109697},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003953},
author = {Benteng Ma and Jing Zhang and Yong Xia and Dacheng Tao},
keywords = {Image processing, Image classification, Neural network, Neural architecture search},
abstract = {Neural architecture search (NAS) attracts much research attention, contributing to its ability to identify better architectures than manually-designed ones. Recently, differential neural architecture search methods have been widely used due to their impressive effectiveness and performance. They represent the network architecture as a repetitive proxy-directed acyclic graph (DAG) and optimize the network weights and architecture weights alternatively in a differential manner. However, existing methods model the architecture weights on each edge (i.e., a layer in the network) as statistically independent variables, ignoring the dependency between edges in DAG induced by their directed topological connections. In this paper, we make the first attempt to investigate such a dependency or relationship by proposing a novel inter-layer transition NAS method. It casts the architecture optimization into a sequential decision process where the dependency between the architecture weights of connected edges is explicitly modeled. Specifically, edges are divided into inner and outer groups according to whether or not their predecessor edges are in the same cell. While the architecture weights of outer edges are optimized independently, those of inner edges are derived sequentially based on the architecture weights of their predecessor edges and the learnable transition matrices in an attentive probability transition manner. Experiments on five benchmark classification datasets, four searching spaces, and NAS-Bench-201 confirm the value of modeling inter-layer dependency and demonstrate the proposed method outperforms other methods.}
}
@article{KIM2023109751,
title = {A discriminative SPD feature learning approach on Riemannian manifolds for EEG classification},
journal = {Pattern Recognition},
volume = {143},
pages = {109751},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109751},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004491},
author = {Byung Hyung Kim and Jin Woo Choi and Honggu Lee and Sungho Jo},
keywords = {Discriminative, EEG, Non-stationary, SPD Matrix, Riemannian, Barycenter},
abstract = {Covariance matrix learning methods have become popular for many classification tasks owing to their ability to capture interesting structures in non-linear data while respecting the Riemannian geometry of the underlying symmetric positive definite (SPD) manifolds. Several deep learning architectures applied to these matrix learning methods have recently been proposed in classification tasks by learning discriminative Euclidean-based embeddings. In this paper, we propose a new Riemannian-based deep learning network to generate more discriminative features for electroencephalogram (EEG) classification. Our key innovation lies in learning the Riemannian barycenter for each class within a Riemannian geometric space. The proposed model normalizes the distribution of SPD matrices and learns the center of each class to penalize the distances between the matrix and the corresponding class centers. As a result, our framework can further simultaneously reduce the intra-class distances, enlarge the inter-class distances for the learned features, and consistently outperform other state-of-the-art methods on three widely used EEG datasets and the data from our stress-induced experiment in virtual reality. Experimental results demonstrate the superiority of the proposed framework for learning the non-stationary nature of EEG signals due to the robustness of the covariance descriptor and the benefits of considering the barycenters on the Riemannian geometry.}
}
@article{XING2023109820,
title = {Contrastive deep support vector data description},
journal = {Pattern Recognition},
volume = {143},
pages = {109820},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109820},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005186},
author = {Hong-Jie Xing and Ping-Ping Zhang},
keywords = {Deep support vector data description, Contrastive learning, Anomaly detection, One-class classification, Hypersphere collapse},
abstract = {In comparison with support vector data description (SVDD), deep SVDD (DSVDD) is more suitable for dealing with large-scale data sets. DSVDD uses mapping network to replace the role of kernel mapping in SVDD. Moreover, the objective of DSVDD is to simultaneously learn the optimal connection weights of mapping network and the minimum volume of hypersphere. To further improve the performance of DSVDD for tackling large-scale data sets and obtain the discriminative features of the given samples in a self-supervised learning manner, contrastive DSVDD (CDSVDD) is proposed in this study. In the pre-training phase of CDSVDD, the contrastive loss and the rotation prediction loss are jointly minimized to achieve the optimal feature representations. Furthermore, the learned feature representations are utilized to determine the hypersphere center. In the training phase of CDSVDD, the distances between the obtained feature representations and the hypersphere center together with the contrastive loss are simultaneously minimized to derive the optimal network connection weights, the minimum volume of hypersphere and the optimal feature representations. In addition, CDSVDD can efficiently solve the hypersphere collapse problem of DSVDD. The ablation study on CDSVDD verifies that compared with the case of determining the hypersphere center by the feature representations of the original samples, the hypersphere center determined by the feature representations of the augmented samples makes CDSVDD achieve better hypersphere boundary and more compact feature representations. Experimental results on the four benchmark data sets demonstrate that the proposed CDSVDD acquires better detection performance in comparison with its six pertinent methods.}
}
@article{CUI2023109759,
title = {Temporal-Relational hypergraph tri-Attention networks for stock trend prediction},
journal = {Pattern Recognition},
volume = {143},
pages = {109759},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109759},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004570},
author = {Chaoran Cui and Xiaojie Li and Chunyun Zhang and Weili Guan and Meng Wang},
keywords = {Stock trend prediction, Stock investment simulation, Hypergraph convolutional networks, Attention mechanism},
abstract = {Predicting the future price trends of stocks is a challenging yet intriguing problem given its critical role to help investors make profitable decisions. In this paper, we present a collaborative temporal-relational modeling framework for end-to-end stock trend prediction. Different from existing studies relying on the pairwise correlations between stocks, we argue that stocks are naturally connected as a collective group, and introduce two heterogeneous hypergraphs to separately characterize the stock group-wise relationships of industry-belonging and fund-holding. A novel hypergraph tri-attention network (HGTAN) is proposed to augment the hypergraph convolutional networks with a hierarchical organization of intra-hyperedge, inter-hyperedge, and inter-hypergraph attention modules. In this manner, HGTAN adaptively determines the importance of nodes, hyperedges, and hypergraphs during the information propagation among stocks, so that the potential synergies between stock movements can be fully exploited. Experimental evaluation and investment simulation on real-world stock data demonstrate the effectiveness of our approach.}
}
@article{ELAMOURI2023109804,
title = {Constrained DTW preserving shapelets for explainable time-series clustering},
journal = {Pattern Recognition},
volume = {143},
pages = {109804},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109804},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005022},
author = {Hussein El Amouri and Thomas Lampert and Pierre Gançarski and Clément Mallet},
keywords = {Shapelets, Semi-supervised learning, Constrained clustering, Time-series, Representation learning},
abstract = {The analysis of time series is becoming ever more popular due to the proliferation of sensors. A well-known similarity measure for time-series is Dynamic Time Warping (DTW), which does not respect the axioms of a metric. These, however, can be reintroduced through Learning DTW-Preserving Shapelets (LDPS). This article extends LDPS and presents constrained DTW-preserving shapelets (CDPS). CDPS directs the time-series representation to captures the user’s interpretation of the data by considering a limited amount of user knowledge in the from of must-link- cannot link constraints. Subsequently, unconstrained algorithms can be used to generate a clustering that respects the constraints without explicit knowledge of them. Out-of-sample data can be transformed into this space, overcoming the limitations of traditional transductive constrained-clustering algorithms. Furthermore, several Shapelet Cluster Explanation (SCE) approaches are proposed that explain the clustering and can simplify the representation while preserving clustering performance. State-of-the-art performance is demonstrated on multiple time-series datasets and an open-source implementation will be made publicly available upon acceptance.}
}
@article{LEE2023109731,
title = {Resampling approach for one-Class classification},
journal = {Pattern Recognition},
volume = {143},
pages = {109731},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109731},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004296},
author = {Hae-Hwan Lee and Seunghwan Park and Jongho Im},
keywords = {One-class classification, Data-driven approach, Oversampling, Calibration},
abstract = {The performance of a classification model depends significantly on the degree to which the support of each data class overlaps. Successfully distinguishing between classes is difficult if the support is similar. In the one-class classification (OCC) problem, wherein the data comprise only a single class, the classifier performance is significantly degraded if the population support of each class is similar. In this study, we propose a resampling algorithm that enhances classifier performance by utilizing the macro information that is most easily obtainable in these two problem situations. The algorithm aims to improve classifier performance by reprocessing the given data into data with mitigated class imbalance through raking and sampling techniques. This performance improvement is demonstrated by comparing representative classifiers used in the existing OCC problem with traditional binary classifier models, which are unavailable on a single-class dataset.}
}
@article{XU2023109787,
title = {Conditional Independence Induced Unsupervised Domain Adaptation},
journal = {Pattern Recognition},
volume = {143},
pages = {109787},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109787},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004855},
author = {Xiao-Lin Xu and Geng-Xin Xu and Chuan-Xian Ren and Dao-Qing Dai and Hong Yan},
keywords = {Domain Adaptation, Discriminant Analysis, Feature Learning, Conditional Independence, Classification},
abstract = {Learning domain-adaptive features is important to tackle the dataset bias problem, where data distributions in the labeled source domain and the unlabeled target domain can be different. The critical issue is to identify and then reduce the redundant information including class-irrelevant and domain-specific features. In this paper, a conditional independence induced unsupervised domain adaptation (CIDA) method is proposed to tackle the challenges. It aims to find the low-dimensional and transferable feature representation of each observation, namely the latent variable in the domain-adaptive subspace. Technically, two mutual information terms are optimized at the same time. One is the mutual information between the latent variable and the class label, and the other is the mutual information between the latent variable and the domain label. Note that the key module can be approximately reformulated as a conditional independence/dependence based optimization problem, and thus, it has a probabilistic interpretation with the Gaussian process. Temporary labels of the target samples and the model parameters are alternatively optimized. The objective function can be incorporated with deep network architectures, and the algorithm is implemented iteratively in an end-to-end manner. Extensive experiments are conducted on several benchmark datasets, and the results show effectiveness of CIDA.}
}
@article{WANG2023109663,
title = {Local nonlinear dimensionality reduction via preserving the geometric structure of data},
journal = {Pattern Recognition},
volume = {143},
pages = {109663},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109663},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003643},
author = {Xiang Wang and Junxing Zhu and Zichen Xu and Kaijun Ren and Xinwang Liu and Fengyun Wang},
keywords = {Dimensionality reduction, Embedding learning, Geometric preservation, Random walk},
abstract = {Dimensionality reduction has many applications in data visualization and machine learning. Existing methods can be classified into global ones and local ones. The global methods usually learn the linear relationship in data, while the local ones learn the manifold intrinsic geometry structure, which has a significant impact on pattern recognition. However, most of existing local methods obtain an embedding with eigenvalue or singular value decomposition, where the computational complexities are very high in a large amount of high-dimensional data. In this paper, we propose a local nonlinear dimensionality reduction method named Vec2vec, which employs a neural network with only one hidden layer to reduce the computational complexity. We first build a neighborhood similarity graph from the input matrix, and then define the context of data points with the random walk properties in the graph. Finally, we train the neural network with the context of data points to learn the embedding of the matrix. We conduct extensive experiments of data classification and clustering on nine image and text datasets to evaluate the performance of our method. Experimental results show that Vec2vec is better than several state-of-the-art dimensionality reduction methods, except that it is equivalent to UMAP on data clustering tasks in the statistical hypothesis tests, but Vec2vec needs less computational time than UMAP in high-dimensional data. Furthermore, we propose a more lightweight method named Approximate Vec2vec (AVec2vec) with little performance degradation, which employs an approximate method to build the neighborhood similarity graph. AVec2vec is still better than some state-of-the-art local dimensionality reduction methods and competitive with UMAP on data classification and clustering tasks in the statistical hypothesis tests.}
}
@article{WANG2023109795,
title = {Hyperspectral anomaly detection based on variational background inference and generative adversarial network},
journal = {Pattern Recognition},
volume = {143},
pages = {109795},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109795},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004934},
author = {Zhiwei Wang and Xue Wang and Kun Tan and Bo Han and Jianwei Ding and Zhaoxian Liu},
keywords = {Background distribution characteristics, GAN, Hyperspectral anomaly detection},
abstract = {Hyperspectral anomaly detection is aimed at detecting targets with significant spectral differences from their surroundings. Recently, deep generative models have been applied to anomaly detections, while the existing generative adversarial network (GAN)-based methods have difficulty in accurately modeling the background and achieving spectrum reconstruction. In this article, a hyperspectral anomaly detection network based on variational background inference and generative adversarial framework (VBIGAN-AD) is proposed. The proposed VBIGAN model can learn the background distribution characteristics of HSIs and enhance the detection performance by the use of reconstruction errors. Specifically, the VBIGAN framework consists of sample and latent GANs, which establishes the relationship between data samples and latent samples through two sub-networks to capture the data distribution. Furthermore, the variational inference method is introduced and the hyperspectral background distribution can be converged to a multivariate normal distribution. To accurately learn the background distribution characteristics and reconstruct the background spectra, the coupling loss is conducted by enforcing feature match in the two discriminators on the basis of composite loss, and the results show that the additional loss can promote the detection performance. As a result, the reconstruction errors generated by the VBIGAN-AD method is utilized to detect abnormal targets. The experiments conducted on five datasets proved the robustness and applicability of the proposed VBIGAN-AD method.}
}
@article{YANG2023109722,
title = {Preferred vector machine for forest fire detection},
journal = {Pattern Recognition},
volume = {143},
pages = {109722},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109722},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300420X},
author = {Xubing Yang and Zhichun Hua and Li Zhang and Xijian Fan and Fuquan Zhang and Qiaolin Ye and Liyong Fu},
keywords = {Forest fire detection, Fire detection rate, Error warning rate, SVM, Dual representation},
abstract = {Machine learning-based fire detection/recognition is very popular in forest-monitoring systems. However, without considering the prior knowledge, e.g., equal attention on both classes of the fire and non-fire samples, fire miss-detected phenomena frequently appeared in the current methods. In this work, considering model’s interpretability and the limited data for model-training, we propose a novel pixel-precision method, termed as PreVM (Preferred Vector Machine). To guarantee high fire detection rate under precise control, a new L0 norm constraint is introduced to the fire class. Computationally, instead of the traditional L1 re-weighted techniques in L0 norm approximation, this L0 constraint can be converted into linear inequality and incorporated into the process of parameter selection. To further speed up model-training and reduce error warning rate, we also present a kernel-based L1 norm PreVM (L1-PreVM). Theoretically, we firstly prove the existence of dual representation for the general Lp (p≥1) norm regularization problems in RKHS (Reproducing Kernel Hilbert Space). Then, we provide a mathematical evidence for L1 norm kernelization to conquer the case when feature samples do not appear in pairs. The work also includes an extensive experimentation on the real forest fire images and videos. Compared with the-state-of-art methods, the results show that our PreVM is capable of simultaneously achieving higher fire detection rates and lower error warning rates, and L1-PreVM is also superior in real-time detection.}
}
@article{MIAO2023109743,
title = {SMPR: Single-stage multi-person pose regression},
journal = {Pattern Recognition},
volume = {143},
pages = {109743},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109743},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004417},
author = {Huixin Miao and Junqi Lin and Junjie Cao and Xiaoguang He and Zhixun Su and Risheng Liu},
keywords = {Pose estimation, Dense prediction, Instance-aware keypoints, Pose scoring},
abstract = {Existing multi-person pose estimators can be roughly divided into two-stage approaches (top-down and bottom-up approaches) and one-stage approaches. The two-stage methods either suffer high computational redundancy for additional person detectors or group keypoints heuristically after predicting all the instance-free keypoints. The recently proposed single-stage methods do not rely on the above two extra stages but have lower performance than the latest bottom-up approaches. In this work, a novel single-stage multi-person pose regression, termed SMPR, is presented. It follows the paradigm of dense prediction and predicts instance-aware keypoints from every location. Besides feature aggregation, we propose better strategies to define positive pose hypotheses for training which all play an important role in dense pose estimation. The network also learns the scores of estimated poses. The pose scoring strategy further improves the pose estimation performance by prioritizing superior poses during non-maximum suppression (NMS). We show that our method not only outperforms existing single-stage methods but also be competitive with the latest bottom-up methods, with 70.2 AP and 77.5 AP75 on the COCO test-dev pose benchmark. The code is available at https://github.com/cmdi-dlut/SMPR .}
}
@article{DELGADOSANTOS2023109798,
title = {Exploring transformers for behavioural biometrics: A case study in gait recognition},
journal = {Pattern Recognition},
volume = {143},
pages = {109798},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109798},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300496X},
author = {Paula Delgado-Santos and Ruben Tolosana and Richard Guest and Farzin Deravi and Ruben Vera-Rodriguez},
keywords = {Biometrics, Behavioural biometrics, Gait recognition, Deep learning, Transformers, Mobile devices},
abstract = {Biometrics on mobile devices has attracted a lot of attention in recent years as it is considered a user-friendly authentication method. This interest has also been motivated by the success of Deep Learning (DL). Architectures based on Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) have established convenience for the task, improving the performance and robustness in comparison to traditional machine learning techniques. However, some aspects must still be revisited and improved. To the best of our knowledge, this is the first article that explores and proposes a novel gait biometric recognition systems based on Transformers, which currently obtain state-of-the-art performance in many applications. Several state-of-the-art architectures (Vanilla, Informer, Autoformer, Block-Recurrent Transformer, and THAT) are considered in the experimental framework. In addition, new Transformer configurations are proposed to further increase the performance. Experiments are carried out using the two popular public databases: whuGAIT and OU-ISIR. The results achieved prove the high ability of the proposed Transformer, outperforming state-of-the-art CNN and RNN architectures.}
}
@article{WANG2023109775,
title = {Learning pixel-adaptive weights for portrait photo retouching},
journal = {Pattern Recognition},
volume = {143},
pages = {109775},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109775},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004739},
author = {Binglu Wang and Chengzhe Lu and Dawei Yan and Yongqiang Zhao and Ning Li and Xuelong Li},
keywords = {Portrait photo retouching, 3D Lookup table, Visual attention},
abstract = {The lookup table-based methods achieve promising retouching performance by learning image-adaptive weights to combine 3-dimensional lookup tables (3D LUTs) and conducting pixel-to-pixel color transformation. However, this paradigm ignores the local context cues and applies the same transformation to portrait pixels and background pixels that exhibit the same raw RGB values. In contrast, an expert usually conducts different operations to adjust the color temperatures, tones of portrait regions, and background regions. This inspires us to model local context cues to improve the retouching quality explicitly.Thus, the center pixel of an image patch is first retouched by predicting pixel-adaptive lookup table weights. To modulate the influence of neighboring pixels, as neighboring pixels exhibit different affinities to the center pixel, a local attention mask is estimated. Then, the quality of the local attention mask is further improved by applying supervision, which is based on the affinity map calculated by the ground-truth portrait mask. For group-level consistency, we propose to directly constrain the variance of mean color components in the Lab space. Extensive experiments on the PPR10K dataset demonstrate the effectiveness of the proposed method, the retouching performance on high-resolution photos is improved by over 0.5dB in terms of PSNR, and the group-level inconsistency is reduced by 2.1.}
}
@article{WU2023109783,
title = {An efficient EM algorithm for two-layer mixture model of gaussian process functional regressions},
journal = {Pattern Recognition},
volume = {143},
pages = {109783},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109783},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004818},
author = {Di Wu and Yurong Xie and Zhe Qiang},
keywords = {Mixture of gaussian processes, Hierarchical mixture of experts, Classification EM algorithm, Computational efficiency, Local maximum problem, Curve clustering},
abstract = {The mixture of Gaussian processes is effective for regression, but it cannot handle the non-stationary curve clustering problem well. The two-layer mixture of Gaussian process functional regressions (TMGPFR) model was established to deal with this problem. In this paper, we first propose the classification EM (CEM) algorithm to solve that the optimization algorithm is inefficient for TMGPFRs, and then propose the deterministic annealing CEM algorithm for TMGPFRs to overcome the local maximum problem of the CEM algorithm. Lastly, experiments are conducted on synthetic and real-world data sets, and the results show that our proposed algorithms are more effective than the compared algorithms on curve clustering and regression.}
}
@article{HUANG2023109715,
title = {Longitudinal prediction of postnatal brain magnetic resonance images via a metamorphic generative adversarial network},
journal = {Pattern Recognition},
volume = {143},
pages = {109715},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109715},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004132},
author = {Yunzhi Huang and Sahar Ahmad and Luyi Han and Shuai Wang and Zhengwang Wu and Weili Lin and Gang Li and Li Wang and Pew-Thian Yap},
keywords = {Infant brain MRI, Longitudinal prediction, Metamorphic GAN},
abstract = {Missing scans are inevitable in longitudinal studies due to either subject dropouts or failed scans. In this paper, we propose a deep learning framework to predict missing scans from acquired scans, catering to longitudinal infant studies. Prediction of infant brain MRI is challenging owing to the rapid contrast and structural changes particularly during the first year of life. We introduce a trustworthy metamorphic generative adversarial network (MGAN) for translating infant brain MRI from one time point to another. MGAN has three key features: (i) Image translation leveraging spatial and frequency information for detail-preserving mapping; (ii) Quality-guided learning strategy that focuses attention on challenging regions. (iii) Multi-scale hybrid loss function that improves translation of image contents. Experimental results indicate that MGAN outperforms existing GANs by accurately predicting both tissue contrasts and anatomical details.}
}
@article{VINDAS2023109812,
title = {Guided deep embedded clustering regularization for multifeature medical signal classification},
journal = {Pattern Recognition},
volume = {143},
pages = {109812},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109812},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005101},
author = {Yamil Vindas and Emmanuel Roux and Blaise Kévin Guépié and Marilys Almar and Philippe Delachartre},
keywords = {Multifeature learning, Deep regularization, Guided training, Signal classification, Transcranial doppler},
abstract = {Medical signal classification often focuses on one representation (raw signal or time frequency). In that context, recent works have shown the value of exploiting different representations simultaneously. We propose a regularized end-to-end trained model for classification in a medical context exploiting both the raw signal and a time-frequency representation (TFR). First, a 2D convolutional neural network (CNN) encoder and a 1D CNN-transformer encoder start by extracting embedded representations from the TFR and the raw signal, respectively. Then, the obtained embeddings are fused to form a common latent space that is used for classification. We propose to guide the training of each encoder by applying two iterated losses. Moreover, we propose to regularize the fused common latent space using deep embedded clustering. Extensive experiments on three medical datasets and ablation studies show the adaptability and good performance of our method for medical signal classification. Our method makes it possible to improve the classification performance from 4% to 12% MCC on a transcranial Doppler dataset, when compared with single-feature counterparts, while giving more stable models. The code is available at: https://github.com/gdec-submission/gdec/.}
}
@article{KALE2023109791,
title = {Face age synthesis: A review on datasets, methods, and open research areas},
journal = {Pattern Recognition},
volume = {143},
pages = {109791},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109791},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004892},
author = {Ayşe Kale and Oğuz Altun},
keywords = {Age progression, Age regression, Face aging, GANs},
abstract = {Face age synthesis is the determination of how a person looks in the future or the past by reconstructing their facial image. Determining the change in the human face over the years is a critical process for cross-age face recognition systems in forensic issues such as finding missing people and fugitive criminals. Therefore, it is a subject that has attracted attention in recent years. With the implementation of deep learning methods, better quality and photo-realistic images began to be produced. However, researchers continue to improve both aging accuracy and identity preservation requirements. We group the studies in the literature under two categories: classical methods and deep learning methods. We review both categories in the methods used, evaluation methods, and databases.}
}
@article{LIU2023109739,
title = {FedCL: Federated contrastive learning for multi-center medical image classification},
journal = {Pattern Recognition},
volume = {143},
pages = {109739},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109739},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004375},
author = {Zhenbing Liu and Fengfeng Wu and Yumeng Wang and Mengyu Yang and Xipeng Pan},
keywords = {Federated learning, Contrastive learning, Image classification},
abstract = {Federated learning, which allows distributed medical institutions to train a shared deep learning model with privacy protection, has become increasingly popular recently. However, in practical application, due to data heterogeneity between different hospitals, the performance of the model will be degraded in the training process. In this paper, we propose a federated contrastive learning (FedCL) approach. FedCL integrates the idea of contrastive learning into the federated learning framework. Specifically, it combines the local model and the global model for contrastive learning, so that the local model gradually approaches the global model with the increase of communication rounds, which improves the generalization ability of the model. We validate our method on two public datasets. Extensive experiments show that our method is superior to other federated learning algorithms in medical image classification.}
}
@article{YI2023109799,
title = {Graph classification via discriminative edge feature learning},
journal = {Pattern Recognition},
volume = {143},
pages = {109799},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109799},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004971},
author = {Yang Yi and Xuequan Lu and Shang Gao and Antonio Robles-Kelly and Yuejie Zhang},
keywords = {GCNNs, Graph construction, Graph datasets, Graph classification},
abstract = {Spectral graph convolutional neural networks (GCNNs) have been producing encouraging results in graph classification tasks. However, most spectral GCNNs utilize fixed graphs when aggregating node features while omitting edge feature learning and failing to get an optimal graph structure. Moreover, many existing graph datasets do not provide initialized edge features, further restraining the ability of learning edge features via spectral GCNNs. In this paper, we try to address this issue by designing an edge feature scheme and an add-on layer between every two stacked graph convolution layers in spectral GCNN. Both are lightweight while effective in filling the gap between edge feature learning and performance enhancement of graph classification. The edge feature scheme makes edge features adapt to node representations at different spectral graph convolution layers. The add-on layer helps adjust the edge features to an optimal graph structure. To test the effectiveness of our method, we take Euclidean positions as initial node features and extract graphs with semantic information from point cloud objects. The node features of our extracted graphs are more scalable for edge feature learning than most existing graph datasets (in one-hot encoded label format). Three new graph datasets are constructed based on ModelNet40, ModelNet10 and ShapeNet Part datasets. Experimental results show that our method outperforms state-of-the-art graph classification methods on the new datasets. Our code and the constructed graph datasets will be released to the community.}
}
@article{DING2023109747,
title = {Deep forest auto-Encoder for resource-Centric attributes graph embedding},
journal = {Pattern Recognition},
volume = {143},
pages = {109747},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109747},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004454},
author = {Yan Ding and Yujuan Zhai and Ming Hu and Jia Zhao},
keywords = {Graph embedding, Deep random forest, Deep auto-encoder, Self-attention},
abstract = {Graph embedding is an important technique used for representing graph structure data that preserves intrinsic features in a low-dimensional space suitable for graph-based applications. Graphs containing node attributes and weighted links are commonly employed to model various real-world problems and issues in computer science. In recent years, a hot research topic has been the exploitation of diverse information, including node attributes and topological semantic information, in graph embedding. However, due to limitations in deep learning based on neural networks, such information has not been fully utilized nor adequately integrated in existing models, leaving graph embedding unsatisfactory, especially for large resource graphs (e.g., knowledge graphs and task interaction graphs). In this study, we introduce a resource-centric graph embedding approach based on deep random forests learning, which reconstructs graphs using a deep autoencoder to achieve high effectiveness. To accomplish this, our approach employs three key components. The first component is a preprocessor driven by graph similarity, alongside modularity and self-attention modules, to comprehensively integrate graph representation. The second component utilizes local graph information structures to enhance the raw graph. Finally, we integrate diverse information using multi-grained scanning and dual-level cascade forests in the deep learning extractor and generator, ultimately producing the final graph embedding. Experimental results on seven real-world scenarios show that our approach outperforms state-of-the-art embedding methods.}
}
@article{CAI2023109771,
title = {Domain embedding transfer for unequal RGB-D image recognition},
journal = {Pattern Recognition},
volume = {143},
pages = {109771},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109771},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004697},
author = {Ziyun Cai and Xiao-Yuan Jing and Ling Shao},
keywords = {Domain adaptation, RGB-D data, Visual categorization, Unequal category},
abstract = {Most recent unsupervised domain adaptation (UDA) approaches concentrate on single RGB source to single RGB target task. They have to face the real-world scenario, where the source domain can be collected from multiple modalities, e.g., RGB data and depth data. Our work focuses on a more practical and challenging scenario which recognizes RGB images by learning from RGB-D data under the label inequality scenario. We are confronted with three challenges: multiple modalities in the source domain, domain shifting problem and unequal label numbers. To address the aforementioned settings, a novel method, referred to as Domain depth Embedding Transfer (DdET) is proposed, which takes advantage of the depth data in the source domain and handles the domain distribution mismatch under label inequality scenario simultaneously. We conduct comprehensive experiments on five cross domain image classification tasks and observe that DdET can perform favorably against state-of-the-art methods, especially under label inequality scenario.}
}
@article{SHI2023109750,
title = {Source-free and black-box domain adaptation via distributionally adversarial training},
journal = {Pattern Recognition},
volume = {143},
pages = {109750},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109750},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300448X},
author = {Yucheng Shi and Kunhong Wu and Yahong Han and Yunfeng Shao and Bingshuai Li and Fei Wu},
keywords = {Source-free unsupervised domain adaptation, Distributionally adversarial training, Data and model privacy, Black-box probe},
abstract = {Source-free unsupervised domain adaptation is one class of practical deep learning methods which generalize in the target domain without transferring data from source domain. However, existing source-free domain adaptation methods rely on source model transferring. In many data-critical scenarios, the transferred source models may suffer from membership inference attacks and expose private data. In this paper, we aim to overcome a more practical and challenging setting where the source models cannot be transferred to the target domain. The source models are considered as queryable black-box models which only output hard labels. We use public third-party data to probe the source model and obtain supervision information, dispensing with transferring source model. To fill the gap between third-party data and target data, we further propose Distributionally Adversarial Training (DAT) to align the distribution of third-party data with target data, gain more informative query results and improve the data efficiency. We call this new framework Black-box Probe Domain Adaptation (BPDA) which adopts query mechanism and DAT to probe and refine supervision information. Experimental results on several domain adaptation datasets demonstrate the practicability and data efficiency of BPDA in query-only and source-free unsupervised domain adaptation.}
}
@article{MAO2023109779,
title = {Multi-proxy feature learning for robust fine-grained visual recognition},
journal = {Pattern Recognition},
volume = {143},
pages = {109779},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109779},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004776},
author = {Shunan Mao and Yaowei Wang and Xiaoyu Wang and Shiliang Zhang},
keywords = {Fine-grained visual recognition, Noisy label, Long tail, Proxy learning},
abstract = {Visual representation for fine-grained visual recognition can be learned by mandatorily enforcing all samples of the same category into a uniform representation. This strict training objective performs well under closed-set setting but is not applicable to data in the wild containing noisy annotations and long-tailed distributions, e.g., it may lead to a feature space biased to head categories. This paper tackles this challenge by pursuing a more balanced and discriminative feature space by first retaining intra-class variances to isolate noises, then eliminating intra-class variances to improve the visual recognition performance. We propose the Compact Memory Updater to maintain a memory bank, which memorizes proxy features to represent multiple typical appearances of each category in the training set. The Proxy-based Feature Enhancement hence leverages proxy features to ensure samples of the same category have similar features. Iteratively running those two modules boosts the robustness and discriminative power of the learnt representation, hence facilitates various fine-grained visual recognition tasks including person re-identification (re-id), image classification and retrieval. Extensive experiments on noisy and long-tailed training sets show this Multi-Proxy Feature Learning (MPFL) framework achieves promising performance. For instance on a training set with 90% one-shot categories, MPFL outperforms the recent long-tailed person re-id method LEAP-AF by 16.9% in rank-1 accuracy.}
}
@article{LIU2023109727,
title = {Trigonometric projection statistics histograms for 3D local feature representation and shape description},
journal = {Pattern Recognition},
volume = {143},
pages = {109727},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109727},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004259},
author = {Xingsheng Liu and Anhu Li and Jianfeng Sun and Zhiyong Lu},
keywords = {3D feature descriptor, Local reference frame, Trigonometric projection mechanism, Object recognition, Shape registration},
abstract = {Feature representation as a significant approach to three-dimensional (3D) shape description has been widely employed in computer vision. However, most existing methods are suffering from the emerging challenges for descriptiveness, robustness and efficiency. This paper presents a novel feature descriptor named trigonometric projection statistics histograms (TPSH). By constructing the repeatable local reference frame based on a multi-attribute weighting strategy, TPSH can address many prevailing nuisances such as noise, occlusion and varying resolution. The trigonometric projection mechanism is originally proposed for TPSH generation, which combines two perspective views to encode both spatial distribution and geometrical measurements from local shape into statistics histograms. The experimental evaluation on public datasets proves that TPSH outperforms state-of-the-art methods in descriptiveness and robustness while maintaining storage compactness and computational efficiency. It is demonstrated that TPSH can not only be suited for 3D object recognition and shape registration, but also generalized across various acquisition devices, data modalities and application scenarios.}
}
@article{ZHANG2023109763,
title = {Switching clusters’ synchronization for discrete space-time complex dynamical networks via boundary feedback controls},
journal = {Pattern Recognition},
volume = {143},
pages = {109763},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109763},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004612},
author = {Tianwei Zhang and Zhouhong Li},
keywords = {Complex networks, Space-time discretization, Switching system, Wirtinger’s inequality, Cluster synchronization},
abstract = {Unlike the existing literatures that consider only discrete-time networks, this paper explores the double effects of both discrete time and discrete spatial diffusions in a switching complex dynamical networks. By means of the knowledge of clusters controls, a clusters synchronous frame of space-time discrete switching complex networks with boundary feedback controller is newly proposed and established. With the helps of some indispensable vector-valued sequence inequalities and Lyapunov function with switching signals and clusters’ information, the boundary feedback controllers are designed to synchronize space-time discrete switching complex networks coupled with nodes’ states or spatial diffusions in the form of clusters. Additionally, a realizable computer algorithm is given to make the derived results of this paper easier to enforce. The current work is pioneering in consideration of discrete spatial diffusions and provides a theoretical and practical basis for future research in this regard.}
}
@article{WANG2023109809,
title = {Information-diffused graph tracking with linear complexity},
journal = {Pattern Recognition},
volume = {143},
pages = {109809},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109809},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005071},
author = {Zhixing Wang and Jinzhen Yao and Chuanming Tang and Jianlin Zhang and Qiliang Bao and Zhenming Peng},
keywords = {Siamese tracker, Graph neural network, Attention mechanism, Linear attention},
abstract = {Mainstream tracking approaches have achieved remarkable performance by adopting transformer structures. However, transformer structures’ inherent design of dot-product with softmax normalization incurs quadratic computation complexity regarding sequence length. This issue is further complicated when vision tasks employ softmax attention, as sequence length scales with the square of images’ sizes. Even though sparse attention and low-rank decomposition can alleviate over-inflated computation, it is still laborious to balance trackers’ accuracy, computation cost, and inference speed. To tackle the above problems, we propose an Information-Diffused Graph tracking pipeline with linear complexity (IDGtrack). As the feature constraint relationship in the physical world is an important cue for vision tasks, graph modules are constructed with information-diffused adjacency matrices to substitute softmax attention, which is not only efficient for linear computations but also maintains the non-negativity and global distribution of the attention matrix. Distinct from traditional linear attention methods exclusive to self-attention, a self-integrated and cross-context graph module with linear complexity is explored where a complete bipartite graph is established between the target and search region, facilitating a comprehensive perception of local and background information. Extensive experiments are conducted on public tracking benchmarks, demonstrating that our method achieves state-of-the-art (SOTA) performance with 111 FPS on GPU RTX3090.}
}
@article{HU2023109760,
title = {Attention‐guided evolutionary attack with elastic‐net regularization on face recognition},
journal = {Pattern Recognition},
volume = {143},
pages = {109760},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109760},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004582},
author = {Cong Hu and Yuanbo Li and Zhenhua Feng and Xiaojun Wu},
keywords = {Face recognition, Convolutional neural networks, Adversarial examples, Evolutionary attack, Attention mechanisms},
abstract = {In recent years, face recognition has achieved promising results along with the development of advanced Deep Neural Networks (DNNs). The existing face recognition systems are vulnerable to adversarial examples, which brings potential security risks. Evolutionary Attack (EA) has been successfully used to fool face recognition by inducing a minimum perturbation to a face image with few queries. However, EA employs the global information of face images but ignores their local characteristics. In addition, restricting the ℓ2-norm of adversarial perturbations hinders the diversity of adversarial perturbations. To solve the above problems, we propose Attention-guided Evolutionary Attack with Elastic-Net Regularization (ERAEA) for attacking face recognition. ERAEA extracts local facial characteristics by attention mechanism, effectively improving the attack effect and image perception quality. In particular, ERAEA adopts an attention mechanism to guide evolutionary direction, which operates on the covariance matrix as it contains crucial information about the evolutionary path. Furthermore, we design an adaptive elastic-net regularization to diversify the adversarial perturbation, accelerating the optimization performance. Extensive experiments obtained on three benchmarks demonstrate that our proposed method achieves better perturbation norm than the state-of-the-art methods with limited queries on face recognition and generates adversarial face images with higher perceptual quality. Besides, ERAEA requires fewer queries to achieve a fixed adversarial perturbation norm.}
}
@article{RASHIDI2023109694,
title = {An active foveated gaze prediction algorithm based on a Bayesian ideal observer},
journal = {Pattern Recognition},
volume = {143},
pages = {109694},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109694},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003928},
author = {Shima Rashidi and Weilun Xu and Dian Lin and Andrew Turpin and Lars Kulik and Krista Ehinger},
keywords = {Eye movements, Visual search, Bayesian ideal observer},
abstract = {Predicting human eye movements is a crucial task for understanding human behavior and has numerous applications in machine vision. Most current models for predicting eye movements are data-driven and require large datasets of recorded eye movements, which can be expensive and time-consuming to collect. In this paper, we present a novel theory-based model for predicting eye movements in a foveated visual system that maximizes information gain at each fixation. Our model uses a region-proposal network and eccentricity-based max pooling to account for the loss of detail in peripheral vision. We apply our model to predict human fixations in a visual search task for objects in real-world scenes. Unlike data-driven models, our model does not require training on large eye movement datasets and can generalize to any set of natural images and targets. We evaluate the generalization capability of our model by demonstrating its results on two publicly available visual search datasets, Ehinger and COCO-search18, without any further training on those datasets. Our model outperforms or performs comparably to data-driven models that are directly trained on human eye movement datasets.}
}
@article{DIALLO2023109764,
title = {Auto-attention mechanism for multi-view deep embedding clustering},
journal = {Pattern Recognition},
volume = {143},
pages = {109764},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109764},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004624},
author = {Bassoma Diallo and Jie Hu and Tianrui Li and Ghufran Ahmad Khan and Xinyan Liang and Hongjun Wang},
keywords = {Deep embedding clustering, Deep multi-view clustering, Multi-view autoencoder, Auto-attention},
abstract = {In several fields, deep learning has achieved tremendous success. Multi-view learning is a workable method for handling data from several sources. For clustering multi-view data, deep learning and multi-view learning are excellent options. However, a persistent challenge is a need for the current deep learning approach to independently drive divergent neural networks for different perspectives while working with multi-view data. The current methods use the number of viewpoints to calculate neural network statistics. Consequently, as the number of views rises, it results in a considerable calculation. Furthermore, they vainly try to unite various viewpoints at the training. Incorporating a triple fusion technique, this research suggests an innovative multi-view deep embedding clustering (MDEC) model. The suggested model can jointly acquire the specific knowledge in each view as well as the information fragment of the collective views. The main goal of the MDEC is to lower the errors made when learning the features of each view and correlating data from many views. To address the optimization problem, the MDEC model advises a suitable iterative updating approach. In testing modern deep learning and non-deep learning algorithms, the experimental study on small and large-scale multi-view data shows encouraging results for the MDEC model. In multi-view clustering, this work demonstrates the benefit of the deep learning-based approach over the non-ones. However, future work will address a variety of issues related to MDEC including the speed.}
}
@article{KOU2023109788,
title = {Infrared small target segmentation networks: A survey},
journal = {Pattern Recognition},
volume = {143},
pages = {109788},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109788},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004867},
author = {Renke Kou and Chunping Wang and Zhenming Peng and Zhihe Zhao and Yaohong Chen and Jinhui Han and Fuyu Huang and Ying Yu and Qiang Fu},
keywords = {Infrared small target, Characteristic analysis, Segmentation network, Deep learning, Collaborative technology, Data-driven, False alarm, Missed detection},
abstract = {Fast and robust small target detection is one of the key technologies in the infrared (IR) search and tracking systems. With the development of deep learning, there are many data-driven IR small target segmentation algorithms, but they have not been extensively surveyed; we believe our proposed survey is the first to systematically survey them. Focusing on IR small target segmentation tasks, we summarized 7 characteristics of IR small targets, 3 feature extraction methods, 8 design strategies, 30 segmentation networks, 8 loss functions, and 13 evaluation indexes. Then, the accuracy, robustness, and computational complexities of 18 segmentation networks on 5 public datasets were compared and analyzed. Finally, we have discussed the existing problems and future trends in the field of IR small target detection. The proposed survey is a valuable reference for both beginners adapting to current trends in IR small target detection and researchers already experienced in this field.}
}
@article{ZHU2023109772,
title = {Tri-HGNN: Learning triple policies fused hierarchical graph neural networks for pedestrian trajectory prediction},
journal = {Pattern Recognition},
volume = {143},
pages = {109772},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109772},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004703},
author = {Wenjun Zhu and Yanghong Liu and Peng Wang and Mengyi Zhang and Tian Wang and Yang Yi},
keywords = {Trajectory prediction, Hierarchical policy, Graph neural networks,},
abstract = {In complex and dynamic urban traffic scenarios, the accurate trajectory prediction of surrounding pedestrians with interactive behaviors plays a vital role in the self-driving system. Intrinsic factors and extrinsic factors will inevitably influence the pedestrians trajectory. Intrinsic factors such as pedestrians diversified intentions bring rich and diverse multi-modal future possibilities. Besides, extrinsic factors affecting the future trajectory are accompanied by context semantics such as interactions among pedestrians. However, most of the existing methods discuss two problems (interaction and intention) separately. Considering both two factors impact the trajectory of pedestrians, a Triple Policies Fused Hierarchical Graph Neural Networks (Tri-HGNN) is proposed to model spatial and temporal interactions and intentions among the whole scene of pedestrians at each time step and predict the multiple future trajectories. Tri-HGNN contains three different policies: (i) Extrinsic-level policy is used to extract spatial nodes embedding from the interaction graph of pedestrian trajectories by using the Graph Attention Network. (ii) Intrinsic-level policy adopts the Graph Convolutional Network to infer the human intention for more accurate prediction. Moreover, human intention is influenced by the intrinsic interaction generated among pedestrians, so we fuse the interaction features to grasp the influence of the extrinsic interaction. (iii) Basic-level policy then integrates the heuristic information obtained from other two policies and concatenates it with historical trajectories to make multiple predictions through Temporal Convolution Network. Experimental results show that our model improves performance compared with state-of-the-art methods on the ETH/UCY and SDD benchmarks.}
}
@article{ZHAO2023109796,
title = {APUNet: Attention-guided upsampling network for sparse and non-uniform point cloud},
journal = {Pattern Recognition},
volume = {143},
pages = {109796},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109796},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004946},
author = {Tianming Zhao and Linfeng Li and Tian Tian and Jiayi Ma and Jinwen Tian},
keywords = {Point cloud upsampling, Distance prior, DisTransformer, Attention mechanism, Feature prediction},
abstract = {Point cloud upsampling is a basic low-level task, that is important for improving the quality of a point cloud. However, existing point cloud upsampling methods perform poorly on sparse and non-uniform point clouds, due to that they fail to fully model the relationship between points. To address this issue, in this paper, we propose an attention-guided network called APUNet to exploit the correlation between points, which can perform unsampling for sparse and non-uniform point cloud. In particular, we first propose a feature extraction unit, DisTransformer, which can effectively model the relationship between points by introducing a distance prior to the attention mechanism. We also design a point cloud feature extraction network based on DisTransformer. By computing the correlation between patches and the correlation between points, we fuse the global and local features to better model the correlation of the whole object. Furthermore, we propose a feature prediction module based on attention mechanisms that avoids generating clustered points by transforming the point cloud expansion task into a point cloud prediction task. Qualitative and quantitative experiments reveal the superiority of our method compared to the current state-of-the-art methods. Compared with other point cloud upsampling methods, APUNet can much better upsample non-uniform and extremely sparse point clouds.}
}
@article{JI2023109744,
title = {Tri-objective optimization-based cascade ensemble pruning for deep forest},
journal = {Pattern Recognition},
volume = {143},
pages = {109744},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109744},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004429},
author = {Junzhong Ji and Junwei Li},
keywords = {Ensemble learning, Ensemble pruning, Deep forest, Multi-objective optimization, Coupled diversity},
abstract = {Deep forest is a new multi-layer ensemble model, where the high time costs and storage requirements inhibit its large-scale application. However, current deep forest pruning methods used to alleviate these drawbacks do not consider its cascade coupling characteristics. Therefore, we propose a tri-objective optimization-based cascade ensemble pruning (TOOCEP) algorithm for it. Concretely, we first present a tri-objective optimization-based single-layer pruning (TOOSLP) method to prune its single-layer by simultaneously optimizing three objectives, namely accuracy, independent diversity, and coupled diversity. Particularly, the coupled diversity is designed for deep forest to deal with the coupling relationships between its adjacent layers. Then, we perform TOOSLP in a cascade framework to prune the deep forest layer-by-layer. Experimental results on 15 UCI datasets show that TOOCEP outperforms several state-of-the-art methods in accuracy and pruned rate, which significantly reduces the storage space and accelerate the prediction speed of deep forest.}
}
@article{NOVOAPARADELA2023109805,
title = {Fast deep autoencoder for federated learning},
journal = {Pattern Recognition},
volume = {143},
pages = {109805},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109805},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005034},
author = {David Novoa-Paradela and Oscar Fontenla-Romero and Bertha Guijarro-Berdiñas},
keywords = {Deep autoencoder, Anomaly detection, Federated learning, Edge computing, Machine learning},
abstract = {This paper presents a novel, fast and privacy preserving implementation of deep autoencoders. DAEF (Deep AutoEncoder for Federated learning), unlike traditional neural networks, trains a deep autoencoder network in a non-iterative way, which drastically reduces training time. Training can be performed incrementally, in parallel and distributed and, thanks to its mathematical formulation, the information to be exchanged does not endanger the privacy of the training data. The method has been evaluated and compared with other state-of-the-art autoencoders, showing interesting results in terms of accuracy, speed and use of available resources. This makes DAEF a valid method for edge computing and federated learning, in addition to other classic machine learning scenarios.}
}
@article{MA2023109716,
title = {Optimal transport based pyramid graph kernel for autism spectrum disorder diagnosis},
journal = {Pattern Recognition},
volume = {143},
pages = {109716},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109716},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004144},
author = {Kai Ma and Shuo Huang and Peng Wan and Daoqiang Zhang},
keywords = {Optimal transport, Brain network, Graph kernel, Autism spectrum disorder, fMRI},
abstract = {Brain network, which characterizes the functional and structural interactions of brain regions with graph theory, has been widely utilized to diagnose brain diseases, such as autism spectrum disorder (ASD). It is a challenge to measure the network (or graph) similarity in brain network analysis. Graph kernel (i.e., kernel defined on graphs) offers an efficient tool for measuring the similarity of paired brain networks and yields the excellent classification performance in brain disease diagnosis. However, most of the existing graph kernels neglected the hierarchical architecture information of brain networks. To address this problem, in this paper, we propose an optimal transport based pyramid graph kernel for measuring brain network similarity and then apply it to brain disease classification. The main idea is to transform brain networks into pyramid structures, which reflect the hierarchical architecture information of the brain network with multi-resolution histograms. The optimal transport distance in pyramid structures is calculated for measuring transport costs between paired brain networks. Finally, the optimal transport based pyramid graph kernel is computed based on this optimal transport distance. To evaluate the effectiveness of the proposed optimal transport based pyramid graph kernel, the extensive experiments are performed in functional magnetic resonance imaging data of brain disease from the Autism Brain Imaging Data Exchange database. The experimental results show that our proposed optimal transport based pyramid graph kernel outperforms the state-of-the-art methods in ASD classification tasks.}
}
@article{LIN2023109813,
title = {A self-adaptive soft-recoding strategy for performance improvement of error-correcting output codes},
journal = {Pattern Recognition},
volume = {143},
pages = {109813},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109813},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005113},
author = {Guangyi Lin and Jie Gao and Nan Zeng and Yong Xu and Kunhong Liu and Beizhan Wang and Junfeng Yao and Qingqiang Wu},
keywords = {Error-correcting output code, Soft-recoding, Ternary coding, Regression, Multiclass classification},
abstract = {The technique of error-correcting output codes (ECOC) has been proven to be of high discriminative ability in many classification applications. However, most algorithms on the ECOC were designed based on the binary or ternary codes (referred to as the hard codes), which might fail to precisely correct errors in dealing with tough tasks. In this study, a Soft-Recoding strategy based on a self-adaptive algorithm is proposed, which replaces the traditional hard codes with the real-value elements to better fit the output distribution of the base learners. This is achieved by minimizing the ratio of two distances: the distance of the output vector to the ground-truth class, and the average distance of the output vector to the remaining classes. Extensive experiments using five different hard ECOC algorithms and the corresponding softened versions on twenty datasets with diversified numbers of features and classes confirm the effectiveness of our Soft-Recoding strategy in promoting the performance of the original ECOC algorithms. Our source code and additional results are available at: github.com/MLDMXM2017/SA-soft-recoding.}
}
@article{HUA2023109719,
title = {Dynamic scene deblurring with continuous cross-layer attention transmission},
journal = {Pattern Recognition},
volume = {143},
pages = {109719},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109719},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300417X},
author = {Xia Hua and Mingxin Li and Junxiong Fei and Jianguo Liu and Yu Shi and Hanyu Hong},
keywords = {Image deblurring, Attention mechanism, Deep convolutional neural networks},
abstract = {The deep convolutional neural networks (CNNs) using attention mechanism have achieved great success for dynamic scene deblurring. In most of these networks, only the features refined by the attention maps can be passed to the next layer and the attention maps of different layers are separated from each other, which does not make full use of the attention information from different layers in the CNN. To address this problem, we introduce a new continuous cross-layer attention transmission (CCLAT) mechanism that can exploit hierarchical attention information from all the convolutional layers. Based on the CCLAT mechanism, we use a very simple attention module to construct a novel residual dense attention fusion block (RDAFB). In RDAFB, the attention maps inferred from the outputs of the preceding RDAFB and each layer are directly connected to the subsequent ones, leading to a CCLAT mechanism. Taking RDAFB as the building block, we design an effective architecture for dynamic scene deblurring named RDAFNet. The experiments on benchmark datasets show that the proposed model outperforms the state-of-the-art deblurring approaches, and demonstrate the effectiveness of CCLAT mechanism. The source code is available on: https://github.com/xjmz6/RDAFNet.}
}
@article{ZHANG2023109740,
title = {Data-Driven single image deraining: A Comprehensive review and new perspectives},
journal = {Pattern Recognition},
volume = {143},
pages = {109740},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109740},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004387},
author = {Zhao Zhang and Yanyan Wei and Haijun Zhang and Yi Yang and Shuicheng Yan and Meng Wang},
keywords = {Data-driven single image deraining, Comprehensive review, New perspective of data, Data, Rain model, Network architecture, Solving paradigms, In-depth analysis, Effectiveness of data},
abstract = {Single Image Deraining (SID) aims at recovering the rain-free background from an image degraded by rain streaks. For the powerful fitting ability of deep neural networks and massive training data, data-driven deep SID methods have obtained significant improvement over traditional model/prior-based ones. Current studies usually focus on improving the deraining performance by proposing different categories of deraining networks, while neglecting the interpretation of the solving process. As a result, the generalization ability may still be limited in real-world scenarios, and the deraining results also cannot effectively improve the performance of subsequent high-level tasks (e.g., object detection). To explore these issues, we in this paper re-examine the three important factors (i.e., data, rain model and network architecture) for the SID problem, and specifically analyze them by proposing new and more reasonable criteria (i.e., general vs. specific, synthetical vs. mathematical, black-box vs. white-box). We also study the relationship of the three factors from a new perspective of data, and reveal two different solving paradigms (explicit vs. implicit) for the SID task. We further discuss the current mainstream data-driven SID methods from five aspects, i.e., training strategy, network pipeline, domain knowledge, data preprocessing, and objective function, and some useful conclusions are summarized by statistics. Besides, we profoundly studied one of the three factors, i.e., data, and measured the performance of current methods on different datasets through extensive experiments to reveal the effectiveness of SID data. Finally, with the comprehensive review and in-depth analysis, we draw some valuable conclusions and suggestions for future research.}
}
@article{LUO2023109776,
title = {Classification of tumor in one single ultrasound image via a novel multi-view learning strategy},
journal = {Pattern Recognition},
volume = {143},
pages = {109776},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109776},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004740},
author = {Yaozhong Luo and Qinghua Huang and Longzhong Liu},
keywords = {Image classification, Deep learning, Multi-view learning, Breast cancer recognition},
abstract = {Computer-aided diagnosis (CAD) technology has been widely used in the early diagnosis of breast cancer. Nowadays, most of the existing breast ultrasound classification methods need to crop a tumor-centered image (TCI) on each image as the input of the system. These methods ignore the fact that the tumor as well as its surrounding tissues can actually be viewed from multiple aspects, and it is difficult to extract multi-resolution information applying only a single view image. In addition, the current methods do not effectively extract fine-grained features, and subtle details play an important role in breast classification. In our research, we propose a novel strategy to generate multi-resolution TCIs in a single ultrasound image, resulting in a multi-data-input learning task. Hence, a conventional single image based learning task is converted into a multi-view learning task, and an improved combined style fusion method suitable for a deep network is proposed, which integrates the advantage of the decision-based and feature-based methods to fuse the information of different views. At the same time, we first attempt to introduce the fine-grained classification method into breast classifications and capture the pairwise correlation between feature channels at each position to extract subtle information. The comparative experimental results show that our method can effectively improve the classification performance and achieves the best results in five metrics.}
}
@article{SHENG2023109724,
title = {Modeling global distribution for federated learning with label distribution skew},
journal = {Pattern Recognition},
volume = {143},
pages = {109724},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109724},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004223},
author = {Tao Sheng and Chengchao Shen and Yuan Liu and Yeyu Ou and Zhe Qu and Yixiong Liang and Jianxin Wang},
keywords = {Federated learning, Label distribution skew, Generative adversarial network, Non-Independent and identically distributed},
abstract = {Federated learning achieves joint training of deep models by connecting decentralized datasources, which can significantly mitigate the risk of privacy leakage. However, in a more general case, the distributions of labels among clients are different, called “label distribution skew”. Directly applying conventional federated learning without consideration of label distribution skew issue significantly hurts the performance of the global model. To this end, we propose a novel federated learning method, named FedMGD, to alleviate the performance degradation caused by the label distribution skew issue. It introduces a global Generative Adversarial Network to model the global data distribution without access to local datasets, so the global model can be trained using the global information of data distribution without privacy leakage. The experimental results demonstrate that our proposed method significantly outperforms the state-of-the-art on several public benchmarks. Code is available at https://www.github.com/Sheng-T/FedMGD.}
}
@article{WOO2023109800,
title = {MKConv: Multidimensional feature representation for point cloud analysis},
journal = {Pattern Recognition},
volume = {143},
pages = {109800},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109800},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004983},
author = {Sungmin Woo and Dogyoon Lee and Sangwon Hwang and Woo Jin Kim and Sangyoun Lee},
keywords = {Point cloud, Feature learning, Convolutional neural network, 3D vision},
abstract = {Despite the remarkable success of deep learning, an optimal convolution operation on point clouds remains elusive owing to their irregular data structure. Existing methods mainly focus on designing an effective continuous kernel function that can handle an arbitrary point in continuous space. Various approaches exhibiting high performance have been proposed, but we observe that the standard pointwise feature is represented by 1D channels and can become more informative when its representation involves additional spatial feature dimensions. In this paper, we present Multidimensional Kernel Convolution (MKConv), a novel convolution operator that learns to transform the point feature representation from a vector to a multidimensional matrix. Unlike standard point convolution, MKConv proceeds via two steps. (i) It first activates the spatial dimensions of local feature representation by exploiting multidimensional kernel weights. These spatially expanded features can represent their embedded information through spatial correlation as well as channel correlation in feature space, carrying more detailed local structure information. (ii) Then, discrete convolutions are applied to the multidimensional features which can be regarded as a grid-structured matrix. In this way, we can utilize the discrete convolutions for point cloud data without voxelization that suffers from information loss. Furthermore, we propose a spatial attention module, Multidimensional Local Attention (MLA), to provide comprehensive structure awareness within the local point set by reweighting the spatial feature dimensions. We demonstrate that MKConv has excellent applicability to point cloud processing tasks including object classification, object part segmentation, and scene semantic segmentation with superior results.}
}
@article{ABBAD2023109748,
title = {Exploring multivariate generalized gamma manifold for color texture retrieval},
journal = {Pattern Recognition},
volume = {143},
pages = {109748},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109748},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004466},
author = {Zakariae Abbad and Ahmed Drissi {El Maliani} and Said Ouatik {El Alaoui} and Mohammed {El Hassouni} and Mohamed Tahar Kadaoui Abbassi},
keywords = {Color texture retrieval, Gaussian copula, Geodesic distance, Manifolds, Wavelet transforms, Graph theory},
abstract = {This work proposes a novel method for color-textured image retrieval on a Multivariate Generalized Gamma Distribution manifold (MGΓD). Thanks to the Gaussian copula theory, we define the expression of MGΓD, which efficiently models the statistical dependence structure between dual-tree complex wavelet transform (DTCWT) of the color components. The major contribution of this paper is to provide a geometric perspective to the MGΓD by treating it as a Riemannian manifold while proposing the geodesic distance (GD) as a measure of Riemannian similarity on it. Based on information geometry tools, we conduct a geometrical study of the MGΓD manifold, allowing us to derive two suitable approximations of the GD. The experiments are performed on five well-known color texture databases, considering the content-based image retrieval (CBIR) framework and using the RGB color space. The obtained results demonstrate the efficiency of the geometric interpretation through the proposed GD as a natural and intuitive similarity measure on the studied statistical manifold.}
}
@article{DONG2023109732,
title = {Momentum contrast transformer for COVID-19 diagnosis with knowledge distillation},
journal = {Pattern Recognition},
volume = {143},
pages = {109732},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109732},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004302},
author = {Aimei Dong and Jian Liu and Guodong Zhang and Zhonghe Wei and Yi Zhai and Guohua Lv},
keywords = {Momentum contrastive learning, Knowledge distillation, Vision transformer, COVID-19 diagnosis},
abstract = {Intelligent diagnosis has been widely studied in diagnosing novel corona virus disease (COVID-19). Existing deep models typically do not make full use of the global features such as large areas of ground glass opacities, and the local features such as local bronchiolectasis from the COVID-19 chest CT images, leading to unsatisfying recognition accuracy. To address this challenge, this paper proposes a novel method to diagnose COVID-19 using momentum contrast and knowledge distillation, termed MCT-KD. Our method takes advantage of Vision Transformer to design a momentum contrastive learning task to effectively extract global features from COVID-19 chest CT images. Moreover, in transfer and fine-tuning process, we integrate the locality of convolution into Vision Transformer via special knowledge distillation. These strategies enable the final Vision Transformer simultaneously focuses on global and local features from COVID-19 chest CT images. In addition, momentum contrastive learning is self-supervised learning, solving the problem that Vision Transformer is challenging to train on small datasets. Extensive experiments confirm the effectiveness of the proposed MCT-KD. In particular, our MCT-KD is able to achieve 87.43% and 96.94% accuracy on two publicly available datasets, respectively.}
}
@article{CHEN2023109780,
title = {Data-free quantization via mixed-precision compensation without fine-tuning},
journal = {Pattern Recognition},
volume = {143},
pages = {109780},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109780},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004788},
author = {Jun Chen and Shipeng Bai and Tianxin Huang and Mengmeng Wang and Guanzhong Tian and Yong Liu},
keywords = {Neural network compression, Date-free quantization},
abstract = {Neural network quantization is a very promising solution in the field of model compression, but its resulting accuracy highly depends on a training/fine-tuning process and requires the original data. This not only brings heavy computation and time costs but also is not conducive to privacy and sensitive information protection. Therefore, a few recent works are starting to focus on data-free quantization. However, data-free quantization does not perform well while dealing with ultra-low precision quantization. Although researchers utilize generative methods of synthetic data to address this problem partially, data synthesis needs to take a lot of computation and time. In this paper, we propose a data-free mixed-precision compensation (DF-MPC) method to recover the performance of an ultra-low precision quantized model without any data and fine-tuning process. By assuming the quantized error caused by a low-precision quantized layer can be restored via the reconstruction of a high-precision quantized layer, we mathematically formulate the reconstruction loss between the pre-trained full-precision model and its layer-wise mixed-precision quantized model. Based on our formulation, we theoretically deduce the closed-form solution by minimizing the reconstruction loss of the feature maps. Since DF-MPC does not require any original/synthetic data, it is a more efficient method to approximate the full-precision model. Experimentally, our DF-MPC is able to achieve higher accuracy for an ultra-low precision quantized model compared to the recent methods without any data and fine-tuning process.}
}
@article{LIANG2023109810,
title = {Variational Bayesian deep network for blind Poisson denoising},
journal = {Pattern Recognition},
volume = {143},
pages = {109810},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109810},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005083},
author = {Hao Liang and Rui Liu and Zhongyuan Wang and Jiayi Ma and Xin Tian},
keywords = {Blind poisson denoising, Variational bayesian, Parameter estimation},
abstract = {Deep learning-based approaches have recently achieved considerable results in Poisson denoising under low-light conditions. However, most existing methods mainly focus on the network architecture design, which lacks physical interpretability and thus unsuitable for blind denoising in real environments with unknown levels of noises. To address this issue, we propose a variational Bayesian deep network for blind Poisson denoising (VBDNet). We mainly consider an approximate posterior form for the noise variance in a variational Bayesian framework and utilize a neural network to parameterize the variance of Poisson noise. For network design, VBDNet is divided into two sub-networks. The noise estimation sub-network is responsible for the Bayesian inference. This network improves the blind denoising ability of the subsequent denoising sub-network by learning Poisson noise characteristics under different noise levels in the training process. A network of U-Net structures implements the denoising sub-network for noise removal. By combining the advantage of Bayesian inference (noise estimation sub-network) and deep learning (denoising sub-network), VBDNet outperforms other state-of-the-art methods on both synthetic and natural data. The code and details are available at https://github.com/HLImg/VBDNet.}
}
@article{SOLTANZADEH2023109721,
title = {Addressing the class-imbalance and class-overlap problems by a metaheuristic-based under-sampling approach},
journal = {Pattern Recognition},
volume = {143},
pages = {109721},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109721},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004193},
author = {Paria Soltanzadeh and M. Reza Feizi-Derakhshi and Mahdi Hashemzadeh},
keywords = {Imbalanced classification, Imbalanced datasets, Class overlap, Class imbalance, Metaheuristic algorithms, Under-sampling},
abstract = {The problem of imbalanced class distribution in real-world datasets severely impairs the performance of classification algorithms. The learning task becomes more complicated and challenging when there is also the class-overlap problem in imbalanced data. This research tackles these problems by presenting an under-sampling approach based on a metaheuristic method in which the under-sampling problem is mapped into an optimization problem. The proposed approach aims to select an optimal subset of the majority samples to handle the imbalanced and the class-overlap problems simultaneously while avoiding the excessive elimination of majority samples, especially in overlapped regions. The quality of the generated solutions is evaluated by a classifier and optimized in an evolutionary process. Unlike most existing under-sampling methods, the majority samples are not removed only from the overlapped regions; the classifier performance determines the desired regions for eliminating the majority samples. Extensive experiments conducted on 66 synthetic and 24 real-world datasets with different imbalance ratios and overlapping degrees and two large high-dimensional datasets show a significant performance improvement from the proposed method compared to the competitors.}
}
@article{SUN2023109729,
title = {Matching based on variance minimization of component distances using edges of free-form surfaces},
journal = {Pattern Recognition},
volume = {143},
pages = {109729},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109729},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004272},
author = {Jingyu Sun and Yadong Gong and Jibin Zhao and Huan Zhang and Liya Jin},
keywords = {Normal and tangential distance, Variance minimization, Edge of point cloud, Allowance distribution, Fine registration},
abstract = {The basis for guidance in the field of automated robot processing is modeling by visual scanning. Matching algorithms are the real link that can be made between the ideal model and the subsequent robot processing. The matching algorithm that meets the machining requirements plays a pivotal role in the entire process, which provides the exact location of design models and measurement data. In order to meet the requirement of making the machining allowance uniform, a fine registration method which considers the variance minimization of the normal and tangential distances between the edge neighbors of the scattered point cloud is proposed. The edge neighbors are achieved by local growing of edge seeds based on the minimization of energy of supervoxel, which can represent the model more accurate than edge points extracted directly. The edge neighbor points are used to participate in the calculation, and the objective function of minimizing the variance of the two-way distance with the introduction of weight coefficients is proposed to constrain the iterative process. The effect of the distance in two directions on the result is analyzed, to determine the appropriate weight coefficients so that the matching calculation converges quickly and accurately. In comparison with other classical and state-of-the-art matching methods, the method in this paper performs well in terms of solution efficiency and accuracy of results. Moreover, the ability of this paper’s method to resist Gaussian noise is investigated, and it is found that this paper’s method has good robustness when σ is less than 1 for Gaussian noise. Ultimately, a uniformly distributed residual model is obtained to provide a visually guided basis for subsequent machining.}
}
@article{ZHANG2023109737,
title = {Deep representation learning for domain generalization with information bottleneck principle},
journal = {Pattern Recognition},
volume = {143},
pages = {109737},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109737},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004351},
author = {Jiao Zhang and Xu-Yao Zhang and Chuang Wang and Cheng-Lin Liu},
keywords = {Domain generalization, Information bottleneck, Representation learning},
abstract = {Although deep neural networks have achieved superior performance on many classical tasks, they deteriorate in real applications due to the unpredictable distribution shift. Domain generalization (DG) focuses on improving the generalization ability of the predictive model in unseen domains by training on multiple available source domains. All these domains share the same categories but commonly obey different distributions. In this paper, we establish a new theoretical framework for domain generalization from the perspective of the information bottleneck (IB) principle, which links representation learning in DG with domain-invariant representation learning and maximizing feature entropy (MFE). Based on the theoretical framework, we provide a feasible solution by class-wise instance discrimination combined with inter-dimension decorrelation and intra-dimension uniformity to learn the desired representation for domain generalization, which achieves excellent performance on multiple datasets without knowing domain labels. Extensive experiments show that the proposed regularization rule (MFE) can improve invariance-based DG methods consistently. Moreover, as an extreme case of domain generalization, we also show that MFE is promising to improve adversarial robustness.}
}
@article{YU2023109792,
title = {Adapt-Infomap: Face clustering with adaptive graph refinement in infomap},
journal = {Pattern Recognition},
volume = {143},
pages = {109792},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109792},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004909},
author = {Xiaotian Yu and Yifan Yang and Aibo Wang and Ling Xing and Haokui Zhang and Hanling Yi and Guangming Lu and Xiaoyu Wang},
keywords = {Face clustering, Map equation, Graph partitioning},
abstract = {Face clustering is a critical task in computer vision due to the increasing number of applications such as augmented reality or photo album management. The primary challenge in this task arises from the imperfections in image feature representations. Given image features extracted from an existing pre-trained representation model, it remains an unresolved problem that how to leverage the inherent characteristics of similarities among unlabelled images to improve the clustering performance. In order to solve face clustering in an unsupervised manner, we develop an effective and robust framework named as Adapt-Infomap. First, we reformulate face clustering as a process of non-overlapping community detection. Specially, Adapt-Infomap achieves face clustering by minimizing the entropy of information flows (also known as the map equation) on an affinity graph of images. Since the affinity graph of images might contain noisy edges, we develop an outlier detection strategy in Adapt-Infomap to adaptively refine the affinity graph. Experiments with ablation studies demonstrate that Adapt-Infomap significantly outperforms existing methods and achieves new state-of-the-arts on three popular large-scale datasets for face clustering, e.g., an absolute improvement of more than 10% and 3% comparing with prior unsupervised and supervised methods respectively in terms of average of Pairwise F-score.}
}
@article{KONG2023109793,
title = {FGBC: Flexible graph-based balanced classifier for class-imbalanced semi-supervised learning},
journal = {Pattern Recognition},
volume = {143},
pages = {109793},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109793},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004910},
author = {Xiangyuan Kong and Xiang Wei and Xiaoyu Liu and Jingjie Wang and Weiwei Xing and Wei Lu},
keywords = {Semi-supervised learning, Class-imbalanced learning, Graph network, Label propagation, MixUp},
abstract = {Semi-supervised learning (SSL) has witnessed resounding success in many standard class-balanced benchmark datasets. However, real-world data often exhibit class-imbalanced distributions, which poses significant challenges for existing SSL algorithms. In general, fully supervised models trained on a class-imbalanced dataset are biased toward the majority classes, and this issue becomes more severe for class-imbalanced semi-supervised learning (CISSL) conditions. To address this issue, we put forward a novel CISSL framework dubbed FGBC by introducing a flexible graph-based balanced classifier with three innovations. Specifically, because the propagation of label information becomes difficult for tail classes, we propose a graph-based classifier head attached to the representation layer of the existing SSL framework for efficient pseudo-label propagation. Then, by considering that the learning status of different classes in CISSL may vary, we introduce a flexible threshold adjustment in pseudo-labeling to further select balanced samples to participate in training. Furthermore, to alleviate the risk of overfitting tail classes, we devised a class-aware feature MixUp (CFM) augmentation algorithm, which can further enhance the features of each class by considering their class sizes. Experimental results demonstrate that FGBC achieves state-of-the-art performance on datasets from CIFAR-10/100, SVHN and Small ImageNet-127 under various levels of CISSL conditions.}
}
@article{SCABINI2023109802,
title = {RADAM: Texture recognition through randomized aggregated encoding of deep activation maps},
journal = {Pattern Recognition},
volume = {143},
pages = {109802},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109802},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005009},
author = {Leonardo Scabini and Kallil M. Zielinski and Lucas C. Ribas and Wesley N. Gonçalves and Bernard {De Baets} and Odemir M. Bruno},
keywords = {Texture analysis, Randomized neural networks, Transfer learning, Convolutional networks, Feature extraction},
abstract = {Texture analysis is a classical yet challenging task in computer vision for which deep neural networks are actively being applied. Most approaches are based on building feature aggregation modules around a pre-trained backbone and then fine-tuning the new architecture on specific texture recognition tasks. Here we propose a new method named Random encoding of Aggregated Deep Activation Maps (RADAM) which extracts rich texture representations without ever changing the backbone. The technique consists of encoding the output at different depths of a pre-trained deep convolutional network using a Randomized Autoencoder (RAE). The RAE is trained locally to each image using a closed-form solution, and its decoder weights are used to compose a 1-dimensional texture representation that is fed into a linear SVM. This means that no fine-tuning or backpropagation is needed for the backbone. We explore RADAM on several texture benchmarks and achieve state-of-the-art results with different computational budgets. Our results suggest that pre-trained backbones may not require additional fine-tuning for texture recognition if their learned representations are better encoded.}
}
@article{SHAO2023109765,
title = {Video anomaly detection with NTCN-ML: A novel TCN for multi-instance learning},
journal = {Pattern Recognition},
volume = {143},
pages = {109765},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109765},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004636},
author = {Wenhao Shao and Ruliang Xiao and Praboda Rajapaksha and Mengzhu Wang and Noel Crespi and Zhigang Luo and Roberto Minerva},
keywords = {Video process, Pattern recognition, Anomaly detection, Feature extraction, Temporal convolutional network, Deep learning},
abstract = {A key challenge in video anomaly detection is the identification of rare abnormal patterns in the positive instances as they exhibit only a small variation compared to normal patterns, and they are largely biased by the dominant negative instances. To address this issue, we propose a weakly supervised video anomaly detection model called NTCN-ML - Novel Temporal Convolutional Network Multi-Instance Learning Model. The NTCN-ML model extracts temporal representations of video data to construct a time-series pattern to optimize the multi-instance learning process. The model examines the correlation between positive and negative samples in the multi-instance learning process to balance the feature association between rare positive and negative instances. The video anomaly detection with the NTCN-ML model achieved 95.3% and 85.1% accuracy for UCF-Crime and ShanghaiTech datasets, respectively, and outperformed the baseline models.}
}
@article{ZHAO2023109789,
title = {AGMN: Association graph-based graph matching network for coronary artery semantic labeling on invasive coronary angiograms},
journal = {Pattern Recognition},
volume = {143},
pages = {109789},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109789},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004879},
author = {Chen Zhao and Zhihui Xu and Jingfeng Jiang and Michele Esposito and Drew Pienta and Guang-Uei Hung and Weihua Zhou},
keywords = {Coronary artery disease, Coronary arterial anatomy, Semantic labeling, Graph matching network},
abstract = {Semantic labeling of coronary arterial segments in invasive coronary angiography (ICA) is important for automated assessment and report generation of coronary artery stenosis in computer-aided coronary artery disease (CAD) diagnosis. However, separating and identifying individual coronary arterial segments is challenging because morphological similarities of different branches on the coronary arterial tree and human-to-human variabilities exist. Inspired by the training procedure of interventional cardiologists for interpreting the structure of coronary arteries, we propose an association graph-based graph matching network (AGMN) for coronary arterial semantic labeling. We first extract the vascular tree from invasive coronary angiography (ICA) and convert it into multiple individual graphs. Then, an association graph is constructed from two individual graphs where each vertex represents the relationship between two arterial segments. Thus, we convert the arterial segment labeling task into a vertex classification task; ultimately, the semantic artery labeling becomes equivalent to identifying the artery-to-artery correspondence on graphs. More specifically, the AGMN extracts the vertex features by the embedding module using the association graph, aggregates the features from adjacent vertices and edges by graph convolution network, and decodes the features to generate the semantic mappings between arteries. By learning the mapping of arterial branches between two individual graphs, the unlabeled arterial segments are classified by the labeled segments to achieve semantic labeling. A dataset containing 263 ICAs was employed to train and validate the proposed model, and a five-fold cross-validation scheme was performed. Our AGMN model achieved an average accuracy of 0.8264, an average precision of 0.8276, an average recall of 0.8264, and an average F1-score of 0.8262, which significantly outperformed existing coronary artery semantic labeling methods. In conclusion, we have developed and validated a new algorithm with high accuracy, interpretability, and robustness for coronary artery semantic labeling on ICAs.}
}
@article{ZHANG2023109773,
title = {Crop classification based on multi-temporal PolSAR images with a single tensor network},
journal = {Pattern Recognition},
volume = {143},
pages = {109773},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109773},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004715},
author = {Wei-Tao Zhang and Lu Liu and Yv Bai and Yi-Bang Li and Jiao Guo},
keywords = {Polarimetric synthetic aperture radar (PolSAR), Crop classification, Tensor affine transformation network (TATN)},
abstract = {Accurate and reliable discrimination of crop categories is a significant data source for agricultural monitoring and food security evaluation research. The convolutional neural network (CNN) model is one of the most popular classifiers for crop discrimination based on polarimetric synthetic aperture radar (PolSAR) data. However, it is better to avoid directly using large amounts of raw features that extracted from PolSAR data for CNN models because of the “dimension disaster” problem caused by multiple periods and various feature extraction schemes. Consequently, an extra feature compression model has to be incorporated to mitigate the “dimension disaster” problem. However, ill coupling of the two models may result in degraded classification performance, thus the combination of two models has to be further optimized. In this paper, we propose a novel single tensor affine transformation network (TATN) for crop classification using multi-temporal PolSAR data, where the input sample of the network is a higher order tensor formed by raw features, and the hidden layers of the network adopt the tensor affine transformation rather than convolution to extract discriminative features for classification. Since the tensor affine transformation preserves the structural information of the original input tensor samples, the TATN is expected to achieve a higher crop classification accuracy. Moreover, the TATN holds less amount of parameters than most of deep learning models, which enables to avoid the extra feature compression procedure. The experimental results validate the merits of our model.}
}
@article{DUQUEDOMINGO2023109797,
title = {One Shot Learning with class partitioning and cross validation voting (CP-CVV)},
journal = {Pattern Recognition},
volume = {143},
pages = {109797},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109797},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004958},
author = {Jaime Duque-Domingo and Roberto Medina Aparicio and Luis Miguel González Rodrigo},
keywords = {One Shot Learning, CP-CVV, Siamese, ConvNeXt, ViT, Regnet, Wide ResNet, ResNeXt, CVV, CNN, Ensemble models, Voting, Grocery product classification, CIFARFS},
abstract = {One Shot Learning includes all those techniques that make it possible to classify images using a single image per category. One of its possible applications is the identification of food products. For a grocery store, it is interesting to record a single image of each product and be able to recognise it again from other images, such as photos taken by customers. Within deep learning, Siamese neural networks are able to verify whether two images belong to the same category or not. In this paper, a new Siamese network training technique, called CP-CVV, is presented. It uses the combination of different models trained with different classes. The separation of validation classes has been done in such a way that each of the combined models is different in order to avoid overfitting with respect to the validation. Unlike normal training, the test images belong to classes that have not previously been used in training, allowing the model to work on new categories, of which only one image exists. Different backbones have been evaluated in the Siamese composition, but also the integration of multiple models with different backbones. The results show that the model improves on previous works and allows the classification problem to be solved, an additional step towards the use of Siamese networks. To the best of our knowledge, there is no existing work that has proposed integrating Siamese neural networks using a class-based validation set separation technique so as to be better at generalising for unknown classes. Additionally, we have applied Cross-Validation-Voting with ConvNeXt to improve the existing classification results of a well-known Grocery Store Dataset.}
}
@article{TAO2023109761,
title = {Learning discriminative feature representation with pixel-level supervision for forest smoke recognition},
journal = {Pattern Recognition},
volume = {143},
pages = {109761},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109761},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004594},
author = {Huanjie Tao and Qianyue Duan and Minghao Lu and Zhenwu Hu},
keywords = {Deep neural network, Component separation, Forest smoke recognition, Supervision information},
abstract = {Existing vision-based smoke recognition methods still face the issues of low detection rates and high false alarm rates in complex scenes. One reason is that they label light smoke and heavy smoke as the same value, which ignores the differences in multiple attribute information involved in the smoke imaging process. To solve this issue, this paper presents a pixel-level supervision neural network (PSNet) to learn discriminative feature representations for forest smoke recognition. First, the pixel-level supervision information, including the background component, smoke component, fusion ratio, and class information, is cooperatively considered to effectively guide the model training process. To avoid negative transfer caused by the asynchronous optimization of shared layer parameters and achieve synchronous minimization of each loss term, a regularization term based on the smoke imaging principle and a weight dynamic updating method are proposed to balance the weight coefficients of different loss terms. Second, a detail-difference-aware module (DDAM) based on a detail-difference-aware block (DDAB) and a spatial attention block (SAB) is proposed to distinguish smoke and smoke-like targets by fusing xy-shared convolution and z-shared convolution, which adaptively allocates the weights over different positions to prioritize the most informative visual elements in the spatial domain. Third, an attention-based feature separation module (AFSM) is proposed to relieve mutual interference in extracting background features and smoke features by designing component interaction attention (CIA), background component attention (BCA), smoke component attention (SCA), and enhanced residual blocks (ERBs), which can guide the interaction and separation process of background information and smoke information to enhance the discriminative spatial features and suppress interference features. ERB effectively eliminates noise and enhances smoke edge information based on median filters. Finally, to further enhance the feature representation capability, a multiconnection aggregation method (MCAM) is proposed by fully aggregating local and global features simultaneously. Extensive experiments show that our method achieves better performance than existing smoke recognition methods. Extensive experiments show that our PSNet achieves better performance than existing smoke recognition methods. For smoke recognition, our PSNet achieves a 96.95% detection rate, 3.02% false alarm rate, and 0.9694 F1-score. The average calculation time for each image is only 0.0195. For smoke component separation, our PSNet also achieves 0.0014 on evaluation criteria mean square error between predicted smoke component images and labelled smoke component images. These key experimental results are better than those of previous methods.}
}
@article{DAI2023109806,
title = {KD-Former: Kinematic and dynamic coupled transformer network for 3D human motion prediction},
journal = {Pattern Recognition},
volume = {143},
pages = {109806},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109806},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005046},
author = {Ju Dai and Hao Li and Rui Zeng and Junxuan Bai and Feng Zhou and Junjun Pan},
keywords = {Human motion prediction, Motion kinematics, Motion dynamics, Transformer},
abstract = {Recent studies have made remarkable progress on 3D human motion prediction by describing motion with kinematic knowledge. However, kinematics only considers the 3D positions or rotations of human skeletons, failing to reveal the physical characteristics of human motion. Motion dynamics reflects the forces between joints, explicitly encoding the skeleton topology, whereas rarely exploited in motion prediction. In this paper, we propose the Kinematic and Dynamic coupled transFormer (KD-Former), which incorporates dynamics with kinematics, to learn powerful features for high-fidelity motion prediction. Specifically, We first formulate a reduced-order dynamic model of human body to calculate the forces of all joints. Then we construct a non-autoregressive encoder-decoder framework based on the transformer structure. The encoder involves a kinematic encoder and a dynamic encoder, which are respectively responsible for extracting the kinematic and dynamic features for given history sequences via a spatial transformer and a temporal transformer. Future query sequences are decoded in parallel in the decoder by leveraging the encoded kinematic and dynamic information of history sequences. Experiments on Human3.6M and CMU MoCap benchmarks verify the effectiveness and superiority of our method. Code will be available at: https://github.com/wslh852/KD-Former.git.}
}
@article{WANG2023109745,
title = {Uncovering Hidden Vulnerabilities in Convolutional Neural Networks through Graph-based Adversarial Robustness Evaluation},
journal = {Pattern Recognition},
volume = {143},
pages = {109745},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109745},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004430},
author = {Ke Wang and Zicong Chen and Xilin Dang and Xuan Fan and Xuming Han and Chien-Ming Chen and Weiping Ding and Siu-Ming Yiu and Jian Weng},
keywords = {Graph of patterns, Graph distance algorithm, Adversarial robustness, Interpretable graph-based systems, Convolutional neural networks},
abstract = {Convolutional neural networks (CNNs) are widely used for image classification, but their vulnerability to adversarial attacks poses challenges to their reliability and security. However, current adversarial robustness (AR) measures lack a theoretical foundation, limiting the insight into the decision process. To address this issue, we propose a new AR evaluation framework based on Graph of Patterns (GoPs) models and graph distance algorithms. Our approach provides a fine-grained analysis of AR from three perspectives, providing targeted insight into the vulnerability of CNNs. Compared to current standards, our approach is theoretically grounded and allows fine-tuning of model components without repeated attempts and validation. Our experimental results demonstrate its effectiveness in uncovering hidden vulnerabilities in CNNs and providing actionable approaches to improve their AR. Our GoPs modeling approach and graph distance algorithms can be extended to apply to other graph machine learning tasks such as Metric Learning on multi-relational graphs. Overall, our framework represents significant progress in AR evaluation, providing a more interpretable, targeted, and efficient approach to assess CNN robustness in complex graph-based systems.}
}
@article{GIANNOULIS2023109814,
title = {DITAN: A deep-learning domain agnostic framework for detection and interpretation of temporally-based multivariate ANomalies},
journal = {Pattern Recognition},
volume = {143},
pages = {109814},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109814},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005125},
author = {Michail Giannoulis and Andrew Harris and Vincent Barra},
keywords = {Multivariate time series, Anomaly detection, Neural networks, Generic normality feature learning, Predictability modeling},
abstract = {We present DITAN, a novel unsupervised domain-agnostic framework for detecting and interpreting temporal-based anomalies. It is based on an encoder-decoder architecture with both implicit/explicit attention and adjustable layers/units for predicting normality as regular patterns in sequential data. A two-stage thresholding methodology with built-in pruning is used to detect anomalies, while root cause and similarities are interpreted in data and units space. Our approach is designed to intersect the 9 fundamental characteristics extracted from the union of related works. We demonstrate the DITAN modules on real-world datasets of 6 multivariate time series contaminated by point and contextual temporal-based anomalies at a varying duration. Experiments show a dominant predictability power of DITAN against the originally proposed models. DITAN is able to determine critical regions and thus identify anomalous events similarly well. Informative similarities between anomalous records are interpreted, since almost all similarities in units space are also verified in data space.}
}
@article{ZHANG2023109741,
title = {Fully context-aware image inpainting with a learned semantic pyramid},
journal = {Pattern Recognition},
volume = {143},
pages = {109741},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109741},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004399},
author = {Wendong Zhang and Yunbo Wang and Bingbing Ni and Xiaokang Yang},
keywords = {Image inpainting, Multi-Scale semantic priors, Learned semantic pyramid, Stochastic semantic inference},
abstract = {Restoring reasonable and realistic content for arbitrary missing regions in images is an important yet challenging task. Although recent image inpainting models have made significant progress in generating vivid visual details, they can still lead to texture blurring or structural distortions due to contextual ambiguity when dealing with more complex scenes. To address this issue, we propose the Semantic Pyramid Network (SPN) motivated by the idea that learning multi-scale semantic priors from specific pretext tasks can greatly benefit the recovery of locally missing content in images. SPN consists of two components. First, it distills semantic priors from a pretext model into a multi-scale feature pyramid, achieving a consistent understanding of the global context and local structures. Within the prior learner, we present an optional module for variational inference to realize probabilistic image inpainting driven by various learned priors. The second component of SPN is a fully context-aware image generator, which adaptively and progressively refines low-level visual representations at multiple scales with the (stochastic) prior pyramid. We train the prior learner and the image generator as a unified model without any post-processing. Our approach achieves the state of the art on multiple datasets, including Places2, Paris StreetView, CelebA, and CelebA-HQ, under both deterministic and probabilistic inpainting setups.}
}
@article{TAKHANOV2023109777,
title = {Autoencoders for a manifold learning problem with a jacobian rank constraint},
journal = {Pattern Recognition},
volume = {143},
pages = {109777},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109777},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004752},
author = {Rustem Takhanov and Y. Sultan Abylkairov and Maxat Tezekbayev},
keywords = {Manifold learning, Dimensionality reduction, Alternating algorithm, Ky fan antinorm, Autoencoders, Rank constraints},
abstract = {We formulate the manifold learning problem as the problem of finding an operator that maps any point to a close neighbor that lies on a “hidden” k-dimensional manifold. We call this operator the correcting function. Under this formulation, autoencoders can be viewed as a tool to approximate the correcting function. Given an autoencoder whose Jacobian has rank k, we deduce from the classical Constant Rank Theorem that its range has a structure of a k-dimensional manifold. A k-dimensionality of the range can be forced by the architecture of an autoencoder (by fixing the dimension of the code space), or alternatively, by an additional constraint that the rank of the autoencoder mapping is not greater than k. This constraint is included in the objective function as a new term, namely a squared Ky-Fan k-antinorm of the Jacobian function. We claim that this constraint is a factor that effectively reduces the dimension of the range of an autoencoder, additionally to the reduction defined by the architecture. We also add a new curvature term into the objective. To conclude, we experimentally compare our approach with the CAE+H method on synthetic and real-world datasets.}
}
@article{ZHANG2023109801,
title = {Construction of a feature enhancement network for small object detection},
journal = {Pattern Recognition},
volume = {143},
pages = {109801},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109801},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004995},
author = {Hongyun Zhang and Miao Li and Duoqian Miao and Witold Pedrycz and Zhaoguo Wang and Minghui Jiang},
keywords = {Collision detection, Granular computing, High-Resolution block, FENet, HR-FPN, Small object detection,},
abstract = {Limited by the size, location, number of samples and other factors of the small object itself, the small object is usually insufficient, which degrades the performance of the small object detection algorithms. To address this issue, we construct a novel Feature Enhancement Network (FENet) to improve the performance of small object detection. Firstly, an improved data augmentation method based on collision detection and spatial context extension (CDCI) is proposed to effectively expand the possibility of small object detection. Then, based on the idea of Granular Computing, a multi-granular deformable convolution network is constructed to acquire the offset feature representation at the different granularity levels. Finally, we design a high-resolution block (HR block) and build High-Resolution Block-based Feature Pyramid by parallel embedding HR block in FPN (HR-FPN) to make full use different granularity and resolution features. By above strategies, FENet can acquire sufficient feature information of small objects. In this paper, we firstly applied the multi-granularity deformable convolution to feature extraction of small objects. Meanwhile, a new feature fusion module is constructed by optimizing feature pyramid to maintain the detailed features and enrich the semantic information of small objects. Experiments show that FENet achieves excellent performance compared with performance of other methods when applied to the publicly available COCO dataset, VisDrone dataset and TinyPerson dataset. The code is available at https://github.com/cowarder/FENet.}
}
@article{CSIMOES2023109749,
title = {Gaussian kernel fuzzy c-means with width parameter computation and regularization},
journal = {Pattern Recognition},
volume = {143},
pages = {109749},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109749},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004478},
author = {Eduardo {C． Simões} and Francisco de A. {T． de Carvalho}},
keywords = {Gaussian kernel fuzzy clustering, Kernelization of the metric, Width parameter, Entropy regularization},
abstract = {The conventional Gaussian kernel fuzzy c-means clustering algorithms require selecting the width hyper-parameter, which is data-dependent and fixed for the entire execution. Not only that, but these parameters are the same for every dataset variable. Therefore, the variables have the same importance in the clustering task, including irrelevant variables. This paper proposes a Gaussian kernel fuzzy c-means with kernelization of the metric and automated computation of width parameters. These width parameters change at each iteration of the algorithm and vary from each variable and from each cluster. Thus, this algorithm can re-scale the variables differently, thus highlighting those that are relevant to the clustering task. Fuzzy clustering algorithms with regularization have become popular due to their high performance in large-scale data clustering, robustness for initialization, and low computational complexity. Because the width parameters of the variables can also be controlled by entropy, this paper also proposes Gaussian kernel fuzzy c-means algorithms with kernelization of the metric and automated computation of width parameters through entropy regularization. To demonstrate their usefulness, the proposed algorithms are compared with the conventional KFCM-K algorithm and previous algorithms that automatically compute the width parameter of the Gaussian kernel.}
}
@article{PENG2023109785,
title = {MSINet: Mining scale information from digital surface models for semantic segmentation of aerial images},
journal = {Pattern Recognition},
volume = {143},
pages = {109785},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109785},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004831},
author = {Chengli Peng and Haifeng Li and Chao Tao and Yansheng Li and Jiayi Ma},
keywords = {Semantic segmentation, Aerial image, Multi-scale, Digital surface model},
abstract = {Compared with other kinds of images, aerial images have more obvious object scale distinction and larger resolution, which results in that the whole scale information of aerial images can hardly be explored. To address this difficulty, we develop a novel network based on the digital surface models (DSMs) of aerial images in this paper. The proposed network termed MSINet can efficiently mine scale information through the DSMs from two aspects. Firstly, we propose an interpolation pyramid algorithm to encode the scale information from the DSMs and hence provide a scale prior information to the normal segmentation network. The interpolation pyramid algorithm implements interpolation operations with different scales on the DSMs and detects the pixel value change after the interpolation operations. Objects with different scales will express diverse changes, which provides useful information to encode their scale information. Secondly, aiming to address the problem that the DSMs contain a large amount of noise in the boundary part, a spatial information enhancement module and a mutual-guidance module are developed in this paper. These two modules can fix the misleading guidance information caused by the noise in the boundary part of the DSMs and hence achieve more accurate scale information inserting. The extensive experimental results prove that our methods can outperform other competitors in terms of qualitative and quantitative performance.}
}
@article{WAN2023109733,
title = {High-order interaction feature selection for classification learning: A robust knowledge metric perspective},
journal = {Pattern Recognition},
volume = {143},
pages = {109733},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109733},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004314},
author = {Jihong Wan and Hongmei Chen and Tianrui Li and Min Li and Xiaoling Yang},
keywords = {Feature selection, Fuzzy rough set, High-order interaction, Robust knowledge metric, Uncertainty measures, Classification},
abstract = {Feature selection is an important learning task in data mining and knowledge discovery. Nevertheless, the fuzziness, uncertainty, and noise presented by the data greatly complicate the construction of learning models. Moreover, most works focus on exploring low-order correlations between variables using low-dimensional mutual information, without paying attention to high-order interaction for multiple variables, resulting in the loss of some potentially important dependency information. Driven by these two issues, a robust knowledge metric approach is invented to perceive and excavate the latent information hidden in interaction. In this study, firstly, a robust fuzzy granularity space is constructed from different granular structures induced by different features, and the robust fuzzy uncertainty measures (RFUMs) are successively devised. Then, RFUMs are used to measure pair-wise, three-order, and even higher-order interaction dependencies among features. Further, a constrained high-order interaction evaluation function inspired by the N-gram language model is formulated, and a corresponding high-order interaction feature selection algorithm with RFUMs (HIFS-RFUMs) is designed. Next, comparative experiments with seven representative algorithms on twenty datasets illustrate its effectiveness. In addition, ablation experiments are conducted on the high-order interaction feature selection algorithm with fuzzy uncertainty measures (HIFS-FUMs) and the relative reduction algorithm with RFUMs (R2-RFUMs), which demonstrate the robustness of the metric and the effectiveness for mining high-order interactive features, respectively.}
}
@article{HUANG2023109736,
title = {Single-particle reconstruction in cryo-EM based on three-dimensional weighted nuclear norm minimization},
journal = {Pattern Recognition},
volume = {143},
pages = {109736},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109736},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300434X},
author = {Chaoyan Huang and Tingting Wu and Juncheng Li and Bin Dong and Tieyong Zeng},
keywords = {Single-particle reconstruction, Cryogenic electron microscopy, Forward-backward splitting algorithm, Three-dimensional weighted nuclear norm minimization},
abstract = {Single-particle reconstruction (SPR) in cryogenic electron microscopy (cryo-EM) aims at aligning and averaging two-dimensional micrographs to reconstruct a three-dimensional particle. How to reconstruct micrographs from heavy noise is a crucial point for achieving better micrograph quality, and thus many methods focus on noise removal. However, new problems such as over-smoothing often occur in their results due to failure in handling heavy noise well. This paper proposes a three-dimensional weighted nuclear norm minimization (3DWNNM) model for SPR in the cryo-EM task to address these issues. Specifically, we design a minimization solver based on the forward-backward splitting algorithm to tackle our model efficiently. Under certain conditions, this solution has an energy-decaying feature and performs exceptionally well in reconstruction. Numerical experiments fully demonstrate the effectiveness and the robustness of the proposed method.}
}