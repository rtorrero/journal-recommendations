@article{CHEN2022108926,
title = {SWIPENET: Object detection in noisy underwater scenes},
journal = {Pattern Recognition},
volume = {132},
pages = {108926},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108926},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004071},
author = {Long Chen and Feixiang Zhou and Shengke Wang and Junyu Dong and Ning Li and Haiping Ma and Xin Wang and Huiyu Zhou},
keywords = {Underwater object detection, Curriculum Multi-Class Adaboost, Sample-weighted detection loss, Noisy data},
abstract = {Deep learning based object detection methods have achieved promising performance in controlled environments. However, these methods lack sufficient capabilities to handle underwater object detection due to these challenges: (1) images in the underwater datasets and real applications are blurry whilst accompanying severe noise that confuses the detectors and (2) objects in real applications are usually small. In this paper, we propose a Sample-WeIghted hyPEr Network (SWIPENET), and a novel training paradigm named Curriculum Multi-Class Adaboost (CMA), to address these two problems at the same time. Firstly, the backbone of SWIPENET produces multiple high resolution and semantic-rich Hyper Feature Maps, which significantly improve small object detection. Secondly, inspired by the human education process that drives the learning from easy to hard concepts, we propose the noise-robust CMA training paradigm that learns the clean data first and then move on to learns the diverse noisy data. Experiments on four underwater object detection datasets show that the proposed SWIPENET+CMA framework achieves better or competitive accuracy in object detection against several state-of-the-art approaches.}
}
@article{LIU2022108959,
title = {Dynamic self-attention with vision synchronization networks for video question answering},
journal = {Pattern Recognition},
volume = {132},
pages = {108959},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108959},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004393},
author = {Yun Liu and Xiaoming Zhang and Feiran Huang and Shixun Shen and Peng Tian and Lang Li and Zhoujun Li},
keywords = {Video question answering, Dynamic self-attention, Vision synchronization},
abstract = {Video Question Answering (VideoQA) has gained increasing attention as an important task in understanding the rich spatio-temporal contents, i.e., the appearance and motion in the video. However, existing approaches mainly use the question to learn attentions over all the sampled appearance and motion features separately, which neglect two properties of VideoQA: (1) the answer to the question is often reflected on a few frames and video clips, and most video contents are superfluous; (2) appearance and motion features are usually concomitant and complementary to each other in time series. In this paper, we propose a novel VideoQA model, i.e., Dynamic Self-Attention with Vision Synchronization Networks (DSAVS), to address these problems. Specifically, a gated token selection mechanism is proposed to dynamically select the important tokens from appearance and motion sequences. These chosen tokens are fed into a self-attention mechanism to model the internal dependencies for more effective representation learning. To capture the correlation between the appearance and motion features, a vision synchronization block is proposed to synchronize the two types of vision features at the time slice level. Then, the visual objects can be correlated with their corresponding activities and the performance is further improved. Extensive experiments conducted on three public VideoQA data sets confirm the effectivity and superiority of our model compared with state-of-the-art methods.}
}
@article{GUAN2022108967,
title = {MonoPoly: A practical monocular 3D object detector},
journal = {Pattern Recognition},
volume = {132},
pages = {108967},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108967},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004472},
author = {He Guan and Chunfeng Song and Zhaoxiang Zhang and Tieniu Tan},
keywords = {Object detection, Monocular 3D, Real-time, Light-weight},
abstract = {3D object detection plays a pivotal role in driver assistance systems and has practical requirements for small storage and fast inference. Monocular 3D detection alternatives abandon the complexity of LiDAR setup and pursues the effectiveness and efficiency of the vision scheme. In this work, we propose a set of anchor-free monocular 3D detectors called MonoPoly based on the keypoint paradigm. Specifically, we design a polynomial feature aggregation sampling module to extract multi-scale context features for auxiliary training and alleviate classification and localization misalignment through an attention-aware loss. Extensive experiments show that the proposed MonoPoly series achieves an excellent trade-off between performance and model size while maintaining real-time efficiency on KITTI and nuScenes datasets.}
}
@article{ZHAO2022108947,
title = {Siamese networks with an online reweighted example for imbalanced data learning},
journal = {Pattern Recognition},
volume = {132},
pages = {108947},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108947},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004277},
author = {Linchang Zhao and Zhaowei Shang and Jin Tan and Mingliang Zhou and Mu Zhang and Dagang Gu and Taiping Zhang and Yuan Yan Tang},
keywords = {Few-shot learning, Reweighted example learning, Data mining, Imbalanced learning},
abstract = {One key challenging problem in data mining and decision-making is to establish a decision support system based on unbalanced datasets. In this study, we propose a novel algorithm to handle unbalanced learning problems that integrates the advantages of Siamese convolutional neural networks (SCNN) and the online reweighted example (ORE) algorithm into a unified method. First, the SCNN model is established for learning and extracting deep feature features at different levels. Second, the ORE algorithm is used to address the problem of data with a class-imbalanced distribution. Compared with baseline approaches, the experimental results show that our proposed method substantially enhances the performance of both within-project defect prediction and cross-project defect prediction.}
}
@article{WANG2022108925,
title = {Learning pseudo labels for semi-and-weakly supervised semantic segmentation},
journal = {Pattern Recognition},
volume = {132},
pages = {108925},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108925},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200406X},
author = {Yude Wang and Jie Zhang and Meina Kan and Shiguang Shan},
keywords = {Semi-supervised, Weakly supervised, Semi-and-weakly supervised, Semantic segmentation, Pseudo label, Self-training},
abstract = {In this paper, we aim to tackle semi-and-weakly supervised semantic segmentation (SWSSS), where many image-level classification labels and a few pixel-level annotations are available. We believe the most crucial point for solving SWSSS is to produce high-quality pseudo labels, and our method deals with it from two perspectives. Firstly, we introduce a class-aware cross entropy (CCE) loss for network training. Compared to conventional cross entropy loss, CCE loss encourages the model to distinguish concurrent classes only and simplifies the learning target of pseudo label generation. Secondly, we propose a progressive cross training (PCT) method to build cross supervision between two networks with a dynamic evaluation mechanism, which progressively introduces high-quality predictions as additional supervision for network training. Our method significantly improves the quality of generated pseudo labels in the regime with extremely limited annotations. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods significantly. The code is released for public access11https://github.com/YudeWang/Learning-Pseudo-Label.}
}
@article{BAI2022108975,
title = {Self-supervised spectral clustering with exemplar constraints},
journal = {Pattern Recognition},
volume = {132},
pages = {108975},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108975},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004551},
author = {Liang Bai and Yunxiao Zhao and Jiye Liang},
keywords = {Spectral clustering, Self-supervised algorithm, Exemplar constraint, Optimization model},
abstract = {As a leading graph clustering technique, spectral clustering is one of the most widely used clustering methods that captures complex clusters in data. However, some of its deficiencies, such as the high computational complexity in eigen decomposition and the guidance without supervised information, limit its real applications. To get rid of the deficiencies, we propose a self-supervised spectral clustering algorithm. In this algorithm, we define an exemplar constraint which reflects the relations between objects and exemplars. We provide the related analysis to show that it is more suitable for unsupervised learning. Based on the exemplar constraint, we build an optimization model for self-supervised spectral clustering so that we can simultaneously learn clustering results and exemplar constraints. Furthermore, we propose an iterative method to solve the new optimization problem. Compared to other existing versions of spectral clustering algorithms, the new algorithm can use the low computational costs to discover a high-quality cluster structure of a data set without prior information. Furthermore, we did a number of experiments of algorithm comparison and parameter analysis on benchmark data sets to illustrate that the proposed algorithm is very effective and efficient.}
}
@article{LI2022108922,
title = {Kernel dependence regularizers and Gaussian processes with applications to algorithmic fairness},
journal = {Pattern Recognition},
volume = {132},
pages = {108922},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108922},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004034},
author = {Zhu Li and Adrián Pérez-Suay and Gustau Camps-Valls and Dino Sejdinovic},
keywords = {Fairness, Kernel methods, Gaussian processes, Regularization, Hilbert-Schmidt independence criterion},
abstract = {Current adoption of machine learning in industrial, societal and economical activities has raised concerns about the fairness, equity and ethics of automated decisions. Predictive models are often developed using biased datasets and thus retain or even exacerbate biases in their decisions and recommendations. Removing the sensitive covariates, such as gender or race, is insufficient to remedy this issue since the biases may be retained due to other related covariates. We present a regularization approach to this problem that trades off predictive accuracy of the learned models (with respect to biased labels) for the fairness in terms of statistical parity, i.e. independence of the decisions from the sensitive covariates. In particular, we consider a general framework of regularized empirical risk minimization over reproducing kernel Hilbert spaces and impose an additional regularizer of dependence between predictors and sensitive covariates using kernel-based measures of dependence, namely the Hilbert-Schmidt Independence Criterion (HSIC) and its normalized version. This approach leads to a closed-form solution in the case of squared loss, i.e. ridge regression. We also provide statistical consistency results for both risk and fairness bound for our approach. Moreover, we show that the dependence regularizer has an interpretation as modifying the corresponding Gaussian process (GP) prior. As a consequence, a GP model with a prior that encourages fairness to sensitive variables can be derived, allowing principled hyperparameter selection and studying of the relative relevance of covariates under fairness constraints. Experimental results in synthetic examples and in real problems of income and crime prediction illustrate the potential of the approach to improve fairness of automated decisions.}
}
@article{LUO2022108955,
title = {Domain consistency regularization for unsupervised multi-source domain adaptive classification},
journal = {Pattern Recognition},
volume = {132},
pages = {108955},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108955},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004356},
author = {Zhipeng Luo and Xiaobing Zhang and Shijian Lu and Shuai Yi},
keywords = {Domain adaptation, Transfer learning, Adversarial learning, Feature alignment},
abstract = {Deep learning-based multi-source unsupervised domain adaptation (MUDA) has been actively studied in recent years. Compared with single-source unsupervised domain adaptation (SUDA), domain shift in MUDA exists not only between the source and target domains but also among multiple source domains. Most existing MUDA algorithms focus on extracting domain-invariant representations among all domains whereas the task-specific decision boundaries among classes are largely neglected. In this paper, we propose an end-to-end trainable network that exploits domain Consistency Regularization for unsupervised Multi-source domain Adaptive classification (CRMA). CRMA aligns not only the distributions of each pair of source and target domains but also that of all domains. For each pair of source and target domains, we employ an intra-domain consistency to regularize a pair of domain-specific classifiers to achieve intra-domain alignment. In addition, we design an inter-domain consistency that targets joint inter-domain alignment among all domains. To address different similarities between multiple source domains and the target domain, we design an authorization strategy that assigns different authorities to domain-specific classifiers adaptively for optimal pseudo label prediction and self-training. Extensive experiments show that CRMA tackles unsupervised domain adaptation effectively under a multi-source setup and achieves superior adaptation consistently across multiple MUDA datasets.}
}
@article{ZHAO2022108983,
title = {Motion-blurred image restoration framework based on parameter estimation and fuzzy radial basis function neural networks},
journal = {Pattern Recognition},
volume = {132},
pages = {108983},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108983},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004630},
author = {Shengmin Zhao and Sung-Kwun Oh and Jin-Yul Kim and Zunwei Fu and Witold Pedrycz},
keywords = {Motion-blurred image restoration framework, Point spread function, Blur parameter estimation based on the particle swarm optimization, Polynomial-based radial basis function neural network, Image Quality Assessment},
abstract = {The restoration of motion-blurred images has always been a complex problem in image restoration. The current single blurred image algorithm cannot very well solve the estimation error of motion blur parameters. A comprehensive motion-blurred image restoration framework is proposed, which includes motion-blurred data generation, blur parameter estimation, and image quality assessment of restored images. First, we designed and used four image data sets with different degrees of blurring. We innovatively propose a blur parameter estimation algorithm based on the particle swarm optimization (B-PSO) algorithm. The Naturalness Image Quality Evaluator (NIQE) is used as the fitness function of the PSO algorithm. The framework also introduces a polynomial-based radial basis function neural network (P-RBFNN) as a new image quality assessment (IQA) method, with good image classification performance. Test results from public datasets show that the proposed framework can accurately estimate blur parameters. The peak signal-to-noise ratio (PSNR) reaches 29.976 dB, the structural similarity (SSIM) reaches 0.9044, and the classification rate is 96%. The proposed restoration framework produces the best image restoration results.}
}
@article{RAHIMZADEHARASHLOO2022108930,
title = {ℓp-Norm Support Vector Data Description},
journal = {Pattern Recognition},
volume = {132},
pages = {108930},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108930},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004113},
author = {Shervin {Rahimzadeh Arashloo}},
keywords = {One-class classification, Kernel methods, Support vector data description, -norm penalty},
abstract = {The support vector data description (SVDD) approach serves as a de facto standard for one-class classification where the learning task entails inferring the smallest hyper-sphere to enclose target objects while linearly penalising the errors/slacks via an ℓ1-norm penalty term. In this study, we generalise this modelling formalism to a general ℓp-norm (p≥1) penalty function on slacks. By virtue of an ℓp-norm function, in the primal space, the proposed approach enables formulating a non-linear cost for slacks. From a dual problem perspective, the proposed method introduces a dual norm into the objective function, thus, proving a controlling mechanism to tune into the intrinsic sparsity/uniformity of the problem for enhanced descriptive capability. A theoretical analysis based on Rademacher complexities characterises the generalisation performance of the proposed approach while the experimental results on several datasets confirm the merits of the proposed method compared to other alternatives.}
}
@article{FAN2022108963,
title = {GFNet: Automatic segmentation of COVID-19 lung infection regions using CT images based on boundary features},
journal = {Pattern Recognition},
volume = {132},
pages = {108963},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108963},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004435},
author = {Chaodong Fan and Zhenhuan Zeng and Leyi Xiao and Xilong Qu},
keywords = {Image segmentation, COVID-19, Edge-guidance, Convolutional neural network, CT image},
abstract = {In early 2020, the global spread of the COVID-19 has presented the world with a serious health crisis. Due to the large number of infected patients, automatic segmentation of lung infections using computed tomography (CT) images has great potential to enhance traditional medical strategies. However, the segmentation of infected regions in CT slices still faces many challenges. Specially, the most core problem is the high variability of infection characteristics and the low contrast between the infected and the normal regions. This problem leads to fuzzy regions in lung CT segmentation. To address this problem, we have designed a novel global feature network(GFNet) for COVID-19 lung infections: VGG16 as backbone, we design a Edge-guidance module(Eg) that fuses the features of each layer. First, features are extracted by reverse attention module and Eg is combined with it. This series of steps enables each layer to fully extract boundary details that are difficult to be noticed by previous models, thus solving the fuzzy problem of infected regions. The multi-layer output features are fused into the final output to finally achieve automatic and accurate segmentation of infected areas. We compared the traditional medical segmentation networks, UNet, UNet++, the latest model Inf-Net, and methods of few shot learning field. Experiments show that our model is superior to the above models in Dice, Sensitivity, Specificity and other evaluation metrics, and our segmentation results are clear and accurate from the visual effect, which proves the effectiveness of GFNet. In addition, we verify the generalization ability of GFNet on another “never seen” dataset, and the results prove that our model still has better generalization ability than the above model. Our code has been shared at https://github.com/zengzhenhuan/GFNet.}
}
@article{LIN2022108917,
title = {BSCA-Net: Bit Slicing Context Attention network for polyp segmentation},
journal = {Pattern Recognition},
volume = {132},
pages = {108917},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108917},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003983},
author = {Yi Lin and Jichun Wu and Guobao Xiao and Junwen Guo and Geng Chen and Jiayi Ma},
keywords = {Medical image segmentation, Polyp segmentation, Colonoscopy, Attention mechanism},
abstract = {In this paper, we propose a novel Bit-Slicing Context Attention Network (BSCA-Net), an end-to-end network, to improve the extraction ability of boundary information for polyp segmentation. The core of BSCA-Net is a new Bit Slice Context Attention (BSCA) module, which exploits the bit-plane slicing information to effectively extract the boundary information between polyps and the surrounding tissue. In addition, we design a novel Split-Squeeze-Bottleneck-Union (SSBU) module, to exploit the geometrical information from different aspects. Also, based on SSBU, we propose an multipath concat attention decoder (MCAD) and an multipath attention concat encoder (MACE), to further improve the network performance for polyp segmentation. Finally, by combining BSCA, SSBU, MCAD and MACE, the proposed BSCA-Net is able to effectively suppress noises in feature maps, and simultaneously improve the ability of feature expression in different levels, for polyp segmentation. Empirical experiments on five benchmark datasets (Kvasir, CVC-ClinicDB, ETIS, CVC-ColonDB and CVC-300) demonstrate the superior of the proposed BSCA-Net over existing cutting-edge methods.}
}
@article{MARCHETTI2022108913,
title = {Score-Oriented Loss (SOL) functions},
journal = {Pattern Recognition},
volume = {132},
pages = {108913},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108913},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003946},
author = {F. Marchetti and S. Guastavino and M. Piana and C. Campi},
keywords = {Supervised machine learning, Binary classification, Loss functions, Skill scores},
abstract = {Loss functions engineering and the assessment of prediction performances are two crucial and intertwined aspects of supervised machine learning. This paper focuses on binary classification to introduce a class of loss functions that are defined on probabilistic confusion matrices and that allow an automatic and a priori maximization of the skill scores. These loss functions are tested in various classification experiments, which show that the probability distribution function associated with the confusion matrices significantly impacts the outcome of the score maximization process, and that the proposed functions are competitive with other state-of-the-art probabilistic losses.}
}
@article{LIU2022108951,
title = {Alleviating the over-smoothing of graph neural computing by a data augmentation strategy with entropy preservation},
journal = {Pattern Recognition},
volume = {132},
pages = {108951},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108951},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004319},
author = {Xue Liu and Dan Sun and Wei Wei},
keywords = {Graph representation, Graph convolutional networks, Information theory, Graph entropy},
abstract = {The Graph Convolutional Networks (GCN) proposed by Kipf and Welling is an effective model to improve semi-supervised learning of pattern recognition, but faces the obstacle of over-smoothing, which will weaken the representation ability of GCN. Recently some works are proposed to tackle above limitation by randomly perturbing graph topology or feature matrix to generate data augmentations as input for training. However, these operations inevitably do damage to the integrity of information structures and have to sacrifice the smoothness of feature manifold. In this paper, we first introduce a novel graph entropy definition as a measure to quantitatively evaluate the smoothness of a data manifold and then point out that this graph entropy is controlled by triangle motif-based information structures. Considering the preservation of graph entropy, we propose an effective strategy to generate randomly perturbed training data but maintain both graph topology and graph entropy. Extensive experiments have been conducted on real-world datasets and the results verify the effectiveness of our proposed method in improving semi-supervised node classification accuracy compared with a surge of baselines. Beyond that, our proposed approach could significantly enhance the robustness of training process for GCN.}
}
@article{2022109003,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {132},
pages = {109003},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(22)00483-6},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004836}
}
@article{JUNG2022108958,
title = {Counterfactual explanation based on gradual construction for deep networks},
journal = {Pattern Recognition},
volume = {132},
pages = {108958},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108958},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004381},
author = {Hong-Gyu Jung and Sin-Han Kang and Hee-Dong Kim and Dong-Ok Won and Seong-Whan Lee},
keywords = {Explainable AI, Counterfactual explanation, Interpretability, Model-agnostics, Generative model},
abstract = {To understand the black-box characteristics of deep networks, counterfactual explanation that deduces not only the important features of an input space but also how those features should be modified to classify input as a target class has gained an increasing interest. The patterns that deep networks have learned from a training dataset can be grasped by observing the feature variation among various classes. However, current approaches perform the feature modification to increase the classification probability for the target class irrespective of the internal characteristics of deep networks. This often leads to unclear explanations that deviate from real-world data distributions. To address this problem, we propose a counterfactual explanation method that exploits the statistics learned from a training dataset. Especially, we gradually construct an explanation by iterating over masking and composition steps. The masking step aims to select an important feature from the input data to be classified as a target class. Meanwhile, the composition step aims to optimize the previously selected feature by ensuring that its output score is close to the logit space of the training data that are classified as the target class. Experimental results show that our method produces human-friendly interpretations on various classification datasets and verify that such interpretations can be achieved with fewer feature modification.}
}
@article{SHANG2022108966,
title = {Uncorrelated feature selection via sparse latent representation and extended OLSDA},
journal = {Pattern Recognition},
volume = {132},
pages = {108966},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108966},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004460},
author = {Ronghua Shang and Jiarui Kong and Weitong Zhang and Jie Feng and Licheng Jiao and Rustam Stolkin},
keywords = {Unsupervised feature selection, Sparse latent representation, OLSDA, Pseudo-labels, Uncorrelated constraints},
abstract = {Modern unsupervised feature selection methods predominantly obtain the cluster structure and pseudo-labels information through spectral clustering. However, the pseudo-labels obtained by spectral clustering are usually mixed between positive and negative. Moreover, the Laplacian matrix in spectral clustering typically affects feature selection. Additionally, spectral clustering does not consider the interconnection information between data. To address these problems, this paper proposes uncorrelated feature selection via sparse latent representation and extended orthogonal least square discriminant analysis (OLSDA), which we term SLREO). Firstly, SLREO retains the interconnection between data by latent representation learning, and preserves the internal information between the data. In order to remove redundant interconnection information, an l2,1-norm constraint is applied to the residual matrix of potential representation learning. Secondly, SLREO obtains non-negative pseudo-labels through orthogonal least square discriminant analysis (OLSDA) of embedded non-negative manifold structure. It not only avoids the appearance of negative pseudo-labels, but also eliminates the effect of the Laplacian matrix on feature selection. The manifold information of the data is also preserved. Furthermore, the matrix of the learned latent representation and OLSDA is used as pseudo-labels information. It not only ensures that the generated pseudo-labels are non-negative, but also makes the pseudo-labels closer to the true class labels. Finally, in order to avoid trivial solutions, an uncorrelated constraint and l2,1-norm constraint are imposed on the feature transformation matrix. These constraints ensure row sparsity of the feature transformation matrix, select low-redundant and discriminative features, and improve the effect of feature selection. Experimental results show that the Clustering Accuracy (ACC) and Normalized Mutual Information (NMI) of SLREO are significantly improved, as compared with six other published algorithms, tested on 11 benchmark datasets.}
}
@article{SHU2022108921,
title = {Privileged multi-task learning for attribute-aware aesthetic assessment},
journal = {Pattern Recognition},
volume = {132},
pages = {108921},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108921},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004022},
author = {Yangyang Shu and Qian Li and Lingqiao Liu and Guandong Xu},
keywords = {Aesthetic assessment, Privileged information, Multi-task learning},
abstract = {Aesthetic attributes are crucial for aesthetics because they explicitly present some photo quality cues that a human expert might use to evaluate a photo’s aesthetic quality. However, the aesthetic attributes have not been largely and sufficiently exploited for photo aesthetic assessment. In this paper, we propose a novel approach to photo aesthetic assessment with the help of aesthetic attributes. The aesthetic attributes are used as privileged information (PI), which is often available during training phase but unavailable in prediction phase due to the high collection expense. The proposed framework consists of a deep multi-task network as generator and a fully connected network as discriminator. Deep multi-task network learns the aesthetic attributes and score simultaneously to capture their dependencies and extract better feature representations. Specifically, we use ranking constraint in the label space, similarity constraint and prior probabilities loss in the privileged information space to make the output of multi-task network converge to that of ground truth. Adversarial loss is used to identify and distinguish the predicted privileged information of a deep multi-task network from the ground truth PI distribution. Experimental results on two benchmark databases demonstrate the superiority of the proposed method to state-of-the-art.}
}
@article{LIU2022108848,
title = {Learning multimodal relationship interaction for visual relationship detection},
journal = {Pattern Recognition},
volume = {132},
pages = {108848},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108848},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003296},
author = {Zhixuan Liu and Wei-Shi Zheng},
keywords = {Visual relationship detection, Scene graph generation, Relationship context, Multimodal relationship interaction},
abstract = {Visual relationship detection aims to recognize visual relationships in scenes as triplets 〈subject-predicate-object〉. Previous works have shown remarkable progress by introducing multimodal features, external linguistics, scene context, etc. Due to the loss of informative multimodal hyper-relations (i.e. relations of relationships), the meaningful contexts of relationships are not fully captured yet, which limits the reasoning ability. In this work, we propose a Multimodal Similarity Guided Relationship Interaction Network (MSGRIN) to explicitly model the relations of relationships in graph neural network paradigm. In a visual scene, the MSGRIN takes the visual relationships as nodes to construct an adaptive graph and enhances deep message passing by introducing Entity Appearance Reconstruction, Entity Relevance Filtering and Multimodal Similarity Attention. We have conducted extensive experiments on two datasets: Visual Relationship Detection (VRD) and Visual Genome (VG). The evaluation results demonstrate that the proposed MSGRIN has empirically performed more effectively overall.}
}
@article{XU2022108929,
title = {Infrared and visible image fusion via parallel scene and texture learning},
journal = {Pattern Recognition},
volume = {132},
pages = {108929},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108929},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004101},
author = {Meilong Xu and Linfeng Tang and Hao Zhang and Jiayi Ma},
keywords = {Image fusion, Infrared, Scene and texture learning, Recurrent neural network},
abstract = {Image fusion plays a pivotal role in numerous high-level computer vision tasks. Existing deep learning-based image fusion methods usually leverage an implicit manner to achieve feature extraction, which would cause some characteristics of source images, e.g., contrast and structural information, are unable to be fully extracted and integrated into the fused images. In this work, we propose an infrared and visible image fusion method via parallel scene and texture learning. Our key objective is to deploy two branches of deep neural networks, namely the content branch and detail branch, to synchronously extract different characteristics from source images and then reconstruct the fused image. The content branch focuses primarily on coarse-grained information and is deployed to estimate the global content of source images. The detail branch primarily pays attention to fine-grained information, and we design an omni-directional spatially variant recurrent neural networks in this branch to model the internal structure of source images more accurately and extract texture-related features in an explicit manner. Extensive experiments show that our approach achieves significant improvements over state-of-the-arts on qualitative and quantitative evaluations with comparatively less running time consumption. Meanwhile, we also demonstrate the superiority of our fused results in the object detection task. Our code is available at: https://github.com/Melon-Xu/PSTLFusion.}
}
@article{WANG2022108908,
title = {COVID-19 contact tracking by group activity trajectory recovery over camera networks},
journal = {Pattern Recognition},
volume = {132},
pages = {108908},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108908},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003892},
author = {Chao Wang and XiaoChen Wang and Zhongyuan Wang and WenQian Zhu and Ruimin Hu},
keywords = {Contact tracking, COVID-19, Group activity, Trajectory recovery},
abstract = {Contact tracking plays an important role in the epidemiological investigation of COVID-19, which can effectively reduce the spread of the epidemic. As an excellent alternative method for contact tracking, mobile phone location-based methods are widely used for locating and tracking contacts. However, current inaccurate positioning algorithms that are widely used in contact tracking lead to the inaccurate follow-up of contacts. Aiming to achieve accurate contact tracking for the COVID-19 contact group, we extend the analysis of the GPS data to combine GPS data with video surveillance data and address a novel task named group activity trajectory recovery. Meanwhile, a new dataset called GATR-GPS is constructed to simulate a realistic scenario of COVID-19 contact tracking, and a coordinated optimization algorithm with a spatio-temporal constraint table is further proposed to realize efficient trajectory recovery of pedestrian trajectories. Extensive experiments on the novel collected dataset and commonly used two existing person re-identification datasets are performed, and the results evidently demonstrate that our method achieves competitive results compared to the state-of-the-art methods.}
}
@article{LI2022108946,
title = {Table Structure Recognition and Form Parsing by End-to-End Object Detection and Relation Parsing},
journal = {Pattern Recognition},
volume = {132},
pages = {108946},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108946},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004265},
author = {Xiao-Hui Li and Fei Yin and He-Sen Dai and Cheng-Lin Liu},
keywords = {Table detection, Table structure recognition, Template-free form parsing, Graph neural network, End-to-end training},
abstract = {The recognition of two-dimensional structure of tables and forms from document images is a challenge due to the complexity of document structures and the diversity of layouts. In this paper, we propose a graph neural network (GNN) based unified framework named Table Structure Recognition Network (TSRNet) to jointly detect and recognize the structures of various tables and forms. First, a multi-task fully convolutional network (FCN) is used to segment primitive regions such as text segments and ruling lines from document images, then a GNN is used to classify and group these primitive regions into page objects such as tables and cells. At last, the relationships between neighboring page objects are analyzed using another GNN based parsing module. The parameters of all the modules in the system can be trained end-to-end to optimize the overall performance. Experiments of table detection and structure recognition for modern documents on the POD 2017, cTDaR 2019 and PubTabNet datasets and template-free form parsing for historical documents on the NAF dataset show that the proposed method can handle various table/form structures and achieve superior performance.}
}
@article{SHU2022108978,
title = {Wasserstein distributional harvesting for highly dense 3D point clouds},
journal = {Pattern Recognition},
volume = {132},
pages = {108978},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108978},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004587},
author = {Dong Wook Shu and Sung Woo Park and Junseok Kwon},
keywords = {3D point cloud harvesting, Progressive sampling, Stochastic instance normalization},
abstract = {In this paper, we present a novel 3D point cloud harvesting method, which can harvest 3D points from an estimated surface distribution in an unsupervised manner (i.e., an input is a prior distribution). Our method outputs the surface distribution of a 3D object and samples 3D points from the distribution based on the proposed progressive random sampling strategy. The progressive sampling regards a prior distribution itself as a network input and uses a progressively increasing number of latent variables for training, which can diversify the coordinates of 3D points with fast convergence. Subsequently, our stochastic instance normalization transforms the implicit distribution into other distributions, which enables diverse shapes of 3D objects. Experimental results show that our method is competitive with other state-of-the-art methods. Our method can harvest an arbitrary number of 3D points, wherein the 3D object is represented in detail with highly dense 3D points or a part of it is described with partial sampling.}
}
@article{WU2022108957,
title = {Covered Style Mining via Generative Adversarial Networks for Face Anti-spoofing},
journal = {Pattern Recognition},
volume = {132},
pages = {108957},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108957},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200437X},
author = {Yiqiang Wu and Dapeng Tao and Yong Luo and Jun Cheng and Xuelong Li},
keywords = {Face anti-spoofing, Generative adversarial networks, Deep learning},
abstract = {Face anti-spoofing, a biometric authentication method, is a central part of automatic face recognition. Recently, two sets of approaches have performed particularly well against presentation attacks: 1) pixel-wise supervision-based methods, which intend to provide fine-grained pixel information to learn specific auxiliary maps; and 2) anomaly detection-based methods, which regard face anti-spoofing as an open-set training task and learn spoof detectors using only bona fide data, where the detectors are shown to generalize well to unknown attacks. However, these approaches depend on handcrafted prior information to control the generation of intermediate difference maps and easily fall into local optima. In this paper, we propose a novel frame-level face anti-spoofing method, Covered Style Mining-GAN (CSM-GAN), which converts face anti-spoofing detection into a style transfer process without any prior information. Specifically, CSM-GAN has four main components: the Covered Style Encoder (CSE), responsible for mining the difference map containing the photography style and discriminative clues; the Auxiliary Style Classifier (ASC), consisting of several stacked Difference Capture Blocks (DCB) responsible for distinguishing bona fide faces from spoofing faces; and the Style Transfer Generator (STG) and Style Adversarial Discriminator (SAD), which form generative adversarial networks to achieve style transfer. Comprehensive experiments on several benchmark datasets show that the proposed method not only outperforms current state-of-the-art but also produces better visual diversity in difference maps.}
}
@article{YANG2022108916,
title = {Multi-feature sparse similar representation for person identification},
journal = {Pattern Recognition},
volume = {132},
pages = {108916},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108916},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003971},
author = {Meng Yang and Lei Liao and Kangyin Ke and Guangwei Gao},
keywords = {Multi-feature, Person identification, Sparse representation},
abstract = {Person identification with a single feature (e.g., face recognition, speaker verification, person re-identification, etc.) has been studied extensively for many years, while few works focus on multi-feature person identification. Though promising performance has been achieved by only using the information of facial images, voice, or pedestrian appearance, it is still challenging to recognize a person with only a single feature in some situations (e.g., a person at a distance or occluded by other objects, and a partial person out of view). In this paper, we present a multi-feature sparse similar representation (MFSSR) method to effectively fuse face features, body features, and global image features for the task of person identification. In MFSSR, we designed a reconstructed deep spatial feature for representing the appearance of human body by using the spatial correlation coding of partial deep spatial features. Then we presented a multi-feature sparse similar representation model for jointly using different features, e.g., face, body, and the global image. Besides, considering that the coding coefficients associated with good samples but not outliers should be more similar among different features, we jointly represent different features by imposing a weighted ℓ1-norm distance regularization, instead of the conventional ℓ2-norm regularization, on the coefficients. Experimental results on several multi-feature person identification databases have clearly shown the superior performance of the proposed model.}
}
@article{XU2022108954,
title = {Towards generalizable person re-identification with a bi-stream generative model},
journal = {Pattern Recognition},
volume = {132},
pages = {108954},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108954},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004344},
author = {Xin Xu and Wei Liu and Zheng Wang and Ruimin Hu and Qi Tian},
keywords = {Person re-identification, Generalizable re-ID, Camera-Camera problem, Camera-Person problem},
abstract = {Generalizable person re-identification (re-ID) has attracted growing attention due to its powerful adaptation capability in the unseen data domain. However, existing solutions often neglect either crossing cameras (e.g., illumination and resolution differences) or pedestrian misalignments (e.g., viewpoint and pose discrepancies), which easily leads to poor generalization capability when adapted to the new domain. In this paper, we formulate these difficulties as: 1) Camera-Camera (CC) problem, which denotes the various human appearance changes caused by different cameras; 2) Camera-Person (CP) problem, which indicates the pedestrian misalignments caused by the same identity person under different camera viewpoints or changing pose. To solve the above issues, we propose a Bi-stream Generative Model (BGM) to learn the fine-grained representations fused with camera-invariant global feature and pedestrian-aligned local feature, which contains an encoding network and two stream decoding sub-network. Guided by original pedestrian images, one stream is employed to learn a camera-invariant global feature for the CC problem via filtering cross-camera interference factors. For the CP problem, another stream learns a pedestrian-aligned local feature for pedestrian alignment using information-complete densely semantically aligned part maps. Moreover, a part-weighted loss function is presented to reduce the influence of missing parts on pedestrian alignment. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods on the large-scale generalizable re-ID benchmarks, involving domain generalization setting and cross-domain setting.}
}
@article{HAN2022108934,
title = {Single image based 3D human pose estimation via uncertainty learning},
journal = {Pattern Recognition},
volume = {132},
pages = {108934},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108934},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004149},
author = {Chuchu Han and Xin Yu and Changxin Gao and Nong Sang and Yi Yang},
keywords = {Uncertainty, 3D pose estimation, Graph convolutional network},
abstract = {In monocular image scenes, 3D human pose estimation exhibits inherent ambiguity due to the loss of depth information and occlusions. Simply regressing body joints with high uncertainties will lead to model overfitting and poor generalization. In this paper, we propose an uncertainty-based framework to jointly learn 3D human poses and the uncertainty of each joint. Our proposed joint estimation framework aims to mitigate the adverse effects of training samples with high uncertainties and facilitate the training procedure. To be specific, we model each body joint as a Laplace distribution for uncertainty representation. Since visual joints often exhibit low uncertainties while occluded ones have high uncertainties, we develop an adaptive scaling factor, named the uncertainty-aware scaling factor, to ease the network optimization in accordance with the joint uncertainties. By doing so, our network is able to converge faster and significantly reduce the adverse effects caused by those ambiguous joints. Furthermore, we present an uncertainty-aware graph convolutional network by exploiting the learned joint uncertainties and the relationships among joints to refine the initial joint localization. Extensive experiments on single-person (Human3.6M) and multi-person (MuCo-3DHP & MuPoTS-3D) 3D human pose estimation datasets demonstrate the effectiveness of our method.}
}
@article{TONG2022108962,
title = {Neural architecture search via reference point based multi‐objective evolutionary algorithm},
journal = {Pattern Recognition},
volume = {132},
pages = {108962},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108962},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004423},
author = {Lyuyang Tong and Bo Du},
keywords = {Neural architecture search, Multi-objective evolutionary algorithm, The image classification},
abstract = {For neural architecture search, NSGA-Net has searched a representative neural architecture set of Pareto-optimal solutions to consider both accuracy and computation complexity simultaneously. However, some decision-makers only concentrate on such neural architectures in the subpart regions of Pareto-optimal Frontier that they have interests in. Under the above circumstances, certain uninterested neural architectures may cost many computing resources. In order to consider the preference of decision-makers, we propose the reference point based NSGA-Net (RNSGA-Net) for neural architecture search. The core of RNSGA-Net adopts the reference point approach to guarantee the Pareto-optimal region close to the reference points and also combines the advantage of NSGAII with the fast nondominated sorting approach to split the Pareto front. Moreover, we augment an extra bit value of the original encoding to represent two types of residual block and one type of dense block for residual connection and dense connection in the RNSGA-Net. In order to satisfy the decision-maker preference, the multi-objective is measured to search competitive neural architecture by minimizing an error metric and FLOPs of computational complexity. Experiment results on the CIFAR-10 dataset demonstrate that RNSGA-Net can improve NSGA-Net in terms of the more structured representation space and the preference of decision-makers.}
}
@article{SHEN2022108942,
title = {Distribution alignment for cross-device palmprint recognition},
journal = {Pattern Recognition},
volume = {132},
pages = {108942},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108942},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004228},
author = {Lei Shen and Yingyi Zhang and Kai Zhao and Ruixin Zhang and Wei Shen},
keywords = {Palmprint recognition, Deep learning, Loss function, Biometric recognition, Person Reidentification},
abstract = {With the development of IoT and mobile devices, cross-device palmprint recognition is becoming an emerging research topic in multimedia for its great application potential. Due to the diverse characteristics of different devices, e.g.resolution or artifacts caused by post-processing, cross-device palmprint recognition remains a challenging problem. In this paper, we make efforts to improve cross-device palmprint recognition in two aspects: (1) we put forward a novel distribution-based loss to narrow the representation gap across devices, and (2) we establish a new cross-device benchmark based on existing palmprint recognition datasets. Different from many recent studies that only utilize instance-level or pairwise-level information between devices, the proposed progressive target distribution loss (PTD loss) uses the distributional information. Moreover, we establish a progressive target mechanism that will be dynamically updated during training, making the optimization easier and smoother. The newly established benchmark contains more samples and more types of IoT devices than previous benchmarks, which can facilitate cross-device palmprint research. Extensive comparisons on several benchmarks reveal that: (1) our method outperforms other cross-device biometric recognition approaches significantly; (2) our method presents superior performance compared to SOTA competitors on several general palmprint recognition benchmarks; Code and data are openly available at https://kaizhao.net/palmprint.}
}
@article{YU2022108915,
title = {Corrigendum to “LiDAR-based localization using universal encoding and memory-aware regression” Pattern Recognition Volume 128 (2022) 108685},
journal = {Pattern Recognition},
volume = {132},
pages = {108915},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108915},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200396X},
author = {Shangshu Yu and Cheng Wang and Chenglu Wen and Ming Cheng and Minghao Liu and Zhihong Zhang and Xin Li}
}
@article{ALYASEEN2022108912,
title = {Wrapper feature selection method based differential evolution and extreme learning machine for intrusion detection system},
journal = {Pattern Recognition},
volume = {132},
pages = {108912},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108912},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003934},
author = {Wathiq Laftah Al-Yaseen and Ali Kadhum Idrees and Faezah Hamad Almasoudy},
keywords = {Intrusion detection system (IDS), Feature selection, Differential evolution (DE), Extreme learning machine (ELM), NSL-KDD},
abstract = {The intrusion detection system (IDS) has gained a rapid increase of interest due to its widely recognized potential in various security fields, however, it suffers from several challenges. Different network datasets have several redundant and irrelevant features that affect the decision of the IDS classifier. Therefore, it is essential to decrease these features to improve the system performance. In this paper, an efficient wrapper feature selection method is proposed for improving the performance and decreasing the processing time of the IDS. The proposed approach employs a differential evaluation algorithm to select the useful features whilst the extreme learning machine classifier is applied after feature selection to evaluate the selected features. Many experiments are performed using the full NSL-KDD dataset to evaluate the performance of the proposed method. The results prove that the proposed approach can efficiently reduce the features, increase the accuracy, reduce the false alarm rates, and improve the processing time of the IDS in comparison to other recent related works.}
}
@article{YANG2022108949,
title = {Center Prediction Loss for Re-identification},
journal = {Pattern Recognition},
volume = {132},
pages = {108949},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108949},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004290},
author = {Lu Yang and Yunlong Wang and Lingqiao Liu and Peng Wang and Yanning Zhang},
keywords = {Person re-identification, Loss, Deep metric learning},
abstract = {The training loss function that enforces certain training sample distribution patterns plays a critical role in building a re-identification (ReID) system. Besides the basic requirement of discrimination, i.e., the features corresponding to different identities should not be mixed, additional intra-class distribution constraints, such as features from the same identities should be close to their centers, have been adopted to construct losses. Despite the advances of various new loss functions, it is still challenging to strike the balance between the need of reducing the intra-class variation and allowing certain distribution freedom. Traditional intra-class losses try to shrink samples of the same class into one point in the feature space and may easily drop their intra-class similarity structure. In this paper, we propose a new loss based on center predictivity, that is, a sample must be positioned in a location of the feature space such that from it we can roughly predict the location of the center of same-class samples. The prediction error is then regarded as a loss called Center Prediction Loss (CPL). Unlike most existing metric learning loss functions, CPL involves learnable parameters, i.e., the center predictor, which brings a remarkable change in the properties of the loss. In particular, it allows higher freedom in intra-class distributions. And the parameters in CPL will be discarded after training. Extensive experiments on various real-world ReID datasets show that the proposed loss can achieve superior performance and can also be complementary to existing losses.}
}
@article{GUO2022108928,
title = {Metric learning via perturbing hard-to-classify instances},
journal = {Pattern Recognition},
volume = {132},
pages = {108928},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108928},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004095},
author = {Xinyao Guo and Wei Wei and Jianqing Liang and Chuangyin Dang and Jiye Liang},
keywords = {Metric learning, Hard-to-classify instances, Instance perturbation, Alternating minimization},
abstract = {Constraint selection is an effective means to alleviate the problem of a massive amount of constraints in metric learning. However, it is difficult to find and deal with all association constraints with the same hard-to-classify instance (i.e., an instance surrounded by dissimilar instances), negatively affecting metric learning algorithms. To address this problem, we propose a new metric learning algorithm from the perspective of selecting instances, Metric Learning via Perturbing of Hard-to-classify Instances (ML-PHI), which directly perturbs the hard-to-classify instances to reduce over-fitting for the hard-to-classify instances. ML-PHI perturbs hard-to-classify instances to be closer to similar instances while keeping the positions of the remaining instances as constant as possible. As a result, the negative impacts of hard-to-classify instances are effectively reduced. We have conducted extensive experiments on real data sets, and the results show that ML-PHI is effective and outperforms state-of-the-art methods.}
}
@article{AUDIBERT2022108945,
title = {Do deep neural networks contribute to multivariate time series anomaly detection?},
journal = {Pattern Recognition},
volume = {132},
pages = {108945},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108945},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004253},
author = {Julien Audibert and Pietro Michiardi and Frédéric Guyard and Sébastien Marti and Maria A. Zuluaga},
keywords = {Anomaly detection, Multivariate time series, Neural networks},
abstract = {Anomaly detection in time series is a complex task that has been widely studied. In recent years, the ability of unsupervised anomaly detection algorithms has received much attention. This trend has led researchers to compare only learning-based methods in their articles, abandoning some more conventional approaches. As a result, the community in this field has been encouraged to propose increasingly complex learning-based models mainly based on deep neural networks. To our knowledge, there are no comparative studies between conventional, machine learning-based and, deep neural network methods for the detection of anomalies in multivariate time series. In this work, we study the anomaly detection performance of sixteen conventional, machine learning-based and, deep neural network approaches on five real-world open datasets. By analyzing and comparing the performance of each of the sixteen methods, we show that no family of methods outperforms the others. Therefore, we encourage the community to reincorporate the three categories of methods in the anomaly detection in multivariate time series benchmarks.}
}
@article{MARCHETTI2022108920,
title = {Local-to-Global Support Vector Machines (LGSVMs)},
journal = {Pattern Recognition},
volume = {132},
pages = {108920},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108920},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004010},
author = {F. Marchetti and E. Perracchione},
keywords = {Local-to-global support vector machines, Partition of unity, Supervised classification, Kernel models},
abstract = {For supervised classification tasks that involve a large number of instances, we propose and study a new efficient tool, namely the Local-to-Global Support Vector Machine (LGSVM) method. Its background somehow lies in the framework of approximation theory and of local kernel-based models, such as the Partition of Unity (PU) method. Indeed, even if the latter needs to be accurately tailored for classification tasks, such as allowing the use of the cosine semi-metric for defining the patches, the LGSVM is a global method constructed by gluing together the local SVM contributions via compactly supported weights. When the number of instances grows, such a construction of a global classifier enables us to significantly reduce the usually high complexity cost of SVMs. This claim is supported by a theoretical analysis of the LGSVM and of its complexity as well as by extensive numerical experiments carried out by considering benchmark datasets.}
}
@article{KHO2022108953,
title = {Exploiting shape cues for weakly supervised semantic segmentation},
journal = {Pattern Recognition},
volume = {132},
pages = {108953},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108953},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004332},
author = {Sungpil Kho and Pilhyeon Lee and Wonyoung Lee and Minsong Ki and Hyeran Byun},
keywords = {Semantic segmentation, Weakly supervised learning, Texture biases, Shape cues},
abstract = {Weakly supervised semantic segmentation (WSSS) aims to produce pixel-wise class predictions with only image-level labels for training. To this end, previous methods adopt the common pipeline: they generate pseudo masks from class activation maps (CAMs) and use such masks to supervise segmentation networks. However, it is challenging to derive comprehensive pseudo masks that cover the whole extent of objects due to the local property of CAMs, i.e., they tend to focus solely on small discriminative object parts. In this paper, we associate the locality of CAMs with the texture-biased property of convolutional neural networks (CNNs). Accordingly, we propose to exploit shape information to supplement the texture-biased CNN features, thereby encouraging mask predictions to be not only comprehensive but also well-aligned with object boundaries. We further refine the predictions in an online fashion with a novel refinement method that takes into account both the class and the color affinities, in order to generate reliable pseudo masks to supervise the model. Importantly, our model is end-to-end trained within a single-stage framework and therefore efficient in terms of the training cost. Through extensive experiments on PASCAL VOC 2012, we validate the effectiveness of our method in producing precise and shape-aligned segmentation results. Specifically, our model surpasses the existing state-of-the-art single-stage approaches by large margins. What is more, it also achieves a new state-of-the-art performance over multi-stage approaches, when adopted in a simple two-stage pipeline without bells and whistles.}
}
@article{SUN2022108969,
title = {Stochastic gate-based autoencoder for unsupervised hyperspectral band selection},
journal = {Pattern Recognition},
volume = {132},
pages = {108969},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108969},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004496},
author = {He Sun and Lei Zhang and Lizhi Wang and Hua Huang},
keywords = {Hyperspectral data, Unsupervised band selection, Autoencoder, Stochastic gate},
abstract = {Due to its strong feature representation ability, the deep learning (DL)-based method is preferable for the unsupervised band selection task of hyperspectral image (HSI). However, the current DL-based UBS methods have not further investigated the nonlinear relationship between spectral bands, a more robust DL model with effective loss function is desired. To solve the above problem, a novel stochastic gate-based autoencoder (SGAE) has been proposed for the UBS task. With the proposed stochastic gate layer, the desired band subset with learnable parameters can be directly obtained. For obtaining better UBS results, a nonlinear regularization term is added with the loss function to supervise the training process of SGAE. Furthermore, an early stopping criteria with a regularization term-based threshold is developed. Experimental results on four publicly available remote sensing datasets prove the effectiveness of our SGAE.}
}
@article{ZHUANG2022108907,
title = {Multi-criteria Selection of Rehearsal Samples for Continual Learning},
journal = {Pattern Recognition},
volume = {132},
pages = {108907},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108907},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003880},
author = {Chen Zhuang and Shaoli Huang and Gong Cheng and Jifeng Ning},
keywords = {Continual Learning, Multiple Criteria, Rehersal Method, Learning to learn},
abstract = {Retaining a small subset to replay is a direct and effective way to prevent catastrophic forgetting in continual learning. However, due to data complexity and restricted memory, picking a proper subset for rehearsal is challenging and still being explored. In this work, we present a Multi-criteria Subset Selection approach that can stabilize and advance replay-based continual learning. The method picks rehearsal samples by integrating multiple criteria, including distance to prototype, intra-class cluster variation, and classifier loss. By doing so, it maximizes the comprehensive representation power of the sampled subset by ensuring its representativeness, diversity, and discriminability. We empirically find that singular criteria are likely to fail in particular tasks, while multi-criteria minimizes this risk and stabilizes task training throughout the continual learning process. Moreover, our method improves replay-based methods consistently and achieves state-of-the-art performance on both CIFAR100 and Tiny-Imagenet datasets.}
}
@article{WANG2022108961,
title = {Learnable dynamic margin in deep metric learning},
journal = {Pattern Recognition},
volume = {132},
pages = {108961},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108961},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004411},
author = {Yifan Wang and Pingping Liu and Yijun Lang and Qiuzhan Zhou and Xue Shan},
keywords = {Deep metric learning, Proxy-based loss, Adaptive margin, Image retrieval, Fine-grained images},
abstract = {With the deepening of deep neural network research, deep metric learning has been further developed and achieved good results in many computer vision tasks. Deep metric learning trains the deep neural network by designing appropriate loss functions, and the deep neural network projects the training samples into an embedding space, where similar samples are very close, while dissimilar samples are far away. In the past two years, the proxy-based loss achieves remarkable improvements, boosts the speed of convergence and is robust against noisy labels and outliers due to the introduction of proxies. In the previous proxy-based losses, fixed margins were used to achieve the goal of metric learning, but the intra-class variance of fine-grained images were not fully considered. In this paper, a new proxy-based loss is proposed, which aims to set a learnable margin for each class, so that the intra-class variance can be better maintained in the final embedding space. Moreover, we also add a loss between proxies, so as to improve the discrimination between classes and further maintain the intra-class distribution. Our method is evaluated on fine-grained image retrieval, person re-identification and remote sensing image retrieval common benchmarks. The standard network trained by our loss achieves state-of-the-art performance. Thus, the possibility of extending our method to different fields of pattern recognition is confirmed.}
}
@article{IBANEZ2022108933,
title = {Generalized discriminant analysis via kernel exponential families},
journal = {Pattern Recognition},
volume = {132},
pages = {108933},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108933},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004058},
author = {Isaías Ibañez and Liliana Forzani and Diego Tomassi},
keywords = {Discriminant analysis, Sufficient dimension reduction, Reproducing kernel Hilbert spaces, Support vector machine},
abstract = {This paper introduces a novel supervised dimension reduction method for classification and regression problems using reproducing kernel Hilbert spaces. The proposed approach takes advantage of the modeling power of kernel exponential families to extract nonlinear summary statistics of the data that are sufficient to preserve information about the target response. For the special case of finite dimensional exponential family distributions, the proposed method is shown to simplify the known solutions for sufficient dimension reduction. A connection with support vector machines is shown and exploited to obtain efficient estimation procedures. Experiments with simulated and real data illustrate the potential of the proposed approach.}
}
@article{CUI2022108988,
title = {Progressive downsampling and adaptive guidance networks for dynamic scene deblurring},
journal = {Pattern Recognition},
volume = {132},
pages = {108988},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108988},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200468X},
author = {Jinkai Cui and Weihong Li and Wei Guo and Weiguo Gong},
keywords = {Progressive downsampling, Adaptive guidance, Blended activation, Multisupervision, Dynamic scene deblurring},
abstract = {The existing learning-based dynamic scene deblurring methods have made good progress to some extent. However, these methods are usually based on multiscale strategy, which has the following shortcomings: (1) The bilinear downsampling operation will cause some loss of important high-frequency information, e.g., strong edges, which also further affects the network learning a better deblurring mapping. (2) Existing methods only use a single activation function, which limits the ability of the network model to fit data and causes the network performance to be easily saturated. Therefore, we propose an end-to-end progressive downsampling and adaptive guidance network called PDAG-Net for solving above problems. The proposed PDAG-Net can retain more strong edges and other high-frequency information of a blurry image so as to make the network learn a more effective deblurring mapping between the input and label images. In the proposed PDAG-Net, we implement a multiscale blended activation residual block called MSBA-ResBlock for learning the nonlinear characteristics of dynamic scene blur, which can also alleviate the performance saturation problem caused by a single activation function and improve multiscale feature extraction ability. Finally, we propose a multisupervision strategy for obtaining more robust and effective features and making the network possess more stable trainging and faster convergence. Extensive experimental results on a public dataset indicate that the proposed network outperforms the state-of-the-art image deblurring methods.}
}
@article{FAN2022108932,
title = {Riemannian dynamic generalized space quantization learning},
journal = {Pattern Recognition},
volume = {132},
pages = {108932},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108932},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004137},
author = {MengLing Fan and Fengzhen Tang and Yinan Guo and Xingang Zhao},
keywords = {Learning vector quantization, Dynamic learning vector quantization, Riemannian manifold, Short-term memory},
abstract = {Many existing works represent signals by covariance matrices and then develop learning methods on the Riemannian symmetric positive-definite (SPD) manifold to deal with such data. However, they summarize each instance with a single covariance matrix, omitting some potential important information, such as the time evolution of the correlation in signals. In this paper, we represent each instance by a sequence of covariance matrices and develop a novel dynamic generalized learning Riemannian space quantization (DGLRSQ) method to deal with such data representations. The proposed DGLRSQ method incorporates short-term memory mechanism in generalized learning Riemannian space quantization (GLRSQ), which is an extension of Euclidean generalized learning vector quantization to deal with SPD matrix-valued data. The proposed method can capture the temporal evolution of the correlation in signals and thus provides better performance to its the counterpart – GLRSQ, which treats each instance as a signal covariance matrix. Empirical investigations on synthetic data and motor imagery EEG data show the superior performance of the proposed method.}
}
@article{JIANG2022108965,
title = {JSL3d: Joint subspace learning with implicit structure supervision for 3D pose estimation},
journal = {Pattern Recognition},
volume = {132},
pages = {108965},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108965},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004459},
author = {Mengxi Jiang and Shihao Zhou and Cuihua Li and Yunqi Lei},
keywords = {, , , },
abstract = {Estimating 3D human poses from a single image is an important task in computer graphics. Most model-based estimation methods represent the labeled/detected 2D poses and the projection of approximated 3D poses using vector representations of body joints. However, such lower-dimensional vector representations fail to maintain the spatial relations of original body joints, because the representations do not consider the inherent structure of body joints. In this paper, we propose JSL3d, a novel joint subspace learning approach with implicit structure supervision based on Sparse Representation (SR) model, capturing the latent spatial relations of 2D body joints by an end-to-end autoencoder network. JSL3djointly combines the learned latent spatial relations and 2D joints as inputs for the standard SR inference frame. The optimization is simultaneously processed via geometric priors in both latent and original feature spaces. We have evaluated JSL3dusing four large-scale and well-recognized benchmarks, including Human3.6M, HumanEva-I, CMU MoCap and MPII. The experiment results demonstrate the effectiveness of JSL3d.}
}
@article{WANG2022108903,
title = {Multiple geometry representations for 6D object pose estimation in occluded or truncated scenes},
journal = {Pattern Recognition},
volume = {132},
pages = {108903},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108903},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003843},
author = {Jichun Wang and Lemiao Qiu and Guodong Yi and Shuyou Zhang and Yang Wang},
keywords = {Neural network, Pose estimation, Keypoints, Edge vectors, Symmetry correspondences},
abstract = {Deep learning-based 6D object pose estimation methods from a single RGBD image have recently received increasing attention because of their powerful representation learning capabilities. These methods, however, cannot handle severe occlusion and truncation. In this paper, we present a novel 6D object pose estimation method based on multiple geometry representations. Specifically, we introduce a network to fuse the appearance and geometry features extracted from input color and depth images. Then, we utilize these per-point fusion features to estimate keypoint offsets, edge vectors, and dense symmetry correspondences in the canonical coordinate system. Finally, a two-stage pose regression module is applied to compute the 6D pose of an object. Relative to the unitary 3D keypoint-based strategy, such combination of multiple geometry representations provides sufficient and diverse information, especially for occluded or truncated scenes. To show the robustness to occlusion and truncation of the proposed method, we conduct comparative experiments on the Occlusion LineMOD, Truncation LineMOD, and T-LESS datasets. Results reveal that the proposed method outperforms state-of-the-art techniques by a large margin.}
}
@article{ZHENG2022108941,
title = {Unsupervised domain adaptation in homogeneous distance space for person re-identification},
journal = {Pattern Recognition},
volume = {132},
pages = {108941},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108941},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004216},
author = {Dingyuan Zheng and Jimin Xiao and Yunchao Wei and Qiufeng Wang and Kaizhu Huang and Yao Zhao},
keywords = {Person re-identification, Unsupervised domain adaptation, Distribution alignment, Clustering, Pseudo label},
abstract = {Data distribution alignment and clustering-based self-training are two feasible solutions to tackle unsupervised domain adaptation (UDA) on person re-identification (re-ID). Most existing alignment-based methods solely learn the source domain decision boundaries and align the data distribution of the target domain to the source domain, thus the re-ID performance on the target domain completely depends on the shared decision boundaries and how well the alignment is performed. However, two domains can hardly be precisely aligned because of the label space discrepancy of two domains, resulting in poor target domain re-ID performance. Although clustering-based self-training approaches could learn independent decision boundaries on the pseudo-labelled target domain data, they ignore both the accurate ID-related information of the labelled source domain data and the underlying relations between two domains. To fully exploit the source domain data to learn discriminative target domain ID-related features, in this paper, we propose a novel cross-domain alignment method in the homogeneous distance space, which is constructed by the newly designed stair-stepping alignment (SSA) matcher. Such alignment method can be integrated into both alignment-based framework and clustering-based framework. Extensive experiments validate the effectiveness of our proposed alignment method in these two frameworks. We achieve superior performance when the proposed alignment module is integrated into the clustering-based framework. Codes will be available at: http://github.com/Dingyuan-Zheng/HDS.}
}
@article{PATRO2022108898,
title = {Explanation vs. attention: A two-player game to obtain attention for VQA and visual dialog},
journal = {Pattern Recognition},
volume = {132},
pages = {108898},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108898},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200379X},
author = {Badri N. Patro and  Anupriy and Vinay P. Namboodiri},
keywords = {CNN, LSTM, Explanation, Attention, Grad-CAM, MMD, CORAL, GAN, VQA, Visual Dialog, Deep learning},
abstract = {In this paper, we aim to obtain improved attention for a visual question answering (VQA) task. It is challenging to provide supervision for attention. An observation we make is that visual explanations as obtained through class activation mappings (specifically Grad-CAM) that are meant to explain the performance of various networks could form a means of supervision. However, as the distributions of attention maps and that of Grad-CAMs differ, it would not be suitable to directly use these as a form of supervision. Rather, we propose the use of a discriminator that aims to distinguish samples of visual explanation and attention maps. The use of adversarial training of the attention regions as a two-player game between attention and explanation serves to bring the distributions of attention maps and visual explanations closer. Significantly, we observe that providing such a means of supervision also results in attention maps that are more closely related to human attention resulting in a substantial improvement over baseline stacked attention network (SAN) models. It also results in a good improvement in rank correlation metric on the VQA task. This method can also be combined with recent MCB based methods and results in consistent improvement. We also provide comparisons with other means for learning distributions such as based on Correlation Alignment (Coral), Maximum Mean Discrepancy (MMD) and Mean Square Error (MSE) losses and observe that the adversarial loss outperforms the other forms of learning the attention maps. A generalization of the work is also provided by extending our approach to the task of ‘Visual Dialog’ where the attention is more contextual. Thorough evaluation for this task is also provided. Visualization of the results confirms our hypothesis that attention maps improve using the proposed form of supervision.}
}
@article{SHI2022108879,
title = {Weighting and pruning based ensemble deep random vector functional link network for tabular data classification},
journal = {Pattern Recognition},
volume = {132},
pages = {108879},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108879},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003600},
author = {Qiushi Shi and Minghui Hu and Ponnuthurai Nagaratnam Suganthan and Rakesh Katuwal},
keywords = {Ensemble deep random vector functional link (edRVFL), Weighting methods, Pruning, UCI classification datasets},
abstract = {In this paper, we first integrate normalization to the Ensemble Deep Random Vector Functional Link network (edRVFL). This re-normalization step can help the network avoid divergence of the hidden features. Then, we propose novel variants of the edRVFL network. Weighted edRVFL (WedRVFL) uses weighting methods to give training samples different weights in different layers according to how the samples were classified confidently in the previous layer thereby increasing the ensemble’s diversity and accuracy. Furthermore, a pruning-based edRVFL (PedRVFL) has also been proposed. We prune some inferior neurons based on their importance for classification before generating the next hidden layer. Through this method, we ensure that the randomly generated inferior features will not propagate to deeper layers. Subsequently, the combination of weighting and pruning, called Weighting and Pruning based Ensemble Deep Random Vector Functional Link Network (WPedRVFL), is proposed. We compare their performances with other state-of-the-art classification methods on 24 tabular UCI classification datasets. The experimental results illustrate the superior performance of our proposed methods.}
}
@article{LI2022108911,
title = {Learning intra-domain style-invariant representation for unsupervised domain adaptation of semantic segmentation},
journal = {Pattern Recognition},
volume = {132},
pages = {108911},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108911},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003922},
author = {Zongyao Li and Ren Togo and Takahiro Ogawa and Miki Haseyama},
keywords = {Style-invariant representation, Self-ensembling, Domain adaptation},
abstract = {In this paper, we aim to tackle the problem of unsupervised domain adaptation (UDA) of semantic segmentation and improve the UDA performance with a novel conception of learning intra-domain style-invariant representation. Previous UDA methods focused on reducing the inter-domain inconsistency between the source domain and the target domain. However, due to the different data distributions of the two domains, reducing the inter-domain inconsistency cannot ensure the generalization ability of the trained model in the target domain. Therefore, to improve the UDA performance, we take into consideration the intra-domain diversity of the target domain for the first time in studies on UDA and aim to train the model to generalize well to the diverse intra-domain styles. To achieve this, we propose a self-ensembling method to learn the intra-domain style-invariant representation and we introduce a semantic-aware multimodal image-to-image translation model to obtain images with diversified intra-domain styles. Our method achieves state-of-the-art performance on two synthetic-to-real adaptation benchmarks, and we demonstrate the effectiveness of our method by conducting extensive experiments.}
}
@article{LIU2022108960,
title = {An End-to-end Supervised Domain Adaptation Framework for Cross-Domain Change Detection},
journal = {Pattern Recognition},
volume = {132},
pages = {108960},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108960},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200440X},
author = {Jia Liu and Wenjie Xuan and Yuhang Gan and Yibing Zhan and Juhua Liu and Bo Du},
keywords = {Change Detection, Supervised Domain Adaptation, Image Adaptation, Feature Adaptation},
abstract = {Change detection is a crucial but extremely challenging task in remote sensing image analysis, and much progress has been made with the rapid development of deep learning. However, most existing deep learning-based change detection methods try to elaborately design complicated neural networks with powerful feature representations. However, they ignore the universal domain shift induced by time-varying land cover changes, including luminance fluctuations and seasonal changes between pre-event and post-event images, thereby producing suboptimal results. In this paper, we propose an end-to-end supervised domain adaptation framework for cross-domain change detection named SDACD, to effectively alleviate the domain shift between bi-temporal images for better change predictions. Specifically, our SDACD presents collaborative adaptations from both image and feature perspectives with supervised learning. Image adaptation exploits generative adversarial learning with cycle-consistency constraints to perform cross-domain style transformation, which effectively narrows the domain gap in a two-side generation fashion. As for feature adaptation, we extract domain-invariant features to align different feature distributions in the feature space, which could further reduce the domain gap of cross-domain images. To further improve the performance, we combine three types of bi-temporal images for the final change prediction, including the initial input bi-temporal images and two generated bi-temporal images from the pre-event and post-event domains. Extensive experiments and analyses conducted on two benchmarks demonstrate the effectiveness and generalizability of our proposed framework. Notably, our framework pushes several representative baseline models up to new State-Of-The-Art records, achieving 97.34% and 92.36% on the CDD and WHU building datasets, respectively. The source code and models are publicly available at https://github.com/Perfect-You/SDACD.}
}
@article{SHEN2022108909,
title = {Joint operation and attention block search for lightweight image restoration},
journal = {Pattern Recognition},
volume = {132},
pages = {108909},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108909},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003909},
author = {Hao Shen and Zhong-Qiu Zhao and Wenrui Liao and Weidong Tian and De-Shuang Huang},
keywords = {Image restoration, Neural architecture search, Attention mechanism},
abstract = {Recently, block-based design methods have shown effectiveness in image restoration tasks, which are usually designed in a handcrafted manner and have computation and memory consumption challenges in practice. In this paper, we propose a joint operation and attention block search algorithm for image restoration, which focuses on searching for optimal combinations of operation blocks and attention blocks. Specifically, we first construct two search spaces: operation block search space and attention block search space. The former is used to explore the suitable operation of each layer and aims to construct a lightweight and effective operation search module (OSM). The latter is applied to discover the optimal connection of various attention mechanisms and aims to enhance the feature expression. The searched structure is called the attention search module (ASM). Then we combine OSM and ASM to construct a joint search module (JSM), which serves as the basic module to build the final network. Moreover, we propose a cross-scale fusion module (CSFM) to effectively integrate multiple hierarchical features from JSMs, which helps to mine feature corrections of intermediate layers. Extensive experiments on image super-resolution, gray image denoising, and JPEG image deblocking tasks demonstrate that our proposed network can achieve competitive performance. The source code is available on https://github.com/it-hao/JSNet.}
}
@article{GOMEZ2022108927,
title = {BR-NPA: A non-parametric high-resolution attention model to improve the interpretability of attention},
journal = {Pattern Recognition},
volume = {132},
pages = {108927},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108927},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004083},
author = {Tristan Gomez and Suiyi Ling and Thomas Fréour and Harold Mouchère},
keywords = {Deep learning, Interpretability, Spatial attention, Resolution, Non-parametric},
abstract = {The prevalence of employing attention mechanisms has brought along concerns about the interpretability of attention distributions. Although it provides insights into how a model is operating, utilizing attention as the explanation of model predictions is still highly dubious. The community is still seeking more interpretable strategies for better identifying local active regions that contribute the most to the final decision. To improve the interpretability of existing attention models, we propose a novel Bilinear Representative Non-Parametric Attention (BR-NPA) strategy that captures the task-relevant human-interpretable information. The target model is first distilled to have higher-resolution intermediate feature maps. From which, representative features are then grouped based on local pairwise feature similarity, to produce finer-grained, more precise attention maps highlighting task-relevant parts of the input. The obtained attention maps are ranked according to the activity level of the compound feature, which provides information regarding the important level of the highlighted regions. The proposed model can be easily adapted in a wide variety of modern deep models, where classification is involved. Extensive quantitative and qualitative experiments showcase more comprehensive and accurate visual explanations compared to state-of-the-art attention models and visualization methods across multiple tasks including fine-grained image classification, few-shot classification, and person re-identification, without compromising the classification accuracy. The proposed visualization model sheds imperative light on how neural networks ‘pay their attention’ differently in different tasks.}
}
@article{LIU2022108944,
title = {In the eye of the beholder: A survey of gaze tracking techniques},
journal = {Pattern Recognition},
volume = {132},
pages = {108944},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108944},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004241},
author = {Jiahui Liu and Jiannan Chi and Huijie Yang and Xucheng Yin},
keywords = {Gaze estimation, eye features, appearance-based, personal calibration, head motion},
abstract = {Gaze tracking estimates and tracks the user’s gaze by analyzing facial or eye features, it is an important way to realize automated vision-based interaction. This paper introduces the visual information used in gaze tracking, and discusses the commonly used gaze estimation methods and their research dynamics, including: 2D mapping-based methods, 3D model-based methods, and appearance-based methods. In this way, some key issues that need to be solved in these methods are considered, and their research trends are discussed. Their characteristics in system configuration, personal calibration, head motion, gaze accuracy and robustness are also compared. Finally, the applications of gaze tracking techniques are analyzed from various application factors and fields. This paper reviews the latest development of gaze tracking, focuses more on various gaze tracking algorithms and their existing challenges. The development trends of gaze tracking are prospected, which provides ideas for future theoretical research and practical applications.}
}
@article{SHI2022108923,
title = {Robust convolutional neural networks against adversarial attacks on medical images},
journal = {Pattern Recognition},
volume = {132},
pages = {108923},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108923},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004046},
author = {Xiaoshuang Shi and Yifan Peng and Qingyu Chen and Tiarnan Keenan and Alisa T. Thavikulwat and Sungwon Lee and Yuxing Tang and Emily Y. Chew and Ronald M. Summers and Zhiyong Lu},
keywords = {CNNs, Adversarial examples, Sparsity denoising},
abstract = {Convolutional neural networks (CNNs) have been widely applied to medical images. However, medical images are vulnerable to adversarial attacks by perturbations that are undetectable to human experts. This poses significant security risks and challenges to CNN-based applications in clinic practice. In this work, we quantify the scale of adversarial perturbation imperceptible to clinical practitioners and investigate the cause of the vulnerability in CNNs. Specifically, we discover that noise (i.e., irrelevant or corrupted discriminative information) in medical images might be a key contributor to performance deterioration of CNNs against adversarial perturbations, as noisy features are learned unconsciously by CNNs in feature representations and magnified by adversarial perturbations. In response, we propose a novel defense method by embedding sparsity denoising operators in CNNs for improved robustness. Tested with various state-of-the-art attacking methods on two distinct medical image modalities, we demonstrate that the proposed method can successfully defend against those unnoticeable adversarial attacks by retaining as much as over 90% of its original performance. We believe our findings are critical for improving and deploying CNN-based medical applications in real-world scenarios.}
}
@article{LIU2022108952,
title = {Non-rigid point set registration based on local neighborhood information support},
journal = {Pattern Recognition},
volume = {132},
pages = {108952},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108952},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004320},
author = {Chuanju Liu and Dongmei Niu and Peng Wang and Xiuyang Zhao and Bo Yang and Caiming Zhang},
keywords = {Non-rigid point set registration, Gaussian mixture model, Expectation–Maximization method, Local neighborhood information},
abstract = {Non-rigid point set registration is a crucial task and an unsolved problem in the field of computer vision. One commonly used method for solving the problem is based on the Gaussian mixture model (GMM). In this method, the point set registration is formalized as a probability density estimation problem. Most GMM-based methods achieve registration by maintaining global and local structures of points. However, the previous methods did not filter the neighborhood information in the local structure, and the quality of local neighborhood information directly affects the accuracy of registration. Therefore, extracting effective local neighborhood information is still a challenge. We propose a novel point set registration method based on GMM by extracting local neighborhood information. The two point sets X and Y are regarded as the centroids of GMM and data points produced by GMM, respectively. Our method computes initial correspondences by comparing the feature descriptors of point sets, and the initial correspondences are updated by considering the neighborhood information. Our method then uses the Expectation–Maximization method to solve the GMM. In the experimental results, the efficiency and advantages of our method relative to the current methods are verified by applying five commonly used datasets.}
}
@article{ZHAO2022108984,
title = {Progressive Deep Non-Negative Matrix Factorization Architecture with Graph Convolution-based Basis Image Reorganization},
journal = {Pattern Recognition},
volume = {132},
pages = {108984},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108984},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004642},
author = {Yang Zhao and Furong Deng and Jihong Pei and Xuan Yang},
keywords = {Deep non-negative matrix factorization, Graph convolution, Basis image reconstruction, Basis image factorization, Face recognition},
abstract = {Deep non-negative matrix factorization is committed to using multi-layer structure to extract underlying parts-based representation. However, the basis images obtained by continuous depth factorization is too sparse, resulting in too fragmented parts reflected by the basis image. This makes the number of factorization layers limited and the underlying local feature representation is inaccurate. Therefore, we propose a novel progressive deep non-negative matrix factorization (PDNMF) architecture that adds a basis image reconstruction step to the successive basis image factorization steps. This helps the basis image in depth factorization to maintain better robustness of feature representation. In the reconstruction step, the attribute similarity graph (ASG) is constructed to describe the semantic expression ability of each basis image. With the help of the ASG, the basis image enhances its own semantic integrity through graph convolution without drastically destroying its representation. The evaluation in image recognition shows that the recognition accuracy of the proposed PDNMF improves with the increase of layers. Our method outperforms the state-of-the-art deep factorization methods in image recognition.}
}
@article{DIETTERICH2022108931,
title = {The familiarity hypothesis: Explaining the behavior of deep open set methods},
journal = {Pattern Recognition},
volume = {132},
pages = {108931},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108931},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004125},
author = {Thomas G. Dietterich and Alex Guyer},
keywords = {Anomaly detection, Open set learning, Computer vision, Object recognition, Novel category detection, Representation learning, Deep learning},
abstract = {In many object recognition applications, the set of possible categories is an open set, and the deployed recognition system will encounter novel objects belonging to categories unseen during training. Detecting such “novel category” objects is usually formulated as an anomaly detection problem. Anomaly detection algorithms for feature-vector data identify anomalies as outliers, but outlier detection has not worked well in deep learning. Instead, methods based on the computed logits of visual object classifiers give state-of-the-art performance. This paper proposes the Familiarity Hypothesis that these methods succeed because they are detecting the absence of familiar learned features rather than the presence of novelty. This distinction is important, because familiarity-based detection will fail in many situations where novelty is present. For example when an image contains both a novel object and a familiar one, the familiarity score will be high, so the novel object will not be noticed. The paper reviews evidence from the literature and presents additional evidence from our own experiments that provide strong support for this hypothesis. The paper concludes with a discussion of whether familiarity-based detection is an inevitable consequence of representation learning.}
}
@article{LI2022108948,
title = {Automatically classifying non-functional requirements using deep neural network},
journal = {Pattern Recognition},
volume = {132},
pages = {108948},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108948},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004289},
author = {Bing Li and Xiuwen Nong},
keywords = {Non-functional requirements, Non-functional requirements classification, BERT, N-gram, Bi-LSTM, Multi-sample dropout},
abstract = {Non-functional requirements are property that software products must have in order to meet the user’s business requirements, and are additional constraints on the quality and characteristics of software systems. They are generally written by software designers and documented in various parts of requirements documentation. When developing systems, developers need to classify non-functional requirements from requirements documents, and classifying these non-functional requirements requires professional skills, experience, and domain knowledge, which is challenging and time-consuming for developers. It would be beneficial to implement automatic classification of non-functional requirements from requirements documents, which could reduce the manual, time, and mental fatigue involved in identifying specific non-functional requirements from a large number of requirements. In this paper, a deep neural network model called NFRNet is designed to automatically classify non-functional requirements from software requirement documents. The network consists of two parts. One is an improved BERT word embedding model based on N-gram masking for learning context representation of the requirement descriptions, and the other is a Bi-LSTM classification network for capture context information of the requirement descriptions. We use a Softmax classifier in the end to classify the requirement descriptions. At the same time, in order to accelerate the training and improve the generalization ability of the model, the network uses multi-sample dropout regularization technology. This new regularization technology can reduce the number of iterations needed for training, accelerate the training of deep neural networks, and the networks trained achieved lower error rates. In addition, we expanded the original non-functional requirements dataset (PROMISE dataset) and designed a new dataset called SOFTWARE NFR. The new dataset far exceeds the original dataset in terms of the number of requirement description sentences and the number of non-functional requirements categories. It can be taken as a new testbed for non-functional requirements classification. Through cross-validation on the new dataset, the experimental results show that the network designed in this paper is significantly better than the other 17 classification methods in terms of Precision, Recall, and F1-score. At the same time, for the training set and the validation set, using the multi-sample dropout regularization technology can accelerate the training speed, reduce the number of iterations, and achieve lower error rates and loss.}
}
@article{LI2022108918,
title = {Unsupervised domain adaptation with progressive adaptation of subspaces},
journal = {Pattern Recognition},
volume = {132},
pages = {108918},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108918},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003995},
author = {Weikai Li and Songcan Chen},
keywords = {Unsupervised domain adaptation, Partial domain adaptation, Subspace learning, Pseudo label},
abstract = {Unsupervised Domain Adaptation (UDA) aims to classify unlabeled target domain by transferring knowledge from labeled source domain with domain shift. Most of the existing UDA methods try to mitigate the adverse impact induced by the shift via reducing domain discrepancy. However, such approaches easily suffer a notorious mode collapse issue due to the lack of labels in target domain. Naturally, one of the effective ways to mitigate this issue is to reliably estimate the pseudo labels for target domain, which itself is hard. To overcome this, we propose a novel UDA method named Progressive Adaptation of Subspaces approach (PAS) in which we utilize such an intuition that appears much reasonable to gradually obtain reliable pseudo labels. Specifically, we progressively and steadily refine the shared subspaces as bridge of knowledge transfer by adaptively anchoring/selecting and leveraging those target samples with reliable pseudo labels. Subsequently, the refined subspaces can in turn provide more reliable pseudo-labels of the target domain, making the mode collapse highly mitigated. Our thorough evaluation demonstrates that PAS is not only effective for common UDA, but also outperforms the state-of-the arts for more challenging Partial Domain Adaptation (PDA) situation, where the source label set subsumes the target one.}
}
@article{LV2022108956,
title = {Memory‐augmented neural networks based dynamic complex image segmentation in digital twins for self‐driving vehicle},
journal = {Pattern Recognition},
volume = {132},
pages = {108956},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108956},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004368},
author = {Zhihan Lv and Liang Qiao and Shuo Yang and Jinhua Li and Haibin Lv and Francesco Piccialli},
keywords = {Deep learning, Image segmentation, Memory-augmented neural networks, LSTM, Self-driving, Digital twins},
abstract = {With the continuous increase of the amount of information, people urgently need to identify the information in the image in more detail in order to obtain richer information from the image. This work explores the dynamic complex image segmentation of self-driving vehicle under Digital Twins (DTs) based on Memory-augmented Neural Networks (MANNs), so as to further improve the performance of self-driving in intelligent transportation. In view of the complexity of the environment and the dynamic changes of the scene in intelligent transportation, this work constructs a segmentation model for dynamic complex image of self-driving vehicle under DTs based on MANNs by optimizing the Deep Learning algorithm and further combining with the DTs technology, so as to recognize the information in the environment image during the self-driving. Finally, the performance of the constructed model is analyzed by experimenting with different image datasets (PASCALVOC 2012, NYUDv2, PASCAL CONTEXT, and real self-driving complex traffic image data). The results show that compared with other classical algorithms, the established MANN-based model has an accuracy of about 85.80%, the training time is shortened to 107.00 s, the test time is 0.70 s, and the speedup ratio is high. In addition, the average algorithm parameter of the given energy function α=0.06 reaches the maximum value. Therefore, it is found that the proposed model shows high accuracy and short training time, which can provide experimental reference for future image visual computing and intelligent information processing.}
}
@article{KIM2022108894,
title = {Conditional motion in-betweening},
journal = {Pattern Recognition},
volume = {132},
pages = {108894},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108894},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003752},
author = {Jihoon Kim and Taehyun Byun and Seungyoun Shin and Jungdam Won and Sungjoon Choi},
keywords = {Motion in-betweening, Conditional motion generation, Generative model, Motion data augmentation},
abstract = {Motion in-betweening (MIB) is a process of generating intermediate skeletal movement between the given start and target poses while preserving the naturalness of the motion, such as periodic footstep motion while walking. Although state-of-the-art MIB methods are capable of producing plausible motions given sparse key-poses, they often lack the controllability to generate motions satisfying the semantic contexts required in practical applications. We focus on the method that can handle pose or semantic conditioned MIB tasks using a unified model. We also present a motion augmentation method to improve the quality of pose-conditioned motion generation via defining a distribution over smooth trajectories. Our proposed method outperforms the existing state-of-the-art MIB method in pose prediction errors while providing additional controllability. Our code and results are available on our project web page: https://jihoonerd.github.io/Conditional-Motion-In-Betweening.}
}
@article{YANG2022108874,
title = {Learning deep feature correspondence for unsupervised anomaly detection and segmentation},
journal = {Pattern Recognition},
volume = {132},
pages = {108874},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108874},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003557},
author = {Jie Yang and Yong Shi and Zhiquan Qi},
keywords = {Anomaly detection, Anomaly segmentation, Feature correspondence, Dual network},
abstract = {Developing machine learning models that can detect and localize the unexpected or anomalous structures within images is very important for numerous computer vision tasks, such as the defect inspection of manufactured products. However, it is challenging especially when there are few or even no anomalous image samples available. In this paper, we propose an unsupervised mechanism, i.e. deep feature correspondence (DFC), which can be effectively leveraged to detect and segment out the anomalies in images solely with the prior knowledge from anomaly-free samples. We develop our DFC in an asymmetric dual network framework that consists of a generic feature extraction network and an elaborated feature estimation network, and detect the possible anomalies within images by modeling and evaluating the associated deep feature correspondence between the two dual network branches. Furthermore, to improve the robustness of the DFC and further boost the detection performance, we specifically propose a self-feature enhancement (SFE) strategy and a multi-context residual learning (MCRL) network module. Extensive experiments have been carried out to validate the effectiveness of our DFC and the proposed SFE and MCRL. Our approach is very effective for detecting and segmenting the anomalies that appear in confined local regions of images, especially the industrial anomalies. It advances the state-of-the-art performances on the benchmark dataset – MVTec AD. Besides, when applied to a real industrial inspection scene, it outperforms the comparatives by a large margin.}
}
@article{CHEN2022108964,
title = {Enhancement of DNN-based multilabel classification by grouping labels based on data imbalance and label correlation},
journal = {Pattern Recognition},
volume = {132},
pages = {108964},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108964},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004447},
author = {Ling Chen and Yuhong Wang and Hao Li},
keywords = {Multilabel classification, data imbalance, label correlation, neural network},
abstract = {Multilabel classification (MLC) is a challenging task in real-world applications, such as project document classification which led us to conduct this research. In the past decade, deep neural networks (DNNs) have been explored in MLC due to their flexibility in dealing with annotated data. However, DNN-based MLC still suffers many problems. Two critical problems are data imbalance and label correlation. These two problems will become more prominent when a training dataset is limited and with a large label set. In this study, special neural network configurations were developed to enhance the performance of DNN-based MLC based on data imbalance and label correlation. The classification accuracy of minority labels and users-preferred labels was increased using customized label groups. The proposed method was evaluated using river restoration project documents and other fifteen datasets. The results show that the proposed method generally increases f1-score for minority labels up to 10%. Adding label dependence into label groups improves the f1-score of user-preferred majority labels up to 5%. The accuracy increase varies in different datasets.}
}
@article{ZHAO2022108943,
title = {Towards a category-extended object detector with limited data},
journal = {Pattern Recognition},
volume = {132},
pages = {108943},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108943},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200423X},
author = {Bowen Zhao and Chen Chen and Xi Xiao and Shutao Xia},
keywords = {Object detector, Category-extended, Limited data, Multi-dataset},
abstract = {Object detectors are typically learned on fully-annotated training data with fixed predefined categories. However, categories are often required to be increased progressively. Usually, only the original training set annotated with old classes and some new training data labeled with new classes are available in such scenarios. Based on the limited datasets, a unified detector that can handle all categories is strongly needed. We propose a practical scheme to achieve it in this work. A conflict-free loss is designed to avoid label ambiguity, leading to an acceptable detector in one training round. To further improve performance, we propose a retraining phase in which Monte Carlo Dropout is employed to calculate the localization confidence to mine more accurate bounding boxes, and an overlap-weighted method is proposed for making better use of pseudo annotations during retraining. Extensive experiments demonstrate the effectiveness of our method.}
}
@article{EELAHI2022108972,
title = {Online temporal classification of human action using action inference graph},
journal = {Pattern Recognition},
volume = {132},
pages = {108972},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108972},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004526},
author = {G M Mashrur {E Elahi} and Yee-Hong Yang},
keywords = {Online temporal classification, Action recognition, Action detection, Action inference graph},
abstract = {Nowadays, deep learning methods have achieved state-of-the-art results in human action recognition. These methods process a full video sequence to recognize an action, which is unnecessary because many frames are similar. Recently, keyframe-based methods are proposed to overcome this issue. Though keyframe based methods have shown competitive performance in action recognition, both methods still process all the required frames of a video clip and average the results of individual clips/frames to recognize the action of the video. We argue that by simply using the average of the results of the video clips, deep models are not using the motion information of the video and thus leads to an inaccurate recognition of the action. To cope with the aforementioned issue, we propose a new online temporal classification model (OTCM) that classifies an action from a video in an online fashion and addresses the issue of averaging by making decision of each frame of a video sequence. As well, we propose a new action inference graph (AIG) that enables early recognition. Hence, the proposed model can recognize an action early before using all the keyframes or the whole video sequence and thus, requires less computation for recognizing human actions. Moreover, our OTCM can perform online action detection. To the best of our knowledge, this is the first time that the OTCM model along with the AIG is proposed. The experimental results of the benchmark datasets show that the proposed OTCM model has achieved and set a new record of the SOTA results, in particular, without using full video sequences.}
}
@article{DADSETAN2022108919,
title = {Deep learning of longitudinal mammogram examinations for breast cancer risk prediction},
journal = {Pattern Recognition},
volume = {132},
pages = {108919},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108919},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004009},
author = {Saba Dadsetan and Dooman Arefan and Wendie A. Berg and Margarita L. Zuley and Jules H. Sumkin and Shandong Wu},
keywords = {Breast cancer, Risk prediction, Deep learning, Digital mammogram, Longitudinal data},
abstract = {Information in digital mammogram images has been shown to be associated with the risk of developing breast cancer. Longitudinal breast cancer screening mammogram examinations may carry spatiotemporal information that can enhance breast cancer risk prediction. No deep learning models have been designed to capture such spatiotemporal information over multiple examinations to predict the risk. In this study, we propose a novel deep learning structure, LRP-NET, to capture the spatiotemporal changes of breast tissue over multiple negative/benign screening mammogram examinations to predict near-term breast cancer risk in a case-control setting. Specifically, LRP-NET is designed based on clinical knowledge to capture the imaging changes of bilateral breast tissue over four sequential mammogram examinations. We evaluate our proposed model with two ablation studies and compare it to three models/settings, including 1) a “loose” model without explicitly capturing the spatiotemporal changes over longitudinal examinations, 2) LRP-NET but using a varying number (i.e., 1 and 3) of sequential examinations, and 3) a previous model that uses only a single mammogram examination. On a case-control cohort of 200 patients, each with four examinations, our experiments on a total of 3200 images show that the LRP-NET model outperforms the compared models/settings.}
}
@article{YANG2022108910,
title = {Tree-based data augmentation and mutual learning for offline handwritten mathematical expression recognition},
journal = {Pattern Recognition},
volume = {132},
pages = {108910},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108910},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003910},
author = {Chen Yang and Jun Du and Jianshu Zhang and Changjie Wu and Mingjun Chen and JiaJia Wu},
keywords = {Tree-based data augmentation, Tree-based mutual learning, Encoder-decoder, Offline handwritten mathematical expression recognition},
abstract = {Recently, thanks to the successful application of the attention-based encoder-decoder framework, handwritten mathematical expression recognition (HMER) has achieved significant improvement. However, HMER is still a challenging task in the handwriting recognition area, which suffers from the ambiguity of handwritten symbols, the two-dimensional structure of mathematical expressions, and the lack of labeled data. In this paper, we attempt to improve the recognition performance and generalization ability of the existing state-of-the-art method from two perspectives: data augmentation and model design. We first propose a tree-based multi-level (including symbol level, sub-expression level, and image level) data augmentation strategy, which can generate many synthetic images. Then, we present a novel encoder-decoder hybrid model via tree-based mutual learning to fully utilize the complementarity between tree decoder and string decoder. Benefitting from our data augmentation strategy, we achieve 58.47%/57.82%/62.67% and 74.45% expression recognition accuracy respectively on the CROHME14/16/19 competition datasets and the OffRaSHME20 competition dataset. Moreover, tree-based data augmentation is a key technology to our champion system for the OffRaSHME20 competition. Our tree-based mutual learning method further improves the recognition accuracy to 61.63%/59.81%/64.38% and 75.68% on these datasets. Further quantitative and qualitative analyses also demonstrate the effectiveness and robustness of our proposed methods.}
}
@article{CHEN2022108980,
title = {CAAN: Context-Aware attention network for visual question answering},
journal = {Pattern Recognition},
volume = {132},
pages = {108980},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108980},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004605},
author = {Chongqing Chen and Dezhi Han and Chin-Chen Chang},
keywords = {Visual question answering, Attention mechanism, Understanding bias, Absolute position, Contextual information},
abstract = {Understanding multimodal information is the key to visual question answering (VQA) tasks. Most existing approaches use attention mechanisms to acquire fine-grained information understanding. However, these approaches with merely attention mechanisms do not solve the potential understanding bias problem. Hence, this paper introduces contextual information into VQA for the first time and presents a context-aware attention network (CAAN) to tackle the case. By improving the modular co-attention network (MCAN) framework, CAAN’s main work includes: designing a novel absolute position calculation method based on the coordinates of each image region in the image and the image’s actual size, the position information of all image regions are integrated as contextual information to enhance the visual representation; based on the question itself, several internal contextual information representations are introduced to participate in the modeling of the question words, solving the understanding bias caused by the similarity of the question. Additionally, we also designed two models of different scales, namely CAAN-base and CAAN-large, to explore the effect of the field of view on interaction. Finally, extensive experimental results show that CAAN significantly outperforms MCAN and achieves comparable or even better performance than other state-of-the-art approaches, proving our method can tackle the understanding bias.}
}
@article{HU2022108906,
title = {Feature Nonlinear Transformation Non-Negative Matrix Factorization with Kullback-Leibler Divergence},
journal = {Pattern Recognition},
volume = {132},
pages = {108906},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108906},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003879},
author = {Lirui Hu and Ning Wu and Xiao Li},
keywords = {Non-negative matrix factorization, Nonlinear transformation, Feature extraction, Object recognition, Clustering, Kullback-Leibler divergence},
abstract = {This paper introduces a Feature Nonlinear Transformation Non-Negative Matrix Factorization with Kullback-Leibler Divergence (FNTNMF-KLD) for extracting the nonlinear features of a matrix in standard NMF. This method uses a nonlinear transformation to act on the feature matrix for constructing a NMF model based on the objective function of Kullback-Leibler Divergence, and the Taylor series expansion and the Newton iteration formula of solving root are used to obtain the iterative update rules of the basis matrix and the feature matrix. Experimental results show that the proposed method obtains the nonlinear features of data matrix in a more efficient way. In object recognition and clustering tasks, better accuracy can be achieved over some typical NMF methods.}
}
@article{LI2022108914,
title = {Weighted 3D volume reconstruction from series of slice data using a modified Allen–Cahn equation},
journal = {Pattern Recognition},
volume = {132},
pages = {108914},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108914},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003958},
author = {Yibao Li and Xin Song and Soobin Kwak and Junseok Kim},
keywords = {Shape transformation, 3D volume reconstruction, Allen–Cahn equation},
abstract = {In this study, we develop a fast and accurate computational method for a weighted three-dimensional (3D) volume reconstruction from a series of slice data using a phase-field model. The proposed method is based on a modified Allen–Cahn (AC) equation with a fidelity term. The algorithm automatically generates the necessary slices between the given slices by solving the governing equation. To reconstruct a 3D volume, we first set a source slice and target slice. Next, we set the source slice as the initial condition and the target slice as the fidelity function. Finally, we retain the numerical solutions during an evolution as intermediate slices between the source and target slices. There are two criteria for choosing the intermediate slice: One is based on the area of the symmetric difference between the phase-field solution and the target and the other is based on the change of the phase-field solution relative to the area of the target. We use the weighted average of the two criteria. To validate the efficiency and accuracy of the proposed numerical algorithm, several computational experiments are conducted. Computational test results confirm the superior performance of the proposed algorithm.}
}
@article{LUO2022108901,
title = {Scale-selective and noise-robust extended local binary pattern for texture classification},
journal = {Pattern Recognition},
volume = {132},
pages = {108901},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108901},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200382X},
author = {Qiwu Luo and Jiaojiao Su and Chunhua Yang and Olli Silven and Li Liu},
keywords = {Local binary pattern (LBP), Texture descriptor, Feature extraction, Texture classification},
abstract = {As one of the most successful local feature descriptors, the local binary pattern (LBP) estimates the texture distribution rule of an image based on the signs of differences between neighboring pixels to obtain intensity- and rotation- invariance. In this paper, we propose a novel image descriptor to address scale transformation and noise interference simultaneously. We name it scale-selective and noise-robust extended LBP (SNELBP). First, each image in training sets is transformed into different scale spaces by a Gaussian filter. Second, noise-robust pattern histograms are obtained from each scale space by using our previously proposed median robust extended LBP (MRELBP). Then, scale-invariant histograms are determined by selecting the maximum among all scale levels for a certain image. Finally, the most informative patterns are selected from the dictionary pretrained by the two-stage compact dominant feature selection method (CDFS), maintaining the descriptor more lightweight with sufficiently low time cost. Extensive experiments on five public databases (Outex_TC_00011, TC_00012, KTH-TIPS, UMD and NEU) and one fresh texture database (JoJo) under two kinds of interferences (Gaussian and salt pepper) indicate that our SNELBP yields more competitive results than thirty classical LPB variants as well as eight typical deep learning methods.}
}