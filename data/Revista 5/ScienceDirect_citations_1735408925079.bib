@article{LI2023109024,
title = {SaberNet: Self-attention based effective relation network for few-shot learning},
journal = {Pattern Recognition},
volume = {133},
pages = {109024},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109024},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005040},
author = {Zijun Li and Zhengping Hu and Weiwei Luo and Xiao Hu},
keywords = {Few-shot learning, Feature representation, Task analysis, Transformers},
abstract = {Few-shot learning is an essential and challenging field in machine learning since the agent needs to learn novel concepts with a few data. Recent methods aim to learn comparison or relation between query and support samples to tackle few-shot tasks but have not exceeded human performance and made full use of relations in few-shot tasks. Humans can recognize multiple variants of objects located anywhere in images and compare the relation among learned instances. Inspired by the human learning mechanism, we explore the definition of relations in relation networks and propose self-attention relation modules for feature and learning ability. First, we introduce vision self-attention to generate and purify features in few-shot learning. The comparison of different patches leads the backbone to infer relations between local features, which enforces feature extraction focus on more details. Second, we propose task-specific feature augmentation modules to infer relations and weight different contributions of components in few-shot tasks. The proposed SaberNet is conceptually simple and empirically powerful. Its performance surpasses the baseline a great margin, including pushing 5-way 1-shot CUB accuracy to 89.75% (12.73% absolute improvement), Cars to 76.71% (12.99% absolute improvement) and Flowers to 84.33% (7.67% absolute improvement).}
}
@article{HE2023109028,
title = {Temporal sparse adversarial attack on sequence-based gait recognition},
journal = {Pattern Recognition},
volume = {133},
pages = {109028},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109028},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005088},
author = {Ziwen He and Wei Wang and Jing Dong and Tieniu Tan},
keywords = {Adversarial attack, Gait recognition, Temporal sparsity},
abstract = {Gait recognition is widely used in social security applications due to its advantages in long-distance human identification. Recently, sequence-based methods have achieved high accuracy by learning abundant temporal and spatial information. However, their robustness under adversarial attacks in an open world has not been clearly explored. In this paper, we demonstrate that the state-of-the-art gait recognition model is vulnerable to such attacks. To this end, we propose a novel temporal sparse adversarial attack method. Different from previous additive noise models which add perturbations on original samples, we employ a generative adversarial network based architecture to semantically generate adversarial high-quality gait silhouettes or video frames. Moreover, by sparsely substituting or inserting a few adversarial gait silhouettes, the proposed method ensures its imperceptibility and achieves a strong attack ability. The experimental results show that if only one-fortieth of the frames are attacked, the accuracy of the target model drops dramatically.}
}
@article{ZHAO2023109056,
title = {Continuous label distribution learning},
journal = {Pattern Recognition},
volume = {133},
pages = {109056},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109056},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005362},
author = {Xingyu Zhao and Yuexuan An and Ning Xu and Xin Geng},
keywords = {Label distribution learning, Continuous label distribution, Label ambiguity, Label encoding, Label correlations},
abstract = {Label distribution learning (LDL) is a suitable paradigm to deal with label ambiguity through learning the correlations among different labels. Most existing label distribution learning methods consider the labels to be discrete and directly establish the mapping from features to labels. However, in many real-world applications, labels naturally form a continuous distribution, which is ignored by the existing methods. As a result, the distribution information of labels can not be accurately described and finally affects the whole learning system. The goal of this paper is to propose a novel approach which can capture the continuous distribution of different labels explicitly and effectively. Specifically, we propose Continuous Label Distribution Learning (CLDL) which describes labels as a continuous density function and learns the distribution information of the labels in the latent space. In this way, the high-order correlations among different labels can be effectively extracted and only a few parameters for describing the continuous distribution need to be learned. Extensive description degree prediction experiments on real-world datasets validate the superiority of CLDL over the existing approaches.}
}
@article{LI2023109044,
title = {Rethinking referring relationships from a perspective of mask-level relational reasoning},
journal = {Pattern Recognition},
volume = {133},
pages = {109044},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109044},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005246},
author = {Chengyang Li and Liping Zhu and Gangyi Tian and Yi Hou and Heng Zhou},
keywords = {Referring relationship, Multimodal learning, Image and text, Visual grounding, Deep learning},
abstract = {Referring relationship aims at localizing subject and object entities in an image, according to a triple text <subject, predicate, object>. Previous methods use iterative attention to shift between image regions for modeling predicate. However, predicate sometimes is implicit and difficult to be represented in the image domain. Convolution modeling method to express predicate is simple and inappropriate. Besides, relational reasoning information in the text itself is not fully utilized. To this end, we rethink referring relationship from a mask-level relational reasoning perspective to improve model interpretability. For text-to-image reasoning, we design Mask Generate and Mask Transfer modules, so as to fully integrate the text priors into the reasoning and prediction of masks. For image-to-text reasoning, we propose an unsupervised triple reconstruction method to guide text-to-image reasoning and improve multimodal generalization. By bi-directional reasoning between image and text, the proposed method MRR fully conforms to the multimodal relational reasoning process. Experiments show that MRR achieves state-of-the-art performance on two datasets of referring relationships, VRD and Visual Genome.}
}
@article{SOHRAB2023108999,
title = {Graph-embedded subspace support vector data description},
journal = {Pattern Recognition},
volume = {133},
pages = {108999},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108999},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004794},
author = {Fahad Sohrab and Alexandros Iosifidis and Moncef Gabbouj and Jenni Raitoharju},
keywords = {One-Class classification, Support vector data description, Subspace learning, Spectral regression},
abstract = {In this paper, we propose a novel subspace learning framework for one-class classification. The proposed framework presents the problem in the form of graph embedding. It includes the previously proposed subspace one-class techniques as its special cases and provides further insight on what these techniques actually optimize. The framework allows to incorporate other meaningful optimization goals via the graph preserving criterion and reveals a spectral solution and a spectral regression-based solution as alternatives to the previously used gradient-based technique. We combine the subspace learning framework iteratively with Support Vector Data Description applied in the subspace to formulate Graph-Embedded Subspace Support Vector Data Description. We experimentally analyzed the performance of newly proposed different variants. We demonstrate improved performance against the baselines and the recently proposed subspace learning methods for one-class classification.}
}
@article{BICEGO2023109036,
title = {DisRFC: a dissimilarity-based Random Forest Clustering approach},
journal = {Pattern Recognition},
volume = {133},
pages = {109036},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109036},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005167},
author = {Manuele Bicego},
keywords = {Random forests clustering, Dissimilarities, Unsupervised learning, Clustering, Non-vectorial representation},
abstract = {In this paper we present a novel Random Forest Clustering approach, called Dissimilarity Random Forest Clustering (DisRFC), which requires in input only pairwise dissimilarities. Thanks to this characteristic, the proposed approach is appliable to all those problems which involve non-vectorial representations, such as strings, sequences, graphs or 3D structures. In the proposed approach, we first train an Unsupervised Dissimilarity Random Forest (UD-RF), a novel variant of Random Forest which is completely unsupervised and based on dissimilarities. Then, we exploit the trained UD-RF to project the patterns to be clustered in a binary vectorial space, where the clustering is finally derived using fast and effective K-means procedures. In the paper we introduce different variants of DisRFC, which are thoroughly and positively evaluated on 12 different problems, also in comparison with alternative state-of-the-art approaches.}
}
@article{ESKANDARI2023109007,
title = {Online and offline streaming feature selection methods with bat algorithm for redundancy analysis},
journal = {Pattern Recognition},
volume = {133},
pages = {109007},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109007},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004873},
author = {S. Eskandari and M. Seifaddini},
keywords = {Feature selection, Online feature selection, Streamwise feature selection, Dimension reduction, Bat algorithm},
abstract = {Streaming feature selection (SFS), is the task of selecting the most informative features in dealing with high-dimensional or incrementally growing problems. Several SFS algorithms have been proposed in the literature. However, they do not consider all feature subsets at the redundancy analysis step due to computational concerns. Moreover, they do not reconsider previously removed features which leads to losing most of the useful information. In this paper, the redundancy analysis step is defined as a binary optimization problem. Then, a binary bat algorithm (BBA) is adopted to find the minimal informative subsets. In this way, a large number of feature subsets can be considered effectively at the redundancy analysis step. In addition, an effective priority list is used to maintain previously removed redundant features. Such a list allows the re-examination of informative features. As a result, it is possible to consider the mutual information between features that are not streamed in an small time interval. Experimental studies on fifteen different types of datasets show that our approach is superior to state-of-the-art online and offline streaming feature selection methods in terms of classification accuracy.}
}
@article{ZHANG2023109020,
title = {Pyramid Geometric Consistency Learning For Semantic Segmentation},
journal = {Pattern Recognition},
volume = {133},
pages = {109020},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109020},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005003},
author = {Xian Zhang and Qiang Li and Zhibin Quan and Wankou Yang},
keywords = {Semantic segmentation, Consistency learning, Supervised contrastive learning},
abstract = {Semantic segmentation is a critical in vision fields. Randomly transforms each image into different augmented samples and supervise the views with transformed semantics labels. However, even if the views are expanded from the same sample, the prediction results obtained by the same network will be very different. Therefore, we argue that between the augmented samples, the transformation-equivariance and the representational consistency also need to be supervised. Motivated by this, we propose a simple cross-data augmentation for semantic segmentation, in which we also leverage the pixel-level consistency constraint learning between pairs of augmented samples. As a result, our scheme significantly can improve the performances of existing semantic segmentation models without additional computation overhead. We verified the effectiveness of this method on Deeplab V3 Plus. Experiments show that our method can achieve stable performance improvement on mainstream data sets such as Pascal VOC 2012, Camvid, Cityscapes, etc.}
}
@article{SONG2023109015,
title = {Answering knowledge-based visual questions via the exploration of Question Purpose},
journal = {Pattern Recognition},
volume = {133},
pages = {109015},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109015},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004952},
author = {Lingyun Song and Jianao Li and Jun Liu and Yang Yang and Xuequn Shang and Mingxuan Sun},
keywords = {Visual question answering, DNN, Question Purpose},
abstract = {Visual question answering has been greatly advanced by deep learning technologies, but still remains an open problem subjected to two aspects of factors. First, previous works estimate the correctness of each candidate answer mainly by its semantic correlations with visual questions, overlooking the fact that some questions and their answers are semantically inconsistent. Second, previous works that require external knowledge mainly uses the knowledge facts retrieved by key words or visual objects. However, the retrieved knowledge facts may only be related to the semantics of the question, but are useless or even misleading for answer prediction. To address these issues, we investigate how to capture the purpose of visual questions and propose a Purpose Guided Visual Question Answering model, called PGVQA. It mainly has two appealing properties: (1) It can estimate the correctness of candidate answers based on the Question Purpose (QP) that reveals which aspects of the concept are examined by visual questions. This is helpful for avoiding the negative effect of the semantic inconsistency between answers and questions. (2) It can incorporate the knowledge facts accordant with the QP into answer prediction, which helps to improve the probability of answering visual questions correctly. Empirical studies on benchmark datasets show that PGVQA achieves state-of-the-art performance.}
}
@article{LI2023108979,
title = {Adaptive momentum variance for attention-guided sparse adversarial attacks},
journal = {Pattern Recognition},
volume = {133},
pages = {108979},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108979},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004599},
author = {Chao Li and Wen Yao and Handing Wang and Tingsong Jiang},
keywords = {Deep neural networks, Black-box adversarial attacks, Transferability, Momentum variances},
abstract = {The phenomenon that deep neural networks are vulnerable to adversarial examples has been found for several years. Under the black-box setting, transfer-based methods usually produce the adversarial examples on a white-box model, which serves as the surrogate model in the black-box attack, and hope that the same adversarial examples can also fool the black-box model. However, these methods have high success rates for the surrogate model and exhibit weak transferability for the black-box model. In addition, some studies have shown that deep neural networks are also vulnerable to sparse alterations of the input, but existing sparse attacks mainly focus on the number of attacked pixels without restricting the size of the perturbations, which is perceptible to human eyes. To address the above problems, we propose a transfer-based sparse attack method, called adaptive momentum variance based iterative gradient method with a class activation map, where the method considers a simple adaptive momentum variance and a refining perturbation mechanism to improve the transferability of adversarial examples. Also, a class activation map, which is also known as attention mechanism, is employed to explore the relationship between the number of the perturbed pixels and the attack performance in the case of limiting the intensity of perturbation. The proposed method is compared with a number of the state-of-the-art transfer-based adversarial attack methods on the ImageNet dataset, and the empirical results demonstrate that our method achieves a significant increase in transferability with only attacking about 50% of the pixels.}
}
@article{FAN2023108971,
title = {A landmark-free approach for automatic, dense and robust correspondence of 3D faces},
journal = {Pattern Recognition},
volume = {133},
pages = {108971},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108971},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004514},
author = {Zhenfeng Fan and Xiyuan Hu and Chen Chen and Xiaolian Wang and Silong Peng},
keywords = {3D face, Dense correspondence, Non-rigid registration},
abstract = {Global dense registration of 3D faces commonly prioritizes correspondences of facial landmarks which are fiducial points for the anatomical structures. However, it is not always easy to pre-annotate the landmarks accurately in raw scans of 3D faces. Contrary to the current state-of-the-art in dense 3D face correspondence, we propose a general framework without pre-annotated landmarks, which promotes its robustness and allows the meshes to deform in a uniform manner. The proposed framework includes two stages: first the correspondences are established using a template face; and then we select some well-reconstructed samples to build a prior model and leverage it into the correspondence process of other samples. In both stages, the dense registration is revisited in two perspectives: semantic and topological correspondence. In the latter stage, we further incorporate shape and normal statistics of 3D faces to regularize the correspondence process for more robust results. This provides a feasible way to handle data with noises and occlusions, as well as large deformation caused by facial expressions. Our basic idea is to gradually refine the correspondence of individual points in a way global-to-local. At the same time, we solve the local-to-global deformation based on the refined correspondences. The two processes are alternated, and aided by some confidence checks for each individual points. In the experiments, the proposed method is evaluated both qualitatively and quantitatively on three datasets including two publicly available ones: FRGC v2.0 and BU-3DFE datasets, demonstrating its effectiveness.}
}
@article{WANG2023108987,
title = {Better pseudo-label: Joint domain-aware label and dual-classifier for semi-supervised domain generalization},
journal = {Pattern Recognition},
volume = {133},
pages = {108987},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108987},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004678},
author = {Ruiqi Wang and Lei Qi and Yinghuan Shi and Yang Gao},
keywords = {Semi-supervised learning, Domain generalization, Image recognition, Feature representation},
abstract = {With the goal of directly generalizing trained model to unseen target domains, domain generalization (DG), a newly proposed learning paradigm, has attracted considerable attention. Previous DG models usually require a sufficient quantity of annotated samples from observed source domains during training. In this paper, we relax this requirement about full annotation and investigate semi-supervised domain generalization (SSDG) where only one source domain is fully annotated along with the other domains totally unlabeled in the training process. With the challenges of tackling the domain gap between observed source domains and predicting unseen target domains, we propose a novel deep framework via joint domain-aware labels and dual-classifier to produce high-quality pseudo-labels. Concretely, to predict accurate pseudo-labels under domain shift, a domain-aware pseudo-labeling module is developed. Also, considering inconsistent goals between generalization and pseudo-labeling: former prevents overfitting on all source domains while latter might overfit the unlabeled source domains for high accuracy, we employ a dual-classifier to independently perform pseudo-labeling and domain generalization in the training process. When accurate pseudo-labels are generated for unlabeled source domains, the domain mixup operation is applied to augment new domains between labeled and unlabeled domains, which is beneficial for boosting the generalization capability of the model. Extensive results on publicly available DG benchmark datasets show the efficacy of our proposed SSDG method.}
}
@article{SONG2023108995,
title = {Decoupling multi-task causality for improved skin lesion segmentation and classification},
journal = {Pattern Recognition},
volume = {133},
pages = {108995},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108995},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004757},
author = {Lei Song and Haoqian Wang and Z. Jane Wang},
keywords = {Skin lesion analysis, Multi-task decoupled, Deep learning, Task causality},
abstract = {Multi-task learning has been used widely in many computer aided diagnosis applications recently, while the trade-off between different tasks remains challenging. Also, the inherent causality is less studied. In this paper, we focus on skin lesion analysis, including lesion classification, detection and segmentation. By defining the chain relationship (i.e., lesion detection boosts contour segmentation, and segmentation boosts lesion classification in turn), and further decoupling each pair-wise causality (e.g., detection to segmentation) from the Pareto efficiency view, we can solve the common trade-off issue between multi-task. On this basis, we propose a novel paradigm to improve the skin lesion segmentation and classification separately, and favourable feature fusion ways for each task are explored. Moreover, to address the huge model size problem, we design an effective model compression scheme (MCS). Extensive experiments on the ISIC2017 and PH2 datasets are conducted to evaluate the proposed paradigm. The results demonstrate that the popular models such as ResNet, DenseNet and UNet for lesion analysis can be boosted by applying the proposed paradigm, and the designed MCS reduces the amount of model parameters efficiently. We achieve performance improvements on skin lesion segmentation and classification without strenuous network design and soaring model complexity. This proposed approach is promising for the multi-task diagnosis setting in other medical applications.}
}
@article{XIE2023108974,
title = {WITS: Weakly-supervised individual tooth segmentation model trained on box-level labels},
journal = {Pattern Recognition},
volume = {133},
pages = {108974},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108974},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200454X},
author = {Ruicheng Xie and Yunyun Yang and Zhaoyang Chen},
keywords = {Tooth detection, Deep learning, Active contour, Oral CBCT images, Level set},
abstract = {Accurately and automatically segmenting teeth from cone-beam computed tomography (CBCT) images plays an essential role in dental disease diagnosis and treatment. This paper presents an automatic tooth segmentation model that combines deep learning methods and level-set approaches. The proposed model uses a deep learning method to detect each tooth’s location and size and generates prior ellipses from those detected boundary boxes. Calculating each point’s signed distance to the prior edge and using them as prior weights, the restriction term can constrain the evolution of level set functions according to the distance to the prior ellipses. Then, we use the curvature direction to find out joint points of teeth and employ a variational model to separate them to get individual results. By quantitative evaluation, we show that the proposed model can accurately segment teeth. The performance is more accurate and stable than those of classical level-set models and deep-learning models. For example, the Dice coefficient is increased by 7% than that of the U-Net model. Besides, we will release the code on https://github.com/ruicx/Individual-Tooth-Segmentation-with-Rectangle-Labels.}
}
@article{PHUTKE2023109040,
title = {Image inpainting via spatial projections},
journal = {Pattern Recognition},
volume = {133},
pages = {109040},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109040},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005209},
author = {Shruti S Phutke and Subrahmanyam Murala},
keywords = {Spatial projections, Inpainting, Object removal},
abstract = {Image inpainting is now-a-days sought after due to its wide variety of applications in the reconstruction of the corrupted image, occlusion removal, reflection removal, etc. Existing image inpainting approaches utilize different types of attention mechanisms to inpaint the image and produce visibly admirable results. These methods are more concerned at weighing the feature maps of the hole region with some weight from the non-hole region. But, due to the lack of spatial contextual correlation in the attention maps, the inpainted image may suffer from the inconsistencies among hole and non-hole regions. Transformer-based inpainting methods give significant results by capturing the relationship between the patches with a compromise of high computational complexity. In this context, we propose a novel spatial projection layer (SPL) without any attention mechanism to project the spatial contextual information in the hole region from non-hole regions for producing a spatially plausible inpainted image. The SPL is proposed mainly to focus on the non-hole spatial information in the high-level feature maps for filling the hole regions efficiently. Also, while training the network, we propose the use of edge loss with a Canny edge operator for image inpainting to focus on the relevant edges instead of noise contents. Analysis with the extensive experiments, ablation, and user study on the proposed architecture demonstrates the superiority over existing state-of-the-art methods for image inpainting. The code is available at: https://github.com/shrutiphutke/spatial_projection_inpainting.}
}
@article{SAMBATURU2023109011,
title = {ScribbleNet: Efficient interactive annotation of urban city scenes for semantic segmentation},
journal = {Pattern Recognition},
volume = {133},
pages = {109011},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109011},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004915},
author = {Bhavani Sambaturu and Ashutosh Gupta and C.V. Jawahar and Chetan Arora},
abstract = {Annotation is a crucial first step in the semantic segmentation of urban images that facilitates the development of autonomous navigation systems. However, annotating complex urban images is time-consuming and challenging. It requires significant human effort making it expensive and error-prone. To reduce human effort during annotation, multiple images need to be annotated in a short time-span. In this paper, we introduce ScribbleNet, an interactive image segmentation algorithm to address this issue. Our approach provides users with a pre-segmented image that iteratively improves the segmentation using scribble as an annotation input. This method is based on conditional inference and exploits the learnt correlations in a deep neural network (DNN). ScribbleNet can: (1) work with urban city scenes captured in unseen environments, (2) annotate new classes not present in the training set, and (3) correct several labels at once. We compare this method with other interactive segmentation approaches on multiple datasets such as CityScapes, BDD, Mapillary Vistas, KITTI, and IDD. ScribbleNet reduces the annotation time of an image by up to 14.7 × over manual annotation and up to 5.4× over the current approaches. The algorithm is integrated into the publicly available LabelMe image annotation tool and will be released as an open-source software.}
}
@article{SU2023109047,
title = {From Distortion Manifold to Perceptual Quality: a Data Efficient Blind Image Quality Assessment Approach},
journal = {Pattern Recognition},
volume = {133},
pages = {109047},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109047},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005271},
author = {Shaolin Su and Qingsen Yan and Yu Zhu and Jinqiu Sun and Yanning Zhang},
keywords = {Image quality assessment, No-Reference, Generalizability, Distortion manifold},
abstract = {Though current no-reference image quality assessment (NR-IQA) approaches have achieved impressive performance gain thanks to deep learning techniques, it is claimed that the risk of over-fitting exists. To improve model generalization ability, most of the current researches incorporate mass data to train or tune the data-driven models. However, the process of image data collection and quality label annotation is quite time-consuming and labour-intensive. Therefore, in this paper, we explore an alternative solution to promote model generalizability but with relatively small fractions of training data. Compared with previous approaches which make effort to approximate the whole complex image distribution, we propose to explicitly learn an image distortion manifold first, which lies in a much lower dimension space and also representative in capturing general degradation patterns. We then project the images to their perceived quality from the learned manifold to obtain quality predictions. Since the manifold embeds general distortion features despite of varying image contents, it can be learned with relatively small amount of samples. In order to learn the manifold and quality projection, we introduce a two-branched network to learn both low level distortions and high level semantics. We also propose a simple but efficient training framework, composing of a masked labelling strategy and a gradual weighting curriculum to fulfill the task. Thanks to the learned distortion manifold, the proposed model achieves superior generalizability compared with previous models. Extensive experiments demonstrate its effectiveness in terms of training with limited data, testing on large scale images, and with unseen types of distorted images.}
}
@article{LIU2023109039,
title = {LAE-Net: A locally-adaptive embedding network for low-light image enhancement},
journal = {Pattern Recognition},
volume = {133},
pages = {109039},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109039},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005192},
author = {Xiaokai Liu and Weihao Ma and Xiaorui Ma and Jie Wang},
keywords = {Locally-adaptive, Image enhancement, Multi-distribution, Image entropy, Kernel selection},
abstract = {In the low-light enhancement task, one of the major challenges lies in how to balance the image enhancement properties of light intensity, detail presentation and color fidelity. In natural scenes, the multi-distribution of frequency and illumination characteristics in the spatial domain makes the balance more difficult. To solve this problem, we propose a Locally-Adaptive Embedding Network, namely LAE-Net, to realize high-quality low-light image enhancement with locally-adaptive kernel selection and feature adaptation for multi-distribution issues. Specifically, for the frequency multi-distribution, we rethink the spatial-frequency characteristic of human eyes, experimentally explore the relationship among the receptive field size, the image spatial frequency and the light enhancement properties, and propose an Entropy-Inspired Kernel-Selection Convolution, where each neuron can adaptively adjust the receptive field size according to its spatial frequency characterized by information entropy. For the illumination multi-distribution, we propose an Illumination Attentive Transfer subnet, where the neurons can simultaneously sense global consistency and local details, and accordingly hint where to focus the efforts on, thereby adjusting the refined features. Extensive experiments with ablation analysis show the effectiveness of our method and the proposed method outperforms many related state-of-the-art techniques on four benchmark datasets: MEF, LIME, NPE and DICM.}
}
@article{HOU2023109035,
title = {Game-theoretic hypergraph matching with density enhancement},
journal = {Pattern Recognition},
volume = {133},
pages = {109035},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109035},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005155},
author = {Jian Hou and Huaqiang Yuan and Marcello Pelillo},
keywords = {Feature matching, Hypergraph matching, Game-theoretic, Density enhancement},
abstract = {Feature matching plays a fundamental role in computer vision and pattern recognition. As straightforward comparison of feature descriptors is not enough to provide reliable matching results in many situations, graph matching makes use of the pairwise relationship between features to improve matching accuracy. Hypergraph matching further employs the relationship among multiple features to provide more invariance between feature correspondences. Existing hypergraph matching algorithms usually solve an assignment problem, where outliers may result in a large number of false matches. In this paper we cast the hypergraph matching problem as a non-cooperative multi-player game, and obtain the matches by extracting the evolutionary stable strategies. Our algorithm exerts a strong constraint on the consistency of obtained matches, and false matches are excluded effectively. In order to increase the number of matches without increasing the computation load evidently, we present a density enhancement method to improve the matching results. We further propose two methods to enforce the one-to-one constraint, thereby removing false matches and maintaining a high matching accuracy. Experiments with both synthetic and real datasets validate the effectiveness of our algorithm.}
}
@article{CHOI2023109055,
title = {Disentangling the correlated continuous and discrete generative factors of data},
journal = {Pattern Recognition},
volume = {133},
pages = {109055},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109055},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005350},
author = {Jaewoong Choi and Geonho Hwang and Myungjoo Kang},
keywords = {Variational autoencoder, Disentanglement, Generative model, Representation learning},
abstract = {Real-world data typically include discrete generative factors, such as category labels and the existence of objects, as well as continuous generative factors. Continuous generative factors may be dependent on or independent of discrete generative factors. For instance, an intra-class variation of a category is dependent on the discrete generative factor, whereas a common variation of all categories is not. Most previous attempts to integrate discrete generative factors into disentanglement assumed statistical independence between the continuous and discrete variables. In this paper, we propose a Variational Autoencoder(VAE) model capable of disentangling both continuous generative factors. To represent these generative factors, we introduce two sets of continuous latent variables: a private variable and a public variable. The private and public variables represent the intra-class variations and common variations in categories, respectively. Our proposed framework models the private variable as a Gaussian mixture and the public variable as a Gaussian. Each mode of the private variable is responsible for a class of discrete variables. Our proposed model, called Discond-VAE, DISentangles the class-dependent CONtinuous factors from the Discrete factors by introducing private variables. The experiments showed that Discond-VAE could discover private and public factors from the data. Moreover, even under the dataset with only public factors, Discond-VAE does not fail and adapts private variables to represent public factors.}
}
@article{ZHANG2023109027,
title = {OW-TAL: Learning Unknown Human Activities for Open-World Temporal Action Localization},
journal = {Pattern Recognition},
volume = {133},
pages = {109027},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109027},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005076},
author = {Yaru Zhang and Xiao-Yu Zhang and Haichao Shi},
keywords = {Temporal action localization, Open-world learning, Self-paced learning},
abstract = {Current temporal action localization methods work well on a closed-world assumption, in which all action categories to be localized are known as a priori. However, this assumption doesn’t apply to open-world scenarios, as novel categories that never appeared in the training stage will be encountered without explicit supervision. Distinct from the closed-world setting, localizing actions under the open-world setup poses two significant challenges: 1) identifying unknown actions from diverse knowns and localizing their temporal boundaries. 2) defying forgetting of previous actions when incrementally updating knowledge of identified unknown actions. To address the aforementioned challenges, we develop a two-branch framework with Unknown and Known action modeling Networks, a.k.a. UK-Net, for the problem of Open-World Temporal Action Localization (OW-TAL). The potential patterns underlying unknown and known actions, as well as their dynamic transformation, are modeled in a unified pipeline. Specifically, a self-attention based position-sensitive module is designed to produce actionness scores for unknown actions in a class-agnostic way. Besides, an iterative optimization strategy is developed to enable knowledge derived from known categories to be shared with the unknowns. In addition, a self-paced learning strategy is proposed to instructionally guide class-incremental learning while defying catastrophic forgetting. Benefiting from the above components, our UK-Net yields superior performance on three challenging datasets, i.e., THUMOS14, ActivityNet1.2, and MUSES. Experimental results also demonstrate the competitive performance of our method when compared with traditional closed-world counterparts.}
}
@article{FEI2023109051,
title = {DcTr: Noise-robust point cloud completion by dual-channel transformer with cross-attention},
journal = {Pattern Recognition},
volume = {133},
pages = {109051},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109051},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005313},
author = {Ben Fei and Weidong Yang and Lipeng Ma and Wen-Ming Chen},
keywords = {Point cloud, 3D Vision, Transformer, Cross-attention, Dual-channel transformer},
abstract = {Current point cloud completion research mainly utilizes the global shape representation and local features to recover the missing regions of 3D shape for the partial point cloud. However, these methods suffer from inefficient utilization of local features and unstructured points prediction in local patches, hardly resulting in a well-arranged structure for points. To tackle these problems, we propose to employ Dual-channel Transformer and Cross-attention (CA) for point cloud completion (DcTr). The DcTr is apt at using local features and preserving a well-structured generation process. Specifically, the dual-channel transformer leverages point-wise attention and channel-wise attention to summarize the deconvolution patterns used in the previous Dual-channel Transformer Point Deconvolution (DCTPD) stage to produce the deconvolution in the current DCTPD stage. Meanwhile, we employ cross-attention to convey the geometric information from the local regions of incomplete point clouds for the generation of complete ones at different resolutions. In this way, we can generate the locally compact and structured point cloud by capturing the structure characteristic of 3D shape in local patches. Our experimental results indicate that DcTr outperforms the state-of-the-art point cloud completion methods under several benchmarks and is robust to various kinds of noise.}
}
@article{BOSQUET2023108998,
title = {A full data augmentation pipeline for small object detection based on generative adversarial networks},
journal = {Pattern Recognition},
volume = {133},
pages = {108998},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108998},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004782},
author = {Brais Bosquet and Daniel Cores and Lorenzo Seidenari and Víctor M. Brea and Manuel Mucientes and Alberto Del Bimbo},
keywords = {Small object detection, Data augmentation, Generative adversarial network},
abstract = {Object detection accuracy on small objects, i.e., objects under 32 × 32 pixels, lags behind that of large ones. To address this issue, innovative architectures have been designed and new datasets have been released. Still, the number of small objects in many datasets does not suffice for training. The advent of the generative adversarial networks (GANs) opens up a new data augmentation possibility for training architectures without the costly task of annotating huge datasets for small objects. In this paper, we propose a full pipeline for data augmentation for small object detection which combines a GAN-based object generator with techniques of object segmentation, image inpainting, and image blending to achieve high-quality synthetic data. The main component of our pipeline is DS-GAN, a novel GAN-based architecture that generates realistic small objects from larger ones. Experimental results show that our overall data augmentation method improves the performance of state-of-the-art models up to 11.9% APs@.5 on UAVDT and by 4.7% APs@.5 on iSAID, both for the small objects subset and for a scenario where the number of training instances is limited.}
}
@article{MA2023109006,
title = {Robust Table Detection and Structure Recognition from Heterogeneous Document Images},
journal = {Pattern Recognition},
volume = {133},
pages = {109006},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109006},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004861},
author = {Chixiang Ma and Weihong Lin and Lei Sun and Qiang Huo},
keywords = {Table detection, Table structure recognition, Corner detection, Spatial CNN, Grid CNN, Split-and-merge},
abstract = {We introduce a new table detection and structure recognition approach named RobusTabNet to detect the boundaries of tables and reconstruct the cellular structure of each table from heterogeneous document images. For table detection, we propose to use CornerNet as a new region proposal network to generate higher quality table proposals for Faster R-CNN, which has significantly improved the localization accuracy of Faster R-CNN for table detection. Consequently, our table detection approach achieves state-of-the-art performance on three public table detection benchmarks, namely cTDaR TrackA, PubLayNet and IIIT-AR-13K, by only using a lightweight ResNet-18 backbone network. Furthermore, we propose a new split-and-merge based table structure recognition approach, in which a novel spatial CNN based separation line prediction module is proposed to split each detected table into a grid of cells, and a Grid CNN based cell merging module is applied to recover the spanning cells. As the spatial CNN module can effectively propagate contextual information across the whole table image, our table structure recognizer can robustly recognize tables with large blank spaces and geometrically distorted (even curved) tables. Thanks to these two techniques, our table structure recognition approach achieves state-of-the-art performance on three public benchmarks, including SciTSR, PubTabNet and cTDaR TrackB2-Modern. Moreover, we have further demonstrated the advantages of our approach in recognizing tables with complex structures, large blank spaces, as well as geometrically distorted or even curved shapes on a more challenging in-house dataset.}
}
@article{QASIMABBAS2023109031,
title = {Transformed domain convolutional neural network for Alzheimer's disease diagnosis using structural MRI},
journal = {Pattern Recognition},
volume = {133},
pages = {109031},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109031},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005118},
author = {S. {Qasim Abbas} and Lianhua Chi and Yi-Ping Phoebe Chen},
keywords = {Alzheimer disease (AD) detection, Brain disease, Convolutional neural network (CNN), Supervised learning, Structural magnetic resonance imaging (sMRI), Transform domain AD classification, AD diagnosis},
abstract = {Structural magnetic resonance imaging (sMRI) has become a prevalent and potent imaging modality for the computer-aided diagnosis (CAD) of neurological diseases like dementia. Recently, a handful of deep learning techniques such as convolutional neural networks (CNNs) have been proposed to diagnose Alzheimer's disease (AD) by learning the atrophy patterns available in sMRIs. Although CNN-based techniques have demonstrated superior performance and characteristics compared to conventional learning-based classifiers, their diagnostic performance still needs to be improved for reliable classification results. The drawback of current CNN-based approaches is the requirement to locate discriminative landmark (LM) locations by identifying regions of interest (ROIs) in sMRIs, thus the performance of the whole framework is highly influenced by the LM detection step. To overcome this issue, we propose a novel three-dimensional Jacobian domain convolutional neural network (JD-CNN) to diagnose AD subjects and achieve excellent classification performance without the involvement of the LM detection framework. We train the proposed JD-CNN model on the basis of features generated by transforming the sMRI from the spatial domain to the Jacobian domain. The proposed JD-CNN is evaluated on baseline T1-weighted sMRI data collected from 154 healthy control (HC) and 84 Alzheimer's disease (AD) subjects in the Alzheimer's disease neuroimaging initiative (ADNI) database. The proposed JD-CNN exhibits superior classification performance to previously reported state-of-the-art techniques.}
}
@article{CHEN2023108986,
title = {Few-shot learning with unsupervised part discovery and part-aligned similarity},
journal = {Pattern Recognition},
volume = {133},
pages = {108986},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108986},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004666},
author = {Wentao Chen and Zhang Zhang and Wei Wang and Liang Wang and Zilei Wang and Tieniu Tan},
keywords = {Few-shot learning, Self-supervised learning, Part discovery network, Part-aligned similarity},
abstract = {Few-shot learning aims to recognize novel concepts with only a few examples. To this end, previous studies resort to acquiring a strong inductive bias via meta-learning on a group of similar tasks, which however needs a large labeled base dataset to sample training tasks. In this paper, we show that such inductive bias can be learned from a flat collection of unlabeled images, and instantiated as transferable representations among seen and unseen classes. Specifically, we propose a novel unsupervised Part Discovery Network (PDN) to learn transferable representations from unlabeled images, which automatically selects the most discriminative part from an input image and then maximizes its similarities to the global view of the input and other neighbors with similar semantics. To better leverage the learned representations for few-shot learning, we further propose Part-Aligned Similarity (PAS), the key of which is to measure image similarities based on a set of discriminative and aligned parts. We conduct extensive studies on five popular few-shot learning datasets to evaluate our approach. The experimental results show that our approach outperforms previous unsupervised methods by a large margin and is even comparable with state-of-the-art supervised methods.}
}
@article{MORENOPINO2023109014,
title = {Deep autoregressive models with spectral attention},
journal = {Pattern Recognition},
volume = {133},
pages = {109014},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109014},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004940},
author = {Fernando Moreno-Pino and Pablo M. Olmos and Antonio Artés-Rodríguez},
keywords = {Attention models, Deep learning, Filtering, Global-local contexts, Signal processing, Spectral domain attention, Time series forecasting},
abstract = {Time series forecasting is an important problem across many domains, playing a crucial role in multiple real-world applications. In this paper, we propose a forecasting architecture that combines deep autoregressive models with a Spectral Attention (SA) module, which merges global and local frequency domain information in the model’s embedded space. By characterizing in the spectral domain the embedding of the time series as occurrences of a random process, our method can identify global trends and seasonality patterns. Two spectral attention models, global and local to the time series, integrate this information within the forecast and perform spectral filtering to remove time series’s noise. The proposed architecture has a number of useful properties: it can be effectively incorporated into well-known forecast architectures, requiring a low number of parameters and producing explainable results that improve forecasting accuracy. We test the Spectral Attention Autoregressive Model (SAAM) on several well-known forecast datasets, consistently demonstrating that our model compares favorably to state-of-the-art approaches.}
}
@article{YOU2023109023,
title = {Dynamic dense CRF inference for video segmentation and semantic SLAM},
journal = {Pattern Recognition},
volume = {133},
pages = {109023},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109023},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005039},
author = {Mingyu You and Chaoxian Luo and Hongjun Zhou and Shaoqing Zhu},
keywords = {Incremental multi-class video segmentation, Semantic robotSimultaneous Localization and mMapping, Dynamic dense conditional random field},
abstract = {The dense conditional random field (dense CRF) is an effective post-processing tool for image/video segmentation and semantic SLAM. In this paper, we extend the traditional dense CRF inference algorithm to incremental sensor data modelling. The algorithm efficiently infers the maximum a posteriori probability (MAP) solution for a dynamically changing dense CRF model that is applied to incremental multi-class video segmentation and semantic SLAM. The computational cost is roughly proportional to the total change in the Gaussian pairwise edges of the dense CRF. In our system, with an increase in the number of frames of the sensor data, MAP calculations take approximately the same time to compute the overall three-dimensional dense CRF modelled for the entire video. Compared with the traditional dense CRF for video segmentation, this method is more suitable for incremental (in-line) video segmentation and robot semantic SLAM. The results of experiments show that if part of a pairwise edge is altered, our dynamic algorithm is significantly faster than the widely known standard dense CRF algorithm. In addition, the accuracy of its inference does not change. Several multi-class video segmentation tests confirmed the efficiency of inference of the algorithm. In another application, we used the dynamic dense CRF to incrementally integrate robot SLAM and video segmentation. The results show that an accurate SLAM can improve the accuracy of video segmentation, and the computational cost of the dense CRF MAP can be constrained over a constant range. The application of our algorithm is not limited to video segmentation: It is generic, and can be used to yield similar improvements in many optimization solutions for MAP in dynamically changing models.}
}
@article{MELNYKOV2023108994,
title = {Conditional mixture modeling and model-based clustering},
journal = {Pattern Recognition},
volume = {133},
pages = {108994},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108994},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004745},
author = {Volodymyr Melnykov and Yang Wang},
keywords = {finite mixture model, model-based clustering, non-compact clusters, regression, variable selection},
abstract = {Due to a potentially high number of parameters, finite mixture models are often at the risk of overparameterization even for a moderate number of components. This can lead to overfitting individual components and result in mixture order underestimation. One of the most popular approaches to address this issue is to reduce the number of parameters by considering parsimonious models. The vast majority of techniques in this direction focuses on the reparameterization of covariance matrices associated with mixture components. We propose an alternative approach based on the parsimonious parameterization of location parameters that enjoys remarkable modeling flexibility especially in the presence of non-compact clusters. Due to an attractive closed form formulation, speedy parameter estimation is available by means of the EM algorithm. The utility of the proposed method is illustrated on synthetic and well-known classification data sets.}
}
@article{ZHOU2023108970,
title = {GCM: Efficient video recognition with glance and combine module},
journal = {Pattern Recognition},
volume = {133},
pages = {108970},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108970},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004502},
author = {Yichen Zhou and Ziyuan Huang and Xulei Yang and Marcelo Ang and Teck Khim Ng},
keywords = {Glance and combine module, Video action recognition, Spatio-temporal convolution, Action recognition datasets},
abstract = {In this work, we present an efficient and powerful building block for video action recognition, dubbed Glance and Combine Module (GCM). In order to obtain a broader perspective of the video features, GCM introduces an extra glancing operation with a larger receptive field over both the spatial and temporal dimensions, and combines features with different receptive fields for further processing. We show in our ablation studies that the proposed GCM is much more efficient than other forms of 3D spatio-temporal convolutional blocks. We build a series of GCM networks by stacking GCM repeatedly, and train them from scratch on the target datasets directly. On the Kinetics-400 dataset which focuses more on appearance rather than action, our GCM networks can achieve similar accuracy as others without pre-training on ImageNet. For the more action-centric recognition datasets such as Something-Something (V1 & V2) and Multi-Moments in Time, the GCM networks achieve state-of-the-art performance with less than two thirds the computational complexity of other models. With only 19.2 GFLOPs of computation, our GCMNet15 can obtain 63.9% top-1 classification accuracy on Something-Something-V2 validation set under single-crop testing. On the fine-grained action recognition dataset FineGym, we beat the previous state-of-the-art accuracy achieved with 2-stream methods by more than 6% using only RGB input.}
}
@article{DING2023109018,
title = {Self-regularized prototypical network for few-shot semantic segmentation},
journal = {Pattern Recognition},
volume = {133},
pages = {109018},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109018},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004988},
author = {Henghui Ding and Hui Zhang and Xudong Jiang},
keywords = {Few-shot segmentation, Prototype, Prototypical network, Self-regularized, Non-parametric distance fidelity, Iterative query inference, SRPNet, CNN},
abstract = {The deep CNNs in image semantic segmentation typically require a large number of densely-annotated images for training and have difficulties in generalizing to unseen object categories. Therefore, few-shot segmentation has been developed to perform segmentation with just a few annotated examples. In this work, we tackle the few-shot segmentation using a self-regularized prototypical network (SRPNet) based on prototype extraction for better utilization of the support information. The proposed SRPNet extracts class-specific prototype representations from support images and generates segmentation masks for query images by a distance metric - the fidelity. A direct yet effective prototype regularization on support set is proposed in SRPNet, in which the generated prototypes are evaluated and regularized on the support set itself. The extent to which the generated prototypes restore the support mask imposes an upper limit on performance. The performance on the query set should never exceed the upper limit no matter how complete the knowledge is generalized from support set to query set. With the specific prototype regularization, SRPNet fully exploits knowledge from the support and offers high-quality prototypes that are representative for each semantic class and meanwhile discriminative for different classes. The query performance is further improved by an iterative query inference (IQI) module that combines a set of regularized prototypes. Our proposed SRPNet achieves new state-of-art performance on 1-shot and 5-shot segmentation benchmarks.}
}
@article{XU2023109010,
title = {BH2I-GAN: Bidirectional Hash_code-to-Image Translation using Multi-Generative Multi-Adversarial Nets},
journal = {Pattern Recognition},
volume = {133},
pages = {109010},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109010},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004903},
author = {Liming Xu and Xianhua Zeng and Weisheng Li and Yicai Xie},
keywords = {Deep hashing, Generative adversarial nets, Low storage cost, Hash_code-to-image, Supervised manifold similarity},
abstract = {Given the benefits of high retrieval efficiency and low storage cost, hashing method has received an increasing attention. In particular, deep learning-based hashing has been widely used in data mining and information retrieval. However, almost all the existing methods only achieve the goal of high retrieval precision, and limit the evaluation of hashing methods to objective aspect. In this paper, we propose a novel bidirectional hash_code-to-image translation model by using multi-generative multi-adversarial nets to reduce storage cost truly and obtain satisfactory user acceptance on the basis of high retrieval precision. Firstly, we propose supervised manifold metric to reduce Hamming distance between similar instances while increasing the Hamming distance between dissimilar instances, which have been proved to be helpful for high retrieval precision and good user acceptance. Then, we utilize multi-generative and multi-adversarial networks to construct hash mapping and inverse hash generation. During inverse generation, theoretical analysis is conducted to show that inverse hash network can avoid unstable training and mode collapse. Besides, we prove that Poisson distribution induced by hash codes can be initialized as generative distribution to fit real distribution. Experimental results show that our method outperforms several state-of-the-art approaches on three popular datasets. Specifically, ours yields average about 9.3% increment in Mean Average Precision(MAP) on three datasets, and achieves over 90% user satisfaction. Besides, it successfully reduces storage cost by 1,634 times in COCO 2017 large-scale dataset.}
}
@article{CHEN2023108982,
title = {Incremental learning for transductive support vector machine},
journal = {Pattern Recognition},
volume = {133},
pages = {108982},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108982},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004629},
author = {Haiyan Chen and Ying Yu and Yizhen Jia and Bin Gu},
keywords = {Transductive support vector machine, Incremental learning, Non-convex optimization, Infinitesimal annealing},
abstract = {Semi-supervised learning is ubiquitous in real-world machine learning applications due to its good performance for handling the data where only a few number of samples are labeled while most of then are unlabeled. Transductive support vector machine (TSVM) is an important semi-supervised learning method which formulates the problem as a nonconvex combinatorial optimization problem. The infinitesimal annealing algorithm is a novel training method of TSVM which can alleviate the impact of the combinatorial and non-convex natures in TSVM and achieve a fast training of TSVM. However, it is still a challenging problem to handle large-scale data for TSVM even using the infinitesimal annealing algorithm. To mitigate this problem, in this paper, we propose an incremental learning algorithm for TSVM (ILTSVM) based on the path following technique under the framework of infinitesimal annealing. Specifically, for new samples, we call CP-Step to change the solution and partition by increasing the size of the penalty coefficient. The difference between training labeled samples and training unlabeled samples is that the variation range of the penalty coefficient of labeled samples is larger than that of unlabeled samples. If in the process of CP-Step, pseudo-labels of unlabeled samples are classified incorrectly, call DJ-Step to flip the pseudo-labels, and use incremental and decremental algorithms to make the KKT condition satisfied. We also analyze the time complexity and convergence of ILTSVM. The experimental results show that compared with other incremental or batch learning algorithms, our algorithm is the most effective and fastest method for training TSVM.}
}
@article{HE2023108990,
title = {Co-Attention Fusion Network for Multimodal Skin Cancer Diagnosis},
journal = {Pattern Recognition},
volume = {133},
pages = {108990},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108990},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004708},
author = {Xiaoyu He and Yong Wang and Shuang Zhao and Xiang Chen},
keywords = {Skin cancer diagnosis, Convolutional neural networks, Multimodal fusion, Attention mechanism},
abstract = {Recently, multimodal image-based methods have shown great performance in skin cancer diagnosis. These methods usually use convolutional neural networks (CNNs) to extract the features of two modalities (i.e., dermoscopy and clinical images), and fuse these features for classification. However, they commonly have the following two shortcomings: 1) the feature extraction processes of the two modalities are independent and lack cooperation, which may lead to limited representation ability of the extracted features, and 2) the multimodal fusion operation is a simple concatenation followed by convolutions, thus causing rough fusion features. To address these two issues, we propose a co-attention fusion network (CAFNet), which uses two branches to extract the features of dermoscopy and clinical images and a hyper-branch to refine and fuse these features at all stages of the network. Specifically, the hyper-branch is composed of multiple co-attention fusion (CAF) modules. In each CAF module, we first design a co-attention (CA) block with a cross-modal attention mechanism to achieve the cooperation of two modalities, which enhances the representation ability of the extracted features through mutual guidance between the two modalities. Following the CA block, we further propose an attention fusion (AF) block that dynamically selects appropriate fusion ratios to conduct the pixel-wise multimodal fusion, which can generate fine-grained fusion features. In addition, we propose a deep-supervised loss and a combined prediction method to obtain a more robust prediction result. The results show that CAFNet achieves the average accuracy of 76.8% on the seven-point checklist dataset and outperforms state-of-the-art methods.}
}
@article{HE2023109038,
title = {Single image super‐resolution based on progressive fusion of orientation‐aware features},
journal = {Pattern Recognition},
volume = {133},
pages = {109038},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109038},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005180},
author = {Zewei He and Du Chen and Yanpeng Cao and Jiangxin Yang and Yanlong Cao and Xin Li and Siliang Tang and Yueting Zhuang and Zhe-ming Lu},
keywords = {Single image super-resolution, Channel attention, Orientation-aware, Feature extraction, Feature fusion},
abstract = {Single image super-resolution (SISR) is an active research topic in the fields of image processing, computer vision and pattern recognition, restoring high-frequency details and textures based on the low-resolution input image. In this paper, we aim to build more accurate and faster SISR models via developing better-performing feature extraction and fusion techniques. Firstly, we proposed a novel Orientation-Aware feature extraction/selection Module (OAM), which contains a mixture of 1D and 2D convolutional kernels (i.e., 3×1, 1×3, and 3×3) for extracting orientation-aware features. The channel attention mechanism is deployed within each OAM, performing scene-specific selection of informative outputs of the orientation-dependent kernels (e.g., horizontal, vertical, and diagonal). Secondly, we present an effective fusion architecture to progressively integrate multi-scale features extracted in different convolutional stages. Instead of directly combining low-level and high-level features, similar outputs of adjacent feature extraction modules are grouped and further compressed to generate a more concise representation of a specific convolutional stage for high-accuracy SISR task. Based on the above two important improvements, we present a compact but effective CNN-based model for high-quality SISR via Progressive Fusion of Orientation-Aware features (SISR-PF-OA). Extensive experimental results verify the superiority of the proposed SISR-PF-OA model, performing favorably against the state-of-the-art models in terms of both restoration accuracy and computational efficiency (e.g., SISR-PF-OA outperforms RCAN model, achieving higher PSNR 31.25 dB vs. 31.21 dB and using fewer FLOPs 764.41 G vs. 1020.28 G on the Manga109 dataset for scale factor ×4 SISR task.). The source codes will be made publicly available.}
}
@article{WAN2023109034,
title = {Low-rank 2D local discriminant graph embedding for robust image feature extraction},
journal = {Pattern Recognition},
volume = {133},
pages = {109034},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109034},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005143},
author = {Minghua Wan and Xueyu Chen and Tianming Zhan and Guowei Yang and Hai Tan and Hao Zheng},
keywords = {Feature extraction, Two-dimensional locality preserving projections (2DLPP), Low-rank, Graph embedding (GE), Discrimination information},
abstract = {As a popular feature extraction algorithm, the 2D local preserving projections (2DLPP) algorithm has been successfully applied in many fields. Using 2D image representation, the 2DLPP algorithm preserves the manifold attributes and retains the local information of high-dimensional space data. However, the 2DLPP algorithm may encounter some problems in real-world applications, such as a lack of discriminatory ability, singularity problems, and sensitivity to occlusion and noise in data. Therefore, this paper introduces low-rank into the 2DLPP algorithm and proposes a new feature extraction algorithm, which is the low-rank two-dimensional local discriminant graph embedding (LR-2DLDGE), to solve these problems. To improve the LR-2DLDGE algorithm robustness, we fuse the discriminant information in graph embedding and the low-rank properties of the data. The algorithm has three advantages: First, the algorithm uses a graph embedding (GE) framework to maintain the local neighbourhood discrimination information between data. Second, the LR-2DLDGE algorithm ensures that the data points are as independent as possible from different classes in the feature space. Finally, the algorithm uses the L1-norm as a constraint and reduces the influence of noise and corruption through low-rank learning. The theoretical computational complexity and convergence of the algorithm are explicated and proved. Extensive experimental results on three occluded and noisy image datasets confirm the effectively and robustness of LR-2DLDGE, respectively.}
}
@article{WANG2023108989,
title = {A new algorithm for support vector regression with automatic selection of hyperparameters},
journal = {Pattern Recognition},
volume = {133},
pages = {108989},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108989},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004691},
author = {You-Gan Wang and Jinran Wu and Zhi-Hua Hu and Geoffrey J. McLachlan},
keywords = {Automatic selection, Loss functions, Noise models, Parameter estimation, Probability regularization},
abstract = {The hyperparameters in support vector regression (SVR) determine the effectiveness of the support vectors with fitting and predictions. However, the choice of these hyperparameters has always been challenging in both theory and practice. The ν-support vector regression eliminates the need to specify an ϵ value elegantly, but at the cost of specifying or postulating a ν value. We propose an extended primal objective function arising from probability regularization leading to an automatic selection of ϵ, and we can express ν as an explicit function of ϵ. The resultant hyperparameter values can be interpreted as ‘working’ values required only in training but not testing or prediction. This regularized algorithm, namely ϵ*-SVR, automatically provides a data-dependent ϵ and is found to have a close connection to the ν-support vector regression in the sense that ν as a fraction is a sensible function of ϵ. The ϵ*-SVR automatically selects both ν and ϵ values. We illustrate these findings with some public benchmark datasets.}
}
@article{BEHJATI2023108997,
title = {Single image super-resolution based on directional variance attention network},
journal = {Pattern Recognition},
volume = {133},
pages = {108997},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108997},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004770},
author = {Parichehr Behjati and Pau Rodriguez and Carles Fernández and Isabelle Hupont and Armin Mehri and Jordi Gonzàlez},
keywords = {Single image super-resolution, Efficient network, Attention mechanism},
abstract = {Recent advances in single image super-resolution (SISR) explore the power of deep convolutional neural networks (CNNs) to achieve better performance. However, most of the progress has been made by scaling CNN architectures, which usually raise computational demands and memory consumption. This makes modern architectures less applicable in practice. In addition, most CNN-based SR methods do not fully utilize the informative hierarchical features that are helpful for final image recovery. In order to address these issues, we propose a directional variance attention network (DiVANet), a computationally efficient yet accurate network for SISR. Specifically, we introduce a novel directional variance attention (DiVA) mechanism to capture long-range spatial dependencies and exploit inter-channel dependencies simultaneously for more discriminative representations. Furthermore, we propose a residual attention feature group (RAFG) for parallelizing attention and residual block computation. The output of each residual block is linearly fused at the RAFG output to provide access to the whole feature hierarchy. In parallel, DiVA extracts most relevant features from the network for improving the final output and preventing information loss along the successive operations inside the network. Experimental results demonstrate the superiority of DiVANet over the state of the art in several datasets, while maintaining relatively low computation and memory footprint. The code is available at https://github.com/pbehjatii/DiVANet.}
}
@article{LIN2023109026,
title = {Image manipulation detection by multiple tampering traces and edge artifact enhancement},
journal = {Pattern Recognition},
volume = {133},
pages = {109026},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109026},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005064},
author = {Xun Lin and Shuai Wang and Jiahao Deng and Ying Fu and Xiao Bai and Xinlei Chen and Xiaolei Qu and Wenzhong Tang},
keywords = {Image manipulation detection, Transformer, Edge artifact enhancement, Edge supervision},
abstract = {Image manipulation detection has attracted considerable attention owing to the increasing security risks posed by fake images. Previous studies have proven that tampering traces hidden in images are essential for detecting manipulated regions. However, existing methods have limitations in generalization and the ability to tackle post-processing methods. This paper presents a novel Network to learn and Enhance Multiple tampering Traces (EMT-Net), including noise distribution and visual artifacts. For better generalization, EMT-Net extracts global and local noise features from noise maps using transformers and captures local visual artifacts from original RGB images using convolutional neural networks. Moreover, we enhance fused tampering traces using the proposed edge artifacts enhancement modules and edge supervision strategy to discover subtle edge artifacts hidden in images. Thus, EMT-Net can prevent the risks of losing slight visual clues against well-designed post-processing methods. Experimental results indicate that the proposed method can detect manipulated regions and outperform state-of-the-art approaches under comprehensive quantitative metrics and visual qualities. In addition, EMT-Net shows robustness when various post-processing methods further manipulate images.}
}
@article{SUN2023109029,
title = {Multi-scale multi-hierarchy attention convolutional neural network for fetal brain extraction},
journal = {Pattern Recognition},
volume = {133},
pages = {109029},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109029},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200509X},
author = {Liang Sun and Wei Shao and Qi Zhu and Meiling Wang and Gang Li and Daoqiang Zhang},
keywords = {Fetal brain extraction, In utero MR images, Multi-scale, Multi-hierarchy, 3D convolutional neural network},
abstract = {Fetal brain extraction from in utero magnetic resonance imaging (MRI) scans is a key step for fetal brain development analysis. As the unpredicted fetal motion and maternal breathing generally result in blurring and ghosting in the slices of phase encoding direction, using the conventional 3D convolutional neural networks for fetal brain extraction with pseudo 3D fetal brain MR scans will lead to sub-optimal extraction performance. To address this issue, in this paper, we propose a novel multi-scale multi-hierarchy attention convolutional neural network (MSMHA-CNN) for fetal brain extraction in MR images. Specifically, to effectively utilize the 3D contextual information of the in utero MR image for fetal brain extraction, we employ multiple convolutional operations with different local receptive fields (i.e., with different kernel sizes) in each layer to learn the multi-scale feature representation for fetal brain extraction. To effectively use the learned multi-scale feature maps, we introduce a channel-wise spatial attention architecture to adaptively fuse those multi-scale feature maps derived from convolutional operations with different kernel sizes. In this way, the learned multi-scale features can be explicitly used to fetal brain extraction process. Besides, to take advantage of high-level feature maps at all spatial resolutions, we adopt the feature pyramid architecture to learn multi-hierarchy features for boosting the performance. We compare our proposed method with several state-of-the-art methods on two in utero MRI scan datasets (a total of 180 scans) for fetal brain extraction. The experimental results suggest the superior performance of the proposed MSMHA-CNN in comparison with its competitors.}
}
@article{FERRARI2023109022,
title = {Online change-point detection with kernels},
journal = {Pattern Recognition},
volume = {133},
pages = {109022},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109022},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005027},
author = {André Ferrari and Cédric Richard and Anthony Bourrier and Ikram Bouchikhi},
keywords = {Non-parametric change-point detection, Reproducing kernel Hilbert space, Kernel least-mean-square algorithm, Online algorithm, Convergence analysis},
abstract = {Change-points in time series data are usually defined as the time instants at which changes in their properties occur. Detecting change-points is critical in a number of applications as diverse as detecting credit card and insurance frauds, or intrusions into networks. Recently the authors introduced an online kernel-based change-point detection method built upon direct estimation of the density ratio on consecutive time intervals. This paper further investigates this algorithm, making improvements and analyzing its behavior in the mean and mean square sense, in the absence and presence of a change point. These theoretical analyses are validated with Monte Carlo simulations. The detection performance of the algorithm is illustrated through experiments on real-world data and compared to state of the art methodologies.}
}
@article{LIN2023109042,
title = {Exploratory Adversarial Attacks on Graph Neural Networks for Semi-Supervised Node Classification},
journal = {Pattern Recognition},
volume = {133},
pages = {109042},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109042},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005222},
author = {Xixun Lin and Chuan Zhou and Jia Wu and Hong Yang and Haibo Wang and Yanan Cao and Bin Wang},
keywords = {Gradient-based attacks, Maximal gradient, Graph neural networks, Semi-supervised node classification},
abstract = {Graph neural networks (GNNs) have been successfully used to analyze non-Euclidean network data. Recently, there emerge a number of works to investigate the robustness of GNNs by adding adversarial noises into the graph topology, where the gradient-based attacks are widely studied due to their inherent efficiency and high effectiveness. However, the gradient-based attacks often lead to sub-optimal results due to the discrete structure of graph data. To address this issue, we propose a novel exploratory adversarial attack (termed as EpoAtk) to boost the gradient-based perturbations on graphs. The exploratory strategy in EpoAtk includes three phases, generation, evaluation and recombination, with the goal of sidestepping the possible misinformation that the maximal gradient provides. In particular, our evaluation phase introduces a self-training objective containing three effective evaluation functions to fully exploit the useful information of unlabeled nodes. EpoAtk is evaluated on multiple benchmark datasets for the task of semi-supervised node classification in different attack settings. Extensive experimental results demonstrate that the proposed method achieves consistent and significant improvements over the state-of-the-art adversarial attacks with the same attack budgets.}
}
@article{AGIBETOV2023108977,
title = {Neural graph embeddings as explicit low-rank matrix factorization for link prediction},
journal = {Pattern Recognition},
volume = {133},
pages = {108977},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108977},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004575},
author = {Asan Agibetov},
keywords = {Graph embedding, Random walks, Matrix factorization, Information theory, Link prediction},
abstract = {Learning good quality neural graph embeddings has long been achieved by minimzing the pointwise mutual information (PMI) for co-occuring nodes in simulated random walks. This design choice has been mostly popularized by the direct application of the highly-successful word embedding algorithm word2vec to predicting the formation of new links in social, co-citation, and biological networks. However, such a skeuomorphic design of graph embedding methods entails a truncation of information coming from pairs of nodes with low PMI. To circumvent this issue, we propose an improved approach to learning low-rank factorization embeddings that incorporate information from such unlikely pairs of nodes and show that it can improve the link prediction performance of baseline methods from 1.2% to 24.2%. Based on our results and observations, we outline further steps that could improve the design of next graph embedding algorithms that are based on matrix factorizaion.}
}
@article{ZHENG2023109009,
title = {Robust Physical-World Attacks on Face Recognition},
journal = {Pattern Recognition},
volume = {133},
pages = {109009},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109009},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004897},
author = {Xin Zheng and Yanbo Fan and Baoyuan Wu and Yong Zhang and Jue Wang and Shirui Pan},
keywords = {Physical-world adversarial attack, Face recognition, Environmental variations, Curriculum learning},
abstract = {Face recognition has been greatly facilitated by the development of deep neural networks (DNNs) and has been widely applied to many safety-critical applications. However, recent studies have shown that DNNs are very vulnerable to adversarial examples, raising severe concerns on the security of real-world face recognition. In this work, we study sticker-based physical attacks on face recognition for better understanding its adversarial robustness. To this end, we first analyze in-depth the complicated physical-world conditions confronted by attacking face recognition, including the different variations of stickers, faces, and environmental conditions. Then, we propose a novel robust physical attack framework, dubbed PadvFace, to model these challenging variations specifically. Furthermore, we reveal that the attack complexities vary under different physical-world conditions and propose an efficient Curriculum Adversarial Attack (CAA) algorithm that gradually adapts adversarial stickers to environmental variations from easy to complex. Finally, we construct a standardized testing protocol to facilitate the fair evaluation of physical attacks on face recognition, and extensive experiments on both physical dodging and impersonation attacks demonstrate the superior performance of the proposed method.}
}
@article{GIULIVI2023108985,
title = {Adversarial scratches: Deployable attacks to CNN classifiers},
journal = {Pattern Recognition},
volume = {133},
pages = {108985},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108985},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004654},
author = {Loris Giulivi and Malhar Jere and Loris Rossi and Farinaz Koushanfar and Gabriela Ciocarlie and Briland Hitaj and Giacomo Boracchi},
keywords = {Adversarial perturbations, Adversarial attacks, Deep learning, Convolutional neural networks, Bézier curves},
abstract = {A growing body of work has shown that deep neural networks are susceptible to adversarial examples. These take the form of small perturbations applied to the model’s input which lead to incorrect predictions. Unfortunately, most literature focuses on visually imperceivable perturbations to be applied to digital images that often are, by design, impossible to be deployed to physical targets. We present Adversarial Scratches: a novel L0 black-box attack, which takes the form of scratches in images, and which possesses much greater deployability than other state-of-the-art attacks. Adversarial Scratches leverage Bézier Curves to reduce the dimension of the search space and possibly constrain the attack to a specific location. We test Adversarial Scratches in several scenarios, including a publicly available API and images of traffic signs. Results show that our attack achieves higher fooling rate than other deployable state-of-the-art methods, while requiring significantly fewer queries and modifying very few pixels.}
}
@article{ZHOU2023109030,
title = {CSR: Cascade Conditional Variational Auto Encoder with Socially-aware Regression for Pedestrian Trajectory Prediction},
journal = {Pattern Recognition},
volume = {133},
pages = {109030},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109030},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005106},
author = {Hao Zhou and Dongchun Ren and Xu Yang and Mingyu Fan and Hai Huang},
keywords = {Pedestrian trajectory prediction, Socially-aware model, Conditional variational autoencoder (CVAE)},
abstract = {Pedestrian trajectory prediction is a key technology in many real applications such as video surveillance, social robot navigation, and autonomous driving, and significant progress has been made in this research topic. However, there remain two limitations of previous studies. First, the losses of the last time steps are heavier weighted than that of the beginning time steps in the objective function at the learning stage, causing the prediction errors generated at the beginning to accumulate to large errors at the last time steps at the inference stage. Second, the prediction results of multiple pedestrians in the prediction horizon might be socially incompatible with the interactions modeled by past trajectories. To overcome these limitations, this work proposes a novel trajectory prediction method called CSR, which consists of a cascaded conditional variational autoencoder (CVAE) module and a socially-aware regression module. The CVAE module estimates the future trajectories in a cascaded sequential manner. Specifically, each CVAE concatenates the past trajectories and the predicted location points so far as the input and predicts the adjacent location at the following time step. The socially-aware regression module generates offsets from the estimated future trajectories to produce the corrected predictions, which are more reasonable and accurate than the estimated trajectories. Experiments results demonstrate that the proposed method exhibits significant improvements over state-of-the-art methods on the Stanford Drone Dataset (SDD) and the ETH/UCY dataset of approximately 38.0% and 22.2%, respectively. The code is available at https://github.com/zhouhao94/CSR.}
}
@article{WANG2023108993,
title = {BP-triplet net for unsupervised domain adaptation: A Bayesian perspective},
journal = {Pattern Recognition},
volume = {133},
pages = {108993},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108993},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004733},
author = {Shanshan Wang and Lei Zhang and Pichao Wang and MengZhu Wang and Xingyi Zhang},
keywords = {Cross domain class alignment, Unsupervised domain adaptation, Metric learning, Bayesian perspective},
abstract = {Triplet loss, one of the deep metric learning (DML) methods, is to learn the embeddings where examples from the same class are closer than examples from different classes. Motivated by DML, we propose an effective BP-triplet Loss for unsupervised domain adaption (UDA) from the perspective of Bayesian learning and we name the model as BP-Triplet Net. In previous metric learning based methods for UDA, sample pairs across domains are treated equally, which is not appropriate due to the domain bias. In our work, considering the different importance of pair-wise samples for both feature learning and domain alignment, we deduce our BP-triplet loss for effective UDA from the perspective of Bayesian learning. Our BP-triplet loss adjusts the weights of pair-wise samples in intra-domain and inter-domain. Especially, it can self attend to the hard pairs (including hard positive pair and hard negative pair). Together with the commonly used adversarial loss for domain alignment, the quality of target pseudo labels is progressively improved. Our method achieved low joint error of the ideal source and target hypothesis. The expected target error can then be upper bounded following Ben-David’s theorem. Comprehensive evaluations on four benchmark datasets demonstrate the effectiveness of the proposed approach for UDA. Code is available at https://github.com/wangshanshanAHU/BP-Triplet-Net.}
}
@article{XU2023108973,
title = {GripNet: Graph information propagation on supergraph for heterogeneous graphs},
journal = {Pattern Recognition},
volume = {133},
pages = {108973},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108973},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004538},
author = {Hao Xu and Shengqi Sang and Peizhen Bai and Ruike Li and Laurence Yang and Haiping Lu},
keywords = {Graph representation learning, Heterogeneous graph, Data integration, Multi-relational link prediction, Node classification},
abstract = {Heterogeneous graph representation learning aims to learn low-dimensional vector representations of different types of entities and relations to empower downstream tasks. Existing popular methods either capture semantic relationships but indirectly leverage node/edge attributes in a complex way, or leverage node/edge attributes directly without taking semantic relationships into account. When involving multiple convolution operations, they also have poor scalability. To overcome these limitations, this paper proposes a flexible and efficient Graph information propagation Network (GripNet) framework. Specifically, we introduce a new supergraph data structure consisting of supervertices and superedges. A supervertex is a semantically-coherent subgraph. A superedge defines an information propagation path between two supervertices. GripNet learns new representations for the supervertex of interest by propagating information along the defined path using multiple layers. We construct multiple large-scale graphs and evaluate GripNet against competing methods to show its superiority in link prediction, node classification, and data integration. The code and data are available at https://github.com/nyxflower/GripNet.}
}
@article{BENITOALTAMIRANO2023108981,
title = {Back-compatible Color QR Codes for colorimetric applications},
journal = {Pattern Recognition},
volume = {133},
pages = {108981},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108981},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004617},
author = {Ismael Benito-Altamirano and David Martínez-Carpena and Olga Casals and Cristian Fábrega and Andreas Waag and Joan Daniel Prades},
keywords = {Barcodes, QR codes, Color correction, Color calibration, Colorchecker, Colorimetry},
abstract = {Color correction techniques in digital photography often rely on the use of color correction charts, which require including this relatively large object in the field of view. We propose here to use QR Codes to pack these color charts in a compact form factor, in a fully compatible manner with conventional black and white QR Codes; this is, without losing any of their easy location, sampling and digital data storage features. First, we present an algorithm to build these new colored QR Codes that preserves the original QR Code functionality - much more than other coloring proposals based on the random substitution of black and white pixels by colors - that relies on the ability of the native CRC code to correct and counteract these alterations. Second, we demonstrate that, as a result, these QR Codes can allocate far many more colors than the conventional color correction charts, enabling much more accurate color correction schemes in a more convenient and usable format.}
}
@article{CORDEIRO2023109013,
title = {LongReMix: Robust learning with high confidence samples in a noisy label environment},
journal = {Pattern Recognition},
volume = {133},
pages = {109013},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109013},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004939},
author = {Filipe R. Cordeiro and Ragav Sachdeva and Vasileios Belagiannis and Ian Reid and Gustavo Carneiro},
keywords = {Noisy label learning, Deep learning, Empirical vicinal risk, Semi-supervised learning},
abstract = {State-of-the-art noisy-label learning algorithms rely on an unsupervised learning to classify training samples as clean or noisy, followed by a semi-supervised learning (SSL) that minimises the empirical vicinal risk using a labelled set formed by samples classified as clean, and an unlabelled set with samples classified as noisy. The classification accuracy of such noisy-label learning methods depends on the precision of the unsupervised classification of clean and noisy samples, and the robustness of SSL to small clean sets. We address these points with a new noisy-label training algorithm, called LongReMix, which improves the precision of the unsupervised classification of clean and noisy samples and the robustness of SSL to small clean sets with a two-stage learning process. The stage one of LongReMix finds a small but precise high-confidence clean set, and stage two augments this high-confidence clean set with new clean samples and oversamples the clean data to increase the robustness of SSL to small clean sets. We test LongReMix on CIFAR-10 and CIFAR-100 with introduced synthetic noisy labels, and the real-world noisy-label benchmarks CNWL (Red Mini-ImageNet), WebVision, Clothing1M, and Food101-N. The results show that our LongReMix produces significantly better classification accuracy than competing approaches, particularly in high noise rate problems. Furthermore, our approach achieves state-of-the-art performance in most datasets. The code is available at https://github.com/filipe-research/LongReMix.}
}
@article{MOHAIMENUZZAMAN2023109025,
title = {Environmental Sound Classiﬁcation on the Edge: A Pipeline for Deep Acoustic Networks on Extremely Resource-Constrained Devices},
journal = {Pattern Recognition},
volume = {133},
pages = {109025},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109025},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005052},
author = {Md Mohaimenuzzaman and Christoph Bergmeir and Ian West and Bernd Meyer},
keywords = {Deep learning, Audio classification, Environmental sound classification, Acoustics, Intelligent sound recognition, Micro-Controller, IoT, Edge-AI},
abstract = {Significant efforts are being invested to bring state-of-the-art classification and recognition to edge devices with extreme resource constraints (memory, speed, and lack of GPU support). Here, we demonstrate the first deep network for acoustic recognition that is small, flexible and compression-friendly yet achieves state-of-the-art performance for raw audio classification. Rather than handcrafting a once-off solution, we present a generic pipeline that automatically converts a large deep convolutional network via compression and quantization into a network for resource-impoverished edge devices. After introducing ACDNet, which produces above state-of-the-art accuracy on ESC-10 (96.65%), ESC-50 (87.10%), UrbanSound8K (84.45%) and AudioEvent (92.57%), we describe the compression pipeline and show that it allows us to achieve 97.22% size reduction and 97.28% FLOP reduction while maintaining close to state-of-the-art accuracy 96.25%, 83.65%, 78.27% and 89.69% on these datasets. We describe a successful implementation on a standard off-the-shelf microcontroller and, beyond laboratory benchmarks, report successful tests on real-world datasets.}
}
@article{XUE2023109041,
title = {Investigating intrinsic degradation factors by multi-branch aggregation for real-world underwater image enhancement},
journal = {Pattern Recognition},
volume = {133},
pages = {109041},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109041},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005210},
author = {Xinwei Xue and Zexuan Li and Long Ma and Qi Jia and Risheng Liu and Xin Fan},
keywords = {Underwater image enhancement, Multi-branch learning, Real-world underwater images, Comprehensive evaluation},
abstract = {Recently, improving the visual quality of underwater images has received extensive attentions in both computer vision and ocean engineering fields. However, existing works mostly focus on directly learning clear images from degraded observations but without careful investigations on the intrinsic degradation factors, thus require mass training data and lack generalization ability. In this work, we propose a new method, named Multi-Branch Aggregation Network (termed as MBANet) to partially address the above issue. Specifically, by analyzing underwater degradation factors from the perspective of both color distortions and veil effects, MBANet first constructs a multi-branch multi-variable architecture to obtain one intermediate coarse result and two degraded factors. We then establish a physical model inspired process to fully utilize our estimated degraded factors and thus obtain the desired clear output images. A series of evaluations on multiple datasets show the superiority of our method against existing state-of-the-art approaches, both in execution speed and accuracy. Furthermore, we demonstrate that our MBANet can significantly improve the performance of salience object detection in the underwater environment.}
}
@article{MUNJAL2023109049,
title = {Query-guided networks for few-shot fine-grained classification and person search},
journal = {Pattern Recognition},
volume = {133},
pages = {109049},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109049},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005295},
author = {Bharti Munjal and Alessandro Flaborea and Sikandar Amin and Federico Tombari and Fabio Galasso},
keywords = {Meta-learning, Few-shot learning, Fine-grained classification, Person search, Person re-identification},
abstract = {Few-shot fine-grained classification and person search appear as distinct tasks and literature has treated them separately. But a closer look unveils important similarities: both tasks target categories that can only be discriminated by specific object details; and the relevant models should generalize to new categories, not seen during training. We propose a novel unified Query-Guided Network (QGN) applicable to both tasks. QGN consists of a Query-guided Siamese-Squeeze-and-Excitation subnetwork which re-weights both the query and gallery features across all network layers, a Query-guided Region Proposal subnetwork for query-specific localisation, and a Query-guided Similarity subnetwork for metric learning. QGN improves on a few recent few-shot fine-grained datasets, outperforming other techniques on CUB by a large margin. QGN also performs competitively on the person search CUHK-SYSU and PRW datasets, where we perform in-depth analysis.}
}
@article{YANG2023108968,
title = {Retinal image enhancement with artifact reduction and structure retention},
journal = {Pattern Recognition},
volume = {133},
pages = {108968},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108968},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004484},
author = {Bingyu Yang and He Zhao and Lvchen Cao and Hanruo Liu and Ningli Wang and Huiqi Li},
keywords = {Retinal image enhancement, Generative adversarial networks, High frequency},
abstract = {Enhancement of low-quality retinal fundus images is beneficial to clinical diagnosis of ophthalmic diseases and computer-aided analysis. Enhancement accuracy is a challenge for image generation models, especially when there is no supervision by paired images. To reduce artifacts and retain structural consistency for accuracy improvement, we develop an unpaired image generation method for fundus image enhancement with the proposed high-frequency extractor and feature descriptor. Specifically, we summarize three causes of tiny vessel-like artifacts which always appear in other image generation methods. A high frequency prior is incorporated into our model to reduce artifacts by the proposed high-frequency extractor. In addition, the feature descriptor is trained alternately with the generator using segmentation datasets and generated image pairs to ensure the fidelity of the image structure. Pseudo-label loss is proposed to improve the performance of the feature descriptor. Experimental results show that the proposed method performs better than other methods both qualitatively and quantitatively. The enhancement can improve the performance of segmentation and classification in retinal images.}
}
@article{LAN2023109033,
title = {AEDNet: Adaptive Edge-Deleting Network For Subgraph Matching},
journal = {Pattern Recognition},
volume = {133},
pages = {109033},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109033},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005131},
author = {Zixun Lan and Ye Ma and Limin Yu and Linglong Yuan and Fei Ma},
keywords = {Subgraph matching, Graph neural network, Neural matching},
abstract = {Subgraph matching is to find all subgraphs in a data graph that are isomorphic to an existing query graph. Subgraph matching is an NP-hard problem, yet has found its applications in many areas. Many learning-based methods have been proposed for graph matching, whereas few have been designed for subgraph matching. The subgraph matching problem is generally more challenging, mainly due to the different sizes between the two graphs, resulting in considerable large space of solutions. Also the extra edges existing in the data graph connecting to the matched nodes may lead to two matched nodes of two graphs having different adjacency structures and often being identified as distinct objects. Due to the extra edges, the existing learning based methods often fail to generate sufficiently similar node-level embeddings for matched nodes. This study proposes a novel Adaptive Edge-Deleting Network (AEDNet) for subgraph matching. The proposed method is trained in an end-to-end fashion. In AEDNet, a novel sample-wise adaptive edge-deleting mechanism removes extra edges to ensure consistency of adjacency structure of matched nodes, while a unidirectional cross-propagation mechanism ensures consistency of features of matched nodes. We applied the proposed method on six datasets with graph sizes varying from 20 to 2300. Our evaluations on six open datasets demonstrate that the proposed AEDNet outperforms six state-of-the-arts and is much faster than the exact methods on large graphs.}
}
@article{LI2023108976,
title = {A unified model for the sparse optimal scoring problem},
journal = {Pattern Recognition},
volume = {133},
pages = {108976},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108976},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004563},
author = {Guoquan Li and Linxi Yang and Kequan Zhao},
keywords = {Optimal scoring, Linear discriminant analysis, Feature selection, norm, Sparseness},
abstract = {Optimal scoring (OS), an equivalent form of linear discriminant analysis (LDA), is an important supervised learning method and dimensionality reduction tool. However, it is still a challenge for the classical OS on small sample size (SSS) datasets. In this paper, to find sparse discriminant vectors, we propose a unified model for sparse optimal scoring (SOS) by virtue of the generalized ℓq-norm (0≤q≤1). To overcome the difficulty in treating the generalized ℓq-norm, we propose an efficient alternative direction method of multipliers (ADMM), where proximity operator of ℓq-norm is employed for different q values. Meanwhile, the convergence results of our method are also established. Numerical experiments on artificial and benchmark datasets demonstrate the effectiveness and feasibility of our proposed method.}
}
@article{BAI2023109037,
title = {Query efficient black-box adversarial attack on deep neural networks},
journal = {Pattern Recognition},
volume = {133},
pages = {109037},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109037},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005179},
author = {Yang Bai and Yisen Wang and Yuyuan Zeng and Yong Jiang and Shu-Tao Xia},
keywords = {Black-box adversarial attack, Adversarial distribution, Query efficiency, Neural process},
abstract = {Deep neural networks (DNNs) have demonstrated excellent performance on various tasks, yet they are under the risk of adversarial examples that can be easily generated when the target model is accessible to an attacker (white-box setting). As plenty of machine learning models have been deployed via online services that only provide query outputs from inaccessible models (e.g., Google Cloud Vision API2), black-box adversarial attacks raise critical security concerns in practice rather than white-box ones. However, existing query-based black-box adversarial attacks often require excessive model queries to maintain a high attack success rate. Therefore, in order to improve query efficiency, we explore the distribution of adversarial examples around benign inputs with the help of image structure information characterized by a Neural Process, and propose a Neural Process based black-box adversarial attack (NP-Attack) in this paper. Our proposed NP-Attack could be further boosted when applied with surrogate models or tiling tricks. Extensive experiments show that NP-Attack could greatly decrease the query counts under the black-box setting.}
}
@article{REN2023108992,
title = {Grouping-based Oversampling in Kernel Space for Imbalanced Data Classification},
journal = {Pattern Recognition},
volume = {133},
pages = {108992},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108992},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004721},
author = {Jinjun Ren and Yuping Wang and Yiu-ming Cheung and Xiao-Zhi Gao and Xiaofang Guo},
keywords = {Imbalanced data classification, Kernel method, Support vector machine, Oversampling},
abstract = {The class-imbalanced classification is a difficult problem because not only traditional classifiers are more biased towards the majority classes and inclined to generate incorrect predictions, but also the existing algorithms often have difficulty tackling this kind of problem with the class overlapping. Oversampling is a widely used and effective method to obtain balanced samples for imbalanced data, but the existing oversampling methods usually result in more serious class overlapping due to improper choice of the reference samples. To circumvent this shortcoming, according to the different possibilities of minority class samples appearing in the overlapping regions in the feature space, a grouping scheme for the minority class samples is first designed to identify the overlapping region samples. Then, a new oversampling method based on this grouping scheme is proposed to make the new samples far away from the overlapping region and rectify the decision boundary properly. Subsequently, a new effective classification algorithm is developed for imbalanced data. Extensive experiments show that the proposed algorithm is superior to the seventeen benchmark algorithms in terms of three performance metrics, especially on high imbalance ratio data sets.}
}
@article{JUNG2023109061,
title = {Conditional GAN with 3D discriminator for MRI generation of Alzheimer’s disease progression},
journal = {Pattern Recognition},
volume = {133},
pages = {109061},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109061},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005416},
author = {Euijin Jung and Miguel Luna and Sang Hyun Park},
keywords = {Conditional GAN, Alzheimer’s disease, 3D Discriminator, Magnetic resonance image generation, Adaptive identity loss},
abstract = {Many studies aim to predict the degree of deformation on affected brain regions as Alzheimer’s disease (AD) progresses. However, those studies have been often limited since it is difficult to obtain sequential longitudinal MR data of affected patients. Recently, conditional generative adversarial networks (cGANs) have been used to estimate the changes between unpaired images by modeling their differences. However, generating high-quality 3D magnetic resonance (MR) brain images with cGANs requires a large amount of computation. Previous models have been mostly designed to operate in 2D space taking individual slices or down-sampled 3D space, but these approaches often cause spatial artifacts such as discontinuities between slices or unnatural changes in 3D space. To address these limitations, we propose a novel cGAN that can synthesize high-quality 3D MR images at different stages of AD by integrating an additional module that ensures smooth and realistic transitions in 3D space. Specifically, the proposed cGAN model consists of an attention-based 2D generator, a 2D discriminator, and a 3D discriminator that is able to synthesize continuous 2D slices along the axial view resulting in good quality 3D MR volumes. Moreover, we propose an adaptive identity loss so that relevant transformations take place without compromising the features to identify patients. In our experiments, the proposed method showed better image generation performance than previously proposed GAN methods in terms of image quality and image generation suitable for the condition.}
}
@article{LIU2023109008,
title = {Noise-robust oversampling for imbalanced data classification},
journal = {Pattern Recognition},
volume = {133},
pages = {109008},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109008},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004885},
author = {Yongxu Liu and Yan Liu and Bruce X.B. Yu and Shenghua Zhong and Zhejing Hu},
keywords = {Imbalanced learning, Classification, Clustering},
abstract = {The class imbalance problem is characterized by an unequal data distribution in which majority classes have a greater number of data samples than minority classes. Oversampling methods generate samples for minority classes to balance the data distribution. However, the generated minority samples may overlap with majority samples, resulting in noise. In this paper, we propose a noise-robust oversampling algorithm for mixed-type and multi-class imbalanced data. Our proposed noise-robust designs include an algorithm to eliminate noise within clusters of data samples, adaptive embedding to generate samples safely, and a safe boundary for enlarging class boundaries. The heterogeneous distance metric and adapted decomposition strategy render our noise-robust algorithm suitable for mixed-type and multi-class imbalanced data. Experimental results on 20 benchmark datasets demonstrate the effectiveness of the proposed algorithm.}
}
@article{YI2023109019,
title = {UAVformer: A Composite Transformer Network for Urban Scene Segmentation of UAV Images},
journal = {Pattern Recognition},
volume = {133},
pages = {109019},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109019},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200499X},
author = {Shi Yi and Xi Liu and Junjie Li and Ling Chen},
keywords = {Urban scenes segmentation, UAV image, Composite backbone, Aggregation windows multi-head self-attention transformer block, V-shaped decoder},
abstract = {Urban scenes segmentation based on UAV (Unmanned aerial vehicle) view is a fundamental task for the applications of smart city such as city planning, land use monitoring, traffic monitoring, and crowd estimation. While urban scenes in UAV image characteristic by large scale variation of objects size and complexity background, which posed challenges to urban scenes segmentation of UAV image. The feature extracting backbone of existing networks cannot extract complex features of UAV image effectively, which limits the performance of urban scenes segmentation. To design segmentation network capable of extracting features of large scale variation urban ground scenes, this study proposed a novel composite transformer network for urban scenes segmentation of UAV image. A composite backbone with aggregation windows multi-head self-attention transformer blocks is proposed to make the extracted features more representatives by adaptive multi-level features fusion, and the full utilisation of contextual information and local information. Position attention modules are inserted in each stage between encoder and decoder to further enhance the spatial attention of extracted feature maps. Finally, a V-shaped decoder which is capable of utilising multi-level features is designed to get accurately dense prediction. The accuracy of urban scenes segmentation could significantly be enhanced in this way and successfully segmented the large scale variation objects from UAV views. Extensive ablation experiments and comparative experiments for the proposed network have been conducted on the public available urban scenes segmentation datasets for UAV imagery. Experimental results have demonstrated the effectiveness of designed network structure and the superiority of proposed network over state-of-the-art methods. Specifically, reached 53.2% mIoU on the UAVid dataset and 77.6% mIoU on the UDD6 dataset, respectively.}
}
@article{YANG2023109053,
title = {Lane Detection with Versatile AtrousFormer and Local Semantic Guidance},
journal = {Pattern Recognition},
volume = {133},
pages = {109053},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109053},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005337},
author = {Jiaxing Yang and Lihe Zhang and Huchuan Lu},
keywords = {Lane detection, Global AtrousFormer, Local AtrousFormer, Enhanced feature extractor, Local semantic guided decoder},
abstract = {Lane detection is one of the core functions in autonomous driving and has aroused widespread attention recently. The networks to segment lane instances, especially with bad appearance, must be able to explore lane distribution properties. Most existing methods tend to resort to CNN-based techniques. A few have a try on incorporating the recent adorable, the seq2seq Transformer [1]. However, their innate drawbacks of weak global information collection ability and exorbitant computation overhead prohibit a wide range of the further applications. In this work, we propose Global Atrous Transformer (AtrousFormer) to solve the problem. Its variant local AtrousFormer is interleaved into feature extractor to enhance extraction. Their collecting information first by rows and then by columns in a dedicated manner finally equips our network with stronger information gleaning ability and better computation efficiency. To further improve the performance, we also propose a local semantic guided decoder to delineate the identities and shapes of lanes more accurately. Extensive results on three challenging benchmarks (CULane, TuSimple, and BDD100K) show that our network performs favorably against the state of the arts.}
}
@article{HUO2023109032,
title = {Collaborative Learning with Unreliability Adaptation for Semi-Supervised Image Classification},
journal = {Pattern Recognition},
volume = {133},
pages = {109032},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109032},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200512X},
author = {Xiaoyang Huo and Xiangping Zeng and Si Wu and Wenjun Shen and Hau-San Wong},
keywords = {Semi-supervised learning, Image classification, Unreliability adaptation, Collaborative learning},
abstract = {Constructing training goals for unlabeled data is crucial for image classification in the semi-supervised setting. Consistency regularization typically encourages a model to produce consistent predictions with the given training goals, while unreliability adaptation aims to learn the transition probabilities from model predictions to training goals, instead of enforcing their consistency. In this paper, we present a model of Collaborative learning with Unreliability Adaptation (CoUA), in which multiple constituent networks collaboratively learn with each other by adapting their predictions. Toward this end, an additional adaptation module is incorporated into each network to learn a transition probability from its own prediction to that of the paired network. Therefore, the networks can exchange training experience, without being overly sensitive to the unreliability of predictions. To further enhance the collaborative learning, each network is encouraged to produce consistent predictions with the consensus results, while being resistant to the adversarial perturbations against others. Therefore, the networks are able to mutually reinforce each other. We perform extensive experiments on multiple image classification benchmarks to verify the superiority of the co-adaptation based collaborative learning mechanism.}
}
@article{WEI2023108996,
title = {An accurate stereo matching method based on color segments and edges},
journal = {Pattern Recognition},
volume = {133},
pages = {108996},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108996},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004769},
author = {Hui Wei and Lingjiang Meng},
keywords = {Binocular vision, Stereo matching, Industrial robot},
abstract = {Stereo matching algorithms of binocular vision suffer from low accuracy when dealing with natural scenes (such as industrial robot scenes). Biological vision is sensitive to object edges; it divides objects by their edges, and then perceives their distances. Similar to the biological eye mechanism, this study proposes a matching algorithm that combines segment- and edge-matching to obtain the disparity. In segment matching, pixel strings from the same row of the left and right images are divided into pixel segments, whose colors and lengths are used as clues to determine several types of matching pixel segment pairs according to non-crossing mapping. The analysis of the spatial state yields several types of stimulus bars. Disparities can be obtained from the relation between pixel segment pairs and stimulus bars. In edge matching, the DTW (Dynamic Time Warping) algorithm and the gradient are used to determine the initial edge pixel matching results. The remaining edge point disparity is obtained by fitting a fill to the existing edge point disparity. Finally, segment and edge matching results are combined to check and fill and post-processing. This new matching method transforms pixel matching to pixel segment matching and edge matching, which can reduces the time complexity. The algorithm can be implemented in an industrial robot environment for high-precision needle threading guidance, which neither traditional binocular matching nor deep learning matching algorithms can do.}
}
@article{2023109093,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {133},
pages = {109093},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(22)00573-8},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005738}
}
@article{ZHANG2023109012,
title = {Towards prior gap and representation gap for long-tailed recognition},
journal = {Pattern Recognition},
volume = {133},
pages = {109012},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109012},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004927},
author = {Ming-Liang Zhang and Xu-Yao Zhang and Chuang Wang and Cheng-Lin Liu},
keywords = {Long-tailed learning, Prior gap, Representation gap, Image recognition},
abstract = {Most deep learning models are elaborately designed for balanced datasets, and thus they inevitably suffer performance degradation in practical long-tailed recognition tasks, especially to the minority classes. There are two crucial issues in learning from imbalanced datasets: skew decision boundary and unrepresentative feature space. In this work, we establish a theoretical framework to analyze the sources of these two issues from Bayesian perspective, and find that they are closely related to the prior gap and the representation gap, respectively. Under this framework, we show that existing long-tailed recognition methods manage to remove either the prior gap or the presentation gap. Different from these methods, we propose to simultaneously remove the two gaps to achieve more accurate long-tailed recognition. Specifically, we propose the prior calibration strategy to remove the prior gap and introduce three strategies (representative feature extraction, optimization strategy adjustment and effective sample modeling) to mitigate the representation gap. Extensive experiments on five benchmark datasets validate the superiority of our method against the state-of-the-art competitors.}
}
@article{ZHENG2023108991,
title = {Margin embedding net for robust margin collaborative representation-based classification},
journal = {Pattern Recognition},
volume = {133},
pages = {108991},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108991},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200471X},
author = {Zhichao Zheng and Huaijiang Sun and Ying Zhou},
keywords = {Collaborative representation, Feature extraction, Marginal sample, Image classification},
abstract = {Collaborative Representation-based Classification method (CRC) shows great potential in classification task. However, redundancies in both features and samples limit the application of CRC seriously. The existing works only solve one of them and ignore the other, which leads to performance degradation. To address this problem, we explore collaborative representation mechanism and propose a classification method termed Robust Margin Collaborative Representation-based Classification (RMCRC) which uses a few but more representative robust marginal samples to eliminate redundancy between samples. As the performance of RMCRC is related to robust marginal samples and class separability assumption closely, we further propose a feature extraction method termed Margin Embedding Net (MEN) for RMCRC. In MEN, virtual samples are generated by a generative model to enhance effectiveness of robust marginal samples and generalizability of RMCRC. Then, an embedding network with triplet loss is used to eliminate the redundancy in features and ensure the assumption is satisfied. Specifically, we construct triplet according to the collaborative representation. Hence, MEN fits RMCRC very well. Extensive experimental results validate effectiveness of proposed method.}
}