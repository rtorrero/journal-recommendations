@article{ZHU2023109597,
title = {Factorized multi-Graph matching},
journal = {Pattern Recognition},
volume = {140},
pages = {109597},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109597},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002984},
author = {Liangliang Zhu and Xinwen Zhu and Xiurui Geng},
keywords = {Graph matching, Multi-graph matching, Tensor, Factorization},
abstract = {In recent years, multi-graph matching has become a popular yet challenging task in graph theory. There exist two major problems in multi-graph matching, i.e., the cycle-consistency problem, and the high time and space complexity problem. On one hand, the pairwise-based multi-graph matching methods are of low time and space complexity, but in order to keep the cycle-consistency of the matching results, they need additional constraints. Besides, the accuracy of the pairwise-based multi-graph matching is highly dependent on the selected optimization algorithms. On the other hand, the tensor-based multi-graph matching methods can avoid the cycle-consistency problem, while their time and space complexity is extremely high. In this paper, we found the equivalence between the pairwise-based and the tensor-based multi-graph matching methods under some specific circumstances. Based on this finding, we proposed a new multi-graph matching method, which not only avoids the cycle-consistency problem, but also reduces the complexity. In addition, we further improved the proposed method by introducing a lossless factorization of the affinity matrix in the multi-graph matching methods. Synthetic and real data experiments demonstrate the superiority of our method.}
}
@article{QI2023109546,
title = {Unsupervised generalizable multi-source person re-identification: A Domain-specific adaptive framework},
journal = {Pattern Recognition},
volume = {140},
pages = {109546},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109546},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002467},
author = {Lei Qi and Jiaqi Liu and Lei Wang and Yinghuan Shi and Xin Geng},
keywords = {Unsupervised domain generalization person ReID, Domain-specific adaptive normalization},
abstract = {Domain generalization (DG) has attracted much attention in person re-identification (ReID) recently. It aims to make a model trained on multiple source domains generalize to an unseen target domain. Although achieving promising progress, existing methods usually need the source domains to be labeled, which could be a significant burden for practical ReID tasks. In this paper, we turn to investigate “unsupervised” domain generalization for ReID, by assuming that no label is available for any source domains. To address this challenging setting, we propose a simple and efficient domain-specific adaptive framework, and realize it with an adaptive normalization module designed upon the batch and instance normalization techniques. In doing so, we successfully yield reliable pseudo-labels to implement training and also enhance the domain generalization capability of the model as required. In addition, we show that our framework can even be applied to improve person ReID under the settings of supervised domain generalization and unsupervised domain adaptation, demonstrating competitive performance with respect to relevant methods. Extensive experimental study on benchmark datasets is conducted to validate the proposed framework. A significance of our work lies in that it shows the potential of unsupervised domain generalization for person ReID and sets a strong baseline for the further research on this topic. The code is available at https://github.com/Qi5Lei/DSAF.}
}
@article{ZHANG2023109525,
title = {Crowdmeta: Crowdsourcing truth inference with meta-Knowledge transfer},
journal = {Pattern Recognition},
volume = {140},
pages = {109525},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109525},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300225X},
author = {Jing Zhang and Sunyue Xu and Victor S. Sheng},
keywords = {Crowdsourcing, Truth inference, Transfer learning, Meta learning},
abstract = {Crowdsourcing provides a fast and low-cost solution to collect annotations for training data in computer vision. However, there are two challenges in crowdsourced image annotation: First, when crowdsourced workers perform annotation tasks in an unfamiliar domain, their accuracy will dramatically decline due to the lack of expertise; Second, the difficulties of tasks may be different due to the noises in images, which is only related to the features of images themselves and will affect the judgment of workers. It is well known that transferring knowledge from relevant domains can form a better representation for training samples, which benefits the estimation of workers’ expertise in truth inference models. However, the existing knowledge transfer processes for crowdsourcing require a considerable number of well-collected samples in source domains. Comprehensively considering the above issues, this paper proposes a novel probabilistic model for crowdsourcing truth inference, which fuses few-shot meta-learning and transfer learning. The proposed model transfers meta-knowledge from the source domain to form better high-level representations of the instances in the target domain. Simultaneously utilizing both high-level representations and instance features, the quality of workers and the difficulty of instances can be better modeled and inferred. Experimental results on a number of datasets show that the proposed model not only outperforms the state-of-the-art models but also significantly reduces the number of instances required in the source domain.}
}
@article{ALI2023109522,
title = {Boundary-constrained robust regularization for single image dehazing},
journal = {Pattern Recognition},
volume = {140},
pages = {109522},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109522},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002224},
author = {Usman Ali and Jeongdan Choi and KyoungWook Min and Young-Kyu Choi and Muhammad Tariq Mahmood},
keywords = {Image dehazing, Transmission map, Nonconvex energy function, Regularization, Boundary constraints},
abstract = {Generally, for single image dehazing, regularization-based schemes improve the initial transmission map iteratively by using a guidance map as a structural prior. We conducted experiments on a large number of hazy images and observed that a constrained transmission map affects the quality of the recovered image. However, regularization-based methods do not constrain the transmission map to its physically valid range during the iterative process. It degrades its robustness to outliers, and consequently, deteriorates the quality of the recovered image. In addition, conventional methods fuse the structural information of the guidance and initial transmission map without considering any structural differences between them. To address these issues, in this paper, we present a robust regularization scheme that constraints the transmission map during its enhancement. In the proposed scheme, a nonconvex energy function is constructed that leverages the mutual structural information of the guidance and transmission map. The nonconvex problem is solved by a majorize-minimization algorithm, and the intermediate transmission maps are constrained through the appropriate lower and upper bounds. The retrieved transmission map has better edge-preserving properties, and ultimately, results in a high-quality haze-free image that has faithful colors and fine details. The proposed scheme is tested on benchmark datasets and results are evaluated through quantitative metrics. The comparative analysis has revealed the effectiveness of the proposed scheme.}
}
@article{MO2023109485,
title = {Scatter matrix decomposition for jointly sparse learning},
journal = {Pattern Recognition},
volume = {140},
pages = {109485},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109485},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001851},
author = {Dongmei Mo and Zhihui Lai and Jie Zhou and Hu Qinghua},
keywords = {Feature extraction, Pattern recognition, Classification, Linear discriminant analysis, Joint sparsity},
abstract = {Orthogonal Linear Discriminant Analysis (OLDA) based on generalized Eigen-equation is widely used in the field of computer vision and pattern recognition. However, the performance of OLDA for feature extraction and classification needs to be improved as it lacks sparsity for better interpretation of the features. Moreover, computing the orthogonal sparse projections based on LDA is very difficult and is still unsolved. To solve these problems, in this paper, we propose a method called Jointly Sparse Orthogonal Linear Discriminant Analysis (JSOLDA). Different from the existing OLDA, JSOLDA is proposed from a novel viewpoint of scatter matrix decomposition. Theoretical analysis shows that OLDA can be derived by the constrained scatter matrix decomposition. In addition, by imposing L2,1-norm on the penalty term, the proposed JSOLDA can obtain the jointly sparse orthogonal projections to perform feature extraction. We also design an iterative algorithm to obtain the optimal solution. Systematic theoretical analysis between the OLDA and JSOLDA are uncovered. Both of convergence and computational complexity are also discussed. Experimental results on four data sets (i.e., COIL100, USPS, ICADAR2003 and CMU PIE) indicate that JSOLDA outperforms several well-known LDA-based and L2,1-norm based methods.}
}
@article{XUE2023109538,
title = {Joint optimization for attention-based generation and recognition of chinese characters using tree position embedding},
journal = {Pattern Recognition},
volume = {140},
pages = {109538},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109538},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002388},
author = {Mobai Xue and Jun Du and Bin Wang and Bo Ren and Yu Hu},
keywords = {Chinese character generation and recognition, Radical analysis, Joint optimization, Tree position embedding},
abstract = {Despite the growing interest in Chinese character generation, creating a nonexistent character remains an open challenge. Radical-based Chinese character generation is still a novel task while radical-based Chinese character recognition is more technologically advanced. To fully utilize the knowledge of recognition task, we first propose an attention-based generator. The generator chooses the most relevant radical to generate each zone with an attention mechanism. Then, we present a joint optimization approach to training generation-recognition models, which can help the generator and recognizer learn from each other effectively. The joint optimization is implemented via contrastive learning and dual learning. Considering the symmetry of the generation and recognition, contrastive learning aims to strengthen the performance of the encoder of recognizer and the decoder of generator. Since the generation and recognition tasks can form a closed loop, dual learning feeds the output from one to another as input. Based on the feedback signals generated during the two tasks, we can iteratively update the two models until convergence. Finally, as our model ignores the order information of a sequence, we exploit position embedding to extend the image representation ability and propose tree position embedding to represent the positional information for tree structure captions of Chinese characters. The experimental results in printed and nature scenes show that the proposed method improves the quality of the generating images and increases the recognition accuracy for Chinese characters.}
}
@article{LIU2023109566,
title = {A novel relation aware wrapper method for feature selection},
journal = {Pattern Recognition},
volume = {140},
pages = {109566},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109566},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002662},
author = {Zhaogeng Liu and Jielong Yang and Li Wang and Yi Chang},
keywords = {Feature selection, Sample relation, Feature relation, Classification},
abstract = {Feature selection, aiming at eliminating irrelevant and redundant features, is an important data preprocessing technology for downstream tasks, e.g., classification. With the explosive growth of data in various fields, some data are high-dimensional and contain critical and complex hidden relationships, which brings new challenges to feature selection: i) How to find out the underlying available relationships from the data, and ii) how to use the learned relations to better select features? To deal with these challenges, we propose a novel wrapper feature selection method named Relation Aware Feature Selection Method (ERASE), which can learn and use the underlying sample relations and feature relations for feature selection. Different from existing methods, our method jointly learns sample relationships and feature relationships through a graph of samples and trees of features. Furthermore, it uses the relations to select the optimal feature subset according to the new proposed Relation-based Sequence Floating Selection Strategy. Extensive experimental results on nine datasets from different domains demonstrate that our method achieves the best performance in most cases compared with other feature selection methods, including state-of-the-art wrapper methods.}
}
@article{HOU2023109558,
title = {CANet: Contextual Information and Spatial Attention Based Network for Detecting Small Defects in Manufacturing Industry},
journal = {Pattern Recognition},
volume = {140},
pages = {109558},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109558},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002583},
author = {Xiuquan Hou and Meiqin Liu and Senlin Zhang and Ping Wei and Badong Chen},
keywords = {Small defect detection, Contextual information, Spatial attention, Multi-scale feature fusion, Automatic visual inspection},
abstract = {Despite the promising development of Automatic Visual Inspection (AVI) in the manufacturing industry, detecting small-sized defects with fewer pixels coverage remains a challenging problem due to its insufficient attention and lack of semantic information. Most exsiting convolutional inspection methods overlook the long-range dependence of context and lack adaptive fusion strategies to exploit heterogeneous features. To address these issues in AVI, this paper proposes a novel contextual information and spatial attention based network (CANet), which consists of two steps, namely CAblock and LaplacianFPN, for effective perception and exploitation of small defect features. Specifically, CAblock extracts semantic information with rich context by encoding spatial long-range dependence and decoding contextual information as channel-specific bias through a Spatial Attention Encoder (SAE) and a Context Block Decoder (CBD), respectively. LaplacianFPN further performs adaptive feature fusion considering both feature consistency and heterogeneity via two parallel branches. As a benchmark, a self-built Engine Surface Defects (ESD) dataset collected in real industry containing 89.70% small defects is constructed. Experimental results show that CANet achieves mAP-50 improvements of 1.5% and 4.3% compared to state-of-the-art methods on NEU-DET and ESD, which demonstrates the effectiveness of the proposed method. The code is now available at https://github.com/xiuqhou/CANet.}
}
@article{CHEN2023109506,
title = {Multi-task semi-supervised crowd counting via global to local self-correction},
journal = {Pattern Recognition},
volume = {140},
pages = {109506},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109506},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002066},
author = {Jiwei Chen and Zengfu Wang},
keywords = {Crowd counting, Semi-supervised, Pseudo labels, Global to local self-correction},
abstract = {In this paper, we propose a novel multi-task semi-supervised method. To sufficiently exploit massive unlabeled data, multi-task pseudo-labels and global to local self-correction strategy are proposed. Specifically, labeled images and massive amounts of unlabeled images with proposed multi-task pseudo-labels are leveraged for model optimization. The density level of the whole image is predicted in classification task. The density is estimated in density regression task. The crowd area is segmented out in segmentation task. To suppress incorrect predictions caused by the inevitable noises from some unlabeled data misleading the model, the counting relationship between classification task and density task is exploited to propose the global self-correction strategy, and the semantic consistency between density task and segmentation task is mined to propose the local self-correction strategy. The classification task and segmentation task contribute in generating the final highly refined density map from the density task. Extensive experiments on six benchmark datasets indicate the superiority of our method over the SOTA methods in semi-supervised paradigm.}
}
@article{LI2023109534,
title = {Towards better long-tailed oracle character recognition with adversarial data augmentation},
journal = {Pattern Recognition},
volume = {140},
pages = {109534},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109534},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002340},
author = {Jing Li and Qiu-Feng Wang and Kaizhu Huang and Xi Yang and Rui Zhang and John Y. Goulermas},
keywords = {Oracle character recognition, Long tail, Data imbalance, Data augmentation, Mixup strategy, Generative adversarial networks},
abstract = {Deciphering oracle bone script is of great significance to the study of ancient Chinese culture as well as archaeology. Although recent studies on oracle character recognition have made substantial progress, they still suffer from the long-tailed data situation that results in a noticeable performance drop on the tail classes. To mitigate this issue, we propose a generative adversarial framework to augment oracle characters in the problematic classes. In this framework, the generator produces synthetic data through convex combinations of all the available samples in the corresponding classes, and is further optimized through adversarial learning with the classifier and simultaneously the discriminator. Meanwhile, we introduce Repatch to generalize samples in the generator. Since tail classes do not have sufficient data for convex combinations, we propose the TailMix mechanism to generate suitable tail class samples from other classes. Experimental results show that our proposed algorithm obtains remarkable performance in oracle character recognition and achieves new state-of-the-art average (total) accuracy with 86.03% (89.46%), 86.54% (93.86%), 95.22% (96.17%) on the three datasets Oracle-AYNU, OBC306 and Oracle-20K, respectively.}
}
@article{ZHU2023109578,
title = {Topic-aware video summarization using multimodal transformer},
journal = {Pattern Recognition},
volume = {140},
pages = {109578},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109578},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002789},
author = {Yubo Zhu and Wentian Zhao and Rui Hua and Xinxiao Wu},
keywords = {Topic-aware video summarization, Multimodal transformer, Video summarization dataset},
abstract = {Video summarization aims to generate a short and compact summary to represent the original video. Existing methods mainly focus on how to extract a general objective synopsis that precisely summaries the video content. However, in real scenarios, a video usually contains rich content with multiple topics and people may cast diverse interests on the visual contents even for the same video. In this paper, we propose a novel topic-aware video summarization task that generates multiple video summaries with different topics. To support the study of this new task, we first build a video benchmark dataset by collecting videos from various types of movies and annotate them with topic labels and frame-level importance scores. Then we propose a multimodal Transformer model for the topic-aware video summarization, which simultaneously predicts topic labels and generates topic-related summaries by adaptively fusing multimodal features extracted from the video. Experimental results show the effectiveness of our method.}
}
@article{ZHANG2023109554,
title = {Rigorous non-disjoint discretization for naive Bayes},
journal = {Pattern Recognition},
volume = {140},
pages = {109554},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109554},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002546},
author = {Huan Zhang and Liangxiao Jiang and Geoffrey I. Webb},
keywords = {Naive Bayes, Singleton interval, Proportional weighting, Discretization},
abstract = {Naive Bayes is a classical machine learning algorithm for which discretization is commonly used to transform quantitative attributes into qualitative attributes. Of numerous discretization methods, Non-Disjoint Discretization (NDD) proposes a novel perspective by forming overlapping intervals and always locating a value toward the middle of an interval. However, existing approaches to NDD fail to adequately consider the effect of multiple occurrences of a single value — a commonly occurring circumstance in practice. By necessity, all occurrences of a single value fall within the same interval. As a result, it is often not possible to discretize an attribute into intervals containing equal numbers of training instances. Current methods address this issue in an ad hoc manner, reducing the specificity of the resulting atomic intervals. In this study, we propose a non-disjoint discretization method for NB, called Rigorous Non-Disjoint Discretization (RNDD), that handles multiple occurrences of a single value in a systematic manner. Our extensive experimental results suggest that RNDD significantly outperforms NDD along with all other existing state-of-the-art competitors.}
}
@article{LIU2023109530,
title = {Cloud-VAE: Variational autoencoder with concepts embedded},
journal = {Pattern Recognition},
volume = {140},
pages = {109530},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109530},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002303},
author = {Yue Liu and Zitu Liu and Shuang Li and Zhenyao Yu and Yike Guo and Qun Liu and Guoyin Wang},
keywords = {Variational autoencoder, Disentangled representation, Concept embedded, Cloud Model, Deep Learning Interpretability},
abstract = {Variational Autoencoder (VAE) has been widely and successfully used in learning coherent latent representation of data. However, the lack of interpretability in the latent space constructed by the VAE under the prior distribution is still an urgent problem. This paper proposes a VAE with understandable concept embedding named Cloud-VAE, which constructs interpretable latent space by disentangling the latent variables and considering their uncertainty based on cloud model. Firstly, cloud model-based clustering algorithm cast initial constraint of latent space into a prior distribution of concept which can be embedded into the latent space of the VAE to disentangle the latent variables. Secondly, reparameterization trick based on forward cloud transformation algorithm is designed to estimate the latent space concept by increasing the randomness of latent variables. Furthermore, variational lower bound of Cloud-VAE is derived to guide the training process to construct concepts of latent space, realizing the mutual mapping between latent space and concept space. Finally, experimental results on 6 benchmark datasets show that Cloud-VAE has good clustering and reconstruction performance, which can explicitly explain the aggregation process of the model and discover more interpretable disentangled representations.}
}
@article{LIU2023109550,
title = {Learn from each other to Classify better: Cross-layer mutual attention learning for fine-grained visual classification},
journal = {Pattern Recognition},
volume = {140},
pages = {109550},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109550},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002509},
author = {Dichao Liu and Longjiao Zhao and Yu Wang and Jien Kato},
keywords = {Fine-grained recognition, Image classification, Deep features},
abstract = {Fine-grained visual classification (FGVC) is valuable yet challenging. The difficulty of FGVC mainly lies in its intrinsic inter-class similarity, intra-class variation, and limited training data. Moreover, with the popularity of deep convolutional neural networks, researchers have mainly used deep, abstract, semantic information for FGVC, while shallow, detailed information has been neglected. This work proposes a cross-layer mutual attention learning network (CMAL-Net) to solve the above problems. Specifically, this work views the shallow to deep layers of CNNs as “experts” knowledgeable about different perspectives. We let each expert give a category prediction and an attention region indicating the found clues. Attention regions are treated as information carriers among experts, bringing three benefits: (i) helping the model focus on discriminative regions; (ii) providing more training data; (iii) allowing experts to learn from each other to improve the overall performance. CMAL-Net achieves state-of-the-art performance on three competitive datasets: FGVC-Aircraft, Stanford Cars, and Food-11. The source code is available at https://github.com/Dichao-Liu/CMAL}
}
@article{LIU2023109519,
title = {MEP-3M: A large-scale multi-modal E-commerce product dataset},
journal = {Pattern Recognition},
volume = {140},
pages = {109519},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109519},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002194},
author = {Fan Liu and Delong Chen and Xiaoyu Du and Ruizhuo Gao and Feng Xu},
keywords = {Dataset, E-commerce product classification, Fine-grained learning, Hierarchical classification, Automatic Checkout},
abstract = {The product categories are vital for the E-commerce platforms due to the core applications on automatic product category assignment, personalized product recommendations, etc. In this paper, we construct a large-scale Multi-modal E-commerce Products classification dataset MEP-3M, which is large-scale, hierarchical-categorized, multi-modal, fine-grained, and long-tailed. Statistically, MEP-3M consists of over 3 million products, thus achieves the largest data scale in comparison to the existing E-commerce product datasets. The products in MEP-3M are represented in three modalities: image, textual description, and OCR text, and labeled with tree-like labels. The third level labels are extremely fine-grained. In addition, we exploit four novel practical tasks on this dataset, Product classification, Hierarchical Product Classification, Fine-grained Product Classification, and Product Representation Learning. For each task, we present some image-only, text-only, and multi-modal baseline performances for further researches. The MEP-3M dataset will be released at https://github.com/ChenDelong1999/MEP-3M.}
}
@article{WANG2023109523,
title = {Feature clustering-Assisted feature selection with differential evolution},
journal = {Pattern Recognition},
volume = {140},
pages = {109523},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109523},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002236},
author = {Peng Wang and Bing Xue and Jing Liang and Mengjie Zhang},
keywords = {Differential evolution, Feature selection, Multiple optimal feature subsets, Classification},
abstract = {Modern data collection technologies may produce thousands of or even more features in a single dataset. The high dimensionality of data poses a barrier to determining discriminating features due to the curse of dimensionality. Thanks to the global search ability, many population-based feature selection approaches have been proposed. However, very few studies pay attention on that a feature selection task has multiple optimal feature subsets. To search for multiple optimal feature subsets, we propose a feature clustering-assisted feature selection method. The proposed method employs the knowledge of correlation measures to group features. And, this correlation knowledge is embedded into the encoding method and the search process. A niching-based mutation operator is also used to explore the vicinity of a target individual. The aim is to find different feature subsets with very similar or the same classification performance. In addition, a modification operator is proposed aiming to increase the population diversity to improve the feature selection performance. The experiments on 16 datasets show that the proposed algorithm outperforms other popular feature selection methods in terms of classification accuracy and feature subset size.}
}
@article{LI2023109551,
title = {Dense light field reconstruction based on epipolar focus spectrum},
journal = {Pattern Recognition},
volume = {140},
pages = {109551},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109551},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002510},
author = {Yaning Li and Xue Wang and Hao Zhu and Guoqing Zhou and Qing Wang},
keywords = {Light field representation, Epipolar focus spectrum (EFS), Dense light field reconstruction, Depth independent, Frequency domain},
abstract = {Existing light field (LF) representations, such as epipolar plane image (EPI) and sub-aperture images, do not consider the structural characteristics across the views, so they usually require additional disparity and spatial structure cues for follow-up tasks. Besides, they have difficulties dealing with occlusions or large disparity scenes. To this end, this paper proposes a novel Epipolar Focus Spectrum (EFS) representation by rearranging the EPI spectrum. Different from the classical EPI representation where an EPI line corresponds to a specific depth, there is a one-to-one mapping from the EFS line to the view. By exploring the EFS sampling task, the analytical function is derived for constructing a non-aliasing EFS. To demonstrate its effectiveness, we develop a trainable EFS-based pipeline for light field reconstruction, where a dense light field can be reconstructed by compensating the missing EFS lines given a sparse light field, yielding promising results with cross-view consistency, especially in the presence of severe occlusion and large disparity. Experimental results on both synthetic and real-world datasets demonstrate the validity and superiority of the proposed method over SOTA methods.}
}
@article{THAKARE2023109567,
title = {RareAnom: A Benchmark Video Dataset for Rare Type Anomalies},
journal = {Pattern Recognition},
volume = {140},
pages = {109567},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109567},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002674},
author = {Kamalakar Vijay Thakare and Debi Prosad Dogra and Heeseung Choi and Haksub Kim and Ig-Jae Kim},
keywords = {Video anomaly detection, Unsupervised learning, Temporal encoding, Rare anomalies, Anomaly classification},
abstract = {Existing video anomaly detection methods and datasets suffer from restricted anomaly categories containing single-source (CCTV) videos recorded in controlled environment, inadequate annotations, and lack of adequate supervision. To mitigate these problems, we introduce a new dataset (RareAnom) containing 17 rare types of real-world anomalies (2200 videos) recorded using multiple sources (e.g., CCTV, handheld cameras, dash-cams, and mobile phones) with rich temporal annotations. A new fully unsupervised anomaly detection and classification method has been proposed. It has three stages: training of a 3D Convolution Autoencoder using pseudo-labelled video segments, anomaly detection using latent features, and classification. Unlike the existing datasets, we have benchmarked RareAnom using three levels of supervision: fully, weakly, and unsupervised. It has been compared with UCF-Crime and XD-Violence datasets. The proposed anomaly detection and classification method beats the latest unsupervised methods by 4.49%, 8.66%, and 6.77% on RareAnom, UCF-Crime, and XD-violence datasets, respectively.}
}
@article{ZHU2023109543,
title = {Learning relation-based features for fine-grained image retrieval},
journal = {Pattern Recognition},
volume = {140},
pages = {109543},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109543},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002431},
author = {Yingying Zhu and Gang Cao and Zhanyuan Yang and Xiufan Lu},
keywords = {Fine-grained image retrieval, Implicit relation, Feature aggregation},
abstract = {Fine-Grained Image Retrieval (FGIR) is a fundamental yet challenging task that has recently received considerable attention. However, two critical issues remain unresolved. On the one hand, convolutional neural networks (CNNs) trained with image-level labels tend to focus on the most discriminative image patches but overlook the implicit relation among them. On the other hand, existing large models developed for FGIR are computationally expensive and difficult to learn discriminative features. To address these issues without additional object-level annotations or localization sub-networks, we propose a novel unified framework for fine-grained image retrieval. Specifically, a novel Relation-based Convolutional Descriptor Aggregation (RCDA) method for extracting subtle yet discriminative features from fine-grained images is introduced. The RCDA method consists of a local feature generation network and a relation extraction (RE) module that models both explicit information and implicit relations. The explicit information is modeled by computing feature similarities, while the implicit relation is mined via an expectation-maximization algorithm. Moreover, we further leverage the knowledge distillation technique to optimize the parameters of the feature generation network and speed up the fine-tuning procedure by transferring knowledge from a large model to a smaller model. Experimental results on three benchmark datasets (CUB-200-2011, Stanford-Car and FGVC-Aircraft) demonstrate that the proposed method not only achieves a significant improvement over baseline models but also outperforms state-of-the-art methods by a large margin (6.4%, 1.3%, 23.2%, respectively).}
}
@article{WANG2023109559,
title = {Identifying effective trajectory predictions under the guidance of trajectory anomaly detection model},
journal = {Pattern Recognition},
volume = {140},
pages = {109559},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109559},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002595},
author = {Chunnan Wang and Chen Liang and Xiang Chen and Hongzhi Wang},
keywords = {Stochastic trajectory prediction, Anomaly detection, Trajectory anomaly detection, Automated machine learning},
abstract = {Trajectory Prediction (TP) is an important research topic in computer vision and robotics fields. Recently, many stochastic TP models have been proposed to deal with this problem and have achieved better performance than the traditional models with deterministic trajectory outputs. However, these stochastic models can generate a number of future trajectories with different qualities. They are lack of self-evaluation ability, that is, to examine the rationality of their prediction results, thus failing to guide users to identify high-quality ones from their candidate results. This hinders them from playing their best in real applications. In this paper, we make up for this defect and propose TPAD, a novel TP evaluation method based on the trajectory Anomaly Detection (AD) technique. In TPAD, we firstly combine the Automated Machine Learning (AutoML) technique and the experience in the AD and TP field to automatically design an effective trajectory AD model. Then, we utilize the learned trajectory AD model to examine the rationality of the predicted trajectories, and screen out good TP results for users. Extensive experimental results demonstrate that TPAD can effectively identify near-optimal prediction results, improving stochastic TP models’ practical application effect.}
}
@article{GAO2023109535,
title = {Transfer learning on stratified data: joint estimation transferred from strata},
journal = {Pattern Recognition},
volume = {140},
pages = {109535},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109535},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002352},
author = {Yimiao Gao and Yuehan Yang},
keywords = {Transfer learning, Stratified data, Penalized regression, Semiparametric regression},
abstract = {This paper studies the target model with the help of auxiliary models from different but possibly related groups. Inspired by transfer learning, we propose a method called joint estimation transferred from strata (JETS). To obtain a sparse solution, JETS constructs a penalized framework combining a term that penalizes the target model and an additional term that penalizes the differences between auxiliary models and the target model. In this way, JETS overcomes the challenge caused by the limited samples in high-dimensional study, and obtains stable and accurate estimates regardless of whether auxiliary samples contain noisy information. We demonstrate that this method enjoys the computational advantage of the traditional methods such as the lasso. During simulations and applications, the proposed method is compared with several existing methods and JETS outperforms others.}
}
@article{ZHAO2023109579,
title = {RA-YOLOX: Re-parameterization align decoupled head and novel label assignment scheme based on YOLOX},
journal = {Pattern Recognition},
volume = {140},
pages = {109579},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109579},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002790},
author = {Zuopeng Zhao and Chen He and Guangming Zhao and Jie Zhou and Kai Hao},
keywords = {Object detection, YOLO series, Decoupled head, Label assignment},
abstract = {YOLOX is a state-of-the-art one-stage object detection model for real-time applications that employs a decoupled head and advanced label assignment. Despite its impressive performance, YOLOX has limitations that prevent it from achieving optimal accuracy in real-time settings. To improve these limitations, we propose a new approach called re-parameterization align YOLOX (RA-YOLOX). Our approach employs a novel re-parameterization align decoupled head to align the classification and regression tasks, enhancing the learning of connection information between classification and regression. In addition, we propose a novel label assignment(LA) scheme that effectively defines positive and negative samples and precisely designs loss weight function. Our LA scheme enables the detector to focus on high-quality positive samples and filter out low-quality positive samples during training. We provide three sizes of lite models, namely RA-YOLOX-s, RA-YOLOX-tiny, and RA-YOLOX-nano, all of which outperform YOLOX models of similar size by an average precision of 2.3%, 1.5%, and 1.7%, respectively, on the MS COCO-2017 validation set, demonstrating the efficacy of our approach. Our code is available at github.com/hcmyhc/RA-YOLOX.}
}
@article{CHEN2023109527,
title = {Towards Automatic Model Compression via a Unified Two-Stage Framework},
journal = {Pattern Recognition},
volume = {140},
pages = {109527},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109527},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002273},
author = {Weihan Chen and Peisong Wang and Jian Cheng},
keywords = {Deep neural networks, Model compression, Quantization, Pruning},
abstract = {Deep Neural Networks have become ubiquitous in various domains. Meanwhile, the problems of massive storage and computation costs have hindered the deployment of these models to real-world applications. This paper proposes a novel and unified two-stage framework for automatic model compression. To determine the compression ratio of each layer, we improve the optimization from two aspects. First, to predict the performance of each compression policy, we propose Dynamic BN, which improves the correlation significantly with little computation overhead. Second, to search for the compression ratio allocation, we propose an efficient and hyperparameter-free solving algorithm based on the proposed Hessian matrix approximation and Knapsack problem reformulation. Moreover, comprehensive experiments and analyses are conducted on the CIFAR-100&ImageNet datasets and various network architectures to demonstrate its performance advantages over existing model compression methods under the quantization-only, pruning-only, and pruning-quantization settings.}
}
@article{TABEALHOJEH2023109563,
title = {RMAML: Riemannian meta-learning with orthogonality constraints},
journal = {Pattern Recognition},
volume = {140},
pages = {109563},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109563},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002637},
author = {Hadi Tabealhojeh and Peyman Adibi and Hossein Karshenas and Soumava Kumar Roy and Mehrtash Harandi},
keywords = {Meta-learning, Geometry-aware optimization, Riemannian manifolds, Few-shot image classification},
abstract = {Meta-learning is the core capability that enables intelligent systems to rapidly generalize their prior experience to learn new tasks. In general, the optimization-based methods formalize the meta-learning as a bi-level optimization problem, that is a nested optimization framework, in which meta-parameters are optimized (or learned) at the outer-level, while the inner-level optimizes the task-specific parameters. In this paper, we introduce RMAML, a meta-learning method that enforces orthogonality constraints to the bi-level optimization problem. We develop a geometry aware framework that generalizes the bi-level optimization problem to the Riemannian (constrained) setting. Using the Riemannian operations such as orthogonal projection, retraction and parallel transport, the bi-level optimization is reformulated so that it respects the Riemannian geometry. Moreover, we observe that a superior stable optimization and an improved generalization ability can be achieved when the parameters and meta-parameters of the method are modeled using a Stiefel Manifold. We empirically show that RMAML can easily reach competitive performances against several state of the art algorithms for few-shot classification and consistently outperforms its Euclidean counterpart, MAML. For example, by using the geometry of the Stiefel manifold to structure the fully-connected layers in a deep neural network, a 7% increase in single-domain few-shot classification accuracy is achieved. For the cross-domain few-shot learning, RMAML outperforms MAML by up to 9% of accuracy. Our ablation study also demonstrates the effectiveness of RMAML over MAML in terms of higher accuracy with a reduced number of tasks and (or) inner-level updates.}
}
@article{CAO2023109542,
title = {GraphRevisedIE: Multimodal information extraction with graph-revised network},
journal = {Pattern Recognition},
volume = {140},
pages = {109542},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109542},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300242X},
author = {Panfeng Cao and Jian Wu},
keywords = {Document information extraction, Graph convolutional network, Transformer},
abstract = {Key information extraction (KIE) from visually rich documents (VRD) has been a challenging task in document intelligence because of not only the complicated and diverse layouts of VRD that make the model hard to generalize but also the lack of methods to exploit the multimodal features in VRD. In this paper, we propose a light-weight model named GraphRevisedIE that effectively embeds multimodal features such as textual, visual, and layout features from VRD and leverages graph revision and graph convolution to enrich the multimodal embedding with global context. Extensive experiments on multiple real-world datasets show that GraphRevisedIE generalizes to documents of varied layouts and achieves comparable or better performance compared to previous KIE methods. We also publish a business license dataset that contains both real-life and synthesized documents to facilitate research of document KIE.}
}
@article{ALFARO2023109590,
title = {Pairwise learning for the partial label ranking problem},
journal = {Pattern Recognition},
volume = {140},
pages = {109590},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109590},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002911},
author = {Juan C. Alfaro and Juan A. Aledo and José A. Gámez},
keywords = {Preference learning, (Partial) label ranking, Supervised classification, Pairwise decomposition, Optimal bucket order problem},
abstract = {The partial label ranking problem is a particular preference learning scenario that focuses on learning preference models from data, such that they predict a complete ranking with ties defined over the values of the class variable for a given input instance. This work proposes to transform the rankings into preference relations among pairs of class labels and to learn a standard classifier for each of them. This classifier is then used to estimate the probability of each event from the preference relation between the two compared class labels. Finally, the probabilities obtained for each preference comparison are used to compute a preference matrix utilized to solve the corresponding rank aggregation problem and so obtain the ranking among all the class labels. The experimental evaluation shows that the proposed method is ranked ahead of competing algorithms in accuracy while obtaining similar CPU time results.}
}
@article{ZHOU2023109555,
title = {Cross-level Feature Aggregation Network for Polyp Segmentation},
journal = {Pattern Recognition},
volume = {140},
pages = {109555},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109555},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002558},
author = {Tao Zhou and Yi Zhou and Kelei He and Chen Gong and Jian Yang and Huazhu Fu and Dinggang Shen},
keywords = {Polyp segmentation, boundary-aware features, cross-level feature fusion, boundary aggregated module},
abstract = {Accurate segmentation of polyps from colonoscopy images plays a critical role in the diagnosis and cure of colorectal cancer. Although effectiveness has been achieved in the field of polyp segmentation, there are still several challenges. Polyps often have a diversity of size and shape and have no sharp boundary between polyps and their surrounding. To address these challenges, we propose a novel Cross-level Feature Aggregation Network (CFA-Net) for polyp segmentation. Specifically, we first propose a boundary prediction network to generate boundary-aware features, which are incorporated into the segmentation network using a layer-wise strategy. In particular, we design a two-stream structure based segmentation network, to exploit hierarchical semantic information from cross-level features. Furthermore, a Cross-level Feature Fusion (CFF) module is proposed to integrate the adjacent features from different levels, which can characterize the cross-level and multi-scale information to handle scale variations of polyps. Further, a Boundary Aggregated Module (BAM) is proposed to incorporate boundary information into the segmentation network, which enhances these hierarchical features to generate finer segmentation maps. Quantitative and qualitative experiments on five public datasets demonstrate the effectiveness of our CFA-Net against other state-of-the-art polyp segmentation methods. The source code and segmentation maps will be released at https://github.com/taozh2017/CFANet.}
}
@article{LYU2023109531,
title = {FETNet: Feature erasing and transferring network for scene text removal},
journal = {Pattern Recognition},
volume = {140},
pages = {109531},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109531},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002315},
author = {Guangtao Lyu and Kun Liu and Anna Zhu and Seiichi Uchida and Brian Kenji Iwana},
keywords = {Scene text removal, Text segmentation, One-stage, Self-attention},
abstract = {The scene text removal (STR) task aims to remove text regions and recover the background smoothly in images for private information protection. Most existing STR methods adopt encoder-decoder-based CNNs, with direct copies of the features in the skip connections. However, the encoded features contain both text texture and structure information. The insufficient utilization of text features hampers the performance of background reconstruction in text removal regions. To tackle these problems, we propose a novel Feature Erasing and Transferring (FET) mechanism to reconfigure the encoded features for STR in this paper. In FET, a Feature Erasing Module (FEM) is designed to erase text features. An attention module is responsible for generating the feature similarity guidance. The Feature Transferring Module (FTM) is introduced to transfer the corresponding features in different layers based on the attention guidance. With this mechanism, a one-stage, end-to-end trainable network called FETNet is constructed for scene text removal. In addition, to facilitate research on both scene text removal and segmentation tasks, we introduce a novel dataset, Flickr-ST, with multi-category annotations. A sufficient number of experiments and ablation studies are conducted on the public datasets and Flickr-ST. Our proposed method achieves state-of-the-art performance using most metrics, with remarkably higher quality scene text removal results.}
}
@article{LUO2023109598,
title = {Self-information of radicals: A new clue for zero-shot Chinese character recognition},
journal = {Pattern Recognition},
volume = {140},
pages = {109598},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109598},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002996},
author = {Guo-Feng Luo and Da-Han Wang and Xia Du and Hua-Yi Yin and Xu-Yao Zhang and Shunzhi Zhu},
keywords = {Chinese character recognition, Zero-shot learning, Self-information of radicals, Character uncertainty elimination, Radical information embedding},
abstract = {Zero-shot Chinese character recognition (ZSCCR) is an important research topic in Chinese character recognition as it attempts to recognize unseen Chinese characters. As basic components and mid-level representations, radicals are significant for ZSCCR. However, previous methods treat the importance of radicals equally, ignoring the different contributions of radicals in distinguishing characters. In this paper, we propose the self-information of radicals (SIR) to measure the importance of radicals in recognizing Chinese characters. The proposed SIR can be easily adopted by two commonly used radical-based ZSCCR frameworks, i.e., sequence matching based and attribute embedding based. For sequence matching based ZSCCR, we propose a novel Chinese character uncertainty elimination (CUE) framework to alleviate the radical sequence mismatch problem. For attribute embedding based ZSCCR, we propose a novel radical information embedding (RIE) method that can highlight the importance of indispensable radicals and weaken the influence of some unnecessary radicals. We conducted comprehensive experiments on the CASIA-HWDB, ICDAR2013, CTW datasets, and AHCDB datasets to evaluate the proposed method. Experiments show that our proposed methods can achieve superior performance to the state-of-the-art methods, which demonstrate the effectiveness and the high extensibility of the proposed SIR.}
}
@article{WANG2023109547,
title = {AA-trans: Core attention aggregating transformer with information entropy selector for fine-grained visual classification},
journal = {Pattern Recognition},
volume = {140},
pages = {109547},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109547},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002479},
author = {Qi Wang and JianJun Wang and Hongyu Deng and Xue Wu and Yazhou Wang and Gefei Hao},
keywords = {Fine-grained visual, Image classification, Vision transformer, Attention aggregator, Information entropy},
abstract = {The task of fine-grained visual classification (FGVC) is to distinguish targets from subordinate classifications. Since fine-grained images have the inherent characteristic of large inter-class variances and small intra-class variances, it is considered an extremely difficult task. Most existing approaches adopt CNN-based networks as feature extractors, which causes the extracted discriminative regions to contain most parts of the object in this way, thus failing to locate the really important parts. Recently, the vision transformer (ViT) has demonstrated its power on a wide range of image tasks, which uses an attention mechanism to capture global contextual information to establish a remote dependency on the target and thus extract more powerful features. Nevertheless, the ViT model still focuses more on global coarse-grained information rather than local fine-grained information, which may lead to its undesirable performance in fine-grained image classification. To this end, we redesigned an attention aggregating transformer (AA-Trans) to better capture minor differences among images by improving the ViT structure in this paper. In detail, we propose a core attention aggregator (CAA), which enables better information sharing between each transformer layer. Besides, we further propose an innovative information entropy selector (IES) to guide the network in acquiring discriminative parts of the image precisely. Extensive experiments show that our proposed model structure can achieve a new state-of-the-art performance on several mainstream datasets.}
}
@article{ZHU2023109589,
title = {FSConv: Flexible and separable convolution for convolutional neural networks compression},
journal = {Pattern Recognition},
volume = {140},
pages = {109589},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109589},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300290X},
author = {Yangyang Zhu and Luofeng Xie and Zhengfeng Xie and Ming Yin and Guofu Yin},
keywords = {CNNs compression, Representative feature maps, Redundant feature maps, Intrinsic information, Tiny hidden details},
abstract = {Because of limited computation resources, convolutional neural networks (CNNs) are difficult to deploy on mobile devices. To overcome this issue, many methods have successively reduced parameters in CNNs with the idea of removing redundancy among feature maps. We observe similarities between feature maps at the same layer but not complete consistency. Intuitively, the difference between similar feature maps is an essential ingredient for the success of CNNs. Therefore, we propose a flexible and separable convolution (FSConv) in a different perspective to embrace redundancy while requiring less computation, which can implicitly cluster feature maps into different clusters without introducing similarity measurements. Our proposed model extracts intrinsic information from the representative part through ordinary convolution in each cluster and reveals tiny hidden details from the redundant part through groupwise/depthwise convolution. Experimental results demonstrate that FSConv-equipped networks always perform better than previous state-of-the-art CNNs compression algorithms. Code is available at https://github.com/Clarkxielf/FSConv-Flexible-and-Separable-Convolution-for-Convolutional-Neural-Networks-Compression.}
}
@article{TIAN2023109548,
title = {Bi-Attention enhanced representation learning for image-text matching},
journal = {Pattern Recognition},
volume = {140},
pages = {109548},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109548},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002480},
author = {Yumin Tian and Aqiang Ding and Di Wang and Xuemei Luo and Bo Wan and Yifeng Wang},
keywords = {Image-text matching, Bi-attention, Polynomial loss},
abstract = {Image-text matching has become a research hotspot in recent years. The key point of image-text matching is to accurately measure the similarity between an image and a sentence. However, most existing methods either focus on the inter-modality similarities between regions in image and words in text or the intra-modality similarities within image regions or words, such that they cannot well exploit detailed correlations between images and texts. Furthermore, existing methods typically train their models using a triplet ranking loss, which relies on the similarity of randomly sampled triples. Since the weights of positive and negative samples are not adjusted, it cannot provide enough gradient information for training, resulting in slow convergence and limited performance. To address the above problems, we propose an image-text matching method named Bi-Attention Enhanced Representation Learning (BAERL). It builds a self-attention learning sub-network to exploit intra-modality correlations within image regions or words and a co-attention learning sub-network to exploit inter-modality correlations between image regions and words. Then, representations obtained by two sub-networks capture holistic correlations between images and texts. In addition, BAERL uses the self-similarity polynomial loss instead of triplet ranking loss to train the model. The self-similarity polynomial loss can adaptively assign appropriate weights to different pairs based on their similarity scores so as to further improve the retrieval performance. Experiments on two benchmark datasets demonstrate the superior performance of the proposed BAERL method over several state-of-the-art methods.}
}
@article{YANG2023109552,
title = {HeadPose-Softmax: Head pose adaptive curriculum learning loss for deep face recognition},
journal = {Pattern Recognition},
volume = {140},
pages = {109552},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109552},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002522},
author = {Jifan Yang and Zhongyuan Wang and Baojin Huang and Jinsheng Xiao and Chao Liang and Zhen Han and Hua Zou},
keywords = {Face recognition, Multi-view face, Curriculum learning, Pose-aware},
abstract = {Face recognition has been one of the most popular applications in the field of target detection. Currently, frontal faces can be easily detected, but multi-view face detection remains a difficult task because of various factors such as illumination, various poses, occlusions, and facial expressions. Margin-based loss functions are used to increase the feature margins between different classes, thus enhancing the discriminability of face recognition models, but the performance in face detection in complex scenes (e.g., high pitch angle face detection in surveillance environments) can be significantly degraded. Recently, the idea of a mining-based strategy to emphasize hard samples has been used to achieve good results in multi-view face detection. However, most of the existing methods do not explicitly emphasize samples based on their importance, resulting in the underutilization of hard samples. In this paper, we propose a curriculum learning loss function (HeadPose-Softmax) to classify the difficulty of a sample based on its facial pose, and embed the concept of curriculum learning into the loss function to implement a novel training strategy for deep face recognition. The loss function explicitly emphasizes the importance of the samples according to the different difficulty of each sample, which allows the model to make fuller use of hard samples, focus on learning pose invariant features, and improve the accuracy of the model in multi-view face detection tasks. Specifically, our HeadPose-Softmax dynamically adjusts the relative importance of the hard samples according to the pose angle of the face in the hard samples during the training phase. At each stage, different samples are assigned different importance according to their corresponding difficulty. Extensive experimental results under popular benchmarks show that our HeadPose-Softmax can enhance the accuracy of the model in multi-view face detection and outperform the state-of-the-art competitors.}
}
@article{LIU2023109568,
title = {Distributional and spatial-temporal robust representation learning for transportation activity recognition},
journal = {Pattern Recognition},
volume = {140},
pages = {109568},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109568},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002686},
author = {Jing Liu and Yang Liu and Wei Zhu and Xiaoguang Zhu and Liang Song},
keywords = {Transportation activity recognition, Multimodal sensing, Deep learning, Statistical feature, Spatial-temporal feature},
abstract = {Transportation activity recognition (TAR) provides valuable support for intelligent transportation applications, such as urban transportation planning, driving behavior analysis, and traffic prediction. There are many advantages of movable sensor-based TAR, and the key challenge is to capture salient features from segmented data for representing diverse patterns of activity. Although existing methods based on statistical information are efficient, they usually rely on domain knowledge to construct high-quality features manually. Likewise, the methods based on spatial-temporal relationships achieve good performance but fail to extract statistical features. The features extracted by these two methods have proven to be crucial for the classification of activity. How to combine them to acquire a more robust representation remains an open question. In this work, we introduce a novel parallel model named Distributional and Spatial-Temporal Robust Representation (DSTRR), which combines automatic learning of statistical, spatial, and temporal features into a unified framework. This model leads to three optimized subnets and thus obtains a robust representation specific to TAR. Extensive experiments performed on three public datasets show that DSTRR is a state-of-the-art method compared with the baseline methods. The results of ablation study and visualization not only demonstrate the effectiveness of each component in DSTRR, but also show the model remains robust to a wide range of parameter variations.}
}
@article{GAO2023109479,
title = {Multicycle disassembly-based decomposition algorithm to train multiclass support vector machines},
journal = {Pattern Recognition},
volume = {140},
pages = {109479},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109479},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001796},
author = {Tong Gao and Hao Chen},
keywords = {Multicycle disassembly-based decomposition algorithm, Multiclass support vector machine, Decomposition algorithm, Support vector machine training},
abstract = {Employing the classic optimization solver to train a multiclass support vector machine (SVM) requires prohibitive training time as the sample size and number of categories increase. It has been proposed to develop the corresponding decomposition algorithm (DA) as it is efficient for training SVMs. However, the dual problem of multiclass SVM comprises complex constraints that complicate DA design, so no corresponding DA has yet been developed. We propose a multicycle disassembly-based DA (MCD-DA) to efficiently solve the training problem of multiclass SVM. First, a graph model is constructed to re-express the constraints in multiclass SVM. Then, the original complex feasible region is partitioned into several simple sub-feasible regions, and multiple cycle-based disassembly strategies are designed to update the working variables analytically within each specific sub-feasible region. We mathematically verify that MCD-DA can stop within a finite number of cycle disassemblies and reach the τ-optimal solution satisfying relaxed Karush–Kuhn–Tucker conditions. Remarkably, MCD-DA as a universal decomposition algorithm can be used to solve many other SVM variants, including C-SVM, v-SVM, and one-class SVM. Experimental results using six UCI datasets demonstrate that MCD-DA outperforms typical optimization algorithms for more sample cases.}
}
@article{WANG2023109516,
title = {A uniform transformer-based structure for feature fusion and enhancement for RGB-D saliency detection},
journal = {Pattern Recognition},
volume = {140},
pages = {109516},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109516},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002169},
author = {Yue Wang and Xu Jia and Lu Zhang and Yuke Li and James H. Elder and Huchuan Lu},
keywords = {Saliency detection, RGB-D image, Transformer, Attention},
abstract = {RGB-D saliency detection integrates information from both RGB images and depth maps to improve the prediction of salient regions under challenging conditions. The key to RGB-D saliency detection is to fully mine and fuse information at multiple scales across the two modalities. Previous approaches tend to apply the multi-scale and multi-modal fusion separately via local operations, which fails to capture long-range dependencies. Here we propose a transformer-based structure to address this issue. The proposed architecture is composed of two modules: an Intra-modality Feature Enhancement Module (IFEM) and an Inter-modality Feature Fusion Module (IFFM). IFFM conducts a sufficient feature fusion by integrating features from multiple scales and two modalities over all positions simultaneously. IFEM enhances feature on each scale by selecting and integrating complementary information from other scales within the same modality before IFFM. We show that transformer is a uniform operation which presents great efficacy in both feature fusion and feature enhancement, and simplifies the model design. Extensive experimental results on five benchmark datasets demonstrate that our proposed network performs favorably against most state-of-the-art RGB-D saliency detection methods. Furthermore, our model is efficient for having relatively smaller FLOPs and model size compared with other methods.}
}
@article{KORBAN2023109595,
title = {Semantics-enhanced early action detection using dynamic dilated convolution},
journal = {Pattern Recognition},
volume = {140},
pages = {109595},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109595},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002960},
author = {Matthew Korban and Xin Li},
keywords = {Early action detection, Action semantics, Dilated convolutional network},
abstract = {This paper proposes a new pipeline to perform early action detection from skeleton-based untrimmed videos. Our pipeline includes two new technical components. The first is a new Dynamic Dilated Convolutional Network (DDCN), which supports dynamic temporal sampling and makes feature learning more robust against temporal scale variance in action sequences. The second is a new semantic referencing module, which uses identified objects in the scene and their co-existence relationship with actions to adjust the probabilities of inferred actions. Such semantic guidance can help distinguish many ambiguous actions, which is a core challenge in the early detection of incomplete actions. Our pipeline achieves state-of-the-art performance in early action detection in two widely used skeleton-based untrimmed video benchmarks. The source codes are available at: https://github.com/Powercoder64/DDCN_SRM.}
}
@article{ZHANG2023109544,
title = {Motif Entropy Graph Kernel},
journal = {Pattern Recognition},
volume = {140},
pages = {109544},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109544},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002443},
author = {Liang Zhang and Longqiang Yi and Yu Liu and Cheng Wang and Da Zhou},
keywords = {Graph representation, Motif entropy, Graph kernel, Wasserstein distance},
abstract = {Graph kernels have achieved excellent performance in graph classification tasks. In this paper, we propose a novel deep motif entropy graph kernel for the purpose of graph classification. For better capturing the differences between substructures, we gauge detailed information through a family of K-layer expansion motifs rooted at each node and combine the Weisfeiler-Lehman algorithm to subdivide motifs, which is further enhanced by motif entropy. Experiments on eight graph-structured datasets demonstrate that our method is able to outperform the state-of-the-art kernel methods for the tasks of graph classification.}
}
@article{QIAO2023109539,
title = {Hierarchical disentangling network for object representation learning},
journal = {Pattern Recognition},
volume = {140},
pages = {109539},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109539},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300239X},
author = {Shishi Qiao and Ruiping Wang and Shiguang Shan and Xilin Chen},
keywords = {Object understanding, Hierarchical learning, Representation disentanglement, Generative adversarial network, Network interpretability},
abstract = {An object can be described as the combination of primary visual attributes. Disentangling such underlying primitives is the long-term objective of representation learning. It is observed that categories have natural hierarchical characteristics, i.e., any two objects can share some common primitives at a particular category level while possess unique traits at another. However, previous works usually operate in a flat manner (i.e., at a particular level) to disentangle the representations of objects. Even though they may obtain the primitives to constitute objects as the categories at that level, their results are obviously not efficient and complete. In this paper, we propose a Hierarchical Disentangling Network (HDN) to exploit the rich hierarchical characteristics among categories to divide the disentangling process in a coarse-to-fine manner (i.e., level-wise), such that each level only focuses on learning the specific representations and finally the common and unique representations at all levels jointly constitute the raw object. Specifically, HDN is designed based on an encoder-decoder architecture. To simultaneously ensure the level-wise disentanglement and interpretability of the encoded representations, a novel hierarchical Generative Adversarial Network (GAN) is introduced. Quantitative and qualitative evaluations on popular object datasets validate the effectiveness of our method.}
}
@article{AGHAJANZADEH2023109587,
title = {Task weighting based on particle filter in deep multi-task learning with a view to uncertainty and performance},
journal = {Pattern Recognition},
volume = {140},
pages = {109587},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109587},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002881},
author = {Emad Aghajanzadeh and Tahereh Bahraini and Amir Hossein Mehrizi and Hadi Sadoghi Yazdi},
keywords = {Multi task learning, Uncertainty, Hyper-parameter tuning, Deep learning, Particle filter, Bayesian estimation},
abstract = {Recently multi-task learning (MTL) has been widely used in different applications to build more robust models by sharing knowledge across several related tasks. However, one challenge that arises is the variability in the learning pace of different tasks causing the inefficiency of naively training all tasks. Therefore, it is of great importance to consider some coefficients to balance tasks in the process of learning, but, due to the large search space and the significance of setting them properly, conventional search methods such as grid or random search are no longer effective. In this paper, we propose a learning mechanism for these coefficients based on the high efficiency of the particle filter (PF) algorithm to deal with nonlinear search problems. PF considers each state of the tasks’ coefficients as a particle and recursively converges coefficients to an optimum point. While in most previous works coefficients were evaluated to only increase performance, to address the recent concerns related to applying AI in real-world applications, we also incorporate uncertainty alongside our method to prevent learning coefficients leading to unstable outcomes. This mechanism is independent of the models main learning process and can be easily added to every learning system without changing its training algorithm. Extensive experiments on real-world data sets demonstrate the superiority of the proposed method over the state-of-the-art methods on both performance and uncertainty. We also proved the acceptable performance of the method using Cramer Rao lower bound theory.}
}
@article{HEDEGAARD2023109528,
title = {Continual spatio-temporal graph convolutional networks},
journal = {Pattern Recognition},
volume = {140},
pages = {109528},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109528},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002285},
author = {Lukas Hedegaard and Negar Heidari and Alexandros Iosifidis},
keywords = {Graph convolutional networks, Continual inference, Efficient deep learning, Skeleton-based action recognition},
abstract = {Graph-based reasoning over skeleton data has emerged as a promising approach for human action recognition. However, the application of prior graph-based methods, which predominantly employ whole temporal sequences as their input, to the setting of online inference entails considerable computational redundancy. In this paper, we tackle this issue by reformulating the Spatio-Temporal Graph Convolutional Neural Network as a Continual Inference Network, which can perform step-by-step predictions in time without repeat frame processing. To evaluate our method, we create a continual version of ST-GCN, CoST-GCN, alongside two derived methods with different self-attention mechanisms, CoAGCN and CoS-TR. We investigate weight transfer strategies and architectural modifications for inference acceleration, and perform experiments on the NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets. Retaining similar predictive accuracy, we observe up to 109× reduction in time complexity, on-hardware accelerations of 26×, and reductions in maximum allocated memory of 52% during online inference.}
}
@article{2023109616,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {140},
pages = {109616},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(23)00317-5},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003175}
}
@article{DAI2023109540,
title = {Global spatio-temporal synergistic topology learning for skeleton-based action recognition},
journal = {Pattern Recognition},
volume = {140},
pages = {109540},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109540},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002406},
author = {Meng Dai and Zhonghua Sun and Tianyi Wang and Jinchao Feng and Kebin Jia},
keywords = {Action recognition, Spatio-temporal synergistic, Skeleton, Topology learning},
abstract = {Compared to RGB video-based action recognition, skeleton-based action recognition algorithm has attracted much more attention due to being more lightweight, better generalization and robustness. The extraction of temporal and spatial features is a crucial factor for skeleton-based action recognition. However, existing feature extraction methods suffer from two limitations: (1) the isolated extraction of temporal and spatial feature cannot capture temporal feature connections among non-adjacent joints and (2) convolution-limited perceptual fields cannot capture global temporal features of joints effectively. In this work, we propose a global spatio-temporal synergistic feature learning module (GSTL), which generates global spatio-temporal synergistic topology of joints by spatio-temporal feature fusion. By further combining the GSTL with a temporal modeling unit, we develop a powerful global spatio-temporal synergistic topology learning network (GSTLN), and it achieves competitive performance with fewer parameters on three challenge datasets: NTU RGB + D, NTU RGB + D 120, and NW-UCLA.}
}
@article{DING2023109532,
title = {An enhanced vision transformer with wavelet position embedding for histopathological image classification},
journal = {Pattern Recognition},
volume = {140},
pages = {109532},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109532},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002327},
author = {Meidan Ding and Aiping Qu and Haiqin Zhong and Zhihui Lai and Shuomin Xiao and Penghui He},
keywords = {Histopathological image classification, Vision transformer, Convolutional neural network, Wavelet position embedding, External multi-head attention},
abstract = {Histopathological image classification is a fundamental task in pathological diagnosis workflow. It remains a huge challenge due to the complexity of histopathological images. Recently, hybrid methods combining convolutional neural networks(CNN) with vision transformers(ViT) are proposed to this field. These methods can well represent the global and local contextual information and achieve excellent classification performances. However, the downsampling operation like max-pooling which ignores the sampling theorem transmits the jagged artifacts into transformer, which would lead to an aliasing phenomenon. It makes the subsequent feature maps focus on the incorrect regions and influences the final classification results. In this work, we propose an enhanced vision transformer with wavelet position embedding to tackle this challenge. In particular, a wavelet position embedding module, which introduces the wave transform into position embedding, is employed to enhance the smoothness of discontinuous feature information by decomposing sequences into amplitude and phase in pathological feature maps. In addition, an external multi-head attention is proposed to replace self-attention in the transformer block with two linear layers. It reduces the cost of computation and excavates potential correlations between different samples. We evaluate the proposed method on three public histopathological classification challenging datasets, and perform a quantitative comparison with previous state-of-the-art methods. The results empirically demonstrate that our method achieves the best accuracy. Furthermore, it has the least parameters and a very low FLOPs. In conclusion, the enhanced vision transformer shows high classification performances and demonstrates significant potential for assisting pathologists in pathological diagnosis.}
}
@article{LIU2023109560,
title = {Improve Temporal Action Proposals using Hierarchical Context},
journal = {Pattern Recognition},
volume = {140},
pages = {109560},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109560},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002601},
author = {Qinying Liu and Zilei Wang and Shenghai Rong},
keywords = {Temporal action proposal, Contexts, Attention model},
abstract = {Temporal action proposal (TAP) aims to generate accurate candidates of action instances in an untrimmed video. It has been proved that contexts are critically important to this task. In this paper, we propose a novel hierarchical context network (HCN) to further explore the snippet-level and proposal-level contexts, which are used to improve the representations of snippets and proposals, respectively. First, we pinpoint that different scales of snippet-level contexts are not equally important for different action instances. To this end, we incorporate a novel gating mechanism into the U-Net structure to capture the content-adaptive snippet-level contexts. Second, to exploit the proposal-level contexts, we propose a task-specific self-attention model with high efficiency. By stacking multiple attention models, we can deeply explore the proposal-level contexts in a wide range. Finally, to leverage both levels of context, we equip HCN with three branches to evaluate proposals from local to global perspectives. Our experiments on the ActivityNet-1.3 and THUMOS14 datasets show that HCN significantly outperforms previous TAP methods. Additionally, further experiments demonstrate that our method can substantially improve the state-of-the-art action detection performance when combined with existing action classifiers.}
}
@article{QIU2023109580,
title = {Underestimation modification for intrinsic dimension estimation},
journal = {Pattern Recognition},
volume = {140},
pages = {109580},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109580},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002807},
author = {Haiquan Qiu and Youlong Yang and Hua Pan},
keywords = {Intrinsic dimension, Parameter selection, Estimation method, Underestimation modification, Smooth manifold},
abstract = {The intrinsic dimension is the dimension of the low-dimensional manifold where the high-dimensional data is located. Accurately estimating the intrinsic dimension of the data set is helpful for data-dimensionality reduction and preprocessing. Due to the unknown spatial distribution of data and the limited sample size of a dataset, estimation methods which only use distance information tend to underestimate the intrinsic dimension of dataset. To reduce the estimation complexity and improve the accuracy, two estimation algorithms based on ID(κ) are proposed, where κ is the scaling ratio of the neighborhood radius of the sample point. First, according to the selection criteria of parameter κ, an improved algorithm for selecting the optimal scaling ratio κ is proposed, which reduces the computational complexity and improves the stability of estimation. Second, using simulation datasets with the same sample size and known intrinsic dimensions, the relationship between the estimated dimension and the true intrinsic dimension is obtained, and an underestimation modification method for intrinsic dimension estimation is proposed. Results of comparative experiments on simulation and real datasets indicate that the underestimation modification algorithm has high estimation accuracy and robustness.}
}
@article{XIONG2023109549,
title = {A black-box reversible adversarial example for authorizable recognition to shared images},
journal = {Pattern Recognition},
volume = {140},
pages = {109549},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109549},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002492},
author = {Lizhi Xiong and Yue Wu and Peipeng Yu and Yuhui Zheng},
keywords = {Reversible adversarial example, Reversible data hiding, Prediction-error histogram},
abstract = {Shared images on the Internet are easily collected, classified, and analyzed by unauthorized commercial companies through Deep Neural Networks (DNNs). The illegal use of these data damages the rights and interests of authorized companies and individuals. How to ensure that network-shared data is legally used by authorized users and not used by unauthorized DNNs has become an urgent problem. Reversible Adversarial Example (RAE) provides an effective solution, which can mislead the classification of unauthorized DNNs and does not affect the authorized users. The existing RAE schemes assumed that we could know the parameters of the target model and thus generate reversible adversarial examples. However, model parameters are often protected to avoid leakage, increasing the difficulty of generating accurate RAEs. In this paper, we first propose a Black-box Reversible Adversarial Example (B-RAE) scheme to generate robust reversible adversarial examples. We aim to protect image privacy while maintaining data usability in real scenarios. Experimental results and analysis have demonstrated that the proposed B-RAE is more effective and robust compared with the existing schemes.}
}
@article{HU2023109569,
title = {A Noising-Denoising Framework for Point Cloud Upsampling via Normalizing Flows},
journal = {Pattern Recognition},
volume = {140},
pages = {109569},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109569},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002698},
author = {Xin Hu and Xin Wei and Jian Sun},
keywords = {Point cloud, Arbitrary ratio upsampling, Normalizing flows},
abstract = {Point cloud upsampling aims to generate dense and uniform point cloud from the sparse input point cloud. One challenge is how to flexibly upsample the sparse point cloud in arbitrary ratios, even without the given supervised high resolution point cloud. To address this challenge, we propose a noising-denoising framework, dubbed ND-PUFlow, for arbitrary 3D point cloud upsampling (3DPU) in supervised and self-supervised settings. It consists of two stages, i.e., dense noisy points generation and noisy points denoising via continuous normalizing flows (CNFs). In the first stage, noisy points are generated by adding noise to the input points. In the second stage, CNFs move each noisy point to the underlying surface, forming a dense and clean point cloud. Extensive experiments show that our method is competitive in both supervised and self-supervised settings, and in most cases, it achieves the best performance on benchmark datasets for 3DPU.}
}
@article{WANG2023109596,
title = {GSAL: Geometric structure adversarial learning for robust medical image segmentation},
journal = {Pattern Recognition},
volume = {140},
pages = {109596},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109596},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002972},
author = {Kun Wang and Xiaohong Zhang and Yuting Lu and Wei Zhang and Sheng Huang and Dan Yang},
keywords = {Medical image segmentation, Geometric structure learning, Adversarial learning, Computer-Aided diagnosis (CAD)},
abstract = {Automatic medical image segmentation plays a crucial role in clinical diagnosis and treatment. However, it is still a challenging task due to the complex interior characteristics (e.g., inconsistent intensity, low contrast, texture heterogeneity) and ambiguous external boundary structures. In this paper, we introduce a novel geometric structure learning mechanism (GSLM) to overcome the limitations of existing segmentation models that lack learning ”focus, path, and difficulty.” The geometric structure in this mechanism is jointly characterized by the skeleton-like structure extracted by the mask distance transform (MDT) and the boundary structure extracted by the mask distance inverse transform (MDIT). Among them, the skeleton-like and boundary pay attention to the trend of interior characteristics consistency and external structure continuity, respectively. With this idea, we design GSAL, a novel end-to-end geometric structure adversarial learning for robust medical image segmentation. GSAL has four components: a geometric structure generator, which yields the geometric structure to learn the most discriminative features that preserve interior characteristics consistency and external boundary structure continuity, skeleton-like and boundary structure discriminators, which enhance and correct the characterization of internal and external geometry to mutually promote the capture of global contextual dependencies, and a geometric structure fusion sub-network, which fuses the two complementary and refined skeleton-like and boundary structures to generate the high-quality segmentation results. The proposed approach has been successfully applied to three different challenging medical image segmentation tasks, including polyp segmentation, COVID-19 lung infection segmentation, and lung nodule segmentation. Extensive experimental results demonstrate that the proposed GSAL achieves favorably against most state-of-the-art methods under different evaluation metrics. The code is available at: https://github.com/DLWK/GSAL.}
}
@article{KONG2023109545,
title = {Low-Tubal-Rank tensor recovery with multilayer subspace prior learning},
journal = {Pattern Recognition},
volume = {140},
pages = {109545},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109545},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002455},
author = {Weichao Kong and Feng Zhang and Wenjin Qin and Jianjun Wang},
keywords = {Tensor robust principal component analysis, Tensor completion, Multilayer subspace prior information, ADMM, T-SVD},
abstract = {Currently, low-rank tensor recovery employing the subspace prior information is an emerging topic, which has attracted considerable attention. However, existing studies cannot flexibly and fully utilize the accessible subspace prior information, thereby leading to suboptimal restored performance. Aiming at addressing this issue, based on the tensor singular value decomposition (t-SVD), this article presents a novel strategy that integrates more than two layers of subspace knowledge about columns and rows of target tensor into one unified recovery framework. Specially, we first design a multilayer subspace prior learning scheme, and then apply it to two common low-rank tensor recovery problems, i.e., tensor completion and tensor robust component principal analysis. Crucially, we prove that our approach can achieve exact recovery of tensors under a significantly weaker incoherence assumption than the analogous conditions previously proposed. Furthermore, two efficient algorithms with convergence guarantees based on alternating direction method of multipliers (ADMM) are proposed to solve the corresponding models. The experimental results on synthetic and real tensor data show that the proposed algorithms outperform other state-of-the-art algorithms in terms of both qualitative and quantitative metrics.}
}
@article{YANG2023109521,
title = {Discriminative semi-supervised learning via deep and dictionary representation for image classification},
journal = {Pattern Recognition},
volume = {140},
pages = {109521},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109521},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002212},
author = {Meng Yang and Jie Ling and Jiaming Chen and Mao Feng and Jian Yang},
keywords = {Semi-supervised, Deep and dictionary learning, Image classification},
abstract = {Supervised dictionary learning and deep learning have achieved promising performance in the classification task. However, in many real-world applications there usually exist very limited labeled training samples, although abundant unlabeled data is relatively easy to collect. How to effectively exploit the discrimination of unlabeled data is still an open question, hence semi-supervised learning has attracted much attention from wide fields. Semi-supervised deep feature learning has well exploited the feature discrimination from only the discriminative viewpoint, while dictionary representation-based classification has also been applied to semi-supervised learning but with shallow features. In this paper, we propose a novel discriminative semi-supervised learning via deep and dictionary representation (DSSLDDR), which jointly utilizes the discrimination of dictionary representation for data reconstruction and the distinguishing feature of each sample. To exploit the powerful discrimination of dictionary representation, class-specific dictionaries are required to discriminatively reconstruct a sample, with the reconstruction error to predict the sample’s class label. To exploit the semantic information, the deep neural network extracts discriminative features by using multiple nonlinear transformations to generate the powerful descriptor. Then the class-specific dictionary learning and deep network learning are integrated together to conduct more accurate class estimation for unlabeled data and learn a more discriminative classifier, where an entropy regularization is designed to balance and control the class estimation of unlabeled data. Furthermore, we propose the DSSLDDR++, the extension model of DSSLDDR based on consistency/contrastive learning to further improve the accuracy of class estimation for unlabeled data, making a more powerful semi-supervised learning classifier. Extensive experiments on benchmark datasets show the effectiveness of the proposed methods.}
}
@article{XU2023109588,
title = {MoCA: Incorporating domain pretraining and cross attention for textbook question answering},
journal = {Pattern Recognition},
volume = {140},
pages = {109588},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109588},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002893},
author = {Fangzhi Xu and Qika Lin and Jun Liu and Lingling Zhang and Tianzhe Zhao and Qi Chai and Yudai Pan and Yi Huang and Qianying Wang},
keywords = {Textbook question answering, Multimodal, Pretraining, Attention},
abstract = {Textbook Question Answering (TQA) is a complex multimodal task to infer answers given large context descriptions and abundant diagrams. Compared with Visual Question Answering (VQA), TQA contains a large number of uncommon terminologies and various diagram inputs. It brings new challenges to the representation capability of language model for domain-specific spans. Also, it requires the model to take fully advantage of the complementary information of different diagram types, which pushes the multimodal fusion task to a more complex level. To tackle the above issues, we propose a novel model named MoCA, which incorporates Multi-stage domain pretraining and Cross-guided multimodal Attention for the TQA task. Firstly, we introduce a multi-stage domain pretraining module to conduct unsupervised post-pretraining with a span mask strategy and supervised pre-finetune. Especially for domain post-pretraining, we propose a heuristic generation algorithm to employ the terminology corpus. Secondly, to fully consider the rich inputs of context and diagrams, we propose a cross-guided multimodal attention mechanism to update the features of text, question diagram and instructional diagram based on a progressive strategy. Further, a dual gating mechanism is adopted to improve the model ensemble of three background retrievals. The experimental results show the superiority of our model, which outperforms the state-of-the-art methods on the validation and test split respectively. Also, ablation and comparison experiments verify the effectiveness of each module proposed in our model.}
}
@article{ZHANG2023109537,
title = {Line graph contrastive learning for link prediction},
journal = {Pattern Recognition},
volume = {140},
pages = {109537},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109537},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002376},
author = {Zehua Zhang and Shilin Sun and Guixiang Ma and Caiming Zhong},
keywords = {Line graph, Contrastive learning, Link prediction, Node classification, Mutual information},
abstract = {Link prediction tasks focus on predicting possible future connections. Most existing researches measure the likelihood of links by different similarity scores on node pairs and predict links between nodes. However, the similarity-based approaches have some challenges in information loss on nodes and generalization ability on similarity indexes. To address the above issues, we propose a Line Graph Contrastive Learning (LGCL) method to obtain rich information with multiple perspectives. LGCL obtains a subgraph view by h-hop subgraph sampling with target node pairs. After transforming the sampled subgraph into a line graph, the link prediction task is converted into a node classification task, which graph convolution progress can learn edge embeddings from graphs more effectively. Then we design a novel cross-scale contrastive learning framework on the line graph and the subgraph to maximize the mutual information of them, so that fuses the structure and feature information. The experimental results demonstrate that the proposed LGCL outperforms the state-of-the-art methods and has better performance on generalization and robustness.}
}
@article{GUO2023109565,
title = {Robust semi-supervised multi-view graph learning with sharable and individual structure},
journal = {Pattern Recognition},
volume = {140},
pages = {109565},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109565},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002650},
author = {Wei Guo and Zhe Wang and Wenli Du},
keywords = {Semi-supervised learning, Multi-view learning, Clean data, Manifold structure},
abstract = {The construction of a high-quality multi-view consensus graph is key to graph-based semi-supervised multi-view learning (GSSMvL) methods. However, most existing GSSMvL methods explore sample relationships in the original multi-view feature space, which obtains a contaminated graph that cannot reveal the underlying manifold structure of the samples. Moreover, traditional GSSMvL methods fail to explore the diverse structures of multi-view features, which may lose their complementary information and lead to a suboptimal graph. In this paper, we propose a novel unified robust semi-supervised multi-view graph learning framework based on the sharable and individual structure (RSSMvSI), which can eliminate the influence of noise and exploit the knowledge of multi-view data in a reasonable manner. Specifically, we first learn clean data by manipulating sparse noise with l2,1 norm. We then simultaneously explore the sharable and individual self-representation subspace on the learned clean multi-view data. The key point is that noisy data does not participate in subspace learning, which improves the robustness of the proposed method. By constructing the optimal consensus graph with the learned sharable and individual subspace, RSSMvSI can better utilize the complementary information of multi-view data and approximate the manifold structure of samples. To the best of our knowledge, this is the first attempt to learn the self-representation subspace on recovered multi-view clean data. Extensive experiments on various real-world multi-view datasets demonstrate the superiority and robustness against state-of-the-art methods.}
}
@article{DECAUX2023109529,
title = {Semi-automatic muscle segmentation in MR images using deep registration-based label propagation},
journal = {Pattern Recognition},
volume = {140},
pages = {109529},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109529},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002297},
author = {Nathan Decaux and Pierre-Henri Conze and Juliette Ropars and Xinyan He and Frances T. Sheehan and Christelle Pons and Douraied {Ben Salem} and Sylvain Brochard and François Rousseau},
keywords = {Semi-automatic segmentation, Musculoskeletal system, Label propagation, Deep registration},
abstract = {Fully automated approaches based on convolutional neural networks have shown promising performances on muscle segmentation from magnetic resonance (MR) images, but still rely on an extensive amount of training data to achieve valuable results. Muscle segmentation for pediatric and rare diseases cohorts is therefore still often done manually. Producing dense delineations over 3D volumes remains a time-consuming and tedious task, with significant redundancy between successive slices. In this work, we propose a segmentation method relying on registration-based label propagation, which provides 3D muscle delineations from a limited number of annotated 2D slices. Based on an unsupervised deep registration scheme, our approach ensures the preservation of anatomical structures by penalizing deformation compositions that do not produce consistent segmentation from one annotated slice to another. Evaluation is performed on MR data from lower leg and shoulder joints. Results demonstrate that the proposed semi-automatic multi-label segmentation model outperforms state-of-the-art techniques.}
}
@article{GUO2023109508,
title = {Sensitivity pruner: Filter-Level compression algorithm for deep neural networks},
journal = {Pattern Recognition},
volume = {140},
pages = {109508},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109508},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300208X},
author = {Suhan Guo and Bilan Lai and Suorong Yang and Jian Zhao and Furao Shen},
keywords = {Filter pruning, Saliency-based pruning, End-to-end pruning framework, Sampling bias},
abstract = {As neural networks get deeper for better performance, the demand for deployable models on resource-constrained devices also grows. In this work, we propose eliminating less sensitive filters to compress models. The previous method evaluates neuron importance using the connection matrix gradient in a single shot. To mitigate the sampling bias, we integrate this measure into the previously proposed “pruning while fine-tuning” framework. Besides classification errors, we introduce the difference between the learned and the single-shot strategy as the second loss component with a self-adjustive hyper-parameter that balances the training goal between improving accuracy and pruning more filters. Our Sensitivity Pruner (SP) adapts the unstructured pruning saliency metric to structured pruning tasks and enables the strategy to be derived sequentially to accommodate the updating sparsity. Experimental results demonstrate that SP significantly reduces the computational cost and the pruned models give comparable or better performance on CIFAR10, CIFAR100, and ILSVRC-12 datasets.}
}
@article{SINGHA2023109557,
title = {A real-time semantic segmentation model using iteratively shared features in multiple sub-encoders},
journal = {Pattern Recognition},
volume = {140},
pages = {109557},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109557},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002571},
author = {Tanmay Singha and Duc-Son Pham and Aneesh Krishna},
keywords = {Semantic segmentation, Deep convolution neural networks, Multi-encoder, Decoder, Feature scaling, Feature aggregation, Feature reuse, Resource-constrained applications, Mobile devices},
abstract = {Recent studies show a significant growth in semantic segmentation. However, many semantic segmentation models still have a large number of parameters, making them unsuitable for resource-constrained embedded devices. To address this issue, we propose an efficient Shared Feature Reuse Segmentation (SFRSeg) model containing several novelties: a new yet effective shared-branch multiple sub-encoders design, a context mining module and a semantic aggregating module for better context granularity. In particular, our shared-branch approach improves the entire feature hierarchy by sharing the spatial and context knowledge in both shallow and deep branches. After every shared point in each sub-encoder, a proposed cascading context mining (CCM) module is deployed to filter out the noisy spatial details from the feature maps and provides a diverse size of receptive fields for capturing the latent context between multi-scale geometric shapes in the scene. To overcome the gradient vanishing issue at the early stage, we reduce the number of layers in the first sub-encoder and employ a unique multiple sub-encoders design which reprocesses the rich global feature maps through multiple sub-encoders for better feature refinement. Later, the rich semantic features generated by the efficient sub-encoders at different levels are fused by the proposed Hybrid Path Attention Semantic Aggregation (HPA-SA) module that effectively reduces the semantic gap between feature maps at different levels and alleviate the well-known boundary degeneration effect. To make it computationally efficient for resource-constrained embedded devices, a series of lightweight methods such as a lightweight encoder, a squeeze-and-excitation design, separable convolution filters, channel reduction (CR) are carefully exploited. With an exceptional performance on Cityscapes (70.6% test mIoU) and CamVid (74.7% test mIoU) data sets, the proposed model is shown to be superior over existing light real-time semantic segmentation models whilst having only 1.6 million parameters.}
}
@article{KAPOOR2023109505,
title = {Aeriform in-action: A novel dataset for human action recognition in aerial videos},
journal = {Pattern Recognition},
volume = {140},
pages = {109505},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109505},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002054},
author = {Surbhi Kapoor and Akashdeep Sharma and Amandeep Verma and Sarbjeet Singh},
keywords = {UAV, Dataset, Human detection, Human action recognition, Aerial videos},
abstract = {Human actions being diverse in nature cannot be generalized, thus making it quite difficult to train a machine to recognize such diversified actions. This challenge is further compounded by the lack of availability of datasets for aerial surveillance, as collecting and annotating a large dataset is a formidable task. This paper aims to solve the problem of data scarcity by introducing a new dataset, Aeriform in-action for recognizing human actions from aerial videos. The proposed dataset consists of 32 high-resolution videos containing 13 action classes with 55,477 frames (without augmentation) and almost 400,000 annotations. It includes complex and aggressive actions such as kicking and punching, as well as drone signaling actions like waving and handshaking. The dataset also includes human-object interactions like carrying and reading. In addition to the dataset, this paper also presents a two-step deep learning framework for recognizing human actions based on the integration of human detection and action recognition module. The action recognition module adopts a modified version of the ResNeXt101 architecture (M-ResNext101) to recognize human actions in aerial videos. The performance of the proposed M-ResNext101 model is compared with 13 other deep learning models, and it outperforms all of them with an accuracy of 76.44% on the test data. The proposed dataset for human action recognition in aerial videos is available on https://surbhi-31.github.io/Aeriform-in-action/.}
}
@article{CHO2023109541,
title = {Ambiguity-aware robust teacher (ART): Enhanced self-knowledge distillation framework with pruned teacher network},
journal = {Pattern Recognition},
volume = {140},
pages = {109541},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109541},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002418},
author = {Yucheol Cho and Gyeongdo Ham and Jae-Hyeok Lee and Daeshik Kim},
keywords = {Knowledge distillation, Self-knowledge distillation, Network pruning, Teacher-student model, Long-tail samples, Ambiguous samples, Sample ambiguity, Data augmentation},
abstract = {Self-knowledge distillation (self-KD) methods, which use a student model itself as the teacher model instead of a large and complex teacher model, are currently a subject of active study. Since most previous self-KD approaches relied on the knowledge of a single teacher model, if the teacher model incorrectly predicted confusing samples, poor-quality knowledge was transferred to the student model. Unfortunately, natural images are often ambiguous for teacher models due to multiple objects, mislabeling, or low quality. In this paper, we propose a novel knowledge distillation framework named ambiguity-aware robust teacher knowledge distillation (ART-KD) that provides refined knowledge, that reflects the ambiguity of the samples with network pruning. Since the pruned teacher model is simply obtained by copying and pruning the teacher model, re-training process is unnecessary in ART-KD. The key insight of ART-KD lies in the predictions of a teacher model and pruned teacher model for ambiguous samples providing different distributions with low similarity. From these two distributions, we obtain a joint distribution considering the ambiguity of the samples as teacher’s knowledge for distillation. We comprehensively evaluate our method on public classification benchmarks, as well as more challenging benchmarks for fine-grained visual recognition (FGVR), achieving much superior performance to state-of-the-art counterparts.}
}
@article{HUANG2023109533,
title = {Reciprocal normalization for domain adaptation},
journal = {Pattern Recognition},
volume = {140},
pages = {109533},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109533},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002339},
author = {Zhiyong Huang and Kekai Sheng and Ke Li and Jian Liang and Taiping Yao and Weiming Dong and Dengwen Zhou and Xing Sun},
keywords = {Domain adaptation, Feature normalization, Deep neural network},
abstract = {Batch normalization (BN) is widely used in modern deep neural networks, which has been shown to represent the domain-related knowledge, and thus is ineffective for cross-domain tasks like unsupervised domain adaptation (UDA). Existing BN variant methods aggregate source and target domain knowledge in the same channel in normalization module. However, the misalignment between the features of corresponding channels across domains often leads to a sub-optimal transferability. In this paper, we exploit the cross-domain relation and propose a novel normalization method, Reciprocal Normalization (RN). Specifically, RN first presents a Reciprocal Compensation (RC) module to acquire the compensatory for each channel in both domains based on the cross-domain channel-wise correlation. Then RN develops a Reciprocal Aggregation (RA) module to adaptively aggregate the feature with its cross-domain compensatory components. As an alternative to BN, RN is more suitable for UDA problems and can be easily integrated into popular domain adaptation methods. Experiments show that the proposed RN outperforms existing normalization counterparts by a large margin and helps state-of-the-art adaptation approaches achieve better results. The source code is available on https://github.com/Openning07/reciprocal-normalization-for-DA.}
}
@article{SUN2023109561,
title = {Exemplar-free class incremental learning via discriminative and comparable parallel one-class classifiers},
journal = {Pattern Recognition},
volume = {140},
pages = {109561},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109561},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002613},
author = {Wenju Sun and Qingyong Li and Jing Zhang and Danyu Wang and Wen Wang and YangLi-ao Geng},
keywords = {Incremental learning, Continual learning, Lifelong learning, One-class learning, Image classification},
abstract = {The exemplar-free class incremental learning (IL) requires classification models to learn new-class knowledge incrementally without retaining any old samples. Recently, the IL framework based on parallel one-class classifiers (POC) has demonstrated promising performance. It trains a one-class classifier (OCC) for each category and thus is immune to the catastrophic forgetting problem. However, the single-class training strategy may incur weak discriminability and low comparability between different classifiers in POC. To meet this challenge, we propose a new IL framework, referred to as Discriminative and Comparable Parallel One-class Classifiers (DCPOC). Instead of ordinary OCCs (e.g., deep SVDD) used in other POC methods, DCPOC adopts variational auto-encoders (VAE) as OCCs because VAEs can be used not only to identify classes for given samples but also to generate pseudo samples for the trained classes. With this advantage, DCPOC trains a new-class VAE in contrast with the old-class VAEs, which benefits the new-class VAE to reconstruct better for new-class samples but worse for old-class pseudo samples, thus enhancing the comparability. Furthermore, DCPOC introduces a hinge reconstruction loss to reinforce the discriminability. We evaluate our method on MNIST, CIFAR10, CIFAR100, Tiny-ImageNet, and ImageNet. The experimental results show that DCPOC achieves state-of-the-art performance on these datasets.11The source code is publicly available at https://github.com/SunWenJu123/DCPOC}
}
@article{GONCALVES2023109577,
title = {Regression by Re-Ranking},
journal = {Pattern Recognition},
volume = {140},
pages = {109577},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109577},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002777},
author = {Filipe Marcel Fernandes Gonçalves and Daniel Carlos Guimarães Pedronette and Ricardo {da Silva Torres}},
keywords = {Regression, Re-ranking, Prediction, Manifold, Unsupervised learning},
abstract = {Several approaches based on regression have been developed in the past few years with the goal of improving prediction results, including the use of ranking strategies. Re-ranking has been exploited and successfully employed in several applications, improving rankings by encoding the manifold structure and redefining distances among elements from a dataset. Despite the promising results observed, re-ranking has not been evaluated in regressions tasks. This paper proposes a novel, generic, and customizable framework entitled Regression by Re-ranking (RbR), which explores the ability of re-ranking algorithms in determining relevant rankings of objects in prediction tasks. The framework relies on the integration of a base regressor, unsupervised re-ranking learning techniques, and predictions associated with nearest neighbours weighted according to their ranking positions. The RbR framework was evaluated under a rigorous experimental protocol and presented significant results in improving the prediction when compared to state-of-the-art approaches.}
}
@article{LIN2023109556,
title = {Diluted binary neural network},
journal = {Pattern Recognition},
volume = {140},
pages = {109556},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109556},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300256X},
author = {Yuhan Lin and Lingfeng Niu and Yang Xiao and Ruizhi Zhou},
keywords = {Model compression, Network quantization, Binary neural network, Ternary neural network, Sparse regularization},
abstract = {Binary neural networks (BNNs) are promising on resource-constrained devices because they reduce memory consumption and accelerate inference effectively. However, they are still potential on performance improvement. Prior studies attribute performance degradation of BNNs to limited representation ability and gradient mismatch. In this paper, we find that it also results from the mandatory representation of small full-precision auxiliary weights to large values. To tackle with this issue, we propose an approach dubbed as Diluted Binary Neural Network (DBNN). Besides avoiding mandatory representation effectively, the proposed DBNN also alleviates sign flip problem to a large extent. For activations, we jointly minimize quantization error and maximize information entropy to develop the binarization scheme. Compared with existing sparsity-binarization approaches, DBNN trains network from scratch without other procedures and achieves larger sparsity. Experiments on several datasets with various networks demonstrate the superiority of our approach.}
}
@article{JU2023109553,
title = {Keep an eye on faces: Robust face detection with heatmap-Assisted spatial attention and scale-Aware layer attention},
journal = {Pattern Recognition},
volume = {140},
pages = {109553},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109553},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002534},
author = {Lei Ju and Josef Kittler and Muhammad Awais Rana and Wankou Yang and Zhenhua Feng},
keywords = {Face detection, Supervised spatial attention, Heatmap prediction},
abstract = {Modern anchor-based face detectors learn discriminative features using large-capacity networks and extensive anchor settings. In spite of their promising results, they are not without problems. First, most anchors extract redundant features from the background. As a consequence, the performance improvements are achieved at the expense of a disproportionate computational complexity. Second, the predicted face boxes are only distinguished by a classifier supervised by pre-defined positive, negative and ignored anchors. This strategy may ignore potential contributions from cohorts of anchors labeled negative/ignored during inference simply because of their inferior initialisation, although they can regress well to a target. In other words, true positives and representative features may get filtered out by unreliable confidence scores. To deal with the first concern and achieve more efficient face detection, we propose a Heatmap-assisted Spatial Attention (HSA) module and a Scale-aware Layer Attention (SLA) module to extract informative features using lower computational costs. To be specific, SLA incorporates the information from all the feature pyramid layers, weighted adaptively to remove redundant layers. HSA predicts a reshaped Gaussian heatmap and employs it to facilitate a spatial feature selection by better highlighting facial areas. For more reliable decision-making, we merge the predicted heatmap scores and classification results by voting. Since our heatmap scores are based on the distance to the face centres, they are able to retain all the well-regressed anchors. The experiments obtained on several well-known benchmarks demonstrate the merits of the proposed method.}
}