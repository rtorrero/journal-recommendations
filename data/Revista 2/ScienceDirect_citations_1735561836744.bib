@article{JIA201946,
title = {A generalized neural network for distributed nonsmooth optimization with inequality constraint},
journal = {Neural Networks},
volume = {119},
pages = {46-56},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302072},
author = {Wenwen Jia and Sitian Qin and Xiaoping Xue},
keywords = {Generalized neural network, Distributed nonsmooth constrained optimization, Consensus, Multi-agent network},
abstract = {In this paper, a generalized neural network with a novel auxiliary function is proposed to solve a distributed non-differentiable optimization over a multi-agent network. The constructed auxiliary function can ensure that the state solution of proposed neural network is bounded, and enters the inequality constraint set in finite time. Furthermore, the proposed neural network is demonstrated to reach consensus and ultimately converges to the optimal solution under several mild assumptions. Compared with the existing methods, the neural network proposed in this paper has a simple structure with a low amount of state variables, and does not depend on projection operator method for constrained distributed optimization. Finally, two numerical simulations and an application in power system are delineated to show the characteristics and practicability of the presented neural network.}
}
@article{LEE2020419,
title = {Distinct role of flexible and stable encodings in sequential working memory},
journal = {Neural Networks},
volume = {121},
pages = {419-429},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.034},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303168},
author = {Hyeonsu Lee and Woochul Choi and Youngjin Park and Se-Bum Paik},
keywords = {Working memory, Serial-position effect, Flexible and stable encodings, Human psychophysics, Neural network simulation},
abstract = {The serial-position effect in working memory is considered important for studying how a sequence of sensory information can be retained and manipulated simultaneously in neural memory circuits. Here, via a precise analysis of the primacy and recency effects in human psychophysical experiments, we propose that stable and flexible codings take distinct roles of retaining and updating information in working memory, and that their combination induces serial-position effects spontaneously. We found that stable encoding retains memory to induce the primacy effect, while flexible encoding used for learning new inputs induces the recency effect. A model simulation based on human data, confirmed that a neural network with both flexible and stable synapses could reproduce the major characteristics of serial-position effects. Our new prediction, that the control of resource allocation by flexible–stable coding balance can modulate memory performance in sequence-specific manner, was supported by pre-cued memory performance data in humans.}
}
@article{PITTI2020242,
title = {Gated spiking neural network using Iterative Free-Energy Optimization and rank-order coding for structure learning in memory sequences (INFERNO GATE)},
journal = {Neural Networks},
volume = {121},
pages = {242-258},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.023},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930303X},
author = {Alexandre Pitti and Mathias Quoy and Catherine Lavandier and Sofiane Boucenna},
keywords = {Free-energy, Rank-order coding, Structural learning, Gating mechanism, Prefrontal cortex, Language development},
abstract = {We present a framework based on iterative free-energy optimization with spiking neural networks for modeling the fronto-striatal system (PFC-BG) for the generation and recall of audio memory sequences. In line with neuroimaging studies carried out in the PFC, we propose a genuine coding strategy using the gain-modulation mechanism to represent abstract sequences based solely on the rank and location of items within them. Based on this mechanism, we show that we can construct a repertoire of neurons sensitive to the temporal structure in sequences from which we can represent any novel sequences. Free-energy optimization is then used to explore and to retrieve the missing indices of the items in the correct order for executive control and compositionality. We show that the gain-modulation mechanism permits the network to be robust to variabilities and to have long-term dependencies as it implements a gated recurrent neural network. This model, called Inferno Gate, is an extension of the neural architecture Inferno standing for Iterative Free-Energy Optimization of Recurrent Neural Networks with Gating or Gain-modulation. In experiments performed with an audio database of ten thousand MFCC vectors, Inferno Gate is capable of encoding efficiently and retrieving chunks of fifty items length. We then discuss the potential of our network to model the features of working memory in the PFC-BG loop for structural learning, goal-direction and hierarchical reinforcement learning.}
}
@article{SHUKLA2019235,
title = {Online sequential class-specific extreme learning machine for binary imbalanced learning},
journal = {Neural Networks},
volume = {119},
pages = {235-248},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302357},
author = {Sanyam Shukla and Bhagat Singh Raghuwanshi},
keywords = {Extreme learning machine, Imbalanced learning, Online sequential class-specific extreme learning machine, Classification},
abstract = {Many real-world applications suffer from the class imbalance problem, in which some classes have significantly fewer examples compared to the other classes. In this paper, we focus on online sequential learning methods, which are considerably more preferable to tackle the large size imbalanced classification problems effectively. For example, weighted online sequential extreme learning machine (WOS-ELM), voting based weighted online sequential extreme learning machine (VWOS-ELM) and weighted online sequential extreme learning machine with kernels (WOS-ELMK), etc. handle the imbalanced learning effectively. One of our recent works class-specific extreme learning machine (CS-ELM) uses class-specific regularization and has been shown to perform better for imbalanced learning. This work proposes a novel online sequential class-specific extreme learning machine (OSCSELM), which is a variant of CS-ELM. OSCSELM supports online learning technique in both chunk-by-chunk and one-by-one learning mode. It targets to handle the class imbalance problem for both small and larger datasets. The proposed work has less computational complexity in contrast with WOS-ELM for imbalanced learning. The proposed method is assessed by utilizing benchmark real-world imbalanced datasets. Experimental results illustrate the effectiveness of the proposed approach as it outperforms the other methods for imbalanced learning.}
}
@article{LI202037,
title = {Modeling place cells and grid cells in multi-compartment environments: Entorhinal–hippocampal loop as a multisensory integration circuit},
journal = {Neural Networks},
volume = {121},
pages = {37-51},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302631},
author = {Tianyi Li and Angelo Arleo and Denis Sheynikhovich},
keywords = {Place cells, Grid cells, Multisensory integration, Hippocampus, Computational model, Neural network},
abstract = {Hippocampal place cells and entorhinal grid cells are thought to form a representation of space by integrating internal and external sensory cues. Experimental data show that different subsets of place cells are controlled by vision, self-motion or a combination of both. Moreover, recent studies in environments with a high degree of visual aliasing suggest that a continuous interaction between place cells and grid cells can result in a deformation of hexagonal grids or in a progressive loss of visual cue control over grid fields. The computational nature of such a bidirectional interaction remains unclear. In this work we present a neural network model of the dynamic interaction between place cells and grid cells within the entorhinal–hippocampal processing loop. The model was tested in two recent experimental paradigms involving environments with visually similar compartments that provided conflicting evidence about visual cue control over self-motion-based spatial codes. Analysis of the model behavior suggests that the strength of entorhinal–hippocampal dynamical loop is the key parameter governing differential cue control in multi-compartment environments. Moreover, construction of separate spatial representations of visually identical compartments required a progressive weakening of visual cue control over place fields in favor of self-motion based mechanisms. More generally our results suggest a functional segregation between plastic and dynamic processes in hippocampal processing.}
}
@article{DOLD2019200,
title = {Stochasticity from function — Why the Bayesian brain may need no noise},
journal = {Neural Networks},
volume = {119},
pages = {200-213},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302199},
author = {Dominik Dold and Ilja Bytschok and Akos F. Kungl and Andreas Baumbach and Oliver Breitwieser and Walter Senn and Johannes Schemmel and Karlheinz Meier and Mihai A. Petrovici},
keywords = {Spiking networks, Noise and stochasticity, Probabilistic computing, Generative and discriminative models, Neuromorphic hardware},
abstract = {An increasing body of evidence suggests that the trial-to-trial variability of spiking activity in the brain is not mere noise, but rather the reflection of a sampling-based encoding scheme for probabilistic computing. Since the precise statistical properties of neural activity are important in this context, many models assume an ad-hoc source of well-behaved, explicit noise, either on the input or on the output side of single neuron dynamics, most often assuming an independent Poisson process in either case. However, these assumptions are somewhat problematic: neighboring neurons tend to share receptive fields, rendering both their input and their output correlated; at the same time, neurons are known to behave largely deterministically, as a function of their membrane potential and conductance. We suggest that spiking neural networks may have no need for noise to perform sampling-based Bayesian inference. We study analytically the effect of auto- and cross-correlations in functional Bayesian spiking networks and demonstrate how their effect translates to synaptic interaction strengths, rendering them controllable through synaptic plasticity. This allows even small ensembles of interconnected deterministic spiking networks to simultaneously and co-dependently shape their output activity through learning, enabling them to perform complex Bayesian computation without any need for noise, which we demonstrate in silico, both in classical simulation and in neuromorphic emulation. These results close a gap between the abstract models and the biology of functionally Bayesian spiking networks, effectively reducing the architectural constraints imposed on physical neural substrates required to perform probabilistic computing, be they biological or artificial.}
}
@article{ABDERRAHMANE2020366,
title = {Design Space Exploration of Hardware Spiking Neurons for Embedded Artificial Intelligence},
journal = {Neural Networks},
volume = {121},
pages = {366-386},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.024},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303041},
author = {Nassim Abderrahmane and Edgar Lemaire and Benoît Miramond},
keywords = {Artificial intelligence, Neuromorphic computing, Spiking neural networks, Neural coding, Embedded systems, Power consumption},
abstract = {Machine learning is yielding unprecedented interest in research and industry, due to recent success in many applied contexts such as image classification and object recognition. However, the deployment of these systems requires huge computing capabilities, thus making them unsuitable for embedded systems. To deal with this limitation, many researchers are investigating brain-inspired computing, which would be a perfect alternative to the conventional Von Neumann architecture based computers (CPU/GPU) that meet the requirements for computing performance, but not for energy-efficiency. Therefore, neuromorphic hardware circuits that are adaptable for both parallel and distributed computations need to be designed. In this paper, we focus on Spiking Neural Networks (SNNs) with a comprehensive study of neural coding methods and hardware exploration. In this context, we propose a framework for neuromorphic hardware design space exploration, which allows to define a suitable architecture based on application-specific constraints and starting from a wide variety of possible architectural choices. For this framework, we have developed a behavioral level simulator for neuromorphic hardware architectural exploration named NAXT. Moreover, we propose modified versions of the standard Rate Coding technique to make trade-offs with the Time Coding paradigm, which is characterized by the low number of spikes propagating in the network. Thus, we are able to reduce the number of spikes while keeping the same neuron’s model, which results in an SNN with fewer events to process. By doing so, we seek to reduce the amount of power consumed by the hardware. Furthermore, we present three neuromorphic hardware architectures in order to quantitatively study the implementation of SNNs. One of these architectures integrates a novel hybrid structure: a highly-parallel computation core for most solicited layers, and time-multiplexed computation units for deeper layers. These architectures are derived from a novel funnel-like Design Space Exploration framework for neuromorphic hardware.}
}
@article{ZEID201986,
title = {Moving in time: Simulating how neural circuits enable rhythmic enactment of planned sequences},
journal = {Neural Networks},
volume = {120},
pages = {86-107},
year = {2019},
note = {special Issue in Honor of the 80th Birthday of Stephen Grossberg},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302230},
author = {Omar Zeid and Daniel Bullock},
keywords = {Competitive queuing, Computational neuroscience, Music performance, Rhythm, Systems neuroscience},
abstract = {Many complex actions are mentally pre-composed as plans that specify orderings of simpler actions. To be executed accurately, planned orderings must become active in working memory, and then enacted one-by-one until the sequence is complete. Examples include writing, typing, and speaking. In cases where the planned complex action is musical in nature (e.g. a choreographed dance or a piano melody), it appears to be possible to deploy two learned sequences at the same time, one composed from actions and a second composed from the time intervals between actions. Despite this added complexity, humans readily learn and perform rhythm-based action sequences. Notably, people can learn action sequences and rhythmic sequences separately, and then combine them with little trouble (Ullén & Bengtsson 2003). Related functional MRI data suggest that there are distinct neural regions responsible for the two different sequence types (Bengtsson et al. 2004). Although research on musical rhythm is extensive, few computational models exist to extend and inform our understanding of its neural bases. To that end, this article introduces the TAMSIN (Timing And Motor System Integration Network) model, a systems-level neural network model capable of performing arbitrary item sequences in accord with any rhythmic pattern that can be represented as a sequence of integer multiples of a base interval. In TAMSIN, two Competitive Queuing (CQ) modules operate in parallel. One represents and controls item order (the ORD module) and the second represents and controls the sequence of inter-onset-intervals (IOIs) that define a rhythmic pattern (RHY module). Further circuitry helps these modules coordinate their signal processing to enable performative output consistent with a desired beat and tempo.}
}
@article{ZHAO202040,
title = {A smoothing neural network for minimization l1-lp in sparse signal reconstruction with measurement noises},
journal = {Neural Networks},
volume = {122},
pages = {40-53},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303284},
author = {You Zhao and Xing He and Tingwen Huang and Junjian Huang and Peng Li},
keywords = {Neural network, -norm minimization, -norm , Smoothing approximation},
abstract = {This paper investigates a smoothing neural network (SNN) to solve a robust sparse signal reconstruction in compressed sensing (CS), where the objective function is nonsmooth l1-norm and the feasible set satisfies an inequality of lp-norm 2&#x2265;p&#x2265;1 which is used for measuring residual errors. With a smoothing approximate technique, the non-smooth and non-Lipschitz continuous issues of the l1-norm and the gradient of lp-norm can be solved efficiently. We propose a SNN which is modeled by a differential equation and give its circuit implementation. In this case, we prove the proposed SNN converges to the optimal of considered problem. Simulation results are discussed to demonstrate the efficiency of the proposed algorithm.}
}
@article{WANG2020140,
title = {Sliding mode control of neural networks via continuous or periodic sampling event-triggering algorithm},
journal = {Neural Networks},
volume = {121},
pages = {140-147},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930262X},
author = {Shiqin Wang and Yuting Cao and Tingwen Huang and Yiran Chen and Peng Li and Shiping Wen},
keywords = {Neural network, Sliding mode control, Event-triggering, Periodic sampling},
abstract = {This paper presents the theoretical results on sliding mode control (SMC) of neural networks via continuous or periodic sampling event-triggered algorithm. Firstly, SMC with continuous sampling event-triggered scheme is developed and the practical sliding mode can be achieved. In addition, there is a consistent positive lower bound for the time interval between two successive trigger events which implies that the Zeno phenomenon will not occur. Next, a more economical and realistic SMC technique is presented with periodic sampling event-triggered algorithm, which guarantees the robust stability of the augmented system. Finally, two illustrative examples are presented to substantiate the effectiveness of the derived theoretical results.}
}
@article{MENG2019143,
title = {Salience-aware adaptive resonance theory for large-scale sparse data clustering},
journal = {Neural Networks},
volume = {120},
pages = {143-157},
year = {2019},
note = {special Issue in Honor of the 80th Birthday of Stephen Grossberg},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302758},
author = {Lei Meng and Ah-Hwee Tan and Chunyan Miao},
keywords = {Adaptive resonance theory, Clustering, Sparse data, Subspace learning, Feature weighting, Parameter adaptation},
abstract = {Sparse data is known to pose challenges to cluster analysis, as the similarity between data tends to be ill-posed in the high-dimensional Hilbert space. Solutions in the literature typically extend either k-means or spectral clustering with additional steps on representation learning and/or feature weighting. However, adding these usually introduces new parameters and increases computational cost, thus inevitably lowering the robustness of these algorithms when handling massive ill-represented data. To alleviate these issues, this paper presents a class of self-organizing neural networks, called the salience-aware adaptive resonance theory (SA-ART) model. SA-ART extends Fuzzy ART with measures for cluster-wise salient feature modeling. Specifically, two strategies, i.e. cluster space matching and salience feature weighting, are incorporated to alleviate the side-effect of noisy features incurred by high dimensionality. Additionally, cluster weights are bounded by the statistical means and minimums of the samples therein, making the learning rate also self-adaptable. Notably, SA-ART allows clusters to have their own sets of self-adaptable parameters. It has the same time complexity of Fuzzy ART and does not introduce additional hyperparameters that profile cluster properties. Comparative experiments have been conducted on the ImageNet and BlogCatalog datasets, which are large-scale and include sparsely-represented data. The results show that, SA-ART achieves 51.8% and 18.2% improvement over Fuzzy ART, respectively. While both have a similar time cost, SA-ART converges faster and can reach a better local minimum. In addition, SA-ART consistently outperforms six other state-of-the-art algorithms in terms of precision and F1 score. More importantly, it is much faster and exhibits stronger robustness to large and complex data.}
}
@article{PERSHIN202052,
title = {On the validity of memristor modeling in the neural network literature},
journal = {Neural Networks},
volume = {121},
pages = {52-56},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302515},
author = {Yuriy V. Pershin and Massimiliano {Di Ventra}},
keywords = {Memristive neural networks, Memristor, Memristive system, Memcapacitor},
abstract = {An analysis of the literature shows that there are two types of non-memristive models that have been widely used in the modeling of so-called “memristive” neural networks. Here, we demonstrate that such models have nothing in common with the concept of memristive elements: they describe either non-linear resistors or certain bi-state systems, which all are devices without memory. Therefore, the results presented in a significant number of publications are at least questionable, if not completely irrelevant to the actual field of memristive neural networks.}
}
@article{2021II,
title = {INN/ENNS/JNNS - Membership Applic. Form},
journal = {Neural Networks},
volume = {144},
pages = {II},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00421-4},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004214}
}
@article{ZHANG202094,
title = {A new learning paradigm for random vector functional-link network: RVFL+},
journal = {Neural Networks},
volume = {122},
pages = {94-105},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.039},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303211},
author = {Peng-Bo Zhang and Zhi-Xin Yang},
keywords = {RVFL+, KRVFL+, Learning using privileged information, The Rademacher complexity, SVM+, Random vector functional link networks},
abstract = {In school, a teacher plays an important role in various classroom teaching patterns. Likewise to this human learning activity, the learning using privileged information (LUPI) paradigm provides additional information generated by the teacher to ’teach’ learning models during the training stage. Therefore, this novel learning paradigm is a typical Teacher–Student Interaction mechanism. This paper is the first to present a random vector functional link (RVFL) network based on the LUPI paradigm, called RVFL+. The novel RVFL+ incorporates the LUPI paradigm that can leverage additional source of information into the RVFL, which offers an alternative way to train the RVFL. Rather than simply combining two existing approaches, the newly-derived RVFL+ fills the gap between classical randomized neural networks and the newfashioned LUPI paradigm. Moreover, the proposed RVFL+ can perform in conjunction with the kernel trick for highly complicated nonlinear feature learning, termed KRVFL+. Furthermore, the statistical property of the proposed RVFL+ is investigated, and the authors present a sharp and high-quality generalization error bound based on the Rademacher complexity. Competitive experimental results on 14 real-world datasets illustrate the great effectiveness and efficiency of the novel RVFL+ and KRVFL+, which can achieve better generalization performance than state-of-the-art methods.}
}
@article{DONG202010,
title = {Centralized/decentralized event-triggered pinning synchronization of stochastic coupled networks with noise and incomplete transitional rate},
journal = {Neural Networks},
volume = {121},
pages = {10-20},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302643},
author = {Hailing Dong and Jiamu Zhou and Mingqing Xiao},
keywords = {Synchronization, Stochastic noise, Markovian switching, Event-triggered control, Incomplete transitional rates},
abstract = {This paper studies the synchronous problem of Markovian switching complex networks associated with partly unknown transitional rates, stochastic noise, and randomly coupling strength. In order to achieve the synchronization for these array networks, event-triggered pinning control is established and developed, in which the pinning node undergoes a self-adapted switch, governed by a Markov chain. Two types of event-triggered sampling controls, centralized and decentralized event-triggered sampling, respectively, are established. Sufficient conditions for synchronization are developed by constructing a desirable stochastic Lyapunov functional as well as by employing the properties of Markov chain and Itoˆ integration. Numerical simulations are provided to demonstrate the effectiveness of the proposed approach.}
}
@article{LIU20201,
title = {Label-activating framework for zero-shot learning},
journal = {Neural Networks},
volume = {121},
pages = {1-9},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.023},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302400},
author = {Yang Liu and Xinbo Gao and Quanxue Gao and Jungong Han and Ling Shao},
keywords = {Zero-shot learning, Label space, Semantic space, Discriminative},
abstract = {Existing zero-shot learning (ZSL) models usually learn mappings between visual space and semantic space. However, few of them take the label information into account. Indirect Attribute Prediction (IAP) learns the posterior probability of each attribute by label space, but labels of seen and unseen classes are defined in different spaces, which is not suitable for Generalized ZSL (GZSL). We propose a Label-Activating Framework (LAF) for semantic-based classification. The purpose of the proposed framework is to activate the label space by learning mappings from vision and semantics to labels. In the training phase, the original label space made up of one-hot vectors is used as common space, on which visual features and semantic information are embedded. After the label space is activated, labels of unseen classes can be regarded as the linear combination of labels of seen classes. In this case, seen and unseen labels are defined in the same space, and the label space has specific meanings rather than only signs of each class. Doing so makes the activated label space become very discriminative, especially for GZSL, which is therefore more challenging and reasonable for real-world tasks. In addition, we develop a specific model based on the framework, which effectively mitigate the projection domain shift problem. Extensive experiments show our framework outperforms state-of-the-art methods and also its suitability for GZSL.}
}
@article{GLIGIC2020132,
title = {Named entity recognition in electronic health records using transfer learning bootstrapped Neural Networks},
journal = {Neural Networks},
volume = {121},
pages = {132-139},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.032},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930259X},
author = {Luka Gligic and Andrey Kormilitzin and Paul Goldberg and Alejo Nevado-Holgado},
keywords = {Neural Networks, NLP, Named entity recognition, Electronic health records, Transfer learning, LSTM},
abstract = {Neural networks (NNs) have become the state of the art in many machine learning applications, such as image, sound (LeCun et al., 2015) and natural language processing (Young et al., 2017; Linggard et al., 2012). However, the success of NNs remains dependent on the availability of large labelled datasets, such as in the case of electronic health records (EHRs). With scarce data, NNs are unlikely to be able to extract this hidden information with practical accuracy. In this study, we develop an approach that solves these problems for named entity recognition, obtaining 94.6 F1 score in I2B2 2009 Medical Extraction Challenge (Uzuner et al., 2010), 4.3 above the architecture that won the competition. To achieve this, we bootstrap our NN models through transfer learning by pretraining word embeddings on a secondary task performed on a large pool of unannotated EHRs and using the output embeddings as a foundation of a range of NN architectures. Beyond the official I2B2 challenge, we further achieve 82.4 F1 on extracting relationships between medical terms using attention-based seq2seq models bootstrapped in the same manner.}
}
@article{PESSOA2019158,
title = {Neural dynamics of emotion and cognition: From trajectories to underlying neural geometry},
journal = {Neural Networks},
volume = {120},
pages = {158-166},
year = {2019},
note = {special Issue in Honor of the 80th Birthday of Stephen Grossberg},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302242},
author = {Luiz Pessoa},
keywords = {Emotion, Cognition, Dynamics, Trajectories, Manifold},
abstract = {How can we study, characterize, and understand the neural underpinnings of cognitive-emotional behaviors as inherently dynamic processes? In the past 50 years, Stephen Grossberg has developed a research program that embraces the themes of dynamics, decentralized computation, emergence, selection and competition, and autonomy. The present paper discusses how these principles can be heeded by experimental scientists to advance the understanding of the brain basis of behavior. It is suggested that a profitable way forward is to focus on investigating the dynamic multivariate structure of brain data. Accordingly, central research problems involve characterizing “neural trajectories” and the associated geometry of the underlying “neural space.” Finally, it is argued that, at a time when the development of neurotechniques has reached a fever pitch, neuroscience needs to redirect its focus and invest comparable energy in the conceptual and theoretical dimensions of its research endeavor. Otherwise we run the risk of being able to measure “every atom” in the brain in a theoretical vacuum.}
}
@article{BAXTER202068,
title = {Constructing multilayered neural networks with sparse, data-driven connectivity using biologically-inspired, complementary, homeostatic mechanisms},
journal = {Neural Networks},
volume = {122},
pages = {68-93},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.025},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303053},
author = {Robert A. Baxter and William B Levy},
keywords = {Synaptogenesis, Apoptosis, Brain development, Energy efficient, Unsupervised learning, Neural network},
abstract = {The immense complexity of the brain requires that it be built and controlled by intrinsic, self-regulating mechanisms. One such mechanism, the formation of new connections via synaptogenesis, plays a central role in neuronal connectivity and, ultimately, performance. Adaptive synaptogenesis networks combine synaptogenesis, associative synaptic modification, and synaptic shedding to construct sparse networks. Here, inspired by neuroscientific observations, novel aspects of brain development are incorporated into adaptive synaptogenesis. The extensions include: (i) multiple layers, (ii) neuron survival and death based on information transmission, and (iii) bigrade growth factor signaling to control the onset of synaptogenesis in succeeding layers and to control neuron survival and death in preceding layers. Also guiding this research is the assumption that brains must achieve a compromise between good performance and low energy expenditures. Simulations of the network model demonstrate the parametric and functional control of both performance and energy expenditures, where performance is measured in terms of information loss and classification errors, and energy expenditures are assumed to be a monotonically increasing function of the number of neurons. Major insights from this study include (a) the key role a neural layer between two other layers has in controlling synaptogenesis and neuron elimination, (b) the performance and energy-savings benefits of delaying the onset of synaptogenesis in a succeeding layer, and (c) how the elimination of neurons in a preceding layer provides energy savings, code compression, and can be accomplished without significantly degrading information transfer or classification performance.}
}
@article{REZAI2020122,
title = {How are response properties in the middle temporal area related to inference on visual motion patterns?},
journal = {Neural Networks},
volume = {121},
pages = {122-131},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.027},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302527},
author = {Omid Rezai and Lucas Stoffl and Bryan Tripp},
keywords = {Middle temporal area, Representation, Deep networks, Sensitivity analysis, Representational similarity},
abstract = {Neurons in the primate middle temporal area (MT) respond to moving stimuli, with strong tuning for motion speed and direction. These responses have been characterized in detail, but the functional significance of these details (e.g. shapes and widths of speed tuning curves) is unclear, because they cannot be selectively manipulated. To estimate their functional significance, we used a detailed model of MT population responses as input to convolutional networks that performed sophisticated motion processing tasks (visual odometry and gesture recognition). We manipulated the distributions of speed and direction tuning widths, and studied the effects on task performance. We also studied performance with random linear mixtures of the responses, and with responses that had the same representational dissimilarity as the model populations, but were otherwise randomized. The width of speed and direction tuning both affected task performance, despite the networks having been optimized individually for each tuning variation, but the specific effects were different in each task. Random linear mixing improved performance of the odometry task, but not the gesture recognition task. Randomizing the responses while maintaining representational dissimilarity resulted in poor odometry performance. In summary, despite full optimization of the deep networks in each case, each manipulation of the representation affected performance of sophisticated visual tasks. Representation properties such as tuning width and representational similarity have been studied extensively from other perspectives, but this work provides new insight into their possible roles in sophisticated visual inference.}
}
@article{WANDETO2019116,
title = {The quantization error in a Self-Organizing Map as a contrast and colour specific indicator of single-pixel change in large random patterns},
journal = {Neural Networks},
volume = {120},
pages = {116-128},
year = {2019},
note = {special Issue in Honor of the 80th Birthday of Stephen Grossberg},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302801},
author = {John M. Wandeto and Birgitta Dresp-Langley},
keywords = {Self-Organizing Maps, Quantization error, Image time series, Medical images, Random-dot images, Change detection},
abstract = {The quantization error in a fixed-size Self-Organizing Map (SOM) with unsupervised winner-take-all learning has previously been used successfully to detect, in minimal computation time, highly meaningful changes across images in medical time series and in time series of satellite images. Here, the functional properties of the quantization error in SOM are explored further to show that the metric is capable of reliably discriminating between the finest differences in local contrast intensities and contrast signs. While this capability of the QE is akin to functional characteristics of a specific class of retinal ganglion cells (the so-called Y-cells) in the visual systems of the primate and the cat, the sensitivity of the QE surpasses the capacity limits of human visual detection. Here, the quantization error in the SOM is found to reliably signal changes in contrast or colour when contrast information is removed from or added to the image, but not when the amount and relative weight of contrast information is constant and only the local spatial position of contrast elements in the pattern changes. While the RGB Mean reflects coarser changes in colour or contrast well enough, the SOM-QE is shown to outperform the RGB Mean in the detection of single-pixel changes in images with up to five million pixels. This could have important implications in the context of unsupervised image learning and computational building block approaches to large sets of image data (big data), including deep learning blocks, and automatic detection of contrast change at the nanoscale in Transmission or Scanning Electron Micrographs (TEM, SEM), or at the subpixel level in multispectral and hyper-spectral imaging data.}
}
@article{DEPANNEMAECKER2020420,
title = {Realistic spiking neural network: Non-synaptic mechanisms improve convergence in cell assembly},
journal = {Neural Networks},
volume = {122},
pages = {420-433},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.038},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930320X},
author = {Damien Depannemaecker and Luiz Eduardo {Canton Santos} and Antônio Márcio Rodrigues and Carla Alessandra Scorza and Fulvio Alexandre Scorza and Antônio-Carlos Guimarães de Almeida},
keywords = {Spiking neural network, Biophysical model, Synchronism, Burst activity, Convergence},
abstract = {Learning in neural networks inspired by brain tissue has been studied for machine learning applications. However, existing works primarily focused on the concept of synaptic weight modulation, and other aspects of neuronal interactions, such as non-synaptic mechanisms, have been neglected. Non-synaptic interaction mechanisms have been shown to play significant roles in the brain, and four classes of these mechanisms can be highlighted: (i) electrotonic coupling; (ii) ephaptic interactions; (iii) electric field effects; and iv) extracellular ionic fluctuations. In this work, we proposed simple rules for learning inspired by recent findings in machine learning adapted to a realistic spiking neural network. We show that the inclusion of non-synaptic interaction mechanisms improves cell assembly convergence. By including extracellular ionic fluctuation represented by the extracellular electrodiffusion in the network, we showed the importance of these mechanisms to improve cell assembly convergence. Additionally, we observed a variety of electrophysiological patterns of neuronal activity, particularly bursting and synchronism when the convergence is improved.}
}
@article{ZHANG20191,
title = {Hierarchical feature fusion framework for frequency recognition in SSVEP-based BCIs},
journal = {Neural Networks},
volume = {119},
pages = {1-9},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301959},
author = {Yangsong Zhang and Erwei Yin and Fali Li and Yu Zhang and Daqing Guo and Dezhong Yao and Peng Xu},
keywords = {Brain-computer interface(BCI), Feature fusion, Steady-state visual evoked potential (SSVEP), Correlated component analysis (CORRCA), Electroencephalogram (EEG)},
abstract = {Effective frequency recognition algorithms are critical in steady-state visual evoked potential (SSVEP) based brain–computer interfaces (BCIs). In this study, we present a hierarchical feature fusion framework which can be used to design high-performance frequency recognition methods. The proposed framework includes two primary techniques for fusing features: spatial dimension fusion (SD) and frequency dimension fusion (FD). Both SD and FD fusions are obtained using a weighted strategy with a nonlinear function. To assess our novel methods, we used the correlated component analysis (CORRCA) method to investigate the efficiency and effectiveness of the proposed framework. Experimental results were obtained from a benchmark dataset of thirty-five subjects and indicate that the extended CORRCA method used within the framework significantly outperforms the original CORCCA method. Accordingly, the proposed framework holds promise to enhance the performance of frequency recognition methods in SSVEP-based BCIs.}
}
@article{WANG2020329,
title = {A waiting-time-based event-triggered scheme for stabilization of complex-valued neural networks},
journal = {Neural Networks},
volume = {121},
pages = {329-338},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303144},
author = {Xiaohong Wang and Zhen Wang and Qiankun Song and Hao Shen and Xia Huang},
keywords = {Complex-valued neural networks, Stabilization, Event-triggered control, Time-dependent Lyapunov functional},
abstract = {This paper addresses the global stabilization of complex-valued neural networks (CVNNs) via event-triggered control. First, a waiting-time-based event-triggered scheme is designed to reduce the data transmission rate. Therein, an exponential decay term is introduced into the predefined threshold function, which may postpone the triggering instant of the necessary data and therefore reduce the frequency of data transmission. Then, with the help of the input delay approach, a time-dependent piecewise-defined Lyapunov–Krasovskii functional is constructed for closed-loop system to formulate a less conservative stability criterion. In addition, by resorting to matrix transformation, the co-design method for both the feedback gains and the trigger parameters is derived. Finally, a numerical example is given to illustrate the feasibility and superiority of the proposed event-triggered scheme and the obtained theoretical results.}
}
@article{XIE2019222,
title = {Spectral constraint adversarial autoencoders approach to feature representation in hyperspectral anomaly detection},
journal = {Neural Networks},
volume = {119},
pages = {222-234},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302291},
author = {Weiying Xie and Jie Lei and Baozhu Liu and Yunsong Li and Xiuping Jia},
keywords = {Adversarial autoencoders (AAE), Hyperspectral anomaly detection, Unsupervised feature learning, Spectral constraint, Background suppression},
abstract = {Anomaly detection in hyperspectral images (HSIs) faces various levels of difficulty due to the high dimensionality, redundant information and deteriorated bands. To address these problems, we propose a novel unsupervised feature representation approach by incorporating a spectral constraint strategy into adversarial autoencoders (AAE) without any prior knowledge in this paper. Our approach, called SC_AAE (spectral constraint AAE), is based on the characteristics of HSIs to obtain better discrimination represented by hidden nodes. To be specific, we adopt a spectral angle distance into the loss function of AAE to enforce spectral consistency. Considering the different contribution rates of each hidden node to anomaly detection, we individually fuse the hidden nodes by an adaptive weighting method. A bi-layer architecture is then designed to suppress the variational background (BKG) while preserving features of anomalies. The experimental results demonstrate that our proposed method outperforms the state-of-the-art methods.}
}
@article{LI2020276,
title = {Multi-output parameter-insensitive kernel twin SVR model},
journal = {Neural Networks},
volume = {121},
pages = {276-293},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303028},
author = {Yanmeng Li and Huaijiang Sun and Wenzhu Yan and Xiaoqian Zhang},
keywords = {MIMO systems, Multivariate regression, Multi-output twin support vector regression, Parameter-insensitivity},
abstract = {Multi-output regression aims at mapping a multivariate input feature space to a multivariate output space. Currently, it is effective to extend the traditional support vector regression (SVR) mechanism to solve the multi-output case. However, some methods adopting a combination of single-output SVR models exhibit the severe drawback of not considering the possible correlations between outputs, and other multi-output SVRs show high computational complexity and are typically sensitive to parameters due to the influence of noise. To handle these problems, in this study, we determine the multi-output regression function through a pair of nonparallel up- and down-bound functions solved by two smaller-sized quadratic programming problems, which results in a fast learning speed. This method is named multi-output twin support vector regression (M-TSVR). Moreover, when the noise is heteroscedastic, based on our M-TSVR, we introduce a pair of multi-input/output nonparallel parameter insensitive up- and down-bound functions to evaluate a regression model named multi-output parameter-insensitive twin support vector regression (M-PITSVR). To handle the nonlinear case, we derive the kernelized extensions of M-TSVR and M-PITSVR. Finally, a series of comparative experiments with several other multi-output-based methods are performed on twelve multi-output datasets. The experimental results indicate that the proposed multi-output regressors yield fast learning speed as well as a better and more stable prediction performance.}
}
@article{RAZZAK2020441,
title = {Integrating joint feature selection into subspace learning: A formulation of 2DPCA for outliers robust feature selection},
journal = {Neural Networks},
volume = {121},
pages = {441-451},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.030},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302576},
author = {Imran Razzak and Raghib Abu Saris and Michael Blumenstein and Guandong Xu},
keywords = {PCA, 2DPCA, Outliers, Dimensionality reduction, Principal component analysis},
abstract = {Since the principal component analysis and its variants are sensitive to outliers that affect their performance and applicability in real world, several variants have been proposed to improve the robustness. However, most of the existing methods are still sensitive to outliers and are unable to select useful features. To overcome the issue of sensitivity of PCA against outliers, in this paper, we introduce two-dimensional outliers-robust principal component analysis (ORPCA) by imposing the joint constraints on the objective function. ORPCA relaxes the orthogonal constraints and penalizes the regression coefficient, thus, it selects important features and ignores the same features that exist in other principal components. It is commonly known that square Frobenius norm is sensitive to outliers. To overcome this issue, we have devised an alternative way to derive objective function. Experimental results on four publicly available benchmark datasets show the effectiveness of joint feature selection and provide better performance as compared to state-of-the-art dimensionality-reduction methods.}
}
@article{KATZ201910,
title = {A programmable neural virtual machine based on a fast store-erase learning rule},
journal = {Neural Networks},
volume = {119},
pages = {10-30},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302059},
author = {Garrett E. Katz and Gregory P. Davis and Rodolphe J. Gentili and James A. Reggia},
keywords = {Programmable neural networks, Local learning, Itinerant attractor dynamics, Multiplicative gating, Symbolic processing},
abstract = {We present a neural architecture that uses a novel local learning rule to represent and execute arbitrary, symbolic programs written in a conventional assembly-like language. This Neural Virtual Machine (NVM) is purely neurocomputational but supports all of the key functionality of a traditional computer architecture. Unlike other programmable neural networks, the NVM uses principles such as fast non-iterative local learning, distributed representation of information, program-independent circuitry, itinerant attractor dynamics, and multiplicative gating for both activity and plasticity. We present the NVM in detail, theoretically analyze its properties, and conduct empirical computer experiments that quantify its performance and demonstrate that it works effectively.}
}
@article{YANG2020161,
title = {Operation-aware Neural Networks for user response prediction},
journal = {Neural Networks},
volume = {121},
pages = {161-168},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302850},
author = {Yi Yang and Baile Xu and Shaofeng Shen and Furao Shen and Jian Zhao},
keywords = {Neural networks, Click-through rate prediction, Factorization machines},
abstract = {User response prediction makes a crucial contribution to the rapid development of online advertising system and recommendation system. The importance of learning feature interactions has been emphasized by many works. Many deep models are proposed to automatically learn high-order feature interactions. Since most features in advertising systems and recommendation systems are high-dimensional sparse features, deep models usually learn a low-dimensional distributed representation for each feature in the bottom layer. Besides traditional fully-connected architectures, some new operations, such as convolutional operations and product operations, are proposed to learn feature interactions better. In these models, the representation is shared among different operations. However, the best representation for each operation may be different. In this paper, we propose a new neural model named Operation-aware Neural Networks (ONN) which learns different representations for different operations. Our experimental results on two large-scale real-world ad click/conversion datasets demonstrate that ONN consistently outperforms the state-of-the-art models in both offline-training environment and online-training environment.}
}
@article{SAUNDERS2019332,
title = {Locally connected spiking neural networks for unsupervised feature learning},
journal = {Neural Networks},
volume = {119},
pages = {332-340},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302333},
author = {Daniel J. Saunders and Devdhar Patel and Hananel Hazan and Hava T. Siegelmann and Robert Kozma},
keywords = {Spiking neural network, Machine learning, Spike-timing-dependent plasticity, BindsNET, Unsupervised learning, Image classification},
abstract = {In recent years, spiking neural networks (SNNs) have demonstrated great success in completing various machine learning tasks. We introduce a method for learning image features with locally connected layers in SNNs using a spike-timing-dependent plasticity (STDP) rule. In our approach, sub-networks compete via inhibitory interactions to learn features from different locations of the input space. These locally-connected SNNs (LC-SNNs) manifest key topological features of the spatial interaction of biological neurons. We explore a biologically inspired n-gram classification approach allowing parallel processing over various patches of the image space. We report the classification accuracy of simple two-layer LC-SNNs on two image datasets, which respectively match state-of-art performance and are the first results to date. LC-SNNs have the advantage of fast convergence to a dataset representation, and they require fewer learnable parameters than other SNN approaches with unsupervised learning. Robustness tests demonstrate that LC-SNNs exhibit graceful degradation of performance despite the random deletion of large numbers of synapses and neurons. Our results have been obtained using the BindsNET library, which allows efficient machine learning implementations of spiking neural networks.}
}
@article{CAO2019178,
title = {Synchronization of memristive neural networks with leakage delay and parameters mismatch via event-triggered control},
journal = {Neural Networks},
volume = {119},
pages = {178-189},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930228X},
author = {Yuting Cao and Shengbo Wang and Zhenyuan Guo and Tingwen Huang and Shiping Wen},
keywords = {Memristive neural networks, Synchronization, Parameters mismatch, Leakage delay},
abstract = {In this paper, we investigate the synchronization problem on delayed memristive neural networks (MNNs) with leakage delay and parameters mismatch via event-triggered control. We divide MNNs with parameters mismatch into two categories for discussion. One is state-dependent and can achieve synchronization by designing a suitable controller. A novel Lyapunov functional is constructed to analyze the synchronization problem. Moreover, the triggering conditions are independent from the delay boundaries and can be static or dynamic. Another category of parameters mismatch is structure-dependent and can only achieve quasi-synchronization by appropriate controller. By using matrix measure method and generalized Halanay inequality, a quasi-synchronization criterion is established. The controllers in this paper are discrete state-dependent and can be updated under the event-based triggering condition, which is more simpler than the previous results. In the end of our paper, two illustrative examples are given to support our results.}
}
@article{PATINOSAUCEDO2020319,
title = {Event-driven implementation of deep spiking convolutional neural networks for supervised classification using the SpiNNaker neuromorphic platform},
journal = {Neural Networks},
volume = {121},
pages = {319-328},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302692},
author = {Alberto Patiño-Saucedo and Horacio Rostro-Gonzalez and Teresa Serrano-Gotarredona and Bernabé Linares-Barranco},
keywords = {Neuromorphic hardware, Artificial neural networks, Spiking neural networks, MNIST, SpiNNaker, Event processing},
abstract = {Neural networks have enabled great advances in recent times due mainly to improved parallel computing capabilities in accordance to Moore’s Law, which allowed reducing the time needed for the parameter learning of complex, multi-layered neural architectures. However, with silicon technology reaching its physical limits, new types of computing paradigms are needed to increase the power efficiency of learning algorithms, especially for dealing with deep spatio-temporal knowledge on embedded applications. With the goal of mimicking the brain’s power efficiency, new hardware architectures such as the SpiNNaker board have been built. Furthermore, recent works have shown that networks using spiking neurons as learning units can match classical neural networks in supervised tasks. In this paper, we show that the implementation of state-of-the-art models on both the MNIST and the event-based NMNIST digit recognition datasets is possible on neuromorphic hardware. We use two approaches, by directly converting a classical neural network to its spiking version and by training a spiking network from scratch. For both cases, software simulations and implementations into a SpiNNaker 103 machine were performed. Numerical results approaching the state of the art on digit recognition are presented, and a new method to decrease the spike rate needed for the task is proposed, which allows a significant reduction of the spikes (up to 34 times for a fully connected architecture) while preserving the accuracy of the system. With this method, we provide new insights on the capabilities offered by networks of spiking neurons to efficiently encode spatio-temporal information.}
}
@article{DOBORJEH2019162,
title = {Personalised modelling with spiking neural networks integrating temporal and static information},
journal = {Neural Networks},
volume = {119},
pages = {162-177},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.021},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302175},
author = {Maryam Doborjeh and Nikola Kasabov and Zohreh Doborjeh and Reza Enayatollahi and Enmei Tu and Amir H. Gandomi},
keywords = {Integrated data domains, Prediction, Classification, Personalised modelling, Spiking neural networks, Pattern recognition},
abstract = {This paper proposes a new personalised prognostic/diagnostic system that supports classification, prediction and pattern recognition when both static and dynamic/spatiotemporal features are presented in a dataset. The system is based on a proposed clustering method (named d2WKNN) for optimal selection of neighbouring samples to an individual with respect to the integration of both static (vector-based) and temporal individual data. The most relevant samples to an individual are selected to train a Personalised Spiking Neural Network (PSNN) that learns from sets of streaming data to capture the space and time association patterns. The generated time-dependant patterns resulted in a higher accuracy of classification/prediction (80% to 93%) when compared with global modelling and conventional methods. In addition, the PSNN models can support interpretability by creating personalised profiling of an individual. This contributes to a better understanding of the interactions between features. Therefore, an end-user can comprehend what interactions in the model have led to a certain decision (outcome). The proposed PSNN model is an analytical tool, applicable to several real-life health applications, where different data domains describe a person’s health condition. The system was applied to two case studies: (1) classification of spatiotemporal neuroimaging data for the investigation of individual response to treatment and (2) prediction of risk of stroke with respect to temporal environmental data. For both datasets, besides the temporal data, static health data were also available. The hyper-parameters of the proposed system, including the PSNN models and the d2WKNN clustering parameters, are optimised for each individual.}
}
@article{XU2020512,
title = {Deep CovDenseSNN: A hierarchical event-driven dynamic framework with spiking neurons in noisy environment},
journal = {Neural Networks},
volume = {121},
pages = {512-519},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.034},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302618},
author = {Qi Xu and Jianxin Peng and Jiangrong Shen and Huajin Tang and Gang Pan},
keywords = {Spiking neurons, Encoding, Feature extraction, Noisy environment},
abstract = {Neurons in the brain use an event signal, termed spike, encode temporal information for neural computation. Spiking neural networks (SNNs) take this advantage to serve as biological relevant models. However, the effective encoding of sensory information and also its integration with downstream neurons of SNNs are limited by the current shallow structures and learning algorithms. To tackle this limitation, this paper proposes a novel hybrid framework combining the feature learning ability of continuous-valued convolutional neural networks (CNNs) and SNNs, named deep CovDenseSNN, such that SNNs can make use of feature extraction ability of CNNs during the encoding stage, but still process features with unsupervised learning rule of spiking neurons. We evaluate them on MNIST and its variations to show that our model can extract and transmit more important information than existing models, especially for anti-noise ability in the noisy environment. The proposed architecture provides efficient ways to perform feature representation and recognition in a consistent temporal learning framework, which is easily adapted to neuromorphic hardware implementations and bring more biological realism into modern image classification models, with the hope that the proposed framework can inform us how sensory information is transmitted and represented in the brain.}
}
@article{WAN2019321,
title = {Exponential synchronization of semi-Markovian coupled neural networks with mixed delays via tracker information and quantized output controller},
journal = {Neural Networks},
volume = {118},
pages = {321-331},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301923},
author = {Xiaoxiao Wan and Xinsong Yang and Rongqiang Tang and Zunshui Cheng and Habib M. Fardoun and Fuad E. Alsaadi},
keywords = {Mixed delays, Exponential synchronization, Semi-Markovian switching, Delay-partitioning approach, Tracker information},
abstract = {In this paper, exponential synchronization of semi-Markovian coupled neural networks (NNs) with bounded time-varying delay and infinite-time distributed delay (mixed delays) is investigated. Since semi-Markov switching occurs by time-varying probability, it is difficult to capture its precise switching signal. To overcome this difficulty, a tracker is used to track the switching information with some accuracy. Then a quantized output controller (QOC) is designed by using the tracked information. Novel Lyapunov–Krasovskii functionals (LKFs) with negative terms and delay-partitioning approach, which reduce the conservativeness of the obtained results, are utilized to obtain LMI conditions ensuring the exponential synchronization. Moreover, an algorithm is proposed to design the control gains. Our results include both those derived by mode-dependent and mode-independent control schemes as special cases. Finally, numerical simulations validate the effectiveness of the methodology.}
}
@article{BRITODASILVA2019167,
title = {A survey of adaptive resonance theory neural network models for engineering applications},
journal = {Neural Networks},
volume = {120},
pages = {167-203},
year = {2019},
note = {special Issue in Honor of the 80th Birthday of Stephen Grossberg},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302734},
author = {Leonardo Enzo {Brito da Silva} and Islam Elnabarawy and Donald C. Wunsch},
keywords = {Adaptive resonance theory, Clustering, Classification, Regression, Reinforcement learning, Survey},
abstract = {This survey samples from the ever-growing family of adaptive resonance theory (ART) neural network models used to perform the three primary machine learning modalities, namely, unsupervised, supervised and reinforcement learning. It comprises a representative list from classic to contemporary ART models, thereby painting a general picture of the architectures developed by researchers over the past 30 years. The learning dynamics of these ART models are briefly described, and their distinctive characteristics such as code representation, long-term memory, and corresponding geometric interpretation are discussed. Useful engineering properties of ART (speed, configurability, explainability, parallelization and hardware implementation) are examined along with current challenges. Finally, a compilation of online software libraries is provided. It is expected that this overview will be helpful to new and seasoned ART researchers.}
}
@article{XIE2019151,
title = {Multi-source sequential knowledge regression by using transfer RNN units},
journal = {Neural Networks},
volume = {119},
pages = {151-161},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302217},
author = {Xiurui Xie and Guisong Liu and Qing Cai and Pengfei Wei and Hong Qu},
keywords = {Transfer learning, Recurrent neural network, Sequential knowledge regression, Deep learning},
abstract = {Transfer learning has achieved a lot of success in deep neural networks to reuse useful knowledge from source domains. However, most of the existing transfer learning strategies on neural networks are for classification tasks or based on simple training strategies, which have limited use in multi-source knowledge regression due to the ineffectiveness of learning common latent features and source information loss in regression. In this paper, we propose transferable Recurrent Neural Network (RNN) units on the Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) to adapt source knowledge in multi-source regression scenarios. Specifically, two knowledge adaptation methods are proposed, the first one utilizes similarity weights as the transfer coefficients of each source, and the other defines a transfer-gate to control the flow of source knowledge. By using the proposed methods, useful source knowledge embedded in both internal state and output is adapted. Extensive experiments on both synthetic data and human motion prediction tasks on the Human 3.6M dataset demonstrate the superiority of our transfer RNN units compared with conventional models.}
}
@article{SAYEED2020396,
title = {Using a deep convolutional neural network to predict 2017 ozone concentrations, 24 hours in advance},
journal = {Neural Networks},
volume = {121},
pages = {396-408},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.033},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303156},
author = {Alqamah Sayeed and Yunsoo Choi and Ebrahim Eslami and Yannic Lops and Anirban Roy and Jia Jung},
keywords = {Convolutional, Neural network, Ozone prediction, Real-time forecasting, Deep learning},
abstract = {In this study, we use a deep convolutional neural network (CNN) to develop a model that predicts ozone concentrations 24 h in advance. We have evaluated the model for 21 continuous ambient monitoring stations (CAMS) across Texas. The inputs for the CNN model consist of meteorology (e.g., wind field, temperature) and air pollution concentrations (NO x and ozone) from the previous day. The model is trained for predicting next-day, 24-hour ozone concentrations. We acquired meteorological and air pollution data from 2014 to 2017 from the Texas Commission on Environmental Quality (TCEQ). For 19 of the 21 stations in the study, results show that the yearly index of agreement (IOA) is above 0.85, confirming the acceptable accuracy of the CNN model. The results also show the model performed well, even for stations with varying monthly trends of ozone concentrations (specifically CAMS-012, located in El-Paso, and CAMS-013, located in Fort Worth, both with IOA=0.89). In addition, to ensure that the model was robust, we tested it on stations where fewer meteorological variables are monitored. Although these stations have fewer input features, their performance is similar to that of other stations. However, despite its success at capturing daily trends, the model mostly underpredicts the daily maximum ozone, which provides a direction for future study and improvement. As this model predicts ozone concentrations 24 h in advance with greater accuracy and computationally fewer resources, it can serve as an early warning system for individuals susceptible to ozone and those engaging in outdoor activities.}
}
@article{CIRRINCIONE202057,
title = {The GH-EXIN neural network for hierarchical clustering},
journal = {Neural Networks},
volume = {121},
pages = {57-73},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302060},
author = {Giansalvo Cirrincione and Gabriele Ciravegna and Pietro Barbiero and Vincenzo Randazzo and Eros Pasero},
keywords = {Convex hull, Dynamic tree, Hierarchical divisive clustering, Neural network, Self-organization, Two-way clustering},
abstract = {Hierarchical clustering is an important tool for extracting information from data in a multi-resolution way. It is more meaningful if driven by data, as in the case of divisive algorithms, which split data until no more division is allowed. However, they have the drawback of the splitting threshold setting. The neural networks can address this problem, because they basically depend on data. The growing hierarchical GH-EXIN neural network builds a hierarchical tree in an incremental (data-driven architecture) and self-organized way. It is a top-down technique which defines the horizontal growth by means of an anisotropic region of influence, based on the novel idea of neighborhood convex hull. It also reallocates data and detects outliers by using a novel approach on all the leaves, simultaneously. Its complexity is estimated and an analysis of its user-dependent parameters is given. The advantages of the proposed approach, with regard to the best existing networks, are shown and analyzed, qualitatively and quantitatively, both in benchmark synthetic problems and in a real application (image recognition from video), in order to test the performance in building hierarchical trees. Furthermore, an important and very promising application of GH-EXIN in two-way hierarchical clustering, for the analysis of gene expression data in the study of the colorectal cancer is described.}
}
@article{2021ii,
title = {Editorial Board},
journal = {Neural Networks},
volume = {144},
pages = {ii},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00417-2},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004172}
}
@article{LIN2020259,
title = {Event-triggered passivity and synchronization of delayed multiple-weighted coupled reaction–diffusion neural networks with non-identical nodes},
journal = {Neural Networks},
volume = {121},
pages = {259-275},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.031},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302588},
author = {Shanrong Lin and Yanli Huang and Shunyan Ren},
keywords = {Event-triggered control, Synchronization, Multiple weights, Passivity, Non-identical nodes, Coupled reaction–diffusion neural networks},
abstract = {This paper solves the event-triggered passivity and synchronization problems for delayed multiple-weighted coupled reaction–diffusion neural networks (DMWCRDNNs) composed of non-identical nodes with and without parameter uncertainties. On one side, by designing appropriate event-triggered controllers, several passivity and synchronization criteria for DMWCRDNNs with certain parameters under the designed event-triggered conditions are derived based on the Lyapunov functional method and some inequality techniques. On the other side, we consider that the external perturbations may lead the parameters in network model to containing uncertainties, robust event-triggered passivity and synchronization for DMWCRDNNs with parameter uncertainties are investigated. Finally, two examples with numerical simulation results are provided to illustrate the effectiveness of the obtained theoretical results.}
}
@article{POON2019299,
title = {Hierarchical gated recurrent neural network with adversarial and virtual adversarial training on text classification},
journal = {Neural Networks},
volume = {119},
pages = {299-312},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302345},
author = {Hoon-Keng Poon and Wun-She Yap and Yee-Kai Tee and Wai-Kong Lee and Bok-Min Goi},
keywords = {Machine learning, Adversarial training, Text classification, Small-scale datasets, Neural network},
abstract = {Document classification aims to assign one or more classes to a document for ease of management by understanding the content of a document. Hierarchical attention network (HAN) has been showed effective to classify documents that are ambiguous. HAN parses information-intense documents into slices (i.e., words and sentences) such that each slice can be learned separately and in parallel before assigning the classes. However, introducing hierarchical attention approach leads to the redundancy of training parameters which is prone to overfitting. To mitigate the concern of overfitting, we propose a variant of hierarchical attention network using adversarial and virtual adversarial perturbations in 1) word representation, 2) sentence representation and 3) both word and sentence representations. The proposed variant is tested on eight publicly available datasets. The results show that the proposed variant outperforms the hierarchical attention network with and without using random perturbation. More importantly, the proposed variant achieves state-of-the-art performance on multiple benchmark datasets. Visualizations and analysis are provided to show that perturbation can effectively alleviate the overfitting issue and improve the performance of hierarchical attention network.}
}
@article{LI201931,
title = {His-GAN: A histogram-based GAN model to improve data generation quality},
journal = {Neural Networks},
volume = {119},
pages = {31-45},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301893},
author = {Wei Li and Wei Ding and Rajani Sadasivam and Xiaohui Cui and Ping Chen},
keywords = {-GAN, Histogram-based evaluation metric, Numeric simulation data generation},
abstract = {Generative Adversarial Network (GAN) has become an active research field due to its capability to generate quality simulation data. However, two consistent distributions (generated data distribution and original data distribution) produced by GAN cannot guarantee that generated data are always close to real data. Traditionally GAN is mainly applied to images, and it becomes more challenging for numeric datasets. In this paper, we propose a histogram-based GAN model (His-GAN). The purpose of our proposed model is to help GAN produce generated data with high quality. Specifically, we map generated data and original data into a histogram, then we count probability percentile on each bin and calculate dissimilarity with traditional f-divergence measures (e.g., Hellinger distance, Jensen–Shannon divergence) and Histogram Intersection Kernel. After that, we incorporate this dissimilarity score into training of the GAN model to update the generator’s parameters to improve generated data quality. This is because the parameters have an influence on the generated data quality. Moreover, we revised GAN training process by feeding GAN model with one group of samples (these samples can come from one class or one cluster that hold similar characteristics) each time, so the final generated data could contain the characteristics from a single group to overcome the challenge of figuring out complex characteristics from mixed groups/clusters of data. In this way, we can generate data that is more indistinguishable from original data. We conduct extensive experiments to validate our idea with MNIST, CIFAR-10, and a real-world numeric dataset, and the results clearly show the effectiveness of our approach.}
}
@article{PARK2019139,
title = {Semi-supervised distributed representations of documents for sentiment analysis},
journal = {Neural Networks},
volume = {119},
pages = {139-150},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302187},
author = {Saerom Park and Jaewook Lee and Kyoungok Kim},
keywords = {Sentiment analysis, Semi-supervised representation learning, Distributed representation, Natural language processing, Discriminative learning, Neural probabilistic language model},
abstract = {Learning document representation is important in applying machine learning algorithms for sentiment analysis. Distributed representation learning models of words and documents, one of neural language models, have overcome some limits of vector space models such as bag-of-words model and have been utilized successively in many natural language processing tasks including sentiment analysis. However, because such models learn the embeddings only with a context-based objective, it is hard for embeddings to reflect the sentiment of texts. In this research, we address this problem by introducing a semi-supervised sentiment-discriminative objective using partial sentiment information of documents. Our method not only reflects the partial sentiment information, but also preserves local structures induced from original distributed representation learning objectives by considering only sentiment relationships between neighboring documents. Using real-world datasets, the proposed method has been validated by sentiment visualization and classification tasks. The visualization results of Amazon review datasets demonstrate the enhancement of the sentiment class separation when document representations of our proposed method are compared to other methods. Sentiment prediction from our representations also appears to be consistently superior to other representations in both Amazon and Yelp datasets. This work can be extended to develop effective document embeddings applied to other discriminative tasks.}
}
@article{WAN2020231,
title = {Multistability and attraction basins of discrete-time neural networks with nonmonotonic piecewise linear activation functions},
journal = {Neural Networks},
volume = {122},
pages = {231-238},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303272},
author = {Peng Wan and Dihua Sun and Min Zhao and Li Wan and Shuang Jin},
keywords = {Multistability, Discrete-time, Activation function, Instability, Attraction basin, Neural networks},
abstract = {This paper is concerned with multistability and attraction basins of discrete-time neural networks with nonmonotonic piecewise linear activation functions. Under some reasonable conditions, the addressed networks have (2m+1)n equilibrium points. (m+1)n of which are locally asymptotically stable, and the others are unstable. The attraction basins of the locally asymptotically stable equilibrium points are given in the form of hyperspherical regions. These results here, which include existence, uniqueness, locally asymptotical stability, instability and attraction basins of the multiple equilibrium points, generalize and improve the earlier publications. Finally, an illustrative example with numerical simulation is given to show the feasibility and the effectiveness of the theoretical results. The theoretical results and illustrative example indicate that the activation functions improve the storage capacity of neural networks significantly.}
}
@article{SADEGHI2019338,
title = {Efficient training of interval Neural Networks for imprecise training data},
journal = {Neural Networks},
volume = {118},
pages = {338-351},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301935},
author = {Jonathan Sadeghi and Marco {de Angelis} and Edoardo Patelli},
keywords = {Machine learning, Imprecise probability, Uncertainty quantification, Neural Networks, Interval Predictor Models},
abstract = {This paper describes a robust and computationally feasible method to train and quantify the uncertainty of Neural Networks. Specifically, we propose a back propagation algorithm for Neural Networks with interval predictions. In order to maintain numerical stability we propose minimising the maximum of the batch of errors at each step. Our approach can accommodate incertitude in the training data, and therefore adversarial examples from a commonly used attack model can be trivially accounted for. We present results on a test function example, and a more realistic engineering test case. The reliability of the predictions of these networks is guaranteed by the non-convex Scenario approach to chance constrained optimisation, which takes place following training, and is hence robust to the performance of the optimiser. A key result is that, by using minibatches of size M, the complexity of the proposed approach scales as O(M⋅Niter), and does not depend upon the number of training data points as with other Interval Predictor Model methods. In addition, troublesome penalty function methods are avoided. To the authors’ knowledge this contribution presents the first computationally feasible approach for dealing with convex set based epistemic uncertainty in huge datasets.}
}
@article{XIE2020409,
title = {Adaptive latent similarity learning for multi-view clustering},
journal = {Neural Networks},
volume = {121},
pages = {409-418},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302746},
author = {Deyan Xie and Quanxue Gao and Qianqian Wang and Xiangdong Zhang and Xinbo Gao},
keywords = {Affinity matrix, Multi-view clustering, Similarity matrix},
abstract = {Most existing clustering methods employ the original multi-view data as input to learn the similarity matrix which characterizes the underlying cluster structure shared by multiple views. This reduces the flexibility of multi-view clustering methods due to the fact that multi-view data usually contains noise or the variation between multi-view data points, which should belong to the same cluster, is larger than the variation between data points belonging to different clusters. To address these problems, we propose a novel multi-view clustering model, namely adaptive latent similarity learning (ALSL) for multi-view clustering. ALSL employs the adaptively learned graph, which characterizes the relationship between clusters, as the new input to learn the latent data representation and integrates the latent similarity representation learning, manifold learning and spectral clustering into a unified framework. With the complementarity of multiple views, the latent similarity representation characterizes the underlying cluster structure shared by multiple views. Our model is intuitive and can be optimized efficiently by using the Augmented Lagrangian Multiplier with Alternating Direction Minimization (ALM-ADM) algorithm. Extensive experiments on benchmark datasets have demonstrated the superiority of the proposed method.}
}
@article{DENG2019313,
title = {Multiclass heterogeneous domain adaptation via bidirectional ECOC projection},
journal = {Neural Networks},
volume = {119},
pages = {313-322},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302229},
author = {Wan-Yu Deng and Ying-Ying Dong and Guang-Da Liu and Ying Wang and Jie Men},
keywords = {Heterogeneous domain adaptation, Transfer learning, ECOC, MMD, Cross-domain learning},
abstract = {Heterogeneous domain adaptation aims to exploit the source domain data to train a prediction model for the target domain with different input feature space. Current methods either map the data points from different domains with different feature space to a common latent subspace or use asymmetric projections for learning the classifier. However, these learning methods separate common space learning and shared classifier training. This may lead complex model structure and more parameters to be determined. To appropriately address this problem, we propose a novel bidirectional ECOC projection method, named HDA-ECOC, for heterogeneous domain adaptation. The proposed method projects the inputs and outputs (labels) of two domains into a common ECOC coding space, such that, the common space learning and the shared classifier training can be performed simultaneously. Then, classification of the target testing sample can be directly addressed by an ECOC decoding. Moreover, the unlabeled target data is exploited by estimating the two domains projected instances consistency through a maximum mean discrepancy (MMD) criterion. We formulate this method as a dual convex minimization problem and propose an alternating optimization algorithm for solving it. For performance evaluation, experiments are performed on cross-lingual text classification and cross-domain digital image classification with heterogeneous feature space. The experimental results demonstrate that the proposed method is effective and efficient in solving the heterogeneous domain adaptation problems.}
}
@article{IBTEHAZ202074,
title = {MultiResUNet : Rethinking the U-Net architecture for multimodal biomedical image segmentation},
journal = {Neural Networks},
volume = {121},
pages = {74-87},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.025},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302503},
author = {Nabil Ibtehaz and M. Sohel Rahman},
keywords = {Convolutional neural networks, Medical imaging, Semantic segmentation, U-Net},
abstract = {In recent years Deep Learning has brought about a breakthrough in Medical Image Segmentation. In this regard, U-Net has been the most popular architecture in the medical imaging community. Despite outstanding overall performance in segmenting multimodal medical images, through extensive experimentations on some challenging datasets, we demonstrate that the classical U-Net architecture seems to be lacking in certain aspects. Therefore, we propose some modifications to improve upon the already state-of-the-art U-Net model. Following these modifications, we develop a novel architecture, MultiResUNet, as the potential successor to the U-Net architecture. We have tested and compared MultiResUNet with the classical U-Net on a vast repertoire of multimodal medical images. Although only slight improvements in the cases of ideal images are noticed, remarkable gains in performance have been attained for the challenging ones. We have evaluated our model on five different datasets, each with their own unique challenges, and have obtained a relative improvement in performance of 10.15%, 5.07%, 2.63%, 1.41%, and 0.62% respectively. We have also discussed and highlighted some qualitatively superior aspects of MultiResUNet over classical U-Net that are not really reflected in the quantitative measures.}
}
@article{2021I,
title = {Current Events},
journal = {Neural Networks},
volume = {144},
pages = {I},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00420-2},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004202}
}
@article{YANG2020117,
title = {Regularized correntropy criterion based semi-supervised ELM},
journal = {Neural Networks},
volume = {122},
pages = {117-129},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.030},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303120},
author = {Jie Yang and Jiuwen Cao and Tianlei Wang and Anke Xue and Badong Chen},
keywords = {Semi-supervised learning, Extreme learning machine, Regularized correntropy criterion, Mean square error},
abstract = {Along with the explosive growing of data, semi-supervised learning attracts increasing attention in the past years due to its powerful capability in labeling unlabeled data and knowledge mining. As an emerging method, the semi-supervised extreme learning machine (SSELM), that builds on ELM, has been developed for data classification and shown superiorities in learning efficiency and accuracy. However, the optimization of SSELM as well as most of the other ELMs is generally based on the mean square error (MSE) criterion, which has been shown less effective in dealing with non-Gaussian noises. In this paper, a robust regularized correntropy criterion based SSELM (RC-SSELM) is developed. The optimization of the output weight matrix of RC-SSELM is derived by the fixed-point iteration based approach. A convergent analysis of the proposed RC-SSELM is presented based on the half-quadratic optimization technique. Experimental results on 4 synthetic datasets and 13 benchmark UCI datasets are provided to show the superiorities of the proposed RC-SSELM over SSELM and other state-of-the-art methods.}
}
@article{WU202024,
title = {Joint Ranking SVM and Binary Relevance with robust Low-rank learning for multi-label classification},
journal = {Neural Networks},
volume = {122},
pages = {24-39},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303247},
author = {Guoqiang Wu and Ruobing Zheng and Yingjie Tian and Dalian Liu},
keywords = {Multi-label classification, Rank-SVM, Binary Relevance, Robust Low-rank learning, Kernel methods},
abstract = {Multi-label classification studies the task where each example belongs to multiple labels simultaneously. As a representative method, Ranking Support Vector Machine (Rank-SVM) aims to minimize the Ranking Loss and can also mitigate the negative influence of the class-imbalance issue. However, due to its stacking-style way for thresholding, it may suffer error accumulation and thus reduces the final classification performance. Binary Relevance (BR) is another typical method, which aims to minimize the Hamming Loss and only needs one-step learning. Nevertheless, it might have the class-imbalance issue and does not take into account label correlations. To address the above issues, we propose a novel multi-label classification model, which joints Ranking support vector machine and Binary Relevance with robust Low-rank learning (RBRL). RBRL inherits the ranking loss minimization advantages of Rank-SVM, and thus overcomes the disadvantages of BR suffering the class-imbalance issue and ignoring the label correlations. Meanwhile, it utilizes the hamming loss minimization and one-step learning advantages of BR, and thus tackles the disadvantages of Rank-SVM including another thresholding learning step. Besides, a low-rank constraint is utilized to further exploit high-order label correlations under the assumption of low dimensional label space. Furthermore, to achieve nonlinear multi-label classifiers, we derive the kernelization RBRL. Two accelerated proximal gradient methods (APG) are used to solve the optimization problems efficiently. Extensive comparative experiments with several state-of-the-art methods illustrate a highly competitive or superior performance of our method RBRL.}
}
@article{SEIFFERTT201932,
title = {Adaptive Resonance Theory in the time scales calculus},
journal = {Neural Networks},
volume = {120},
pages = {32-39},
year = {2019},
note = {special Issue in Honor of the 80th Birthday of Stephen Grossberg},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302278},
author = {John Seiffertt},
keywords = {Machine learning, Adaptive resonance theory, Unsupervised learning, Control theory, Time scales},
abstract = {Engineering applications of algorithms based on Adaptive Resonance Theory have proven to be fast, reliable, and scalable solutions to modern industrial machine learning problems. A key emerging area of research is in the combination of different kinds of inputs within a single learning architecture along with ensuring the systems have the capacity for lifelong learning. We establish a dynamic equation model of ART in the time scales calculus capable of handling inputs in such mixed domains. We prove theorems establishing that the orienting subsystem can affect learning in the long-term memory storage unit as well as that those remembered exemplars result in stable categories. Further, we contribute to the mathematics of time scales literature itself with novel takes on logic functions in the calculus as well as new representations for the action of weight matrices in generalized domains. Our work extends the core ART theory and algorithms to these important mixed input domains and provides the theoretical foundation for further extensions of ART-based learning strategies for applied engineering work.}
}
@article{MHASKAR2020229,
title = {An analysis of training and generalization errors in shallow and deep networks},
journal = {Neural Networks},
volume = {121},
pages = {229-241},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.028},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302552},
author = {H.N. Mhaskar and T. Poggio},
keywords = {Deep learning, Generalization error, Interpolatory approximation},
abstract = {This paper is motivated by an open problem around deep networks, namely, the apparent absence of over-fitting despite large over-parametrization which allows perfect fitting of the training data. In this paper, we analyze this phenomenon in the case of regression problems when each unit evaluates a periodic activation function. We argue that the minimal expected value of the square loss is inappropriate to measure the generalization error in approximation of compositional functions in order to take full advantage of the compositional structure. Instead, we measure the generalization error in the sense of maximum loss, and sometimes, as a pointwise error. We give estimates on exactly how many parameters ensure both zero training error as well as a good generalization error. We prove that a solution of a regularization problem is guaranteed to yield a good training error as well as a good generalization error and estimate how much error to expect at which test data.}
}
@article{LIU201985,
title = {Adaptive robust principal component analysis},
journal = {Neural Networks},
volume = {119},
pages = {85-92},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302035},
author = {Yang Liu and Xinbo Gao and Quanxue Gao and Ling Shao and Jungong Han},
keywords = {RPCA, Flexibility, Adaptively},
abstract = {Robust Principal Component Analysis (RPCA) is a powerful tool in machine learning and data mining problems. However, in many real-world applications, RPCA is unable to well encode the intrinsic geometric structure of data, thereby failing to obtain the lowest rank representation from the corrupted data. To cope with this problem, most existing methods impose the smooth manifold, which is artificially constructed by the original data. This reduces the flexibility of algorithms. Moreover, the graph, which is artificially constructed by the corrupted data, is inexact and does not characterize the true intrinsic structure of real data. To tackle this problem, we propose an adaptive RPCA (ARPCA) to recover the clean data from the high-dimensional corrupted data. Our proposed model is advantageous due to: (1) The graph is adaptively constructed upon the clean data such that the system is more flexible. (2) Our model simultaneously learns both clean data and similarity matrix that determines the construction of graph. (3) The clean data has the lowest-rank structure that enforces to correct the corruptions. Extensive experiments on several datasets illustrate the effectiveness of our model for clustering and low-rank recovery tasks.}
}
@article{TIAN2020461,
title = {Image denoising using deep CNN with batch renormalization},
journal = {Neural Networks},
volume = {121},
pages = {461-473},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302394},
author = {Chunwei Tian and Yong Xu and Wangmeng Zuo},
keywords = {Image denoising, CNN, Residual learning, Batch renormalization, Dilated convolution},
abstract = {Deep convolutional neural networks (CNNs) have attracted great attention in the field of image denoising. However, there are two drawbacks: (1) it is very difficult to train a deeper CNN for denoising tasks, and (2) most of deeper CNNs suffer from performance saturation. In this paper, we report the design of a novel network called a batch-renormalization denoising network (BRDNet). Specifically, we combine two networks to increase the width of the network, and thus obtain more features. Because batch renormalization is fused into BRDNet, we can address the internal covariate shift and small mini-batch problems. Residual learning is also adopted in a holistic way to facilitate the network training. Dilated convolutions are exploited to extract more information for denoising tasks. Extensive experimental results show that BRDNet outperforms state-of-the-art image-denoising methods. The code of BRDNet is accessible at http://www.yongxu.org/lunwen.html.}
}
@article{CARPENTER20195,
title = {Looking to the future: Learning from experience, averting catastrophe},
journal = {Neural Networks},
volume = {120},
pages = {5-8},
year = {2019},
note = {special Issue in Honor of the 80th Birthday of Stephen Grossberg},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302837},
author = {Gail A. Carpenter},
keywords = {Self-supervised learning, Semi-supervised learning, Neural networks, Artificial Intelligence (AI), Adaptive Resonance Theory (ART), Self-supervised (ART)},
abstract = {As humans go through life sifting vast quantities of complex information, we extract knowledge from settings that are more ambiguous than our early homes and classrooms. Learning from experience in an individual’s unique context generally improves expert performance, despite the risks inherent in brain dynamics that can transform previously reliable expectations. Designers of twenty-first century technologies face the challenges and responsibilities posed by fielded systems that continue to learn on their own. The neural model Self-supervised ART, which can acquire significantly new knowledge in unpredictable contexts, is an example of one such system.}
}
@article{LIU2020356,
title = {Finite-time resilient H∞ state estimation for discrete-time delayed neural networks under dynamic event-triggered mechanism},
journal = {Neural Networks},
volume = {121},
pages = {356-365},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302679},
author = {Yufei Liu and Bo Shen and Huisheng Shu},
keywords = {Discrete-time delayed neural networks, Dynamic event-triggered mechanism, Finite-time bounded,  performance, Resilient state estimator},
abstract = {In this paper, the finite-time resilient H∞ state estimation problem is investigated for a class of discrete-time delayed neural networks. For the sake of energy saving, a dynamic event-triggered mechanism is employed in the design of state estimator for the discrete-time delayed neural networks. In order to handle the possible fluctuation of the estimator gain parameters when the state estimator is implemented, a resilient state estimator is adopted. By constructing a Lyapunov–Krasovskii functional, a sufficient condition is established, which guarantees that the estimation error system is bounded and the H∞ performance requirement is satisfied within the finite time. Then, the desired estimator gains are obtained via solving a set of linear matrix inequalities. Finally, a numerical example is employed to illustrate the usefulness of the proposed state estimation scheme.}
}
@article{ROY2020148,
title = {Tree-CNN: A hierarchical Deep Convolutional Neural Network for incremental learning},
journal = {Neural Networks},
volume = {121},
pages = {148-160},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302710},
author = {Deboleena Roy and Priyadarshini Panda and Kaushik Roy},
keywords = {Convolutional Neural Networks, Deep learning, Incremental learning, Transfer learning},
abstract = {Over the past decade, Deep Convolutional Neural Networks (DCNNs) have shown remarkable performance in most computer vision tasks. These tasks traditionally use a fixed dataset, and the model, once trained, is deployed as is. Adding new information to such a model presents a challenge due to complex training issues, such as “catastrophic forgetting”, and sensitivity to hyper-parameter tuning. However, in this modern world, data is constantly evolving, and our deep learning models are required to adapt to these changes. In this paper, we propose an adaptive hierarchical network structure composed of DCNNs that can grow and learn as new data becomes available. The network grows in a tree-like fashion to accommodate new classes of data, while preserving the ability to distinguish the previously trained classes. The network organizes the incrementally available data into feature-driven super-classes and improves upon existing hierarchical CNN models by adding the capability of self-growth. The proposed hierarchical model, when compared against fine-tuning a deep network, achieves significant reduction of training effort, while maintaining competitive accuracy on CIFAR-10 and CIFAR-100.}
}
@article{ZHU2019214,
title = {Multi-representation adaptation network for cross-domain image classification},
journal = {Neural Networks},
volume = {119},
pages = {214-221},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301984},
author = {Yongchun Zhu and Fuzhen Zhuang and Jindong Wang and Jingwu Chen and Zhiping Shi and Wenjuan Wu and Qing He},
keywords = {Domain adaptation, Multi-representation},
abstract = {In image classification, it is often expensive and time-consuming to acquire sufficient labels. To solve this problem, domain adaptation often provides an attractive option given a large amount of labeled data from a similar nature but different domains. Existing approaches mainly align the distributions of representations extracted by a single structure and the representations may only contain partial information, e.g., only contain part of the saturation, brightness, and hue information. Along this line, we propose Multi-Representation Adaptation which can dramatically improve the classification accuracy for cross-domain image classification and specially aims to align the distributions of multiple representations extracted by a hybrid structure named Inception Adaptation Module (IAM). Based on this, we present Multi-Representation Adaptation Network (MRAN) to accomplish the cross-domain image classification task via multi-representation alignment which can capture the information from different aspects. In addition, we extend Maximum Mean Discrepancy (MMD) to compute the adaptation loss. Our approach can be easily implemented by extending most feed-forward models with IAM, and the network can be trained efficiently via back-propagation. Experiments conducted on three benchmark image datasets demonstrate the effectiveness of MRAN.}
}
@article{HE2020497,
title = {Dynamic behaviors of the FitzHugh–Nagumo neuron model with state-dependent impulsive effects},
journal = {Neural Networks},
volume = {121},
pages = {497-511},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.031},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303132},
author = {Zhilong He and Chuandong Li and Ling Chen and Zhengran Cao},
keywords = {FitzHugh–Nagumo neuron model, State-dependent impulse, Order- periodic solution, Period-doubling bifurcation, Chaos, Noise},
abstract = {In present work, in order to reproduce spiking and bursting behavior of real neurons, a new hybrid biological neuron model is established and analyzed by combining the FitzHugh–Nagumo (FHN) neuron model, the threshold for spike initiation and the state-dependent impulsive effects (impulse resetting process). Firstly, we construct Poincaré mappings under different conditions by means of geometric analysis, and then obtain some sufficient criteria for the existence and stability of order-1 or order-2 periodic solution to the impulsive neuron model by finding the fixed point of Poincaré mapping and some geometric analysis techniques. Numerical simulations are given to illustrate and verify our theoretical results. The bifurcation diagrams are presented to describe the phenomena of period-doubling route to chaos, which implies that the dynamic behavior of the neuron model become more complex due to impulsive effects. Furthermore, the correctness and effectiveness of the proposed FitzHugh–Nagumo neuron model with state-dependent impulsive effects are verified by circuit simulation. Finally, the conclusions of this paper are analyzed and summarized, and the effects of random factors on the electrophysiological activities of neuron are discussed by numerical simulation.}
}
@article{HEALY201940,
title = {Episodic memory: A hierarchy of spatiotemporal concepts},
journal = {Neural Networks},
volume = {120},
pages = {40-57},
year = {2019},
note = {special Issue in Honor of the 80th Birthday of Stephen Grossberg},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.021},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303016},
author = {Michael J. Healy and Thomas P. Caudell},
keywords = {Category theory, Colimit, Concept, Cospan, Episode, Span},
abstract = {We propose a new model of episodic memory. It consists of a hierarchy of partial sequences of events, blended for consistency across space and time by feedforward/feedback links to concepts expressing their shared information. This blended concept hierarchy is acquired and represented incrementally through synaptic or connection-weighted adaptation in a neural network with episodic memory capability. We derive this model through argumentation, explain its derivation within a category-theoretic model of representation, and discuss the capabilities we propose for this model.}
}
@article{ZHANG2020163,
title = {A human-in-the-loop deep learning paradigm for synergic visual evaluation in children},
journal = {Neural Networks},
volume = {122},
pages = {163-173},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303259},
author = {Kai Zhang and Xiaoyan Li and Lin He and Chong Guo and Yahan Yang and Zhou Dong and Haoqing Yang and Yi Zhu and Chuan Chen and Xiaojing Zhou and Wangting Li and Zhenzhen Liu and Xiaohang Wu and Xiyang Liu and Haotian Lin},
keywords = {Evaluating the visual acuity of children, Human-in-the-loop, Deep learning, Object localization, Image identification, Integration of software and hardware},
abstract = {Visual development during early childhood is a vital process. Examining the visual acuity of children is essential for early detection of visual abnormalities, but performing visual examination in children is challenging. Here, we developed a human-in-the-loop deep learning (DL) paradigm that combines traditional vision examination and DL with integration of software and hardware, thus facilitating the execution of vision examinations, offsetting the shortcomings of human doctors, and improving the abilities of both DL and doctors to evaluate the vision of children. Because this paradigm contains two rounds (a human round and DL round), doctors can learn from DL and the two can mutually supervise each other such that the precision of the DL system in evaluating the visual acuity of children is improved. Based on DL-based object localization and image identification, the experiences of doctors and the videos captured in the first round, the DL system in the second round can simulate doctors in evaluating the visual acuity of children with a final accuracy of 75.54%. For comparison, we also assessed an automatic deep learning method that did not consider the experiences of doctors, but its performance was not satisfactory. This entire paradigm can evaluate the visual acuity of children more accurately than humans alone. Furthermore, the paradigm facilitates automatic evaluation of the vision of children with a wearable device.}
}
@article{LANGE2020339,
title = {Interfering with a memory without erasing its trace},
journal = {Neural Networks},
volume = {121},
pages = {339-355},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.027},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303090},
author = {Gesa Lange and Mario Senden and Alexandra Radermacher and Peter {De Weerd}},
keywords = {Perceptual learning, Behavioral interference, Expert skill, Early visual cortex, Tuning curves, Recurrent neural network},
abstract = {Previous research has shown that performance of a novice skill can be easily interfered with by subsequent training of another skill. We address the open questions whether extensively trained skills show the same vulnerability to interference as novice skills and which memory mechanism regulates interference between expert skills. We developed a recurrent neural network model of V1 able to learn from feedback experienced over the course of a long-term orientation discrimination experiment. After first exposing the model to one discrimination task for 3480 consecutive trials, we assessed how its performance was affected by subsequent training in a second, similar task. Training the second task strongly interfered with the first (highly trained) discrimination skill. The magnitude of interference depended on the relative amounts of training devoted to the different tasks. We used these and other model outcomes as predictions for a perceptual learning experiment in which human participants underwent the same training protocol as our model. Specifically, over the course of three months participants underwent baseline training in one orientation discrimination task for 15 sessions before being trained for 15 sessions on a similar task and finally undergoing another 15 sessions of training on the first task (to assess interference). Across all conditions, the pattern of interference observed empirically closely matched model predictions. According to our model, behavioral interference can be explained by antagonistic changes in neuronal tuning induced by the two tasks. Remarkably, this did not stem from erasing connections due to earlier learning but rather from a reweighting of lateral inhibition.}
}
@article{KUMARASINGHE2020169,
title = {Deep learning and deep knowledge representation in Spiking Neural Networks for Brain-Computer Interfaces},
journal = {Neural Networks},
volume = {121},
pages = {169-185},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.029},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302564},
author = {Kaushalya Kumarasinghe and Nikola Kasabov and Denise Taylor},
keywords = {Deep learning, Knowledge representation, Spiking Neural Networks, NeuCube, Electroencephalography, Brain-Computer Interface},
abstract = {Objective: This paper argues that Brain-Inspired Spiking Neural Network (BI-SNN) architectures can learn and reveal deep in time-space functional and structural patterns from spatio-temporal data. These patterns can be represented as deep knowledge, in a partial case in the form of deep spatio-temporal rules. This is a promising direction for building new types of Brain-Computer Interfaces called Brain-Inspired Brain–Computer Interfaces (BI-BCI). A theoretical framework and its experimental validation on deep knowledge extraction and representation using SNN are presented. Results: The proposed methodology was applied in a case study to extract deep knowledge of the functional and structural organisation of the brain’s neural network during the execution of a Grasp and Lift task. The BI-BCI successfully extracted the neural trajectories that represent the dorsal and ventral visual information processing streams as well as its connection to the motor cortex in the brain. Deep spatiotemporal rules on functional and structural interaction of distinct brain areas were then used for event prediction in BI-BCI. Significance: The computational framework can be used for unveiling the topological patterns of the brain and such knowledge can be effectively used to enhance the state-of-the-art in BCI.}
}
@article{MARIC2019113,
title = {Neural dynamics of spreading attentional labels in mental contour tracing},
journal = {Neural Networks},
volume = {119},
pages = {113-138},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302047},
author = {Mateja Marić and Dražen Domijan},
keywords = {Contour tracing, Object-based attention, Perceptual grouping, Recurrent competitive map, Winner-takes-all},
abstract = {Behavioral and neural data suggest that visual attention spreads along contour segments to bind them into a unified object representation. Such attentional labeling segregates the target contour from distractors in a process known as mental contour tracing. A recurrent competitive map is developed to simulate the dynamics of mental contour tracing. In the model, local excitation opposes global inhibition and enables enhanced activity to propagate on the path offered by the contour. The extent of local excitatory interactions is modulated by the output of the multi-scale contour detection network, which constrains the speed of activity spreading in a scale-dependent manner. Furthermore, an L-junction detection network enables tracing to switch direction at the L-junctions, but not at the X- or T-junctions, thereby preventing spillover to a distractor contour. Computer simulations reveal that the model exhibits a monotonic increase in tracing time as a function of the distance to be traced. Also, the speed of tracing increases with decreasing proximity to the distractor contour and with the reduced curvature of the contours. The proposed model demonstrated how an elaborated version of the winner-takes-all network can implement a complex cognitive operation such as contour tracing.}
}
@article{MA2019286,
title = {Transformed ℓ1 regularization for learning sparse deep neural networks},
journal = {Neural Networks},
volume = {119},
pages = {286-298},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302321},
author = {Rongrong Ma and Jianyu Miao and Lingfeng Niu and Peng Zhang},
keywords = {Deep neural networks, Non-convex regularization, Transformed , Group sparsity},
abstract = {Deep Neural Networks (DNNs) have achieved extraordinary success in numerous areas. However, DNNs often carry a large number of weight parameters, leading to the challenge of heavy memory and computation costs. Overfitting is another challenge for DNNs when the training data are insufficient. These challenges severely hinder the application of DNNs in resource-constrained platforms. In fact, many network weights are redundant and can be removed from the network without much loss of performance. In this paper, we introduce a new non-convex integrated transformed ℓ1 regularizer to promote sparsity for DNNs, which removes redundant connections and unnecessary neurons simultaneously. Specifically, we apply the transformed ℓ1 regularizer to the matrix space of network weights and utilize it to remove redundant connections. Besides, group sparsity is integrated to remove unnecessary neurons. An efficient stochastic proximal gradient algorithm is presented to solve the new model. To the best of our knowledge, this is the first work to develop a non-convex regularizer in sparse optimization based method to simultaneously promote connection-level and neuron-level sparsity for DNNs. Experiments on public datasets demonstrate the effectiveness of the proposed method.}
}
@article{HAO2020387,
title = {A biologically plausible supervised learning method for spiking neural networks using the symmetric STDP rule},
journal = {Neural Networks},
volume = {121},
pages = {387-395},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302680},
author = {Yunzhe Hao and Xuhui Huang and Meng Dong and Bo Xu},
keywords = {Spiking neural networks, Dopamine-modulated spike-timing dependent plasticity, Pattern recognition, Supervised learning, Biologically plausibility},
abstract = {Spiking neural networks (SNNs) possess energy-efficient potential due to event-based computation. However, supervised training of SNNs remains a challenge as spike activities are non-differentiable. Previous SNNs training methods can be generally categorized into two basic classes, i.e., backpropagation-like training methods and plasticity-based learning methods. The former methods are dependent on energy-inefficient real-valued computation and non-local transmission, as also required in artificial neural networks (ANNs), whereas the latter are either considered to be biologically implausible or exhibit poor performance. Hence, biologically plausible (bio-plausible) high-performance supervised learning (SL) methods for SNNs remain deficient. In this paper, we proposed a novel bio-plausible SNN model for SL based on the symmetric spike-timing dependent plasticity (sym-STDP) rule found in neuroscience. By combining the sym-STDP rule with bio-plausible synaptic scaling and intrinsic plasticity of the dynamic threshold, our SNN model implemented SL well and achieved good performance in the benchmark recognition task (MNIST dataset). To reveal the underlying mechanism of our SL model, we visualized both layer-based activities and synaptic weights using the t-distributed stochastic neighbor embedding (t-SNE) method after training and found that they were well clustered, thereby demonstrating excellent classification ability. Furthermore, to verify the robustness of our model, we trained it on another more realistic dataset (Fashion-MNIST), which also showed good performance. As the learning rules were bio-plausible and based purely on local spike events, our model could be easily applied to neuromorphic hardware for online training and may be helpful for understanding SL information processing at the synaptic level in biological neural systems.}
}
@article{VARKARAKIS2020101,
title = {Deep neural network and data augmentation methodology for off-axis iris segmentation in wearable headsets},
journal = {Neural Networks},
volume = {121},
pages = {101-121},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.020},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302163},
author = {Viktor Varkarakis and Shabab Bazrafkan and Peter Corcoran},
keywords = {Deep neural networks, Data augmentation, Off-axis, Iris segmentation, AR/VR},
abstract = {A data augmentation methodology is presented and applied to generate a large dataset of off-axis iris regions and train a low-complexity deep neural network. Although of low complexity the resulting network achieves a high level of accuracy in iris region segmentation for challenging off-axis eye-patches. Interestingly, this network is also shown to achieve high levels of performance for regular, frontal, segmentation of iris regions, comparing favourably with state-of-the-art techniques of significantly higher complexity. Due to its lower complexity this network is well suited for deployment in embedded applications such as augmented and mixed reality headsets.}
}
@article{JIANG2020452,
title = {Synchronization in an array of coupled neural networks with delayed impulses: Average impulsive delay method},
journal = {Neural Networks},
volume = {121},
pages = {452-460},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302849},
author = {Bangxin Jiang and Jianquan Lu and Jungang Lou and Jianlong Qiu},
keywords = {Synchronization, Coupled neural networks, Delayed impulse, Average impulsive delay (AID), Average impulsive interval (AII)},
abstract = {In the paper, synchronization of coupled neural networks with delayed impulses is investigated. In order to overcome the difficulty that time delays can be flexible and even larger than impulsive interval, we propose a new method of average impulsive delay (AID). By the methods of average impulsive interval (AII) and AID, some sufficient synchronization criteria for coupled neural networks with delayed impulses are obtained. We prove that the time delay in impulses can play double roles, namely, it may desynchronize a synchronous network or synchronize a nonsynchronized network. Moreover, a unified relationship is established among AII, AID and rate coefficients of the impulsive dynamical network such that the network is globally exponentially synchronized (GES). Further, we discuss the case that time delays in impulses may be unbounded, which has not been considered in existing results. Finally, two examples are presented to demonstrate the validity of the derived results.}
}
@article{BRITODASILVA2020208,
title = {Distributed dual vigilance fuzzy adaptive resonance theory learns online, retrieves arbitrarily-shaped clusters, and mitigates order dependence},
journal = {Neural Networks},
volume = {121},
pages = {208-228},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.033},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302606},
author = {Leonardo Enzo {Brito da Silva} and Islam Elnabarawy and Donald C. Wunsch},
keywords = {Fuzzy, Adaptive Resonance theory, Clustering, Distributed representation, Topology, Visual assessment of cluster tendency},
abstract = {This paper presents a novel adaptive resonance theory (ART)-based modular architecture for unsupervised learning, namely the distributed dual vigilance fuzzy ART (DDVFA). DDVFA consists of a global ART system whose nodes are local fuzzy ART modules. It is equipped with distributed higher-order activation and match functions and a dual vigilance mechanism. Together, these allow DDVFA to perform unsupervised modularization, create multi-prototype cluster representations, retrieve arbitrarily-shaped clusters, and reduce category proliferation. Another important contribution is the reduction of order-dependence, an issue that affects any agglomerative clustering method. This paper demonstrates two approaches for mitigating order-dependence: pre-processing using visual assessment of cluster tendency (VAT) or post-processing using a novel Merge ART module. The former is suitable for batch processing, whereas the latter also works for online learning. Experimental results in online mode carried out on 30 benchmark data sets show that DDVFA cascaded with Merge ART statistically outperformed the best other ART-based systems when samples were randomly presented. Conversely, they were found to be statistically equivalent in offline mode when samples were pre-processed using VAT. Remarkably, performance comparisons to non-ART-based clustering algorithms show that DDVFA (which learns incrementally) was also statistically equivalent to the non-incremental (offline) methods of density-based spatial clustering of applications with noise (DBSCAN), single linkage hierarchical agglomerative clustering (SL-HAC), and k-means, while retaining the appealing properties of ART. Links to the source code and data are provided. Considering the algorithm’s simplicity, online learning capability, and performance, it is an ideal choice for many agglomerative clustering applications.}
}
@article{WANDETO2019273,
title = {The quantization error in a Self-Organizing Map as a contrast and colour specific indicator of single-pixel change in large random patterns},
journal = {Neural Networks},
volume = {119},
pages = {273-285},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930231X},
author = {John M. Wandeto and Birgitta Dresp-Langley},
keywords = {Self-Organizing Maps, Quantization error, Image time series, Medical images, Random-dot images, Change detection},
abstract = {The quantization error in a fixed-size Self-Organizing Map (SOM) with unsupervised winner-take-all learning has previously been used successfully to detect, in minimal computation time, highly meaningful changes across images in medical time series and in time series of satellite images. Here, the functional properties of the quantization error in SOM are explored further to show that the metric is capable of reliably discriminating between the finest differences in local contrast intensities and contrast signs. While this capability of the QE is akin to functional characteristics of a specific class of retinal ganglion cells (the so-called Y-cells) in the visual systems of the primate and the cat, the sensitivity of the QE surpasses the capacity limits of human visual detection. Here, the quantization error in the SOM is found to reliably signal changes in contrast or colour when contrast information is removed from or added to the image, but not when the amount and relative weight of contrast information is constant and only the local spatial position of contrast elements in the pattern changes. While the RGB Mean reflects coarser changes in colour or contrast well enough, the SOM-QE is shown to outperform the RGB Mean in the detection of single-pixel changes in images with up to five million pixels. This could have important implications in the context of unsupervised image learning and computational building block approaches to large sets of image data (big data), including deep learning blocks, and automatic detection of contrast change at the nanoscale in Transmission or Scanning Electron Micrographs (TEM, SEM), or at the subpixel level in multispectral and hyper-spectral imaging data.}
}
@article{MAJUMDAR2019271,
title = {Recurrent transform learning},
journal = {Neural Networks},
volume = {118},
pages = {271-279},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301911},
author = {Angshul Majumdar and Megha Gupta},
keywords = {Demand forecasting, Dynamical model, Load forecasting, Transform learning},
abstract = {Recurrent neural networks (RNN) model time series by feeding back the representation from the previous time instant as an input for the current instant along with exogenous inputs. Two main shortcomings of RNN are – 1. The problem of vanishing gradients while backpropagating through time, and 2. Inability to learn in an unsupervised manner. Variants like long-short term memory (LSTM) network and gated recurrent units (GRU) have partially circumvented the first issue; the second issue still remains. In this work we propose a new variant of RNN based on the transform learning model — named recurrent transform learning (RTL). It can learn in an unsupervised, supervised and semi-supervised fashion; it does not require backpropagation and hence do not suffer from the pitfalls of vanishing gradients. The proposed model is applied on a real-life example of short-term load forecasting, where we show that RTL improves over existing variants of RNN as well as on a state-of-the-art technique in load forecasting based on sparse coding.}
}
@article{BRNA2019129,
title = {Uncertainty-based modulation for lifelong learning},
journal = {Neural Networks},
volume = {120},
pages = {129-142},
year = {2019},
note = {special Issue in Honor of the 80th Birthday of Stephen Grossberg},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302722},
author = {Andrew P. Brna and Ryan C. Brown and Patrick M. Connolly and Stephen B. Simons and Renee E. Shimizu and Mario Aguilar-Simon},
keywords = {Self-supervision, Few-shot learning, Lifelong learning, Catastrophic forgetting},
abstract = {The creation of machine learning algorithms for intelligent agents capable of continuous, lifelong learning is a critical objective for algorithms being deployed on real-life systems in dynamic environments. Here we present an algorithm inspired by neuromodulatory mechanisms in the human brain that integrates and expands upon Stephen Grossberg’s ground-breaking Adaptive Resonance Theory proposals. Specifically, it builds on the concept of uncertainty, and employs a series of “neuromodulatory” mechanisms to enable continuous learning, including self-supervised and one-shot learning. Algorithm components were evaluated in a series of benchmark experiments that demonstrate stable learning without catastrophic forgetting. We also demonstrate the critical role of developing these systems in a closed-loop manner where the environment and the agent’s behaviors constrain and guide the learning process. To this end, we integrated the algorithm into an embodied simulated drone agent. The experiments show that the algorithm is capable of continuous learning of new tasks and under changed conditions with high classification accuracy (>94%) in a virtual environment, without catastrophic forgetting. The algorithm accepts high dimensional inputs from any state-of-the-art detection and feature extraction algorithms, making it a flexible addition to existing systems. We also describe future development efforts focused on imbuing the algorithm with mechanisms to seek out new knowledge as well as employ a broader range of neuromodulatory processes.}
}
@article{BAO2019190,
title = {Non-fragile state estimation for fractional-order delayed memristive BAM neural networks},
journal = {Neural Networks},
volume = {119},
pages = {190-199},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302205},
author = {Haibo Bao and Ju H. Park and Jinde Cao},
keywords = {State estimation, BAM neural networks, Memristive, Fractional-order, Non-fragile},
abstract = {This paper deals with the non-fragile state estimation problem for a class of fractional-order memristive BAM neural networks (FMBAMNNs) with and without time delays for the first time. By means of a novel transformation and interval matrix approach, non-fragile estimators are designed and parameter mismatch problem is averted. Sufficient criteria are established to ascertain the error system is asymptotically stable based on fractional-order Lyapunov functionals and linear matrix inequalities (LMIs). Two examples are put forward to show the effectiveness of the obtained results.}
}
@article{SHEN201974,
title = {Nonlinear approximation via compositions},
journal = {Neural Networks},
volume = {119},
pages = {74-84},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301996},
author = {Zuowei Shen and Haizhao Yang and Shijun Zhang},
keywords = {Deep neural networks, ReLU activation function, Nonlinear approximation, Function composition, Hölder continuity, Parallel computing},
abstract = {Given a function dictionary D and an approximation budget N∈N, nonlinear approximation seeks the linear combination of the best N terms {Tn}1≤n≤N⊆D to approximate a given function f with the minimum approximation error εL,f≔min{gn}⊆R,{Tn}⊆D‖f(x)−∑n=1NgnTn(x)‖.Motivated by recent success of deep learning, we propose dictionaries with functions in a form of compositions, i.e., T(x)=T(L)∘T(L−1)∘⋯∘T(1)(x)for all T∈D, and implement T using ReLU feed-forward neural networks (FNNs) with L hidden layers. We further quantify the improvement of the best N-term approximation rate in terms of N when L is increased from 1 to 2 or 3 to show the power of compositions. In the case when L>3, our analysis shows that increasing L cannot improve the approximation rate in terms of N. In particular, for any function f on [0,1], regardless of its smoothness and even the continuity, if f can be approximated using a dictionary when L=1 with the best N-term approximation rate εL,f=O(N−η), we show that dictionaries with L=2 can improve the best N-term approximation rate to εL,f=O(N−2η). We also show that for Hölder continuous functions of order α on [0,1]d, the application of a dictionary with L=3 in nonlinear approximation can achieve an essentially tight best N-term approximation rate εL,f=O(N−2α∕d). Finally, we show that dictionaries consisting of wide FNNs with a few hidden layers are more attractive in terms of computational efficiency than dictionaries with narrow and very deep FNNs for approximating Hölder continuous functions if the number of computer cores is larger than N in parallel computing.}
}
@article{XU2019332,
title = {Synchronization of chaotic neural networks with time delay via distributed delayed impulsive control},
journal = {Neural Networks},
volume = {118},
pages = {332-337},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930190X},
author = {Zhilu Xu and Dongxue Peng and Xiaodi Li},
keywords = {Distributed delayed impulsive control, Time delay, Chaotic neural networks, Synchronization},
abstract = {This letter investigates the impulsive synchronization of chaotic neural networks with time delays. A novel impulsive delayed inequality is proposed, where the control effect of distributed delayed impulses is fully considered. Based on the inequality, a distributed delayed impulsive controller is proposed for exponential synchronization of chaotic neural networks. Finally, numerical examples are provided to illustrate the effectiveness of the proposed theoretical results.}
}
@article{VALADEZGODINEZ2020196,
title = {On the accuracy and computational cost of spiking neuron implementation},
journal = {Neural Networks},
volume = {122},
pages = {196-217},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.026},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303065},
author = {Sergio Valadez-Godínez and Humberto Sossa and Raúl Santiago-Montero},
keywords = {Spiking neuron, Accuracy, Computational cost, Numerical method, Simulation, Time step},
abstract = {Since more than a decade ago, three statements about spiking neuron (SN) implementations have been widely accepted: 1) Hodgkin and Huxley (HH) model is computationally prohibitive, 2) Izhikevich (IZH) artificial neuron is as efficient as Leaky Integrate-and-Fire (LIF) model, and 3) IZH model is more efficient than HH model (Izhikevich, 2004). As suggested by Hodgkin and Huxley (1952), their model operates in two modes: by using the α’s and β’s rate functions directly (HH model) and by storing them into tables (HHT model) for computational cost reduction. Recently, it has been stated that: 1) HHT model (HH using tables) is not prohibitive, 2) IZH model is not efficient, and 3) both HHT and IZH models are comparable in computational cost (Skocik & Long, 2014). That controversy shows that there is no consensus concerning SN simulation capacities. Hence, in this work, we introduce a refined approach, based on the multiobjective optimization theory, describing the SN simulation capacities and ultimately choosing optimal simulation parameters. We have used normalized metrics to define the capacity levels of accuracy, computational cost, and efficiency. Normalized metrics allowed comparisons between SNs at the same level or scale. We conducted tests for balanced, lower, and upper boundary conditions under a regular spiking mode with constant and random current stimuli. We found optimal simulation parameters leading to a balance between computational cost and accuracy. Importantly, and, in general, we found that 1) HH model (without using tables) is the most accurate, computationally inexpensive, and efficient, 2) IZH model is the most expensive and inefficient, 3) both LIF and HHT models are the most inaccurate, 4) HHT model is more expensive and inaccurate than HH model due to α’s and β’s table discretization, and 5) HHT model is not comparable in computational cost to IZH model. These results refute the theory formulated over a decade ago (Izhikevich, 2004) and go more in-depth in the statements formulated by Skocik and Long (2014). Our statements imply that the number of dimensions or FLOPS in the SNs are theoretical but not practical indicators of the true computational cost. The metric we propose for the computational cost is more precise than FLOPS and was found to be invariant to computer architecture. Moreover, we found that the firing frequency used in previous works is a necessary but an insufficient metric to evaluate the simulation accuracy. We also show that our results are consistent with the theory of numerical methods and the theory of SN discontinuity. Discontinuous SNs, such LIF and IZH models, introduce a considerable error every time a spike is generated. In addition, compared to the constant input current, the random input current increases the computational cost and inaccuracy. Besides, we found that the search for optimal simulation parameters is problem-specific. That is important because most of the previous works have intended to find a general and unique optimal simulation. Here, we show that this solution could not exist because it is a multiobjective optimization problem that depends on several factors. This work sets up a renewed thesis concerning the SN simulation that is useful to several related research areas, including the emergent Deep Spiking Neural Networks.}
}
@article{TAN201958,
title = {Self-organizing neural networks for universal learning and multimodal memory encoding},
journal = {Neural Networks},
volume = {120},
pages = {58-73},
year = {2019},
note = {special Issue in Honor of the 80th Birthday of Stephen Grossberg},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.020},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302370},
author = {Ah-Hwee Tan and Budhitama Subagdja and Di Wang and Lei Meng},
keywords = {Adaptive resonance theory, Universal learning, Memory encoding},
abstract = {Learning and memory are two intertwined cognitive functions of the human brain. This paper shows how a family of biologically-inspired self-organizing neural networks, known as fusion Adaptive Resonance Theory (fusion ART), may provide a viable approach to realizing the learning and memory functions. Fusion ART extends the single-channel Adaptive Resonance Theory (ART) model to learn multimodal pattern associative mappings. As a natural extension of ART, various forms of fusion ART have been developed for a myriad of learning paradigms, ranging from unsupervised learning to supervised learning, semi-supervised learning, multimodal learning, reinforcement learning, and sequence learning. In addition, fusion ART models may be used for representing various types of memories, notably episodic memory, semantic memory and procedural memory. In accordance with the notion of embodied intelligence, such neural models thus provide a computational account of how an autonomous agent may learn and adapt in a real-world environment. The efficacy of fusion ART in learning and memory shall be discussed through various examples and illustrative case studies.}
}
@article{WANG2020430,
title = {A sparse deep belief network with efficient fuzzy learning framework},
journal = {Neural Networks},
volume = {121},
pages = {430-440},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.035},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930317X},
author = {Gongming Wang and Qing-Shan Jia and Junfei Qiao and Jing Bi and Caixia Liu},
keywords = {Deep belief network, Deep learning, Sparse representation, Fuzzy neural network, Nonlinear system modeling},
abstract = {Deep belief network (DBN) is one of the most feasible ways to realize deep learning (DL) technique, and it has been attracting more and more attentions in nonlinear system modeling. However, DBN cannot provide satisfactory results in learning speed, modeling accuracy and robustness, which is mainly caused by dense representation and gradient diffusion. To address these problems and promote DBN’s development in cross-models, we propose a Sparse Deep Belief Network with Fuzzy Neural Network (SDBFNN) for nonlinear system modeling. In this novel framework, the sparse DBN is considered as a pre-training technique to realize fast weight-initialization and to obtain feature vectors. It can balance the dense representation to improve its robustness. A fuzzy neural network is developed for supervised modeling so as to eliminate the gradient diffusion. Its input happens to be the obtained feature vector. As a novel cross-model, SDBFNN combines the advantages of both pre-training technique and fuzzy neural network to improve modeling capability. Its convergence is also analyzed as well. A benchmark problem and a practical problem in wastewater treatment are conducted to demonstrate the superiority of SDBFNN. The extensive experimental results show that SDBFNN achieves better performance than the existing methods in learning speed, modeling accuracy and robustness.}
}
@article{DECASTRO202054,
title = {A broad class of discrete-time hypercomplex-valued Hopfield neural networks},
journal = {Neural Networks},
volume = {122},
pages = {54-67},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.040},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303223},
author = {Fidelis Zanetti {de Castro} and Marcos Eduardo Valle},
keywords = {Hopfield neural network, Hypercomplex-valued neural network, Stability analysis, Clifford algebra, Cayley–Dickson algebra},
abstract = {In this paper, we address the stability of a broad class of discrete-time hypercomplex-valued Hopfield-type neural networks. To ensure the neural networks belonging to this class always settle down at a stationary state, we introduce novel hypercomplex number systems referred to as real-part associative hypercomplex number systems. Real-part associative hypercomplex number systems generalize the well-known Cayley–Dickson algebras and real Clifford algebras and include the systems of real numbers, complex numbers, dual numbers, hyperbolic numbers, quaternions, tessarines, and octonions as particular instances. Apart from the novel hypercomplex number systems, we introduce a family of hypercomplex-valued activation functions called B-projection functions. Broadly speaking, a B-projection function projects the activation potential onto the set of all possible states of a hypercomplex-valued neuron. Using the theory presented in this paper, we confirm the stability analysis of several discrete-time hypercomplex-valued Hopfield-type neural networks from the literature. Moreover, we introduce and provide the stability analysis of a general class of Hopfield-type neural networks on Cayley–Dickson algebras.}
}
@article{TAHERKHANI2020253,
title = {A review of learning in biologically plausible spiking neural networks},
journal = {Neural Networks},
volume = {122},
pages = {253-272},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.036},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303181},
author = {Aboozar Taherkhani and Ammar Belatreche and Yuhua Li and Georgina Cosma and Liam P. Maguire and T.M. McGinnity},
keywords = {Spiking neural network (SNN), Learning, Synaptic plasticity},
abstract = {Artificial neural networks have been used as a powerful processing tool in various areas such as pattern recognition, control, robotics, and bioinformatics. Their wide applicability has encouraged researchers to improve artificial neural networks by investigating the biological brain. Neurological research has significantly progressed in recent years and continues to reveal new characteristics of biological neurons. New technologies can now capture temporal changes in the internal activity of the brain in more detail and help clarify the relationship between brain activity and the perception of a given stimulus. This new knowledge has led to a new type of artificial neural network, the Spiking Neural Network (SNN), that draws more faithfully on biological properties to provide higher processing abilities. A review of recent developments in learning of spiking neurons is presented in this paper. First the biological background of SNN learning algorithms is reviewed. The important elements of a learning algorithm such as the neuron model, synaptic plasticity, information encoding and SNN topologies are then presented. Then, a critical review of the state-of-the-art learning algorithms for SNNs using single and multiple spikes is presented. Additionally, deep spiking neural networks are reviewed, and challenges and opportunities in the SNN field are discussed.}
}
@article{FUKUSHIMA2019323,
title = {Efficient IntVec: High recognition rate with reduced computational cost},
journal = {Neural Networks},
volume = {119},
pages = {323-331},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.024},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302497},
author = {Kunihiko Fukushima},
keywords = {Pattern recognition, Interpolating-vector, Neocognitron, Deep CNN, Computational cost},
abstract = {In many deep neural networks for pattern recognition, the input pattern is classified in the deepest layer based on features extracted through intermediate layers. IntVec (interpolating-vector) is known to be a powerful method for this process of classification. Although the recognition error can be made much smaller by IntVec than by WTA (winner-take-all) or even by SVM (support vector machines), IntVec requires a large computational cost. This paper proposes a new method, by which the computational cost by IntVec can be reduced drastically without increasing the recognition error. Although we basically use IntVec for recognition, we substitute it with WTA, which requires much smaller computational cost, under a certain condition. To be more specific, we first try to classify the input vector using WTA. If a class is a complete loser by WTA, we judge it also a loser by IntVec and omit the calculation of IntVec for that class. If a class is an unrivaled winner by WTA, calculation of IntVec itself can be omitted for all classes.}
}
@article{ADIGUN20199,
title = {Noise-boosted bidirectional backpropagation and adversarial learning},
journal = {Neural Networks},
volume = {120},
pages = {9-31},
year = {2019},
note = {special Issue in Honor of the 80th Birthday of Stephen Grossberg},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302771},
author = {Olaoluwa Adigun and Bart Kosko},
keywords = {Bidirectional backpropagation, Neural networks, Noise benefit, Stochastic resonance, Expectation–Maximization algorithm, Bidirectional associative memory},
abstract = {Bidirectional backpropagation trains a neural network with backpropagation in both the backward and forward directions using the same synaptic weights. Special injected noise can then improve the algorithm’s training time and accuracy because backpropagation has a likelihood structure. Training in each direction is a form of generalized expectation–maximization because backpropagation itself is a form of generalized expectation–maximization. This requires backpropagation invariance in each direction: The gradient log-likelihood in each direction must give back the original update equations of the backpropagation algorithm. The special noise makes the current training signal more probable as bidirectional backpropagation climbs the nearest hill of joint probability or log-likelihood. The noise for injection differs for classification and regression even in the same network because of the constraint of backpropagation invariance. The backward pass in a bidirectionally trained classifier estimates the centroid of the input pattern class. So the feedback signal that arrives back at the input layer of a classifier tends to estimate the local pattern-class centroid. Simulations show that noise speeded convergence and improved the accuracy of bidirectional backpropagation on both the MNIST test set of hand-written digits and the CIFAR-10 test set of images. The noise boost further applies to regular and Wasserstein bidirectionally trained adversarial networks. Bidirectionality also greatly reduced the problem of mode collapse in regular adversarial networks.}
}
@article{LEVINE201974,
title = {One or two minds? Neural network modeling of decision making by the unified self},
journal = {Neural Networks},
volume = {120},
pages = {74-85},
year = {2019},
note = {special Issue in Honor of the 80th Birthday of Stephen Grossberg},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302254},
author = {Daniel S. Levine},
keywords = {Decision making, Adaptive resonance theory, Fuzzy trace theory, Amygdala, Prefrontal cortex, Anterior cingulate},
abstract = {Ever since the seminal work of Tversky and Kahneman starting in the late 1960s, it has generally been accepted that many characteristic human decision patterns do not follow the norms of economic theories based on rational utility maximization and consistency across frames. Yet people do often make decisions and numerical judgments that are mathematically consistent. In order to account for the range of rational, intuitive, and emotional influences on decision making, prominent psychologists have developed a number of dual-process or dual-system theories of decision processes. Among these theories are System 1 and System 2, Cognitive-Experiential Self-theory, and Fuzzy Trace Theory. Fuzzy Trace Theory (FTT) is the most amenable of these theories to a unified biologically based neural network approach. This article reviews several years of research on a decision theory that integrates the gist and verbatim representations of FTT into a network model. The model is based on several constructs by Grossberg and his colleagues including Adaptive Resonance Theory and Gated Dipole Theory, combined with selective attention and probabilistic distribution of some parameters representing individual differences in decision style. It incorporates data on the functions of several brain regions including sensory cortex, amygdala, orbitofrontal cortex, ventral striatum, thalamus, and anterior cingulate, and premotor cortex.}
}
@article{LI201993,
title = {Low-rank analysis–synthesis dictionary learning with adaptively ordinal locality},
journal = {Neural Networks},
volume = {119},
pages = {93-112},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302011},
author = {Zhengming Li and Zheng Zhang and Jie Qin and Sheng Li and Hongmin Cai},
keywords = {Low-rank, Analysis dictionary learning, Synthesis dictionary learning, Object classification, Ordinal locality},
abstract = {Analysis dictionary learning (ADL) has been successfully applied to a variety of learning systems. However, the ordinal locality of analysis dictionary has rarely been explored in constructing discriminative terms. In this paper, a discriminative low-rank analysis–synthesis dictionary learning (LR-ASDL) algorithm with the adaptively ordinal locality is proposed for object classification. Specifically, we first explicitly introduce the relations between the analysis atoms and profiles (i.e., row vectors of the coefficients matrix). That is, the similarity between two profiles depends on that between the corresponding analysis atoms. Moreover, an adaptively ordinal locality preserving(AOLP) term is constructed by simultaneously exploiting the profiles and analysis atoms, which can be learned in a supervised way. In this way, the neighborhood correlations between analysis atoms and the high-order ranking information of each analysis atom’s neighbors can be simultaneously preserved in the learning process. Particularly, this helps to uncover the intrinsic underlying data factors and inherit the geometry structure information of training samples. Furthermore, the low-rank model is imposed on the synthesis atoms to further facilitate the learned dictionaries to be more discriminative. Extensive experimental results on eight databases demonstrate that the LR-ASDL algorithm clearly outperforms some analysis and synthesis dictionary learning algorithms using deep and hand-crafted features.}
}
@article{SANTOSPATA201966,
title = {Modulating grid cell scale and intrinsic frequencies via slow high-threshold conductances: A simplified model},
journal = {Neural Networks},
volume = {119},
pages = {66-73},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301844},
author = {Diogo Santos-Pata and Riccardo Zucca and Héctor López-Carral and Paul F.M.J. Verschure},
keywords = {Grid cells, Entorhinal, Hyperpolarization, Navigation, Space},
abstract = {Grid cells in the medial entorhinal cortex (MEC) have known spatial periodic firing fields which provide a metric for the representation of self-location and path planning. The hexagonal tessellation pattern of grid cells scales up progressively along the MEC’s layer II dorsal-to-ventral axis. This scaling gradient has been hypothesized to originate either from inter-population synaptic dynamics as postulated by attractor networks, or from projected theta frequency waves to different axis levels, as in oscillatory models. Alternatively, cellular dynamics and specifically slow high-threshold conductances have been proposed to have an impact on the grid cell scale. To test the hypothesis that intrinsic hyperpolarization-activated cation currents account for both the scaled gradient and the oscillatory frequencies observed along the dorsal-to-ventral axis, we have modeled and analyzed data from a population of grid cells simulated with spiking neurons interacting through low-dimensional attractor dynamics. We observed that the intrinsic neuronal membrane properties of simulated cells were sufficient to induce an increase in grid scale and potentiate differences in the membrane potential oscillatory frequency. Overall, our results suggest that the after-spike dynamics of cation currents may play a major role in determining the grid cells’ scale and that oscillatory frequencies are a consequence of intrinsic cellular properties that are specific to different levels of the dorsal-to-ventral axis in the MEC layer II.}
}
@article{BOLOUKIAN2020186,
title = {Recognition of words from brain-generated signals of speech-impaired people: Application of autoencoders as a neural Turing machine controller in deep neural networks},
journal = {Neural Networks},
volume = {121},
pages = {186-207},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930200X},
author = {Behzad Boloukian and Faramarz Safi-Esfahani},
keywords = {Speech disabilities, Brain–computer interface, EEG, Brain signals, Neural Turing machine, Deep neural networks},
abstract = {There is an essential requirement to support people with speech and communication disabilities. A brain–computer interface using electroencephalography (EEG) is applied to satisfy this requirement. A number of research studies to recognize brain signals using machine learning and deep neural networks (DNNs) have been performed to increase the brain signal detection rate, yet there are several defects and limitations in the techniques. Among them is the use in specific circumstances of machine learning. On the one hand, DNNs extract the features well and automatically. On the other hand, their use results in overfitting and vanishing problems. Consequently, in this research, a deep network is designed on the basis of an autoencoder neural Turing machine (DN-AE-NTM) to resolve the problems by the use of NTM external memory. In addition, the DN-AE-NTM copes with all kinds of signals with high detection rates. The data were collected by P300 EEG devices from several individuals under the same conditions. During the test, each individual was requested to skim images with one to six labels and focus on only one of the images. Not to focus on some images is analogous to producing unimportant information in the individual’s brain, which provides unfamiliar signals. Besides the main P300 EEG dataset, EEG recordings of individuals with alcoholism and control individuals and the EEGMMIDB, MNIST, and ORL datasets were implemented and tested. The proposed DN-AE-NTM method classifies data with an average detection rate of 97.5%, 95%, 98%, 99.4%, and 99.1%, respectively, in situations where the signals are noisy so that only 20% of the data are reliable and include useful information.}
}
@article{LOBO202088,
title = {Spiking Neural Networks and online learning: An overview and perspectives},
journal = {Neural Networks},
volume = {121},
pages = {88-100},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302655},
author = {Jesus L. Lobo and Javier {Del Ser} and Albert Bifet and Nikola Kasabov},
keywords = {Online learning, Spiking Neural Networks, Stream data, Concept drift},
abstract = {Applications that generate huge amounts of data in the form of fast streams are becoming increasingly prevalent, being therefore necessary to learn in an online manner. These conditions usually impose memory and processing time restrictions, and they often turn into evolving environments where a change may affect the input data distribution. Such a change causes that predictive models trained over these stream data become obsolete and do not adapt suitably to new distributions. Specially in these non-stationary scenarios, there is a pressing need for new algorithms that adapt to these changes as fast as possible, while maintaining good performance scores. Unfortunately, most off-the-shelf classification models need to be retrained if they are used in changing environments, and fail to scale properly. Spiking Neural Networks have revealed themselves as one of the most successful approaches to model the behavior and learning potential of the brain, and exploit them to undertake practical online learning tasks. Besides, some specific flavors of Spiking Neural Networks can overcome the necessity of retraining after a drift occurs. This work intends to merge both fields by serving as a comprehensive overview, motivating further developments that embrace Spiking Neural Networks for online learning scenarios, and being a friendly entry point for non-experts.}
}
@article{JIN2020474,
title = {Adaptive fault-tolerant consensus for a class of leader-following systems using neural network learning strategy},
journal = {Neural Networks},
volume = {121},
pages = {474-483},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.028},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303107},
author = {Xiaozheng Jin and Xianfeng Zhao and Jiguo Yu and Xiaoming Wu and Jing Chi},
keywords = {Leader-following consensus, Adaptive fault tolerant control, Neural network learning strategy, Actuator faults},
abstract = {In this paper, the leader-following consensus problem of a class of nonlinearly multi-dimensional multi-agent systems with actuator faults is addressed by developing a novel neural network learning strategy. In order to achieve the desirable consensus results, a neural network learning algorithm composed of adaptive technique is proposed to on-line approximate the unknown nonlinear functions and estimate the unknown bounds of actuator faults. Then, on the basis of the approximations and estimations, a robust adaptive distributed fault-tolerant consensus control scheme is investigated so that the bounded results of all signals of the resulting closed-loop leader-following system can be achieved by using Lyapunov stability theorem. Finally, efficiency of the proposed adaptive neural network learning strategy-based consensus control strategies is demonstrated by a coupled nonlinear forced pendulums system.}
}
@article{WANG2019249,
title = {Periodicity and finite-time periodic synchronization of discontinuous complex-valued neural networks},
journal = {Neural Networks},
volume = {119},
pages = {249-260},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.021},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302382},
author = {Zengyun Wang and Jinde Cao and Zuowei Cai and Lihong Huang},
keywords = {Discontinuous complex-valued neural networks, Differential inclusion, Periodic solution, Finite-time periodic synchronization, Kakutans fixed point theorem},
abstract = {This paper discusses the issue of periodicity and finite-time periodic synchronization of discontinuous complex-valued neural networks (CVNNs). Based on a modified version of Kakutani’s fixed point theorem, general conditions are obtained to guarantee the periodicity of discontinuous CVNNs. Next, several criteria for finite-time periodic synchronization (FTPS) are given by using a new proposed finite-time convergence theorem. Different from the traditional convergence lemma, the estimated upper bound of the derivative of the Lyapunov function (LF) is allowed to be indefinite or even positive. In order to achieve FTPS, novel discontinuous control algorithms, including state-feedback control algorithm and generalized pinning control algorithm, are designed. In the generalized pinning control algorithm, a guideline is proposed to select neurons to pin the designed controller. Finally, two simulations are given to substantiate the main results.}
}
@article{GONG2020484,
title = {Privacy-enhanced multi-party deep learning},
journal = {Neural Networks},
volume = {121},
pages = {484-496},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303235},
author = {Maoguo Gong and Jialun Feng and Yu Xie},
keywords = {Privacy, Multi-party deep learning, Differential privacy, Homomorphic encryption, Privacy budget},
abstract = {In multi-party deep learning, multiple participants jointly train a deep learning model through a central server to achieve common objectives without sharing their private data. Recently, a significant amount of progress has been made toward the privacy issue of this emerging multi-party deep learning paradigm. In this paper, we mainly focus on two problems in multi-party deep learning. The first problem is that most of the existing works are incapable of defending simultaneously against the attacks of honest-but-curious participants and an honest-but-curious server without a manager trusted by all participants. To tackle this problem, we design a privacy-enhanced multi-party deep learning framework, which integrates differential privacy and homomorphic encryption to prevent potential privacy leakage to other participants and a central server without requiring a manager that all participants trust. The other problem is that existing frameworks consume high total privacy budget when applying differential privacy for preserving privacy, which leads to a high risk of privacy leakage. In order to alleviate this problem, we propose three strategies for dynamically allocating privacy budget at each epoch to further enhance privacy guarantees without compromising the model utility. Moreover, it provides participants with an intuitive handle to strike a balance between the privacy level and the training efficiency by choosing different strategies. Both analytical and experimental evaluations demonstrate the promising performance of our proposed framework.}
}
@article{KAMRAVA2019310,
title = {Enhancing images of shale formations by a hybrid stochastic and deep learning algorithm},
journal = {Neural Networks},
volume = {118},
pages = {310-320},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301972},
author = {Serveh Kamrava and Pejman Tahmasebi and Muhammad Sahimi},
keywords = {Deep learning, Stochastic modeling, Shale formation, Imaging},
abstract = {Accounting for the morphology of shale formations, which represent highly heterogeneous porous media, is a difficult problem. Although two- or three-dimensional images of such formations may be obtained and analyzed, they either do not capture the nanoscale features of the porous media, or they are too small to be an accurate representative of the media, or both. Increasing the resolution of such images is also costly. While high-resolution images may be used to train a deep-learning network in order to increase the quality of low-resolution images, an important obstacle is the lack of a large number of images for the training, as the accuracy of the network’s predictions depends on the extent of the training data. Generating a large number of high-resolution images by experimental means is, however, very time consuming and costly, hence limiting the application of deep-learning algorithms to such an important class of problems. To address the issue we propose a novel hybrid algorithm by which a stochastic reconstruction method is used to generate a large number of plausible images of a shale formation, using very few input images at very low cost, and then train a deep-learning convolutional network by the stochastic realizations. We refer to the method as hybrid stochastic deep-learning (HSDL) algorithm. The results indicate promising improvement in the quality of the images, the accuracy of which is confirmed by visual, as well as quantitative comparison between several of their statistical properties. The results are also compared with those obtained by the regular deep learning algorithm without using an enriched and large dataset for training, as well as with those generated by bicubic interpolation.}
}
@article{PATEL2019108,
title = {Improved robustness of reinforcement learning policies upon conversion to spiking neuronal network platforms applied to Atari Breakout game},
journal = {Neural Networks},
volume = {120},
pages = {108-115},
year = {2019},
note = {special Issue in Honor of the 80th Birthday of Stephen Grossberg},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302266},
author = {Devdhar Patel and Hananel Hazan and Daniel J. Saunders and Hava T. Siegelmann and Robert Kozma},
keywords = {Spiking neural networks, Reinforcement learning, Deep learning, Robustness, Atari},
abstract = {Deep Reinforcement Learning (RL) demonstrates excellent performance on tasks that can be solved by trained policy. It plays a dominant role among cutting-edge machine learning approaches using multi-layer Neural networks (NNs). At the same time, Deep RL suffers from high sensitivity to noisy, incomplete, and misleading input data. Following biological intuition, we involve Spiking Neural Networks (SNNs) to address some deficiencies of deep RL solutions. Previous studies in image classification domain demonstrated that standard NNs (with ReLU nonlinearity) trained using supervised learning can be converted to SNNs with negligible deterioration in performance. In this paper, we extend those conversion results to the domain of Q-Learning NNs trained using RL. We provide a proof of principle of the conversion of standard NN to SNN. In addition, we show that the SNN has improved robustness to occlusion in the input image. Finally, we introduce results with converting full-scale Deep Q-network to SNN, paving the way for future research to robust Deep RL applications.}
}
@article{VIRGILIOG2020130,
title = {Spiking Neural Networks applied to the classification of motor tasks in EEG signals},
journal = {Neural Networks},
volume = {122},
pages = {130-143},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.037},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303193},
author = {Carlos D. {Virgilio G.} and Juan H. {Sossa A.} and Javier M. Antelis and Luis E. Falcón},
keywords = {Spiking Neural Network, Izhikevich model, EEG signals, Motor imagery, Power Spectral Density, Wavelet Decomposition},
abstract = {Motivated by the recent progress of Spiking Neural Network (SNN) models in pattern recognition, we report on the development and evaluation of brain signal classifiers based on SNNs. The work shows the capabilities of this type of Spiking Neurons in the recognition of motor imagery tasks from EEG signals and compares their performance with other traditional classifiers commonly used in this application. This work includes two stages: the first stage consists of comparing the performance of the SNN models against some traditional neural network models. The second stage, compares the SNN models performance in two input conditions: input features with constant values and input features with temporal information. The EEG signals employed in this work represent five motor imagery tasks: i.e. rest, left hand, right hand, foot and tongue movements. These EEG signals were obtained from a public database provided by the Technological University of Graz (Brunner et al., 2008). The feature extraction stage was performed by applying two algorithms: power spectral density and wavelet decomposition. Likewise, this work uses raw EEG signals for the second stage of the problem solution. All of the models were evaluated in the classification between two motor imagery tasks. This work demonstrates that with a smaller number of Spiking neurons, simple problems can be solved. Better results are obtained by using patterns with temporal information, thereby exploiting the capabilities of the SNNs.}
}
@article{HAO201957,
title = {A subgraph-representation-based method for answering complex questions over knowledge bases},
journal = {Neural Networks},
volume = {119},
pages = {57-65},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302023},
author = {Zhifeng Hao and Biao Wu and Wen Wen and Ruichu Cai},
keywords = {Question answering, Knowledge base, Distributional representation, Memory networks, Recurrent neural network},
abstract = {Knowledge-based question answering has attracted a lot of attention in the research communities of natural language processing and information retrieval. However, existing studies do not adequately address the problem of answering complex questions which involve multiple entities and require extraction of facts from multiple relations. To address this issue, we propose a novel approach which learns the distributional representations of questions and candidate answers in a unified deep-learning framework based on directed-acyclic-graph-structured long short-term memory and memory networks. Specifically, the questions are encoded to match candidate directed acyclic subgraphs of the knowledge base, which are able to include information related to multiple entities and relations in the complex questions. The experimental results show that the proposed approach outperforms other methods on the widely used dataset SPADES, especially when dealing with complex questions with multiple entities.}
}
@article{XIE2019261,
title = {Distributed semi-supervised learning algorithm based on extreme learning machine over networks using event-triggered communication scheme},
journal = {Neural Networks},
volume = {119},
pages = {261-272},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302308},
author = {Jin Xie and Sanyang Liu and Hao Dai},
keywords = {Distributed learning (DL), Semi-supervised learning (SSL), Manifold regularization (MR), Zero gradient sum (ZGS), Extreme learning machine (ELM), Event-triggered (ET)},
abstract = {In this paper, we propose a distributed semi-supervised learning (DSSL) algorithm based on the extreme learning machine (ELM) algorithm over communication network using the event-triggered (ET) communication scheme. In DSSL problems, training data consisting of labeled and unlabeled samples are distributed over a communication network. Traditional semi-supervised learning (SSL) algorithms cannot be used to solve DSSL problems. The proposed algorithm, denoted as ET-DSS-ELM, is based on the semi-supervised ELM (SS-ELM) algorithm, the zero gradient sum (ZGS) distributed optimization strategy and the ET communication scheme. Correspondingly, the SS-ELM algorithm is used to calculate the local initial value, the ZGS strategy is used to calculate the globally optimal value and the ET scheme is used to reduce communication times during the learning process. According to the ET scheme, each node over the communication network broadcasts its updated information only when the event occurs. Therefore, the proposed ET-DSS-ELM algorithm not only takes the advantages of traditional DSSL algorithms, but also saves network resources by reducing communication times. The convergence of the proposed ET-DSS-ELM algorithm is guaranteed by using the Lyapunov method. Finally, some simulations are given to show the efficiency of the proposed algorithm.}
}
@article{DENG2020294,
title = {Rethinking the performance comparison between SNNS and ANNS},
journal = {Neural Networks},
volume = {121},
pages = {294-307},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302667},
author = {Lei Deng and Yujie Wu and Xing Hu and Ling Liang and Yufei Ding and Guoqi Li and Guangshe Zhao and Peng Li and Yuan Xie},
keywords = {Spiking neural networks, Artificial neural networks, Deep learning, Neuromorphic computing, Benchmark},
abstract = {Artificial neural networks (ANNs), a popular path towards artificial intelligence, have experienced remarkable success via mature models, various benchmarks, open-source datasets, and powerful computing platforms. Spiking neural networks (SNNs), a category of promising models to mimic the neuronal dynamics of the brain, have gained much attention for brain inspired computing and been widely deployed on neuromorphic devices. However, for a long time, there are ongoing debates and skepticisms about the value of SNNs in practical applications. Except for the low power attribute benefit from the spike-driven processing, SNNs usually perform worse than ANNs especially in terms of the application accuracy. Recently, researchers attempt to address this issue by borrowing learning methodologies from ANNs, such as backpropagation, to train high-accuracy SNN models. The rapid progress in this domain continuously produces amazing results with ever-increasing network size, whose growing path seems similar to the development of deep learning. Although these ways endow SNNs the capability to approach the accuracy of ANNs, the natural superiorities of SNNs and the way to outperform ANNs are potentially lost due to the use of ANN-oriented workloads and simplistic evaluation metrics. In this paper, we take the visual recognition task as a case study to answer the questions of “what workloads are ideal for SNNs and how to evaluate SNNs makes sense”. We design a series of contrast tests using different types of datasets (ANN-oriented and SNN-oriented), diverse processing models, signal conversion methods, and learning algorithms. We propose comprehensive metrics on the application accuracy and the cost of memory & compute to evaluate these models, and conduct extensive experiments. We evidence the fact that on ANN-oriented workloads, SNNs fail to beat their ANN counterparts; while on SNN-oriented workloads, SNNs can fully perform better. We further demonstrate that in SNNs there exists a trade-off between the application accuracy and the execution cost, which will be affected by the simulation time window and firing threshold. Based on these abundant analyses, we recommend the most suitable model for each scenario. To the best of our knowledge, this is the first work using systematical comparisons to explicitly reveal that the straightforward workload porting from ANNs to SNNs is unwise although many works are doing so and a comprehensive evaluation indeed matters. Finally, we highlight the urgent need to build a benchmarking framework for SNNs with broader tasks, datasets, and metrics.}
}
@article{WANG20201,
title = {Multi-label zero-shot human action recognition via joint latent ranking embedding},
journal = {Neural Networks},
volume = {122},
pages = {1-23},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.029},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303119},
author = {Qian Wang and Ke Chen},
keywords = {Human action recognition, Multi-label learning, Zero-shot learning, Joint latent ranking embedding, Weakly supervised learning},
abstract = {Human action recognition is one of the most challenging tasks in computer vision. Most of the existing works in human action recognition are limited to single-label classification. A real-world video stream, however, often contains multiple human actions. Such a video stream is usually annotated collectively with a set of relevant human action labels, which leads to a multi-label learning problem. Furthermore, there are a great number of meaningful human actions in reality but it would be extremely difficult, if not impossible, to collect/annotate sufficient video clips regarding all these human actions for training a supervised learning model. In this paper, we formulate a real-world human action recognition task as a multi-label zero-shot learning problem. To address this problem, a joint latent ranking embedding framework is proposed. Our framework holistically tackles the issue of unknown temporal boundaries between different actions within a video clip for multi-label learning and exploits the side information regarding the semantic relationship between different human actions for zero-shot learning. Specifically, our framework consists of two component neural networks for visual and semantic embedding respectively. Thus, multi-label zero-shot recognition is done by measuring relatedness scores of concerned action labels to a test video clip in the joint latent visual and semantic embedding spaces. We evaluate our framework in different settings, including a novel data split scheme designed especially for evaluating multi-label zero-shot learning. The experimental results on two weakly annotated multi-label human action datasets (i.e. Breakfast and Charades) demonstrate the effectiveness of our framework.}
}
@article{ZHOU2020308,
title = {Embedding topological features into convolutional neural network salient object detection},
journal = {Neural Networks},
volume = {121},
pages = {308-318},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302709},
author = {Lecheng Zhou and Xiaodong Gu},
keywords = {Conditional random field, Convolutional neural network, Salient object detection, Topological feature},
abstract = {Salient object detection can be applied as a critical preprocessing step in many computer vision tasks. Recent studies of salient object detection mainly employed convolutional neural networks (CNNs) for mining high-level semantic properties. However, the existing methods can still be improved to find precise semantic information in different scenarios. In particular, in the two main methods employed for salient object detection, the patchwise detection models might ignore spatial structures among regions and the fully convolution-based models mainly consider semantic features in a global manner. In this paper, we proposed a salient object detection framework by embedding topological features into a deep neural network for extracting semantics. We segment the input image and compute weight for each region with low-level features. The weighted segmentation result is called a topological map and it provides an additional channel for the CNN to emphasize the structural integrity and locality during the extraction of semantic features. We also utilize the topological map for saliency refinement based on a conditional random field at the end of our model. Experimental results on six benchmark datasets demonstrated that our proposed framework achieves competitive performance compared to other state-of-the-art methods.}
}