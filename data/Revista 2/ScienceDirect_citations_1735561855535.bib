@article{YANG201932,
title = {Online sequential echo state network with sparse RLS algorithm for time series prediction},
journal = {Neural Networks},
volume = {118},
pages = {32-42},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301388},
author = {Cuili Yang and Junfei Qiao and Zohaib Ahmad and Kaizhe Nie and Lei Wang},
keywords = {Echo state networks, Online sequential learning, Sparse recursive least squares algorithm, Regularization method, Time series prediction},
abstract = {Recently, the echo state networks (ESNs) have been widely used for time series prediction. To meet the demand of actual applications and avoid the overfitting issue, the online sequential ESN with sparse recursive least squares (OSESN-SRLS) algorithm is proposed. Firstly, the ℓ0 and ℓ1 norm sparsity penalty constraints of output weights are separately employed to control the network size. Secondly, the sparse recursive least squares (SRLS) algorithm and the subgradients technique are combined to estimate the output weight matrix. Thirdly, an adaptive selection mechanism for the ℓ0 or ℓ1 norm regularization parameter is designed. With the selected regularization parameter, it is proved that the developed SRLS shows comparable or better performance than the regular RLS. Furthermore, the convergence of OSESN-SRLS is theoretically analyzed to guarantee its effectiveness. Simulation results illustrate that the proposed OSESN-SRLS always outperforms other existing ESNs in terms of estimation accuracy and network compactness.}
}
@article{TAO201996,
title = {Discriminative multi-source adaptation multi-feature co-regression for visual classification},
journal = {Neural Networks},
volume = {114},
pages = {96-118},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300693},
author = {Jianwen Tao and Wei Dai},
keywords = {Multi-feature representation, Multi-source adaptation, Multiple latent spaces, Co-regression},
abstract = {Learning an effective visual classifier from few labeled samples is a challenging problem, which has motivated the multi-source adaptation scheme in machine learning. While the advantages of multi-source adaptation have been widely recognized, there still exit three major limitations in extant methods. Firstly, how to effectively select the discriminative sources is yet an unresolved issue. Secondly, multiple different visual features on hand cannot be effectively exploited to represent a target object for boosting the adaptation performance. Last but not least, they mainly focus on either visual understanding or feature learning independently, which may lead to the so-called semantic gap between the low-level features and the high-level semantics. To overcome these defects, we propose a novel Multi-source Adaptation Multi-Feature (MAMF) co-regression framework by jointly exploring multi-feature co-regression, multiple latent spaces learning, and discriminative sources selection. Concretely, MAMF conducts the multi-feature representation co-regression with feature learning by simultaneously uncovering multiple optimal latent spaces and taking into account correlations among multiple feature representations. Moreover, to discriminatively leverage multi-source knowledge for each target feature representation, MAMF automatically selects the discriminative source models trained on source datasets by formulating it as a row-sparsity pursuit problem. Different from the state-of-the-arts, our method is able to adapt knowledge from multiple sources even if the features of each source and the target are partially different but overlapping. Experimental results on three challenging visual domain adaptation tasks consistently demonstrate the superiority of our method in comparison with the related state-of-the-arts.}
}
@article{FUJII201994,
title = {Dynamic mode decomposition in vector-valued reproducing kernel Hilbert spaces for extracting dynamical structure among observables},
journal = {Neural Networks},
volume = {117},
pages = {94-103},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301261},
author = {Keisuke Fujii and Yoshinobu Kawahara},
keywords = {Dynamical systems, Dimensionality reduction, Spectral analysis, Unsupervised learning},
abstract = {Understanding nonlinear dynamical systems (NLDSs) is challenging in a variety of engineering and scientific fields. Dynamic mode decomposition (DMD), which is a numerical algorithm for the spectral analysis of Koopman operators, has been attracting attention as a way of obtaining global modal descriptions of NLDSs without requiring explicit prior knowledge. However, since existing DMD algorithms are in principle formulated based on the concatenation of scalar observables, it is not directly applicable to data with dependent structures among observables, which take, for example, the form of a sequence of graphs. In this paper, we formulate Koopman spectral analysis for NLDSs with structures among observables and propose an estimation algorithm for this problem. This method can extract and visualize the underlying low-dimensional global dynamics of NLDSs with structures among observables from data, which can be useful in understanding the underlying dynamics of such NLDSs. To this end, we first formulate the problem of estimating spectra of the Koopman operator defined in vector-valued reproducing kernel Hilbert spaces, and then develop an estimation procedure for this problem by reformulating tensor-based DMD. As a special case of our method, we propose the method named as Graph DMD, which is a numerical algorithm for Koopman spectral analysis of graph dynamical systems, using a sequence of adjacency matrices. We investigate the empirical performance of our method by using synthetic and real-world data.}
}
@article{CHEN2019289,
title = {Delay-dependent criterion for asymptotic stability of a class of fractional-order memristive neural networks with time-varying delays},
journal = {Neural Networks},
volume = {118},
pages = {289-299},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301947},
author = {Liping Chen and Tingwen Huang and J.A. {Tenreiro Machado} and António M. Lopes and Yi Chai and Ranchao Wu},
keywords = {Fractional-order systems, Memristor-based neural networks, Stability, Stabilization, Time-varying delays},
abstract = {The Lyapunov–Krasovskii functional approach is an important and effective delay-dependent stability analysis method for integer order system. However, it cannot be applied directly to fractional-order (FO) systems. To obtain delay-dependent stability and stabilization conditions of FO delayed systems remains a challenging task. This paper addresses the delay-dependent stability and the stabilization of a class of FO memristive neural networks with time-varying delay. By employing the FO Razumikhin theorem and linear matrix inequalities (LMI), a delay-dependent asymptotic stability condition in the form of LMI is established and used to design a stabilizing state-feedback controller. The results address both the effects of the delay and the FO. In addition, the upper bound of the absolute value of the memristive synaptic weights used in previous studies are released, leading to less conservative conditions. Three numerical simulations illustrate the theoretical results and show their effectiveness.}
}
@article{WEI201935,
title = {Stability of stochastic impulsive reaction–diffusion neural networks with S-type distributed delays and its application to image encryption},
journal = {Neural Networks},
volume = {116},
pages = {35-45},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.03.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300978},
author = {Tengda Wei and Ping Lin and Yangfan Wang and Linshan Wang},
keywords = {Stochastic reaction–diffusion neural network, Impulse, S-type distributed delay, Image encryption},
abstract = {In this paper, we study stochastic impulsive reaction–diffusion neural networks with S-type distributed delays, aiming to obtain the sufficient conditions for global exponential stability. First, an impulsive inequality involving infinite delay is introduced and the asymptotic behaviour of its solution is investigated by the truncation method. Then, global exponential stability in the mean-square sense of the stochastic impulsive reaction–diffusion system is studied by constructing a simple Lyapunov–Krasovskii functional where the S-type distributed delay is handled by the impulsive inequality. Numerical examples are also given to verify the effectiveness of the proposed results. Finally, the obtained theoretical results are successfully applied to an image encryption scheme based on bit-level permutation and the stochastic neural networks.}
}
@article{AGUILERA2019136,
title = {Integrated information in the thermodynamic limit},
journal = {Neural Networks},
volume = {114},
pages = {136-146},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300735},
author = {Miguel Aguilera and Ezequiel {A. Di Paolo}},
keywords = {Integrated information, Phi, Ising model, Thermodynamic limit, Mean-field, Criticality},
abstract = {The capacity to integrate information is a prominent feature of biological, neural, and cognitive processes. Integrated Information Theory (IIT) provides mathematical tools for quantifying the level of integration in a system, but its computational cost generally precludes applications beyond relatively small models. In consequence, it is not yet well understood how integration scales up with the size of a system or with different temporal scales of activity, nor how a system maintains integration as it interacts with its environment. After revising some assumptions of the theory, we show for the first time how modified measures of information integration scale when a neural network becomes very large. Using kinetic Ising models and mean-field approximations, we show that information integration diverges in the thermodynamic limit at certain critical points. Moreover, by comparing different divergent tendencies of blocks that make up a system at these critical points, we can use information integration to delimit the boundary between an integrated unit and its environment. Finally, we present a model that adaptively maintains its integration despite changes in its environment by generating a critical surface where its integrity is preserved. We argue that the exploration of integrated information for these limit cases helps in addressing a variety of poorly understood questions about the organization of biological, neural, and cognitive systems.}
}
@article{FITZSIMMONS201981,
title = {Combining Hopfield neural networks, with applications to grid-based mathematics puzzles},
journal = {Neural Networks},
volume = {118},
pages = {81-89},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301789},
author = {M. Fitzsimmons and H. Kunze},
keywords = {Hopfield neural networks, Constrained set-selection problems, Mathematical analysis, Grid-based number puzzles},
abstract = {Hopfield neural networks are useful for solving certain constrained set-selection problems. We establish that the vector fields associated with general networks of this type can be combined to produce a new network that solves the corresponding combination of set-selection/constraint problems, provided a relatively simple condition is satisfied. That is, we establish that just this one condition needs to be verified in order to be able to combine such networks. We introduce some generalizations of networks that exist in the literature, and, to demonstrate the usefulness of the work, we combine these networks to solve two well-known grid-based math puzzles (i.e. constraint problems): Kakuro and Akari (called Cross Sums and Light Up in North America). We present examples to illustrate the evolution of the solution process. We find that the difficulty rating of a Kakuro puzzle is strongly connected to the number of iterations used by the neural network solver.}
}
@article{FAN2019216,
title = {Switching event-triggered control for global stabilization of delayed memristive neural networks: An exponential attenuation scheme},
journal = {Neural Networks},
volume = {117},
pages = {216-224},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301546},
author = {Yingjie Fan and Xia Huang and Hao Shen and Jinde Cao},
keywords = {Global stabilization, Delayed memristive neural networks, Switching event-triggered control, Exponential attenuation, Interval matrix method},
abstract = {In this paper, an exponential-attenuation-based switching event-trigger (EABSET) scheme is designed to achieve the global stabilization of delayed memristive neural networks (MNNs). The issue is proposed for two reasons: (1) the available methods may be complicated in dealing with the state-dependent memristive connection weights; (2) the existing event-trigger mechanisms may be conservative in decreasing the amount of triggering times. To overcome these difficulties, the stabilization problem is formulated within a framework of networked control first. Then, an exponential attenuation term is introduced into the prescribed threshold function. It can enlarge the time span between two neighboring triggered events and further reduce the frequency of data packets sending out. By utilizing the input delay approach, time-dependent and piecewise Lyapunov functionals, and matrix norm inequalities, some sufficient criteria are obtained to guarantee the global stabilization of delayed MNNs and to design both the controller and the trigger parameters. Finally, some comparison simulation results demonstrate that the novel event-trigger scheme has some advantages over some existing ones.}
}
@article{MYSIN2019119,
title = {Phase relations of theta oscillations in a computer model of the hippocampal CA1 field: Key role of Schaffer collaterals},
journal = {Neural Networks},
volume = {116},
pages = {119-138},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301029},
author = {Ivan E. Mysin and Valentina F. Kitchigina and Yakov B. Kazanovich},
keywords = {Theta rhythm, Medial septum, Schaffer collaterals, CA1 field, PV+ basket neurons, OLM neurons},
abstract = {The hippocampal theta rhythm (4–12 Hz) is one of the most important electrophysiological processes in the hippocampus, it participates in cognitive hippocampal functions, such as navigation in space, novelty detection, and declarative memory. We use neural network modeling to study the mechanism of theta rhythm emergence in the CA1 microcircuitry. Our model of the CA1 field includes biophysical representation of major cell types related to the theta rhythm emergence: excitatory pyramidal cells and two types of inhibitory interneurons, PV+ basket cells and oriens lacunosum–moleculare (OLM) cells. The main inputs to the CA1 cells come from the entorhinal cortex via perforant pathway, the CA3 field via Schaffer collaterals, and the medial septum via fimbria–fornix. By computer simulations we investigated the influence of each input, intrinsic parameters of neurons, and connections between neurons on phase coupling between the theta rhythm and the firing of pyramidal, PV+ basket and OLM cells in the CA1. We found that the input from the CA3 field via Schaffercollaterals plays a major role in the formation of phase relations that have been observed in experiments in vivo. The direct input from the medial septum participates in the formation of proper phase relations, but it is not crucial for the production of the theta rhythm in CA1 neural populations.}
}
@article{KAWAGUCHI2019167,
title = {Depth with nonlinearity creates no bad local minima in ResNets},
journal = {Neural Networks},
volume = {118},
pages = {167-174},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301820},
author = {Kenji Kawaguchi and Yoshua Bengio},
keywords = {Deep learning, Residual neural network, Non-convex optimization, Local minima},
abstract = {In this paper, we prove that depth with nonlinearity creates no bad local minima in a type of arbitrarily deep ResNets with arbitrary nonlinear activation functions, in the sense that the values of all local minima are no worse than the global minimum value of corresponding classical machine-learning models, and are guaranteed to further improve via residual representations. As a result, this paper provides an affirmative answer to an open question stated in a paper in the conference on Neural Information Processing Systems 2018. This paper advances the optimization theory of deep learning only for ResNets and not for other network architectures.}
}
@article{BOUWMANS20198,
title = {Deep neural network concepts for background subtraction:A systematic review and comparative evaluation},
journal = {Neural Networks},
volume = {117},
pages = {8-66},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301303},
author = {Thierry Bouwmans and Sajid Javed and Maryam Sultana and Soon Ki Jung},
keywords = {Background subtraction, Restricted Boltzmann machines, Auto-encoders networks, Convolutional neural networks, Generative adversarial networks},
abstract = {Conventional neural networks have been demonstrated to be a powerful framework for background subtraction in video acquired by static cameras. Indeed, the well-known Self-Organizing Background Subtraction (SOBS) method and its variants based on neural networks have long been the leading methods on the large-scale CDnet 2012 dataset during a long time. Convolutional neural networks, which are used in deep learning, have been recently and excessively employed for background initialization, foreground detection, and deep learned features. The top background subtraction methods currently used in CDnet 2014 are based on deep neural networks, and have demonstrated a large performance improvement in comparison to conventional unsupervised approaches based on multi-feature or multi-cue strategies. Furthermore, since the seminal work of Braham and Van Droogenbroeck in 2016, a large number of studies on convolutional neural networks applied to background subtraction have been published, and a continual gain of performance has been achieved. In this context, we provide the first review of deep neural network concepts in background subtraction for novices and experts in order to analyze this success and to provide further directions. To do so, we first surveyed the background initialization and background subtraction methods based on deep neural networks concepts, and also deep learned features. We then discuss the adequacy of deep neural networks for the task of background subtraction. Finally, experimental results are presented for the CDnet 2014 dataset.}
}
@article{SIMA2019208,
title = {Subrecursive neural networks},
journal = {Neural Networks},
volume = {116},
pages = {208-223},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930125X},
author = {Jiří Šíma},
keywords = {Recurrent neural network, Chomsky hierarchy, Cut language, Quasi-periodic number},
abstract = {It has been known for discrete-time recurrent neural networks (NNs) that binary-state models using the Heaviside activation function (with Boolean outputs 0 or 1) are equivalent to finite automata (level 3 in the Chomsky hierarchy), while analog-state NNs with rational weights, employing the saturated-linear function (with real-number outputs in the interval [0,1]), are Turing complete (Chomsky level 0) even for three analog units. However, it is as yet unknown whether there exist subrecursive (i.e. sub-Turing) NN models which occur on Chomsky levels 1 or 2. In this paper, we provide such a model which is a binary-state NN extended with one extra analog unit (1ANN). We achieve a syntactic characterization of languages that are accepted online by 1ANNs in terms of so-called cut languages which are combined in a certain way by usual operations. We employ this characterization for proving that languages accepted by 1ANNs with rational weights are context-sensitive (Chomsky level 1) and we present explicit examples of such languages that are not context-free (i.e. are above Chomsky level 2). In addition, we formulate a sufficient condition when a 1ANN recognizes a regular language (Chomsky level 3) in terms of quasi-periodicity of parameters derived from its real weights, which is satisfied e.g. for rational weights provided that the inverse of the real self-loop weight of the analog unit is a Pisot number.}
}
@article{SINGH2019192,
title = {Shunt connection: An intelligent skipping of contiguous blocks for optimizing MobileNet-V2},
journal = {Neural Networks},
volume = {118},
pages = {192-203},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301790},
author = {Brijraj Singh and Durga Toshniwal and Sharan Kumar Allur},
keywords = {MobileNet, Compressed network, Shunt connection, Residual connections, Encoder, Model optimization},
abstract = {Enabling deep neural networks for tight resource constraint environments like mobile phones and cameras is the current need. The existing availability in the form of optimized architectures like Squeeze Net, MobileNet etc., are devised to serve the purpose by utilizing the parameter friendly operations and architectures, such as point-wise convolution, bottleneck layer etc. This work focuses on optimizing the number of floating point operations involved in inference through an already compressed deep learning architecture. The optimization is performed by utilizing the advantage of residual connections in a macroscopic way. This paper proposes novel connection on top of the deep learning architecture whose idea is to locate the blocks of a pretrained network which have relatively lesser knowledge quotient and then bypassing those blocks by an intelligent skip connection, named here as Shunt connection. The proposed method helps in replacing the high computational blocks by computation friendly shunt connection. In a given architecture, up to two vulnerable locations are selected where 6 contiguous blocks are selected and skipped at the first location and 2 contiguous blocks are selected and skipped at the second location, leveraging 2 shunt connections. The proposed connection is used over state-of-the-art MobileNet-V2 architecture and manifests two cases, which lead from 33.5% reduction in flops (one connection) up to 43.6% reduction in flops (two connections) with minimal impact on accuracy.}
}
@article{AYINDE2019148,
title = {Redundant feature pruning for accelerated inference in deep neural networks},
journal = {Neural Networks},
volume = {118},
pages = {148-158},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.021},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301273},
author = {Babajide O. Ayinde and Tamer Inanc and Jacek M. Zurada},
keywords = {Deep learning, Feature correlation, Filter pruning, Cosine similarity, Redundancy reduction, Deep neural networks},
abstract = {This paper presents an efficient technique to reduce the inference cost of deep and/or wide convolutional neural network models by pruning redundant features (or filters). Previous studies have shown that over-sized deep neural network models tend to produce a lot of redundant features that are either shifted version of one another or are very similar and show little or no variations, thus resulting in filtering redundancy. We propose to prune these redundant features along with their related feature maps according to their relative cosine distances in the feature space, thus leading to smaller networks with reduced post-training inference computational costs and competitive performance. We empirically show on select models (VGG-16, ResNet-56, ResNet-110, and ResNet-34) and dataset (MNIST Handwritten digits, CIFAR-10, and ImageNet) that inference costs (in FLOPS) can be significantly reduced while overall performance is still competitive with the state-of-the-art.}
}
@article{LEE201988,
title = {DynMat, a network that can learn after learning},
journal = {Neural Networks},
volume = {116},
pages = {88-100},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301030},
author = {Jung Hoon Lee},
keywords = {Complementary learning system, Continuous learning, Synapse-based memory, Neural networks, Convolutional networks},
abstract = {To survive in the dynamically-evolving world, we accumulate knowledge and improve our skills based on experience. In the process, gaining new knowledge does not disrupt our vigilance to external stimuli. In other words, our learning process is ‘accumulative’ and ‘online’ without interruption. However, despite the recent success, artificial neural networks (ANNs) must be trained offline and suffer catastrophic interference between old and new learning, indicating that ANNs’ conventional learning algorithms may not be suitable for building intelligent agents comparable to our brain. In this study, we propose a novel neural network architecture (DynMat) consisting of dual learning systems inspired by the complementary learning system (CLS) theory suggesting that the brain relies on short- and long-term learning systems to learn continuously. Our empirical evaluations show that (1) DynMat can learn a new class without catastrophic interference and (2) it does not strictly require offline training.}
}
@article{ZHANG201950,
title = {Fully complex conjugate gradient-based neural networks using Wirtinger calculus framework: Deterministic convergence and its application},
journal = {Neural Networks},
volume = {115},
pages = {50-64},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300747},
author = {Bingjie Zhang and Yusong Liu and Jinde Cao and Shujun Wu and Jian Wang},
keywords = {Complex-valued neural network, Conjugate gradient, Armijo search, Wirtinger, Convergence},
abstract = {Conjugate gradient method has been verified to be one effective strategy for training neural networks due to its low memory requirements and fast convergence. In this paper, we propose an efficient conjugate gradient method to train fully complex-valued network models in terms of Wirtinger differential operator. Two ways are adopted to enhance the training performance. One is to construct a sufficient descent direction during training by designing a fine tuning conjugate coefficient. Another technique is to pursue the optimal learning rate instead of a fixed constant in each iteration which is determined by employing a generalized Armijo search. In addition, we rigorously prove its weak and strong convergence results, i.e., the gradient norms of objective function with respect to weights approach zero along with the increasing iterations and the weight sequence tends to the optimal point. To verify the effectiveness and rationality of the proposed method, four illustrated simulations have been performed on both typical regression and classification problems.}
}
@article{WU2019191,
title = {An approximation algorithm for graph partitioning via deterministic annealing neural network},
journal = {Neural Networks},
volume = {117},
pages = {191-200},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930142X},
author = {Zhengtian Wu and Hamid Reza Karimi and Chuangyin Dang},
keywords = {Graph partitioning, Neural network, Combinatorial optimization, NP-hard problem, Deterministic annealing neural network algorithm},
abstract = {Graph partitioning, a classical NP-hard combinatorial optimization problem, is widely applied to industrial or management problems. In this study, an approximated solution of the graph partitioning problem is obtained by using a deterministic annealing neural network algorithm. The algorithm is a continuation method that attempts to obtain a high-quality solution by following a path of minimum points of a barrier problem as the barrier parameter is reduced from a sufficiently large positive number to 0. With the barrier parameter assumed to be any positive number, one minimum solution of the barrier problem can be found by the algorithm in a feasible descent direction. With a globally convergent iterative procedure, the feasible descent direction could be obtained by renewing Lagrange multipliers red. A distinctive feature of it is that the upper and lower bounds on the variables will be automatically satisfied on the condition that the step length is a value from 0 to 1. Four well-known algorithms are compared with the proposed one on 100 test samples. Simulation results show effectiveness of the proposed algorithm.}
}
@article{PAHNEHKOLAEI2019307,
title = {Delay-dependent stability analysis of the QUAD vector field fractional order quaternion-valued memristive uncertain neutral type leaky integrator echo state neural networks},
journal = {Neural Networks},
volume = {117},
pages = {307-327},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301558},
author = {Seyed Mehdi Abedi Pahnehkolaei and Alireza Alfi and J.A. Tenreiro Machado},
keywords = {Fractional-order systems, Memristor-based neural networks, Quaternion-valued neural networks, Neutral type neural networks, Linear matrix inequality, Delay-dependent stability},
abstract = {This paper studies the robust stability analysis for a class of memristive-based neural networks (NN). The NN consists of a fractional order neutral type quaternion-valued leaky integrator echo state with parameter uncertainties and time-varying delays. First, the quaternion-valued leaky integrator echo state NN with QUAD vector field activation function is transformed into a real-valued system using a linear mapping function. Then, the Lyapunov–Krasovskii functional is adopted to derive the sufficient conditions on the existence and uniqueness of Filippov solution of the NN equilibrium point. The delay-dependent robust stability analysis of such NN is provided with the help of linear matrix inequality technique. Finally, the theoretical results are validated by means of a numerical example.}
}
@article{NGUYEN2019208,
title = {A multimodal convolutional neuro-fuzzy network for emotion understanding of movie clips},
journal = {Neural Networks},
volume = {118},
pages = {208-219},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301832},
author = {Tuan-Linh Nguyen and Swathi Kavuri and Minho Lee},
keywords = {Fuzzy logic, Deep learning (DL), Convolutional Neural Network (CNN), Convolutional Neuro-Fuzzy Network (CNFN), Multimodal emotion understanding, Interpretable AI},
abstract = {Multimodal emotion understanding enables AI systems to interpret human emotions. With accelerated video surge, emotion understanding remains challenging due to inherent data ambiguity and diversity of video content. Although deep learning has made a considerable progress in big data feature learning, they are viewed as deterministic models used in a “black-box” manner which does not have capabilities to represent inherent ambiguities with data. Since the possibility theory of fuzzy logic focuses on knowledge representation and reasoning under uncertainty, we intend to incorporate the concepts of fuzzy logic into deep learning framework. This paper presents a novel convolutional neuro-fuzzy network, which is an integration of convolutional neural networks in fuzzy logic domain to extract high-level emotion features from text, audio, and visual modalities. The feature sets extracted by fuzzy convolutional layers are compared with those of convolutional neural networks at the same level using t-distributed Stochastic Neighbor Embedding. This paper demonstrates a multimodal emotion understanding framework with an adaptive neural fuzzy inference system that can generate new rules to classify emotions. For emotion understanding of movie clips, we concatenate audio, visual, and text features extracted using the proposed convolutional neuro-fuzzy network to train adaptive neural fuzzy inference system. In this paper, we go one step further to explain how deep learning arrives at a conclusion that can guide us to an interpretable AI. To identify which visual/text/audio aspects are important for emotion understanding, we use direct linear non-Gaussian additive model to explain the relevance in terms of causal relationships between features of deep hidden layers. The critical features extracted are input to the proposed multimodal framework to achieve higher accuracy.}
}
@article{SARIGUL2019279,
title = {Differential convolutional neural network},
journal = {Neural Networks},
volume = {116},
pages = {279-287},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.025},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301315},
author = {M. Sarıgül and B.M. Ozyildirim and M. Avci},
keywords = {Convolutional neural networks, Deep learning, Image classification, Convolution techniques, Pattern recognition, Machine learning},
abstract = {Convolutional neural networks with strong representation ability of deep structures have ever increasing popularity in many research areas. The main difference of Convolutional Neural Networks with respect to existing similar artificial neural networks is the inclusion of the convolutional part. This inclusion directly increases the performance of artificial neural networks. This fact has led to the development of many different convolutional models and techniques. In this work, a novel convolution technique named as Differential Convolution and updated error back-propagation algorithm is proposed. The proposed technique aims to transfer feature maps containing directional activation differences to the next layer. This implementation takes the idea of how convolved features change on the feature map into consideration. In a sense, this process adapts the mathematical differentiation operation into the convolutional process. Proposed improved back propagation algorithm also considers neighborhood activation errors. This property increases the classification performance without changing the number of filters. Four different experiment sets were performed to observe the performance and the adaptability of the differential convolution technique. In the first experiment set utilization of the differential convolution on a traditional convolutional neural network structure made a performance boost up to 55.29% for the test accuracy. In the second experiment set differential convolution adaptation raised the top1 and top5 test accuracies of AlexNet by 5.3% and 4.75% on ImageNet dataset. In the third experiment set differential convolution utilized model outperformed all compared convolutional structures. In the fourth experiment set, the Differential VGGNet model obtained by adapting proposed differential convolution technique performed 93.58% and 75.06% accuracy values for CIFAR10 and CIFAR100 datasets, respectively. The accuracy values of the Differential NIN model containing differential convolution operation were 92.44% and 72.65% for the same datasets. In these experiment sets, it was observed that the differential convolution technique outperformed both traditional convolution and other compared convolution techniques. In addition, easy adaptation of the proposed technique to different convolutional structures and its efficiency demonstrate that popular deep learning models may be improved with differential convolution.}
}
@article{GU2019352,
title = {Efficient inexact proximal gradient algorithms for structured sparsity-inducing norm},
journal = {Neural Networks},
volume = {118},
pages = {352-362},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301881},
author = {Bin Gu and Xiang Geng and Xiang Li and Guansheng Zheng},
keywords = {Structured-sparsity regularization,  norm, Inexact proximal operator, overlapping groups},
abstract = {Structured-sparsity regularization is popular for sparse learning because of its flexibility of encoding the feature structures. This paper considers a generalized version of structured-sparsity regularization (especially for l1∕l∞ norm) with arbitrary group overlap. Due to the group overlap, it is time-consuming to solve the associated proximal operator. Although Mairal et al. have proposed a network-flow algorithm to solve the proximal operator, it is still time-consuming, especially in the high-dimensional setting. To address this challenge, in this paper, we have developed a more efficient solution for l1∕l∞ group lasso with arbitrary group overlap using inexact proximal gradient method. In each iteration, our algorithm only requires to calculate an inexact solution to the proximal sub-problem, which can be done efficiently. On the theoretic side, the proposed algorithm enjoys the same global convergence rate as the exact proximal methods. Experiments demonstrate that our algorithm is much more efficient than the network-flow algorithm while retaining similar generalization performance.}
}
@article{MOTONAKA2019159,
title = {Connecting PM and MAP in Bayesian spectral deconvolution by extending exchange Monte Carlo method and using multiple data sets},
journal = {Neural Networks},
volume = {118},
pages = {159-166},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301364},
author = {Kimiko Motonaka and Seiji Miyoshi},
keywords = {Spectral deconvolution, MCMC, Exchange Monte Carlo, Bayesian inference},
abstract = {Nagata et al. proposed a parameter estimation method using Markov chain Monte Carlo (MCMC) for the spectral deconvolution of observed data. However, a systematic error occurs when the parameters to be estimated are close. In this paper, we first clarify that the exchange symmetry of parameters, which is essentially included in the spectral deconvolution problem, causes the systematic error. In particular, we show that estimation from a single data set is inherently difficult because the posterior distribution becomes unimodal or multimodal depending on the data set when the parameters to be estimated are close. Second, we alleviate the problem to the case of using multiple data sets and propose an extension of the exchange Monte Carlo method to low temperatures. This extension corresponds to bridging the gap between posterior mean (PM) estimation and maximum a posteriori (MAP) estimation. The above alleviation and bridging achieve a good estimation even when the parameters are close.}
}
@article{LI2019285,
title = {Synchronization of impulsive coupled complex-valued neural networks with delay: The matrix measure method},
journal = {Neural Networks},
volume = {117},
pages = {285-294},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.024},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301649},
author = {Lulu Li and Xiaohong Shi and Jinling Liang},
keywords = {Complex-valued neural networks, Synchronization, Matrix measure, Impulsive control},
abstract = {In this paper, the exponential synchronization of the impulsive coupled delayed complex-valued neural networks (CVNNs) is studied. Without constructing the Lyapunov function, a novel approach based on the matrix measure and extended Halanay inequality is presented and some sufficient criteria for exponential synchronization of the addressed CVNNs are derived. In this paper, the restriction of the real and imaginary parts of activation functions which are supposed to depend only on the real and imaginary parts of the variables, respectively, is removed. Furthermore, by employing the average impulsive interval method, the requirement on the upper bound of the impulsive intervals is removed for impulsive signal transmission. Finally, numerical examples are provided to demonstrate the effectiveness of the theoretical results obtained, even for large-scale CVNNs with impulsive coupling.}
}
@article{MA2019225,
title = {Time series classification with Echo Memory Networks},
journal = {Neural Networks},
volume = {117},
pages = {225-239},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301406},
author = {Qianli Ma and Wanqing Zhuang and Lifeng Shen and Garrison W. Cottrell},
keywords = {Echo state networks, Multi-scale convolution, Time series classification},
abstract = {Echo state networks (ESNs) are randomly connected recurrent neural networks (RNNs) that can be used as a temporal kernel for modeling time series data, and have been successfully applied on time series prediction tasks. Recently, ESNs have been applied to time series classification (TSC) tasks. However, previous ESN-based classifiers involve either training the model by predicting the next item of a sequence, or predicting the class label at each time step. The former is essentially a predictive model adapted from time series prediction work, rather than a model designed specifically for the classification task. The latter approach only considers local patterns at each time step and then averages over the classifications. Hence, rather than selecting the most discriminating sections of the time series, this approach will incorporate non-discriminative information into the classification, reducing accuracy. In this paper, we propose a novel end-to-end framework called the Echo Memory Network (EMN) in which the time series dynamics and multi-scale discriminative features are efficiently learned from an unrolled echo memory using multi-scale convolution and max-over-time pooling. First, the time series data are projected into the high dimensional nonlinear space of the reservoir and the echo states are collected into the echo memory matrix, followed by a single multi-scale convolutional layer to extract multi-scale features from the echo memory matrix. Max-over-time pooling is used to maintain temporal invariance and select the most important local patterns. Finally, a fully-connected hidden layer feeds into a softmax layer for classification. This architecture is applied to both time series classification and human action recognition datasets. For the human action recognition datasets, we divide the action data into five different components of the human body, and propose two spatial information fusion strategies to integrate the spatial information over them. With one training-free recurrent layer and only one layer of convolution, the EMN is a very efficient end-to-end model, and ranks first in overall classification ability on 55 TSC benchmark datasets and four 3D skeleton-based human action recognition tasks.}
}
@article{ITO201925,
title = {Semi-supervised deep learning of brain tissue segmentation},
journal = {Neural Networks},
volume = {116},
pages = {25-34},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300954},
author = {Ryo Ito and Ken Nakae and Junichi Hata and Hideyuki Okano and Shin Ishii},
keywords = {Brain tissue segmentation, Semi-supervised learning, Image registration, Deep neural network},
abstract = {Brain image segmentation is of great importance not only for clinical use but also for neuroscience research. Recent developments in deep neural networks (DNNs) have led to the application of DNNs to brain image segmentation, which required extensive human annotations of whole brain images. Annotating three-dimensional brain images requires laborious efforts by expert anatomists because of the differences among images in terms of their dimensionality, noise, contrast, or ambiguous boundaries that even prevent these experts from necessarily attaining consistency. This paper proposes a semi-supervised learning framework to train a DNN based on a relatively small number of annotated (labeled) images, named atlases, but also a relatively large number of unlabeled images by leveraging image registration to attach pseudo-labels to images that were originally unlabeled. We applied our proposed method to two different datasets: open human brain images and our original marmoset brain images. When provided with the same number of atlases for training, we found our method achieved superior and more stable segmentation results than those by existing registration-based and DNN-based methods.}
}
@article{YANG2019247,
title = {Investigating the transferring capability of capsule networks for text classification},
journal = {Neural Networks},
volume = {118},
pages = {247-261},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.06.014},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930187X},
author = {Min Yang and Wei Zhao and Lei Chen and Qiang Qu and Zhou Zhao and Ying Shen},
keywords = {Capsule network, Dynamic routing, Domain adaptation, Multi-label text classification, Cross-domain sentiment classification},
abstract = {Text classification has been attracting increasing attention with the growth of textual data created on the Internet. Great progress has been made by deep neural networks for domains where a large amount of labeled training data is available. However, providing sufficient data is time-consuming and labor-intensive, establishing substantial obstacles for expanding the learned models to new domains or new tasks. In this paper, we investigate the transferring capability of capsule networks for text classification. Capsule networks are able to capture the intrinsic spatial part-whole relationship constituting domain invariant knowledge that bridges the knowledge gap between the source and target domains (or tasks). We propose an iterative adaptation strategy for cross-domain text classification, which adapts the source domain to the target domain. A fast training method with capsule compression and class-guided routing is designed to make the capsule network more efficient in computation for cross-domain text classification. We first conduct experiments to evaluate the performance of the capsule network on six benchmark datasets for generic text classification. The capsule networks outperform the compared models on 4 out of 6 datasets, suggesting the effectiveness of the capsule networks for text classification. More importantly, we demonstrate the transferring capability of the proposed cross-domain capsule network (TL-Capsule) by applying it to two transfer learning applications: single-label to multi-label text classification and cross-domain sentiment classification. The experimental results show that capsule networks consistently and substantially outperform the compared methods for both tasks. To the best of our knowledge, this is the first work that empirically investigates the transferring capability of capsule networks for text modeling.}
}
@article{INDEN2019224,
title = {Evolving neural networks to follow trajectories of arbitrary complexity},
journal = {Neural Networks},
volume = {116},
pages = {224-236},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930111X},
author = {Benjamin Inden and Jürgen Jost},
keywords = {Neuroevolution, Complexity, Incremental evolution, Open-ended evolution, Evolutionary robotics, Trajectory learning},
abstract = {Many experiments have been performed that use evolutionary algorithms for learning the topology and connection weights of a neural network that controls a robot or virtual agent. These experiments are not only performed to better understand basic biological principles, but also with the hope that with further progress of the methods, they will become competitive for automatically creating robot behaviors of interest. However, current methods are limited with respect to the (Kolmogorov) complexity of evolved behavior. Using the evolution of robot trajectories as an example, we show that by adding four features, namely (1) freezing of previously evolved structure, (2) temporal scaffolding, (3) a homogeneous transfer function for output nodes, and (4) mutations that create new pathways to outputs, to standard methods for the evolution of neural networks, we can achieve an approximately linear growth of the complexity of behavior over thousands of generations. Overall, evolved complexity is up to two orders of magnitude over that achieved by standard methods in the experiments reported here, with the major limiting factor for further growth being the available run time. Thus, the set of methods proposed here promises to be a useful addition to various current neuroevolution methods.}
}
@article{ZHAO201967,
title = {Learning joint space–time–frequency features for EEG decoding on small labeled data},
journal = {Neural Networks},
volume = {114},
pages = {67-77},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300711},
author = {Dongye Zhao and Fengzhen Tang and Bailu Si and Xisheng Feng},
keywords = {Brain–computer interfaces, Convolutional neural network, Joint space–time–frequency feature learning, Subject-to-subject weight transfer, Small labeled data},
abstract = {Brain–computer interfaces (BCIs), which control external equipment using cerebral activity, have received considerable attention recently. Translating brain activities measured by electroencephalography (EEG) into correct control commands is a critical problem in this field. Most existing EEG decoding methods separate feature extraction from classification and thus are not robust across different BCI users. In this paper, we propose to learn subject-specific features jointly with the classification rule. We develop a deep convolutional network (ConvNet) to decode EEG signals end-to-end by stacking time–frequency transformation, spatial filtering, and classification together. Our proposed ConvNet implements a joint space–time–frequency feature extraction scheme for EEG decoding. Morlet wavelet-like kernels used in our network significantly reduce the number of parameters compared with classical convolutional kernels and endow the features learned at the corresponding layer with a clear interpretation, i.e. spectral amplitude. We further utilize subject-to-subject weight transfer, which uses parameters of the networks trained for existing subjects to initialize the network for a new subject, to solve the dilemma between a large number of demanded data for training deep ConvNets and small labeled data collected in BCIs. The proposed approach is evaluated on three public data sets, obtaining superior classification performance compared with the state-of-the-art methods.}
}
@article{WANG201917,
title = {Cascade interpolation learning with double subspaces and confidence disturbance for imbalanced problems},
journal = {Neural Networks},
volume = {118},
pages = {17-31},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301765},
author = {Zhe Wang and Chenjie Cao},
keywords = {Random subspaces, Confidence disturbance, Cascade interpolation, Ensemble learning, Imbalanced problems},
abstract = {In this paper, a new ensemble framework named Cascade Interpolation Learning with Double subspaces and Confidence disturbance (CILDC) is designed for the imbalanced classification problems. Developed from the Cascade Forest of the Deep Forest which is the stacking based tree ensembles for big data issues with less hyper-parameters, CILDC aims to generalize the cascade model for more base classifiers. Specifically, CILDC integrates base classifiers through the double subspaces strategy and the random under-sampling preprocessing. Further, one simple but effective confidence disturbance technique is introduced to CILDC to tune the threshold deviation for imbalanced samples. In detail, the disturbance coefficients are multiplied to various confidence vectors before interpolating in each level of CILDC, and the ideal threshold can be adaptively learned through the cascade structure. Furthermore, both the Random Forest and the Naive Bayes are suitable to be the base classifier for CILDC. Subsequently, comprehensive comparison experiments on typical imbalanced datasets demonstrate both the effectiveness and generalization of CILDC.}
}
@article{TANG2019246,
title = {Bayesian rank penalization},
journal = {Neural Networks},
volume = {116},
pages = {246-256},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301248},
author = {Kewei Tang and Zhixun Su and Jie Zhang and Lihong Cui and Wei Jiang and Xiaonan Luo and Xiyan Sun},
keywords = {Bayesian model, Generalized double Pareto, LRR, Low-rank, RPCA},
abstract = {Rank minimization is a key component of many computer vision and machine learning methods, including robust principal component analysis (RPCA) and low-rank representations (LRR). However, usual methods rely on optimization to produce a point estimate without characterizing uncertainty in this estimate, and also face difficulties in tuning parameter choice. Both of these limitations are potentially overcome with Bayesian methods, but there is currently a lack of general purpose Bayesian approaches for rank penalization. We address this gap using a positive generalized double Pareto prior, illustrating the approach in RPCA and LRR. Posterior computation relies on hybrid Gibbs sampling and geodesic Monte Carlo algorithms. We assess performance in simulation examples, and benchmark data sets.}
}
@article{BOSSENS201930,
title = {Learning to learn with active adaptive perception},
journal = {Neural Networks},
volume = {115},
pages = {30-49},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300796},
author = {D.M. Bossens and N.C. Townsend and A.J. Sobey},
keywords = {Adaptive perception, Inductive bias, Self-modifying policies, Reinforcement learning, Partial observability},
abstract = {Increasingly, autonomous agents will be required to operate on long-term missions. This will create a demand for general intelligence because feedback from a human operator may be sparse and delayed, and because not all behaviours can be prescribed. Deep neural networks and reinforcement learning methods can be applied in such environments but their fixed updating routines imply an inductive bias in learning spatio-temporal patterns, meaning some environments will be unsolvable. To address this problem, this paper proposes active adaptive perception, the ability of an architecture to learn when and how to modify and selectively utilise its perception module. To achieve this, a generic architecture based on a self-modifying policy (SMP) is proposed, and implemented using Incremental Self-improvement with the Success Story Algorithm. The architecture contrasts to deep reinforcement learning systems which follow fixed training strategies and earlier SMP studies which for perception relied either entirely on the working memory or on untrainable active perception instructions. One computationally cheap and one more expensive implementation are presented and compared to DRQN, an off-policy deep reinforcement learner using experience replay and Incremental Self-improvement, an SMP, on various non-episodic partially observable mazes. The results show that the simple instruction set leads to emergent strategies to avoid detracting corridors and rooms, and that the expensive implementation allows selectively ignoring perception where it is inaccurate.}
}
@article{ZHU2019127,
title = {Branched convolutional neural networks incorporated with Jacobian deep regression for facial landmark detection},
journal = {Neural Networks},
volume = {118},
pages = {127-139},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301005},
author = {Meilu Zhu and Daming Shi and Junbin Gao},
keywords = {Convolutional neural networks, Jacobian matrix, Cascaded regression, Facial landmark detection},
abstract = {Facial landmark detection is to localize multiple facial key-points for a given facial image. While many methods have achieved remarkable performance in recent years, the accuracy remains unsatisfactory due to some uncontrolled conditions such as occlusion, head pose variations and illumination, under which, the L2 loss function is conventionally dominated by errors from those facial components on which the landmarks are hard predicted. In this paper, a novel branched convolutional neural network incorporated with Jacobian deep regression framework, hereafter referred to as BCNN-JDR, is proposed to solve the facial landmark detection problem. Our proposed framework consists of two parts: initialization stage and cascaded refinement stages. We firstly exploit branched convolutional neural networks as the robust initializer to estimate initial shape, which is incorporated with the knowledge of component-aware branches. By virtue of the component-aware branches mechanism, BCNN can effectively alleviate this issue of the imbalance errors among facial components and provide the robust initial face shape. Following the BCNN, a sequence of refinement stages are cascaded to fine-tune the initial shape within a narrow range. In each refinement stage, the local texture information is adopted to fit the facial local nonlinear variation. Moreover, our entire framework is jointly optimized via the Jacobian deep regression optimization strategy in an end-to-end manner. Jacobian deep regression optimization strategy has an ability to backward propagate the training error of the last stage to all previous stages, which implements a global optimization approach to our proposed framework. Experimental results on benchmark datasets demonstrate that the proposed BCNN-JDR is robust against uncontrolled conditions and outperforms the state-of-the-art approaches.}
}
@article{DELAHUNT201954,
title = {Putting a bug in ML: The moth olfactory network learns to read MNIST},
journal = {Neural Networks},
volume = {118},
pages = {54-64},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301522},
author = {Charles B. Delahunt and J. Nathan Kutz},
keywords = {Hebbian, Sparsity, Olfactory network, Bio-mimesis, Neural networks, One-shot learning},
abstract = {We seek to (i) characterize the learning architectures exploited in biological neural networks for training on very few samples, and (ii) port these algorithmic structures to a machine learning context. The moth olfactory network is among the simplest biological neural systems that can learn, and its architecture includes key structural elements and mechanisms widespread in biological neural nets, such as cascaded networks, competitive inhibition, high intrinsic noise, sparsity, reward mechanisms, and Hebbian plasticity. These structural biological elements, in combination, enable rapid learning. MothNet is a computational model of the moth olfactory network, closely aligned with the moth’s known biophysics and with in vivo electrode data collected from moths learning new odors. We assign this model the task of learning to read the MNIST digits. We show that MothNet successfully learns to read given very few training samples (1–10 samples per class). In this few-samples regime, it outperforms standard machine learning methods such as nearest-neighbors, support-vector machines, and neural networks (NNs), and matches specialized one-shot transfer-learning methods but without the need for pre-training. The MothNet architecture illustrates how algorithmic structures derived from biological brains can be used to build alternative NNs that may avoid the high training data demands of many current engineered NNs.}
}
@article{XIAO2019280,
title = {Multi-perspective neural architecture for recommendation system},
journal = {Neural Networks},
volume = {118},
pages = {280-288},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301807},
author = {Han Xiao and Yidong Chen and Xiaodong Shi and Ge Xu},
keywords = {Recommendation, Neural architecture},
abstract = {Currently, there starts a research trend to leverage neural architecture for recommendation systems. Though several deep recommender models are proposed, most methods are too simple to characterize users’ complex preference. In this paper, for a fine-grained analysis, users’ ratings are explained from multiple perspectives, based on which, we propose our neural architectures. Specifically, our model employs several sequential stages to encode the user and item into hidden representations. In one stage, the user and item are represented from multiple perspectives and in each perspective, the representation of user and that of item put attentions to each other. Last, we metric the output representations from the final stage to approach the users’ ratings. Extensive experiments demonstrate that our method achieves substantial improvements against baselines.}
}
@article{SAHOO201978,
title = {Optimization of sampling intervals for tracking control of nonlinear systems: A game theoretic approach},
journal = {Neural Networks},
volume = {114},
pages = {78-90},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930070X},
author = {Avimanyu Sahoo and Vignesh Narayanan},
keywords = {Approximate dynamic programming, Event-based control, Hamilton–Jacobi–Issac equation, Nonlinear tracking control, Optimal control, Zero-sum-game},
abstract = {This paper presents a near optimal adaptive event-based sampling scheme for tracking control of an affine nonlinear continuous-time system. A zero-sum game approach is proposed by introducing a novel performance index. The optimal value function, i.e., the solution to the associatedHamilton–Jacobi–Issac (HJI) equation is approximated using a functional link neural network (FLNN) with event-based aperiodic state feedback information as inputs. The saddle point approximated optimal solution is employed to design the near optimal event-based control policy and the sampling condition. An impulsive weight update scheme is designed to guarantee local ultimate boundedness of the closed-loop parameters, which is analyzed via extension of Lyapunov stability theory for the impulsive hybrid dynamical systems. Zeno-freeness of the event-sampling scheme is enforced and its effect on stability is analyzed. Finally, numerical simulation results are included to corroborate the analytical design, which shows a 48.82% reduction of feedback communication and computational load.}
}
@article{MALTONI201956,
title = {Continuous learning in single-incremental-task scenarios},
journal = {Neural Networks},
volume = {116},
pages = {56-73},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300838},
author = {Davide Maltoni and Vincenzo Lomonaco},
keywords = {Continuous learning, Lifelong learning, Deep learning, Single-incremental-task, Incremental class learning, Object recognition},
abstract = {It was recently shown that architectural, regularization and rehearsal strategies can be used to train deep models sequentially on a number of disjoint tasks without forgetting previously acquired knowledge. However, these strategies are still unsatisfactory if the tasks are not disjoint but constitute a single incremental task (e.g., class-incremental learning). In this paper we point out the differences between multi-task and single-incremental-task scenarios and show that well-known approaches such as LWF, EWC and SI are not ideal for incremental task scenarios. A new approach, denoted as AR1, combining architectural and regularization strategies is then specifically proposed. AR1 overhead (in terms of memory and computation) is very small thus making it suitable for online learning. When tested on CORe50 and iCIFAR-100, AR1 outperformed existing regularization strategies by a good margin.}
}
@article{MA2019110,
title = {Label-specific feature selection and two-level label recovery for multi-label classification with missing labels},
journal = {Neural Networks},
volume = {118},
pages = {110-126},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301091},
author = {Jianghong Ma and Tommy W.S. Chow},
keywords = {Multi-label learning, Label recovery, Two-level semantic correlations, Label-specific feature selection, Missing label},
abstract = {In multi-label learning, each instance is assigned by several nonexclusive labels. However, these labels are often incomplete, resulting in unsatisfactory performance in label related applications. We design a two-level label recovery mechanism to perform label imputation in training sets. An instance-wise semantic relational graph and a label-wise semantic relational graph are used in this mechanism to recover the label matrix. These two graphs exhibit a capability of capturing reliable two-level semantic correlations. We also design a label-specific feature selection mechanism to perform label prediction in testing sets. The local and global feature–label connection are both exploited in this mechanism to learn an inductive classifier. By updating the matrix that represents the relevance between features and the predicted labels, the label-specific feature selection mechanism is robust to missing labels. At last, intensive experimental results on nine datasets under different domains are presented to demonstrate the effectiveness of the proposed approach.}
}
@article{XIAO2019124,
title = {A new noise-tolerant and predefined-time ZNN model for time-dependent matrix inversion},
journal = {Neural Networks},
volume = {117},
pages = {124-134},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301376},
author = {Lin Xiao and Yongsheng Zhang and Jianhua Dai and Ke Chen and Song Yang and Weibing Li and Bolin Liao and Lei Ding and Jichun Li},
keywords = {Zeroing neural network, Recurrent neural network, Time-dependent matrix inversion, Noise tolerance, Finite-time convergence},
abstract = {In this work, a new zeroing neural network (ZNN) using a versatile activation function (VAF) is presented and introduced for solving time-dependent matrix inversion. Unlike existing ZNN models, the proposed ZNN model not only converges to zero within a predefined finite time but also tolerates several noises in solving the time-dependent matrix inversion, and thus called new noise-tolerant ZNN (NNTZNN) model. In addition, the convergence and robustness of this model are mathematically analyzed in detail. Two comparative numerical simulations with different dimensions are used to test the efficiency and superiority of the NNTZNN model to the previous ZNN models using other activation functions. In addition, two practical application examples (i.e., a mobile manipulator and a real Kinova JACO2 robot manipulator) are presented to validate the applicability and physical feasibility of the NNTZNN model in a noisy environment. Both simulative and experimental results demonstrate the effectiveness and tolerant-noise ability of the NNTZNN model.}
}
@article{CARPENTER2019204,
title = {Looking to the future: Learning from experience, averting catastrophe},
journal = {Neural Networks},
volume = {118},
pages = {204-207},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.025},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301650},
author = {Gail A. Carpenter},
keywords = {Self-supervised learning, Semi-supervised learning, Neural networks, Artificial Intelligence (AI), Adaptive Resonance Theory (ART), Self-supervised (ART)},
abstract = {As humans go through life sifting vast quantities of complex information, we extract knowledge from settings that are more ambiguous than our early homes and classrooms. Learning from experience in an individual’s unique context generally improves expert performance, despite the risks inherent in brain dynamics that can transform previously reliable expectations. Designers of twenty-first century technologies face the challenges and responsibilities posed by fielded systems that continue to learn on their own. The neural model Self-supervised ART, which can acquire significantly new knowledge in unpredictable contexts, is an example of one such system.}
}
@article{LU2019139,
title = {Fixed-time pinning-controlled synchronization for coupled delayed neural networks with discontinuous activations},
journal = {Neural Networks},
volume = {116},
pages = {139-149},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930108X},
author = {Hui Lü and Wangli He and Qing-Long Han and Chen Peng},
keywords = {Fixed-time synchronization, Neural networks, Discontinuous activations, Pinning control},
abstract = {This paper deals with the fixed-time synchronization problem of coupled delayed neural networks with discontinuous activations. Based on pinning control, a discontinuous controller is firstly proposed to guarantee that coupled neural networks achieve synchronization with a desired trajectory in finite time. Then, a discontinuous fixed-time controller is designed. With the fixed-time controller, the settling time can be estimated regardless of initial conditions. By providing a topology-dependent Lyapunov function, some criteria of finite-/fixed-time synchronization are derived. Finally, two numerical examples are given to show the effectiveness of the proposed controllers.}
}
@article{FENG2019179,
title = {Robust manifold broad learning system for large-scale noisy chaotic time series prediction: A perturbation perspective},
journal = {Neural Networks},
volume = {117},
pages = {179-190},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301418},
author = {Shoubo Feng and Weijie Ren and Min Han and Yen Wei Chen},
keywords = {Time series, Tikhonov, Regularization, Perturbation, Manifold, Broad learning system},
abstract = {Noises and outliers commonly exist in dynamical systems because of sensor disturbations or extreme dynamics. Thus, the robustness and generalization capacity are of vital importance for system modeling. In this paper, the robust manifold broad learning system(RM-BLS) is proposed for system modeling and large-scale noisy chaotic time series prediction. Manifold embedding is utilized for chaotic system evolution discovery. The manifold representation is randomly corrupted by perturbations while the features not related to low-dimensional manifold embedding are discarded by feature selection. It leads to a robust learning paradigm and achieves better generalization performance. We also develop an efficient solution for Stiefel manifold optimization, in which the orthogonal constraints are maintained by Cayley transformation and curvilinear search algorithm. Furthermore, we discuss the common thoughts between random perturbation approximation and other mainstream regularization methods. We also prove the equivalence between perturbations to manifold embedding and Tikhonov regularization. Simulation results on large-scale noisy chaotic time series prediction illustrates the robustness and generalization performance of our method.}
}
@article{SEVGEN201960,
title = {New stability results for Takagi–Sugeno fuzzy Cohen–Grossberg neural networks with multiple delays},
journal = {Neural Networks},
volume = {114},
pages = {60-66},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300723},
author = {Selcuk Sevgen},
keywords = {Fuzzy systems, Cohen–Grossberg neural networks, Time delays, Lyapunov stability theorems},
abstract = {This work focuses on global asymptotic stability of Takagi–Sugeno fuzzy Cohen–Grossberg neural networks with multiple time delays. By using the standard Lyapunov stability techniques and nonsingular M-matrix condition of matrices together with employing the nonlinear Lipschitz activation functions, a new easily verifiable sufficient criterion is obtained to guarantee global asymptotic stability of the Cohen–Grossberg neural network model which is represented by a Takagi–Sugeno fuzzy model. A constructive numerical example is studied to demonstrate the effectiveness of the proposed theoretical results. This numerical example is also used to make a comparison between the global stability condition obtained in this study and some of previously published global stability results. This comparison reveals that the condition we propose establishes a novel and alternative stability result for Takagi–Sugeno fuzzy Cohen–Grossberg neural networks of this class.}
}
@article{JARUSEK2019150,
title = {Photomontage detection using steganography technique based on a neural network},
journal = {Neural Networks},
volume = {116},
pages = {150-165},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300966},
author = {Robert Jarusek and Eva Volna and Martin Kotyrba},
keywords = {Photomontage, Steganography, Neural network, StegoNN, CoMoFoD database},
abstract = {This article presents a steganographic method StegoNN based on neural networks. The method is able to identify a photomontage from presented signed images. Unlike other academic approaches using neural networks primarily as classifiers, the StegoNN method uses the characteristics of neural networks to create suitable attributes which are then necessary for subsequent detection of modified photographs. This also results in a fact that if an image is signed by this technique, the detection of modifications does not need any external data (database of non-modified originals) and the quality of the signature in various parts of the image also serves to identify modified (corrupted) parts of the image. The experimental study was performed on photographs from CoMoFoD Database and its results were compared with other approaches using this database based on standard metrics. The performed study showed the ability of the StegoNN method to detect corrupted parts of an image and to mark places which have been most probably image-manipulated. The usage of this method is suitable for reportage photography, but in general, for all cases where verification (provability) of authenticity and veracity of the presented image are required.}
}
@article{YILDIZ201965,
title = {Classification and comparison via neural networks},
journal = {Neural Networks},
volume = {118},
pages = {65-80},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301777},
author = {İlkay Yıldız and Peng Tian and Jennifer Dy and Deniz Erdoğmuş and James Brown and Jayashree Kalpathy-Cramer and Susan Ostmo and J. {Peter Campbell} and Michael F. Chiang and Stratis Ioannidis},
keywords = {Neural network, Joint learning, Comparison, Classification, Siamese network},
abstract = {We consider learning from comparison labels generated as follows: given two samples in a dataset, a labeler produces a label indicating their relative order. Such comparison labels scale quadratically with the dataset size; most importantly, in practice, they often exhibit lower variance compared to class labels. We propose a new neural network architecture based on siamese networks to incorporate both class and comparison labels in the same training pipeline, using Bradley–Terry and Thurstone loss functions. Our architecture leads to a significant improvement in predicting both class and comparison labels, increasing classification AUC by as much as 35% and comparison AUC by as much as 6% on several real-life datasets. We further show that, by incorporating comparisons, training from few samples becomes possible: a deep neural network of 5.9 million parameters trained on 80 images attains a 0.92 AUC when incorporating comparisons.}
}
@article{ZHAO2019268,
title = {Observer-based sliding mode control for synchronization of delayed chaotic neural networks with unknown disturbance},
journal = {Neural Networks},
volume = {117},
pages = {268-273},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301534},
author = {Yongshun Zhao and Xiaodi Li and Peiyong Duan},
keywords = {Chaotic neural networks, Sliding mode control, Lyapunov–Krasovskii functional, Linear matrix inequality (), Disturbance observer},
abstract = {This paper considers the synchronization of delayed chaotic neural networks with unknown disturbance via observer-based sliding mode control. We design a sliding surface involving integral structure and a discontinuous control law such that the trajectories of error system converge to the sliding surface in finite time and remain on it thereafter. Then, by constructing Lyapunov–Krasovskii functional and using the linear matrix inequality (LMI) technique, some sufficient conditions are derived to guarantee the synchronization of chaotic neural networks. The advantages of our proposed results include:(i) It can be applied to synchronous control for drive and response systems with different structures; (ii) It can be applied to the response system with unknown disturbance. Finally, a simulation example is shown to illustrate the proposed methods.}
}
@article{LIANG2019257,
title = {An unsupervised EEG decoding system for human emotion recognition},
journal = {Neural Networks},
volume = {116},
pages = {257-268},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301017},
author = {Zhen Liang and Shigeyuki Oba and Shin Ishii},
keywords = {Electroencephalography, Brain activity, Emotion recognition, Hypergraph, Decoding model},
abstract = {Emotion plays a vital role in human health and many aspects of life, including relationships, behaviors and decision-making. An intelligent emotion recognition system may provide a flexible method to monitor emotion changes in daily life and send warning information when unusual/unhealthy emotional states occur. Here, we proposed a novel unsupervised learning-based emotion recognition system in an attempt to decode emotional states from electroencephalography (EEG) signals. Four dimensions of human emotions were examined: arousal, valence, dominance and liking. To better characterize the trials in terms of EEG features, we used hypergraph theory. Emotion recognition was realized through hypergraph partitioning, which divided the EEG-based hypergraph into a specific number of clusters, with each cluster indicating one of the emotion classes and vertices (trials) in the same cluster sharing similar emotion properties. Comparison of the proposed unsupervised learning-based emotion recognition system with other recognition systems using a well-known public emotion database clearly demonstrated the validity of the proposed system.}
}
@article{LI2019102,
title = {Quasi-projective and complete synchronization of fractional-order complex-valued neural networks with time delays},
journal = {Neural Networks},
volume = {118},
pages = {102-109},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301819},
author = {Hong-Li Li and Cheng Hu and Jinde Cao and Haijun Jiang and Ahmed Alsaedi},
keywords = {Quasi-projective synchronization, Complete synchronization, Fractional-order, Complex-valued neural networks, Time delays},
abstract = {This paper studies quasi-projective synchronization (QPS) and complete synchronization (CS) for a class of fractional-order complex-valued neural networks with time delays by designing suitable controllers. To realize QPS and CS, linear feedback controller and adaptive controller are designed, and a novel fractional-order differential inequality is built by means of Laplace transform and properties of Mittag-Leffler function. By utilizing Lyapunov method, our proposed inequality, fractional-order Razumikhin theorem and some complex analysis techniques, some effective criteria are derived to ensure QPS and CS of the considered networks. Furthermore, the error bound of QPS is obtained. Finally, some numerical results are given to demonstrate the effectiveness of the presented theoretical results.}
}
@article{2021ii,
title = {Editorial Board},
journal = {Neural Networks},
volume = {143},
pages = {ii},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00375-0},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021003750}
}
@article{ILLING201990,
title = {Biologically plausible deep learning — But how far can we go with shallow networks?},
journal = {Neural Networks},
volume = {118},
pages = {90-101},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301741},
author = {Bernd Illing and Wulfram Gerstner and Johanni Brea},
keywords = {Deep learning, Local learning rules, Random projections, Unsupervised feature learning, Spiking networks, MNIST},
abstract = {Training deep neural networks with the error backpropagation algorithm is considered implausible from a biological perspective. Numerous recent publications suggest elaborate models for biologically plausible variants of deep learning, typically defining success as reaching around 98% test accuracy on the MNIST data set. Here, we investigate how far we can go on digit (MNIST) and object (CIFAR10) classification with biologically plausible, local learning rules in a network with one hidden layer and a single readout layer. The hidden layer weights are either fixed (random or random Gabor filters) or trained with unsupervised methods (Principal/Independent Component Analysis or Sparse Coding) that can be implemented by local learning rules. The readout layer is trained with a supervised, local learning rule. We first implement these models with rate neurons. This comparison reveals, first, that unsupervised learning does not lead to better performance than fixed random projections or Gabor filters for large hidden layers. Second, networks with localized receptive fields perform significantly better than networks with all-to-all connectivity and can reach backpropagation performance on MNIST. We then implement two of the networks – fixed, localized, random & random Gabor filters in the hidden layer – with spiking leaky integrate-and-fire neurons and spike timing dependent plasticity to train the readout layer. These spiking models achieve >98.2% test accuracy on MNIST, which is close to the performance of rate networks with one hidden layer trained with backpropagation. The performance of our shallow network models is comparable to most current biologically plausible models of deep learning. Furthermore, our results with a shallow spiking network provide an important reference and suggest the use of data sets other than MNIST for testing the performance of future models of biologically plausible deep learning.}
}
@article{LAREDO2019178,
title = {A neural network-evolutionary computational framework for remaining useful life estimation of mechanical systems},
journal = {Neural Networks},
volume = {116},
pages = {178-187},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301224},
author = {David Laredo and Zhaoyin Chen and Oliver Schütze and Jian-Qiao Sun},
keywords = {Artificial neural networks, Moving time window, RUL estimation, Prognostics, Evolutionary algorithms},
abstract = {This paper presents a framework for estimating the remaining useful life (RUL) of mechanical systems. The framework consists of a multi-layer perceptron and an evolutionary algorithm for optimizing the data-related parameters. The framework makes use of a strided time window along with a piecewise linear model to estimate the RUL for each mechanical component. Tuning the data-related parameters in the optimization framework allows for the use of simple models, e.g. neural networks with few hidden layers and few neurons at each layer, which may be deployed in environments with limited resources such as embedded systems. The proposed method is evaluated on the publicly available C-MAPSS dataset. The accuracy of the proposed method is compared against other state-of-the art methods in the literature. The proposed method is shown to perform better than the compared methods while making use of a compact model.}
}
@article{DAI201911,
title = {Multilayer one-class extreme learning machine},
journal = {Neural Networks},
volume = {115},
pages = {11-22},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300772},
author = {Haozhen Dai and Jiuwen Cao and Tianlei Wang and Muqing Deng and Zhixin Yang},
keywords = {One-class classification, OC-ELM, ML-OCELM, Kernel learning, Outlier/anomaly detection},
abstract = {One-class classification has been found attractive in many applications for its effectiveness in anomaly or outlier detection. Representative one-class classification algorithms include the one-class support vector machine (SVM), Naive Parzen density estimation, autoencoder (AE), etc. Recently, the one-class extreme learning machine (OC-ELM) has been developed for learning acceleration and performance enhancement. But existing one-class algorithms are generally less effective in complex and multi-class classifications. To alleviate the deficiency, a multilayer neural network based one-class classification with ELM (in short, as ML-OCELM) is developed in this paper. The stacked AEs are employed in ML-OCELM to exploit an effective feature representation for complex data. The effective kernel based learning framework is also investigated in the stacked AEs of ML-OCELM, leading to a multilayer kernel based OC-ELM (in short, as MK-OCELM). The MK-OCELM has advantages of less human-intervention parameters and good generalization performance. Experiments on 13 benchmark UCI classification datasets and a real application on urban acoustic classification (UAC) are carried out to show the superiority of the proposed ML-OCELM/MK-OCELM over the OC-ELM and several state-of-the-art algorithms.}
}
@article{ANGHINONI2019295,
title = {Time series trend detection and forecasting using complex network topology analysis},
journal = {Neural Networks},
volume = {117},
pages = {295-306},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301583},
author = {Leandro Anghinoni and Liang Zhao and Donghong Ji and Heng Pan},
keywords = {Time series, Complex networks, Community detection, Trend prediction},
abstract = {Extracting knowledge from time series provides important tools for many real applications. However, many challenging problems still open due to the stochastic nature of large amount of time series. Considering this scenario, new data mining and machine learning techniques have continuously developed. In this paper, we study time series based on its topological features, observed on a complex network generated from the time series data. Specifically, we present a trend detection algorithm for stochastic time series based on community detection and network metrics. The proposed model presents some advantages over traditional time series analysis, such as adaptive number of classes with measurable strength and better noise absorption. The appealing feature of this work is to pave a new way to represent time series trends by communities of complex networks in topological space instead of physical space (spatial–temporal space or frequency spectral) as traditional techniques do. Experimental results on artificial and real data-sets shows that the proposed method is able to classify the time series into local and global patterns. As a consequence, it improves the predictability on time series.}
}
@article{YANG2019101,
title = {Fixed-time synchronization of coupled memristor-based neural networks with time-varying delays},
journal = {Neural Networks},
volume = {116},
pages = {101-109},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301066},
author = {Chao Yang and Lihong Huang and Zuowei Cai},
keywords = {fixed-time synchronization, Differential inclusion, Memristor-based neural networks, Coupled},
abstract = {This paper investigates the fixed-time synchronization of Memristor-based neural networks with time-delayed and coupled. In view of the retarded differential inclusions theory, drive–response concept, the authors give some sufficient conditions to ensure the fixed-time synchronization issue of Memristor-based neural networks. Two novel state-feedback controllers and adaptive controller are designed such that the system can realize fixed-time complete synchronization by means of inequality technique and non-smooth analysis theory. It is worth to point out that, without desiring values of the initial conditions or under the linear growth condition of the controller, the settling time of fixed-time synchronization is estimated. Finally, an example is given to further illustrate the benefits of the proposed switched control approach.}
}
@article{HUANG201967,
title = {Novel bifurcation results for a delayed fractional-order quaternion-valued neural network},
journal = {Neural Networks},
volume = {117},
pages = {67-93},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301340},
author = {Chengdai Huang and Xiaobing Nie and Xuan Zhao and Qiankun Song and Zhengwen Tu and Min Xiao and Jinde Cao},
keywords = {Fractional order, Quaternion-valued, Neural networks, Time delay, Hopf bifurcation},
abstract = {This paper reports the innovative results on the stability and bifurcation for a delayed fractional-order quaternion-valued neural network(FOQVNN). Delay-stimulated bifurcation criteria of the developed FOQVNN are attained. Then, the bifurcation diagrams are perfectly exhibited to authenticate the veracity of the bifurcation results. Besides, the stability zone is more larger of the addressed FOQVNN in comparison with its counterpart if other parameters are intercalated. It further witnesses that the amplitudes of bifurcation oscillation get bigger with the augmentation of time delay. It discloses that the bifurcation phenomena engender earlier as the order incrementally magnifies. The exactness and merits of the achieved analytic results are eventually substantiated by a simulation example.}
}
@article{LIU201965,
title = {Flexible unsupervised feature extraction for image classification},
journal = {Neural Networks},
volume = {115},
pages = {65-71},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300814},
author = {Yang Liu and Feiping Nie and Quanxue Gao and Xinbo Gao and Jungong Han and Ling Shao},
keywords = {Dimensionality reduction, Unsupervised, Feature extraction},
abstract = {Dimensionality reduction is one of the fundamental and important topics in the fields of pattern recognition and machine learning. However, most existing dimensionality reduction methods aim to seek a projection matrix W such that the projection WTx is exactly equal to the true low-dimensional representation. In practice, this constraint is too rigid to well capture the geometric structure of data. To tackle this problem, we relax this constraint but use an elastic one on the projection with the aim to reveal the geometric structure of data. Based on this context, we propose an unsupervised dimensionality reduction model named flexible unsupervised feature extraction (FUFE) for image classification. Moreover, we theoretically prove that PCA and LPP, which are two of the most representative unsupervised dimensionality reduction models, are special cases of FUFE, and propose a non-iterative algorithm to solve it. Experiments on five real-world image databases show the effectiveness of the proposed model.}
}
@article{CERRUELAGARCIA2019175,
title = {Improving the combination of results in the ensembles of prototype selectors},
journal = {Neural Networks},
volume = {118},
pages = {175-191},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301868},
author = {Gonzalo Cerruela-García and Aida {de Haro-García} and José Pérez-Parras Toledano and Nicolás García-Pedrajas},
keywords = {Prototype selection, Ensembles of prototypes selectors, Bagging},
abstract = {Prototype selection is one of the most common preprocessing tasks in data mining applications. The vast amounts of data that we must handle in practical problems render the removal of noisy, redundant or useless instances a convenient first step for any real-world application. Many algorithms have been proposed for prototype selection. For difficult problems, however, the use of only a single method would unlikely achieve the desired performance. Similar to the problem of classification, ensembles of prototype selectors have been proposed to overcome the limitations of single algorithms. In ensembles of prototype selectors, the usual combination method is based on a voting scheme coupled with an acceptance threshold. However, this method is suboptimal, because the relationships among the prototypes are not taken into account. In this paper, we propose a different approach, in which we consider not only the number of times every prototype has been selected but also the subsets of prototypes that are selected. With this additional information we develop GEEBIES, which is a new way of combining the results of ensembles of prototype selectors. In a large set of problems, we show that our proposal outperforms the standard boosting approach. A way of scaling up our method to large datasets is also proposed and experimentally tested.}
}
@article{ZHAO201943,
title = {Global-and-local-structure-based neural network for fault detection},
journal = {Neural Networks},
volume = {118},
pages = {43-53},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301625},
author = {Haitao Zhao and Zhihui Lai and Yudong Chen},
keywords = {Statistical process monitoring, Fault detection, Feedforward neural network, Principal component analysis, Dimension reduction},
abstract = {A novel statistical fault detection method, called the global-and-local-structure-based neural network (GLSNN), is proposed for fault detection. GLSNN is a nonlinear data-driven process monitoring technique through preserving both global and local structures of normal process data. GLSNN is characterized by adaptively training a neural network which takes both the global variance information and the local geometrical structure into consideration. GLSNN is designed to extract the meaningful low-dimensional features from original high-dimensional process data. After nonlinear feature extraction, Hotelling T2 statistic and the squared prediction error (SPE) statistic are adopted for online fault detection. The merits of the proposed GLSNN method are demonstrated by both theoretical analysis and case studies on the Tennessee Eastman (TE) benchmark process. Extensive experimental results show the superiority of GLSNN in terms of missed detection rate (MDR) and false alarm rate (FAR). The source code of GLSNN can be found in https://github.com/htzhaoecust/glsnn.}
}
@article{YANG2019188,
title = {BoSR: A CNN-based aurora image retrieval method},
journal = {Neural Networks},
volume = {116},
pages = {188-197},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301108},
author = {Xi Yang and Nannan Wang and Bin Song and Xinbo Gao},
keywords = {Bag of salient regions, Circular fisheye lens, Aurora image retrieval},
abstract = {The deep learning models especially the CNN have achieved amazing performance on natural image retrieval. However, remote sensing images captured with anamorphic lens are still retrieved via manual selection or traditional SIFT-based methods. How to leverage the advanced CNN models for remote sensing image retrieval is a new task of significance. This paper focuses on the aurora images captured with all-sky-imagers (ASI). By analyzing the imaging principle of ASI and characteristics of aurora, a salient region determination (SRD) scheme is proposed and embedded into the Mask R-CNN framework. Thus, we can regard an image as a “bag” of salient regions (BoSR). In practice, each salient region is represented with a CNN feature extracted from the SRD embedded Mask R-CNN. After clustered to generate a visual vocabulary, each CNN feature is quantized to its nearest center for indexing. In the stage of online retrieval, by computing the similarity scores between query image and all images in the dataset, ranking results can be obtained and image with the highest value is exported as the top rank. Extensive experiments are conducted on the big aurora data, and the results demonstrate that the proposed method improves the retrieval accuracy and efficiency.}
}
@article{NAPOLES201972,
title = {Short-term cognitive networks, flexible reasoning and nonsynaptic learning},
journal = {Neural Networks},
volume = {115},
pages = {72-81},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300930},
author = {Gonzalo Nápoles and Frank Vanhoenshoven and Koen Vanhoof},
keywords = {Short-term memory, Cognitive mapping, Nonsynaptic learning, Modeling, Simulation},
abstract = {While the machine learning literature dedicated to fully automated reasoning algorithms is abundant, the number of methods enabling the inference process on the basis of previously defined knowledge structures is scanter. Fuzzy Cognitive Maps (FCMs) are recurrent neural networks that can be exploited towards this goal because of their flexibility to handle external knowledge. However, FCMs suffer from a number of issues that range from the limited prediction horizon to the absence of theoretically sound learning algorithms able to produce accurate predictions. In this paper we propose a neural system named Short-term Cognitive Networks that tackle some of these limitations. In our model, used for regression and pattern completion, weights are not constricted and may have a causal nature or not. As a second contribution, we present a nonsynaptic learning algorithm to improve the network performance without modifying the previously defined weight matrix. Besides, we derive a stop condition to prevent the algorithm from iterating without significantly decreasing the global simulation error.}
}
@article{THIEDE201923,
title = {Gradient based hyperparameter optimization in Echo State Networks},
journal = {Neural Networks},
volume = {115},
pages = {23-29},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300413},
author = {Luca Anthony Thiede and Ulrich Parlitz},
keywords = {Echo State Network, Reservoir computing, Hyperparameters},
abstract = {Like most machine learning algorithms, Echo State Networks possess several hyperparameters that have to be carefully tuned for achieving best performance. For minimizing the error on a specific task, we present a gradient based optimization algorithm, for the input scaling, the spectral radius, the leaking rate, and the regularization parameter.}
}
@article{PHAN2019220,
title = {Group variable selection via ℓp,0 regularization and application to optimal scoring},
journal = {Neural Networks},
volume = {118},
pages = {220-234},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301510},
author = {Duy Nhat Phan and Hoai An {Le Thi}},
keywords = {Group variable selection,  regularization, DC approximation, DC programming, DCA, Optimal scoring},
abstract = {The need to select groups of variables arises in many statistical modeling problems and applications. In this paper, we consider the ℓp,0-norm regularization for enforcing group sparsity and investigate a DC (Difference of Convex functions) approximation approach for solving the ℓp,0-norm regularization problem. We show that, with suitable parameters, the original and approximate problems are equivalent. Considering two equivalent formulations of the approximate problem we develop DC programming and DCA (DC Algorithm) for solving them. As an application, we implement the proposed algorithms for group variable selection in the optimal scoring problem. The sparsity is obtained by using the ℓp,0-regularization that selects the same features in all discriminant vectors. The resulting sparse discriminant vectors provide a more interpretable low-dimensional representation of data. The experimental results on both simulated datasets and real datasets indicate the efficiency of the proposed algorithms.}
}
@article{TANAKA2019100,
title = {Recent advances in physical reservoir computing: A review},
journal = {Neural Networks},
volume = {115},
pages = {100-123},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300784},
author = {Gouhei Tanaka and Toshiyuki Yamane and Jean Benoit Héroux and Ryosho Nakane and Naoki Kanazawa and Seiji Takeda and Hidetoshi Numata and Daiju Nakano and Akira Hirose},
keywords = {Neural networks, Machine learning, Reservoir computing, Nonlinear dynamical systems, Neuromorphic device},
abstract = {Reservoir computing is a computational framework suited for temporal/sequential data processing. It is derived from several recurrent neural network models, including echo state networks and liquid state machines. A reservoir computing system consists of a reservoir for mapping inputs into a high-dimensional space and a readout for pattern analysis from the high-dimensional states in the reservoir. The reservoir is fixed and only the readout is trained with a simple method such as linear regression and classification. Thus, the major advantage of reservoir computing compared to other recurrent neural networks is fast learning, resulting in low training cost. Another advantage is that the reservoir without adaptive updating is amenable to hardware implementation using a variety of physical systems, substrates, and devices. In fact, such physical reservoir computing has attracted increasing attention in diverse fields of research. The purpose of this review is to provide an overview of recent advances in physical reservoir computing by classifying them according to the type of the reservoir. We discuss the current issues and perspectives related to physical reservoir computing, in order to further expand its practical applications and develop next-generation machine learning systems.}
}
@article{BING202021,
title = {Indirect and direct training of spiking neural networks for end-to-end control of a lane-keeping vehicle},
journal = {Neural Networks},
volume = {121},
pages = {21-36},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301595},
author = {Zhenshan Bing and Claus Meschede and Guang Chen and Alois Knoll and Kai Huang},
keywords = {, End-to-end learning, R-STDP, Lane keeping},
abstract = {Building spiking neural networks (SNNs) based on biological synaptic plasticities holds a promising potential for accomplishing fast and energy-efficient computing, which is beneficial to mobile robotic applications. However, the implementations of SNNs in robotic fields are limited due to the lack of practical training methods. In this paper, we therefore introduce both indirect and direct end-to-end training methods of SNNs for a lane-keeping vehicle. First, we adopt a policy learned using the Deep Q-Learning (DQN) algorithm and then subsequently transfer it to an SNN using supervised learning. Second, we adopt the reward-modulated spike-timing-dependent plasticity (R-STDP) for training SNNs directly, since it combines the advantages of both reinforcement learning and the well-known spike-timing-dependent plasticity (STDP). We examine the proposed approaches in three scenarios in which a robot is controlled to keep within lane markings by using an event-based neuromorphic vision sensor. We further demonstrate the advantages of the R-STDP approach in terms of the lateral localization accuracy and training time steps by comparing them with other three algorithms presented in this paper.}
}
@article{BALDI2019288,
title = {The capacity of feedforward neural networks},
journal = {Neural Networks},
volume = {116},
pages = {288-311},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301078},
author = {Pierre Baldi and Roman Vershynin},
keywords = {Neural networks, Capacity, Complexity, Deep learning},
abstract = {A long standing open problem in the theory of neural networks is the development of quantitative methods to estimate and compare the capabilities of different architectures. Here we define the capacity of an architecture by the binary logarithm of the number of functions it can compute, as the synaptic weights are varied. The capacity provides an upperbound on the number of bits that can be extracted from the training data and stored in the architecture during learning. We study the capacity of layered, fully-connected, architectures of linear threshold neurons with L layers of size n1,n2,…,nL and show that in essence the capacity is given by a cubic polynomial in the layer sizes: C(n1,…,nL)=∑k=1L−1min(n1,…,nk)nknk+1, where layers that are smaller than all previous layers act as bottlenecks. In proving the main result, we also develop new techniques (multiplexing, enrichment, and stacking) as well as new bounds on the capacity of finite sets. We use the main result to identify architectures with maximal or minimal capacity under a number of natural constraints. This leads to the notion of structural regularization for deep architectures. While in general, everything else being equal, shallow networks compute more functions than deep networks, the functions computed by deep networks are more regular and “interesting”.}
}
@article{YU2019104,
title = {Understanding autoencoders with information theoretic concepts},
journal = {Neural Networks},
volume = {117},
pages = {104-123},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301352},
author = {Shujian Yu and José C. Príncipe},
keywords = {Autoencoders, Data processing inequality, Intrinsic dimensionality, Information theory},
abstract = {Despite their great success in practical applications, there is still a lack of theoretical and systematic methods to analyze deep neural networks. In this paper, we illustrate an advanced information theoretic methodology to understand the dynamics of learning and the design of autoencoders, a special type of deep learning architectures that resembles a communication channel. By generalizing the information plane to any cost function, and inspecting the roles and dynamics of different layers using layer-wise information quantities, we emphasize the role that mutual information plays in quantifying learning from data. We further suggest and also experimentally validate, for mean square error training, three fundamental properties regarding the layer-wise flow of information and intrinsic dimensionality of the bottleneck layer, using respectively the data processing inequality and the identification of a bifurcation point in the information plane that is controlled by the given data. Our observations have direct impact on the optimal design of autoencoders, the design of alternative feedforward training methods, and even in the problem of generalization.}
}
@article{XU2019274,
title = {Locally linear SVMs based on boundary anchor points encoding},
journal = {Neural Networks},
volume = {117},
pages = {274-284},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.023},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301637},
author = {Baile Xu and Shaofeng Shen and Furao Shen and Jian Zhao},
keywords = {Locally linear classifier, Kernel SVM, Boundary points, Large-scale data},
abstract = {In this paper, we propose a locally linear classifier based on boundary anchor points encoding (LLBAP) to achieve the efficiency of linear SVM and the power of kernel SVM. LLBAP partitions linearly non-separable data into approximately linearly separable parts based on boundary point scanning and local coding. Each part of data is solved by a linear SVM. Experiments on large-scale benchmark datasets demonstrate that the proposed method is more efficient than kernel SVM in both training and testing phases; its efficiency and classification accuracy also outperform other locally linear classifiers on those benchmark datasets.}
}
@article{KAN2019157,
title = {Exponential synchronization of time-varying delayed complex-valued neural networks under hybrid impulsive controllers},
journal = {Neural Networks},
volume = {114},
pages = {157-163},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300668},
author = {Yu Kan and Jianquan Lu and Jianlong Qiu and Jürgen Kurths},
keywords = {Complex-valued neural networks, Synchronization, Average impulsive interval, Average impulsive gain, Hybrid impulses},
abstract = {This paper focuses on exponential synchronization for master–slave time-varying delayed complex-valued neural networks (CVNNs) under hybrid impulsive controllers. Hybrid impulsive controllers is the extension of impulsive controllers, which can simultaneously permit synchronizing as well as desynchronizing impulses in one impulsive sequence, i.e., hybrid impulses. We separate CVNNs into their real and imaginary parts, which leads to two real-valued neural networks (RVNNs). Based on the concepts of average impulsive interval (AII) and average impulsive gain (AIG), we find that master–slave exponential synchronization for the real and imaginary parts of CVNNs can be realized via hybrid impulsive control under certain conditions. By employing the Lyapunov method, sufficient criteria are established to guarantee synchronization of the given master–slave CVNNs. Finally, the validity of the obtained results is demonstrated via a numerical example.}
}
@article{ZHAO201982,
title = {Equivalence between dropout and data augmentation: A mathematical check},
journal = {Neural Networks},
volume = {115},
pages = {82-89},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300942},
author = {Dazhi Zhao and Guozhu Yu and Peng Xu and Maokang Luo},
keywords = {Deep learning, Data augmentation, Dropout, Neural network, Mathematical check},
abstract = {The great achievements of deep learning can be attributed to its tremendous power of feature representation, where the representation ability comes from the nonlinear activation function and the large number of network nodes. However, deep neural networks suffer from serious issues such as slow convergence, and dropout is an outstanding method to improve the network’s generalization ability and test performance. Many explanations have been given for why dropout works so well, among which the equivalence between dropout and data augmentation is a newly proposed and stimulating explanation. In this article, we discuss the exact conditions for this equivalence to hold. Our main result guarantees that the equivalence relation almost surely holds if the dimension of the input space is equal to or higher than that of the output space. Furthermore, if the commonly used rectified linear unit activation function is replaced by some newly proposed activation function whose value lies in R, then our results can be extended to multilayer neural networks. For comparison, some counterexamples are given for the inequivalent case. Finally, a series of experiments on the MNIST dataset are conducted to illustrate and help understand the theoretical results.}
}
@article{CHE201915,
title = {A collaborative neurodynamic approach to global and combinatorial optimization},
journal = {Neural Networks},
volume = {114},
pages = {15-27},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300425},
author = {Hangjun Che and Jun Wang},
keywords = {Global optimization, Combinatorial optimization, Augmented Lagrangian function, Collaborative neurodynamic approach},
abstract = {In this paper, a collaborative neurodynamic optimization approach is proposed for global and combinatorial optimization. First, a combinatorial optimization problem is reformulated as a global optimization problem. Second, a neurodynamic optimization model based on an augmented Lagrangian function is proposed and its states are proven to be asymptotically stable at a strict local minimum in the presence of nonconvexity in objective function or constraints. In addition, multiple neurodynamic optimization models are employed to search for global optimal solutions collaboratively and particle swarm optimization (PSO) is used to optimize their initial states. The proposed approach is shown to be globally convergent to global optimal solutions as substantiated for solving benchmark problems.}
}
@article{ZENG2019140,
title = {Short-term synaptic plasticity expands the operational range of long-term synaptic changes in neural networks},
journal = {Neural Networks},
volume = {118},
pages = {140-147},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301753},
author = {Guanxiong Zeng and Xuhui Huang and Tianzi Jiang and Shan Yu},
keywords = {Reservoir computing, Sequence learning and retrieval, Short-term depression, Synaptic heterogeneity, Self-organized criticality, Optimal information processing},
abstract = {The brain is highly plastic, with synaptic weights changing across a wide range of time scales, from hundreds of milliseconds to days. Changes occurring at different temporal scales are believed to serve different purposes, with long-term changes for learning and memory and short-term changes for adaptation and synaptic computation. By studying the performance of reservoir computing (RC) models in a memory task, we revealed that short-term synaptic plasticity is fundamentally important for long-term synaptic changes in neural networks. Specifically, short-term depression (STD) greatly expands the operational range of a neural network in which it can accommodate long-term synaptic changes while maintaining system performance. This is achieved by dynamically adjusting neural networks close to a critical state. The effects of STD can be further strengthened by synaptic weight heterogeneity, resulting in networks that can tolerate very large, long-term changes in synaptic weights. Our results highlight a potential mechanism used by the brain to organize plasticity at different time scales, thereby maintaining optimal information processing while allowing internal structural changes necessary for learning and memory.}
}
@article{SIM2019152,
title = {Cost-effective stochastic MAC circuits for deep neural networks},
journal = {Neural Networks},
volume = {117},
pages = {152-162},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301236},
author = {Hyeonuk Sim and Jongeun Lee},
keywords = {Stochastic computing, Convolutional neural network, Stochastic number generator, Hardware acceleration, Low-discrepancy code, Variable latency},
abstract = {Stochastic computing (SC) is a promising computing paradigm that can help address both the uncertainties of future process technology and the challenges of efficient hardware realization for deep neural networks (DNNs). However the impreciseness and long latency of SC have rendered previous SC-based DNN architectures less competitive against optimized fixed-point digital implementations, unless inference accuracy is significantly sacrificed. In this paper we propose a new SC-MAC (multiply-and-accumulate) algorithm, which is a key building block for SC-based DNNs, that is orders of magnitude more efficient and accurate than previous SC-MACs. We also show how our new SC-MAC can be extended to a vector version and used to accelerate both convolution and fully-connected layers of convolutional neural networks (CNNs) using the same hardware. Our experimental results using CNNs designed for MNIST and CIFAR-10 datasets demonstrate that not only is our SC-based CNNs more accurate and 40∼490× more energy-efficient for convolution layers than conventional SC-based ones, but ours can also achieve lower area–delay product and lower energy compared with precision-optimized fixed-point implementations without sacrificing accuracy. We also demonstrate the feasibility of our SC-based CNNs through FPGA prototypes.}
}
@article{HAN201990,
title = {Asynchronous event-based sampling data for impulsive protocol on consensus of non-linear multi-agent systems},
journal = {Neural Networks},
volume = {115},
pages = {90-99},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300826},
author = {Yiyan Han and Chuandong Li and Zhigang Zeng},
keywords = {Impulsive protocol, Discrete-time non-linear multi-agent systems, Event-based asynchronous sampling data, Consensus},
abstract = {In this paper, we discuss the consensus problem of non-linear multi-agent systems where an impulsive protocol with event-based asynchronously sampled data is adopted. Systems that communicate by data asynchronously sampled in limited time intervals are constructed. By separating time instants at which the sampling and communication occur into different ones, resources for such activations that every agent must execute can be reallocated to reduce the system load at communication instants. Event-based schemes are introduced to manipulate the sampling behavior. Two cases that with and without leader in directed networks topologies are both investigated. Sufficient conditions for system parameters and the event-based sampling schemes are given to guarantee the consensus. Numerical simulations are presented to illustrate the effectiveness of our proposed method.}
}
@article{WANG201947,
title = {Robust capped L1-norm twin support vector machine},
journal = {Neural Networks},
volume = {114},
pages = {47-59},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300309},
author = {Chunyan Wang and Qiaolin Ye and Peng Luo and Ning Ye and Liyong Fu},
keywords = {Machine learning, TWSVM, Capped L1-norm, Robustness},
abstract = {Twin support vector machine (TWSVM) is a classical and effective classifier for binary classification. However, its robustness cannot be guaranteed due to the utilization of squared L2-norm distance that can usually exaggerate the influence of outliers. In this paper, we propose a new robust capped L1-norm twin support vector machine (CTWSVM), which sustains the advantages of TWSVM and promotes the robustness in solving a binary classification problem with outliers. The solution of the proposed method can be achieved by optimizing a pair of capped L1-norm related problems using a newly-designed effective iterative algorithm. Also, we present some theoretical analysis on existence of local optimum and convergence of the algorithm. Extensive experiments on an artificial dataset and several UCI datasets demonstrate the robustness and feasibility of our proposed CTWSVM.}
}
@article{JIN2019262,
title = {Correlation-based channel selection and regularized feature optimization for MI-based BCI},
journal = {Neural Networks},
volume = {118},
pages = {262-270},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301960},
author = {Jing Jin and Yangyang Miao and Ian Daly and Cili Zuo and Dewen Hu and Andrzej Cichocki},
keywords = {Brain–computer interface (BCI), Electroencephalogram (EEG), Motor imagery (MI), Common spatial pattern (CSP), Channel selection, Support vector machine (SVM)},
abstract = {Multi-channel EEG data are usually necessary for spatial pattern identification in motor imagery (MI)-based brain computer interfaces (BCIs). To some extent, signals from some channels containing redundant information and noise may degrade BCI performance. We assume that the channels related to MI should contain common information when participants are executing the MI tasks. Based on this hypothesis, a correlation-based channel selection (CCS) method is proposed to select the channels that contained more correlated information in this study. The aim is to improve the classification performance of MI-based BCIs. Furthermore, a novel regularized common spatial pattern (RCSP) method is used to extract effective features. Finally, a support vector machine (SVM) classifier with the Radial Basis Function (RBF) kernel is trained to accurately identify the MI tasks. An experimental study is implemented on three public EEG datasets (BCI competition IV dataset 1, BCI competition III dataset IVa and BCI competition III dataset IIIa) to validate the effectiveness of the proposed methods. The results show that the CCS algorithm obtained superior classification accuracy (78% versus 56.4% for dataset1, 86.6% versus 76.5% for dataset 2 and 91.3% versus 85.1% for dataset 3) compared to the algorithm using all channels (AC), when CSP is used to extract the features. Furthermore, RCSP could further improve the classification accuracy (81.6% for dataset1, 87.4% for dataset2 and 91.9% for dataset 3), when CCS is used to select the channels.}
}
@article{WANG2019201,
title = {Robust auto-weighted projective low-rank and sparse recovery for visual representation},
journal = {Neural Networks},
volume = {117},
pages = {201-215},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930139X},
author = {Lei Wang and Bangjun Wang and Zhao Zhang and Qiaolin Ye and Liyong Fu and Guangcan Liu and Meng Wang},
keywords = {Auto-weighted low-rank and sparse recovery, Robust representation, Feature extraction, Classification},
abstract = {Most existing low-rank and sparse representation models cannot preserve the local manifold structures of samples adaptively, or separate the locality preservation from the coding process, which may result in the decreased performance. In this paper, we propose an inductive Robust Auto-weighted Low-Rank and Sparse Representation (RALSR) framework by joint feature embedding for the salient feature extraction of high-dimensional data. Technically, the model of our RALSR seamlessly integrates the joint low-rank and sparse recovery with robust salient feature extraction. Specifically, RALSR integrates the adaptive locality preserving weighting, joint low-rank/sparse representation and the robustness-promoting representation into a unified model. For accurate similarity measure, RALSR computes the adaptive weights by minimizing the joint reconstruction errors over the recovered clean data and salient features simultaneously, where L1-norm is also applied to ensure the sparse properties of learnt weights. The joint minimization can also potentially enable the weight matrix to have the power to remove noise and unfavorable features by reconstruction adaptively. The underlying projection is encoded by a joint low-rank and sparse regularization, which can ensure it to be powerful for salient feature extraction. Thus, the calculated low-rank sparse features of high-dimensional data would be more accurate for the subsequent classification. Visual and numerical comparison results demonstrate the effectiveness of our RALSR for data representation and classification.}
}
@article{KARIM2019237,
title = {Multivariate LSTM-FCNs for time series classification},
journal = {Neural Networks},
volume = {116},
pages = {237-245},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301200},
author = {Fazle Karim and Somshubra Majumdar and Houshang Darabi and Samuel Harford},
keywords = {Convolutional neural network, Long short term memory, Recurrent neural network, Multivariate time series classification},
abstract = {Over the past decade, multivariate time series classification has received great attention. We propose transforming the existing univariate time series classification models, the Long Short Term Memory Fully Convolutional Network (LSTM-FCN) and Attention LSTM-FCN (ALSTM-FCN), into a multivariate time series classification model by augmenting the fully convolutional block with a squeeze-and-excitation block to further improve accuracy. Our proposed models outperform most state-of-the-art models while requiring minimum preprocessing. The proposed models work efficiently on various complex multivariate time series classification tasks such as activity recognition or action recognition. Furthermore, the proposed models are highly efficient at test time and small enough to deploy on memory constrained systems.}
}
@article{WANG2019110,
title = {The place cell activity is information-efficient constrained by energy},
journal = {Neural Networks},
volume = {116},
pages = {110-118},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300991},
author = {Yihong Wang and Xuying Xu and Rubin Wang},
keywords = {Place cell, Place field, Spatial information, Constrained optimization of functional},
abstract = {Spatial representation is a crucial function of animal’s brain. However, there is still no uniform explanation of how the spatial code is formed in different dimensional spaces to date. The main reason why place cell exhibits unique activity pattern is that the animal needs to retrieve and process spatial information. In this paper, we constructed a constrained optimization model based on information theory to explain the place field formation across species in different dimensional spaces. We proposed the following question that, using only limited amount of neural energy, how to organize the spiking locations (place field) in the available environment to obtain the most efficient spatial information representation? We solved this conditional functional extremum problem by variational techniques. The results showed that on the condition of limited neural energy, the place field will comply with a Gaussian-form distribution automatically to convey the largest amount information per spike. We also found that the animal’s natural habitat property and locomotion experience statistics affected the symmetry of spatial representation in different dimensions. These findings not only reconcile the argument of whether the spatial codes of place cell are isotropic, but also provide an explanation of place field formation by an information-theoretic approach. Furtherly, this research revealed the energy economical and information efficient properties underlie the spatial representation system of the brain.}
}
@article{SYEDALI201928,
title = {Improved result on state estimation for complex dynamical networks with time varying delays and stochastic sampling via sampled-data control},
journal = {Neural Networks},
volume = {114},
pages = {28-37},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300474},
author = {M. {Syed Ali} and M. Usha and Zeynep Orman and Sabri Arik},
keywords = {Complex dynamical networks, Stochastic sampling, State estimation, Kronecker product, Sampled-data control, Linear matrix inequality},
abstract = {This paper investigates state estimation for complex dynamical networks (CDNs) with time-varying delays by using sampled-data control. For the simplicity of technical development, only two different sampling periods are considered whose occurrence probabilities are given constants and satisfy Bernoulli distribution, which can be further extended to the case with multiple stochastic sampling periods. By applying an input-delay approach, the probabilistic sampling state estimator is transformed into a continuous time-delay system with stochastic parameters in the system matrices, where the purpose is to design a state estimator to estimate the network states through available output measurements. By constructing an appropriate Lyapunov–Krasovskii functional (LKF) containing triple and fourth integral terms and applying Wirtinger-based single and double integral inequality, Jenson integral inequality technique, delay-dependent stability conditions are established. The obtained conditions can be readily solved by using the LMI tool box in MATLAB. Finally, a numerical example is provided to demonstrate the validity of the proposed scheme.}
}
@article{OGNIBENE2019269,
title = {Addiction beyond pharmacological effects: The role of environment complexity and bounded rationality},
journal = {Neural Networks},
volume = {116},
pages = {269-278},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301285},
author = {Dimitri Ognibene and Vincenzo G. Fiore and Xiaosi Gu},
keywords = {Addiction, Reinforcement learning, Computational psychiatry, Gambling, Internet gaming, Bounded rationality, Exploration–exploitation},
abstract = {Several decision-making vulnerabilities have been identified as underlying causes for addictive behaviours, or the repeated execution of stereotyped actions despite their adverse consequences. These vulnerabilities are mostly associated with brain alterations caused by the consumption of substances of abuse. However, addiction can also happen in the absence of a pharmacological component, such as seen in pathological gambling and videogaming. We use a new reinforcement learning model to highlight a previously neglected vulnerability that we suggest interacts with those already identified, whilst playing a prominent role in non-pharmacological forms of addiction. Specifically, we show that a dual-learning system (i.e. combining model-based and model-free) can be vulnerable to highly rewarding, but suboptimal actions, that are followed by a complex ramification of stochastic adverse effects. This phenomenon is caused by the overload of the capabilities of an agent, as time and cognitive resources required for exploration, deliberation, situation recognition, and habit formation, all increase as a function of the depth and richness of detail of an environment. Furthermore, the cognitive overload can be aggravated due to alterations (e.g. caused by stress) in the bounded rationality, i.e. the limited amount of resources available for the model-based component, in turn increasing the agent’s chances to develop or maintain addictive behaviours. Our study demonstrates that, independent of drug consumption, addictive behaviours can arise in the interaction between the environmental complexity and the biologically finite resources available to explore and represent it.}
}
@article{ZHAO2019166,
title = {Flexible non-greedy discriminant subspace feature extraction},
journal = {Neural Networks},
volume = {116},
pages = {166-177},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301042},
author = {Henghao Zhao and Liyong Fu and Zhigang Gao and Qiaolin Ye and Zhangjing Yang and Xubing Yang},
keywords = {L-norm-based non-greedy discriminant analysis, L-norm inter-class dispersion, Intra-class dispersion, Robust distance measurement},
abstract = {Recently, L1-norm-based non-greedy linear discriminant analysis (NLDA-L1) for feature extraction has been shown to be effective for dimensionality reduction, which obtains projection vectors by a non-greedy algorithm. However, it usually acquires unsatisfactory performances due to the utilization of L1-norm distance measurement. Therefore, in this brief paper, we propose a flexible non-greedy discriminant subspace feature extraction method, which is an extension of NLDA-L1 by maximizing the ratio of Lp-norm inter-class dispersion to intra-class dispersion. Besides, we put forward a powerful iterative algorithm to solve the resulted objective function and also conduct theoretical analysis on the algorithm. Finally, experimental results on image databases show the effectiveness of our method}
}
@article{DORNAIKA201991,
title = {Joint sparse graph and flexible embedding for graph-based semi-supervised learning},
journal = {Neural Networks},
volume = {114},
pages = {91-95},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300759},
author = {F. Dornaika and Y. {El Traboulsi}},
keywords = {Graph-based embedding, Semi-supervised learning, Non-linear projection, Graph construction, Inductive model, Discriminant embedding},
abstract = {This letter introduces a framework for graph-based semi-supervised learning by estimating a flexible non-linear projection and its linear regression model. Unlike existing works, the proposed framework jointly estimates the graph structure, the non-linear projection, and the linear regression model. By adopting this joint estimation an overall optimality can be reached. A series of experiments are conducted on five image datasets in order to compare the proposed method with some state-of-art semi-supervised methods. This evaluation demonstrates the effectiveness of the proposed embedding method. These experiments show the superiority of the proposed framework over the joint estimation of the graph and soft labels.}
}
@article{2021I,
title = {Current Events},
journal = {Neural Networks},
volume = {143},
pages = {I},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00370-1},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021003701}
}
@article{XU201911,
title = {An optimal time interval of input spikes involved in synaptic adjustment of spike sequence learning},
journal = {Neural Networks},
volume = {116},
pages = {11-24},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930098X},
author = {Yan Xu and Jing Yang and Xiaoqin Zeng},
keywords = {Spiking neurons, Spiking neural networks, Spike sequence learning, Time interval of input spikes, Synaptic adjustment},
abstract = {The supervised learning methods for spiking neurons based on temporal encoding are important foundation for the development of spiking neural networks. During the learning process, the synaptic weights of a spiking neuron are adjusted to make the neuron emit a specific spike train. Because various learning methods use the information of input spikes to calculate the adjustment of synaptic weights, how many input spikes participated in the calculation is a critical factor that can influence learning performance. This paper chooses an important category of learning methods as the research object to study the factor. The input spikes participated in weight adjustment are contained in a time interval. An optimal time interval that contains the most appropriate number of input spikes is proposed based on the characteristic of the category of learning methods. The length of the optimal time interval is determined by comprehensive consideration of desired and actual output spikes. The results of a lot of experiments show that the optimal time interval can obtain the highest learning performance under various experimental settings. If other time intervals are longer than the optimal time interval, an overlapping problem of input spikes will occur and the learning performance will decline a lot. The learning accuracy of the optimal time interval can be about 55% higher than the learning accuracy of an other longer time interval. If other time intervals are shorter than the optimal time interval, the input spikes contained in them will be insufficient to adjust synaptic weights and the learning performance will also decline. The learning accuracy of the optimal time interval can be about 8% higher than the learning accuracy of an other shorter time interval. In addition, the optimal time interval can also improve the generalization ability and pattern storage capability of the category of learning methods.}
}
@article{WANG20191,
title = {Approximate neural optimal control with reinforcement learning for a torsional pendulum device},
journal = {Neural Networks},
volume = {117},
pages = {1-7},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.026},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301327},
author = {Ding Wang and Junfei Qiao},
keywords = {Adaptive critic, Neural optimal control, Nonaffine nonlinearity, Reinforcement learning, Torsional pendulum},
abstract = {A torsional pendulum device containing hyperbolic tangent input nonlinearities can be formulated as a nonaffine system. Unlike basic affine systems, the optimal feedback control of complex nonaffine plants is difficult but quite important. In this paper, the approximate optimal control design of continuous-time nonaffine nonlinear systems is investigated with the help of reinforcement learning. For addressing the learning algorithm conveniently, an effective pre-compensation technique is adopted to perform proper system transformation. Then, the integral policy iteration strategy is incorporated to relieve the demand of system dynamics. Moreover, the actor–critic structure is implemented by virtue of neural network approximators. Finally, the experimental verification for the proposed torsional pendulum plant is conducted after a learning process of 20 iterations and the stability performance with basic robustness guarantee can be observed during two case studies.}
}
@article{MI20191,
title = {Principal Component Analysis based on Nuclear norm Minimization},
journal = {Neural Networks},
volume = {118},
pages = {1-16},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301601},
author = {Jian-Xun Mi and Ya-Nan Zhang and Zhihui Lai and Weisheng Li and Lifang Zhou and Fujin Zhong},
keywords = {Principal component analysis (PCA), Nuclear norm, Robustness, Optimal mean, Low-dimensional representation},
abstract = {Principal component analysis (PCA) is a widely used tool for dimensionality reduction and feature extraction in the field of computer vision. Traditional PCA is sensitive to outliers which are common in empirical applications. Therefore, in recent years, massive efforts have been made to improve the robustness of PCA. However, many emerging PCA variants developed in the direction have some weaknesses. First, few of them pay attention to the 2D structure of error matrix. Second, to estimate data mean from sample set with outliers by averaging is usually biased. Third, if some elements of a sample are disturbed, to extract principal components (PCs) by directly projecting data with transformation matrix causes incorrect mapping of sample to its genuine location in low-dimensional feature subspace. To alleviate these problems, we present a novel robust method, called nuclear norm-based on PCA (N-PCA) to take full advantage of the structure information of error image. Meanwhile, it is developed under a novel unified framework of PCA to remedy the bias of computing data mean and the low-dimensional representation of a sample both of which are treated as unknown variables in a single model together with projection matrix. To solve N-PCA, we propose an iterative algorithm, which has a closed-form solution in each iteration. Experimental results on several open databases demonstrate the effectiveness of the proposed method.}
}
@article{BI2019119,
title = {Early Alzheimer’s disease diagnosis based on EEG spectral images using deep learning},
journal = {Neural Networks},
volume = {114},
pages = {119-135},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300486},
author = {Xiaojun Bi and Haibo Wang},
keywords = {Early diagnosis of AD, Multi-task learning, Deep learning, Deep Boltzmann Machine},
abstract = {Early diagnosis of Alzheimer’s disease (AD) is a proceeding hot issue along with a sharp upward trend in the incidence rate. Recently, early diagnosis of AD employing Electroencephalogram (EEG) as a specific hallmark has been an increasingly significant hot topic area. In consideration of the limited size of available EEG spectral images, how to extract more abstract features for better generalization still remains tremendously troubling. In this paper, we demonstrate that it can be settled well with multi-task learning strategy based on discriminative convolutional high-order Boltzmann Machine with hybrid feature maps. First, differently from our original model — Contractive Slab and Spike Convolutional Deep Boltzmann Machine (CssCDBM), we directly conduct EEG spectral image classification via inducing label layer, resulting in a discriminative version of CssCDBM, referred to as DCssCDBM. This demonstrates DCssCDBM can be extended well into the classification model instead of feature extractor alone previously. Then, the most important approach innovation is that we train our DCssCDBM with multi-task learning framework via EEG spectral images based Identification and verification tasks for overfitting reduction for the first time, which could increase the inter-subject variations and reduce the intra-subject variations respectively, both of which are essential to early diagnosis of AD. The proposed method shows the better ability of high-level representations extraction and demonstrates the advanced results over several state-of-the-art methods.}
}
@article{TANG2019163,
title = {Unsupervised feature selection via latent representation learning and manifold regularization},
journal = {Neural Networks},
volume = {117},
pages = {163-178},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301212},
author = {Chang Tang and Meiru Bian and Xinwang Liu and Miaomiao Li and Hua Zhou and Pichao Wang and Hailin Yin},
keywords = {Unsupervised feature selection, Latent representation learning, Manifold regularization, Non-negative matrix factorization, Local structure preservation},
abstract = {With the rapid development of multimedia technology, massive unlabelled data with high dimensionality need to be processed. As a means of dimensionality reduction, unsupervised feature selection has been widely recognized as an important and challenging pre-step for many machine learning and data mining tasks. Traditional unsupervised feature selection algorithms usually assume that the data instances are identically distributed and there is no dependency between them. However, the data instances are not only associated with high dimensional features but also inherently interconnected with each other. Furthermore, the inevitable noises mixed in data could degenerate the performances of previous methods which perform feature selection in original data space. Without label information, the connection information between data instances can be exploited and could help select relevant features. In this work, we propose a robust unsupervised feature selection method which embeds the latent representation learning into feature selection. Instead of measuring the feature importances in original data space, the feature selection is carried out in the learned latent representation space which is more robust to noises. The latent representation is modelled by non-negative matrix factorization of the affinity matrix which explicitly reflects the relationships of data instances. Meanwhile, the local manifold structure of original data space is preserved by a graph based manifold regularization term in the transformed feature space. An efficient alternating algorithm is developed to optimize the proposed model. Experimental results on eight benchmark datasets demonstrate the effectiveness of the proposed method.}
}
@article{DETORAKIS20191,
title = {Contrastive Hebbian learning with random feedback weights},
journal = {Neural Networks},
volume = {114},
pages = {1-14},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930019X},
author = {Georgios Detorakis and Travis Bartley and Emre Neftci},
keywords = {Random contrastive Hebbian learning, Supervised learning, Unsupervised learning, Random feedback},
abstract = {Neural networks are commonly trained to make predictions through learning algorithms. Contrastive Hebbian learning, which is a powerful rule inspired by gradient backpropagation, is based on Hebb’s rule and the contrastive divergence algorithm. It operates in two phases, the free phase, where the data are fed to the network, and a clamped phase, where the target signals are clamped to the output layer of the network and the feedback signals are transformed through the transpose synaptic weight matrices. This implies symmetries at the synaptic level, for which there is no evidence in the brain so far. In this work, we propose a new variant of the algorithm, called random contrastive Hebbian learning, which does not rely on any synaptic weights symmetries. Instead, it uses random matrices to transform the feedback signals during the clamped phase, and the neural dynamics are described by first order non-linear differential equations. The algorithm is experimentally verified by solving a Boolean logic task, classification tasks (handwritten digits and letters), and an autoencoding task. This article also shows how the parameters affect learning, especially the random matrices. We use the pseudospectra analysis to investigate further how random matrices impact the learning process. Finally, we discuss the biological plausibility of the proposed algorithm, and how it can give rise to better computational models for learning.}
}
@article{KIM20191,
title = {Multi-scale gradual integration CNN for false positive reduction in pulmonary nodule detection},
journal = {Neural Networks},
volume = {115},
pages = {1-10},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300760},
author = {Bum-Chae Kim and Jee Seok Yoon and Jun-Sik Choi and Heung-Il Suk},
keywords = {Multi-scale convolutional neural network, Multi-stream feature integration, False positive reduction, Pulmonary nodule detection, Lung cancer screening},
abstract = {Lung cancer is a global and dangerous disease, and its early detection is crucial for reducing the risks of mortality. In this regard, it has been of great interest in developing a computer-aided system for pulmonary nodules detection as early as possible on thoracic CT scans. In general, a nodule detection system involves two steps: (i) candidate nodule detection at a high sensitivity, which captures many false positives and (ii) false positive reduction from candidates. However, due to the high variation of nodule morphological characteristics and the possibility of mistaking them for neighboring organs, candidate nodule detection remains a challenge. In this study, we propose a novel Multi-scale Gradual Integration Convolutional Neural Network (MGI-CNN), designed with three main strategies: (1) to use multi-scale inputs with different levels of contextual information, (2) to use abstract information inherent in different input scales with gradual integration, and (3) to learn multi-stream feature integration in an end-to-end manner. To verify the efficacy of the proposed network, we conducted exhaustive experiments on the LUNA16 challenge datasets by comparing the performance of the proposed method with state-of-the-art methods in the literature. On two candidate subsets of the LUNA16 dataset, i.e., V1 and V2, our method achieved an average CPM of 0.908 (V1) and 0.942 (V2), outperforming comparable methods by a large margin. Our MGI-CNN is implemented in Python using TensorFlow and the source code is available from https://github.com/ku-milab/MGICNN.}
}
@article{SAMLI2019198,
title = {Some generalized global stability criteria for delayed Cohen–Grossberg neural networks of neutral-type},
journal = {Neural Networks},
volume = {116},
pages = {198-207},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.023},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301297},
author = {Ruya Samli and Sibel Senan and Eylem Yucel and Zeynep Orman},
keywords = {Lyapunov stability analysis, Neutral systems, Delayed neural networks, Matrix theory},
abstract = {This paper carries out a theoretical investigation into the stability problem for the class of neutral-type Cohen–Grossberg neural networks with discrete time delays in states and discrete neutral delays in time derivative of states. By employing a more general type of suitable Lyapunov functional, a set of new generalized sufficient criteria are derived for the global asymptotic stability of delayed neural networks of neutral-type. The proposed stability criteria are independently of the values of the time delays and neutral delays, and they completely rely on some algebraic mathematical relationships involving the values of the elements of the interconnection matrices and the other network parameters. Therefore, it is easy to verify the validity of the obtained results by simply using some algebraic equations representing the stability conditions. A detailed comparison between our proposed results and recently reported corresponding stability results is made, proving that the results given in this paper generalize previously published stability results. A constructive numerical example is also given to demonstrate the applicability of the results of the paper.}
}
@article{BASSANI2019249,
title = {A neural network architecture for learning word–referent associations in multiple contexts},
journal = {Neural Networks},
volume = {117},
pages = {249-267},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301571},
author = {Hansenclever F. Bassani and Aluizio F.R. Araujo},
keywords = {Self-organizing maps, Cross-situational word learning, Context, Learning representations, Neurocomputational model},
abstract = {This article proposes a biologically inspired neurocomputational architecture which learns associations between words and referents in different contexts, considering evidence collected from the literature of Psycholinguistics and Neurolinguistics. The multi-layered architecture takes as input raw images of objects (referents) and streams of word’s phonemes (labels), builds an adequate representation, recognizes the current context, and associates label with referents incrementally, by employing a Self-Organizing Map which creates new association nodes (prototypes) as required, adjusts the existing prototypes to better represent the input stimuli and removes prototypes that become obsolete/unused. The model takes into account the current context to retrieve the correct meaning of words with multiple meanings. Simulations show that the model can reach up to 78% of word–referent association accuracy in ambiguous situations and approximates well the learning rates of humans as reported by three different authors in five Cross-Situational Word Learning experiments, also displaying similar learning patterns in the different learning conditions.}
}
@article{LIU201974,
title = {Identification of piecewise linear dynamical systems using physically-interpretable neural-fuzzy networks: Methods and applications to origami structures},
journal = {Neural Networks},
volume = {116},
pages = {74-87},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301054},
author = {Zuolin Liu and Hongbin Fang and Jian Xu},
keywords = {Self-locking origami structure, Origami dynamics, Piecewise linear stiffness, Fuzzy neural networks, Parameter identification, Model determination},
abstract = {Self-locking origami structures are characterized by their piecewise linear constitutive relations between force and deformation, which, in practice, are always completely opaque and unmeasurable: the number of piecewise segments, the positions of non-smooth points, and the linear parameters of each segment are unknown a priori. However, acquiring this information is of fundamental importance for understanding the origami structure’s dynamic folding process and predicting its dynamic behaviors. This, therefore, arouses our interest in adopting a dynamical identification process to determine the model and to estimate the parameters. In this research, based on the piecewise linear assumption, a physically-interpretable neural-fuzzy network is built to correlate the measured input and output data. Unlike the conventional approaches, the constructed neural network possesses specific physical meaning of its components: the number of neurons relates to the number of piecewise segments, the coefficients of the local linear models relate to the parameters of the constitutive relations, and the validity functions relate to the positions of non-smooth points. By addressing several examples with different backgrounds, the network’s underlying data training methods are illustrated, including the local linear optimization for linear parameters, nested optimization for nonlinear partitions, and Local Linear Model Tree optimization for model selection. Noting that the tackled origami problem holds strong universality in terms of the unknown piecewise characteristics, the proposed approach would thus provide an effective, generic, and physically significant means for handling piecewise linear dynamical systems and meanwhile bring fresh vitality to the artificial neural network research.}
}
@article{XIE20191,
title = {On-line prediction of ferrous ion concentration in goethite process based on self-adjusting structure RBF neural network},
journal = {Neural Networks},
volume = {116},
pages = {1-10},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300802},
author = {Yongfang Xie and Jinjing Yu and Shiwen Xie and Tingwen Huang and Weihua Gui},
keywords = {Radial basis function (RBF), Self-adjusting structure, Gradient-based algorithm, On-line prediction, Goethite process},
abstract = {Outlet ferrous ion concentration is an essential indicator to manipulate the goethite process in the zinc hydrometallurgy plant. However, it cannot be measured on-line, which leads to the delay of this feedback information. In this study, a self-adjusting structure radial basis function neural network (SAS-RBFNN) is developed to predict the outlet ferrous ion concentration on-line. First, a supervised cluster algorithm is proposed to initialize the RBFNN. Then, the network structure is adjusted by the developed self-adjusting structure mechanism. This mechanism can merge or divide the hidden neurons according to the distance of the clusters to achieve the adaptability of the RBFNN. Finally, the connection weights are determined by the gradient-based algorithm. The convergence of the SAS-RBFNN is analyzed by the Lyapunov criterion. A simulation for a benchmark problem shows the effectiveness of the proposed network. The SAS-RBFNN is then applied to predict the outlet ferrous ion concentration in the goethite process. The results demonstrate that this network can provide a more accurate prediction than the mathematical model, even under the fluctuating production condition.}
}
@article{RIOS2019235,
title = {Compositionally-warped Gaussian processes},
journal = {Neural Networks},
volume = {118},
pages = {235-246},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301856},
author = {Gonzalo Rios and Felipe Tobar},
keywords = {Warped Gaussian processes, Gaussian process, Non-Gaussian models, Function compositions, Neural networks},
abstract = {The Gaussian process (GP) is a nonparametric prior distribution over functions indexed by time, space, or other high-dimensional index set. The GP is a flexible model yet its limitation is given by its very nature: it can only model Gaussian marginal distributions. To model non-Gaussian data, a GP can be warped by a nonlinear transformation (or warping) as performed by warped GPs (WGPs) and more computationally-demanding alternatives such as Bayesian WGPs and deep GPs. However, the WGP requires a numerical approximation of the inverse warping for prediction, which increases the computational complexity in practice. To sidestep this issue, we construct a novel class of warpings consisting of compositions of multiple elementary functions, for which the inverse is known explicitly. We then propose the compositionally-warped GP (CWGP), a non-Gaussian generative model whose expressiveness follows from its deep compositional architecture, and its computational efficiency is guaranteed by the analytical inverse warping. Experimental validation using synthetic and real-world datasets confirms that the proposed CWGP is robust to the choice of warpings and provides more accurate point predictions, better trained models and shorter computation times than WGP.}
}
@article{BUTZ2019135,
title = {Learning, planning, and control in a monolithic neural event inference architecture},
journal = {Neural Networks},
volume = {117},
pages = {135-144},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301339},
author = {Martin V. Butz and David Bilkey and Dania Humaidan and Alistair Knott and Sebastian Otte},
keywords = {Dynamical systems, Predictive model learning, Model predictive control, Active inference, Cognitive systems, Event cognition},
abstract = {We introduce REPRISE, a REtrospective and PRospective Inference SchEme, which learns temporal event-predictive models of dynamical systems. REPRISE infers the unobservable contextual event state and accompanying temporal predictive models that best explain the recently encountered sensorimotor experiences retrospectively. Meanwhile, it optimizes upcoming motor activities prospectively in a goal-directed manner. Here, REPRISE is implemented by a recurrent neural network (RNN), which learns temporal forward models of the sensorimotor contingencies generated by different simulated dynamic vehicles. The RNN is augmented with contextual neurons, which enable the encoding of distinct, but related, sensorimotor dynamics as compact event codes. We show that REPRISE concurrently learns to separate and approximate the encountered sensorimotor dynamics: it analyzes sensorimotor error signals adapting both internal contextual neural activities and connection weight values. Moreover, we show that REPRISE can exploit the learned model to induce goal-directed, model-predictive control, that is, approximate active inference: Given a goal state, the system imagines a motor command sequence optimizing it with the prospective objective to minimize the distance to the goal. The RNN activities thus continuously imagine the upcoming future and reflect on the recent past, optimizing the predictive model, the hidden neural state activities, and the upcoming motor activities. As a result, event-predictive neural encodings develop, which allow the invocation of highly effective and adaptive goal-directed sensorimotor control.}
}
@article{YANG2019240,
title = {Hierarchical human-like strategy for aspect-level sentiment classification with sentiment linguistic knowledge and reinforcement learning},
journal = {Neural Networks},
volume = {117},
pages = {240-248},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.021},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301613},
author = {Min Yang and Qingnan Jiang and Ying Shen and Qingyao Wu and Zhou Zhao and Wei Zhou},
keywords = {Aspect-level sentiment classification, Sentiment linguistic knowledge, Human reading cognition, Reinforcement learning},
abstract = {Aspect-level sentiment analysis is a crucial problem in fine-grained sentiment analysis, which aims to automatically predict the sentiment polarity of the specific aspect in its context. Although remarkable progress has been made by deep learning based methods, aspect-level sentiment classification in real-world remains a challenging task. The human reading cognition is rarely explored in sentiment classification, which however is able to improve the effectiveness of the sentiment classification by considering the process of reading comprehension and logical thinking. Motivated by the process of the human reading cognition that follows a hierarchical routine, we propose a novel Hierarchical Human-like strategy for Aspect-level Sentiment classification (HHAS). The model contains three major components, a sentiment-aware mutual attention module, an aspect-specific knowledge distillation module, and a reinforcement learning based re-reading module, which are consistent with the stages of the human reading cognitive process (i.e., pre-reading, active reading, and post-reading). To measure the effectiveness of HHAS, extensive experiments are conducted on three widely used datasets. Experimental results demonstrate that HHAS achieves impressive results and yields state-of-the-art results on the three datasets.}
}
@article{CHEN2019145,
title = {Closed-loop control of nonlinear neural networks: The estimate of control time and energy cost},
journal = {Neural Networks},
volume = {117},
pages = {145-151},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930156X},
author = {Chongyang Chen and Song Zhu and Yongchang Wei},
keywords = {Closed-loop control, Nonlinear neural networks, Finite-time, Energy cost},
abstract = {This paper concentrates on an estimate of the upper bounds for control time and energy cost of a class of nonlinear neural networks (NNs). By constructing the appropriate closed-loop controller uS and utilizing the inequality technique, sufficient conditions are proposed to guarantee achieving control target in finite time of the considered systems. Then, the estimate of the upper bounds for the control energy cost of the designed controller uS is proposed. Our results provide a new controller which can ensure the realization of finite time control and energy consumption control for a class of nonlinear NNs. Meanwhile, the obtained results contribute to qualitative analysis of some nonlinear systems. Finally, numerical examples are presented to demonstrate the effectiveness of our theoretical results.}
}
@article{2021II,
title = {INN/ENNS/JNNS - Membership Applic. Form},
journal = {Neural Networks},
volume = {143},
pages = {II},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00371-3},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021003713}
}
@article{SACOUTO201938,
title = {Attention Inspired Network: Steep learning curve in an invariant pattern recognition model},
journal = {Neural Networks},
volume = {114},
pages = {38-46},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300401},
author = {Luis Sa-Couto and Andreas Wichert},
keywords = {Hubel Wiesel’s hypothesis, Selective attention, Invariant pattern recognition, Deep learning, Steep learning},
abstract = {Hubel and Wiesel’s study about low areas of the visual cortex (VC) inspired deep models for invariant pattern recognition. In such models, simple and complex layers alternate local feature extraction with subsampling to add invariance to distortion or transformations. However, it was shown that to tolerate large changes between examples of the same category, the subsampling operation has to discard so much information that the model loses the capability to discriminate between categories. So, in practice, small changes are tolerated by these layers and, afterwards, a powerful classifier is introduced to do the rest. By incorporating insights from higher areas of the VC, we add to the already used retinotopic step an object-centered step which increases invariance capabilities without losing so much information. By doing so, we reduce the need for a powerful, data hungry classification layer and, thus, are able to introduce a simple classification mechanism which is based on selective attention. The resulting model is tested with an invariant pattern recognition task in the MNIST and ETL-1 datasets. We verify that the model is able to achieve better accuracies with less training examples. More specifically, on the MNIST test set, the model achieves a 100% accuracy when trained with little more than 10% of the training set.}
}
@article{MEHRKANOON201946,
title = {Deep neural-kernel blocks},
journal = {Neural Networks},
volume = {116},
pages = {46-55},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300929},
author = {Siamak Mehrkanoon},
keywords = {Deep learning, Neural networks, Kernel methods, Pooling layer, Competitive learning, Dimensionality reduction},
abstract = {This paper introduces novel deep architectures using the hybrid neural-kernel core model as the first building block. The proposed models follow a combination of a neural networks based architecture and a kernel based model enriched with pooling layers. In particular, in this context three kernel blocks with average, maxout and convolutional pooling layers are introduced and examined. We start with a simple merging layer which averages the output of the previous representation layers. The maxout layer on the other hand triggers competition among different representations of the input. Thanks to this pooling layer, not only the dimensionality of the output of multi-scale representations is reduced but also multiple sub-networks are formed within the same model. In the same context, the pointwise convolutional layer is also employed with the aim of projecting the multi-scale representations onto a new space. Experimental results show an improvement over the core deep hybrid model as well as kernel based models on several real-life datasets.}
}