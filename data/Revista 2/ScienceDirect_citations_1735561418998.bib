@article{AKBARINIA2023228,
title = {Contrast sensitivity function in deep networks},
journal = {Neural Networks},
volume = {164},
pages = {228-244},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.032},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002186},
author = {Arash Akbarinia and Yaniv Morgenstern and Karl R. Gegenfurtner},
keywords = {CSF, Artificial neural networks, Deep learning, Contrast, Visual perception, Visual features},
abstract = {The contrast sensitivity function (CSF) is a fundamental signature of the visual system that has been measured extensively in several species. It is defined by the visibility threshold for sinusoidal gratings at all spatial frequencies. Here, we investigated the CSF in deep neural networks using the same 2AFC contrast detection paradigm as in human psychophysics. We examined 240 networks pretrained on several tasks. To obtain their corresponding CSFs, we trained a linear classifier on top of the extracted features from frozen pretrained networks. The linear classifier is exclusively trained on a contrast discrimination task with natural images. It has to find which of the two input images has higher contrast. The network’s CSF is measured by detecting which one of two images contains a sinusoidal grating of varying orientation and spatial frequency. Our results demonstrate characteristics of the human CSF are manifested in deep networks both in the luminance channel (a band-limited inverted U-shaped function) and in the chromatic channels (two low-pass functions of similar properties). The exact shape of the networks’ CSF appears to be task-dependent. The human CSF is better captured by networks trained on low-level visual tasks such as image-denoising or autoencoding. However, human-like CSF also emerges in mid- and high-level tasks such as edge detection and object recognition. Our analysis shows that human-like CSF appears in all architectures but at different depths of processing, some at early layers, while others in intermediate and final layers. Overall, these results suggest that (i) deep networks model the human CSF faithfully, making them suitable candidates for applications of image quality and compression, (ii) efficient/purposeful processing of the natural world drives the CSF shape, and (iii) visual representation from all levels of visual hierarchy contribute to the tuning curve of the CSF, in turn implying a function which we intuitively think of as modulated by low-level visual features may arise as a consequence of pooling from a larger set of neurons at all levels of the visual system.}
}
@article{KO2023562,
title = {A spectral graph convolution for signed directed graphs via magnetic Laplacian},
journal = {Neural Networks},
volume = {164},
pages = {562-574},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002502},
author = {Taewook Ko and Yoonhyuk Choi and Chong-Kwon Kim},
keywords = {Graph convolution, Spectral convolution, Magnetic Laplacian, Signed directed graphs, Link sign prediction, Link existence prediction},
abstract = {Signed directed graphs contain both sign and direction information on their edges, providing richer information about real-world phenomena compared to unsigned or undirected graphs. However, analyzing such graphs is more challenging due to their complexity, and the limited availability of existing methods. Consequently, despite their potential uses, signed directed graphs have received less research attention. In this paper, we propose a novel spectral graph convolution model that effectively captures the underlying patterns in signed directed graphs. To this end, we introduce a complex Hermitian adjacency matrix that can represent both sign and direction of edges using complex numbers. We then define a magnetic Laplacian matrix based on the adjacency matrix, which we use to perform spectral convolution. We demonstrate that the magnetic Laplacian matrix is positive semi-definite (PSD), which guarantees its applicability to spectral methods. Compared to traditional Laplacians, the magnetic Laplacian captures additional edge information, which makes it a more informative tool for graph analysis. By leveraging the information of signed directed edges, our method generates embeddings that are more representative of the underlying graph structure. Furthermore, we showed that the proposed method has wide applicability for various graph types and is the most generalized Laplacian form. We evaluate the effectiveness of the proposed model through extensive experiments on several real-world datasets. The results demonstrate that our method outperforms state-of-the-art techniques in signed directed graph embedding.}
}
@article{GUAN2023483,
title = {Collaborative neurodynamic optimization for solving nonlinear equations},
journal = {Neural Networks},
volume = {165},
pages = {483-490},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.054},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002988},
author = {Huimin Guan and Yang Liu and Kit Ian Kou and Jinde Cao and Leszek Rutkowski},
keywords = {Nonlinear equations, Distributed optimization, Nonconvex optimization, Collaborative neurodynamic optimization},
abstract = {A distributed optimization method for solving nonlinear equations with constraints is developed in this paper. The multiple constrained nonlinear equations are converted into an optimization problem and we solve it in a distributed manner. Due to the possible presence of nonconvexity, the converted optimization problem might be a nonconvex optimization problem. To this end, we propose a multi-agent system based on an augmented Lagrangian function and prove that it converges to a locally optimal solution to an optimization problem in the presence of nonconvexity. In addition, a collaborative neurodynamic optimization method is adopted to obtain a globally optimal solution. Three numerical examples are elaborated to illustrate the effectiveness of the main results.}
}
@article{SERRANO2023420,
title = {The Deep Learning Generative Adversarial Random Neural Network in data marketplaces: The digital creative},
journal = {Neural Networks},
volume = {165},
pages = {420-434},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.028},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002745},
author = {Will Serrano},
keywords = {Generative Adversarial Network, Random Neural Network, Data marketplaces},
abstract = {Generative Adversarial Networks (GANs) have been proposed as a method to generate multiple replicas from an original version combining a Discriminator and a Generator. The main applications of GANs have been the casual generation of audio and video content. GANs, as a neural method that generates populations of individuals, have emulated genetic algorithms based on biologically inspired operators such as mutation, crossover and selection. This article presents the Deep Learning Generative Adversarial Random Neural Network (RNN) with the same features and functionality as a GAN. Furthermore, the presented algorithm is proposed for an application, the Digital Creative, that generates tradeable replicas in a Data Marketplace, such as 1D functions or audio, 2D and 3D images and video content. The RNN Generator creates individuals mapped from a latent space while the GAN Discriminator evaluates them based on the true data distribution. The performance of the Deep Learning Generative Adversarial RNN has been assessed against several input vectors with different dimensions, in addition to 1D functions and 2D images. The presented results are successful: the learning objective of the RNN Generator creates tradeable replicas at low error, whereas the RNN Discriminator learning target identifies unfit individuals.}
}
@article{WANG2023264,
title = {Aperiodic switching event-triggered stabilization of continuous memristive neural networks with interval delays},
journal = {Neural Networks},
volume = {164},
pages = {264-274},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.036},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002228},
author = {Yaning Wang and Huan Tuo and Huiping Lyu and Zunshui Cheng and Youming Xin},
keywords = {Aperiodic switching event-triggered control, Memristive neural networks, Stability, Interval delay, Linear matrix inequality},
abstract = {The stabilization problem is studied for memristive neural networks with interval delays under aperiodic switching event-triggered control. Note that, most of delayed memristive neural networks models studied are discontinuous, which are not the real memristive neural networks. First, a real model of memristive neural networks is proposed by continuous differential equations, furthermore, it is simplified to neural networks with interval matrix uncertainties. Secondly, an aperiodic switching event-trigger is given, and the considered system switches between aperiodic sampled-data system and continuous event-triggered system. Thirdly, by constructing a time-dependent piecewise-defined Lyapunov functional, the stability criterion and the feedback gain design are obtained by linear matrix inequalities. Compared with the existing results, the stability criterion is with lower conservatism. Finally, two neurons are taken as examples to ensure the feasibility of the results.}
}
@article{LI2023290,
title = {Stabilization of reaction–diffusion fractional-order memristive neural networks},
journal = {Neural Networks},
volume = {165},
pages = {290-297},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.042},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002861},
author = {Ruoxia Li and Jinde Cao and Ning Li},
keywords = {Memristive, Fractional-order neural networks, Reaction–diffusion, Stabilization},
abstract = {This paper investigates the stabilization control of fractional-order memristive neural networks with reaction–diffusion terms. With regard to the reaction–diffusion model, a novel processing method based on Hardy–Poincarè inequality is introduced, as a result, the diffusion terms are estimated associated with the information of the reaction–diffusion coefficients and the regional feature, which may be beneficial to obtain conditions with less conservatism. Then, based on Kakutani’s fixed point theorem of set-valued maps, new testable algebraic conclusion for ensuring the existence of the system’s equilibrium point is obtained. Subsequently, by means of Lyapunov stability theory, it is concluded that the resulting stabilization error system is global asymptotic/Mittag-Leffler stable with a prescribed controller. Finally, an illustrative example about is provided to show the effectiveness of the established results.}
}
@article{ZHU2023535,
title = {An accelerated end-to-end method for solving routing problems},
journal = {Neural Networks},
volume = {164},
pages = {535-545},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002344},
author = {Tianyu Zhu and Xinli Shi and Xiangping Xu and Jinde Cao},
keywords = {Reinforcement learning, Machine learning, Neural networks, Combinatorial optimization, Routing problems},
abstract = {The application of neural network models to solve combinatorial optimization has recently drawn much attention and shown promising results in dealing with similar problems, like Travelling Salesman Problem. The neural network allows to learn solutions based on given problem instances, using reinforcement learning or supervised learning. In this paper, we present a novel end-to-end method to solve routing problems. In specific, we propose a gated cosine-based attention model (GCAM) to train policies, which accelerates the training process and the convergence of policy. Extensive experiments on different scale of routing problems show that the proposed method can achieve faster convergence of the training process than the state-of-the-art deep learning models while achieving solutions of the same quality.}
}
@article{YAO202367,
title = {Event-triggered control for robust exponential synchronization of inertial memristive neural networks under parameter disturbance},
journal = {Neural Networks},
volume = {164},
pages = {67-80},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002101},
author = {Wei Yao and Chunhua Wang and Yichuang Sun and Shuqing Gong and Hairong Lin},
keywords = {Event-triggered control, Inertial memristive neural networks, Robust exponential synchronization, Parameter disturbance},
abstract = {Synchronization of memristive neural networks (MNNs) by using network control scheme has been widely and deeply studied. However, these researches are usually restricted to traditional continuous-time control methods for synchronization of the first-order MNNs. In this paper, we study the robust exponential synchronization of inertial memristive neural networks (IMNNs) with time-varying delays and parameter disturbance via event-triggered control (ETC) scheme. First, the delayed IMNNs with parameter disturbance are changed into first-order MNNs with parameter disturbance by constructing proper variable substitutions. Next, a kind of state feedback controller is designed to the response IMNN with parameter disturbance. Based on feedback controller, some ETC methods are provided to largely decrease the update times of controller. Then, some sufficient conditions are provided to realize robust exponential synchronization of delayed IMNNs with parameter disturbance via ETC scheme. Moreover, the Zeno behavior will not happen in all ETC conditions shown in this paper. Finally, numerical simulations are given to verify the advantages of the obtained results such as anti-interference performance and good reliability.}
}
@article{MANINO2023344,
title = {Towards global neural network abstractions with locally-exact reconstruction},
journal = {Neural Networks},
volume = {165},
pages = {344-357},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023003039},
author = {Edoardo Manino and Iury Bessa and Lucas C. Cordeiro},
keywords = {Neural networks, Abstract Interpretation, Global abstraction},
abstract = {Neural networks are a powerful class of non-linear functions. However, their black-box nature makes it difficult to explain their behaviour and certify their safety. Abstraction techniques address this challenge by transforming the neural network into a simpler, over-approximated function. Unfortunately, existing abstraction techniques are slack, which limits their applicability to small local regions of the input domain. In this paper, we propose Global Interval Neural Network Abstractions with Center-Exact Reconstruction (GINNACER). Our novel abstraction technique produces sound over-approximation bounds over the whole input domain while guaranteeing exact reconstructions for any given local input. Our experiments show that GINNACER is several orders of magnitude tighter than state-of-the-art global abstraction techniques, while being competitive with local ones.}
}
@article{KUANG2023119,
title = {MSCDA: Multi-level semantic-guided contrast improves unsupervised domain adaptation for breast MRI segmentation in small datasets},
journal = {Neural Networks},
volume = {165},
pages = {119-134},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002587},
author = {Sheng Kuang and Henry C. Woodruff and Renee Granzier and Thiemo J.A. {van Nijnatten} and Marc B.I. Lobbes and Marjolein L. Smidt and Philippe Lambin and Siamak Mehrkanoon},
keywords = {Breast segmentation, Unsupervised domain adaptation, Contrastive learning},
abstract = {Deep learning (DL) applied to breast tissue segmentation in magnetic resonance imaging (MRI) has received increased attention in the last decade, however, the domain shift which arises from different vendors, acquisition protocols, and biological heterogeneity, remains an important but challenging obstacle on the path towards clinical implementation. In this paper, we propose a novel Multi-level Semantic-guided Contrastive Domain Adaptation (MSCDA) framework to address this issue in an unsupervised manner. Our approach incorporates self-training with contrastive learning to align feature representations between domains. In particular, we extend the contrastive loss by incorporating pixel-to-pixel, pixel-to-centroid, and centroid-to-centroid contrasts to better exploit the underlying semantic information of the image at different levels. To resolve the data imbalance problem, we utilize a category-wise cross-domain sampling strategy to sample anchors from target images and build a hybrid memory bank to store samples from source images. We have validated MSCDA with a challenging task of cross-domain breast MRI segmentation between datasets of healthy volunteers and invasive breast cancer patients. Extensive experiments show that MSCDA effectively improves the model’s feature alignment capabilities between domains, outperforming state-of-the-art methods. Furthermore, the framework is shown to be label-efficient, achieving good performance with a smaller source dataset. The code is publicly available at https://github.com/ShengKuangCN/MSCDA.}
}
@article{YANG2023135,
title = {Attention guided learnable time-domain filterbanks for speech depression detection},
journal = {Neural Networks},
volume = {165},
pages = {135-149},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.041},
url = {https://www.sciencedirect.com/science/article/pii/S089360802300285X},
author = {Wenju Yang and Jiankang Liu and Peng Cao and Rongxin Zhu and Yang Wang and Jian K. Liu and Fei Wang and Xizhe Zhang},
keywords = {Speech depression detection, Filterbanks, Time–frequency analysis, Interpretability, Affective computing},
abstract = {Depression, as a global mental health problem, is lacking effective screening methods that can help with early detection and treatment. This paper aims to facilitate the large-scale screening of depression by focusing on the speech depression detection (SDD) task. Currently, direct modeling on the raw signal yields a large number of parameters, and the existing deep learning-based SDD models mainly use the fixed Mel-scale spectral features as input. However, these features are not designed for depression detection, and the manual settings limit the exploration of fine-grained feature representations. In this paper, we learn the effective representations of the raw signals from an interpretable perspective. Specifically, we present a joint learning framework with attention-guided learnable time-domain filterbanks for depression classification (DALF), which collaborates with the depression filterbanks features learning (DFBL) module and multi-scale spectral attention learning (MSSA) module. DFBL is capable of producing biologically meaningful acoustic features by employing learnable time-domain filters, and MSSA is used to guide the learnable filters to better retain the useful frequency sub-bands. We collect a new dataset, the Neutral Reading-based Audio Corpus (NRAC), to facilitate the research in depression analysis, and we evaluate the performance of DALF on the NRAC and the public DAIC-woz datasets. The experimental results demonstrate that our method outperforms the state-of-the-art SDD methods with an F1 of 78.4% on the DAIC-woz dataset. In particular, DALF achieves F1 scores of 87.3% and 81.7% on two parts of the NRAC dataset. By analyzing the filter coefficients, we find that the most important frequency range identified by our method is 600–700Hz, which corresponds to the Mandarin vowels /e/ and /eˆ/ and can be considered as an effective biomarker for the SDD task. Taken together, our DALF model provides a promising approach to depression detection.}
}
@article{LE2023175,
title = {Reinforced mixture learning},
journal = {Neural Networks},
volume = {165},
pages = {175-184},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002629},
author = {Yuan Le and Fan Zhou and Yang Bai},
keywords = {Mixture learning, Reinforcement learning, Policy gradient, Expectation–maximization, Spectral embedding},
abstract = {In this article, we formulate the standard mixture learning problem as a Markov Decision Process (MDP). We theoretically show that the objective value of the MDP is equivalent to the log-likelihood of the observed data with a slightly different parameter space constrained by the policy. Different from some classic mixture learning methods such as Expectation–Maximization (EM) algorithm, the proposed reinforced algorithm requires no distribution assumptions and can handle the non-convex clustered data by constructing a model-free reward to evaluate the mixture assignment based on the spectral graph theory and Linear Discriminant Analysis (LDA). Extensive experiments on both synthetic and real examples demonstrate that the proposed method is comparable with the EM algorithm when the Gaussian mixture assumption is satisfied, and significantly outperforms it and other clustering methods in most scenarios when the model is misspecified. A Python implementation of our proposed method is available at https://github.com/leyuanheart/Reinforced-Mixture-Learning.}
}
@article{LI2023677,
title = {Predictive hierarchical reinforcement learning for path-efficient mapless navigation with moving target},
journal = {Neural Networks},
volume = {165},
pages = {677-688},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S089360802300309X},
author = {Hanxiao Li and Biao Luo and Wei Song and Chunhua Yang},
keywords = {Reinforcement learning, Deep learning, Navigation, Moving target},
abstract = {Deep reinforcement learning (DRL) has been proven as a powerful approach for robot navigation over the past few years. DRL-based navigation does not require the pre-construction of a map, instead, high-performance navigation skills can be learned from trial-and-error experiences. However, recent DRL-based approaches mostly focus on a fixed navigation target. It is noted that when navigating to a moving target without maps, the performance of the standard RL structure drops dramatically on both the success rate and path efficiency. To address the mapless navigation problem with moving target, the predictive hierarchical DRL (pH-DRL) framework is proposed by integrating the long-term trajectory prediction to provide a cost-effective solution. In the proposed framework, the lower-level policy of the RL agent learns robot control actions to a specified goal, and the higher-level policy learns to make long-range planning of shorter navigation routes by sufficiently exploiting the predicted trajectories. By means of making decisions over two level of policies, the pH-DRL framework is robust to the unavoidable errors in long-term predictions. With the application of deep deterministic policy gradient (DDPG) for policy optimization, the pH-DDPG algorithm is developed based on the pH-DRL structure. Finally, through comparative experiments on the Gazebo simulator with several variants of the DDPG algorithm, the results demonstrate that the pH-DDPG outperforms other algorithms and achieves a high success rate and efficiency even though the target moves fast and randomly.}
}
@article{WANG2023213,
title = {Reachable set estimation and stochastic sampled-data exponential synchronization of Markovian jump neural networks with time-varying delays},
journal = {Neural Networks},
volume = {165},
pages = {213-227},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.034},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002691},
author = {Linqi Wang and Jianwei Xia and Ju H. Park and Guoliang Chen and Xiangpeng Xie},
keywords = {Markovian jump neural networks, Stochastic sampled-data control, Reachable set estimation, Mean square exponential stability},
abstract = {In this paper, the stochastic sampled-data exponential synchronization problem for Markovian jump neural networks (MJNNs) with time-varying delays and the reachable set estimation (RSE) problem for MJNNs subjected to external disturbances are investigated. Firstly, assuming that two sampled-data periods satisfy Bernoulli distribution, and introducing two stochastic variables to represent the unknown input delay and the sampled-data period respectively, the mode-dependent two-sided loop-based Lyapunov functional (TSLBLF) is constructed, and the conditions for the mean square exponential stability of the error system are derived. Furthermore, a mode-dependent stochastic sampled-data controller is designed. Secondly, by analyzing the unit-energy bounded disturbance of MJNNs, a sufficient condition is proved that all states of MJNNs are confined to an ellipsoid under zero initial condition. In order to make the target ellipsoid contain the reachable set of the system, a stochastic sampled-data controller with RSE is designed. Eventually, two numerical examples and an analog resistor–capacitor network circuit are provided to show that the textual approach can obtain a larger sampled-data period than the existing approach.}
}
@article{WANG2023216,
title = {A novel time series prediction method based on pooling compressed sensing echo state network and its application in stock market},
journal = {Neural Networks},
volume = {164},
pages = {216-227},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.031},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002174},
author = {Zijian Wang and Hui Zhao and Mingwen Zheng and Sijie Niu and Xizhan Gao and Lixiang Li},
keywords = {Echo state network, Pooling activation algorithm, Compressed sensing, Chaotic time series, Stock price prediction},
abstract = {In the prediction of time series, the echo state network (ESN) exhibits exclusive strengths and a unique training structure. Based on ESN model, a pooling activation algorithm consisting noise value and adjusted pooling algorithm is proposed to enrich the update strategy of the reservoir layer in ESN. The algorithm optimizes the distribution of reservoir layer nodes. And the nodes set will be more matched to the characteristics of the data. In addition, we introduce a more efficient and accurate compressed sensing technique based on the existing research. The novel compressed sensing technique reduces the amount of spatial computation of methods. The ESN model based on the above two techniques overcomes the limitations in traditional prediction. In the experimental part, the model is validated with different chaotic time series as well as multiple stocks, and the method shows its efficiency and accuracy in prediction.}
}
@article{ADHIKARI2023115,
title = {Explainable hybrid word representations for sentiment analysis of financial news},
journal = {Neural Networks},
volume = {164},
pages = {115-123},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001880},
author = {Surabhi Adhikari and Surendrabikram Thapa and Usman Naseem and Hai Ya Lu and Gnana Bharathy and Mukesh Prasad},
keywords = {Hybrid word embeddings, XAI, Explainable sentiment analysis, Natural Language Processing, Contextual embeddings, Explainability},
abstract = {Due to the increasing interest of people in the stock and financial market, the sentiment analysis of news and texts related to the sector is of utmost importance. This helps the potential investors in deciding what company to invest in and what are their long-term benefits. However, it is challenging to analyze the sentiments of texts related to the financial domain, given the enormous amount of information available. The existing approaches are unable to capture complex attributes of language such as word usage, including semantics and syntax throughout the context, and polysemy in the context. Further, these approaches failed to interpret the models’ predictability, which is obscure to humans. Models’ interpretability to justify the predictions has remained largely unexplored and has become important to engender users’ trust in the predictions by providing insight into the model prediction. Accordingly, in this paper, we present an explainable hybrid word representation that first augments the data to address the class imbalance issue and then integrates three embeddings to involve polysemy in context, semantics, and syntax in a context. We then fed our proposed word representation to a convolutional neural network (CNN) with attention to capture the sentiment. The experimental results show that our model outperforms several baselines of both classic classifiers and combinations of various word embedding models in the sentiment analysis of financial news. The experimental results also show that the proposed model outperforms several baselines of word embeddings and contextual embeddings when they are separately fed to a neural network model. Further, we show the explainability of the proposed method by presenting the visualization results to explain the reason for a prediction in the sentiment analysis of financial news.}
}
@article{FABER2023248,
title = {VLAD: Task-agnostic VAE-based lifelong anomaly detection},
journal = {Neural Networks},
volume = {165},
pages = {248-273},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.032},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002733},
author = {Kamil Faber and Roberto Corizzo and Bartlomiej Sniezynski and Nathalie Japkowicz},
keywords = {Lifelong learning, Anomaly detection, Lifelong anomaly detection, Continual learning, Neural networks},
abstract = {Lifelong learning represents an emerging machine learning paradigm that aims at designing new methods providing accurate analyses in complex and dynamic real-world environments. Although a significant amount of research has been conducted in image classification and reinforcement learning, very limited work has been done to solve lifelong anomaly detection problems. In this context, a successful method has to detect anomalies while adapting to changing environments and preserving knowledge to avoid catastrophic forgetting. While state-of-the-art online anomaly detection methods are able to detect anomalies and adapt to a changing environment, they are not designed to preserve past knowledge. On the other hand, while lifelong learning methods are focused on adapting to changing environments and preserving knowledge, they are not tailored for detecting anomalies, and often require task labels or task boundaries which are not available in task-agnostic lifelong anomaly detection scenarios. This paper proposes VLAD, a novel VAE-based Lifelong Anomaly Detection method addressing all these challenges simultaneously in complex task-agnostic scenarios. VLAD leverages the combination of lifelong change point detection and an effective model update strategy supported by experience replay with a hierarchical memory maintained by means of consolidation and summarization. An extensive quantitative evaluation showcases the merit of the proposed method in a variety of applied settings. VLAD outperforms state-of-the-art methods for anomaly detection, presenting increased robustness and performance in complex lifelong settings.}
}
@article{SU2023203,
title = {One-shot Federated Learning without server-side training},
journal = {Neural Networks},
volume = {164},
pages = {203-215},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.035},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002216},
author = {Shangchao Su and Bin Li and Xiangyang Xue},
keywords = {Federated learning, One-shot, Model aggregation},
abstract = {Federated Learning (FL) has recently made significant progress as a new machine learning paradigm for privacy protection. Due to the high communication cost of traditional FL, one-shot federated learning is gaining popularity as a way to reduce communication cost between clients and the server. Most of the existing one-shot FL methods are based on Knowledge Distillation; however, distillation based approach requires an extra training phase and depends on publicly available data sets or generated pseudo samples. In this work, we consider a novel and challenging cross-silo setting: performing a single round of parameter aggregation on the local models without server-side training. In this setting, we propose an effective algorithm for Model Aggregation via Exploring Common Harmonized Optima (MA-Echo), which iteratively updates the parameters of all local models to bring them close to a common low-loss area on the loss surface, without harming performance on their own data sets at the same time. Compared to the existing methods, MA-Echo can work well even in extremely non-identical data distribution settings where the support categories of each local model have no overlapped labels with those of the others. We conduct extensive experiments on two popular image classification data sets to compare the proposed method with existing methods and demonstrate the effectiveness of MA-Echo, which clearly outperforms the state-of-the-arts. The source code can be accessed in https://github.com/FudanVI/MAEcho.}
}
@article{LIU2023707,
title = {Collaborative bi-aggregation for directed graph embedding},
journal = {Neural Networks},
volume = {164},
pages = {707-718},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.024},
url = {https://www.sciencedirect.com/science/article/pii/S089360802300268X},
author = {Linsong Liu and Ke-Jia Chen and Zheng Liu},
keywords = {Directed graph, Link prediction, Bi-directional aggregation, Graph representation learning},
abstract = {Directed graph is able to model asymmetric relationships between nodes and research on directed graph embedding is of great significance in downstream graph analysis and inference. Learning source and target embeddings of nodes separately to preserve edge asymmetry has become the dominant approach, but also poses challenge for learning representations of low or even zero in/out degree nodes that are ubiquitous in sparse graphs. In this paper, a collaborative bi-directional aggregation method (COBA) for directed graph embedding is proposed. Firstly, the source and target embeddings of the central node are learned by aggregating from the counterparts of the source and target neighbors, respectively; Secondly, the source/target embeddings of the zero in/out degree central nodes are enhanced by aggregating the counterparts of opposite-directional neighbors (i.e. target/source neighbors); Finally, source and target embeddings of the same node are correlated to achieve collaborative aggregation. Both the feasibility and rationality of the model are theoretically analyzed. Extensive experiments on real-world datasets demonstrate that COBA comprehensively outperforms state-of-the-art methods on multiple tasks and meanwhile validates the effectiveness of proposed aggregation strategies.}
}
@article{LI2023358,
title = {Adversarial feature hybrid framework for steganography with shifted window local loss},
journal = {Neural Networks},
volume = {165},
pages = {358-369},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.053},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002976},
author = {Zhengze Li and Xiaoyuan Yang and Kangqing Shen and Fazhen Jiang and Jin Jiang and Huwei Ren and Yixiao Li},
keywords = {Steganography, Generative adversarial network, Transformer, Information recovery},
abstract = {Image steganography is a long-standing image security problem that aims at hiding information in cover images. In recent years, the application of deep learning to steganography has the tendency to outperform traditional methods. However, the vigorous development of CNN-based steganalyzers still have a serious threat to steganography methods. To address this gap, we present an end-to-end adversarial steganography framework based on CNN and Transformer learned by shifted window local loss, called StegoFormer, which contains Encoder, Decoder, and Discriminator. Encoder is a hybrid model based on U-shaped network and Transformer block, which effectively integrates high-resolution spatial features and global self-attention features. In particular, Shuffle Linear layer is suggested, which can enhance the linear layer’s competence to extract local features. Given the substantial error in the central patch of the stego image, we propose shifted window local loss learning to assist Encoder in generating accurate stego images via weighted local loss. Furthermore, Gaussian mask augmentation method is designed to augment data for Discriminator, which helps to improve the security of Encoder through adversarial training. Controlled experiments show that StegoFormer is superior to the existing advanced steganography methods in terms of anti-steganalysis ability, steganography effectiveness, and information restoration.}
}
@article{TAKAHASHI2023588,
title = {Design of continuous-time recurrent neural networks with piecewise-linear activation function for generation of prescribed sequences of bipolar vectors},
journal = {Neural Networks},
volume = {164},
pages = {588-605},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002575},
author = {Norikazu Takahashi and Tsuyoshi Yamakawa and Yasuhiro Minetoma and Tetsuo Nishi and Tsuyoshi Migita},
keywords = {Recurrent neural network, Piecewise-linear activation function, Sequence, Bipolar vector, Mathematical programming, Limit cycle},
abstract = {A recurrent neural network (RNN) can generate a sequence of patterns as the temporal evolution of the output vector. This paper focuses on a continuous-time RNN model with a piecewise-linear activation function that has neither external inputs nor hidden neurons, and studies the problem of finding the parameters of the model so that it generates a given sequence of bipolar vectors. First, a sufficient condition for the model to generate the desired sequence is derived, which is expressed as a system of linear inequalities in the parameters. Next, three approaches to finding solutions of the system of linear inequalities are proposed: One is formulated as a convex quadratic programming problem and others are linear programming problems. Then, two types of sequences of bipolar vectors that can be generated by the model are presented. Finally, the case where the model generates a periodic sequence of bipolar vectors is considered, and a sufficient condition for the trajectory of the state vector to converge to a limit cycle is provided.}
}
@article{PANDE20231,
title = {Self-supervision assisted multimodal remote sensing image classification with coupled self-looping convolution networks},
journal = {Neural Networks},
volume = {164},
pages = {1-20},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002058},
author = {Shivam Pande and Biplab Banerjee},
keywords = {Cross-modal self-supervision, Hyperspectral images, Image classification, Convolutional neural networks, Coupled self-looping networks},
abstract = {Recently, remote sensing community has seen a surge in the use of multimodal data for different tasks such as land cover classification, change detection and many more. However, handling multimodal data requires synergistically using the information from different sources. Currently, deep learning (DL) techniques are being religiously used in multimodal data fusion owing to their superior feature extraction capabilities. But, DL techniques have their share of challenges. Firstly, DL models are mostly constructed in the forward fashion limiting their feature extraction capability. Secondly, multimodal learning is generally addressed in a supervised setting, which leads to high labelled data requirement. Thirdly, the models generally handle each modality separately, thus preventing any cross-modal interaction. Hence, we propose a novel self-supervision oriented method of multimodal remote sensing data fusion. For effective cross-modal learning, our model solves a self-supervised auxiliary task to reconstruct input features of one modality from the extracted features of another modality, thus enabling more representative pre-fusion features. To counter the forward architecture, our model is composed of convolutions both in backward and forward directions, thus creating self-looping connections, leading to a self-correcting framework. To facilitate cross-modal communication, we have incorporated coupling across modality-specific extractors using shared parameters. We evaluate our approach on three remote sensing datasets, namely Houston 2013 and Houston 2018, which are HSI-LiDAR datasets and TU Berlin, which is an HSI-SAR dataset, where we achieve the respective accuracy of 93.08%, 84.59% and 73.21%, thus beating the state of the art by a minimum of 3.02%, 2.23% and 2.84%.}
}
@article{WANG2023357,
title = {SpikeSEE: An energy-efficient dynamic scenes processing framework for retinal prostheses},
journal = {Neural Networks},
volume = {164},
pages = {357-368},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002332},
author = {Chuanqing Wang and Chaoming Fang and Yong Zou and Jie Yang and Mohamad Sawan},
keywords = {Retinal prostheses, Dynamic vision sensor, Spiking recurrent neural network, Wearable devices},
abstract = {Intelligent and low-power retinal prostheses are highly demanded in this era, where wearable and implantable devices are used for numerous healthcare applications. In this paper, we propose an energy-efficient dynamic scenes processing framework (SpikeSEE) that combines a spike representation encoding technique and a bio-inspired spiking recurrent neural network (SRNN) model to achieve intelligent processing and extreme low-power computation for retinal prostheses. The spike representation encoding technique could interpret dynamic scenes with sparse spike trains, decreasing the data volume. The SRNN model, inspired by the human retina’s special structure and spike processing method, is adopted to predict the response of ganglion cells to dynamic scenes. Experimental results show that the Pearson correlation coefficient of the proposed SRNN model achieves 0.93, which outperforms the state-of-the-art processing framework for retinal prostheses. Thanks to the spike representation and SRNN processing, the model can extract visual features in a multiplication-free fashion. The framework achieves 8 times power reduction compared with the convolutional recurrent neural network (CRNN) processing-based framework. Our proposed SpikeSEE predicts the response of ganglion cells more accurately with lower energy consumption, which alleviates the precision and power issues of retinal prostheses and provides a potential solution for wearable or implantable prostheses.}
}
@article{ZHANG2023472,
title = {Decentralized ADMM with compressed and event-triggered communication},
journal = {Neural Networks},
volume = {165},
pages = {472-482},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023003027},
author = {Zhen Zhang and Shaofu Yang and Wenying Xu},
keywords = {Decentralized optimization, ADMM, Efficient communication, Second-order algorithms},
abstract = {This paper considers the decentralized optimization problem, where agents in a network cooperate to minimize the sum of their local objective functions by communication and local computation. We propose a decentralized second-order communication-efficient algorithm called communication-censored and communication-compressed quadratically approximated alternating direction method of multipliers (ADMM), termed as CC-DQM, by combining event-triggered communication with compressed communication. In CC-DQM, agents are allowed to transmit the compressed message only when the current primal variables have changed greatly compared to its last estimate. Moreover, to relieve the computation cost, the update of Hessian is also scheduled by the trigger condition. Theoretical analysis shows that the proposed algorithm can still maintain an exact linear convergence, despite the existence of compression error and intermittent communication, if the local objective functions are strongly convex and smooth. Finally, numerical experiments demonstrate its satisfactory communication efficiency.}
}
@article{TAKEISHI2023691,
title = {Approximate spectral decomposition of Fisher information matrix for simple ReLU networks},
journal = {Neural Networks},
volume = {164},
pages = {691-706},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002617},
author = {Yoshinari Takeishi and Masazumi Iida and Jun’ichi Takeuchi},
keywords = {Machine learning, Neural networks, Fisher information},
abstract = {We argue the Fisher information matrix (FIM) of one hidden layer networks with the ReLU activation function. For a network, let W denote the d×p weight matrix from the d-dimensional input to the hidden layer consisting of p neurons, and v the p-dimensional weight vector from the hidden layer to the scalar output. We focus on the FIM of v, which we denote as I. Under certain conditions, we characterize the first three clusters of eigenvalues and eigenvectors of the FIM. Specifically, we show that the following approximately holds. (1) Since I is non-negative owing to the ReLU, the first eigenvalue is the Perron–Frobenius eigenvalue. (2) For the cluster of the next maximum values, the eigenspace is spanned by the row vectors of W. (3) The direct sum of the eigenspace of the first eigenvalue and that of the third cluster is spanned by the set of all the vectors obtained as the Hadamard product of any pair of the row vectors of W. We confirmed by numerical calculation that the above is approximately correct when the number of hidden nodes is about 10000.}
}
@article{ZHANG2023146,
title = {CSAST: Content self-supervised and style contrastive learning for arbitrary style transfer},
journal = {Neural Networks},
volume = {164},
pages = {146-155},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.037},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002241},
author = {Yuqi Zhang and Yingjie Tian and Junjie Hou},
keywords = {Arbitrary artistic style, Content self-supervised learning, Style contrastive learning},
abstract = {Arbitrary artistic style transfer has achieved great success with deep neural networks, but it is still difficult for existing methods to tackle the dilemma of content preservation and style translation due to the inherent content-and-style conflict. In this paper, we introduce content self-supervised learning and style contrastive learning to arbitrary style transfer for improved content preservation and style translation, respectively. The former one is based on the assumption that stylization of a geometrically transformed image is perceptually similar to applying the same transformation to the stylized result of the original image. This content self-supervised constraint noticeably improves content consistency before and after style translation, and contributes to reducing noises and artifacts as well. Furthermore, it is especially suitable to video style transfer, due to its ability to promote inter-frame continuity, which is of crucial importance to visual stability of video sequences. For the latter one, we construct a contrastive learning that pull close style representations (Gram matrices) of the same style and push away that of different styles. This brings more accurate style translation and more appealing visual effect. A large number of qualitative and quantitative experiments demonstrate superiority of our method in improving arbitrary style transfer quality, both for images and videos.}
}
@article{GALKE2023156,
title = {Lifelong learning on evolving graphs under the constraints of imbalanced classes and new classes},
journal = {Neural Networks},
volume = {164},
pages = {156-176},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002083},
author = {Lukas Galke and Iacopo Vagliano and Benedikt Franke and Tobias Zielke and Marcel Hoffmann and Ansgar Scherp},
keywords = {Lifelong learning, Evolving graphs, Graph neural networks, Continual learning, Unseen class detection, Graph representation learning},
abstract = {Lifelong graph learning deals with the problem of continually adapting graph neural network (GNN) models to changes in evolving graphs. We address two critical challenges of lifelong graph learning in this work: dealing with new classes and tackling imbalanced class distributions. The combination of these two challenges is particularly relevant since newly emerging classes typically resemble only a tiny fraction of the data, adding to the already skewed class distribution. We make several contributions: First, we show that the amount of unlabeled data does not influence the results, which is an essential prerequisite for lifelong learning on a sequence of tasks. Second, we experiment with different label rates and show that our methods can perform well with only a tiny fraction of annotated nodes. Third, we propose the gDOC method to detect new classes under the constraint of having an imbalanced class distribution. The critical ingredient is a weighted binary cross-entropy loss function to account for the class imbalance. Moreover, we demonstrate combinations of gDOC with various base GNN models such as GraphSAGE, Simplified Graph Convolution, and Graph Attention Networks. Lastly, our k-neighborhood time difference measure provably normalizes the temporal changes across different graph datasets. With extensive experimentation, we find that the proposed gDOC method is consistently better than a naive adaption of DOC to graphs. Specifically, in experiments using the smallest history size, the out-of-distribution detection score of gDOC is 0.09 compared to 0.01 for DOC. Furthermore, gDOC achieves an Open-F1 score, a combined measure of in-distribution classification and out-of-distribution detection, of 0.33 compared to 0.25 of DOC (32% increase).}
}
@article{KURKOVA2023654,
title = {Approximation of classifiers by deep perceptron networks},
journal = {Neural Networks},
volume = {165},
pages = {654-661},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023003052},
author = {Věra Kůrková and Marcello Sanguineti},
keywords = {Approximation by deep networks, Probabilistic bounds on approximation errors, Random classifiers, Concentration of measure, Method of bounded differences, Growth functions},
abstract = {We employ properties of high-dimensional geometry to obtain some insights into capabilities of deep perceptron networks to classify large data sets. We derive conditions on network depths, types of activation functions, and numbers of parameters that imply that approximation errors behave almost deterministically. We illustrate general results by concrete cases of popular activation functions: Heaviside, ramp sigmoid, rectified linear, and rectified power. Our probabilistic bounds on approximation errors are derived using concentration of measure type inequalities (method of bounded differences) and concepts from statistical learning theory.}
}
@article{HE2023395,
title = {Tree-structured neural networks: Spatiotemporal dynamics and optimal control},
journal = {Neural Networks},
volume = {164},
pages = {395-407},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.039},
url = {https://www.sciencedirect.com/science/article/pii/S089360802300223X},
author = {Jiajin He and Min Xiao and Jing Zhao and Zhengxin Wang and Yi Yao and Jinde Cao},
keywords = {Diffusion neural network, Tree structure, Nonlinear state feedback control, Hopf bifurcation, Homogeneous periodic solution},
abstract = {How the network topology drives the response dynamic is a basic question that has not yet been fully answered in neural networks. Elucidating the internal relation between topological structures and dynamics is instrumental in our understanding of brain function. Recent studies have revealed that the ring structure and star structure have a great influence on the dynamical behavior of neural networks. In order to further explore the role of topological structures in the response dynamic, we construct a new tree structure that differs from the ring structure and star structure of traditional neural networks. Considering the diffusion effect, we propose a diffusion neural network model with binary tree structure and multiple delays. How to design control strategies to optimize brain function has also been an open question. Thus, we put forward a novel full-dimensional nonlinear state feedback control strategy to optimize relevant neurodynamics. Some conditions about the local stability and Hopf bifurcation are obtained, and it is proved that the Turing instability does not occur. Moreover, for the formation of the spatially homogeneous periodic solution, some diffusion conditions are also fused together. Finally, several numerical examples are carried out to illustrate the results’ correctness. Meanwhile, some comparative experiments are rendered to reveal the effectiveness of the proposed control strategy.}
}
@article{CHEN2023576,
title = {Input-to-state stability of positive delayed neural networks via impulsive control},
journal = {Neural Networks},
volume = {164},
pages = {576-587},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002526},
author = {Wu-Hua Chen and Xiujuan Li and Shuning Niu and Xiaomei Lu},
keywords = {Input-to-state stability, Positive systems, Delayed neural networks, Time-dependent max-separable Lyapunov function, Impulsive control},
abstract = {This paper is concerned with the positivity and impulsive stabilization of equilibrium points of delayed neural networks (DNNs) subject to bounded disturbances. With the aid of the continuous dependence theorem for impulsive delay differential equations, a relaxed positivity condition is derived, which allows the neuron interconnection matrix to be Metzler if the activation functions satisfy a certain condition. The notion of input-to-state stability (ISS) is introduced to characterize internal global stability and disturbance attenuation performance for impulsively controlled DNNs. The ISS property is analyzed by employing a time-dependent max-separable Lyapunov function which is able to capture the positivity characterization and hybrid structure of the considered DNNs. A ranged dwell-time-dependent ISS condition is obtained, which allows to design an impulsive control law via partial state variables. As a byproduct, an improved global exponential stability criterion for impulse-free positive DNNs is obtained. The applicability of the achieved results is illustrated through three numerical examples.}
}
@article{KIM202349,
title = {Theoretical bounds of generalization error for generalized extreme learning machine and random vector functional link network},
journal = {Neural Networks},
volume = {164},
pages = {49-66},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002009},
author = {Meejoung Kim},
keywords = {Generalized extreme learning machine, Random vector functional link network, Generalization error, Theoretical bound, Tail probability, Moore–Penrose generalized inverse},
abstract = {Ensuring the prediction accuracy of a learning algorithm on a theoretical basis is crucial and necessary for building the reliability of the learning algorithm. This paper analyzes prediction error obtained through the least square estimation in the generalized extreme learning machine (GELM), which applies the limiting behavior of the Moore–Penrose generalized inverse (M–P GI) to the output matrix of ELM. ELM is the random vector functional link (RVFL) network without direct input to output links Specifically, we analyze tail probabilities associated with upper and lower bounds to the error expressed by norms. The analysis employs the concepts of the L2 norm, the Frobenius norm, the stable rank, and the M–P GI. The coverage of theoretical analysis extends to the RVFL network. In addition, a criterion for more precise bounds of prediction errors that may give stochastically better network environments is provided. The analysis is applied to simple examples and large-size datasets to illustrate the procedure and verify the analysis and execution speed with big data. Based on this study, we can immediately obtain the upper and lower bounds of prediction errors and their associated tail probabilities through matrices calculations appearing in the GELM and RVFL. This analysis provides criteria for the reliability of the learning performance of a network in real-time and for network structure that enables obtaining better performance reliability. This analysis can be applied in various areas where the ELM and RVFL are adopted. The proposed analytical method will guide the theoretical analysis of errors occurring in DNNs, which employ a gradient descent algorithm.}
}
@article{ZHANG2023150,
title = {Topology identification for stochastic multi-layer networks via graph-theoretic method},
journal = {Neural Networks},
volume = {165},
pages = {150-163},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.036},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002800},
author = {Chunmei Zhang and Ran Li and Quanxin Zhu and Qin Xu},
keywords = {Topology identification, Stochastic multi-layer networks, Graph-theoretic method},
abstract = {The topological structures of multi-layer networks have an important influence on their dynamical properties, but in most cases the topological structures of networks are unknown. Hence, this paper pays attention to investigating topology identification problems for multi-layer networks with stochastic perturbations. Both intra-layer coupling and inter-layer coupling are incorporated into the research model. Based on the graph-theoretic method and Lyapunov function, topology identification criteria for stochastic multi-layer networks are obtained by designing a suitable adaptive controller. Furthermore, to estimate the time of identification, the finite-time identification criteria are obtained by finite-time control technique. Finally, double-layer Watts–Strogatz small-world networks are presented for numerical simulations to illustrate the correctness of theoretical results.}
}
@article{HEIDARI2023238,
title = {Forward propagation dropout in deep neural networks using Jensen–Shannon and random forest feature importance ranking},
journal = {Neural Networks},
volume = {165},
pages = {238-247},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.044},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002885},
author = {Mohsen Heidari and Mohammad Hossein Moattar and Hamidreza Ghaffari},
keywords = {Deep neural networks, Convolutional neural networks, Dropout, Overfitting, Jensen–Shannon divergence, Random forest},
abstract = {Dropout is a mechanism to prevent deep neural networks from overfitting and improving their generalization. Random dropout is the simplest method, where nodes are randomly terminated at each step of the training phase, which may lead to network accuracy reduction. In dynamic dropout, the importance of each node and its impact on the network performance is calculated, and the important nodes do not participate in the dropout. But the problem is that the importance of the nodes is not calculated consistently. A node may be considered less important and be dropped in one training epoch and on a batch of data before entering the next epoch, in which it may be an important node. On the other hand, calculating the importance of each unit in every training step is costly. In the proposed method, using random forest and Jensen–Shannon divergence, the importance of each node is calculated once. Then, in the forward propagation steps, the importance of the nodes is propagated and used in the dropout mechanism. This method is evaluated and compared with some previously proposed dropout approaches using two different deep neural network architectures on the MNIST, NorB, CIFAR10, CIFAR100, SVHN, and ImageNet datasets. The results suggest that the proposed method has better accuracy with fewer nodes and better generalizability. Also, the evaluations show that the approach has comparable complexity with other approaches and its convergence time is low as compared with state-of-the-art methods.}
}
@article{ZHANG2023497,
title = {Quasi-projective and complete synchronization of discrete-time fractional-order delayed neural networks},
journal = {Neural Networks},
volume = {164},
pages = {497-507},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002368},
author = {Xiao-Li Zhang and Hong-Li Li and Yongguang Yu and Long Zhang and Haijun Jiang},
keywords = {Fractional-order neural networks, Discrete-time, Time delays, Quasi-projective synchronization, Complete synchronization},
abstract = {This paper presents new theoretical results on quasi-projective synchronization (Q-PS) and complete synchronization (CS) of one kind of discrete-time fractional-order delayed neural networks (DFDNNs). At first, three new fractional difference inequalities for exploring the upper bound of quasi-synchronization error and adaptive synchronization are established by dint of Laplace transform and properties of discrete Mittag-Leffler function, which vastly expand a number of available results. Furthermore, two controllers are designed including nonlinear controller and adaptive controller. And on the basis of Lyapunov method, the aforementioned inequalities and properties of fractional-order difference operators, some sufficient synchronization criteria of DFDNNs are derived. Because of the above controllers, synchronization criteria in this paper are less conservative. At last, numerical examples are carried out to illustrate the usefulness of theoretical upshots.}
}
@article{POMPONI2023606,
title = {Continual learning with invertible generative models},
journal = {Neural Networks},
volume = {164},
pages = {606-616},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002642},
author = {Jary Pomponi and Simone Scardapane and Aurelio Uncini},
keywords = {Machine learning, Continual learning, Normalizing flow, Catastrophic forgetting},
abstract = {Catastrophic forgetting (CF) happens whenever a neural network overwrites past knowledge while being trained on new tasks. Common techniques to handle CF include regularization of the weights (using, e.g., their importance on past tasks), and rehearsal strategies, where the network is constantly re-trained on past data. Generative models have also been applied for the latter, in order to have endless sources of data. In this paper, we propose a novel method that combines the strengths of regularization and generative-based rehearsal approaches. Our generative model consists of a normalizing flow (NF), a probabilistic and invertible neural network, trained on the internal embeddings of the network. By keeping a single NF throughout the training process, we show that our memory overhead remains constant. In addition, exploiting the invertibility of the NF, we propose a simple approach to regularize the network’s embeddings with respect to past tasks. We show that our method performs favorably with respect to state-of-the-art approaches in the literature, with bounded computational power and memory overheads.}
}
@article{QIN2023135,
title = {Long short-term memory with activation on gradient},
journal = {Neural Networks},
volume = {164},
pages = {135-145},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.026},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002125},
author = {Chuan Qin and Liangming Chen and Zangtai Cai and Mei Liu and Long Jin},
keywords = {Long short-term memory (LSTM), Gradient activation, Vanishing gradient problem, Exploding gradient problem, Ill-conditioned problem},
abstract = {As the number of long short-term memory (LSTM) layers increases, vanishing/exploding gradient problems exacerbate and have a negative impact on the performance of the LSTM. In addition, the ill-conditioned problem occurs in the training process of LSTM and adversely affects its convergence. In this work, a simple and effective method of the gradient activation is applied to the LSTM, while empirical criteria for choosing gradient activation hyperparameters are found. Activating the gradient refers to modifying the gradient with a specific function named the gradient activation function. Moreover, different activation functions and different gradient operations are compared to prove that the gradient activation is effective on LSTM. Furthermore, comparative experiments are conducted, and their results show that the gradient activation alleviates the above problems and accelerates the convergence of the LSTM. The source code is publicly available at https://github.com/LongJin-lab/ACT-In-NLP.}
}
@article{NIKNAM2023596,
title = {DyVGRNN: DYnamic mixture Variational Graph Recurrent Neural Networks},
journal = {Neural Networks},
volume = {165},
pages = {596-610},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.048},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002927},
author = {Ghazaleh Niknam and Soheila Molaei and Hadi Zare and Shirui Pan and Mahdi Jalili and Tingting Zhu and David Clifton},
keywords = {Dynamic graph representation learning, Dynamic node embedding, Variational graph auto-encoder, Graph recurrent neural network, Attention mechanism},
abstract = {Although graph representation learning has been studied extensively in static graph settings, dynamic graphs are less investigated in this context. This paper proposes a novel integrated variational framework called DYnamic mixture Variational Graph Recurrent Neural Networks (DyVGRNN), which consists of extra latent random variables in structural and temporal modelling. Our proposed framework comprises an integration of Variational Graph Auto-Encoder (VGAE) and Graph Recurrent Neural Network (GRNN) by exploiting a novel attention mechanism. The Gaussian Mixture Model (GMM) and the VGAE framework are combined in DyVGRNN to model the multimodal nature of data, which enhances performance. To consider the significance of time steps, our proposed method incorporates an attention-based module. The experimental results demonstrate that our method greatly outperforms state-of-the-art dynamic graph representation learning methods in terms of link prediction and clustering.22The source code is available at https://github.com/GhazalehNiknam/DyVGRNN.}
}
@article{LIU2023370,
title = {Task guided representation learning using compositional models for zero-shot domain adaptation},
journal = {Neural Networks},
volume = {165},
pages = {370-380},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.030},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002769},
author = {Shuang Liu and Mete Ozay},
keywords = {Domain adaptation, Zero-shot, Representation learning},
abstract = {Zero-shot domain adaptation (ZDA) methods aim to transfer knowledge about a task learned in a source domain to a target domain, while task-relevant data from target domain are not available. In this work, we address learning feature representations which are invariant to and shared among different domains considering task characteristics for ZDA. To this end, we propose a method for task-guided ZDA (TG-ZDA) which employs multi-branch deep neural networks to learn feature representations exploiting their domain invariance and shareability properties. The proposed TG-ZDA models can be trained end-to-end without requiring synthetic tasks and data generated from estimated representations of target domains. The proposed TG-ZDA has been examined using benchmark ZDA tasks on image classification datasets. Experimental results show that our proposed TG-ZDA outperforms state-of-the-art ZDA methods for different domains and tasks.}
}
@article{LEVY2023275,
title = {Growing dendrites enhance a neuron’s computational power and memory capacity},
journal = {Neural Networks},
volume = {164},
pages = {275-309},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.033},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002198},
author = {William B Levy and Robert A. Baxter},
keywords = {Synaptogenesis, Pruning, Generative model, Brain development, Energy efficient, Dendritic spike},
abstract = {Neocortical pyramidal neurons have many dendrites, and such dendrites are capable of, in isolation of one-another, generating a neuronal spike. It is also now understood that there is a large amount of dendritic growth during the first years of a humans life, arguably a period of prodigious learning. These observations inspire the construction of a local, stochastic algorithm based on an earlier stochastic, homeostatic, Hebbian developmental theory. Here we investigate the neurocomputational advantages and limits on this novel algorithm that combines dendritogenesis with supervised adaptive synaptogenesis. Neurons created with this algorithm have enhanced memory capacity, can avoid catastrophic interference (forgetting), and have the ability to unmix mixture distributions. In particular, individual dendrites develop within each class, in an unsupervised manner, to become feature-clusters that correspond to the mixing elements of class-conditional mixture distribution. Error-free classification is demonstrated with input perturbations up to 40%. Although discriminative problems are used to understand the capabilities of the stochastic algorithm and the neuronal connectivity it produces, the algorithm is in the generative class, it thus seems ideal for decisions that require generalization, i.e., extrapolation beyond previous learning.}
}
@article{XIA2023527,
title = {Two-timescale recurrent neural networks for distributed minimax optimization},
journal = {Neural Networks},
volume = {165},
pages = {527-539},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023003040},
author = {Zicong Xia and Yang Liu and Jiasen Wang and Jun Wang},
keywords = {Neurodynamic optimization, Distributed optimization, Minimax optimization, Recurrent neural networks},
abstract = {In this paper, we present two-timescale neurodynamic optimization approaches to distributed minimax optimization. We propose four multilayer recurrent neural networks for solving four different types of generally nonlinear convex–concave minimax problems subject to linear equality and nonlinear inequality constraints. We derive sufficient conditions to guarantee the stability and optimality of the neural networks. We demonstrate the viability and efficiency of the proposed neural networks in two specific paradigms for Nash-equilibrium seeking in a zero-sum game and distributed constrained nonlinear optimization.}
}
@article{PENG2023105,
title = {Optimal H∞ tracking control of nonlinear systems with zero-equilibrium-free via novel adaptive critic designs},
journal = {Neural Networks},
volume = {164},
pages = {105-114},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.021},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002071},
author = {Zhinan Peng and Hanqi Ji and Chaobin Zou and Yiqun Kuang and Hong Cheng and Kaibo Shi and Bijoy Kumar Ghosh},
keywords = {Optimal  tracking control, Nonlinear systems, Nonzero equilibrium, Cost function, Adaptive dynamic programming},
abstract = {In this paper, a novel adaptive critic control method is designed to solve an optimal H∞ tracking control problem for continuous nonlinear systems with nonzero equilibrium based on adaptive dynamic programming (ADP). To guarantee the finiteness of a cost function, traditional methods generally assume that the controlled system has a zero equilibrium point, which is not true in practical systems. In order to overcome such obstacle and realize H∞ optimal tracking control, this paper proposes a novel cost function design with respect to disturbance, tracking error and the derivative of tracking error. Based on the designed cost function, the H∞ control problem is formulated as two-player zero-sum differential games, and then a policy iteration (PI) algorithm is proposed to solve the corresponding Hamilton–Jacobi–Isaacs (HJI) equation. In order to obtain the online solution to the HJI equation, a single-critic neural network structure based on PI algorithm is established to learn the optimal control policy and the worst-case disturbance law. It is worth mentioning that the proposed adaptive critic control method can simplify the controller design process when the equilibrium of the systems is not zero. Finally, simulations are conducted to evaluate the tracking performance of the proposed control methods.}
}
@article{KHAN2023310,
title = {Retinal vessel segmentation via a Multi-resolution Contextual Network and adversarial learning},
journal = {Neural Networks},
volume = {165},
pages = {310-320},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.029},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002757},
author = {Tariq M. Khan and Syed S. Naqvi and Antonio Robles-Kelly and Imran Razzak},
keywords = {Retinal vessel segmentation, Encoder–decoder, Contextual network, Adversarial learning, Diabetic retinopathy},
abstract = {Timely and affordable computer-aided diagnosis of retinal diseases is pivotal in precluding blindness. Accurate retinal vessel segmentation plays an important role in disease progression and diagnosis of such vision-threatening diseases. To this end, we propose a Multi-resolution Contextual Network (MRC-Net) that addresses these issues by extracting multi-scale features to learn contextual dependencies between semantically different features and using bi-directional recurrent learning to model former-latter and latter-former dependencies. Another key idea is training in adversarial settings for foreground segmentation improvement through optimization of the region-based scores. This novel strategy boosts the performance of the segmentation network in terms of the Dice score (and correspondingly Jaccard index) while keeping the number of trainable parameters comparatively low. We have evaluated our method on three benchmark datasets, including DRIVE, STARE, and CHASE, demonstrating its superior performance as compared with competitive approaches elsewhere in the literature.}
}
@article{XIA2023645,
title = {A survey of sum–product networks structural learning},
journal = {Neural Networks},
volume = {164},
pages = {645-666},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002514},
author = {Riting Xia and Yan Zhang and Xueyan Liu and Bo Yang},
keywords = {Sum–product networks, SPN structure learning, Deep neural models, Probabilistic graphical models, Tractable, Expressive efficiency},
abstract = {Sum–product networks (SPNs) in deep probabilistic models have made great progress in computer vision, robotics, neuro-symbolic artificial intelligence, natural language processing, probabilistic programming languages, and other fields. Compared with probabilistic graphical models and deep probabilistic models, SPNs can balance the tractability and expressive efficiency. In addition, SPNs remain more interpretable than deep neural models. The expressiveness and complexity of SPNs depend on their own structure. Thus, how to design an effective SPN structure learning algorithm that can balance expressiveness and complexity has become a hot research topic in recent years. In this paper, we review SPN structure learning comprehensively, including the motivation of SPN structure learning, a systematic review of related theories, the proper categorization of different SPN structure learning algorithms, several evaluation approaches and some helpful online resources. Moreover, we discuss some open issues and research directions for SPN structure learning. To our knowledge, this is the first survey to focus specifically on SPN structure learning, and we hope to provide useful references for researchers in related fields.}
}
@article{TANG2023333,
title = {Multi-view subspace clustering via adaptive graph learning and late fusion alignment},
journal = {Neural Networks},
volume = {165},
pages = {333-343},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002630},
author = {Chuan Tang and Kun Sun and Chang Tang and Xiao Zheng and Xinwang Liu and Jun-Jie Huang and Wei Zhang},
keywords = {Multi-view subspace clustering, Graph learning, Partition fusion},
abstract = {Multi-view subspace clustering has attracted great attention due to its ability to explore data structure by utilizing complementary information from different views. Most of existing methods learn a sample representation coefficient matrix or an affinity graph for each single view, then the final clustering result is obtained from the spectral embedding of a consensus graph using certain traditional clustering techniques, such as k-means. However, clustering performance will be degenerated if the early fusion of partitions cannot fully exploit relationships between all samples. Different from existing methods, we propose a multi-view subspace clustering method via adaptive graph learning and late fusion alignment (AGLLFA). For each view, AGLLFA learns an affinity graph adaptively to capture the similarity relationship among samples. Moreover, a spectral embedding learning term is designed to exploit the latent feature space of different views. Furthermore, we design a late fusion alignment mechanism to generate an optimal clustering partition by fusing view-specific partitions obtained from multiple views. An alternate updating algorithm with validated convergence is developed to solve the resultant optimization problem. Extensive experiments on several benchmark datasets are conducted to illustrate the effectiveness of the proposed method when compared with other state-of-the-art methods. The demo code of this work is publicly available at https://github.com/tangchuan2000/AGLLFA.}
}
@article{YANG202391,
title = {A deep connectome learning network using graph convolution for connectome-disease association study},
journal = {Neural Networks},
volume = {164},
pages = {91-104},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.025},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002113},
author = {Yanwu Yang and Chenfei Ye and Ting Ma},
keywords = {Deep connectome learning, Distance-based connectome network, Connectome-wide association study, Graph neural network},
abstract = {Multivariate analysis approaches provide insights into the identification of phenotype associations in brain connectome data. In recent years, deep learning methods including convolutional neural network (CNN) and graph neural network (GNN), have shifted the development of connectome-wide association studies (CWAS) and made breakthroughs for connectome representation learning by leveraging deep embedded features. However, most existing studies remain limited by potentially ignoring the exploration of region-specific features, which play a key role in distinguishing brain disorders with high intra-class variations, such as autism spectrum disorder (ASD), and attention deficit hyperactivity disorder (ADHD). Here, we propose a multivariate distance-based connectome network (MDCN) that addresses the local specificity problem by efficient parcellation-wise learning, as well as associating population and parcellation dependencies to map individual differences. The approach incorporating an explainable method, parcellation-wise gradient and class activation map (p-GradCAM), is feasible for identifying individual patterns of interest and pinpointing connectome associations with diseases. We demonstrate the utility of our method on two largely aggregated multicenter public datasets by distinguishing ASD and ADHD from healthy controls and assessing their associations with underlying diseases. Extensive experiments have demonstrated the superiority of MDCN in classification and interpretation, where MDCN outperformed competitive state-of-the-art methods and achieved a high proportion of overlap with previous findings. As a CWAS-guided deep learning method, our proposed MDCN framework may narrow the bridge between deep learning and CWAS approaches, and provide new insights for connectome-wide association studies.}
}
@article{ZHANG2023186,
title = {PIPER: A logic-driven deep contrastive optimization pipeline for event temporal reasoning},
journal = {Neural Networks},
volume = {164},
pages = {186-202},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S089360802300206X},
author = {Beibei Zhang and Lishuang Li},
keywords = {Event temporal reasoning, Deep contrastive optimization, Rule-match features, Hierarchical graph distillation network, Multi-stage joint optimization, Single-stage joint optimization},
abstract = {Event temporal relation extraction is an important task for information extraction. The existing methods usually rely on feature engineering and require post-process to achieve optimization, though inconsistent optimization may occur in the post-process module and main neural network due to their independence. Recently, a few works start to incorporate the temporal logic rules into the neural network and achieve joint optimization. However, these methods still suffer from two shortcomings: (1) Although the joint optimization is applied, the differences between rules are neglected in the unified design of rule losses and further the interpretability and flexibility of the design of model are reduced. (2) Because of lacking abundant syntactic connections between events and rule-match features, the performance of the model may be suppressed by the inefficient interaction in training between features and rules. To tackle these issues, this paper proposes PIPER, a logic-driven deep contrastive optimization pipeline for event temporal reasoning. Specifically, we apply joint optimization (including multi-stage and single-stage joint paradigms) by combining independent rule losses (i.e., flexibility) to make PIPER more interpretable. Also, by proposing a hierarchical graph distillation network to obtain more abundant syntactic information, the designed rule-match features can effectively aid in the interaction between low-level features and high-level rules during training. The final experiments on TB-Dense and MATRES demonstrate that the proposed model can achieve competitive performance compared with the recent advances.}
}
@article{MOON2023562,
title = {Genetic data visualization using literature text-based neural networks: Examples associated with myocardial infarction},
journal = {Neural Networks},
volume = {165},
pages = {562-595},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002599},
author = {Jihye Moon and Hugo F. Posada-Quintero and Ki H. Chon},
keywords = {Explainable Artificial Intelligence, Natural language processing, Unsupervised learning, Cross-modal representation, Data visualization, Cardiovascular Disease risk prediction},
abstract = {Data visualization is critical to unraveling hidden information from complex and high-dimensional data. Interpretable visualization methods are critical, especially in the biology and medical fields, however, there are limited effective visualization methods for large genetic data. Current visualization methods are limited to lower-dimensional data and their performance suffers if there is missing data. In this study, we propose a literature-based visualization method to reduce high-dimensional data without compromising the dynamics of the single nucleotide polymorphisms (SNP) and textual interpretability. Our method is innovative because it is shown to (1) preserves both global and local structures of SNP while reducing the dimension of the data using literature text representations, and (2) enables interpretable visualizations using textual information. For performance evaluations, we examined the proposed approach to classify various classification categories including race, myocardial infarction event age groups, and sex using several machine learning models on the literature-derived SNP data. We used visualization approaches to examine clustering of data as well as quantitative performance metrics for the classification of the risk factors examined above. Our method outperformed all popular dimensionality reduction and visualization methods for both classification and visualization, and it is robust against missing and higher-dimensional data. Moreover, we found it feasible to incorporate both genetic and other risk information obtained from literature with our method.}
}
@article{GONG2023381,
title = {BrainS: Customized multi-core embedded multiple scale neuromorphic system},
journal = {Neural Networks},
volume = {165},
pages = {381-392},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.043},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002873},
author = {Bo Gong and Jiang Wang and Meili Lu and Gong Meng and Kai Sun and Siyuan Chang and Zhen Zhang and Xile Wei},
keywords = {Neuromorphic hardware, Multi-scale simulations, Multi-core embedded system, Real-time system},
abstract = {Research on modeling and mechanisms of the brain remains the most urgent and challenging task. The customized embedded neuromorphic system is one of the most effective approaches for multi-scale simulations ranging from ion channel to network. This paper proposes BrainS, a scalable multi-core embedded neuromorphic system capable of accommodating massive and large-scale simulations. It is designed with rich external extension interfaces to support various types of input/output and communication requirements. The 3D mesh-based topology with an efficient memory access mechanism makes exploring the properties of neuronal networks possible. BrainS operates at 168 MHz and contains a model database ranging from ion channel to network scale within the Fundamental Computing Unit (FCU). At the ion channel scale, the Basic Community Unit (BCU) can perform real-time simulations of a Hodgkin–Huxley (HH) neuron with 16000 ion channels, using 125.54 KB of the SRAM. When the number of ion channels is within 64000, the HH neuron is simulated in real-time by 4 BCUs. At the network scale, the basal ganglia-thalamus (BG-TH) network consisting of 3200 Izhikevich neurons, providing a vital motor regulation function, is simulated in 4 BCUs with a power consumption of 364.8 mW. Overall, BrainS has an excellent performance in real-time and flexible configurability, providing an embedded application solution for multi-scale simulation.}
}
@article{ZHAO2023631,
title = {Distance metric learning based on the class center and nearest neighbor relationship},
journal = {Neural Networks},
volume = {164},
pages = {631-644},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002356},
author = {Yifeng Zhao and Liming Yang},
keywords = {Distance metric learning, Class center, Nearest neighbor relationship, Multi-metric learning},
abstract = {Distance metric learning has been a promising technology to improve the performance of algorithms related to distance metrics. The existing distance metric learning methods are either based on the class center or the nearest neighbor relationship. In this work, we propose a new distance metric learning method based on the class center and nearest neighbor relationship (DMLCN). Specifically, when centers of different classes overlap, DMLCN first splits each class into several clusters and uses one center to represent one cluster. Then, a distance metric is learned such that each example is close to the corresponding cluster center and the nearest neighbor relationship is kept for each receptive field. Therefore, while characterizing the local structure of data, the proposed method leads to intra-class compactness and inter-class dispersion simultaneously. Further, to better process complex data, we introduce multiple metrics into DMLCN (MMLCN) by learning a local metric for each center. Following that, a new classification decision rule is designed based on the proposed methods. Moreover, we develop an iterative algorithm to optimize the proposed methods. The convergence and complexity are analyzed theoretically. Experiments on different types of data sets including artificial data sets, benchmark data sets and noise data sets show the feasibility and effectiveness of the proposed methods.}
}
@article{DIABA2023321,
title = {SCADA securing system using deep learning to prevent cyber infiltration},
journal = {Neural Networks},
volume = {165},
pages = {321-332},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.047},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002915},
author = {Sayawu Yakubu Diaba and Theophilus Anafo and Lord Anertei Tetteh and Michael Alewo Oyibo and Andrew Adewale Alola and Miadreza Shafie-khah and Mohammed Elmusrati},
keywords = {Genetically seeded flora, Intrusion detection systems, Long short-term memory, Recurrent neural network, Residual neural network, And transformer neural network},
abstract = {Supervisory Control and Data Acquisition (SCADA) systems are computer-based control architectures specifically engineered for the operation of industrial machinery via hardware and software models. These systems are used to project, monitor, and automate the state of the operational network through the utilization of ethernet links, which enable two-way communications. However, as a result of their constant connectivity to the internet and the lack of security frameworks within their internal architecture, they are susceptible to cyber-attacks. In light of this, we have proposed an intrusion detection algorithm, intending to alleviate this security bottleneck. The proposed algorithm, the Genetically Seeded Flora (GSF) feature optimization algorithm, is integrated with Transformer Neural Network (TNN) and functions by detecting changes in operational patterns that may be indicative of an intruder’s involvement. The proposed Genetically Seeded Flora Transformer Neural Network (GSFTNN) algorithm stands in stark contrast to the signature-based method employed by traditional intrusion detection systems. To evaluate the performance of the proposed algorithm, extensive experiments are conducted using the WUSTL-IIOT-2018 ICS SCADA cyber security dataset. The results of these experiments indicate that the proposed algorithm outperforms traditional algorithms such as Residual Neural Networks (ResNet), Recurrent Neural Networks (RNN), and Long Short-Term Memory (LSTM) in terms of accuracy and efficiency.}
}
@article{GAO2023516,
title = {Enhanced covertness class discriminative universal adversarial perturbations},
journal = {Neural Networks},
volume = {165},
pages = {516-526},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023003076},
author = {Haoran Gao and Hua Zhang and Xin Zhang and Wenmin Li and Jiahui Wang and Fei Gao},
keywords = {Adversarial attacks, Universal adversarial perturbations, Deep neural networks},
abstract = {The main aim of class discriminative universal adversarial perturbations (CD-UAPs) is that the adversary can flexibly control the targeted class and influence remaining classes limitedly. CD-UAPs generated by the existing attack strategies suffer from a high fooling ratio of non-targeted source classes under non-targeted and targeted attacks, and face the increasing risk of discovery. In this paper, we propose a training framework for generating enhanced covertness CD-UAPs. It trains the targeted source class set and the non-targeted source classes set alternately to update the perturbation and introduces logit pairing to mitigate the influence of perturbation on the non-targeted source classes set. Further, we extend CD-UAPs on the targeted (one-targeted) attack to the multi-targeted attack, which perturbs a targeted source class to multiple targeted sink classes that seriously threaten the current scenario. It can not only provide the adversary with freedom of precise attack but reduce the risk of being detected. This attack poses a strong threat to security-sensitive applications. Extensive experiments on the CIFAR-10, CIFAR-100 and ImageNet datasets show our method can generate more deceptive perturbations and enhance the covertness of CD-UAPs. For example, our method improves the absolute fooling ratio gaps of ResNet-20 and VGG-16 by 9.46% and 6.94% compared with the baseline method, respectively. We achieve the multi-targeted attack with a high fooling ratio on the GTSRB dataset. The average absolute target fooling ratio gaps of ResNet-20 and VGG-16 are 81.89% and 76.33%, respectively.}
}
@article{SUN2023106,
title = {An insect-inspired model facilitating autonomous navigation by incorporating goal approaching and collision avoidance},
journal = {Neural Networks},
volume = {165},
pages = {106-118},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002782},
author = {Xuelong Sun and Qinbing Fu and Jigen Peng and Shigang Yue},
keywords = {Path integration, Collision detection-and-avoidance, Vector-based navigation, Insect navigation, Sensory–motor, Lobula giant movement detector (LGMD)},
abstract = {Being one of the most fundamental and crucial capacity of robots and animals, autonomous navigation that consists of goal approaching and collision avoidance enables completion of various tasks while traversing different environments. In light of the impressive navigational abilities of insects despite their tiny brains compared to mammals, the idea of seeking solutions from insects for the two key problems of navigation, i.e., goal approaching and collision avoidance, has fascinated researchers and engineers for many years. However, previous bio-inspired studies have focused on merely one of these two problems at one time. Insect-inspired navigation algorithms that synthetically incorporate both goal approaching and collision avoidance, and studies that investigate the interactions of these two mechanisms in the context of sensory–motor closed-loop autonomous navigation are lacking. To fill this gap, we propose an insect-inspired autonomous navigation algorithm to integrate the goal approaching mechanism as the global working memory inspired by the sweat bee’s path integration (PI) mechanism, and the collision avoidance model as the local immediate cue built upon the locust’s lobula giant movement detector (LGMD) model. The presented algorithm is utilized to drive agents to complete navigation task in a sensory–motor closed-loop manner within a bounded static or dynamic environment. Simulation results demonstrate that the synthetic algorithm is capable of guiding the agent to complete challenging navigation tasks in a robust and efficient way. This study takes the first tentative step to integrate the insect-like navigation mechanisms with different functionalities (i.e., global goal and local interrupt) into a coordinated control system that future research avenues could build upon.}
}
@article{SHI2023428,
title = {A direct discretization recurrent neurodynamics method for time-variant nonlinear optimization with redundant robot manipulators},
journal = {Neural Networks},
volume = {164},
pages = {428-438},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.040},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002265},
author = {Yang Shi and Wangrong Sheng and Shuai Li and Bin Li and Xiaobing Sun and Dimitrios K. Gerontitis},
keywords = {Discrete time-variant nonlinear optimization (DTVNO), Discrete-time recurrent neurodynamics (DTRN), Direct discrete technique, Convergence, Robot manipulators},
abstract = {Discrete time-variant nonlinear optimization (DTVNO) problems are commonly encountered in various scientific researches and engineering application fields. Nowadays, many discrete-time recurrent neurodynamics (DTRN) methods have been proposed for solving the DTVNO problems. However, these traditional DTRN methods currently employ an indirect technical route in which the discrete-time derivation process requires to interconvert with continuous-time derivation process. In order to break through this traditional research method, we develop a novel DTRN method based on the inspiring direct discrete technique for solving the DTVNO problem more concisely and efficiently. To be specific, firstly, considering that the DTVNO problem emerging in the discrete-time tracing control of robot manipulator, we further abstract and summarize the mathematical definition of DTVNO problem, and then we define the corresponding error function. Secondly, based on the second-order Taylor expansion, we can directly obtain the DTRN method for solving the DTVNO problem, which no longer requires the derivation process in the continuous-time environment. Whereafter, such a DTRN method is theoretically analyzed and its convergence is demonstrated. Furthermore, numerical experiments confirm the effectiveness and superiority of the DTRN method. In addition, the application experiments of the robot manipulators are presented to further demonstrate the superior performance of the DTRN method.}
}
@article{WANG2023310,
title = {Semi-supervised deep embedded clustering with pairwise constraints and subset allocation},
journal = {Neural Networks},
volume = {164},
pages = {310-322},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002022},
author = {Yalin Wang and Jiangfeng Zou and Kai Wang and Chenliang Liu and Xiaofeng Yuan},
keywords = {Semi-supervised clustering, Deep embedded clustering, Pairwise constraints, Subset allocation, Sample overlap},
abstract = {Semi-supervised deep clustering methods attract much attention due to their excellent performance on the end-to-end clustering task. However, it is hard to obtain satisfying clustering results since many overlapping samples in industrial text datasets strongly and incorrectly influence the learning process. Existing methods incorporate prior knowledge in the form of pairwise constraints or class labels, which not only largely ignore the correlation between these two supervision information but also cause the problem of weak-supervised constraint or incorrect strong-supervised label guidance. In order to tackle these problems, we propose a semi-supervised method based on pairwise constraints and subset allocation (PCSA-DEC). We redefine the similarity-based constraint loss by forcing the similarity of samples in the same class much higher than other samples and design a novel subset allocation loss to precisely learn strong-supervised information contained in labels which consistent with unlabeled data. Experimental results on the two industrial text datasets show that our method can yield 8.2%–8.7% improvement in accuracy and 13.4%–19.8% on normalized mutual information over the state-of-the-art method.}
}
@article{WANG2023476,
title = {Evolution-communication spiking neural P systems with energy request rules},
journal = {Neural Networks},
volume = {164},
pages = {476-488},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002381},
author = {Liping Wang and Xiyu Liu and Minghe Sun and Yuzhen Zhao},
keywords = {Membrane computing, Spiking neural P systems, Turing universality, Energy request rules, SAT problem},
abstract = {Evolution-communication spiking neural P systems with energy request rules (ECSNP-ER systems) are proposed and developed as a new variant of evolution-communication spiking neural P systems. In ECSNP-ER systems, in addition to spike-evolution rules and spike-communication rules, neurons also have energy request rules. Energy request rules are used to obtain energy from the environment needed for spike evolution and communication in neurons. The definition, structure and operations of ECSNP-ER systems are presented in detail. ECSNP-ER systems are proved to have the same computing capabilities as Turing machines by using them as number generating/accepting devices and function computing devices. Working non-deterministically, ECSNP-ER systems are used to solve NP-complete problems, using the SAT problem as an example, in linear time.}
}
@article{DONG2023298,
title = {Discriminative analysis dictionary learning with adaptively ordinal locality preserving},
journal = {Neural Networks},
volume = {165},
pages = {298-309},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002666},
author = {Jing Dong and Kai Wu and Chang Liu and Xue Mei and Wenwu Wang},
keywords = {Discriminative dictionary learning, Ordinal locality preserving, Analysis dictionary learning, Image classification},
abstract = {Dictionary learning has found broad applications in signal and image processing. By adding constraints to the traditional dictionary learning model, dictionaries with discriminative capability can be obtained which can deal with the task of image classification. The Discriminative Convolutional Analysis Dictionary Learning (DCADL) algorithm proposed recently has achieved promising results with low computational complexity. However, DCADL is still limited in classification performance because of the lack of constraints on dictionary structures. To solve this problem, this study introduces an adaptively ordinal locality preserving (AOLP) term to the original model of DCADL to further improve the classification performance. With the AOLP term, the distance ranking in the neighborhood of each atom can be preserved, which can improve the discrimination of coding coefficients. In addition, a linear classifier for the classification of coding coefficients is trained along with the dictionary. A new method is designed specifically to solve the optimization problem corresponding to the proposed model. Experiments are performed on several commonly used datasets to show the promising results of the proposed algorithm in classification performance and computational efficiency.}
}
@article{PAQUIN2023382,
title = {Stability analysis of stochastic gradient descent for homogeneous neural networks and linear classifiers},
journal = {Neural Networks},
volume = {164},
pages = {382-394},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.028},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002149},
author = {Alexandre Lemire Paquin and Brahim Chaib-draa and Philippe Giguère},
keywords = {Generalization, Deep learning, Stochastic gradient descent, Stability},
abstract = {We prove new generalization bounds for stochastic gradient descent when training classifiers with invariances. Our analysis is based on the stability framework and covers both the convex case of linear classifiers and the non-convex case of homogeneous neural networks. We analyze stability with respect to the normalized version of the loss function used for training. This leads to investigating a form of angle-wise stability instead of euclidean stability in weights. For neural networks, the measure of distance we consider is invariant to rescaling the weights of each layer. Furthermore, we exploit the notion of on-average stability in order to obtain a data-dependent quantity in the bound. This data-dependent quantity is seen to be more favorable when training with larger learning rates in our numerical experiments. This might help to shed some light on why larger learning rates can lead to better generalization in some practical scenarios.}
}
@article{WANG2023341,
title = {Dynamic event-triggered controller design for nonlinear systems: Reinforcement learning strategy},
journal = {Neural Networks},
volume = {163},
pages = {341-353},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001855},
author = {Zichen Wang and Xin Wang and Ning Pang},
keywords = {Reinforcement learning, Dynamic event-triggered strategy, Nonlinear system, Backstepping technique, Actor–critic neural networks},
abstract = {The current investigation aims at the optimal control problem for discrete-time nonstrict-feedback nonlinear systems by invoking the reinforcement learning-based backstepping technique and neural networks. The dynamic-event-triggered control strategy introduced in this paper can alleviate the communication frequency between the actuator and controller. Based on the reinforcement learning strategy, actor–critic neural networks are employed to implement the n-order backstepping framework. Then, a neural network weight-updated algorithm is developed to minimize the computational burden and avoid the local optimal problem. Furthermore, a novel dynamic-event-triggered strategy is introduced, which can remarkably outperform the previously studied static-event-triggered strategy. Moreover, combined with the Lyapunov stability theory, all signals in the closed-loop system are strictly proven to be semiglobal uniformly ultimately bounded. Finally, the practicality of the offered control algorithms is further elucidated by the numerical simulation examples.}
}
@article{YANG2023406,
title = {Adaptive closed-loop paradigm of electrophysiology for neuron models},
journal = {Neural Networks},
volume = {165},
pages = {406-419},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.050},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002940},
author = {Ming Yang and Jiang Wang and Shanshan Li and Kuanchuan Wang and Wei Yue and Chen Liu},
keywords = {Closed-loop paradigm, Radial basis function neural network, Unscented Kalman filter, Stimulus computing, Neuron model, Electrophysiological property},
abstract = {The traditional electrophysiological experiments based on an open-loop paradigm are relatively complicated and limited when facing an individual neuron with uncertain nonlinear factors. Emerging neural technologies enable tremendous growth in experimental data leading to the curse of high-dimensional data, which obstructs the mechanism exploration of spiking activities in the neurons. In this work, we propose an adaptive closed-loop electrophysiology simulation experimental paradigm based on a Radial Basis Function neural network and a highly nonlinear unscented Kalman filter. On account of the complex nonlinear dynamic characteristics of the real neurons, the proposed simulation experimental paradigm could fit the unknown neuron models with different channel parameters and different structures (i.e. single or multiple compartments), and further compute the injected stimulus in time according to the arbitrary desired spiking activities of the neurons. However, the hidden electrophysiological states of the neurons are difficult to be measured directly. Thus, an extra Unscented Kalman filter modular is incorporated in the closed-loop electrophysiology experimental paradigm. The numerical results and theoretical analyses demonstrate that the proposed adaptive closed-loop electrophysiology simulation experimental paradigm achieves desired spiking activities arbitrarily and the hidden dynamics of the neurons are visualized by the unscented Kalman filter modular. The proposed adaptive closed-loop simulation experimental paradigm can avoid the inefficiency of data at increasingly greater scales and enhance the scalability of electrophysiological experiments, thus speeding up the discovery cycle on neuroscience.}
}
@article{SHI2023617,
title = {Multi-granularity knowledge distillation and prototype consistency regularization for class-incremental learning},
journal = {Neural Networks},
volume = {164},
pages = {617-630},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S089360802300237X},
author = {Yanyan Shi and Dianxi Shi and Ziteng Qiao and Zhen Wang and Yi Zhang and Shaowu Yang and Chunping Qiu},
keywords = {Class-incremental learning, Knowledge distillation, Consistency regularization, Image classification},
abstract = {Deep neural networks (DNNs) are prone to the notorious catastrophic forgetting problem when learning new tasks incrementally. Class-incremental learning (CIL) is a promising solution to tackle the challenge and learn new classes while not forgetting old ones. Existing CIL approaches adopted stored representative exemplars or complex generative models to achieve good performance. However, storing data from previous tasks causes memory or privacy issues, and the training of generative models is unstable and inefficient. This paper proposes a method based on multi-granularity knowledge distillation and prototype consistency regularization (MDPCR) that performs well even when the previous training data is unavailable. First, we propose to design knowledge distillation losses in the deep feature space to constrain the incremental model trained on the new data. Thereby, multi-granularity is captured from three aspects: by distilling multi-scale self-attentive features, the feature similarity probability, and global features to maximize the retention of previous knowledge, effectively alleviating catastrophic forgetting. Conversely, we preserve the prototype of each old class and employ prototype consistency regularization (PCR) to ensure that the old prototypes and semantically enhanced prototypes produce consistent prediction, which excels in enhancing the robustness of old prototypes and reduces the classification bias. Extensive experiments on three CIL benchmark datasets confirm that MDPCR performs significantly better over exemplar-free methods and outperforms typical exemplar-based approaches.}
}
@article{PARCHAM202377,
title = {HybridBranchNet: A novel structure for branch hybrid convolutional neural networks architecture},
journal = {Neural Networks},
volume = {165},
pages = {77-93},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.025},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002708},
author = {Ebrahim Parcham and Mansoor Fateh},
keywords = {Convolutional networks, Neural network, Supervised learning, Trainable parameters},
abstract = {ConvNet deep neural networks are developed with a consistent structure. The availability of abundant resources helps these structures to be scaled and redesigned in different sizes so that they can be optimized for different applications. By increasing one or more dimensions of the network, such as depth, resolution and width, the number of trainable network parameters will increase and, as a result, the accuracy and performance It should be noted that the backtracking of the convolutional neural network will improve. However, but increasing the number of network parameters increases the complexity of the network, which is not desirable. Therefore, adjusting the structure of the network, increasing the speed, and reducing the number of network parameters along with ensuring accuracy optimization will be important. This study aims to examine a branch network structure systematically, which can lead to better performance. In this study, in order to increase the speed, to reduce the size of the convolutional network model, and to increase the accuracy optimization, a new scaling method, which optimally designs all dimensions of depth, width, and resolution, is proposed based on a branch neural network. A family of HybridBranchNet networks, which is more accurate and efficient than ConvNets, has been created along with this design. HybridBranchNet3 has a classification accuracy of 83.1%. The proposed model was compared with a family of EfficientNet convolutional networks. The comparison results revealed that the proposed network exceeded the mentioned models in terms of accuracy and speed by 1.03% and 39%, respectively. They also showed that the number of trainable parameters is 13% less than that of the EfficientNet network. The proposed method has an accuracy of 92.3% in the CIFAR-100 dataset and 98.8% in the Flowers-102 dataset. Although the architectures such as CoAtNet have slightly higher classification accuracy than the proposed method, they have a greater number of parameters that cannot be used in a conventional system.}
}
@article{ZHANG2023164,
title = {A regularization perspective based theoretical analysis for adversarial robustness of deep spiking neural networks},
journal = {Neural Networks},
volume = {165},
pages = {164-174},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.038},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002824},
author = {Hui Zhang and Jian Cheng and Jun Zhang and Hongyi Liu and Zhihui Wei},
keywords = {Spiking neural networks, Adversarial robustness, Regularization, Gradient, ANN-to-SNN conversion},
abstract = {Spiking Neural Network (SNN) has been recognized as the third generation of neural networks. Conventionally, a SNN can be converted from a pre-trained Artificial Neural Network (ANN) with less computation and memory than training from scratch. But, these converted SNNs are vulnerable to adversarial attacks. Numerical experiments demonstrate that the SNN trained by optimizing the loss function will be more adversarial robust, but the theoretical analysis for the mechanism of robustness is lacking. In this paper, we provide a theoretical explanation by analyzing the expected risk function. Starting by modeling the stochastic process introduced by the Poisson encoder, we prove that there is a positive semidefinite regularizer. Perhaps surprisingly, this regularizer can make the gradients of the output with respect to input closer to zero, thus resulting in inherent robustness against adversarial attacks. Extensive experiments on the CIFAR10 and CIFAR100 datasets support our point of view. For example, we find that the sum of squares of the gradients of the converted SNNs is 13∼160 times that of the trained SNNs. And, the smaller the sum of the squares of the gradients, the smaller the degradation of accuracy under adversarial attack.}
}
@article{MEI2023408,
title = {Joint feature selection and optimal bipartite graph learning for subspace clustering},
journal = {Neural Networks},
volume = {164},
pages = {408-418},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.044},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002307},
author = {Shikun Mei and Wenhui Zhao and Quanxue Gao and Ming Yang and Xinbo Gao},
keywords = {Subspace clustering, Bipartite graph, Feature selection, Dictionary representation, Laplacian rank constraint},
abstract = {Recently, there has been tremendous interest in developing graph-based subspace clustering in high-dimensional data, which does not require a priori knowledge of the number of dimensions and subspaces. The general steps of such algorithms are dictionary representation and spectral clustering. Traditional methods use the dataset itself as a dictionary when performing dictionary representation. There are some limitations that the redundant information present in the dictionary and features may make the constructed graph structure unclear and require post-processing to obtain labels. To address these problems, we propose a novel subspace clustering model that first introduces feature selection to process the input data, randomly selects some samples to construct a dictionary to remove redundant information and learns the optimal bipartite graph with K-connected components under the constraint of the (normalized) Laplacian rank. Finally, the labels are obtained directly from the graphs. The experimental results on motion segmentation and face recognition datasets demonstrate the superior effectiveness and stability of our algorithm.}
}
@article{SHANG2023345,
title = {Multi-teacher knowledge distillation based on joint Guidance of Probe and Adaptive Corrector},
journal = {Neural Networks},
volume = {164},
pages = {345-356},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002010},
author = {Ronghua Shang and Wenzheng Li and Songling Zhu and Licheng Jiao and Yangyang Li},
keywords = {Knowledge distillation, Linear classifier probes, Convolutional neural networks, Spail attention, Model compression},
abstract = {Knowledge distillation (KD) has been widely used in model compression. But, in the current multi-teacher KD algorithms, the student can only passively acquire the knowledge of the teacher’s middle layer in a single form and all teachers use identical a guiding scheme to the student. To solve these problems, this paper proposes a multi-teacher KD based on joint Guidance of Probe and Adaptive Corrector (GPAC) method. First, GPAC proposes a teacher selection strategy guided by the Linear Classifier Probe (LCP). This strategy allows the student to select better teachers in the middle layer. Teachers are evaluated using the classification accuracy detected by LCP. Then, GPAC designs an adaptive multi-teacher instruction mechanism. The mechanism uses instructional weights to emphasize the student’s predicted direction and reduce the student’s difficulty learning from teachers. At the same time, every teacher can formulate guiding scheme according to the Kullback–Leibler divergence loss of the student and itself. Finally, GPAC develops a multi-level mechanism for adjusting spatial attention loss. this mechanism uses a piecewise function that varies with the number of epochs to adjust the spatial attention loss. This piecewise function classifies the student’ learning about spatial attention into three levels, which can efficiently use spatial attention of teachers. GPAC and the current state-of-the-art distillation methods are tested on CIFAR-10 and CIFAR-100 datasets. The experimental results demonstrate that the proposed method in this paper can obtain higher classification accuracy.}
}
@article{XIANG2023225,
title = {Block-level dependency syntax based model for end-to-end aspect-based sentiment analysis},
journal = {Neural Networks},
volume = {166},
pages = {225-235},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002496},
author = {Yan Xiang and Jiqun Zhang and Junjun Guo},
keywords = {Aspect-based sentiment analysis, Block-level dependency syntax, Block-dependency syntax-guided interactive attention, Adaptive semantic-syntactic fusion, Aspect term},
abstract = {End-to-End aspect-based sentiment analysis (E2E-ABSA) aims to jointly extract aspect terms and identify their sentiment polarities. Although previous research has demonstrated that syntax knowledge can be beneficial for E2E-ABSA, standard syntax dependency parsing struggles to capture the block-level relation between aspect and opinion terms, which hinders the role of syntax in E2E-ABSA. To address this issue, this paper proposes a block-level dependency syntax parsing (BDEP) based model to enhance the performance of E2E-ABSA. BDEP is constructed by incorporating routine dependency syntax parsing and part-of-speech tagging, which enables the capture of block-level relations. Subsequently. the BDEP-guided interactive attention module (BDEP-IAM) is used to obtain the aspect-aware representation of each word. Finally the adaptive fusion module is leveraged to combine the semantic-syntactic representation to simultaneously extract the aspect term and identify aspect-orient sentiment polarity. The model is evaluated on five benchmark datasets, including Laptop14, Rest _ALL, Restaurant14, Restaurant15, and TWITTER, with F1 scores of 62.67%, 76.53%, 75.42%, 62.21%, and 58.03%, respectively. The results show that our model outperforms the other compared state-of-the-art (SOTA) methods on all datasets. Additionally, ablation experiments confirm the efficacy of BDEP and IAM in improving aspect-level sentiment analysis.}
}
@article{FU20231,
title = {Motion perception based on ON/OFF channels: A survey},
journal = {Neural Networks},
volume = {165},
pages = {1-18},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.031},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002770},
author = {Qinbing Fu},
keywords = {Motion perception, ON/OFF channels, Selectivity, Neural modelling, Bio-inspired sensor, Machine application},
abstract = {Motion perception is an essential ability for animals and artificially intelligent systems interacting effectively, safely with surrounding objects and environments. Biological visual systems, that have naturally evolved over hundreds-million years, are quite efficient and robust for motion perception, whereas artificial vision systems are far from such capability. This paper argues that the gap can be significantly reduced by formulation of ON/OFF channels in motion perception models encoding luminance increment (ON) and decrement (OFF) responses within receptive field, separately. Such signal-bifurcating structure has been found in neural systems of many animal species articulating early motion is split and processed in segregated pathways. However, the corresponding biological substrates, and the necessity for artificial vision systems have never been elucidated together, leaving concerns on uniqueness and advantages of ON/OFF channels upon building dynamic vision systems to address real world challenges. This paper highlights the importance of ON/OFF channels in motion perception through surveying current progress covering both neuroscience and computationally modelling works with applications. Compared to related literature, this paper for the first time provides insights into implementation of different selectivity to directional motion of looming, translating, and small-sized target movement based on ON/OFF channels in keeping with soundness and robustness of biological principles. Existing challenges and future trends of such bio-plausible computational structure for visual perception in connection with hotspots of machine learning, advanced vision sensors like event-driven camera finally are discussed.}
}
@article{SHEN2023439,
title = {Domain-adaptive message passing graph neural network},
journal = {Neural Networks},
volume = {164},
pages = {439-454},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.038},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002253},
author = {Xiao Shen and Shirui Pan and Kup-Sze Choi and Xi Zhou},
keywords = {Cross-network node classification, Domain adaptation, Graph neural network, Message passing},
abstract = {Cross-network node classification (CNNC), which aims to classify nodes in a label-deficient target network by transferring the knowledge from a source network with abundant labels, draws increasing attention recently. To address CNNC, we propose a domain-adaptive message passing graph neural network (DM-GNN), which integrates graph neural network (GNN) with conditional adversarial domain adaptation. DM-GNN is capable of learning informative representations for node classification that are also transferrable across networks. Firstly, a GNN encoder is constructed by dual feature extractors to separate ego-embedding learning from neighbor-embedding learning so as to jointly capture commonality and discrimination between connected nodes. Secondly, a label propagation node classifier is proposed to refine each node’s label prediction by combining its own prediction and its neighbors’ prediction. In addition, a label-aware propagation scheme is devised for the labeled source network to promote intra-class propagation while avoiding inter-class propagation, thus yielding label-discriminative source embeddings. Thirdly, conditional adversarial domain adaptation is performed to take the neighborhood-refined class-label information into account during adversarial domain adaptation, so that the class-conditional distributions across networks can be better matched. Comparisons with eleven state-of-the-art methods demonstrate the effectiveness of the proposed DM-GNN.}
}
@article{ZHANG2023451,
title = {MI-CAT: A transformer-based domain adaptation network for motor imagery classification},
journal = {Neural Networks},
volume = {165},
pages = {451-462},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023003064},
author = {Dongxue Zhang and Huiying Li and Jingmeng Xie},
keywords = {Electroencephalograph (EEG), Transformer, Domain adaptation, Motor imagery (MI), Brain–computer interfaces (BCIs)},
abstract = {Due to its convenience and safety, electroencephalography (EEG) data is one of the most widely used signals in motor imagery (MI) brain–computer interfaces (BCIs). In recent years, methods based on deep learning have been widely applied to the field of BCIs, and some studies have gradually tried to apply Transformer to EEG signal decoding due to its superior global information focusing ability. However, EEG signals vary from subject to subject. Based on Transformer, how to effectively use data from other subjects (source domain) to improve the classification performance of a single subject (target domain) remains a challenge. To fill this gap, we propose a novel architecture called MI-CAT. The architecture innovatively utilizes Transformer’s self-attention and cross-attention mechanisms to interact features to resolve differential distribution between different domains. Specifically, we adopt a patch embedding layer for the extracted source and target features to divide the features into multiple patches. Then, we comprehensively focus on the intra-domain and inter-domain features by stacked multiple Cross-Transformer Blocks (CTBs), which can adaptively conduct bidirectional knowledge transfer and information exchange between domains. Furthermore, we also utilize two non-shared domain-based attention blocks to efficiently capture domain-dependent information, optimizing the features extracted from the source and target domains to assist in feature alignment. To evaluate our method, we conduct extensive experiments on two real public EEG datasets, Dataset IIb and Dataset IIa, achieving competitive performance with an average classification accuracy of 85.26% and 76.81%, respectively. Experimental results demonstrate that our method is a powerful model for decoding EEG signals and facilitates the development of the Transformer for brain–computer interfaces (BCIs).}
}
@article{LIU2023124,
title = {A novel sequential structure for lightweight multi-scale feature learning under limited available images},
journal = {Neural Networks},
volume = {164},
pages = {124-134},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.023},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002095},
author = {Peng Liu and Jie Du and Chi-Man Vong},
keywords = {Lightweight model, Multi-scale feature, Sequential structure, Image classification and segmentation},
abstract = {Although multi-scale feature learning can improve the performances of deep models, its parallel structure quadratically increases the model parameters and causes deep models to become larger and larger when enlarging the receptive fields (RFs). This leads to deep models easily suffering from over-fitting issue in many practical applications where the available training samples are always insufficient or limited. In addition, under this limited situation, although lightweight models (with fewer model parameters) can effectively reduce over-fitting, they may suffer from under-fitting because of insufficient training data for effective feature learning. In this work, a lightweight model called Sequential Multi-scale Feature Learning Network (SMF-Net) is proposed to alleviate these two issues simultaneously using a novel sequential structure of multi-scale feature learning. Compared to both deep and lightweight models, the proposed sequential structure in SMF-Net can easily extract features with larger RFs for multi-scale feature learning only with a few and linearly increased model parameters. The experimental results on both classification and segmentation tasks demonstrate that our SMF-Net only has 1.25M model parameters (5.3% of Res2Net50) with 0.7G FLOPS (14.6% of Res2Net50) for classification and 1.54M parameters (8.9% of UNet) with 3.35G FLOPs (10.9% of UNet) for segmentation but achieves higher accuracy than SOTA deep models and lightweight models, even when the training data is very limited available.}
}
@article{ZHANG2023274,
title = {Fixed-time synchronization for quaternion-valued memristor-based neural networks with mixed delays},
journal = {Neural Networks},
volume = {165},
pages = {274-289},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.045},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002897},
author = {Yanlin Zhang and Liqiao Yang and Kit Ian Kou and Yang Liu},
keywords = {Fixed-time synchronization, Nonlinear controllers, Unilateral coefficients, Quaternion-valued, Memristor-based neural networks},
abstract = {In this paper, the fixed-time synchronization (FXTSYN) of unilateral coefficients quaternion-valued memristor-based neural networks (UCQVMNNs) with mixed delays is investigated. A direct analytical approach is suggested to obtain FXTSYN of UCQVMNNs utilizing one-norm smoothness in place of decomposition. When dealing with drive–response system discontinuity issues, use the set-valued map and the differential inclusion theorem. To accomplish the control objective, innovative nonlinear controllers and the Lyapunov functions are designed. Furthermore, some criteria of FXTSYN for UCQVMNNs are given using inequality techniques and the novel FXTSYN theory. And the accurate settling time is obtained explicitly. Finally, in order to show that the obtained theoretical results are accurate, useful, and applicable, numerical simulations are presented at the conclusion.}
}
@article{LIM202319,
title = {SCL: Self-supervised contrastive learning for few-shot image classification},
journal = {Neural Networks},
volume = {165},
pages = {19-30},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.037},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002812},
author = {Jit Yan Lim and Kian Ming Lim and Chin Poo Lee and Yong Xuan Tan},
keywords = {Few-shot learning, Self-supervised learning, Meta-learning, Contrastive learning},
abstract = {Few-shot learning aims to train a model with a limited number of base class samples to classify the novel class samples. However, to attain generalization with a limited number of samples is not a trivial task. This paper proposed a novel few-shot learning approach named Self-supervised Contrastive Learning (SCL) that enriched the model representation with multiple self-supervision objectives. Given the base class samples, the model is trained with the base class loss. Subsequently, contrastive-based self-supervision is introduced to minimize the distance between each training sample with their augmented variants to improve the sample discrimination. To recognize the distant sample, rotation-based self-supervision is proposed to enable the model to learn to recognize the rotation degree of the samples for better sample diversity. The multitask environment is introduced where each training sample is assigned with two class labels: base class label and rotation class label. Complex augmentation is put forth to help the model learn a deeper understanding of the object. The image structure of the training samples are augmented independent of the base class information. The proposed SCL is trained to minimize the base class loss, contrastive distance loss, and rotation class loss simultaneously to learn the generic features and improve the novel class performance. With the multiple self-supervision objectives, the proposed SCL outperforms state-of-the-art few-shot approaches on few-shot image classification benchmark datasets.}
}
@article{CHEN2023681,
title = {Credit assignment with predictive contribution measurement in multi-agent reinforcement learning},
journal = {Neural Networks},
volume = {164},
pages = {681-690},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.021},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002654},
author = {Renlong Chen and Ying Tan},
keywords = {Multi-agent reinforcement learning (MARL), Centralized training with decentralized execution (CTDE), Credit assignment, Policy gradient, Reward reshaping},
abstract = {Credit assignment is a crucial issue in multi-agent tasks employing a centralized training and decentralized execution paradigm. While value decomposition has demonstrated strong performance in Q-learning-based approaches and certain Actor–Critic variants, it remains challenging to achieve efficient credit assignment in multi-agent tasks using policy gradient methods due to decomposable value limitations. This paper introduces Predictive Contribution Measurement, an explicit credit assignment method that compares prediction errors among agents and allocates surrogate rewards based on their relevance to global state transitions, with a theoretical guarantee. With multi-agent proximal policy optimization (MAPPO) as a training backend, we propose Predictive Contribution MAPPO (PC-MAPPO). Our experiments demonstrate that PC-MAPPO, with a 10% warm-up phase, outperforms MAPPO, QMIX, and Weighted QMIX on StarCraft multi-agent challenge tasks, particularly in maps requiring heightened cooperation to defeat enemies, such as the map corridor. Employing a pre-trained predictor, PC-MAPPO achieves significantly improved performance on all tested super-hard maps. In parallel training scenarios, PC-MAPPO exhibits superior data efficiency and achieves state-of-the-art performance compared to other methods.}
}
@article{WANG202381,
title = {TIToK: A solution for bi-imbalanced unsupervised domain adaptation},
journal = {Neural Networks},
volume = {164},
pages = {81-90},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.027},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002137},
author = {Yunyun Wang and Quchuan Chen and Yao Liu and Weikai Li and Songcan Chen},
keywords = {Unsupervised domain adaptation, Semi-supervised learning, Imbalanced learning, Class contrastive knowledge},
abstract = {Unsupervised domain adaptation (UDA) aims to transfer knowledge via domain alignment, and typically assumes balanced data distribution. When deployed in real tasks, however, (i) each domain usually suffers from class imbalance, and (ii) different domains may have different class imbalance ratios. In such bi-imbalanced cases with both within-domain and across-domain imbalance, source knowledge transfer may degenerate the target performance. Some recent efforts have adopted source re-weighting to this issue, in order to align label distributions across domains. However, since target label distribution is unknown, the alignment might be incorrect or even risky. In this paper, we propose an alternative solution named TIToK for bi-imbalanced UDA, by directly Transferring Imbalance-Tolerant Knowledge across domains. In TIToK, a class contrastive loss is presented for classification, in order to alleviate the sensitivity to imbalance in knowledge transfer. Meanwhile, knowledge of class correlation is transferred as a supplementary, which is commonly invariant to imbalance. Finally, discriminative feature alignment is developed for a more robust classifier boundary. Experiments over benchmark datasets show that TIToK achieves competitive performance with the state-of-the-arts, and its performance is less sensitive to imbalance.}
}
@article{PANJA2023185,
title = {Epicasting: An Ensemble Wavelet Neural Network for forecasting epidemics},
journal = {Neural Networks},
volume = {165},
pages = {185-212},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.049},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002939},
author = {Madhurima Panja and Tanujit Chakraborty and Uttam Kumar and Nan Liu},
keywords = {Wavelet methods, MODWT, Epidemiology, Neural networks, Time series forecasting},
abstract = {Infectious diseases remain among the top contributors to human illness and death worldwide, among which many diseases produce epidemic waves of infection. The lack of specific drugs and ready-to-use vaccines to prevent most of these epidemics worsens the situation. These force public health officials and policymakers to rely on early warning systems generated by accurate and reliable epidemic forecasters. Accurate forecasts of epidemics can assist stakeholders in tailoring countermeasures, such as vaccination campaigns, staff scheduling, and resource allocation, to the situation at hand, which could translate to reductions in the impact of a disease. Unfortunately, most of these past epidemics exhibit nonlinear and non-stationary characteristics due to their spreading fluctuations based on seasonal-dependent variability and the nature of these epidemics. We analyze various epidemic time series datasets using a maximal overlap discrete wavelet transform (MODWT) based autoregressive neural network and call it Ensemble Wavelet Neural Network (EWNet) model. MODWT techniques effectively characterize non-stationary behavior and seasonal dependencies in the epidemic time series and improve the nonlinear forecasting scheme of the autoregressive neural network in the proposed ensemble wavelet network framework. From a nonlinear time series viewpoint, we explore the asymptotic stationarity of the proposed EWNet model to show the asymptotic behavior of the associated Markov Chain. We also theoretically investigate the effect of learning stability and the choice of hidden neurons in the proposal. From a practical perspective, we compare our proposed EWNet framework with twenty-two statistical, machine learning, and deep learning models for fifteen real-world epidemic datasets with three test horizons using four key performance indicators. Experimental results show that the proposed EWNet is highly competitive compared to the state-of-the-art epidemic forecasting methods.}
}
@article{TUN2023689,
title = {Contrastive encoder pre-training-based clustered federated learning for heterogeneous data},
journal = {Neural Networks},
volume = {165},
pages = {689-704},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023003192},
author = {Ye Lin Tun and Minh N.H. Nguyen and Chu Myaet Thwal and Jinwoo Choi and Choong Seon Hong},
keywords = {Federated learning, Client clustering, Contrastive learning, Pre-training, Data heterogeneity},
abstract = {Federated learning (FL) is a promising approach that enables distributed clients to collaboratively train a global model while preserving their data privacy. However, FL often suffers from data heterogeneity problems, which can significantly affect its performance. To address this, clustered federated learning (CFL) has been proposed to construct personalized models for different client clusters. One effective client clustering strategy is to allow clients to choose their own local models from a model pool based on their performance. However, without pre-trained model parameters, such a strategy is prone to clustering failure, in which all clients choose the same model. Unfortunately, collecting a large amount of labeled data for pre-training can be costly and impractical in distributed environments. To overcome this challenge, we leverage self-supervised contrastive learning to exploit unlabeled data for the pre-training of FL systems. Together, self-supervised pre-training and client clustering can be crucial components for tackling the data heterogeneity issues of FL. Leveraging these two crucial strategies, we propose contrastive pre-training-based clustered federated learning (CP-CFL) to improve the model convergence and overall performance of FL systems. In this work, we demonstrate the effectiveness of CP-CFL through extensive experiments in heterogeneous FL settings, and present various interesting observations.}
}
@article{ZHAO2023508,
title = {Event-triggered fault-tolerant control for input-constrained nonlinear systems with mismatched disturbances via adaptive dynamic programming},
journal = {Neural Networks},
volume = {164},
pages = {508-520},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002320},
author = {Heng Zhao and Huanqing Wang and Ben Niu and Xudong Zhao and Khalid H. Alharbi},
keywords = {Constrained nonlinear systems, Sliding-mode control, Adaptive dynamic programming (ADP), Event-triggered control, Fault-tolerant control},
abstract = {In this paper, the issue of event-triggered optimal fault-tolerant control is investigated for input-constrained nonlinear systems with mismatched disturbances. To eliminate the effect of abrupt faults and ensure the optimal performance of general nonlinear dynamics, an adaptive dynamic programming (ADP) algorithm is employed to develop a sliding mode fault-tolerant control strategy. When the system trajectories converge to the sliding-mode surface, the equivalent sliding mode dynamics is transformed into a reformulated auxiliary system with a modified cost function. Then, a single critic neural network (NN) is adopted to solve the modified Hamilton–Jacobi–Bellman (HJB) equation. In order to overcome the difficulty that arises from the persistence of excitation (PE) condition, the experience replay technique is utilized to update the critic weights. In this study, a novel control method is proposed, which can effectively eliminate the effects of abrupt faults while achieving optimal control with the minimum cost under a single network architecture. Furthermore, the closed-loop nonlinear system is proved to be uniformly ultimate boundedness based on Lyapunov stability theory. Finally, three examples are presented to verify the validity of the control strategy.}
}
@article{LI2023455,
title = {A multi-view co-training network for semi-supervised medical image-based prognostic prediction},
journal = {Neural Networks},
volume = {164},
pages = {455-463},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.030},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002162},
author = {Hailin Li and Siwen Wang and Bo Liu and Mengjie Fang and Runnan Cao and Bingxi He and Shengyuan Liu and Chaoen Hu and Di Dong and Ximing Wang and Hexiang Wang and Jie Tian},
keywords = {Deep neural network, Medical image analysis, Prognostic prediction, Semi-supervised learning},
abstract = {Prognostic prediction has long been a hotspot in disease analysis and management, and the development of image-based prognostic prediction models has significant clinical implications for current personalized treatment strategies. The main challenge in prognostic prediction is to model a regression problem based on censored observations, and semi-supervised learning has the potential to play an important role in improving the utilization efficiency of censored data. However, there are yet few effective semi-supervised paradigms to be applied. In this paper, we propose a semi-supervised co-training deep neural network incorporating a support vector regression layer for survival time estimation (Co-DeepSVS) that improves the efficiency in utilizing censored data for prognostic prediction. First, we introduce a support vector regression layer in deep neural networks to deal with censored data and directly predict survival time, and more importantly to calculate the labeling confidence of each case. Then, we apply a semi-supervised multi-view co-training framework to achieve accurate prognostic prediction, where labeling confidence estimation with prior knowledge of pseudo time is conducted for each view. Experimental results demonstrate that the proposed Co-DeepSVS has a promising prognostic ability and surpasses most widely used methods on a multi-phase CT dataset. Besides, the introduction of SVR layer makes the model more robust in the presence of follow-up bias.}
}
@article{GUO2023491,
title = {Variational gated autoencoder-based feature extraction model for inferring disease-miRNA associations based on multiview features},
journal = {Neural Networks},
volume = {165},
pages = {491-505},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.052},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002964},
author = {Yanbu Guo and Dongming Zhou and Xiaoli Ruan and Jinde Cao},
keywords = {Graph autoencoders, Feature fusion, Neural networks, Disease-miRNA associations},
abstract = {MicroRNAs (miRNA) play critical roles in diverse biological processes of diseases. Inferring potential disease-miRNA associations enable us to better understand the development and diagnosis of complex human diseases via computational algorithms. The work presents a variational gated autoencoder-based feature extraction model to extract complex contextual features for inferring potential disease-miRNA associations. Specifically, our model fuses three different similarities of miRNAs into a comprehensive miRNA network and then combines two various similarities of diseases into a comprehensive disease network, respectively. Then, a novel graph autoencoder is designed to extract multilevel representations based on variational gate mechanisms from heterogeneous networks of miRNAs and diseases. Finally, a gate-based association predictor is devised to combine multiscale representations of miRNAs and diseases via a novel contrastive cross-entropy function, and then infer disease-miRNA associations. Experimental results indicate that our proposed model achieves remarkable association prediction performance, proving the efficacy of the variational gate mechanism and contrastive cross-entropy loss for inferring disease-miRNA associations.}
}
@article{ZHOU202343,
title = {Distributional generative adversarial imitation learning with reproducing kernel generalization},
journal = {Neural Networks},
volume = {165},
pages = {43-59},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.027},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002721},
author = {Yirui Zhou and Mengxiao Lu and Xiaowei Liu and Zhengping Che and Zhiyuan Xu and Jian Tang and Yangchun Zhang and Yan Peng and Yaxin Peng},
keywords = {Generative adversarial imitation learning, Policy generalization, Computational properties, Distributional reinforcement learning},
abstract = {Generative adversarial imitation learning (GAIL) regards imitation learning (IL) as a distribution matching problem between the state–action distributions of the expert policy and the learned policy. In this paper, we focus on the generalization and computational properties of policy classes. We prove that the generalization can be guaranteed in GAIL when the class of policies is well controlled. With the capability of policy generalization, we introduce distributional reinforcement learning (RL) into GAIL and propose the greedy distributional soft gradient (GDSG) algorithm to solve GAIL. The main advantages of GDSG can be summarized as: (1) Q-value overestimation, a crucial factor leading to the instability of GAIL with off-policy training, can be alleviated by distributional RL. (2) By considering the maximum entropy objective, the policy can be improved in terms of performance and sample efficiency through sufficient exploration. Moreover, GDSG attains a sublinear convergence rate to a stationary solution. Comprehensive experimental verification in MuJoCo environments shows that GDSG can mimic expert demonstrations better than previous GAIL variants.}
}
@article{REYESSANCHEZ2023464,
title = {Automatized offline and online exploration to achieve a target dynamics in biohybrid neural circuits built with living and model neurons},
journal = {Neural Networks},
volume = {164},
pages = {464-475},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.034},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002204},
author = {Manuel Reyes-Sanchez and Rodrigo Amaducci and Pablo Sanchez-Martin and Irene Elices and Francisco B. Rodriguez and Pablo Varona},
keywords = {Interacting living and model neurons, Automatic parameterization, Neural sequences, Hybrid neural dynamics, Biohybrid coupling, Neurotechnology},
abstract = {Biohybrid circuits of interacting living and model neurons are an advantageous means to study neural dynamics and to assess the role of specific neuron and network properties in the nervous system. Hybrid networks are also a necessary step to build effective artificial intelligence and brain hybridization. In this work, we deal with the automatized online and offline adaptation, exploration and parameter mapping to achieve a target dynamics in hybrid circuits and, in particular, those that yield dynamical invariants between living and model neurons. We address dynamical invariants that form robust cycle-by-cycle relationships between the intervals that build neural sequences from such interaction. Our methodology first attains automated adaptation of model neurons to work in the same amplitude regime and time scale of living neurons. Then, we address the automatized exploration and mapping of the synapse parameter space that lead to a specific dynamical invariant target. Our approach uses multiple configurations and parallel computing from electrophysiological recordings of living neurons to build full mappings, and genetic algorithms to achieve an instance of the target dynamics for the hybrid circuit in a short time. We illustrate and validate such strategy in the context of the study of functional sequences in neural rhythms, which can be easily generalized for any variety of hybrid circuit configuration. This approach facilitates both the building of hybrid circuits and the accomplishment of their scientific goal.}
}
@article{SHAN2023463,
title = {Prediction of common labels for universal domain adaptation},
journal = {Neural Networks},
volume = {165},
pages = {463-471},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.057},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023003015},
author = {Xinxin Shan and Tai Ma and Ying Wen},
keywords = {Universal domain adaptation, Prediction of common label, Cross-domain classification, Deep learning},
abstract = {Universal domain adaptation (UniDA) is an unsupervised domain adaptation that selectively transfers the knowledge between different domains containing different label sets. However, the existing methods do not predict the common labels of different domains and manually set a threshold to discriminate private samples, so they rely on the target domain to finely select the threshold and ignore the problem of negative transfer. In this paper, to address the above problems, we propose a novel classification model named Prediction of Common Labels (PCL) for UniDA, in which the common labels are predicted by Category Separation via Clustering (CSC). It is noted that we devise a new evaluation metric called category separation accuracy to measure the performance of category separation. To weaken negative transfer, we select source samples by the predicted common labels to fine-tune model for better domain alignment. In the test process, the target samples are discriminated by the predicted common labels and the results of clustering. Experimental results on three widely used benchmark datasets indicate the effectiveness of the proposed method.}
}
@article{ZU2023419,
title = {A reinforcement learning algorithm acquires demonstration from the training agent by dividing the task space},
journal = {Neural Networks},
volume = {164},
pages = {419-427},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.042},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002289},
author = {Lipeng Zu and Xiao He and Jia Yang and Lianqing Liu and Wenxue Wang},
keywords = {Reinforcement learning, Self-imitation learning, Task space division, Sparse reward function, Robotic grasping},
abstract = {Although reinforcement learning (RL) has made numerous breakthroughs in recent years, addressing reward-sparse environments remains challenging and requires further exploration. Many studies improve the performance of the agents by introducing the state-action pairs experienced by an expert. However, such kinds of strategies almost depend on the quality of the demonstration by the expert, which is rarely optimal in a real-world environment, and struggle with learning from sub-optimal demonstrations. In this paper, a self-imitation learning algorithm based on the task space division is proposed to realize an efficient high-quality demonstration acquire while the training process. To determine the quality of the trajectory, some well-designed criteria are defined in the task space for finding a better demonstration. The results show that the proposed algorithm will improve the success rate of robot control and achieve a high mean Q value per step. The algorithm framework proposed in this paper has illustrated a great potential to learn from a demonstration generated by using self-policy in sparse environments and can be used in reward-sparse environments where the task space can be divided.}
}
@article{PARK2023335,
title = {Prospective classification of Alzheimer’s disease conversion from mild cognitive impairment},
journal = {Neural Networks},
volume = {164},
pages = {335-344},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002046},
author = {Sunghong Park and Chang Hyung Hong and Dong-gi Lee and Kanghee Park and Hyunjung Shin},
keywords = {Alzheimer’s disease, Mild cognitive impairment, Disease progression, Brain MRI, Prospective classification},
abstract = {Alzheimer’s disease (AD) is emerging as a serious problem with the rapid aging of the population, but due to the unclear cause of the disease and the absence of therapy, appropriate preventive measures are the next best thing. For this reason, it is important to early detect whether the disease converts from mild cognitive impairment (MCI) which is a prodromal phase of AD. With the advance in brain imaging techniques, various machine learning algorithms have become able to predict the conversion from MCI to AD by learning brain atrophy patterns. However, at the time of diagnosis, it is difficult to distinguish between the conversion group and the non-conversion group of subjects because the difference between groups is small, but the within-group variability is large in brain images. After a certain period of time, the subjects of conversion group show significant brain atrophy, whereas subjects of non-conversion group show only subtle changes due to the normal aging effect. This difference on brain atrophy makes the brain images more discriminative for learning. Motivated by this, we propose a method to perform classification by projecting brain images into the future, namely prospective classification. The experiments on the Alzheimer’s Disease Neuroimaging Initiative dataset show that the prospective classification outperforms ordinary classification. Moreover, the features of prospective classification indicate the brain regions that significantly influence the conversion from MCI to AD.}
}
@article{DENG202331,
title = {Auditory perception architecture with spiking neural network and implementation on FPGA},
journal = {Neural Networks},
volume = {165},
pages = {31-42},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.026},
url = {https://www.sciencedirect.com/science/article/pii/S089360802300271X},
author = {Bin Deng and Yanrong Fan and Jiang Wang and Shuangming Yang},
keywords = {Perception system, Field-programmable gate array (FPGA), Neuromorphic engineering, Brain-inspired computing, Large-scale spiking neural network (SNN)},
abstract = {Spike-based perception brings up a new research idea in the field of neuromorphic engineering. A high-performance biologically inspired flexible spiking neural network (SNN) architecture provides a novel method for the exploration of perception mechanisms and the development of neuromorphic computing systems . In this article, we present a biological-inspired spike-based SNN perception digital system that can realize robust perception. The system employs a fully paralleled pipeline scheme to improve the performance and accelerate the processing of feature extraction. An auditory perception system prototype is realized on ten Intel Cyclone field-programmable gate arrays, which can reach the maximum frequency of 107.28 MHz and the maximum throughput of 5364 Mbps. Our design also achieves the power of 5. 148 W/system and energy efficiency of 845.85 μJ. Our auditory perception implementation is also proved to have superior robustness compared with other SNN systems. We use TIMIT digit speech in noise in accuracy testing. Result shows that it achieves up to 85.75% speech recognition accuracy under obvious noise conditions (signal-to-noise ratio of 20 dB) and maintain small accuracy attenuation with the decline of the signal-to-noise ratio. The overall performance of our proposed system outperforms the state-of-the-art perception system on SNN.}
}
@article{EINIZADE2023667,
title = {ProductGraphSleepNet: Sleep staging using product spatio-temporal graph learning with attentive temporal aggregation},
journal = {Neural Networks},
volume = {164},
pages = {667-680},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002605},
author = {Aref Einizade and Samaneh Nasiri and Sepideh Hajipour Sardouie and Gari D. Clifford},
keywords = {Graph convolutional neural (GCN) network, Product graph learning (PGL), Graph signal processing (GSP), Sleep staging, Brain connectivity},
abstract = {The classification of sleep stages plays a crucial role in understanding and diagnosing sleep pathophysiology. Sleep stage scoring relies heavily on visual inspection by an expert, which is a time-consuming and subjective procedure. Recently, deep learning neural network approaches have been leveraged to develop a generalized automated sleep staging and account for shifts in distributions that may be caused by inherent inter/intra-subject variability, heterogeneity across datasets, and different recording environments. However, these networks (mostly) ignore the connections among brain regions and disregard modeling the connections between temporally adjacent sleep epochs. To address these issues, this work proposes an adaptive product graph learning-based graph convolutional network, named ProductGraphSleepNet, for learning joint spatio-temporal graphs along with a bidirectional gated recurrent unit and a modified graph attention network to capture the attentive dynamics of sleep stage transitions. Evaluation on two public databases: the Montreal Archive of Sleep Studies (MASS) SS3; and the SleepEDF, which contain full night polysomnography recordings of 62 and 20 healthy subjects, respectively, demonstrates performance comparable to the state-of-the-art (Accuracy: 0.867;0.838, F1-score: 0.818;0.774 and Kappa: 0.802;0.775, on each database respectively). More importantly, the proposed network makes it possible for clinicians to comprehend and interpret the learned spatial and temporal connectivity graphs for sleep stages.}
}
@article{ZHU202338,
title = {Imitating the oracle: Towards calibrated model for class incremental learning},
journal = {Neural Networks},
volume = {164},
pages = {38-48},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001879},
author = {Fei Zhu and Zhen Cheng and Xu-Yao Zhang and Cheng-Lin Liu},
keywords = {Class incremental learning, Continual learning, Lifelong learning},
abstract = {Class-incremental learning (CIL) aims to recognize classes that emerged in different phases. The joint-training (JT), which trains the model jointly with all classes, is often considered as the upper bound of CIL. In this paper, we thoroughly analyze the difference between CIL and JT in feature space and weight space. Motivated by the comparative analysis, we propose two types of calibration: feature calibration and weight calibration to imitate the oracle (ItO), i.e., JT. Specifically, on the one hand, feature calibration introduces deviation compensation to maintain the class decision boundary of old classes in feature space. On the other hand, weight calibration leverages forgetting-aware weight perturbation to increase transferability and reduce forgetting in parameter space. With those two calibration strategies, the model is forced to imitate the properties of joint-training at each incremental learning stage, thus yielding better CIL performance. Our ItO is a plug-and-play method and can be implemented into existing methods easily. Extensive experiments on several benchmark datasets demonstrate that ItO can significantly and consistently improve the performance of existing state-of-the-art methods. Our code is publicly available at https://github.com/Impression2805/ItO4CIL.}
}
@article{KONISHI2023393,
title = {Stable invariant models via Koopman spectra},
journal = {Neural Networks},
volume = {165},
pages = {393-405},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.040},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002848},
author = {Takuya Konishi and Yoshinobu Kawahara},
keywords = {Neural networks, Deep learning, Dynamical systems, Spectral analysis},
abstract = {Weight-tied models have attracted attention in the modern development of neural networks. The deep equilibrium model (DEQ) represents infinitely deep neural networks with weight-tying, and recent studies have shown the potential of this type of approach. DEQs are needed to iteratively solve root-finding problems in training and are built on the assumption that the underlying dynamics determined by the models converge to a fixed point. In this paper, we present the stable invariant model (SIM), a new class of deep models that in principle approximates DEQs under stability and extends the dynamics to more general ones converging to an invariant set (not restricted in a fixed point). The key ingredient in deriving SIMs is a representation of the dynamics with the spectra of the Koopman and Perron–Frobenius operators. This perspective approximately reveals stable dynamics with DEQs and then derives two variants of SIMs. We also propose an implementation of SIMs that can be learned in the same way as feedforward models. We illustrate the empirical performance of SIMs with experiments and demonstrate that SIMs achieve comparative or superior performance against DEQs in several learning tasks.}
}
@article{QI2023489,
title = {An adaptive reinforcement learning-based multimodal data fusion framework for human–robot confrontation gaming},
journal = {Neural Networks},
volume = {164},
pages = {489-496},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.043},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002290},
author = {Wen Qi and Haoyu Fan and Hamid Reza Karimi and Hang Su},
keywords = {Reinforcement learning, Multimodal data fusion, Human–robot confrontation, Adaptive learning, Multiple sensors fusion, Hand Gesture Recognition},
abstract = {Playing games between humans and robots have become a widespread human–robot confrontation (HRC) application. Although many approaches were proposed to enhance the tracking accuracy by combining different information, the problems of the intelligence degree of the robot and the anti-interference ability of the motion capture system still need to be solved. In this paper, we present an adaptive reinforcement learning (RL) based multimodal data fusion (AdaRL-MDF) framework teaching the robot hand to play Rock–Paper–Scissors (RPS) game with humans. It includes an adaptive learning mechanism to update the ensemble classifier, an RL model providing intellectual wisdom to the robot, and a multimodal data fusion structure offering resistance to interference. The corresponding experiments prove the mentioned functions of the AdaRL-MDF model. The comparison accuracy and computational time show the high performance of the ensemble model by combining k-nearest neighbor (k-NN) and deep convolutional neural network (DCNN). In addition, the depth vision-based k-NN classifier obtains a 100% identification accuracy so that the predicted gestures can be regarded as the real value. The demonstration illustrates the real possibility of HRC application. The theory involved in this model provides the possibility of developing HRC intelligence.}
}
@article{LONGO2023721,
title = {De Rham compatible Deep Neural Network FEM},
journal = {Neural Networks},
volume = {165},
pages = {721-739},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023003088},
author = {Marcello Longo and Joost A.A. Opschoor and Nico Disch and Christoph Schwab and Jakob Zech},
keywords = {De Rham complex, Finite Elements, Lavrentiev gap, Neural networks, PINNs},
abstract = {On general regular simplicial partitions T of bounded polytopal domains Ω⊂Rd, d∈{2,3}, we construct exact neural network (NN) emulations of all lowest order finite element spaces in the discrete de Rham complex. These include the spaces of piecewise constant functions, continuous piecewise linear (CPwL) functions, the classical “Raviart–Thomas element”, and the “Nédélec edge element”. For all but the CPwL case, our network architectures employ both ReLU (rectified linear unit) and BiSU (binary step unit) activations to capture discontinuities. In the important case of CPwL functions, we prove that it suffices to work with pure ReLU nets. Our construction and DNN architecture generalizes previous results in that no geometric restrictions on the regular simplicial partitions T of Ω are required for DNN emulation. In addition, for CPwL functions our DNN construction is valid in any dimension d≥2. Our “FE-Nets” are required in the variationally correct, structure-preserving approximation of boundary value problems of electromagnetism in nonconvex polyhedra Ω⊂R3. They are thus an essential ingredient in the application of e.g., the methodology of “physics-informed NNs” or “deep Ritz methods” to electromagnetic field simulation via deep learning techniques. We indicate generalizations of our constructions to higher-order compatible spaces and other, non-compatible classes of discretizations, in particular the “Crouzeix–Raviart” elements and Hybridized, Higher Order (HHO) methods.}
}
@article{HUA202321,
title = {Basis operator network: A neural network-based model for learning nonlinear operators via neural basis},
journal = {Neural Networks},
volume = {164},
pages = {21-37},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002034},
author = {Ning Hua and Wenlian Lu},
keywords = {Neural networks, Operator regression, Universal approximation theorems},
abstract = {It is widely acknowledged that neural networks can approximate any continuous (even measurable) functions between finite-dimensional Euclidean spaces to arbitrary accuracy. Recently, the use of neural networks has started emerging in infinite-dimensional settings. Universal approximation theorems of operators guarantee that neural networks can learn mappings between infinite-dimensional spaces. In this paper, we propose a neural network-based method (BasisONet) capable of approximating mappings between function spaces. To reduce the dimension of an infinite-dimensional space, we propose a novel function autoencoder that can compress the function data. Our model can predict the output function at any resolution using the corresponding input data at any resolution once trained. Numerical experiments demonstrate that the performance of our model is competitive with existing methods on the benchmarks, and our model can address the data on a complex geometry with high precision. We further analyze some notable characteristics of our model based on the numerical results.}
}
@article{PARK2023546,
title = {Generating post-hoc explanations for Skip-gram-based node embeddings by identifying important nodes with bridgeness},
journal = {Neural Networks},
volume = {164},
pages = {546-561},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.029},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002150},
author = {Hogun Park and Jennifer Neville},
keywords = {Node representation learning, Explanation},
abstract = {Node representation learning in a network is an important machine learning technique for encoding relational information in a continuous vector space while preserving the inherent properties and structures of the network. Recently, unsupervised node embedding methods such as DeepWalk (Perozzi et al., 2014), LINE (Tang et al., 2015), struc2vec (Ribeiro et al., 2017), PTE (Tang et al., 2015), UserItem2vec (Wu et al., 2020), and RWJBG (Li et al., 2021) have emerged from the Skip-gram model (Mikolov et al., 2013) and perform better performance in several downstream tasks such as node classification and link prediction than the existing relational models. However, providing post-hoc explanations of unsupervised embeddings remains a challenging problem because of the lack of explanation methods and theoretical studies applicable for embeddings. In this paper, we first show that global explanations to the Skip-gram-based embeddings can be found by computing bridgeness under a spectral cluster-aware local perturbation. Moreover, a novel gradient-based explanation method, which we call GRAPH-wGD, is proposed that allows the top-q global explanations about learned graph embedding vectors more efficiently. Experiments show that the ranking of nodes by scores using GRAPH-wGD is highly correlated with true bridgeness scores. We also observe that the top-q node-level explanations selected by GRAPH-wGD have higher importance scores and produce more changes in class label prediction when perturbed, compared with the nodes selected by recent alternatives, using five real-world graphs.}
}
@article{WEI2023719,
title = {LoyalDE: Improving the performance of Graph Neural Networks with loyal node discovery and emphasis},
journal = {Neural Networks},
volume = {164},
pages = {719-730},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.023},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002678},
author = {Haotong Wei and Yinlin Zhu and Xunkai Li and Bin Jiang},
keywords = {Graph Neural Networks, Semi-supervised Node Classification, Graph Supervision Loyalty},
abstract = {Recent years have witnessed an increasing focus on graph-based semi-supervised learning with Graph Neural Networks (GNNs). Despite existing GNNs having achieved remarkable accuracy, research on the quality of graph supervision information has inadvertently been ignored. In fact, there are significant differences in the quality of supervision information provided by different labeled nodes, and treating supervision information with different qualities equally may lead to sub-optimal performance of GNNs. We refer to this as the graph supervision loyalty problem, which is a new perspective for improving the performance of GNNs. In this paper, we devise FT-Score to quantify node loyalty by considering both the local feature similarity and the local topology similarity, and nodes with higher loyalty are more likely to provide higher-quality supervision. Based on this, we propose LoyalDE (Loyal Node Discovery and Emphasis), a model-agnostic hot-plugging training strategy, which can discover potential nodes with high loyalty to expand the training set, and then emphasize nodes with high loyalty during model training to improve performance. Experiments demonstrate that the graph supervision loyalty problem will fail most existing GNNs. In contrast, LoyalDE brings about at most 9.1% performance improvement to vanilla GNNs and consistently outperforms several state-of-the-art training strategies for semi-supervised node classification.}
}
@article{TSANTEKIDIS2023506,
title = {Modeling limit order trading with a continuous action policy for deep reinforcement learning},
journal = {Neural Networks},
volume = {165},
pages = {506-515},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.051},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002952},
author = {Avraam Tsantekidis and Nikolaos Passalis and Anastasios Tefas},
keywords = {Financial trading, Limit orders, Policy gradient, Deep reinforcement learning},
abstract = {Limit Orders allow buyers and sellers to set a “limit price” they are willing to accept in a trade. On the other hand, market orders allow for immediate execution at any price. Thus, market orders are susceptible to slippage, which is the additional cost incurred due to the unfavorable execution of a trade order. As a result, limit orders are often preferred, since they protect traders from excessive slippage costs due to larger than expected price fluctuations. Despite the price guarantees of limit orders, they are more complex compared to market orders. Orders with overly optimistic limit prices might never be executed, which increases the risk of employing limit orders in Machine Learning (ML)-based trading systems. Indeed, the current ML literature for trading almost exclusively relies on market orders. To overcome this limitation, a Deep Reinforcement Learning (DRL) approach is proposed to model trading agents that use limit orders. The proposed method (a) uses a framework that employs a continuous probability distribution to model limit prices, while (b) provides the ability to place market orders when the risk of no execution is more significant than the cost of slippage. Extensive experiments are conducted with multiple currency pairs, using hourly price intervals, validating the effectiveness of the proposed method and paving the way for introducing limit order modeling in DRL-based trading.}
}
@article{WU2023435,
title = {Novel adaptive zeroing neural dynamics schemes for temporally-varying linear equation handling applied to arm path following and target motion positioning},
journal = {Neural Networks},
volume = {165},
pages = {435-450},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.056},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023003003},
author = {Wenqi Wu and Yunong Zhang},
keywords = {Adaptive continuous zeroing neural dynamics (ACZND), Adaptive discrete zeroing neural dynamics (ADZND), Temporally-varying linear equation (TVLE), Path following, Target motion positioning},
abstract = {While the handling for temporally-varying linear equation (TVLE) has received extensive attention, most methods focused on trading off the conflict between computational precision and convergence rate. Different from previous studies, this paper proposes two complete adaptive zeroing neural dynamics (ZND) schemes, including a novel adaptive continuous ZND (ACZND) model, two general variable time discretization techniques, and two resultant adaptive discrete ZND (ADZND) algorithms, to essentially eliminate the conflict. Specifically, an error-related varying-parameter ACZND model with global and exponential convergence is first designed and proposed. To further adapt to the digital hardware, two novel variable time discretization techniques are proposed to discretize the ACZND model into two ADZND algorithms. The convergence properties with respect to the convergence rate and precision of ADZND algorithms are proved via rigorous mathematical analyses. By comparing with the traditional discrete ZND (TDZND) algorithms, the superiority of ADZND algorithms in convergence rate and computational precision is shown theoretically and experimentally. Finally, simulative experiments, including numerical experiments on a specific TVLE solving as well as four application experiments on arm path following and target motion positioning are successfully conducted to substantiate the efficacy, superiority, and practicability of ADZND algorithms.}
}
@article{NI202360,
title = {Cross-modal hashing with missing labels},
journal = {Neural Networks},
volume = {165},
pages = {60-76},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.035},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002794},
author = {Haomin Ni and Jianjun Zhang and Peipei Kang and Xiaozhao Fang and Weijun Sun and Shengli Xie and Na Han},
keywords = {Cross-modal retrieval, Hashing method, Weak supervision, Missing labels},
abstract = {Hashing-based cross-modal retrieval methods have become increasingly popular due to their advantages in storage and speed. While current methods have demonstrated impressive results, there are still several issues that have not been addressed. Specifically, many of these approaches assume that labels are perfectly assigned, despite the fact that in real-world scenarios, labels are often incomplete or partially missing. There are two reasons for this, as manual labeling can be a complex and time-consuming task, and annotators may only be interested in certain objects. As such, cross-modal retrieval with missing labels is a significant challenge that requires further attention. Moreover, the similarity between labels is frequently ignored, which is important for exploring the high-level semantics of labels. To address these limitations, we propose a novel method called Cross-Modal Hashing with Missing Labels (CMHML). Our method consists of several key components. First, we introduce Reliable Label Learning to preserve reliable information from the observed labels. Next, to infer the uncertain part of the predicted labels, we decompose the predicted labels into latent representations of labels and samples. The representation of samples is extracted from different modalities, which assists in inferring missing labels. We also propose Label Correlation Preservation to enhance the similarity between latent representations of labels. Hash codes are then learned from the representation of samples through Global Approximation Learning. We also construct a similarity matrix according to predicted labels and embed it into hash codes learning to explore the value of labels. Finally, we train linear classifiers to map original samples to a low-dimensional Hamming space. To evaluate the efficacy of CMHML, we conduct extensive experiments on four publicly available datasets. Our method is compared to other state-of-the-art methods, and the results demonstrate that our model performs competitively even when most labels are missing.}
}
@article{LI2023323,
title = {Capsule neural tensor networks with multi-aspect information for Few-shot Knowledge Graph Completion},
journal = {Neural Networks},
volume = {164},
pages = {323-334},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.041},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002277},
author = {Qianyu Li and Jiale Yao and Xiaoli Tang and Han Yu and Siyu Jiang and Haizhi Yang and Hengjie Song},
keywords = {Few-shot knowledge graph completion, Few-shot learning, Knowledge graph, Capsule network, Neural tensor network},
abstract = {Few-shot Knowledge Graph Completion (FKGC) has recently attracted significant research interest due to its ability to expand few-shot relation coverage in Knowledge Graphs. Prevailing FKGC approaches focus on exploiting the one-hop neighbor information of entities to enhance few-shot relation embedding. However, these methods select one-hop neighbors randomly and neglect the rich multi-aspect information of entities. Although some methods have attempted to leverage Long Short-Term Memory (LSTM) to learn few-shot relation embedding, they are sensitive to the input order. To address these limitations, we propose the Capsule Neural Tensor Networks with Multi-Aspect Information approach (short for InforMix-FKGC). InforMix-FKGC employs a one-hop neighbor selection strategy based on how valuable they are and encodes multi-aspect information of entities, including one-hop neighbors, attributes and literal description. Then, a capsule network is responsible for integrating the support set and deriving few-shot relation embedding. Moreover, a neural tensor network is used to match the query set with the support set. In this way, InforMix-FKGC can learn few-shot relation embedding more precisely so as to enhance the accuracy of FKGC. Extensive experiments on the NELL-One and Wiki-One datasets demonstrate that InforMix-FKGC significantly outperforms ten state-of-the-art methods in terms of Mean Reciprocal Rank and Hits@K.}
}
@article{LV202394,
title = {3D graph neural network with few-shot learning for predicting drug–drug interactions in scaffold-based cold start scenario},
journal = {Neural Networks},
volume = {165},
pages = {94-105},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.05.039},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002836},
author = {Qiujie Lv and Jun Zhou and Ziduo Yang and Haohuai He and Calvin Yu-Chian Chen},
keywords = {3D, Graph neural network, Cold start, Few-shot learning, Drug–drug interactions},
abstract = {Understanding drug–drug interactions (DDI) of new drugs is critical for minimizing unexpected adverse drug reactions. The modeling of new drugs is called a cold start scenario. In this scenario, Only a few structural information or physicochemical information about new drug is available. The 3D conformation of drug molecules usually plays a crucial role in chemical properties compared to the 2D structure. 3D graph network with few-shot learning is a promising solution. However, the 3D heterogeneity of drug molecules and the discretization of atomic distributions lead to spatial confusion in few-shot learning. Here, we propose a 3D graph neural network with few-shot learning, Meta3D-DDI, to predict DDI events in cold start scenario. The 3DGNN ensures rotation and translation invariance by calculating atomic pairwise distances, and incorporates 3D structure and distance information in the information aggregation stage. The continuous filter interaction module can continuously simulate the filter to obtain the interaction between the target atom and other atoms. Meta3D-DDI further develops a FSL strategy based on bilevel optimization to transfer meta-knowledge for DDI prediction tasks from existing drugs to new drugs. In addition, the existing cold start setting may cause the scaffold structure information in the training set to leak into the test set. We design scaffold-based cold start scenario to ensure that the drug scaffolds in the training set and test set do not overlap. The extensive experiments demonstrate that our architecture achieves the SOTA performance for DDI prediction under scaffold-based cold start scenario on two real-world datasets. The visual experiment shows that Meta3D-DDI significantly improves the learning for DDI prediction of new drugs. We also demonstrate how Meta3D-DDI can reduce the amount of data required to make meaningful DDI predictions.}
}
@article{CHEN2023521,
title = {A transformer-based deep neural network model for SSVEP classification},
journal = {Neural Networks},
volume = {164},
pages = {521-534},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.045},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002319},
author = {Jianbo Chen and Yangsong Zhang and Yudong Pan and Peng Xu and Cuntai Guan},
keywords = {Brain–computer interface, Steady-state visual evoked potential, Transformer, Deep learning, Filter bank},
abstract = {Steady-state visual evoked potential (SSVEP) is one of the most commonly used control signals in the brain–computer interface (BCI) systems. However, the conventional spatial filtering methods for SSVEP classification highly depend on the subject-specific calibration data. The need for the methods that can alleviate the demand for the calibration data becomes urgent. In recent years, developing the methods that can work in inter-subject scenario has become a promising new direction. As a popular deep learning model nowadays, Transformer has been used in EEG signal classification tasks owing to its excellent performance. Therefore, in this study, we proposed a deep learning model for SSVEP classification based on Transformer architecture in inter-subject scenario, termed as SSVEPformer, which was the first application of Transformer on the SSVEP classification. Inspired by previous studies, we adopted the complex spectrum features of SSVEP data as the model input, which could enable the model to simultaneously explore the spectral and spatial information for classification. Furthermore, to fully utilize the harmonic information, an extended SSVEPformer based on the filter bank technology (FB-SSVEPformer) was proposed to improve the classification performance. Experiments were conducted using two open datasets (Dataset 1: 10 subjects, 12 targets; Dataset 2: 35 subjects, 40 targets). The experimental results show that the proposed models could achieve better results in terms of classification accuracy and information transfer rate than other baseline methods. The proposed models validate the feasibility of deep learning models based on Transformer architecture for SSVEP data classification, and could serve as potential models to alleviate the calibration procedure in the practical application of SSVEP-based BCI systems.}
}
@article{HU2023245,
title = {Co-attention enabled content-based image retrieval},
journal = {Neural Networks},
volume = {164},
pages = {245-263},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001867},
author = {Zechao Hu and Adrian G. Bors},
keywords = {Content-based image retrieval, Co-attention, Clustering},
abstract = {Content-based image retrieval (CBIR) aims to provide the most similar images to a given query. Feature extraction plays an essential role in retrieval performance within a CBIR pipeline. Current CBIR studies would either uniformly extract feature information from the input image and use it directly or employ some trainable spatial weighting module which is then used for similarity comparison between pairs of query and candidate matching images. These spatial weighting modules are normally query non-sensitive and only based on the knowledge learned during the training stage. They may focus towards incorrect regions, especially when the target image is not salient or is surrounded by distractors. This paper proposes an efficient query sensitive co-attention11“Co-attention” in this paper refers to spatial attention conditioned on the query content. mechanism for large-scale CBIR tasks. In order to reduce the extra computation cost required by the query sensitivity to the co-attention mechanism, the proposed method employs clustering of the selected local features. Experimental results indicate that the co-attention maps can provide the best retrieval results on benchmark datasets under challenging situations, such as having completely different image acquisition conditions between the query and its match image.}
}
@article{LI2023228,
title = {Finite-time cluster synchronization for complex dynamical networks under FDI attack: A periodic control approach},
journal = {Neural Networks},
volume = {165},
pages = {228-237},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001909},
author = {Jun-Yi Li and Yang-Cheng Huang and Hong-Xia Rao and Yong Xu and Renquan Lu},
keywords = {Complex dynamical network, Cluster synchronization, False data injection attack, Periodic secure control},
abstract = {In this paper, the finite-time cluster synchronization problem is addressed for complex dynamical networks (CDNs) with cluster characteristics under false data injection (FDI) attacks. A type of FDI attack is taken into consideration to reflect the data manipulation that controllers in CDNs may suffer. In order to improve the synchronization effect while reducing the control cost, a new periodic secure control (PSC) strategy is proposed in which the set of pinning nodes changes periodically. The aim of this paper is to derive the gains of the periodic secure controller such that the synchronization error of the CDN remains at a certain threshold in finite time with the presence of external disturbances and false control signals simultaneously. Through considering the periodic characteristics of PSC, a sufficient condition is obtained to guarantee the desired cluster synchronization performance, based on which the gains of the periodic cluster synchronization controllers are acquired by resolving an optimization problem proposed in this paper. A numerical case is carried out to validate the cluster synchronization performance of the PSC strategy under cyber attacks.}
}