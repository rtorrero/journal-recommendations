@article{ZHANG2020146,
title = {Adaptive tracking synchronization for coupled reaction–diffusion neural networks with parameter mismatches},
journal = {Neural Networks},
volume = {124},
pages = {146-157},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.025},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019304253},
author = {Hao Zhang and Zhixia Ding and Zhigang Zeng},
keywords = {Tracking synchronization, Reaction–diffusion neural network, Adaptive control, Parameter mismatch},
abstract = {In this paper, tracking synchronization for coupled reaction–diffusion neural networks with parameter mismatches is investigated. For such a networked control system, only local neighbor information is used to compensate the mismatch characteristic termed as parameter mismatch, uncertainty or external disturbance. Different from the general boundedness hypothesis, the parameter mismatches are permitted to be unbounded. For the known parameter mismatches, parameter-dependent controller and parameter-independent adaptive controller are respectively designed. While for fully unknown network parameters and parameter mismatches, a distributed adaptive controller is proposed. By means of partial differential equation theories and differential inequality techniques, the tracking synchronization errors driven by these nonlinear controllers are proved to be uniformly ultimately bounded and exponentially convergent to some adjustable bounded domains. Finally, three numerical examples are given to test the effectiveness of the proposed controllers.}
}
@article{YAO2020299,
title = {Liver disease screening based on densely connected deep neural networks},
journal = {Neural Networks},
volume = {123},
pages = {299-304},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303442},
author = {Zhenjie Yao and Jiangong Li and Zhaoyu Guan and Yancheng Ye and Yixin Chen},
keywords = {Dense connected, DNN, Liver disease, Liver function tests},
abstract = {Liver disease is an important public health problem. Liver Function Tests (LFT) is the most achievable test for liver disease diagnosis. Most liver diseases are manifested as abnormal LFT. Liver disease screening by LFT data is helpful for computer aided diagnosis. In this paper, we propose a densely connected deep neural network (DenseDNN), on 13 most commonly used LFT indicators and demographic information of subjects for liver disease screening. The algorithm was tested on a dataset of 76,914 samples (more than 100 times of data than the previous datasets). The Area Under Curve (AUC) of DenseDNN is 0.8919, that of DNN is 0.8867, that of random forest is 0.8790, and that of logistic regression is 0.7974. The performance of deep learning models are significantly better than conventional methods. As for the deep learning methods, DenseDNN shows better performance than DNN.}
}
@article{WU202086,
title = {Multi-context aware user–item embedding for recommendation},
journal = {Neural Networks},
volume = {124},
pages = {86-94},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300101},
author = {Biao Wu and Wen Wen and Zhifeng Hao and Ruichu Cai},
keywords = {Recommender systems, Representation learning, Embedding-based model, Auxiliary information},
abstract = {Real recommender systems usually contain various auxiliary information. Some of the most recent works make meaningful exploration of incorporating auxiliary information into the representation model for competitive recommendation. However, learning user and item representations still faces two challenges: (1) existing works do not well address the problem of integrating multi-type auxiliary information; (2) learning representations for inactive users is still challenging due to the high sparsity of explicit user–item associations. In order to tackle these problems, in this paper, the attributed heterogeneous network and bipartite interaction network are employed to incorporate various auxiliary information and user–item associations. A joint objective function and an efficient algorithm are devised for the representation learning. Experimental results show that the proposed algorithm has significant advantages over the state-of-the-art baselines. What is remarkable is that our proposed method is demonstrated to be especially useful for dealing with low-active users in the system.}
}
@article{BALLESTEROS2020153,
title = {Robust min–max optimal control design for systems with uncertain models: A neural dynamic programming approach},
journal = {Neural Networks},
volume = {125},
pages = {153-164},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300186},
author = {Mariana Ballesteros and Isaac Chairez and Alexander Poznyak},
keywords = {Approximate dynamic-programming, Artificial neural networks, Hamilton–Jacobi–Bellman equation, Bellman function, Sub-optimal controller},
abstract = {The design of an artificial neural network (ANN) based sub-optimal controller to solve the finite-horizon optimization problem for a class of systems with uncertainties is the main outcome of this study. The optimization problem considers a convex performance index in the Bolza form. The dynamic uncertain restriction is considered as a linear system affected by modeling uncertainties, as well as by external bounded perturbations. The proposed controller implements a min–max approach based on the dynamic neural programming approximate solution. An ANN approximates the Value function to get the estimate of the Hamilton–Jacobi–Bellman (HJB) equation solution. The explicit adaptive law for the weights in the ANN is obtained from the approximation of the HJB solution. The stability analysis based on the Lyapunov theory yields to confirm that the approximate Value function serves as a Lyapunov function candidate and to conclude the practical stability of the equilibrium point. A simulation example illustrates the characteristics of the sub-optimal controller. The comparison of the performance indexes obtained with the application of different controllers evaluates the effect of perturbations and the sub-optimal solution.}
}
@article{WAN2020261,
title = {Robust face alignment by cascaded regression and de-occlusion},
journal = {Neural Networks},
volume = {123},
pages = {261-272},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019304010},
author = {Jun Wan and Jing Li and Zhihui Lai and Bo Du and Lefei Zhang},
keywords = {Partial occlusion, Deep regression, Face de-occlusion, Heatmap, Generative adversarial network},
abstract = {Face alignment is a typical facial behavior analysis task in computer vision. However, the performance of face alignment is degraded greatly when the face image is partially occluded. In order to achieve better mapping between facial appearance features and shape increments, we propose a robust and occlusion-free face alignment algorithm in which a face de-occlusion module and a deep regression module are integrated into a cascaded deep generative regression model. The face de-occlusion module is a disentangled representation learning Generative Adversarial Networks (GANs) which aims to locate occlusions and recover the genuine appearance from partially occluded face image. The deep regression module can enhance facial appearance representation by utilizing the recovered faces to obtain more accurate regressors. Then, by the cascaded deep generative regression model, we recover the partially occluded face image and achieve accurate locating of landmarks gradually. It is interesting to show that the cascaded deep generative regression model can effectively locate occlusions and recover more genuine faces, which can be further used to improve the performance of face alignment. Experimental results conducted on four challenging occluded face datasets demonstrate that our method outperforms state-of-the-art methods.}
}
@article{LI202075,
title = {A 3D deep supervised densely network for small organs of human temporal bone segmentation in CT images},
journal = {Neural Networks},
volume = {124},
pages = {75-85},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300071},
author = {Xiaoguang Li and Zhaopeng Gong and Hongxia Yin and Hui Zhang and Zhenchang Wang and Li Zhuo},
keywords = {Computed tomography imaging analysis, Temporal bone, Deep supervised densely network},
abstract = {Computed Tomography (CT) has become an important way for examining the critical anatomical organs of the human temporal bone in the diagnosis and treatment of ear diseases. Segmentation of the critical anatomical organs is an important fundamental step for the computer assistant analysis of human temporal bone CT images. However, it is challenging to segment sophisticated and small organs. To deal with this issue, a novel 3D Deep Supervised Densely Network (3D-DSD Net) is proposed in this paper. The network adopts a dense connection design and a 3D multi-pooling feature fusion strategy in the encoding stage of the 3D-Unet, and a 3D deep supervised mechanism is employed in the decoding stage. The experimental results show that our method achieved competitive performance in the CT data segmentation task of the small organs in the temporal bone.}
}
@article{KANG2020279,
title = {Partition level multiview subspace clustering},
journal = {Neural Networks},
volume = {122},
pages = {279-288},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303326},
author = {Zhao Kang and Xinjia Zhao and Chong Peng and Hongyuan Zhu and Joey Tianyi Zhou and Xi Peng and Wenyu Chen and Zenglin Xu},
keywords = {Multi-view learning, Subspace clustering, Partition space, Information fusion},
abstract = {Multiview clustering has gained increasing attention recently due to its ability to deal with multiple sources (views) data and explore complementary information between different views. Among various methods, multiview subspace clustering methods provide encouraging performance. They mainly integrate the multiview information in the space where the data points lie. Hence, their performance may be deteriorated because of noises existing in each individual view or inconsistent between heterogeneous features. For multiview clustering, the basic premise is that there exists a shared partition among all views. Therefore, the natural space for multiview clustering should be all partitions. Orthogonal to existing methods, we propose to fuse multiview information in partition level following two intuitive assumptions: (i) each partition is a perturbation of the consensus clustering; (ii) the partition that is close to the consensus clustering should be assigned a large weight. Finally, we propose a unified multiview subspace clustering model which incorporates the graph learning from each view, the generation of basic partitions, and the fusion of consensus partition. These three components are seamlessly integrated and can be iteratively boosted by each other towards an overall optimal solution. Experiments on four benchmark datasets demonstrate the efficacy of our approach against the state-of-the-art techniques.}
}
@article{2022II,
title = {INN/ENNS/JNNS - Membership Applic. Form},
journal = {Neural Networks},
volume = {146},
pages = {II},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00502-5},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021005025}
}
@article{CAO2020217,
title = {CS-MRI reconstruction based on analysis dictionary learning and manifold structure regularization},
journal = {Neural Networks},
volume = {123},
pages = {217-233},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019304022},
author = {Jianxin Cao and Shujun Liu and Hongqing Liu and Hongwei Lu},
keywords = {CS-MRI, Analysis dictionary learning, Correlation of patches, Manifold structure regularization},
abstract = {Compressed sensing (CS) significantly accelerates magnetic resonance imaging (MRI) by allowing the exact reconstruction of image from highly undersampling k-space data. In this process, the high sparsity obtained by the learned dictionary and exploitation of correlation among patches are essential to the reconstructed image quality. In this paper, by a use of these two aspects, we propose a novel CS-MRI model based on analysis dictionary learning and manifold structure regularization (ADMS). Furthermore, a proper tight frame constraint is used to obtain an effective overcomplete analysis dictionary with a high sparsifying capacity. The constructed manifold structure regularization nonuniformly enforces the correlation of each group formed by similar patches, which is more consistent with the diverse nonlocal similarity in realistic images. The proposed model is efficiently solved by the alternating direction method of multipliers (ADMM), in which the fast algorithm for each sub-problem is separately developed. The experimental results demonstrate that main components in the proposed method contribute to the final reconstruction performance and the effectiveness of the proposed model.}
}
@article{KUMAR2020106,
title = {Effects of infinite occurrence of hybrid impulses with quasi-synchronization of parameter mismatched neural networks},
journal = {Neural Networks},
volume = {122},
pages = {106-116},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303296},
author = {Rakesh Kumar and Subir Das and Yang Cao},
keywords = {Neural networks, Quasi synchronization, Hybrid impulses, Parameter mismatch, Mixed time-varying delays},
abstract = {This article is deeply concerned with the effects of hybrid impulses on quasi-synchronization of neural networks with mixed time-varying delays and parameter mismatches. Hybrid impulses allow synchronizing as well as desynchronizing impulses in one impulsive sequence, so their infinite time occurrence with the system may destroy the synchronization process. Therefore, the effective hybrid impulsive controller has been designed to deal with the difficulties in achieving the quasi-synchronization under the effects of hybrid impulses, which occur all the time, but their density of occurrence gradually decrease. In addition, the new concepts of average impulsive interval and average impulsive gain have been applied to cope with the simultaneous existence of synchronizing and desynchronizing impulses. Based on the Lyapunov method together with the extended comparison principle and the formula of variation of parameters for mixed time-varying delayed impulsive system, the delay-dependent sufficient criteria of quasi-synchronization have been derived for two separate cases, viz., Ta<∞ and Ta=∞. Finally, the efficiency of the theoretical results has been illustrated by providing two numerical examples.}
}
@article{LU2020429,
title = {Spacial sampled-data control for H∞ output synchronization of directed coupled reaction–diffusion neural networks with mixed delays},
journal = {Neural Networks},
volume = {123},
pages = {429-440},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.026},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019304265},
author = {Binglong Lu and Haijun Jiang and Cheng Hu and Abdujelil Abdurahman},
keywords = {Reaction–diffusion neural network, Directed coupling, Mixed time delay,  output synchronization, Spacial sampled-data control},
abstract = {This work investigates the H∞ output synchronization (HOS) of the directed coupled reaction–diffusion (R–D) neural networks (NNs) with mixed delays. Firstly, a model of the directed state coupled R–D NNs is introduced, which not only contains some discrete and distributed time delays, but also obeys a mixed Dirichlet–Neumann boundary condition. Secondly, a spacial sampled-data controller is proposed to achieve the HOS of the considered networks. This type of controller can reduce the update rate in the process of control by measuring the state of networks at some fixed sampling points in the space region. Moreover, some criteria for the HOS are established by designing an appropriate Lyapunov functional, and some quantitative relations between diffusion coefficients, mixed delays, coupling strength and control parameters are given accurately by these criteria. Thirdly, the case of directed spatial diffusion coupled networks is also studied and, the following finding is obtained: the spatial diffusion coupling can suppress the HOS while the state coupling can promote it. Finally, one example is simulated as the verification of the theoretical results.}
}
@article{JIANG2020305,
title = {Efficient network architecture search via multiobjective particle swarm optimization based on decomposition},
journal = {Neural Networks},
volume = {123},
pages = {305-316},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303971},
author = {Jing Jiang and Fei Han and Qinghua Ling and Jie Wang and Tiange Li and Henry Han},
keywords = {Convolutional neural network, Neural architecture search, Multiobjective particle swarm optimization, Decomposition},
abstract = {The efforts devoted to manually increasing the width and depth of convolutional neural network (CNN) usually require a large amount of time and expertise. It has stimulated a rising demand of neural architecture search (NAS) over these years. However, most popular NAS approaches solely optimize for low prediction error without penalizing high structure complexity. To this end, this paper proposes MOPSO/D-Net, a CNN architecture search method with multiobjective particle swarm optimization based on decomposition (MOPSO/D). The main goal is to reformulate NAS as a multiobjective evolutionary optimization problem, where the optimal architecture is learned by minimizing two conflicting objectives, namely the error rate of classification and number of parameters of the network. Along with the hybrid binary encoding and adaptive penalty-based boundary intersection, an improved MOPSO/D is further proposed to solve the formulated multiobjective NAS and provide diverse tradeoff solutions. Experimental studies verify the effectiveness of MOPSO/D-Net compared with current manual and automated CNN generation methods. The proposed algorithm achieves impressive classification performance with a small number of parameters on each of two benchmark datasets, particularly, 0.4% error rate with 0.16M params on MNIST and 5.88% error rate with 8.1M params on CIFAR-10, respectively.}
}
@article{MHASKAR2020142,
title = {Dimension independent bounds for general shallow networks},
journal = {Neural Networks},
volume = {123},
pages = {142-152},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303454},
author = {H.N. Mhaskar},
keywords = {Shallow and deep networks, Dimension independent bounds, Out-of-sample extension, Tractability of integration},
abstract = {This paper proves an abstract theorem addressing in a unified manner two important problems in function approximation: avoiding curse of dimensionality and estimating the degree of approximation for out-of-sample extension in manifold learning. We consider an abstract (shallow) network that includes, for example, neural networks, radial basis function networks, and kernels on data defined manifolds used for function approximation in various settings. A deep network is obtained by a composition of the shallow networks according to a directed acyclic graph, representing the architecture of the deep network. In this paper, we prove dimension independent bounds for approximation by shallow networks in the very general setting of what we have called G-networks on a compact metric measure space, where the notion of dimension is defined in terms of the cardinality of maximal distinguishable sets, generalizing the notion of dimension of a cube or a manifold. Our techniques give bounds that improve without saturation with the smoothness of the kernel involved in an integral representation of the target function. In the context of manifold learning, our bounds provide estimates on the degree of approximation for an out-of-sample extension of the target function to the ambient space. One consequence of our theorem is that without the requirement of robust parameter selection, deep networks using a non-smooth activation function such as the ReLU, do not provide any significant advantage over shallow networks in terms of the degree of approximation alone.}
}
@article{WEN2020373,
title = {The feature extraction of resting-state EEG signal from amnestic mild cognitive impairment with type 2 diabetes mellitus based on feature-fusion multispectral image method},
journal = {Neural Networks},
volume = {124},
pages = {373-382},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.025},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300356},
author = {Dong Wen and Peng Li and Xiaoli Li and Zhenhao Wei and Yanhong Zhou and Huan Pei and Fengnian Li and Zhijie Bian and Lei Wang and Shimin Yin},
keywords = {Feature-fusion multispectral image, aMCI with T2DM, EEG signal, Convolutional neural network},
abstract = {Recently, combining feature extraction and classification method of electroencephalogram (EEG) signals has been widely used in identifying mild cognitive impairment. However, it remains unclear which feature of EEG signals is best effective in assessing amnestic mild cognitive impairment (aMCI) with type 2 diabetes mellitus (T2DM) when combining one classifier. This study proposed a novel feature extraction method of EEG signals named feature-fusion multispectral image method (FMIM) for diagnosis of aMCI with T2DM. The FMIM was integrated with convolutional neural network (CNN) to classify the processed multispectral image data. The results showed that FMIM could effectively identify aMCI with T2DM from the control group compared to existing multispectral image method (MIM), with improvements including the type and quantity of feature extraction. Meanwhile, part of the invalid calculation could be avoided during the classification process. In addition, the classification evaluation indexes were best under the combination of Alpha2-Beta1-Beta2 frequency bands in data set based on FMIM-1, and were also best under the combination of the Theta-Alpha1-Alpha2-Beta1-Beta2 frequency bands in data set based on FMIM-2. Therefore, FMIM can be used as an effective feature extraction method of aMCI with T2DM, and as a valuable biomarker in clinical applications.}
}
@article{YANG202060,
title = {Robust adaptation regularization based on within-class scatter for domain adaptation},
journal = {Neural Networks},
volume = {124},
pages = {60-74},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300113},
author = {Liran Yang and Ping Zhong},
keywords = {Domain adaptation, Robust loss, Within-class scatter, Joint distribution matching, Manifold regularization},
abstract = {In many practical applications, the assumption that the distributions of the data employed for training and test are identical is rarely valid, which would result in a rapid decline in performance. To address this problem, the domain adaptation strategy has been developed in recent years. In this paper, we propose a novel unsupervised domain adaptation method, referred to as Robust Adaptation Regularization based on Within-Class Scatter (WCS-RAR), to simultaneously optimize the regularized loss, the within-class scatter, the joint distribution between domains, and the manifold consistency. On the one hand, to make the model robust against outliers, we adopt an l2,1-norm based loss function in virtue of its row sparsity, instead of the widely-used l2-norm based squared loss or hinge loss function to determine the residual. On the other hand, to well preserve the structure knowledge of the source data within the same class and strengthen the discriminant ability of the classifier, we incorporate the minimum within-class scatter into the process of domain adaptation. Lastly, to efficiently solve the resulting optimization problem, we extend the form of the Representer Theorem through the kernel trick, and thus derive an elegant solution for the proposed model. The extensive comparison experiments with the state-of-the-art methods on multiple benchmark data sets demonstrate the superiority of the proposed method.}
}
@article{LIANG2020280,
title = {Improved value iteration for neural-network-based stochastic optimal control design},
journal = {Neural Networks},
volume = {124},
pages = {280-295},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S089360802030006X},
author = {Mingming Liang and Ding Wang and Derong Liu},
keywords = {Adaptive critic designs, Adaptive dynamic programming, Neural networks, Optimal control, Stochastic processes, Value iteration},
abstract = {In this paper, a novel value iteration adaptive dynamic programming (ADP) algorithm is presented, which is called an improved value iteration ADP algorithm, to obtain the optimal policy for discrete stochastic processes. In the improved value iteration ADP algorithm, for the first time we propose a new criteria to verify whether the obtained policy is stable or not for stochastic processes. By analyzing the convergence properties of the proposed algorithm, it is shown that the iterative value functions can converge to the optimum. In addition, our algorithm allows the initial value function to be an arbitrary positive semi-definite function. Finally, two simulation examples are presented to validate the effectiveness of the developed method.}
}
@article{RAGHU2020202,
title = {EEG based multi-class seizure type classification using convolutional neural network and transfer learning},
journal = {Neural Networks},
volume = {124},
pages = {202-212},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300198},
author = {S. Raghu and Natarajan Sriraam and Yasin Temel and Shyam Vasudeva Rao and Pieter L. Kubben},
keywords = {Convolution neural network, Electroencephalogram, Epilepsy, Seizure type, Support vector machine, Transfer learning},
abstract = {Recognition of epileptic seizure type is essential for the neurosurgeon to understand the cortical connectivity of the brain. Though automated early recognition of seizures from normal electroencephalogram (EEG) was existing, no attempts have been made towards the classification of variants of seizures. Therefore, this study attempts to classify seven variants of seizures with non-seizure EEG through the application of convolutional neural networks (CNN) and transfer learning by making use of the Temple University Hospital EEG corpus. The objective of our study is to perform a multi-class classification of epileptic seizure type, which includes simple partial, complex partial, focal non-specific, generalized non-specific, absence, tonic, and tonic–clonic, and non-seizures. The 19 channels EEG time series was converted into a spectrogram stack before feeding as input to CNN. The following two different modalities were proposed using CNN: (1) Transfer learning using pretrained network, (2) Extract image features using pretrained network and classify using the support vector machine classifier. The following ten pretrained networks were used to identify the optimal network for the proposed study: Alexnet, Vgg16, Vgg19, Squeezenet, Googlenet, Inceptionv3, Densenet201, Resnet18, Resnet50, and Resnet101. The highest classification accuracy of 82.85% (using Googlenet) and 88.30% (using Inceptionv3) was achieved using transfer learning and extract image features approach respectively. Comparison results showed that CNN based approach outperformed conventional feature and clustering based approaches. It can be concluded that the EEG based classification of seizure type using CNN model could be used in pre-surgical evaluation for treating patients with epilepsy.}
}
@article{CAI2020193,
title = {FOM: Fourth-order moment based causal direction identification on the heteroscedastic data},
journal = {Neural Networks},
volume = {124},
pages = {193-201},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300083},
author = {Ruichu Cai and Jincheng Ye and Jie Qiao and Huiyuan Fu and Zhifeng Hao},
keywords = {Causal discovery, Causal direction, Heteroscedastic data, Fourth-order moment},
abstract = {Identification of the causal direction is a fundamental problem in many scientific research areas. The independence between the noise and the cause variable is a widely used assumption to identify the causal direction. However, such an independence assumption is usually violated due to heteroscedasticity of the real-world data. In this paper, we propose a new criterion for the causal direction identification which is robust to the heteroscedasticity of the data. In detail, the fourth-order moment of noise is proposed to measure the asymmetry between the cause and effect. A heteroscedastic Gaussian process regression-based estimation of the fourth-order moment is proposed accordingly. Under some commonly used assumptions of the causal mechanism, we theoretically show that the noise’s fourth-order moment of the causal direction is smaller than that of the anti-causal direction. Experimental results on both simulated and real-world data illustrate the efficiency of the proposed approach.}
}
@article{2022xi,
title = {Neural Networks Referees in 2021},
journal = {Neural Networks},
volume = {145},
pages = {xi-xviii},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00462-7},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004627}
}
@article{2022ii,
title = {Editorial Board},
journal = {Neural Networks},
volume = {146},
pages = {ii},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00490-1},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004901}
}
@article{BLAKEMAN2020218,
title = {A complementary learning systems approach to temporal difference learning},
journal = {Neural Networks},
volume = {122},
pages = {218-230},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303338},
author = {Sam Blakeman and Denis Mareschal},
keywords = {Complementary learning systems, Reinforcement learning, Hippocampus},
abstract = {Complementary Learning Systems (CLS) theory suggests that the brain uses a ’neocortical’ and a ’hippocampal’ learning system to achieve complex behaviour. These two systems are complementary in that the ’neocortical’ system relies on slow learning of distributed representations while the ’hippocampal’ system relies on fast learning of pattern-separated representations. Both of these systems project to the striatum, which is a key neural structure in the brain’s implementation of Reinforcement Learning (RL). Current deep RL approaches share similarities with a ’neocortical’ system because they slowly learn distributed representations through backpropagation in Deep Neural Networks (DNNs). An ongoing criticism of such approaches is that they are data inefficient and lack flexibility. CLS theory suggests that the addition of a ’hippocampal’ system could address these criticisms. In the present study we propose a novel algorithm known as Complementary Temporal Difference Learning (CTDL), which combines a DNN with a Self-Organizing Map (SOM) to obtain the benefits of both a ’neocortical’ and a ’hippocampal’ system. Key features of CTDL include the use of Temporal Difference (TD) error to update a SOM and the combination of a SOM and DNN to calculate action values. We evaluate CTDL on Grid World, Cart–Pole and Continuous Mountain Car tasks and show several benefits over the classic Deep Q-Network (DQN) approach. These results demonstrate (1) the utility of complementary learning systems for the evaluation of actions, (2) that the TD error signal is a useful form of communication between the two systems and (3) that our approach extends to both discrete and continuous state and action spaces.}
}
@article{CHEN2020412,
title = {A new fixed-time stability theorem and its application to the fixed-time synchronization of neural networks},
journal = {Neural Networks},
volume = {123},
pages = {412-419},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.028},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019304307},
author = {Chuan Chen and Lixiang Li and Haipeng Peng and Yixian Yang and Ling Mi and Hui Zhao},
keywords = {Fixed-time stability, Fixed-time synchronization, Neural networks},
abstract = {In this paper, we derive a new fixed-time stability theorem based on definite integral, variable substitution and some inequality techniques. The fixed-time stability criterion and the upper bound estimate formula for the settling time are different from those in the existing fixed-time stability theorems. Based on the new fixed-time stability theorem, the fixed-time synchronization of neural networks is investigated by designing feedback controller, and sufficient conditions are derived to guarantee the fixed-time synchronization of neural networks. To show the usability and superiority of the obtained theoretical results, we propose a secure communication scheme based on the fixed-time synchronization of neural networks. Numerical simulations illustrate that the new upper bound estimate formula for the settling time is much tighter than those in the existing fixed-time stability theorems. Moreover, the plaintext signals can be recovered according to the new fixed-time stability theorem, while the plaintext signals cannot be recovered according to the existing fixed-time stability theorems.}
}
@article{HERZOG2020153,
title = {Evolving artificial neural networks with feedback},
journal = {Neural Networks},
volume = {123},
pages = {153-162},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930396X},
author = {Sebastian Herzog and Christian Tetzlaff and Florentin Wörgötter},
keywords = {Deep learning, Feedback, Transfer entropy, Convolutional neural network},
abstract = {Neural networks in the brain are dominated by sometimes more than 60% feedback connections, which most often have small synaptic weights. Different from this, little is known how to introduce feedback into artificial neural networks. Here we use transfer entropy in the feed-forward paths of deep networks to identify feedback candidates between the convolutional layers and determine their final synaptic weights using genetic programming. This adds about 70% more connections to these layers all with very small weights. Nonetheless performance improves substantially on different standard benchmark tasks and in different networks. To verify that this effect is generic we use 36000 configurations of small (2–10 hidden layer) conventional neural networks in a non-linear classification task and select the best performing feed-forward nets. Then we show that feedback reduces total entropy in these networks always leading to performance increase. This method may, thus, supplement standard techniques (e.g. error backprop) adding a new quality to network learning.}
}
@article{2022I,
title = {Current Events},
journal = {Neural Networks},
volume = {145},
pages = {I},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00458-5},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004585}
}
@article{PERAZAGOICOLEA202052,
title = {Modeling functional resting-state brain networks through neural message passing on the human connectome},
journal = {Neural Networks},
volume = {123},
pages = {52-69},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303739},
author = {Julio A. Peraza-Goicolea and Eduardo Martínez-Montes and Eduardo Aubert and Pedro A. Valdés-Hernández and Roberto Mulet},
keywords = {Neural message passing, Belief Propagation, Susceptibility Propagation, Brain criticality, Resting State Networks, Brain connectivity},
abstract = {In this work, we propose a natural model for information flow in the brain through a neural message-passing dynamics on a structural network of macroscopic regions, such as the human connectome (HC). In our model, each brain region is assumed to have a binary behavior (active or not), the strengths of interactions among them are encoded in the anatomical connectivity matrix defined by the HC, and the dynamics of the system is defined by the Belief Propagation (BP) algorithm, working near the critical point of the network. We show that in the absence of direct external stimuli the BP algorithm converges to a spatial map of activations that is similar to the Default Mode Network (DMN) of the brain, which has been defined from the analysis of functional MRI data. Moreover, we use Susceptibility Propagation (SP) to compute the matrix of long-range correlations between the different regions and show that the modules defined by a clustering of this matrix resemble several Resting State Networks (RSN) determined experimentally. Both results suggest that the functional DMN and RSNs can be seen as simple consequences of the anatomical structure of the brain and a neural message-passing dynamics between macroscopic regions. With the new model, we explore predictions on how functional maps change when the anatomical brain network suffers structural alterations, like in Alzheimer’s disease and in lesions of the Corpus Callosum. The implications and novel interpretations suggested by the model, as well as the role of criticality, are discussed.}
}
@article{ZHANG202012,
title = {Cluster stochastic synchronization of complex dynamical networks via fixed-time control scheme},
journal = {Neural Networks},
volume = {124},
pages = {12-19},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019304198},
author = {Wanli Zhang and Chuandong Li and Hongfei Li and Xinsong Yang},
keywords = {Cluster synchronization, Complex dynamical networks, Stochastic perturbations, Quantization, Fixed-time technique},
abstract = {By means of fixed-time (FDT) control technique, cluster stochastic synchronization of complex networks (CNs) is investigated. Quantized controller is designed to realize the synchronization of CNs within a settling time. FDT synchronization criteria are established with the help of Lyapunov functional and comparison system methods. It should be noted that the convergence of synchronization is further improved by comparing with existing FDT synchronization results. Numerical simulations are given to illustrate our results.}
}
@article{HAYAKAWA2020343,
title = {On the minimax optimality and superiority of deep neural network learning over sparse parameter spaces},
journal = {Neural Networks},
volume = {123},
pages = {343-361},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.014},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930406X},
author = {Satoshi Hayakawa and Taiji Suzuki},
keywords = {Neural network, Deep learning, Linear estimator, Nonparametric regression, Minimax optimality},
abstract = {Deep learning has been applied to various tasks in the field of machine learning and has shown superiority to other common procedures such as kernel methods. To provide a better theoretical understanding of the reasons for its success, we discuss the performance of deep learning and other methods on a nonparametric regression problem with a Gaussian noise. Whereas existing theoretical studies of deep learning have been based mainly on mathematical theories of well-known function classes such as Hölder and Besov classes, we focus on function classes with discontinuity and sparsity, which are those naturally assumed in practice. To highlight the effectiveness of deep learning, we compare deep learning with a class of linear estimators representative of a class of shallow estimators. It is shown that the minimax risk of a linear estimator on the convex hull of a target function class does not differ from that of the original target function class. This results in the suboptimality of linear methods over a simple but non-convex function class, on which deep learning can attain nearly the minimax-optimal rate. In addition to this extreme case, we consider function classes with sparse wavelet coefficients. On these function classes, deep learning also attains the minimax rate up to log factors of the sample size, and linear methods are still suboptimal if the assumed sparsity is strong. We also point out that the parameter sharing of deep neural networks can remarkably reduce the complexity of the model in our setting.}
}
@article{TAO2020289,
title = {Affinity and class probability-based fuzzy support vector machine for imbalanced data sets},
journal = {Neural Networks},
volume = {122},
pages = {289-307},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303387},
author = {Xinmin Tao and Qing Li and Chao Ren and Wenjie Guo and Qing He and Rui Liu and Junrong Zou},
keywords = {Imbalanced data, Fuzzy support vector machine (FSVM), Affinity, Class probability, Kernelknn},
abstract = {The learning problem from imbalanced data sets poses a major challenge in data mining community. Although conventional support vector machine can generally show relatively robust performance in dealing with the classification problems of imbalanced data sets, it treats all training samples with the same contribution for learning, which results in the final decision boundary biasing toward the majority class especially in the presence of outliers or noises. In this paper, we propose a new affinity and class probability-based fuzzy support vector machine technique (ACFSVM). The affinity of a majority class sample is calculated according to support vector description domain (SVDD) model trained only by the given majority class training samples in kernel space similar to that used for FSVM learning. The obtained affinity can be used for identifying possible outliers and some border samples existing in the majority class training samples. In order to eliminate the effect of noises, we employ the kernel k-nearest neighbor method to determine the class probability of the majority class samples in the same kernel space as before. The samples with lower class probabilities are more likely to be noises and their contribution for learning seems to be reduced by their low memberships constructed by combining the affinities and the class probabilities. Thus, ACFSVM can pay more attention to the majority class samples with higher affinities and class probabilities while reducing their effects of the ones with lower affinities and class probabilities, eventually skewing the final classification boundary toward the majority class. In addition, the minority class samples are assigned relative high memberships to guarantee their importance for the model learning. The extensive experimental results on the different imbalanced datasets from UCI repository demonstrate that the proposed approach can achieve better generalization performance in terms of G-Mean, F-Measure, and AUC as compared to the other existing imbalanced dataset classification techniques.}
}
@article{WU2020308,
title = {Effective metric learning with co-occurrence embedding for collaborative recommendations},
journal = {Neural Networks},
volume = {124},
pages = {308-318},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.021},
url = {https://www.sciencedirect.com/science/article/pii/S089360802030023X},
author = {Hao Wu and Qimin Zhou and Rencan Nie and Jinde Cao},
keywords = {Recommender systems, Metric learning, Cooccurrence-based embedding, Regularization, Top-n recommendations},
abstract = {In recommender systems, matrix factorization and its variants have grown up to be dominant in collaborative filtering due to their simplicity and effectiveness. In matrix factorization based methods, dot product which is actually used as a measure of distance from users to items, does not satisfy the inequality property, and thus may fail to capture the inner grained preference information and further limits the performance of recommendations. Metric learning produces distance functions that capture the essential relationships among rating data and has been successfully explored in collaborative recommendations. However, without the global statistical information of user–user pairs and item–item pairs, it makes the model easy to achieve a suboptimal metric. For this, we present a co-occurrence embedding regularized metric learning model (CRML) for collaborative recommendations. We consider the optimization problem as a multi-task learning problem which includes optimizing a primary task of metric learning and two auxiliary tasks of representation learning. In particular, we develop an effective approach for learning the embedding representations of both users and items, and then exploit the strategy of soft parameter sharing to optimize the model parameters. Empirical experiments on four datasets demonstrate that the CRML model can enhance the naive metric learning model and significantly outperforms the state-of-the-art methods in terms of accuracy of collaborative recommendations.}
}
@article{ZHAO2020144,
title = {A consensus algorithm based on collective neurodynamic system for distributed optimization with linear and bound constraints},
journal = {Neural Networks},
volume = {122},
pages = {144-151},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303302},
author = {Yan Zhao and Qingshan Liu},
keywords = {Distributed optimization, Collective neurodynamic system, Consensus algorithm, Lyapunov function},
abstract = {In this paper, an algorithm based on collective neurodynamic system is investigated for distributed constrained convex optimization, whose objective function is a sum of smooth convex functions and non-smooth L1-norm functions. Inspired by recent advances in distributed convex optimization, the continuous-time and discrete-time distributed optimization algorithms described by collective neurodynamic systems are proposed. In the systems, each of the smooth objective functions is allocated to each node as well as each of the L1-norm functions. However, the L1-norm functions are realized by projection operators. Meanwhile, each node satisfies the local linear and bound constraints. Then a connected network is constituted from all the nodes with consensus to find the optimal solutions.}
}
@article{LEE202092,
title = {Mu-net: Multi-scale U-net for two-photon microscopy image denoising and restoration},
journal = {Neural Networks},
volume = {125},
pages = {92-103},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.026},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300368},
author = {Sehyung Lee and Makiko Negishi and Hidetoshi Urakubo and Haruo Kasai and Shin Ishii},
keywords = {Image denoising, Two-photon microscopy image, Deep learning, U-net, GAN},
abstract = {Advances in two two-photon microscopy (2PM) have made three-dimensional (3D) neural imaging of deep cortical regions possible. However, 2PM often suffers from poor image quality because of various noise factors, including blur, white noise, and photo bleaching. In addition, the effectiveness of the existing image processing methods is limited because of the special features of 2PM images such as deeper tissue penetration but higher image noises owing to rapid laser scanning. To address the denoising problems in 2PM 3D images, we present a new algorithm based on deep convolutional neural networks (CNNs). The proposed model consists of multiple U-nets in which an individual U-net removes noises at different scales and then yields a performance improvement based on a coarse-to-fine strategy. Moreover, the constituent CNNs employ fully 3D convolution operations. Such an architecture enables the proposed model to facilitate end-to-end learning without any pre/post processing. Based on the experiments on 2PM image denoising, we observed that our new algorithm demonstrates substantial performance improvements over other baseline methods.}
}
@article{STELZER2020158,
title = {Performance boost of time-delay reservoir computing by non-resonant clock cycle},
journal = {Neural Networks},
volume = {124},
pages = {158-169},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300125},
author = {Florian Stelzer and André Röhm and Kathy Lüdge and Serhiy Yanchuk},
keywords = {Time-delay, Reservoir computing, Clock cycle, Resonance, Memory capacity, Network representation},
abstract = {The time-delay-based reservoir computing setup has seen tremendous success in both experiment and simulation. It allows for the construction of large neuromorphic computing systems with only few components. However, until now the interplay of the different timescales has not been investigated thoroughly. In this manuscript, we investigate the effects of a mismatch between the time-delay and the clock cycle for a general model. Typically, these two time scales are considered to be equal. Here we show that the case of equal or resonant time-delay and clock cycle could be actively detrimental and leads to an increase of the approximation error of the reservoir. In particular, we can show that non-resonant ratios of these time scales have maximal memory capacities. We achieve this by translating the periodically driven delay-dynamical system into an equivalent network. Networks that originate from a system with resonant delay-times and clock cycles fail to utilize all of their degrees of freedom, which causes the degradation of their performance.}
}
@article{XU2020180,
title = {A neurodynamic approach to nonsmooth constrained pseudoconvex optimization problem},
journal = {Neural Networks},
volume = {124},
pages = {180-192},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019304071},
author = {Chen Xu and Yiyuan Chai and Sitian Qin and Zhenkun Wang and Jiqiang Feng},
keywords = {Nonsmooth pseudoconvex optimization, Neurodynamic approach, Lyapunov function, Global convergence},
abstract = {This paper presents a new neurodynamic approach for solving the constrained pseudoconvex optimization problem based on more general assumptions. The proposed neural network is equipped with a hard comparator function and a piecewise linear function, which make the state solution not only stay in the feasible region, but also converge to an optimal solution of the constrained pseudoconvex optimization problem. Compared with other related existing conclusions, the neurodynamic approach here enjoys global convergence and lower dimension of the solution space. Moreover, the neurodynamic approach does not depend on some additional assumptions, such as the feasible region is bounded, the objective function is lower bounded over the feasible region or the objective function is coercive. Finally, both numerical illustrations and simulation results in support vector regression problem show the well performance and the viability of the proposed neurodynamic approach.}
}
@article{XIAO2020152,
title = {ℓ1-gain filter design of discrete-time positive neural networks with mixed delays},
journal = {Neural Networks},
volume = {122},
pages = {152-162},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303260},
author = {Shunyuan Xiao and Yijun Zhang and Baoyong Zhang},
keywords = {Positive neural networks, Mixed delays, Filtering analysis, -gain performance},
abstract = {This paper mainly focuses on the filter design with ℓ1-gain disturbance attenuation performance for a class of discrete-time positive neural networks. Discrete and distributed time-varying delays occurring in neuron transmission are taken into account. Especially, the probabilistic distribution of distributed delays is described by a Bernoulli random process in the system model. First, criteria on the positiveness and the unique equilibrium of discrete-time neural networks are presented. Second, through linear Lyapunov method, sufficient conditions for globally asymptotic stability with ℓ1-gain disturbance attenuation performance of positive neural networks are proposed. Third, using the results obtained above, criteria on ℓ1-gain stability of the established filtering error system are presented, based on which a linear programming (LP) approach is put forward to design the desired positive filter. Finally, two examples of applications to water distribution network and genetic regulatory network are given to demonstrate the effectiveness and applicability of the derived results.}
}
@article{BORE2020213,
title = {Directed EEG neural network analysis by LAPPS (p≤1) Penalized sparse Granger approach},
journal = {Neural Networks},
volume = {124},
pages = {213-222},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300241},
author = {Joyce Chelangat Bore and Peiyang Li and Dennis Joe Harmah and Fali Li and Dezhong Yao and Peng Xu},
keywords = {Multivariate Granger Analysis, Outliers, Motor Imagery, Sparse network},
abstract = {The conventional multivariate Granger Analysis (GA) of directed interactions has been widely applied in brain network construction based on EEG recordings as well as fMRI. Nevertheless, EEG is usually inevitably contaminated by strong noise, which may cause network distortion due to the L2-norm used in GAs for directed network recovery. The Lp (p ≤1) norm has been shown to be more robust to outliers as compared to LASSO and L2-GAs. Motivated to construct the sparse brain networks under strong noise condition, we hereby introduce a new approach for GA analysis, termed LAPPS (Least Absolute LP (0<p<1) Penalized Solution). LAPPS utilizes the L1-loss function for the residual error to alleviate the effect of outliers, and another Lp-penalty term (p=0.5) to obtain the sparse connections while suppressing the spurious linkages in the networks. The simulation results reveal that LAPPS obtained the best performance under various noise conditions. In a real EEG data test when subjects performed the left and right hand Motor Imagery (MI) for brain network estimation, LAPPS also obtained a sparse network pattern with the hub at the contralateral brain primary motor areas consistent with the physiological basis of MI.}
}
@article{FAN2020383,
title = {Universal approximation with quadratic deep networks},
journal = {Neural Networks},
volume = {124},
pages = {383-392},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300095},
author = {Fenglei Fan and Jinjun Xiong and Ge Wang},
keywords = {Deep learning, Quadratic networks, Approximation theory},
abstract = {Recently, deep learning has achieved huge successes in many important applications. In our previous studies, we proposed quadratic/second-order neurons and deep quadratic neural networks. In a quadratic neuron, the inner product of a vector of data and the corresponding weights in a conventional neuron is replaced with a quadratic function. The resultant quadratic neuron enjoys an enhanced expressive capability over the conventional neuron. However, how quadratic neurons improve the expressing capability of a deep quadratic network has not been studied up to now, preferably in relation to that of a conventional neural network. Specifically, we ask four basic questions in this paper: (1) for the one-hidden-layer network structure, is there any function that a quadratic network can approximate much more efficiently than a conventional network? (2) for the same multi-layer network structure, is there any function that can be expressed by a quadratic network but cannot be expressed with conventional neurons in the same structure? (3) Does a quadratic network give a new insight into universal approximation? (4) To approximate the same class of functions with the same error bound, could a quantized quadratic network have a lower number of weights than a quantized conventional network? Our main contributions are the four interconnected theorems shedding light upon these four questions and demonstrating the merits of a quadratic network in terms of expressive efficiency, unique capability, compact architecture and computational capacity respectively.}
}
@article{ZHU202026,
title = {Multiple Partial Empirical Kernel Learning with Instance Weighting and Boundary Fitting},
journal = {Neural Networks},
volume = {123},
pages = {26-37},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.019},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303788},
author = {Zonghai Zhu and Zhe Wang and Dongdong Li and Wenli Du and Yangming Zhou},
keywords = {Empirical Kernel Mapping, Multiple Empirical Kernel Learning, Instance weighting, Boundary fitting, Pattern recognition},
abstract = {By dividing the original data set into several sub-sets, Multiple Partial Empirical Kernel Learning (MPEKL) constructs multiple kernel matrixes corresponding to the sub-sets, and these kernel matrixes are decomposed to provide the explicit kernel functions. Then, the instances in the original data set are mapped into multiple kernel spaces, which provide better performance than single kernel space. It is known that the instances in different locations and distributions behave differently. Therefore, this paper defines the weight of instance in accordance with the location and distribution of the instances. According to the location, the instances can be categorized into intrinsic instances, boundary instances and noise instances. Generally, the boundary instances, as well as the minority instances in the imbalanced data set, are assigned high weight. Meanwhile, a regularization term, which regulates the classification hyperplane to fit the distribution trend of the class boundary, is constructed by the boundary instances. Then, the weight of instance and the regularization term are introduced into MPEKL to form an algorithm named Multiple Partial Empirical Kernel Learning with Instance Weighting and Boundary Fitting (IBMPEKL). Experiments demonstrate the good performance of IBMPEKL and validate the effectiveness of the instance weighting and boundary fitting.}
}
@article{2022I,
title = {Current Events},
journal = {Neural Networks},
volume = {146},
pages = {I},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00500-1},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021005001}
}
@article{SUN2020163,
title = {Discriminative structure learning of sum–product networks for data stream classification},
journal = {Neural Networks},
volume = {123},
pages = {163-175},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303946},
author = {Zhengya Sun and Cheng-Lin Liu and Jinghao Niu and Wensheng Zhang},
keywords = {Sum–product network, Discriminative structure learning, Data stream classification},
abstract = {Sum–product network (SPN) is a deep probabilistic representation that allows for exact and tractable inference. There has been a trend of online SPN structure learning from massive and continuous data streams. However, online structure learning of SPNs has been introduced only for the generative settings so far. In this paper, we present an online discriminative approach for SPNs for learning both the structure and parameters. The basic idea is to keep track of informative and representative examples to capture the trend of time-changing class distributions. Specifically, by estimating the goodness of model fitting of data points and dynamically maintaining a certain amount of informative examples over time, we generate new sub-SPNs in a recursive and top-down manner. Meanwhile, an outlier-robust margin-based log-likelihood loss is applied locally to each data point and the parameters of SPN are updated continuously using most probable explanation (MPE) inference. This leads to a fast yet powerful optimization procedure and improved discrimination capability between the genuine class and rival classes. Empirical results show that the proposed approach achieves better prediction performance than the state-of-the-art online structure learner for SPNs, while promising order-of-magnitude speedup. Comparison with state-of-the-art stream classifiers further proves the superiority of our approach.}
}
@article{SUN2020374,
title = {Generative adversarial networks with mixture of t-distributions noise for diverse image generation},
journal = {Neural Networks},
volume = {122},
pages = {374-381},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303429},
author = {Jinxuan Sun and Guoqiang Zhong and Yang Chen and Yongbin Liu and Tao Li and Kaizhu Huang},
keywords = {Image generation, Generate adversarial networks, Diversity, Mixture of t-distributions, Class codeword},
abstract = {Image generation is a long-standing problem in the machine learning and computer vision areas. In order to generate images with high diversity, we propose a novel model called generative adversarial networks with mixture of t-distributions noise (tGANs). In tGANs, the latent generative space is formulated using a mixture of t-distributions. Particularly, the parameters of the components in the mixture of t-distributions can be learned along with others in the model. To improve the diversity of the generated images in each class, each noise vector and a class codeword are concatenated as the input of the generator of tGANs. In addition, a classification loss is added to both the generator and the discriminator losses to strengthen their performances. We have conducted extensive experiments to compare tGANs with a state-of-the-art pixel by pixel image generation approach, pixelCNN, and related GAN-based models. The experimental results and statistical comparisons demonstrate that tGANs perform significantly better than pixleCNN and related GAN-based models for diverse image generation.}
}
@article{YOU2020382,
title = {Global Mittag-Leffler stability and synchronization of discrete-time fractional-order complex-valued neural networks with time delay},
journal = {Neural Networks},
volume = {122},
pages = {382-394},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303430},
author = {Xingxing You and Qiankun Song and Zhenjiang Zhao},
keywords = {Mittag-Leffler stability, Discrete-time, Synchronization, Fractional-order complex-valued neural networks, Lyapunov’s direct method, Time delay},
abstract = {Without decomposing complex-valued systems into real-valued systems, this paper investigates existence, uniqueness, global Mittag-Leffler stability and global Mittag-Leffler synchronization of discrete-time fractional-order complex-valued neural networks (FCVNNs) with time delay. Inspired by Lyapunov’s direct method on continuous-time systems, a class of discrete-time FCVNNs is further discussed by employing the fractional-order extension of Lyapunov’s direct method. Firstly, by means of contraction mapping theory and Cauchy’s inequality, a sufficient condition is presented to ascertain the existence and uniqueness of the equilibrium point for discrete-time FCVNNs. Then, based on the theory of discrete fractional calculus, discrete Laplace transform, the theory of complex functions and discrete Mittag-Leffler functions, a sufficient condition is established for global Mittag-Leffler stability of the proposed networks. Additionally, by applying the Lyapunov’s direct method and designing a effective control scheme, the sufficient criterion is derived to ensure the global Mittag-Leffler synchronization of discrete-time FCVNNs. Finally, two numerical examples are also presented to manifest the feasibility and validity of the obtained results.}
}
@article{XIAO2020320,
title = {New approach to global Mittag-Leffler synchronization problem of fractional-order quaternion-valued BAM neural networks based on a new inequality},
journal = {Neural Networks},
volume = {122},
pages = {320-337},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303399},
author = {Jianying Xiao and Shiping Wen and Xujun Yang and Shouming Zhong},
keywords = {Fractional-order system, Quaternion-valued neural networks, Bidirectional associative memory, Mittag-Leffler synchronization},
abstract = {In this paper, a novel kind of neural networks named fractional-order quaternion-valued bidirectional associative memory neural networks (FQVBAMNNs) is formulated. On one hand, applying Hamilton rules in quaternion multiplication which is essentially non-commutative, the system of FQVBAMNNs is separated into eight fractional-order real-valued systems. Meanwhile, the activation functions are considered to be quaternion-valued linear threshold ones which help to reduce the unnecessary computational complexity. On the other hand, based on fractional-order Lyapunov technology, a new fractional-order derivative inequality is established. Mainly by employing the new inequality technique, constructing three novel Lyapunov–Krasovskii functionals (LKFs) and designing simple linear controllers, the global Mittag-Leffler synchronization problems are investigated and the corresponding criteria are acquired for the system of FQVBAMNNs and its special cases such as fractional-order complex-valued BAM neural networks (FCVBAMNNs) and fractional-order real-valued BAM neural networks (FRVBAMNNs), respectively. Finally, two numerical examples are given to show the effectiveness and availability of the proposed results.}
}
@article{XU2020420,
title = {Bayesian deep matrix factorization network for multiple images denoising},
journal = {Neural Networks},
volume = {123},
pages = {420-428},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.023},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930423X},
author = {Shuang Xu and Chunxia Zhang and Jiangshe Zhang},
keywords = {Matrix factorization, Bayesian neural networks, Variational Bayes},
abstract = {This paper aims at proposing a robust and fast low rank matrix factorization model for multiple images denoising. To this end, a novel model, Bayesian deep matrix factorization network (BDMF), is presented, where a deep neural network (DNN) is designed to model the low rank components and the model is optimized via stochastic gradient variational Bayes. By the virtue of deep learning and Bayesian modeling, BDMF makes significant improvement on synthetic experiments and real-world tasks (including shadow removal and hyperspectral image denoising), compared with existing state-of-the-art models.}
}
@article{ZHU2020174,
title = {Learning Cascade Attention for fine-grained image classification},
journal = {Neural Networks},
volume = {122},
pages = {174-182},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303314},
author = {Youxiang Zhu and Ruochen Li and Yin Yang and Ning Ye},
keywords = {Deep Convolutional Neural Network, Fine-grained image classification, Attention model},
abstract = {Fine-grained image classification is a challenging task due to the large inter-class difference and small intra-class difference. In this paper, we propose a novel Cascade Attention Model using the Deep Convolutional Neural Network to address this problem. Our method first leverages the Spatial Confusion Attention to identify ambiguous areas of the input image. Two constraint loss functions are proposed: the Spatial Mask loss and the Spatial And loss; Second, the Cross-network Attention, applying different pre-train parameters to the two stream architecture. Also, two novel loss functions called Cross-network Similarity loss and Satisfied Rank loss are proposed to make the two-stream networks reinforce each other and get better results. Finally, the Network Fusion Attention merges intermediate results with the novel entropy add strategy to obtain the final predictions. All of these modules can work together and can be trained end to end. Besides, different from previous works, our model is fully weak-supervised and fully paralleled, which leads to easier generalization and faster computation. We obtain the state-of-the-art performance on three challenge benchmark datasets (CUB-200-2011, FGVC-Aircraft and Flower 102) with results of 90.8%, 92.1%, and 98.5%, respectively. The model will be publicly available at https://github.com/billzyx/LCA-CNN.}
}
@article{ZHANG2020233,
title = {Adaptive complex-valued stepsize based fast learning of complex-valued neural networks},
journal = {Neural Networks},
volume = {124},
pages = {233-242},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300137},
author = {Yongliang Zhang and He Huang},
keywords = {Adaptive complex-valued stepsize, Complex-valued neural networks, Fast learning, Rotation factors, Scaling factors, Saddle points},
abstract = {Complex-valued gradient descent algorithm is a popular tool to optimize functions of complex variables, especially for the training of complex-valued neural networks. However, the choice of suitable learning stepsize is a challenging task during the training process. In this paper, an adaptive complex-valued stepsize design method is proposed for complex-valued neural networks by generalizing the adaptable learning rate tree technique to the complex domain. The scaling and rotation factors are introduced to simultaneously adjust the amplitude and phase of complex-valued stepsize. The search range is thus expanded from half line to half plane such that better search direction is obtained at each iteration. We analyze the dynamics of the algorithm near a saddle point and find that it is very easy to escape from the saddle point to guarantee fast convergence and high accuracy. Some experimental results on function approximation and pattern classification tasks are presented to illustrate the advantages of the proposed algorithm over some previous ones.}
}
@article{WANG2020362,
title = {Global synchronization of coupled delayed memristive reaction–diffusion neural networks},
journal = {Neural Networks},
volume = {123},
pages = {362-371},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019304083},
author = {Shiqin Wang and Zhenyuan Guo and Shiping Wen and Tingwen Huang},
keywords = {Neural network, Memristor, Reaction–diffusion, Exponential synchronization, Coupling topology},
abstract = {This paper focuses on the global exponential synchronization of multiple memristive reaction–diffusion neural networks (MRDNNs) with time delay. Due to introducing the influences of space as well as time on state variables and replacing resistors with memristors in circuit realization, the state-dependent partial differential mathematical model of MRDNN is more general and realistic than traditional neural network model. Based on Lyapunov functional theory, Divergence theorem and inequality techniques, global exponential synchronization criteria of coupled delayed MRDNNs are derived via directed and undirected nonlinear coupling. Finally, three numerical simulation examples are presented to verify the feasibility of our main results.}
}
@article{MANSILLA2020269,
title = {Learning deformable registration of medical images with anatomical constraints},
journal = {Neural Networks},
volume = {124},
pages = {269-279},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300253},
author = {Lucas Mansilla and Diego H. Milone and Enzo Ferrante},
keywords = {Medical image registration, Convolutional neural networks, X-ray image analysis},
abstract = {Deformable image registration is a fundamental problem in the field of medical image analysis. During the last years, we have witnessed the advent of deep learning-based image registration methods which achieve state-of-the-art performance, and drastically reduce the required computational time. However, little work has been done regarding how can we encourage our models to produce not only accurate, but also anatomically plausible results, which is still an open question in the field. In this work, we argue that incorporating anatomical priors in the form of global constraints into the learning process of these models, will further improve their performance and boost the realism of the warped images after registration. We learn global non-linear representations of image anatomy using segmentation masks, and employ them to constraint the registration process. The proposed AC-RegNet architecture is evaluated in the context of chest X-ray image registration using three different datasets, where the high anatomical variability makes the task extremely challenging. Our experiments show that the proposed anatomically constrained registration model produces more realistic and accurate results than state-of-the-art methods, demonstrating the potential of this approach.}
}
@article{KAREVAN20201,
title = {Transductive LSTM for time-series prediction: An application to weather forecasting},
journal = {Neural Networks},
volume = {125},
pages = {1-9},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.030},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300010},
author = {Zahra Karevan and Johan A.K. Suykens},
keywords = {Transductive learning, Long short-term memory, Weather forecasting},
abstract = {Long Short-Term Memory (LSTM) has shown significant performance on many real-world applications due to its ability to capture long-term dependencies. In this paper, we utilize LSTM to obtain a data-driven forecasting model for an application of weather forecasting. Moreover, we propose Transductive LSTM (T-LSTM) which exploits the local information in time-series prediction. In transductive learning, the samples in the test point vicinity are considered to have higher impact on fitting the model. In this study, a quadratic cost function is considered for the regression problem. Localizing the objective function is done by considering a weighted quadratic cost function at which point the samples in the neighborhood of the test point have larger weights. We investigate two weighting schemes based on the cosine similarity between the training samples and the test point. In order to assess the performance of the proposed method in different weather conditions, the experiments are conducted on two different time periods of a year. The results show that T-LSTM results in better performance in the prediction task.}
}
@article{TIAN2020117,
title = {Attention-guided CNN for image denoising},
journal = {Neural Networks},
volume = {124},
pages = {117-129},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.024},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019304241},
author = {Chunwei Tian and Yong Xu and Zuoyong Li and Wangmeng Zuo and Lunke Fei and Hong Liu},
keywords = {Image denoising, CNN, Sparse block, Feature enhancement block, Attention block},
abstract = {Deep convolutional neural networks (CNNs) have attracted considerable interest in low-level computer vision. Researches are usually devoted to improving the performance via very deep CNNs. However, as the depth increases, influences of the shallow layers on deep layers are weakened. Inspired by the fact, we propose an attention-guided denoising convolutional neural network (ADNet), mainly including a sparse block (SB), a feature enhancement block (FEB), an attention block (AB) and a reconstruction block (RB) for image denoising. Specifically, the SB makes a tradeoff between performance and efficiency by using dilated and common convolutions to remove the noise. The FEB integrates global and local features information via a long path to enhance the expressive ability of the denoising model. The AB is used to finely extract the noise information hidden in the complex background, which is very effective for complex noisy images, especially real noisy images and bind denoising. Also, the FEB is integrated with the AB to improve the efficiency and reduce the complexity for training a denoising model. Finally, a RB aims to construct the clean image through the obtained noise mapping and the given noisy image. Additionally, comprehensive experiments show that the proposed ADNet performs very well in three tasks (i.e. synthetic and real noisy images, and blind denoising) in terms of both quantitative and qualitative evaluations. The code of ADNet is accessible at https://github.com/hellloxiaotian/ADNet.}
}
@article{FLEISCHER2020343,
title = {A unified model of rule-set learning and selection},
journal = {Neural Networks},
volume = {124},
pages = {343-356},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.028},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300381},
author = {Pierson Fleischer and Sébastien Hélie},
keywords = {Connectionist model, Rule learning, Task-switching, Prefrontal cortex, Basal ganglia},
abstract = {The ability to focus on relevant information and ignore irrelevant information is a fundamental part of intelligent behavior. It not only allows faster acquisition of new tasks by reducing the size of the problem space but also allows for generalizations to novel stimuli. Task-switching, task-sets, and rule-set learning are all intertwined with this ability. There are many models that attempt to individually describe these cognitive abilities. However, there are few models that try to capture the breadth of these topics in a unified model and fewer still that do it while adhering to the biological constraints imposed by the findings from the field of neuroscience. Presented here is a comprehensive model of rule-set learning and selection that can capture the learning curve results, error-type data, and transfer effects found in rule-learning studies while also replicating the reaction time data and various related effects of task-set and task-switching experiments. The model also factors in many disparate neurological findings, several of which are often disregarded by similar models.}
}
@article{2022iii,
title = {List of Editorial Board Members},
journal = {Neural Networks},
volume = {145},
pages = {iii-x},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00460-3},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004603}
}
@article{2022ii,
title = {Editorial Board},
journal = {Neural Networks},
volume = {145},
pages = {ii},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00455-X},
url = {https://www.sciencedirect.com/science/article/pii/S089360802100455X}
}
@article{ZOU202056,
title = {Neuromodulated attention and goal-driven perception in uncertain domains},
journal = {Neural Networks},
volume = {125},
pages = {56-69},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.031},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300411},
author = {Xinyun Zou and Soheil Kolouri and Praveen K. Pilly and Jeffrey L. Krichmar},
keywords = {Neuromodulation, Goal-driven perception, Uncertainty, Top-down attention, Contrastive excitation backprop},
abstract = {In uncertain domains, the goals are often unknown and need to be predicted by the organism or system. In this paper, contrastive Excitation Backprop (c-EB) was used in two goal-driven perception tasks – one with pairs of noisy MNIST digits and the other with a robot in an action-based attention scenario. The first task included attending to even, odd, low, and high digits, whereas the second task included action goals, such as “eat”, “work-on-computer”, “read”, and “say-hi” that led to attention to objects associated with those actions. The system needed to increase attention to target items and decrease attention to distractor items and background noise. Because the valid goal was unknown, an online learning model based on the cholinergic and noradrenergic neuromodulatory systems was used to predict a noisy goal (expected uncertainty) and re-adapt when the goal changed (unexpected uncertainty). This neurobiologically plausible model demonstrates how neuromodulatory systems can predict goals in uncertain domains and how attentional mechanisms can enhance the perception for that goal.}
}
@article{HE2020236,
title = {New H∞ state estimation criteria of delayed static neural networks via the Lyapunov–Krasovskii functional with negative definite terms},
journal = {Neural Networks},
volume = {123},
pages = {236-247},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019304009},
author = {Jing He and Yan Liang and Feisheng Yang and Feng Yang},
keywords = {Static neural networks,  state estimation, Time-varying delay, Lyapunov–Krasovskii functional},
abstract = {In the estimation problem for delayed static neural networks (SNNs), constructing a proper Lyapunov–Krasovskii functional (LKF) is crucial for deriving less conservative estimation criteria. In this paper, a delay-product-type LKF with negative definite terms is proposed. Based on the third-order Bessel–Legendre (B–L) integral inequality and mixed convex combination approaches, a less conservative estimator design criterion is derived. Furthermore, the desired estimator gain matrices and the H∞ performance index are obtained by solving a set of linear matrix inequalities (LMIs). Finally, a numerical example is given to demonstrate the effectiveness of the proposed method.}
}
@article{SUSSNER2020288,
title = {Extreme learning machine for a new hybrid morphological/linear perceptron},
journal = {Neural Networks},
volume = {123},
pages = {288-298},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303958},
author = {Peter Sussner and Israel Campiotti},
keywords = {Mathematical morphology, Lattice computing, Morphological neural networks, Hybrid morphological/linear perceptron, Extreme learning machine, Classification},
abstract = {Morphological neural networks (MNNs) can be characterized as a class of artificial neural networks that perform an operation of mathematical morphology at every node, possibly followed by the application of an activation function. Morphological perceptrons (MPs) and (gray-scale) morphological associative memories are among the most widely known MNN models. Since their neuronal aggregation functions are not differentiable, classical methods of non-linear optimization can in principle not be directly applied in order to train these networks. The same observation holds true for hybrid morphological/linear perceptrons and other related models. Circumventing these problems of non-differentiability, this paper introduces an extreme learning machine approach for training a hybrid morphological/linear perceptron, whose morphological components were drawn from previous MP models. We apply the resulting model to a number of well-known classification problems from the literature and compare the performance of our model with the ones of several related models, including some recent MNNs and hybrid morphological/linear neural networks.}
}
@article{RONGALA2020273,
title = {Cuneate spiking neural network learning to classify naturalistic texture stimuli under varying sensing conditions},
journal = {Neural Networks},
volume = {123},
pages = {273-287},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.020},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303880},
author = {Udaya B. Rongala and Alberto Mazzoni and Anton Spanne and Henrik Jörntell and Calogero M. Oddo},
keywords = {Spiking neural network, Neurorobotics, Cuneate neurons, Primary afferents, Tactile sensing, Synaptic weight learning},
abstract = {We implemented a functional neuronal network that was able to learn and discriminate haptic features from biomimetic tactile sensor inputs using a two-layer spiking neuron model and homeostatic synaptic learning mechanism. The first order neuron model was used to emulate biological tactile afferents and the second order neuron model was used to emulate biological cuneate neurons. We have evaluated 10 naturalistic textures using a passive touch protocol, under varying sensing conditions. Tactile sensor data acquired with five textures under five sensing conditions were used for a synaptic learning process, to tune the synaptic weights between tactile afferents and cuneate neurons. Using post-learning synaptic weights, we evaluated the individual and population cuneate neuron responses by decoding across 10 stimuli, under varying sensing conditions. This resulted in a high decoding performance. We further validated the decoding performance across stimuli, irrespective of sensing velocities using a set of 25 cuneate neuron responses. This resulted in a median decoding performance of 96% across the set of cuneate neurons. Being able to learn and perform generalized discrimination across tactile stimuli, makes this functional spiking tactile system effective and suitable for further robotic applications.}
}
@article{LANILLOS2020338,
title = {A review on neural network models of schizophrenia and autism spectrum disorder},
journal = {Neural Networks},
volume = {122},
pages = {338-363},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303363},
author = {Pablo Lanillos and Daniel Oliva and Anja Philippsen and Yuichi Yamashita and Yukie Nagai and Gordon Cheng},
keywords = {Neural networks, Schizophrenia, Autism spectrum disorder, Computational psychiatry, Predictive coding},
abstract = {This survey presents the most relevant neural network models of autism spectrum disorder and schizophrenia, from the first connectionist models to recent deep neural network architectures. We analyzed and compared the most representative symptoms with its neural model counterpart, detailing the alteration introduced in the network that generates each of the symptoms, and identifying their strengths and weaknesses. We additionally cross-compared Bayesian and free-energy approaches, as they are widely applied to model psychiatric disorders and share basic mechanisms with neural networks. Models of schizophrenia mainly focused on hallucinations and delusional thoughts using neural dysconnections or inhibitory imbalance as the predominating alteration. Models of autism rather focused on perceptual difficulties, mainly excessive attention to environment details, implemented as excessive inhibitory connections or increased sensory precision. We found an excessively tight view of the psychopathologies around one specific and simplified effect, usually constrained to the technical idiosyncrasy of the used network architecture. Recent theories and evidence on sensorimotor integration and body perception combined with modern neural network architectures could offer a broader and novel spectrum to approach these psychopathologies. This review emphasizes the power of artificial neural networks for modeling some symptoms of neurological disorders but also calls for further developing of these techniques in the field of computational psychiatry.}
}
@article{WAN20201,
title = {Finite-time and fixed-time anti-synchronization of Markovian neural networks with stochastic disturbances via switching control},
journal = {Neural Networks},
volume = {123},
pages = {1-11},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303521},
author = {Peng Wan and Dihua Sun and Min Zhao},
keywords = {Fixed-time synchronization, Finite-time synchronization, Markovian neural networks, Switching control, Stochastic disturbance},
abstract = {This paper proposes a unified theoretical framework to study the problem of finite/fixed-time drive–response anti-synchronization for a class of Markovian stochastic neural networks. State feedback switching controllers without the sign function are designed to achieve the finite/fixed-time anti-synchronization of the addressed systems. Compared with the existing synchronization criteria, our results indicate that the controllers via the switching control without the sign function are given with less conservativeness, and the controllers without any sign function can deal with the chattering problem. By employing Lyapunov functional method and properties of the Weiner process, several finite/fixed-time synchronization criteria are presented and the corresponding settling times are calculated as well. Finally, three numerical examples are provided to illustrate the effectiveness of the theoretical results.}
}
@article{GAO202082,
title = {Learning physical properties in complex visual scenes: An intelligent machine for perceiving blood flow dynamics from static CT angiography imaging},
journal = {Neural Networks},
volume = {123},
pages = {82-93},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303764},
author = {Zhifan Gao and Xin Wang and Shanhui Sun and Dan Wu and Junjie Bai and Youbing Yin and Xin Liu and Heye Zhang and Victor Hugo C. {de Albuquerque}},
keywords = {Learning physical properties, Tree-structured RNN, LSTM, Fractional flow reserve, CT angiography},
abstract = {Humans perceive physical properties such as motion and elastic force by observing objects in visual scenes. Recent research has proven that computers are capable of inferring physical properties from camera images like humans. However, few studies perceive the physical properties in more complex environment, i.e. humans have difficulty estimating physical quantities directly from the visual observation, or encounter difficulty visualizing the physical process in mind according to their daily experiences. As an appropriate example, fractional flow reserve (FFR), which measures the blood pressure difference across the vessel stenosis, becomes an important physical quantitative value determining the likelihood of myocardial ischemia in clinical coronary intervention procedure. In this study, we propose a novel deep neural network solution (TreeVes-Net) that allows machines to perceive FFR values directly from static coronary CT angiography images. Our framework fully utilizes a tree-structured recurrent neural network (RNN) with a coronary representation encoder. The encoder captures coronary geometric information providing the blood fluid-related representation. The tree-structured RNN builds a long-distance spatial dependency of blood flow information inside the coronary tree. The experiments performed on 13000 synthetic coronary trees and 180 real coronary trees from clinical patients show that the values of the area under ROC curve (AUC) are 0.92 and 0.93 under two clinical criterions. These results can demonstrate the effectiveness of our framework and its superiority to seven FFR computation methods based on machine learning.}
}
@article{TANG2020223,
title = {Person Re-Identification with Feature Pyramid Optimization and Gradual Background Suppression},
journal = {Neural Networks},
volume = {124},
pages = {223-232},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300149},
author = {Yingzhi Tang and Xi Yang and Nannan Wang and Bin Song and Xinbo Gao},
keywords = {Person re-identification, End-to-end, Feature pyramid optimization, Gradual Background Suppression},
abstract = {Compared with face recognition, the performance of person re-identification (re-ID) is still far from practical application. Among various interferences, there are two factors seriously limiting the performance improvement, i.e., the feature discriminability determined by “external network effectiveness”, and the image quality determined by “internal background clutters”. Target at the “external network effectiveness” problem, feature pyramids are effective to learn discriminative features because they can learn both detailed features from high-resolution shallow layers and semantical features from low-resolution deep layers, however, it can only achieve slight improvement on re-ID tasks because of the error back propagation problem. To handle the problem and utilize the effectiveness of feature pyramids, we propose a strategy called Feature Pyramid Optimization (FPO). Instead of concatenating features directly, the selected layers are optimized independently in a top–bottom order. Target at the “internal background clutters” problem, background suppression is generally considered for removing the environmental interference and improving the image quality. Several mask-based methods are used attempting to totally remove background clutters but achieve limited promotion because of the mask sharpening effect. We propose a novel strategy, i.e., Gradual Background Suppression (GBS) to reduce the background clutters and keep the smoothness of images simultaneously. Extensive experiments have been conducted and the results demonstrate the effectiveness of both FPO and GBS.}
}
@article{LIU2020381,
title = {The role of coupling connections in a model of the cortico-basal ganglia-thalamocortical neural loop for the generation of beta oscillations},
journal = {Neural Networks},
volume = {123},
pages = {381-392},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019304216},
author = {Chen Liu and Changsong Zhou and Jiang Wang and Chris Fietkiewicz and Kenneth A. Loparo},
keywords = {Cortico-basal ganglia-thalamocortical circuits, Double-oscillator system, Beta oscillations, Neural mass model, Coupling strength},
abstract = {Excessive neural synchronization in the cortico-basal ganglia-thalamocortical circuits in the beta (β) frequency range (12–35 Hz) is closely associated with dopamine depletion in Parkinson’s disease (PD) and correlated with movement impairments, but the neural basis remains unclear. In this work, we establish a double-oscillator neural mass model for the cortico-basal ganglia-thalamocortical closed-loop system and explore the impacts of dopamine depletion induced changes in coupling connections within or between the two oscillators on neural activities within the loop. Spectral analysis of the neural mass activities revealed that the power and frequency of their principal components are greatly dependent on the coupling strengths between nuclei. We found that the increased intra-coupling in the basal ganglia-thalamic (BG-Th) oscillator contributes to increased oscillations in the lower β frequency band (12–25 Hz), while increased intra-coupling in the cortical oscillator mainly contributes to increased oscillations in the upper β frequency band (26–35 Hz). Interestingly, pathological upper β oscillations in the cortical oscillator may be another origin of the lower β oscillations in the BG-Th oscillator, in addition to increased intra-coupling strength within the BG-Th network. Lower β oscillations in the BG-Th oscillator can also change the dominant oscillation frequency of a cortical nucleus from the upper to the lower β band. Thus, this work may pave the way towards revealing a possible neural basis underlying the Parkinsonian state.}
}
@article{MAMMONE2020357,
title = {A deep CNN approach to decode motor preparation of upper limbs from time–frequency maps of EEG signals at source level},
journal = {Neural Networks},
volume = {124},
pages = {357-372},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.027},
url = {https://www.sciencedirect.com/science/article/pii/S089360802030037X},
author = {Nadia Mammone and Cosimo Ieracitano and Francesco C. Morabito},
keywords = {Brain computer interface, Electroencephalography, Deep learning, Convolutional neural network, Time–frequency analysis, Beamforming},
abstract = {A system that can detect the intention to move and decode the planned movement could help all those subjects that can plan motion but are unable to implement it. In this paper, motor planning activity is investigated by using electroencephalographic (EEG) signals with the aim to decode motor preparation phases. A publicly available database of 61-channels EEG signals recorded from 15 healthy subjects during the execution of different movements (elbow flexion/extension, forearm pronation/supination, hand open/close) of the right upper limb was employed to generate a dataset of EEG epochs preceding resting and movement’s onset. A novel system is introduced for the classification of premovement vs resting and of premovement vs premovement epochs. For every epoch, the proposed system generates a time–frequency (TF) map of every source signal in the motor cortex, through beamforming and Continuous Wavelet Transform (CWT), then all the maps are embedded in a volume and used as input to a deep CNN. The proposed system succeeded in discriminating premovement from resting with an average accuracy of 90.3% (min 74.6%, max 100%), outperforming comparable methods in the literature, and in discriminating premovement vs premovement with an average accuracy of 62.47%. The achieved results encourage to investigate motor planning at source level in the time–frequency domain through deep learning approaches.}
}
@article{YANG202070,
title = {Training high-performance and large-scale deep neural networks with full 8-bit integers},
journal = {Neural Networks},
volume = {125},
pages = {70-82},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.027},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019304290},
author = {Yukuan Yang and Lei Deng and Shuang Wu and Tianyi Yan and Yuan Xie and Guoqi Li},
keywords = {Neural network quantization, 8-bit training, Full quantization, Online learning device},
abstract = {Deep neural network (DNN) quantization converting floating-point (FP) data in the network to integers (INT) is an effective way to shrink the model size for memory saving and simplify the operations for compute acceleration. Recently, researches on DNN quantization develop from inference to training, laying a foundation for the online training on accelerators. However, existing schemes leaving batch normalization (BN) untouched during training are mostly incomplete quantization that still adopts high precision FP in some parts of the data paths. Currently, there is no solution that can use only low bit-width INT data during the whole training process of large-scale DNNs with acceptable accuracy. In this work, through decomposing all the computation steps in DNNs and fusing three special quantization functions to satisfy the different precision requirements, we propose a unified complete quantization framework termed as “WAGEUBN” to quantize DNNs involving all data paths including W (Weights), A (Activation), G (Gradient), E (Error), U (Update), and BN. Moreover, the Momentum optimizer is also quantized to realize a completely quantized framework. Experiments on ResNet18/34/50 models demonstrate that WAGEUBN can achieve competitive accuracy on the ImageNet dataset. For the first time, the study of quantization in large-scale DNNs is advanced to the full 8-bit INT level. In this way, all the operations in the training and inference can be bit-wise operations, pushing towards faster processing speed, decreased memory cost, and higher energy efficiency. Our throughout quantization framework has great potential for future efficient portable devices with online learning ability.}
}
@article{SAHOO202095,
title = {Differential-game for resource aware approximate optimal control of large-scale nonlinear systems with multiple players},
journal = {Neural Networks},
volume = {124},
pages = {95-108},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300022},
author = {Avimanyu Sahoo and Vignesh Narayanan},
keywords = {Approximate dynamic programming, Event-driven control, Neural network control, Nonzero sum game, Optimal control},
abstract = {In this paper, we propose a novel differential-game based neural network (NN) control architecture to solve an optimal control problem for a class of large-scale nonlinear systems involving N-players. We focus on optimizing the usage of the computational resources along with the system performance simultaneously. In particular, the N-players’ control policies are desired to be designed such that they cooperatively optimize the large-scale system performance, and the sampling intervals for each player are desired to reduce the frequency of feedback execution. To develop a unified design framework that achieves both these objectives, we propose an optimal control problem by integrating both the design requirements, which leads to a multi-player differential-game. A solution to this problem is numerically obtained by solving the associated Hamilton-Jacobi (HJ) equation using event-driven approximate dynamic programming (E-ADP) and artificial NNs online and forward-in-time. We employ the critic neural networks to approximate the solution to the HJ equation, i.e., the optimal value function, with aperiodically available feedback information. Using the NN approximated value function, we design the control policies and the sampling schemes. Finally, the event-driven N-player system is remodeled as a hybrid dynamical system with impulsive weight update rules for analyzing its stability and convergence properties. The closed-loop practical stability of the system and Zeno free behavior of the sampling scheme are demonstrated using the Lyapunov method. Simulation results using a numerical example are also included to substantiate the analytical results.}
}
@article{DORADOMORENO2020401,
title = {Multi-task learning for the prediction of wind power ramp events with deep neural networks},
journal = {Neural Networks},
volume = {123},
pages = {401-411},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019304174},
author = {M. Dorado-Moreno and N. Navarin and P.A. Gutiérrez and L. Prieto and A. Sperduti and S. Salcedo-Sanz and C. Hervás-Martínez},
keywords = {Wind power ramp events, Multi-task learning, Multi-output, Deep neural networks, Renewable energies},
abstract = {In Machine Learning, the most common way to address a given problem is to optimize an error measure by training a single model to solve the desired task. However, sometimes it is possible to exploit latent information from other related tasks to improve the performance of the main one, resulting in a learning paradigm known as Multi-Task Learning (MTL). In this context, the high computational capacity of deep neural networks (DNN) can be combined with the improved generalization performance of MTL, by designing independent output layers for every task and including a shared representation for them. In this paper we exploit this theoretical framework on a problem related to Wind Power Ramps Events (WPREs) prediction in wind farms. Wind energy is one of the fastest growing industries in the world, with potential global spreading and deep penetration in developed and developing countries. One of the main issues with the majority of renewable energy resources is their intrinsic intermittency, which makes it difficult to increase the penetration of these technologies into the energetic mix. In this case, we focus on the specific problem of WPREs prediction, which deeply affect the wind speed and power prediction, and they are also related to different turbines damages. Specifically, we exploit the fact that WPREs are spatially-related events, in such a way that predicting the occurrence of WPREs in different wind farms can be taken as related tasks, even when the wind farms are far away from each other. We propose a DNN-MTL architecture, receiving inputs from all the wind farms at the same time to predict WPREs simultaneously in each of the farms locations. The architecture includes some shared layers to learn a common representation for the information from all the wind farms, and it also includes some specification layers, which refine the representation to match the specific characteristics of each location. Finally we modified the Adam optimization algorithm for dealing with imbalanced data, adding costs which are updated dynamically depending on the worst classified class. We compare the proposal against a baseline approach based on building three different independent models (one for each wind farm considered), and against a state-of-the-art reservoir computing approach. The DNN-MTL proposal achieves very good performance in WPREs prediction, obtaining a good balance for all the classes included in the problem (negative ramp, no ramp and positive ramp).}
}
@article{SHEN2020170,
title = {l2–l∞ state estimation for delayed artificial neural networks under high-rate communication channels with Round-Robin protocol},
journal = {Neural Networks},
volume = {124},
pages = {170-179},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300150},
author = {Yuxuan Shen and Zidong Wang and Bo Shen and Fuad E. Alsaadi and Abdullah M. Dobaie},
keywords = {– state estimation, Artificial neural networks, High-rate communication channel, Round-Robin protocol, Time-delays},
abstract = {In this paper, the l2–l∞ state estimation problem is addressed for a class of delayed artificial neural networks under high-rate communication channels with Round-Robin (RR) protocol. To estimate the state of the artificial neural networks, numerous sensors are deployed to measure the artificial neural networks. The sensors communicate with the remote state estimator through a shared high-rate communication channel. In the high-rate communication channel, the RR protocol is utilized to schedule the transmission sequence of the numerous sensors. The aim of this paper is to design an estimator such that, under the high-rate communication channel and the RR protocol, the exponential stability of the estimation error dynamics as well as the l2–l∞ performance constraint are ensured. First, sufficient conditions are given which guarantee the existence of the desired l2–l∞ state estimator. Then, the estimator gains are obtained by solving two sets of matrix inequalities. Finally, numerical examples are provided to verify the effectiveness of the developed l2–l∞ state estimation scheme.}
}
@article{SA2020372,
title = {Synchronization of Hindmarsh Rose Neurons},
journal = {Neural Networks},
volume = {123},
pages = {372-380},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303922},
author = {Malik S.A. and Mir A.H.},
keywords = {Computational neuroscience, Hindmarsh Rose neuron (HR), Digital, Spiking Neural Networks (SNNs), Field programmable gate arrays (FPGAs), Nengo},
abstract = {Modeling and implementation of biological neurons are key to the fundamental understanding of neural network architectures in the brain and its cognitive behavior. Synchronization of neuronal models play a significant role in neural signal processing as it is very difficult to identify the actual interaction between neurons in living brain. Therefore, the synchronization study of these neuronal architectures has received extensive attention from researchers. Higher biological accuracy of these neuronal units demands more computational overhead and requires more hardware resources for implementation. This paper presents a two coupled hardware implementation of Hindmarsh Rose neuron model which is mathematically simpler model and yet mimics several behaviors of a real biological neuron. These neurons are synchronized using an exponential function. The coupled system shows several behaviors depending upon the parameters of HR model and coupling function. An approximation of coupling function is also provided to reduce the hardware cost. Both simulations and a low cost hardware implementations of exponential synaptic coupling function and its approximation are carried out for comparison. Hardware implementation on field programmable gate array (FPGA) of approximated coupling function shows that the coupled network produces different dynamical behaviors with acceptable error. Hardware implementation shows that the approximated coupling function has significantly lower implementation cost. A spiking neural network based on HR neuron is also shown as a practical application of this coupled HR neural networks. The spiking network successfully encodes and decodes a time varying input.}
}
@article{YU2020308,
title = {Model-based optimized phase-deviation deep brain stimulation for Parkinson ’s disease},
journal = {Neural Networks},
volume = {122},
pages = {308-319},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303405},
author = {Ying Yu and Yuqing Hao and Qingyun Wang},
keywords = {Deep brain stimulation, Synchronization level, Parkinson’s disease, Phase deviation},
abstract = {High-frequency deep brain stimulation (HF-DBS) of the subthalamic nucleus (STN), globus pallidus interna (GPi) and globus pallidus externa (GPe) are often considered as effective methods for the treatment of Parkinson’s disease (PD). However, the stimulation of a single nucleus by HF-DBS can cause specific physical damage, produce side effects and usually consume more electrical energy. Therefore, we use a biophysically-based model of basal ganglia-thalamic circuits to explore more effective stimulation patterns to reduce adverse effects and save energy. In this paper, we computationally investigate the combined DBS of two nuclei with the phase deviation between two stimulation waveforms (CDBS). Three different stimulation combination strategies are proposed, i.e., STN and GPe CDBS (SED), STN and GPi CDBS (SID), as well as GPi and GPe CDBS (GGD). Resultantly, it is found that anti-phase CDBS is more effective in improving parkinsonian dynamical properties, including desynchronization of neurons and the recovery of the thalamus relay ability. Detailed simulation investigation shows that anti-phase SED and GGD are superior to SID. Besides, the energy consumption can be largely reduced by SED and GGD (72.5% and 65.5%), compared to HF-DBS. These results provide new insights into the optimal stimulation parameter and target choice of PD, which may be helpful for the clinical practice.}
}
@article{LOBO2020118,
title = {Exploiting the stimuli encoding scheme of evolving Spiking Neural Networks for stream learning},
journal = {Neural Networks},
volume = {123},
pages = {118-133},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303892},
author = {Jesus L. Lobo and Izaskun Oregi and Albert Bifet and Javier {Del Ser}},
keywords = {Stream learning, Gaussian receptive fields, Population encoding, Evolving Spiking Neural Networks},
abstract = {Stream data processing has lately gained momentum with the arrival of new Big Data scenarios and applications dealing with continuously produced information flows. Unfortunately, traditional machine learning algorithms are not prepared to tackle the specific challenges imposed by data stream processing, such as the need for learning incrementally, limited memory and processing time requirements, and adaptation to non-stationary data, among others. To face these paradigms, Spiking Neural Networks have emerged as one of the most promising stream learning techniques, with variants such as Evolving Spiking Neural Networks capable of efficiently addressing many of these challenges. Interestingly, these networks resort to a particular population encoding scheme – Gaussian Receptive Fields – to transform the incoming stimuli into temporal spikes. The study presented in this manuscript sheds light on the predictive potential of this encoding scheme, focusing on how it can be applied as a computationally lightweight, model-agnostic preprocessing step for data stream learning. We provide informed intuition to unveil under which circumstances the aforementioned population encoding method yields effective prediction gains in data stream classification with respect to the case where no preprocessing is performed. Results obtained for a variety of stream learning models and both synthetic and real stream datasets are discussed to empirically buttress the capability of Gaussian Receptive Fields to boost the predictive performance of stream learning methods, spanning further research towards extrapolating our findings to other machine learning problems.}
}
@article{PITONAKOVA2020183,
title = {The robustness-fidelity trade-off in Grow When Required neural networks performing continuous novelty detection},
journal = {Neural Networks},
volume = {122},
pages = {183-195},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303375},
author = {Lenka Pitonakova and Seth Bullock},
keywords = {Novelty detection, Self-organised neural networks, Unsupervised learning},
abstract = {Novelty detection allows robots to recognise unexpected data in their sensory field and can thus be utilised in applications such as reconnaissance, surveillance, self-monitoring, etc. We assess the suitability of Grow When Required Neural Networks (GWRNNs) for detecting novel features in a robot’s visual input in the context of randomised physics-based simulation environments. We compare, for the first time, several GWRNN architectures, including new Plastic architectures in which the number of activated input connections for individual neurons is adjusted dynamically as the robot senses a varying number of salient environmental features. The networks are studied in both one-shot and continuous novelty reporting tasks and we demonstrate that there is a trade-off, not unique to this type of novelty detector, between robustness and fidelity. Robustness is achieved through generalisation over the input space which minimises the impact of network parameters on performance, whereas high fidelity results from learning detailed models of the input space and is especially important when a robot encounters multiple novelties consecutively or must detect that previously encountered objects have disappeared from the environment. We propose a number of improvements that could mitigate the robustness-fidelity trade-off and demonstrate one of them, where localisation information is added to the input data stream being monitored.}
}
@article{UMER2020407,
title = {Person identification using fusion of iris and periocular deep features},
journal = {Neural Networks},
volume = {122},
pages = {407-419},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930348X},
author = {Saiyed Umer and Alamgir Sardar and Bibhas Chandra Dhara and Ranjeet Kumar Rout and Hari Mohan Pandey},
keywords = {Person identification, Iris recognition, Periocular recognition, Data augmentation, Deep learning, Rank-level fusion},
abstract = {A novel method for person identification based on the fusion of iris and periocular biometrics has been proposed in this paper. The challenges for image acquisition for Near-Infrared or Visual Wavelength lights under constrained and unconstrained environments have been considered here. The proposed system is divided into image preprocessing data augmentation followed by feature learning for classification components. In image preprocessing an annular iris, the portion is segmented out from an eyeball image and then transformed into a fixed-sized image region. The parameters of iris localization have been used to extract the local periocular region. Due to different imaging environments, the images suffer from various noise artifacts which create data insufficiency and complicate the recognition task. To overcome this situation, a novel method for data augmentation technique has been introduced here. For features extraction and classification tasks well-known, VGG16, ResNet50, and Inception-v3 CNN architectures have been employed. The performance due to iris and periocular are fused together to increase the performance of the recognition system. The extensive experimental results have been demonstrated in four benchmark iris databases namely: MMU1, UPOL, CASIA-Iris-distance, and UBIRIS.v2. The comparison with the state-of-the-art methods with respect to these databases shows the robustness and effectiveness of the proposed approach.}
}
@article{ZHANG202094,
title = {Discriminative margin-sensitive autoencoder for collective multi-view disease analysis},
journal = {Neural Networks},
volume = {123},
pages = {94-107},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303533},
author = {Zheng Zhang and Qi Zhu and Guo-Sen Xie and Yi Chen and Zhengming Li and Shuihua Wang},
keywords = {Multi-view learning, Latent representation learning, Bioimage classification, Semantic autoencoder, Disease analysis},
abstract = {Medical prediction is always collectively determined based on bioimages collected from different sources or various clinical characterizations described from multiple physiological features. Notably, learning intrinsic structures from multiple heterogeneous features is significant but challenging in multi-view disease understanding. Different from existing methods that separately deal with each single view, this paper proposes a discriminative Margin-Sensitive Autoencoder (MSAE) framework for automated Alzheimer’s disease (AD) diagnosis and accurate protein fold recognition. Generally, our MSAE aims to collaboratively explore the complementary properties of multi-view bioimage features in a semantic-sensitive encoder–decoder paradigm, where the discriminative semantic space is explicitly constructed in a margin-scalable regression model. Specifically, we develop a semantic-sensitive autoencoder, where an encoder projects multi-view visual features into the common semantic-aware latent space, and a decoder is exerted as an additional constraint to reconstruct the respective visual features. In particular, the importance of different views is adaptively weighted by self-adjusting learning scheme, such that their underlying correlations and complementary characteristics across multiple views are simultaneously preserved into the latent common representations. Moreover, a flexible semantic space is formulated by a margin-scalable support vector machine to improve the discriminability of the learning model. Importantly, correntropy induced metric is exploited as a robust regularization measurement to better control outliers for effective classification. A half-quadratic minimization and alternating learning strategy are devised to optimize the resulting framework such that each subproblem exists a closed-form solution in each iterative minimization phase. Extensive experimental results performed on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) datasets show that our MSAE can achieve superior performances for both binary and multi-class classification in AD diagnosis, and evaluations on protein folds demonstrate that our method can achieve very encouraging performance on protein structure recognition, outperforming the state-of-the-art methods.}
}
@article{WEN2020134,
title = {Structured pruning of recurrent neural networks through neuron selection},
journal = {Neural Networks},
volume = {123},
pages = {134-141},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303776},
author = {Liangjian Wen and Xuanyang Zhang and Haoli Bai and Zenglin Xu},
keywords = {Feature selection, Recurrent neural networks, Learning sparse models, Model compression},
abstract = {Recurrent neural networks (RNNs) have recently achieved remarkable successes in a number of applications. However, the huge sizes and computational burden of these models make it difficult for their deployment on edge devices. A practically effective approach is to reduce the overall storage and computation costs of RNNs by network pruning techniques. Despite their successful applications, those pruning methods based on Lasso either produce irregular sparse patterns in weight matrices, which is not helpful in practical speedup. To address these issues, we propose a structured pruning method through neuron selection which can remove the independent neuron of RNNs. More specifically, we introduce two sets of binary random variables, which can be interpreted as gates or switches to the input neurons and the hidden neurons, respectively. We demonstrate that the corresponding optimization problem can be addressed by minimizing the L0 norm of the weight matrix. Finally, experimental results on language modeling and machine reading comprehension tasks have indicated the advantages of the proposed method in comparison with state-of-the-art pruning competitors. In particular, nearly 20× practical speedup during inference was achieved without losing performance for the language model on the Penn TreeBank dataset, indicating the promising performance of the proposed method.}
}
@article{LIU2020109,
title = {Medi-Care AI: Predicting medications from billing codes via robust recurrent neural networks},
journal = {Neural Networks},
volume = {124},
pages = {109-116},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300034},
author = {Deyin Liu and Yuanbo Lin Wu and Xue Li and Lin Qi},
keywords = {Billing codes, Robust recurrent neural networks, Health care data, Medication prediction},
abstract = {In this paper, we present an effective deep prediction framework based on robust recurrent neural networks (RNNs) to predict the likely therapeutic classes of medications a patient is taking, given a sequence of diagnostic billing codes in their record. Accurately capturing the list of medications currently taken by a given patient is extremely challenging due to undefined errors and omissions. We present a general robust framework that explicitly models the possible contamination through overtime decay mechanism on the input billing codes and noise injection into the recurrent hidden states, respectively. By doing this, billing codes are reformulated into its temporal patterns with decay rates on each medical variable, and the hidden states of RNNs are regularized by random noises which serve as dropout to improved RNNs robustness towards data variability in terms of missing values and multiple errors. The proposed method is extensively evaluated on real health care data to demonstrate its effectiveness in suggesting medication orders from contaminated values.}
}
@article{LI202039,
title = {Bipartite synchronization for inertia memristor-based neural networks on coopetition networks},
journal = {Neural Networks},
volume = {124},
pages = {39-49},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303491},
author = {Ning Li and Wei Xing Zheng},
keywords = {Memristive neural networks, Bipartite synchronization, Discontinuous control, Inertia term},
abstract = {This paper addresses the bipartite synchronization problem of coupled inertia memristor-based neural networks with both cooperative and competitive interactions. Generally, coopetition interaction networks are modeled by a signed graph, and the corresponding Laplacian matrix is different from the nonnegative graph. The coopetition networks with structural balance can reach a final state with identical magnitude but opposite sign, which is called bipartite synchronization. Additionally, an inertia system is a second-order differential system. In this paper, firstly, by using suitable variable substitutions, the inertia memristor-based neural networks (IMNNs) are transformed into the first-order differential equations. Secondly, by designing suitable discontinuous controllers, the bipartite synchronization criteria for IMNNs with or without a leader node on coopetition networks are obtained. Finally, two illustrative examples with simulations are provided to validate the effectiveness of the proposed discontinuous control strategies for achieving bipartite synchronization.}
}
@article{SIDDIQUI2020393,
title = {Global collaboration through local interaction in competitive learning},
journal = {Neural Networks},
volume = {123},
pages = {393-400},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019304186},
author = {Abbas Siddiqui and Dionysios Georgiadis},
keywords = {Locally interacting SOM, Competitive & collaborative learning, Point cloud estimation, Topologically preserving maps},
abstract = {Feature maps, that preserve the global topology of arbitrary datasets, can be formed by self-organizing competing agents. So far, it has been presumed that global interaction of agents is necessary for this process. We establish that this is not the case, and that global topology can be uncovered through strictly local interactions. Enforcing uniformity of map quality across all agents results in an algorithm that is able to consistently uncover the global topology of diversely challenging datasets. The applicability and scalability of this approach is further tested on a large point cloud dataset, revealing a linear relation between map training time and size. The presented work not only reduces algorithmic complexity but also constitutes first step towards a distributed self organizing map.}
}
@article{LEIBOLD2020328,
title = {A model for navigation in unknown environments based on a reservoir of hippocampal sequences},
journal = {Neural Networks},
volume = {124},
pages = {328-342},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300162},
author = {Christian Leibold},
keywords = {Hippocampus, Sequences, Reinforcement learning, Replay, Theta oscillations},
abstract = {Hippocampal place cell populations are activated in sequences on multiple time scales during active behavior, resting and sleep states, suggesting that these sequences are the genuine dynamical motifs of the hippocampal circuit. Recently, prewired hippocampal place cell sequences have even been reported to correlate to future behaviors, but so far there is no explanation of what could be the computational benefits of such a mapping between intrinsic dynamical structure and external sensory inputs. Here, I propose a computational model in which a set of predefined internal sequences is used as a dynamical reservoir to construct a spatial map of a large unknown maze based on only a small number of salient landmarks. The model is based on a new variant of temporal difference learning and implements a simultaneous localization and mapping algorithm. As a result sequences during intermittent replay periods can be decoded as spatial trajectories and improve navigation performance, which supports the functional interpretation of replay to consolidate memories of motor actions.}
}
@article{MOIRANGTHEM20201,
title = {Abstractive summarization of long texts by representing multiple compositionalities with temporal hierarchical pointer generator network},
journal = {Neural Networks},
volume = {124},
pages = {1-11},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019304228},
author = {Dennis Singh Moirangthem and Minho Lee},
keywords = {Automatic summarization, Abstraction, RNN encoder–decoder, Seq2seq, Multiple timescale, Temporal hierarchy, Pointer generator},
abstract = {In order to tackle the problem of abstractive summarization of long multi-sentence texts, it is critical to construct an efficient model, which can learn and represent multiple compositionalities better. In this paper, we introduce a temporal hierarchical pointer generator network that can represent multiple compositionalities in order to handle longer sequences of texts with a deep structure. We demonstrate how a multilayer gated recurrent neural network organizes itself with the help of an adaptive timescale in order to represent the compositions. The temporal hierarchical network is implemented with a multiple timescale architecture where the timescale of each layer is also learned during the training process through error backpropagation through time. We evaluate our proposed model using an Introduction-Abstract summarization dataset from scientific articles and the CNN/Daily Mail summarization benchmark dataset. The results illustrate that, we successfully implement a summary generation system for long texts by using the multiple timescale with adaptation concept. We also show that we have improved the summary generation system with our proposed model on the benchmark dataset.}
}
@article{GAUTAM2020191,
title = {Minimum variance-embedded deep kernel regularized least squares method for one-class classification and its applications to biomedical data},
journal = {Neural Networks},
volume = {123},
pages = {191-216},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303934},
author = {Chandan Gautam and Pratik K. Mishra and Aruna Tiwari and Bharat Richhariya and Hari Mohan Pandey and Shuihua Wang and M. Tanveer},
keywords = {One-class classification, Kernel learning, Outlier detection, Alzheimer’s disease, Magnetic resonance imaging, Breast cancer},
abstract = {Deep kernel learning has been well explored for multi-class classification tasks; however, relatively less work is done for one-class classification (OCC). OCC needs samples from only one class to train the model. Most recently, kernel regularized least squares (KRL) method-based deep architecture is developed for the OCC task. This paper introduces a novel extension of this method by embedding minimum variance information within this architecture. This embedding improves the generalization capability of the classifier by reducing the intra-class variance. In contrast to traditional deep learning methods, this method can effectively work with small-size datasets. We conduct a comprehensive set of experiments on 18 benchmark datasets (13 biomedical and 5 other datasets) to demonstrate the performance of the proposed classifier. We compare the results with 16 state-of-the-art one-class classifiers. Further, we also test our method for 2 real-world biomedical datasets viz.; detection of Alzheimer’s disease from structural magnetic resonance imaging data and detection of breast cancer from histopathological images. Proposed method exhibits more than 5% F1 score compared to existing state-of-the-art methods for various biomedical benchmark datasets. This makes it viable for application in biomedical fields where relatively less amount of data is available.}
}
@article{GONG2020364,
title = {Local distinguishability aggrandizing network for human anomaly detection},
journal = {Neural Networks},
volume = {122},
pages = {364-373},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303417},
author = {Maoguo Gong and Huimin Zeng and Yu Xie and Hao Li and Zedong Tang},
keywords = {Human anomaly detection, Local input, Distinguishability, Aggrandizing network},
abstract = {With the growing demand for an intelligent system to prevent abnormal events, many methods have been proposed to detect and locate anomalous behaviors in surveillance videos. However, most of these methods contain two shortcomings mainly: distraction of the network and insufficient discriminating ability. In this paper, we propose a local distinguishability aggrandizing network (LDA-Net) in a supervised manner, consisting of a human detection module and an anomaly detection module. In the human detection module, we obtain segmented patches of specific human subjects and take them as the input of the latter module to focus the network on learning motion characteristics of each person. In addition, considering that the auxiliary information, such as the specific type of an action, can aggrandize the whole network to extract distinguishable detail features of normal and abnormal behaviors, the proposed anomaly detection module comprises a primary binary classification sub-branch and an auxiliary distinguishability aggrandizing sub-branch, through which we can jointly detect anomalies and recognize actions. To further reduce the misclassification of the extremely imbalanced datasets, we design a novel inhibition loss function and embed it into the auxiliary sub-branch of the anomaly detection module. Experiments on several public benchmark datasets for frame-level and pixel-level anomaly detection show that the proposed supervised LDA-Net achieves state-of-the-art results on UCSD Ped2 and Subway Exit datasets.}
}
@article{SONG2020317,
title = {Finite-time nonfragile time-varying proportional retarded synchronization for Markovian Inertial Memristive NNs with reaction–diffusion items},
journal = {Neural Networks},
volume = {123},
pages = {317-330},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019304034},
author = {Xiaona Song and Jingtao Man and Shuai Song and Zhen Wang},
keywords = {Inertial memristive neural networks, Reaction–diffusion items, Markovian jump parameters, Nonfragile time-varying proportional retarded control, Finite-time interval},
abstract = {The issue of synchronization for a class of inertial memristive neural networks over a finite-time interval is investigated in this paper. Specifically, reaction–diffusion items and Markovian jump parameters are both considered in the system model, meanwhile, a novel nonfragile time-varying proportional retarded control strategy is proposed. First, a befitting variable substitution is invoked to transform the original second-order differential system into a first-order one so that the corresponding synchronization error system that is represented by a first-order differential form is established. Second, by utilizing the integral inequality technique, reciprocally convex combination approach and free-weighting matrix method, a less conservative synchronization criterion in terms of linear matrix inequalities is obtained. Finally, three simulations are exploited to illustrate the feasibility, practicability and superiority of the designed controller so that the acquired theoretical results are supported.}
}
@article{ZENG2020331,
title = {ELM embedded discriminative dictionary learning for image classification},
journal = {Neural Networks},
volume = {123},
pages = {331-342},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303740},
author = {Yijie Zeng and Yue Li and Jichao Chen and Xiaofan Jia and Guang-Bin Huang},
keywords = {Discriminative dictionary learning, Extreme learning machine, Sparse representation, Maximum margin criterion},
abstract = {Dictionary learning is a widely adopted approach for image classification. Existing methods focus either on finding a dictionary that produces discriminative sparse representation, or on enforcing priors that best describe the dataset distribution. In many cases, the dataset size is often small with large intra-class variability and nondiscriminative feature space. In this work we propose a simple and effective framework called ELM-DDL to address these issues. Specifically, we represent input features with Extreme Learning Machine (ELM) with orthogonal output projection, which enables diverse representation on nonlinear hidden space and task specific feature learning on output space. The embeddings are further regularized via a maximum margin criterion (MMC) to maximize the inter-class variance and minimize intra-class variance. For dictionary learning, we design a novel weighted class specific ℓ1,2 norm to regularize the sparse coding vectors, which promotes uniformity of the sparse patterns of samples belonging to the same class and suppresses support overlaps of different classes. We show that such regularization is robust, discriminative and easy to optimize. The proposed method is combined with a sparse representation classifier (SRC) to evaluate on benchmark datasets. Results show that our approach achieves state-of-the-art performance compared to other dictionary learning methods.}
}
@article{IERACITANO2020176,
title = {A novel multi-modal machine learning based approach for automatic classification of EEG recordings in dementia},
journal = {Neural Networks},
volume = {123},
pages = {176-190},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303983},
author = {Cosimo Ieracitano and Nadia Mammone and Amir Hussain and Francesco C. Morabito},
keywords = {Machine learning, Continuous wavelet transform, Bispectrum, Alzheimer’s disease, Mild cognitive impairment, Data fusion},
abstract = {Electroencephalographic (EEG) recordings generate an electrical map of the human brain that are useful for clinical inspection of patients and in biomedical smart Internet-of-Things (IoT) and Brain-Computer Interface (BCI) applications. From a signal processing perspective, EEGs yield a nonlinear and nonstationary, multivariate representation of the underlying neural circuitry interactions. In this paper, a novel multi-modal Machine Learning (ML) based approach is proposed to integrate EEG engineered features for automatic classification of brain states. EEGs are acquired from neurological patients with Mild Cognitive Impairment (MCI) or Alzheimer’s disease (AD) and the aim is to discriminate Healthy Control (HC) subjects from patients. Specifically, in order to effectively cope with nonstationarities, 19-channels EEG signals are projected into the time–frequency (TF) domain by means of the Continuous Wavelet Transform (CWT) and a set of appropriate features (denoted as CWT features) are extracted from δ, θ, α1, α2, β EEG sub-bands. Furthermore, to exploit nonlinear phase-coupling information of EEG signals, higher order statistics (HOS) are extracted from the bispectrum (BiS) representation. BiS generates a second set of features (denoted as BiS features) which are also evaluated in the five EEG sub-bands. The CWT and BiS features are fed into a number of ML classifiers to perform both 2-way (AD vs. HC, AD vs. MCI, MCI vs. HC) and 3-way (AD vs. MCI vs. HC) classifications. As an experimental benchmark, a balanced EEG dataset that includes 63 AD, 63 MCI and 63 HC is analyzed. Comparative results show that when the concatenation of CWT and BiS features (denoted as multi-modal (CWT+BiS) features) is used as input, the Multi-Layer Perceptron (MLP) classifier outperforms all other models, specifically, the Autoencoder (AE), Logistic Regression (LR) and Support Vector Machine (SVM). Consequently, our proposed multi-modal ML scheme can be considered a viable alternative to state-of-the-art computationally intensive deep learning approaches.}
}
@article{LI2020395,
title = {Simultaneously learning affinity matrix and data representations for machine fault diagnosis},
journal = {Neural Networks},
volume = {122},
pages = {395-406},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303466},
author = {Yue Li and Yijie Zeng and Tianchi Liu and Xiaofan Jia and Guang-Bin Huang},
keywords = {Extreme learning machine, Autoencoder, Geometry information, Affinity matrix learning, Representation learning, Machine fault diagnosis},
abstract = {Recently, preserving geometry information of data while learning representations have attracted increasing attention in intelligent machine fault diagnosis. Existing geometry preserving methods require to predefine the similarities between data points in the original data space. The predefined affinity matrix, which is also known as the similarity matrix, is then used to preserve geometry information during the process of representations learning. Hence, the data representations are learned under the assumption of a fixed and known prior knowledge, i.e., similarities between data points. However, the assumed prior knowledge is difficult to precisely determine the real relationships between data points, especially in high dimensional space. Also, using two separated steps to learn affinity matrix and data representations may not be optimal and universal for data classification. In this paper, based on the extreme learning machine autoencoder (ELM-AE), we propose to learn the data representations and the affinity matrix simultaneously. The affinity matrix is treated as a variable and unified in the objective function of ELM-AE. Instead of predefining and fixing the affinity matrix, the proposed method adjusts the similarities by taking into account its capability of capturing the geometry information in both original data space and non-linearly mapped representation space. Meanwhile, the geometry information of original data can be preserved in the embedded representations with the help of the affinity matrix. Experimental results on several benchmark datasets demonstrate the effectiveness of the proposed method, and the empirical study also shows it is an efficient tool on machine fault diagnosis.}
}
@article{ZHOU2020319,
title = {Theory of deep convolutional neural networks: Downsampling},
journal = {Neural Networks},
volume = {124},
pages = {319-327},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300204},
author = {Ding-Xuan Zhou},
keywords = {Deep learning, Convolutional neural networks, Approximation theory, Downsampling, Filter masks},
abstract = {Establishing a solid theoretical foundation for structured deep neural networks is greatly desired due to the successful applications of deep learning in various practical domains. This paper aims at an approximation theory of deep convolutional neural networks whose structures are induced by convolutions. To overcome the difficulty in theoretical analysis of the networks with linearly increasing widths arising from convolutions, we introduce a downsampling operator to reduce the widths. We prove that the downsampled deep convolutional neural networks can be used to approximate ridge functions nicely, which hints some advantages of these structured networks in terms of approximation or modeling. We also prove that the output of any multi-layer fully-connected neural network can be realized by that of a downsampled deep convolutional neural network with free parameters of the same order, which shows that in general, the approximation ability of deep convolutional neural networks is at least as good as that of fully-connected networks. Finally, a theorem for approximating functions on Riemannian manifolds is presented, which demonstrates that deep convolutional neural networks can be used to learn manifold features of data.}
}
@article{YU202050,
title = {Exponential and adaptive synchronization of inertial complex-valued neural networks: A non-reduced order and non-separation approach},
journal = {Neural Networks},
volume = {124},
pages = {50-59},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300046},
author = {Juan Yu and Cheng Hu and Haijun Jiang and Leimin Wang},
keywords = {Adaptive design, Complex-valued neural network, Exponential synchronization, Inertial model},
abstract = {This paper mainly deals with the problem of exponential and adaptive synchronization for a type of inertial complex-valued neural networks via directly constructing Lyapunov functionals without utilizing standard reduced-order transformation for inertial neural systems and common separation approach for complex-valued systems. At first, a complex-valued feedback control scheme is designed and a nontrivial Lyapunov functional, composed of the complex-valued state variables and their derivatives, is proposed to analyze exponential synchronization. Some criteria involving multi-parameters are derived and a feasible method is provided to determine these parameters so as to clearly show how to choose control gains in practice. In addition, an adaptive control strategy in complex domain is developed to adjust control gains and asymptotic synchronization is ensured by applying the method of undeterminated coefficients in the construction of Lyapunov functional and utilizing Barbalat Lemma. Lastly, a numerical example along with simulation results is provided to support the theoretical work.}
}
@article{KORNIJCUK202038,
title = {Simplified calcium signaling cascade for synaptic plasticity},
journal = {Neural Networks},
volume = {123},
pages = {38-51},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303909},
author = {Vladimir Kornijcuk and Dohun Kim and Guhyun Kim and Doo Seok Jeong},
keywords = {Synaptic plasticity, Calcium signaling cascade, Back-propagating action potential boost, Synaptic competition},
abstract = {We propose a model for synaptic plasticity based on a calcium signaling cascade. The model simplifies the full signaling pathways from a calcium influx to the phosphorylation (potentiation) and dephosphorylation (depression) of glutamate receptors that are gated by fictive C1 and C2 catalysts, respectively. This model is based on tangible chemical reactions, including fictive catalysts, for long-term plasticity rather than the conceptual theories commonplace in various models, such as preset thresholds of calcium concentration. Our simplified model successfully reproduced the experimental synaptic plasticity induced by different protocols such as (i) a synchronous pairing protocol and (ii) correlated presynaptic and postsynaptic action potentials (APs). Further, the ocular dominance plasticity (or the experimental verification of the celebrated Bienenstock—Cooper—Munro theory) was reproduced by two model synapses that compete by means of back-propagating APs (bAPs). The key to this competition is synapse-specific bAPs with reference to bAP-boosting on the physiological grounds.}
}
@article{DOYA2022xx,
title = {Continual growth and a transition},
journal = {Neural Networks},
volume = {145},
pages = {xx-xxi},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00463-9},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004639},
author = {Kenji Doya and Taro Toyoizumi and DeLiang Wang}
}
@article{ABPEIKAR202020,
title = {Adaptive neural tree exploiting expert nodes to classify high-dimensional data},
journal = {Neural Networks},
volume = {124},
pages = {20-38},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.029},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019304319},
author = {Shadi Abpeikar and Mehdi Ghatee and Gian Luca Foresti and Christian Micheloni},
keywords = {Neural tree, Expert systems, High-dimensional features, Data complexity, Feature clustering},
abstract = {Classification of high dimensional data suffers from curse of dimensionality and over-fitting. Neural tree is a powerful method which combines a local feature selection and recursive partitioning to solve these problems, but it leads to high depth trees in classifying high dimensional data. On the other hand, if less depth trees are used, the classification accuracy decreases or over-fitting increases. This paper introduces a novel Neural Tree exploiting Expert Nodes (NTEN) to classify high-dimensional data. It is based on a decision tree structure, whose internal nodes are expert nodes performing multi-dimensional splitting. Any expert node has three decision-making abilities. Firstly, they can select the most eligible neural network with respect to the data complexity. Secondly, they evaluate the over-fitting. Thirdly, they can cluster the features to jointly minimize redundancy and overlapping. To this aim, metaheuristic optimization algorithms including GA, NSGA-II, PSO and ACO are applied. Based on these concepts, any expert node splits a class when the over-fitting is low, and clusters the features when the over-fitting is high. Some theoretical results on NTEN are derived, and experiments on 35 standard data show that NTEN reaches good classification results, reduces tree depth without over-fitting and degrading accuracy.}
}
@article{2022II,
title = {INN/ENNS/JNNS - Membership Applic. Form},
journal = {Neural Networks},
volume = {145},
pages = {II},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00459-7},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004597}
}
@article{SILVA2020273,
title = {Perceptrons from memristors},
journal = {Neural Networks},
volume = {122},
pages = {273-278},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303351},
author = {Francisco Silva and Mikel Sanz and João Seixas and Enrique Solano and Yasser Omar},
keywords = {Perceptron, Memristor, Backpropagation, Delta rule, Neural network},
abstract = {Memristors, resistors with memory whose outputs depend on the history of their inputs, have been used with success in neuromorphic architectures, particularly as synapses and non-volatile memories. However, to the best of our knowledge, no model for a network in which both the synapses and the neurons are implemented using memristors has been proposed so far. In the present work we introduce models for single and multilayer perceptrons based exclusively on memristors. We adapt the delta rule to the memristor-based single-layer perceptron and the backpropagation algorithm to the memristor-based multilayer perceptron. Our results show that both perform as expected for perceptrons, including satisfying Minsky–Papert’s theorem. As a consequence of the Universal Approximation Theorem, they also show that memristors are universal function approximators. By using memristors for both the neurons and the synapses, our models pave the way for novel memristor-based neural network architectures and algorithms. A neural network based on memristors could show advantages in terms of energy conservation and open up possibilities for other learning systems to be adapted to a memristor-based paradigm, both in the classical and quantum learning realms.}
}
@article{YOU2020248,
title = {Existence and finite-time stability of discrete fractional-order complex-valued neural networks with time delays},
journal = {Neural Networks},
volume = {123},
pages = {248-260},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019304046},
author = {Xingxing You and Qiankun Song and Zhenjiang Zhao},
keywords = {Existence, Discrete time, Fractional-order complex-valued neural networks, Finite-time stability, Delays},
abstract = {Without decomposing complex-valued systems into real-valued systems, the existence and finite-time stability for discrete fractional-order complex-valued neural networks with time delays are discussed in this paper. First of all, in order to obtain the main results, a new discrete Caputo fractional difference equation is proposed in complex field based on the theory of discrete fractional calculus, which generalizes the fractional-order neural networks in the real domain. Additionally, by utilizing Arzela–Ascoli’s theorem, inequality scaling skills and fixed point theorem, some sufficient criteria of delay-dependent are deduced to ensure the existence and finite-time stability of solutions for proposed networks. Finally, the validity and feasibility of the derived theoretical results are indicated by two numerical examples with simulations. Furthermore, we have drawn the following facts: with the lower order, the discrete fractional-order complex-valued neural networks will achieve the finite-time stability more easily.}
}
@article{MUSCI2020108,
title = {A scalable multi-signal approach for the parallelization of self-organizing neural networks},
journal = {Neural Networks},
volume = {123},
pages = {108-117},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303752},
author = {Mirto Musci and Giacomo Parigi and Virginio Cantoni and Marco Piastra},
keywords = {Self-organizing neural networks, Surface reconstruction, GPU parallelization, Self-organizing adaptive map},
abstract = {Self-Organizing Neural Networks (SONNs) have a wide range of applications with massive computational requirements that often need to be satisfied with optimized parallel algorithms and implementations. In literature, SONN have been generally parallelized with GPU computing according to a single-signal paradigm: each GPU thread manages one or more nodes of the network and works concurrently on one input signal at the time. This paper presents two contributions. The first one is the experimental proof that the single-signal approach for SONNs is not optimal for the task, as it is intrinsically sequential at its core and thus inherently limited in its performance. The non-optimality of the single-signal paradigm is illustrated via a specific and simplified benchmark. The second contribution is the introduction of a new multi-signal paradigm for the parallelization of SONNs, whereby multiple signals are processed at once in each iteration hence allowing different GPU threads to work on different signals. The advantages of the multi-signal approach are shown through several benchmarks involving the Self-Organizing Adaptive Map (SOAM) algorithm as a basis for evaluation. Having a graph-based termination condition that depends on the features of the network being grown, the SOAM algorithm allows assessing both functional equivalence and performances of the paradigm proposed without relying on arbitrary thresholds. Nonetheless, the evaluation proposed has a broader scope since it refers to a unified framework for the GPU parallelization of a generic SONN.}
}
@article{NAPOLES2020258,
title = {Deterministic learning of hybrid Fuzzy Cognitive Maps and network reduction approaches},
journal = {Neural Networks},
volume = {124},
pages = {258-268},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300216},
author = {Gonzalo Nápoles and Agnieszka Jastrzębska and Carlos Mosquera and Koen Vanhoof and Władysław Homenda},
keywords = {Fuzzy cognitive maps, Hybrid models, Inverse learning, Interpretability},
abstract = {Hybrid artificial intelligence deals with the construction of intelligent systems by relying on both human knowledge and historical data records. In this paper, we approach this problem from a neural perspective, particularly when modeling and simulating dynamic systems. Firstly, we propose a Fuzzy Cognitive Map architecture in which experts are requested to define the interaction among the input neurons. As a second contribution, we introduce a fast and deterministic learning rule to compute the weights among input and output neurons. This parameterless learning method is based on the Moore–Penrose inverse and it can be performed in a single step. In addition, we discuss a model to determine the relevance of weights, which allows us to better understand the system. Last but not least, we introduce two calibration methods to adjust the model after the removal of potentially superfluous weights.}
}
@article{ZENG2020130,
title = {A causal discovery algorithm based on the prior selection of leaf nodes},
journal = {Neural Networks},
volume = {124},
pages = {130-145},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019304204},
author = {Yan Zeng and Zhifeng Hao and Ruichu Cai and Feng Xie and Liang Ou and Ruihui Huang},
keywords = {Causal discovery, Linear Non-Gaussian Acyclic Models, Leaf nodes, Causal order},
abstract = {In recent years, Linear Non-Gaussian Acyclic Model (LiNGAM) has been widely used for the discovery of causal network. However, solutions based on LiNGAM usually yield high computational complexity as well as unsatisfied accuracy when the data is high-dimensional or the sample size is too small. Such complexity or accuracy problems here are often originated from their prior selection of root nodes when estimating a causal ordering. Thus, a causal discovery algorithm termed as GPL algorithm (the LiNGAM algorithm of Giving Priority to Leaf-nodes) under a mild assumption is proposed in this paper. It assigns priority to leaf nodes other than root nodes. Since leaf nodes do not affect others in a structure, we can directly estimate a causal ordering in a bottom-up way without performing additional operations like data updating process. Corresponding proofs for both feasibility and superiority are offered based on the properties of leaf nodes. Aside from theoretical analyses, practical experiments are conducted on both synthetic and real-world data, which confirm that GPL algorithm outperforms the other two state-of-the-art algorithms in computational complexity and accuracy, especially when dealing with high-dimensional data (up to 200) or small sample size (down to 100 for the dimension of 70).}
}
@article{CAO202070,
title = {Global exponential synchronization of delayed memristive neural networks with reaction–diffusion terms},
journal = {Neural Networks},
volume = {123},
pages = {70-81},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303478},
author = {Yanyi Cao and Yuting Cao and Zhenyuan Guo and Tingwen Huang and Shiping Wen},
keywords = {Global exponential synchronization, Delayed memristive neural networks, Reaction–diffusion terms, Pinning control technique},
abstract = {This paper investigates the global exponential synchronization problem of delayed memristive neural networks (MNNs) with reaction–diffusion terms. First, by utilizing the pinning control technique, two novel kinds of control methods are introduced to achieve synchronization of delayed MNNs with reaction–diffusion terms. Then, with the help of inequality techniques, pinning control technique, the drive–response concept and Lyapunov functional method, two sufficient conditions are obtained in the form of algebraic inequalities, which can be used for ensuring the exponential synchronization of the proposed delayed MNNs with reaction–diffusion terms. Moreover, the obtained results based on algebraic inequality complement and improve the previously known results. Finally, two illustrative examples are given to support the effectiveness and validity of the obtained theoretical results.}
}
@article{NAHMIAS2020243,
title = {Deep feature transfer learning for trusted and automated malware signature generation in private cloud environments},
journal = {Neural Networks},
volume = {124},
pages = {243-257},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300058},
author = {Daniel Nahmias and Aviad Cohen and Nir Nissim and Yuval Elovici},
keywords = {Deep learning, Transfer learning, Convolutional neural networks, Malware detection, Cryptojacking, Automatic signature generation},
abstract = {This paper presents TrustSign, a novel, trusted automatic malware signature generation method based on high-level deep features transferred from a VGG-19 neural network model pretrained on the ImageNet dataset. While traditional automatic malware signature generation techniques rely on static or dynamic analysis of the malware’s executable, our method overcomes the limitations associated with these techniques by producing signatures based on the presence of the malicious process in the volatile memory. By leveraging the cloud’s virtualization technology, TrustSign analyzes the malicious process in a trusted manner, since the malware is unaware and cannot interfere with the inspection procedure. Additionally, by removing the dependency on the malware’s executable, our method is fully capable of signing fileless malware as well. TrustSign’s signature generation process does not require feature engineering or any additional model training, and it is done in a completely unsupervised manner, eliminating the need for a human expert. Because of this, our method has the advantage of dramatically reducing signature generation and distribution time. In fact, in this paper we rethink the typical use of deep convolutional neural networks and use the VGG-19 model as a topological feature extractor for a vastly different task from the one it was trained for. The results of our experimental evaluation demonstrate TrustSign’s ability to generate signatures impervious to the process state over time. By using the signatures generated by TrustSign as input for various supervised classifiers, we achieved up to 99.5% classification accuracy.}
}
@article{GUO2020239,
title = {Multistability of switched neural networks with sigmoidal activation functions under state-dependent switching},
journal = {Neural Networks},
volume = {122},
pages = {239-252},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930334X},
author = {Zhenyuan Guo and Shiqin Ou and Jun Wang},
keywords = {Multistability, Switched neural network, State-dependent, Sigmoidal activation function},
abstract = {This paper presents theoretical results on the multistability of switched neural networks with commonly used sigmoidal activation functions under state-dependent switching. The multistability analysis with such an activation function is difficult because state–space partition is not as straightforward as that with piecewise-linear activations. Sufficient conditions are derived for ascertaining the existence and stability of multiple equilibria. It is shown that the number of stable equilibria of an n-neuron switched neural networks is up to 3n under given conditions. In contrast to existing multistability results with piecewise-linear activation functions, the results herein are also applicable to the equilibria at switching points. Four examples are discussed to substantiate the theoretical results.}
}
@article{OSHEA202012,
title = {Neonatal seizure detection from raw multi-channel EEG using a fully convolutional architecture},
journal = {Neural Networks},
volume = {123},
pages = {12-25},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.11.023},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019303910},
author = {Alison O’Shea and Gordon Lightbody and Geraldine Boylan and Andriy Temko},
keywords = {Convolutional neural networks, EEG, Neonatal seizure detection, Weak labels},
abstract = {A deep learning classifier for detecting seizures in neonates is proposed. This architecture is designed to detect seizure events from raw electroencephalogram (EEG) signals as opposed to the state-of-the-art hand engineered feature-based representation employed in traditional machine learning based solutions. The seizure detection system utilises only convolutional layers in order to process the multichannel time domain signal and is designed to exploit the large amount of weakly labelled data in the training stage. The system performance is assessed on a large database of continuous EEG recordings of 834h in duration; this is further validated on a held-out publicly available dataset and compared with two baseline SVM based systems. The developed system achieves a 56% relative improvement with respect to a feature-based state-of-the art baseline, reaching an AUC of 98.5%; this also compares favourably both in terms of performance and run-time. The effect of varying architectural parameters is thoroughly studied. The performance improvement is achieved through novel architecture design which allows more efficient usage of available training data and end-to-end optimisation from the front-end feature extraction to the back-end classification. The proposed architecture opens new avenues for the application of deep learning to neonatal EEG, where the performance becomes a function of the amount of training data with less dependency on the availability of precise clinical labels.}
}
@article{DOYA2022xix,
title = {Announcement of the Neural Networks Best Paper Award},
journal = {Neural Networks},
volume = {145},
pages = {xix},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(21)00464-0},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004640},
author = {Kenji Doya and DeLiang Wang}
}