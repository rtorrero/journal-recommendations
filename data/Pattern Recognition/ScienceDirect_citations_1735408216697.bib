@article{LI2023109831,
title = {Semi-supervised transfer learning with hierarchical self-regularization},
journal = {Pattern Recognition},
volume = {144},
pages = {109831},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109831},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005290},
author = {Xingjian Li and Abulikemu Abuduweili and Humphrey Shi and Pengkun Yang and Dejing Dou and Haoyi Xiong and Chengzhong Xu},
keywords = {Semi-supervised learning, Transfer learning, Fine-tuning, Deep learning, Hierarchical consistency, Adaptive sample selection},
abstract = {Both semi-supervised learning and transfer learning aim at lowering the annotation burden for training models. However, such two tasks are usually studied separately, i.e. most semi-supervised learning algorithms train models from scratch while transfer learning assumes pre-trained models as the initialization. In this work, we focus on a previously-less-concerned setting that further reduces the annotation efforts through incorporating both semi-supervised and transfer learning, where specifically a pre-trained source model is used as the initialization of semi-supervised learning. As those powerful pre-trained models are ubiquitously available nowadays and can considerably benefit various down-streaming tasks, such a setting is relevant to real-world applications yet challenging to design effective algorithms. Aiming at enabling transfer learning under semi-supervised settings, we propose a hierarchical self-regularization mechanism to exploit unlabeled samples more effectively, where a novel self-regularizer has been introduced to incorporate both individual-level and population-level regularization terms. The former term employs self-distillation to regularize learned deep features for each individual sample, and the latter one enforces self-consistency on feature distributions between labeled and unlabeled samples. Samples involved in both regularizers are weighted by an adaptive strategy, where self-regularization effects of both terms are adaptively controlled by the confidence of every sample. To validate our algorithm, exhaustive experiments have been conducted on diverse datasets such as CIFAR-10 for general object recognition, CUB-200-2011/MIT-indoor-67 for fine-grained classification and MURA for medical image classification. Compared with state-of-the-art semi-supervised learning methods including Pseudo Label, Mean Teacher, MixMatch and FixMatch, our algorithm demonstrates two advantages: first of all, the proposed approach adopts a new point of view to tackle problems caused by inadequate supervision and achieves very competitive results; then, it is complementary to these state-of-the-art methods and thus can be combined with them to get additional improvements. Furthermore, our method can also be applied to fully supervised transfer learning and self-supervised learning. We have published our code at https://github.com/SHI-Labs/Semi-Supervised-Transfer-Learning.}
}
@article{XU2023109811,
title = {Cross-Domain Few-Shot classification via class-shared and class-specific dictionaries},
journal = {Pattern Recognition},
volume = {144},
pages = {109811},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109811},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005095},
author = {Renjie Xu and Lei Xing and Baodi Liu and Dapeng Tao and Weijia Cao and Weifeng Liu},
keywords = {Few-shot learning, Dictionary learning, Cross-Domain, Collaborative representation},
abstract = {In Cross-Domain Few-Shot Classification, researchers mainly utilize models which trained with source domain tasks to adapt to the target domain with very few samples, thus causing serious class-difference-caused domain differences. Although researchers have proposed methods to minimize the domain differences, the existing methods have the following drawbacks: 1) most models do not utilize the common knowledge between the source and target domains, and 2) require additional labeled samples from the target domain for finetuning or domain alignment, which is hard to obtain in reality. To address the problem mentioned above, we propose a class-shared and class-specific dictionaries (CSCSD) learning method. To make better utilization of the common knowledge, we apply a class-shared dictionary which is learned to represent the generality of source and target domain. Moreover, class-specific dictionaries are applied to represent the class-specific knowledge that can’t be represented in the class-shared dictionary. Furthermore, unlike most other models, our CSCSD does not require additional target domain samples to meta-train or finetune. With the dictionaries, CSCSD can obtain more distinguishable collaborative representations of samples with the origin representations extracted with the model. To evaluate the effectiveness of CSCSD, we utilize larger datasets, e.g., MiniImageNet and TieredImageNet as source domains and fine-grained datasets, e.g., CUB, Cars, Places, and Plantae as target domains. With our CSCSD, the Cross-Domain Few-Shot accuracy exceeds most domain adaptive Few-Shot which utilizes additional training set in target domains.}
}
@article{NELLAS2023109871,
title = {Two phase cooperative learning for supervised dimensionality reduction},
journal = {Pattern Recognition},
volume = {144},
pages = {109871},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109871},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005691},
author = {Ioannis A. Nellas and Sotiris K. Tasoulis and Spiros V. Georgakopoulos and Vassilis P. Plagianakos},
keywords = {Artificial neural networks, Deep learning, Dimensionality reduction, Autoencoders, Image classification},
abstract = {The simultaneous minimization of the reconstruction and classification error is a hard non convex problem, especially when a non-linear mapping is utilized. To overcome this obstacle, motivated by the widespread success of Cooperative Neural Networks, an innovative supervised dimensionality reduction framework is proposed, based on a cooperative two phase optimization strategy. Specifically, the proposed framework that requires minimal parameter adjustment consists of an autoencoder for dimensionality reduction and a separator network for separability assessment of the embedding. This scheme results in meaningful and discriminable codes, which are optimized for the classification task and are exploitable by any trainable classifier. The experimental results showed that the proposed methodology achieved competitive results against the state-of-the-art competing methods, while being much more efficient in terms of parameter count. Finally, it was empirically justified that the proposed methodology introduces advanced behavioural explainability, while enabling applicability for image generation tasks.}
}
@article{DAS2023109879,
title = {Estimation of interlayer textural relationships to discriminate the benignancy/malignancy of brain tumors},
journal = {Pattern Recognition},
volume = {144},
pages = {109879},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109879},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005770},
author = {Poulomi Das and Arpita Das},
keywords = {Advanced PCNN module, Classification, FCM clustering algorithm, Interlayer feature quantifiers, NSST based decomposition},
abstract = {A computer-aided diagnosis system is a popular tool to predict the risk factors of brain tumors. However, the existing techniques are unable to provide high detection accuracy due to the inability of capturing the hidden features of brain tumors. In this view, the present work decomposes the MR modality images into different layers using non-sub-sampled shearlet transformation (NSST). Following this, the proposed detection pipeline employs adaptive pulse-coupled neural network (A-PCNN) module and a fuzzy c-means (FCM) clustering algorithm for enhancement and segmentation of suspicious regions respectively. Interlayer textural relationships of tumors are estimated in terms of the layer-wise difference of the gray-level cooccurrence matrix (GLCM), similarity indices, and entropy distributions. Thenceforth, those feature coefficients are coded with binary sequences to express the textural homogeneity/randomness of tumors. Finally, these handcrafted multi-scale feature quantifiers are fed to some standard classifiers such as the k-nearest neighbor (kNN) technique, the linear square support vector machine (LS-SVM), and the decision tree (DT) for discriminating the state of benignancy/malignancy of tumors. Experimental results ensure that the proposed detection model shows superior performance compared to existing methods.}
}
@article{ZHOU2023109827,
title = {Feature fusion network for long-tailed visual recognition},
journal = {Pattern Recognition},
volume = {144},
pages = {109827},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109827},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005253},
author = {Xuesong Zhou and Junhai Zhai and Yang Cao},
keywords = {Long-tailed learning, Head and tail classes, Feature representations, Feature fusion network},
abstract = {Deep learning has achieved remarkable success in recent years; however, deep learning methods face significant challenges on long-tailed datasets, which are prevalent in real-world scenarios. In a long-tailed dataset, there are many more samples in the head classes than in the tail classes, and this class imbalance makes it difficult to learn a good feature representation for both head and tail classes simultaneously, particularly when using a single-stage method. Although the existing two-stage methods can alleviate the problem of single-stage methods not performing well on the tail classes by classifier retraining in the second stage, this does not resolve the problem of insufficient learning of head and tail features. Thus, in this paper, we propose a two-stage feature fusion network (FFN). The proposed FFN addresses this issue using one network for the head classes and another network for the tail classes, each of which is trained with a different loss function. This allows the feature learning module to effectively distinguish between the head and tail classes in the embedding space. The classifier learning module fuses the features obtained from the feature learning module, and the classifier is fine-tuned to classify the input images. Different from traditional two-stage methods, the proposed utilizes different loss functions for the head and tail classes; thus, the classifier can achieve balanced results between the head and tail classes. We conduct extensive experiments on three benchmark datasets comparing the proposed FFN with six state-of-the-art methods including two baseline methods, the experimental results demonstrate that the FFN achieves significant improvement on all three benchmark datasets. The code is publicly available at https://github.com/zxsong999/Feature-Fusion-Network.pytorch.}
}
@article{TAN2023109883,
title = {Semantic Similarity Distance: Towards better text-image consistency metric in text-to-image generation},
journal = {Pattern Recognition},
volume = {144},
pages = {109883},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109883},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005812},
author = {Zhaorui Tan and Xi Yang and Zihan Ye and Qiufeng Wang and Yuyao Yan and Anh Nguyen and Kaizhu Huang},
keywords = {Text-to-image, Image generation, Generative adversarial networks, Semantic consistency},
abstract = {Generating high-quality images from text remains a challenge in visual-language understanding, with text-image consistency being a major concern. Particularly, the most popular metric R-precision may not accurately reflect the text-image consistency, leading to misleading semantics in generated images. Albeit its significance, designing a better text-image consistency metric surprisingly remains under-explored in the community. In this paper, we make a further step forward to develop a novel CLIP-based metric, Semantic Similarity Distance (SSD), which is both theoretically founded from a distributional viewpoint and empirically verified on benchmark datasets. We also introduce Parallel Deep Fusion Generative Adversarial Networks (PDF-GAN), which use two novel components to mitigate inconsistent semantics and bridge the text-image semantic gap. A series of experiments indicate that, under the guidance of SSD, our developed PDF-GAN can induce remarkable enhancements in the consistency between texts and images while preserving acceptable image quality over the CUB and COCO datasets.}
}
@article{MAJUMDAR2023109689,
title = {Uniform misclassification loss for unbiased model prediction},
journal = {Pattern Recognition},
volume = {144},
pages = {109689},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109689},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003874},
author = {Puspita Majumdar and Mayank Vatsa and Richa Singh},
keywords = {Bias, Fairness, Facial attribute prediction, Deep learning, Unbiased predictions},
abstract = {Deep learning algorithms have achieved tremendous success over the past few years. However, the biased behavior of deep models, where the models favor/disfavor certain demographic subgroups, is a major concern in the deep learning community. Several adverse consequences of biased predictions have been observed in the past. One solution to alleviate the problem is to train deep models for fair outcomes. Therefore, in this research, we propose a novel loss function, termed as Uniform Misclassification Loss (UML) to train deep models for unbiased outcomes. The proposed UML function penalizes the model for the worst-performing subgroup for mitigating bias and enhancing the overall model performance. The proposed loss function is also effective while training with imbalanced data as well. Further, a metric, Joint Performance Disparity Measure (JPD) is introduced to jointly measure the overall model performance and the bias in model prediction. Multiple experiments have been performed on four publicly available datasets for facial attribute prediction and comparisons are performed with existing bias mitigation algorithms. Experimental results are reported using performance and bias evaluation metrics. The proposed loss function outperforms existing bias mitigation algorithms that showcase its effectiveness in obtaining unbiased outcomes and improved performance.}
}
@article{QIU2023109863,
title = {Few-shot forgery detection via Guided Adversarial Interpolation},
journal = {Pattern Recognition},
volume = {144},
pages = {109863},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109863},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005617},
author = {Haonan Qiu and Siyu Chen and Bei Gan and Kun Wang and Huafeng Shi and Jing Shao and Ziwei Liu},
keywords = {Forgery detection, DeepFake, Few-shot, Face manipulation},
abstract = {The increase in face manipulation models has led to a critical issue in society—the synthesis of realistic visual media. With the emergence of new forgery approaches at an unprecedented rate, existing forgery detection methods suffer from significant performance drops when applied to unseen novel forgery approaches. In this work, we address the few-shot forgery detection problem by (1) designing a comprehensive benchmark based on coverage analysis among various forgery approaches, and (2) proposing Guided Adversarial Interpolation (GAI). Our key insight is that there exist transferable distribution characteristics between majority and minority forgery classes.11Majority class: class with abundant samples; minority class: class with scarce samples. Specifically, we enhance the discriminative ability against novel forgery approaches via adversarially interpolating the forgery artifacts of the minority samples to the majority samples under the guidance of a teacher network. Unlike the standard re-balancing method which usually results in over-fitting to minority classes, our method simultaneously takes account of the diversity of majority information as well as the significance of minority information. Extensive experiments demonstrate that our GAI achieves state-of-the-art performances on the established few-shot forgery detection benchmark. Notably, our method is also validated to be robust to choices of majority and minority forgery approaches.}
}
@article{ZHANG2023109851,
title = {Graph matching for knowledge graph alignment using edge-coloring propagation},
journal = {Pattern Recognition},
volume = {144},
pages = {109851},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109851},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005496},
author = {Yuxuan Zhang and Yuanxiang Li and Xian Wei and Yongsheng Yang and Lei Liu and Yi Lu Murphey},
keywords = {Knowledge graph, Entity alignment, Relation alignment, Quadratic assignment problem},
abstract = {Knowledge graph (KG) is a kind of structured human knowledge of modeling the relationships between real-world entities. High quality KG is of crucial importance for many knowledge-based applications, e.g., question answering, recommender systems, etc. This paper studies the problem of entity alignment in KGs to promote knowledge fusion. Existing methods model the semantic representation of entities by using graph structural information or attribute information of the KG and then align the entities across different domains by calculating the distances between entities’ embeddings. However, these methods only consider the node-to-node similarity in the alignment procedure while the edge-to-edge similarity is ignored. Our research hypothesis is that the graph edge alignment information is critical in entity alignment. We reformulate the knowledge entity alignment as a quadratic assignment problem (QAP) by adding relation alignment under the one-to-one mapping constraint. To solve the notorious QAP in a large-scale heterogeneous graph like KG, we propose a model, dual neighborhood consensus network (DNCN), which approximately decomposes the QAP into two small-scale linear assignment problems, i.e., entity alignment and relation alignment. After that, an edge-coloring propagation method is proposed to refine the coarse entity alignment result using the relation correspondence. Theoretical proof shows that this method can guarantee the isomorphism between local sub-graphs. The performance of DNCN is evaluated using the DBP15K and DWY100K benchmarks. Experimental results show that DNCN achieves the best performance on the DBP15K benchmark, and is computationally efficient. Ablation studies verify the importance of graph edge alignment information.}
}
@article{LI2023109835,
title = {Dynamics-aware loss for learning with label noise},
journal = {Pattern Recognition},
volume = {144},
pages = {109835},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109835},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005332},
author = {Xiu-Chuan Li and Xiaobo Xia and Fei Zhu and Tongliang Liu and Xu-Yao Zhang and Cheng-Lin Liu},
keywords = {Label noise, Dynamics, Robust loss function},
abstract = {Label noise poses a serious threat to deep neural networks (DNNs). Employing robust loss functions which reconcile fitting ability with robustness is a simple but effective strategy to handle this problem. However, the widely-used static trade-off between these two factors contradicts the dynamics of DNNs learning with label noise, leading to inferior performance. Therefore, we propose a dynamics-aware loss (DAL) to solve this problem. Considering that DNNs tend to first learn beneficial patterns, then gradually overfit harmful label noise, DAL strengthens the fitting ability initially, then gradually improves robustness. Moreover, at the later stage, to further reduce the negative impact of label noise and combat underfitting simultaneously, we let DNNs put more emphasis on easy examples than hard ones and introduce a bootstrapping term. Both the detailed theoretical analyses and extensive experimental results demonstrate the superiority of our method.}
}
@article{GILLIOZ2023109859,
title = {Graph-based pattern recognition on spectral reduced graphs},
journal = {Pattern Recognition},
volume = {144},
pages = {109859},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109859},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005575},
author = {Anthony Gillioz and Kaspar Riesen},
keywords = {Graph matching, Graph classification, Graph reduction},
abstract = {Graph-based pattern recognition – in particular in conjunction with large graphs – is often computationally expensive. This hampers, or makes it at least challenging, to employ graph-based representations for real-world data. To address this issue, we propose a method for reducing the size of the underlying graphs to their most important substructures using spectral graph clustering. The proposed method partitions the nodes of the graphs into clusters and then merges each cluster into supernodes. The motivation of this procedure is to reduce the computational cost of any graph comparison algorithm while maintaining the accuracy of the final classification. To assess the benefits and limitations of our method, we conduct thorough experiments on nine real-world datasets with different levels of graph reductions. The classification is obtained by four different graph classifiers (viz. a KNN based on graph edit distance, two SVMs based on a shortest path graph and a Weisfeiler–Lehman graph kernel, as well as a graph neural network). The results indicate that we can reduce computation time by up to two orders of magnitude without substantially degrading the classification accuracy.}
}
@article{HAO2023109843,
title = {Contrastive Generative Network with Recursive-Loop for 3D point cloud generalized zero-shot classification},
journal = {Pattern Recognition},
volume = {144},
pages = {109843},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109843},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005411},
author = {Yun Hao and Yukun Su and Guosheng Lin and Hanjing Su and Qingyao Wu},
keywords = {3D point cloud, Generalized zero-shot, Contrastive learning, Recursive-loop},
abstract = {Generalized Zero-Shot Learning (GZSL) aims to recognize objects from both seen and unseen categories by transferring semantic knowledge and merely utilizing seen class data for training. Recent feature generation methods in the 2D image domain have made great progress. However, very little is known about its usefulness in 3D point cloud zero-shot learning. This work aims to facilitate research on 3D point cloud generalized zero-shot learning. Different from previous works, we focus on synthesizing the more high-level discriminative point cloud features. To this end, we design a representation enhancement strategy to generate the features. Specifically, we propose a Contrastive Generative Network with Recursive-Loop, termed as CGRL, which can be leveraged to enlarge the inter-class distances and narrow the intra-class gaps. By applying the contrastive representations to the generative model in a recursive-loop form, it can provide the self-guidance for the generator recurrently, which can help yield more discriminative features and train a better classifier. To validate the effectiveness of the proposed method, extensive experiments are conducted on three benchmarks, including ModelNet40, McGill, and ScanObjectNN. Experimental evaluations demonstrate the superiority of our approach and it can outperform the state-of-the-arts by a large margin. Code is available at https://github.com/photon-git/CGRL}
}
@article{APICELLA2023109867,
title = {Adaptive filters in Graph Convolutional Neural Networks},
journal = {Pattern Recognition},
volume = {144},
pages = {109867},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109867},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005654},
author = {Andrea Apicella and Francesco Isgrò and Andrea Pollastro and Roberto Prevete},
keywords = {Graph Convolutional Neural Networks, Deep learning, Dynamic neural networks, Programmable ANNs, Graph structure learning},
abstract = {Over the last few years, the availability of an increasing data generated from non-Euclidean domains, which are usually represented as graphs with complex relationships, and Graph Neural Networks (GNN) have gained a high interest because of their potential in processing graph-structured data. In particular, there is a strong interest in performing convolution on graphs using an extension of the GNN architecture, generally referred to as Graph Convolutional Neural Networks (ConvGNN). Convolution on graphs has been achieved mainly in two forms: spectral and spatial convolutions. Due to the higher flexibility in exploring and exploiting the graph structure of data, there is recently an increasing interest in investigating the possibilities that the spatial approach can offer. The idea of finding a way to adapt the network behaviour to the inputs they process to maximize the total performances has aroused much interest in the neural networks literature over the years. This paper presents a novel method to adapt the behaviour of a ConvGNN to the input performing spatial convolution on graphs using input-specific filters, which are dynamically generated from nodes feature vectors. The experimental assessment confirms the capabilities of the proposed approach, achieving satisfying results using a low number of filters.}
}
@article{LING2023109891,
title = {Motional foreground attention-based video crowd counting},
journal = {Pattern Recognition},
volume = {144},
pages = {109891},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109891},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005897},
author = {Miaogen Ling and Tianhang Pan and Yi Ren and Ke Wang and Xin Geng},
keywords = {Video crowd counting, Frame difference, Attention mechanism},
abstract = {In this paper, we tackle the problem of video crowd counting. Compared with single image crowd counting, video provides gradual spatial and temporal variation information that would help to strengthen the robustness of crowd counting. Therefore, it is critical to make full use of neighboring frames both in feature extraction and final prediction for current frame’s estimation. Based on the above observations, we propose a motional foreground attention-based video crowd counting method. Specifically, we first leverage an foreground estimation module based on ConvNeXt to extract the motional features from bidirectional frame differences and output a foreground estimation map. Then the motional features combined with the static features of current frame are sent into feature fusion network, where foreground estimation map is transformed as attention weights for crowd number estimation. Three new indoor video datasets are manually annotated. The proposed method achieves state-of-the-art performance on all indoor and outdoor video datasets.}
}
@article{SUN2023109870,
title = {Heterogeneous network representation learning based on role feature extraction},
journal = {Pattern Recognition},
volume = {144},
pages = {109870},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109870},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300568X},
author = {Yueheng Sun and Mengyu Jia and Chang Liu and Minglai Shao},
keywords = {Representation learning, Role discovery, Heterogeneous network, Matrix factorization},
abstract = {Since most of the real-world networks are heterogeneous, existing methods cannot characterize the roles of nodes in heterogeneous networks. The neighborhood structure of nodes in heterogeneous networks largely determines the node roles, and the basic statistical features of nodes describe the topology of nodes to some extent, so extracting structural features from the adjacency matrix of networks is crucial for role-oriented network representation learning(structural equivalence). Therefore, in this paper, we propose a heterogeneous network representation learning model based on role feature extraction, called HRFE(Heterogeneous Network Representation Learning for Role Feature Extraction). Firstly, we perform feature extraction for each node in the heterogeneous network to obtain a high-dimensional feature matrix, then perform role discovery using non-negative matrix decomposition techniques to obtain a role-based node representation, and finally verify the effectiveness of the model HRFE through experiments on a large number of real datasets.}
}
@article{MEHRJARDI2023109778,
title = {A survey on deep learning-based image forgery detection},
journal = {Pattern Recognition},
volume = {144},
pages = {109778},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109778},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004764},
author = {Fatemeh Zare Mehrjardi and Ali Mohammad Latif and Mohsen Sardari Zarchi and Razieh Sheikhpour},
keywords = {Forgery detection, Deep learning, Inpainting, Copy move, Splicing, Tampered image, CNN, RNN, R-CNN, Auto-Encoder},
abstract = {Image is known as one of the communication tools between humans. With the development and availability of digital devices such as cameras and cell phones, taking images has become easy anywhere. Images are used in many medical, forensic medicine, and judiciary applications. Sometimes images are used as evidence, so the authenticity and reliability of digital images are increasingly important. Some people manipulate images by adding or deleting parts of an image, which makes the image invalid. Therefore, image forgery detection and localization are important. The development of image editing tools has made this issue an important problem in the field of computer vision. In recent years, many different algorithms have been proposed to detect forgery in the image and pixel levels. All these algorithms are categorized into two main methods: traditional and deep-learning methods. The deep learning method is one of the important branches of artificial intelligence science. This method has become one of the most popular methods in most computer vision problems due to the automatic identification and prediction process and robustness against geometric transformations and post-processing operations. In this study, a comprehensive review of image forgery types, benchmark datasets, evaluation metrics in forgery detection, traditional forgery detection methods, discovering the weaknesses and limitations of traditional methods, forgery detection with deep learning methods, and the performance of this method is presented. According to the expansion of deep-learning methods and their successful performance in most computer vision problems, our main focus in this study is forgery detection based on deep-learning methods. This survey can be helpful for a researcher to obtain a deep background in the forgery detection field.}
}
@article{LI2023109875,
title = {Memory efficient data-free distillation for continual learning},
journal = {Pattern Recognition},
volume = {144},
pages = {109875},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109875},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005733},
author = {Xiaorong Li and Shipeng Wang and Jian Sun and Zongben Xu},
keywords = {Continual learning, Catastrophic forgetting, Knowledge distillation},
abstract = {Deep neural networks suffer from the catastrophic forgetting phenomenon when trained on sequential tasks in continual learning, especially when data from previous tasks are unavailable. To mitigate catastrophic forgetting, various methods either store data from previous tasks, which may raise privacy concerns, or require large memory storage. Particularly, the distillation-based methods mitigate catastrophic forgetting by using proxy datasets. However, proxy datasets may not match the distributions of the original datasets of previous tasks. To address these problems in a setting where the full training data of previous tasks are unavailable and memory resources are limited, we propose a novel data-free distillation method. Our method encodes knowledge of previous tasks into network parameter gradients by Taylor expansion, deducing a regularizer relying on gradients in network training loss. To improve memory efficiency, we design an approach to compressing the gradients in the regularizer. Moreover, we theoretically analyze the approximation error of our method. Experimental results on multiple datasets demonstrate that our proposed method outperforms the existing approaches in continual learning.}
}
@article{JIA2023109823,
title = {A reflectance re-weighted Retinex model for non-uniform and low-light image enhancement},
journal = {Pattern Recognition},
volume = {144},
pages = {109823},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109823},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005216},
author = {Fan Jia and Hok Shing Wong and Tiange Wang and Tieyong Zeng},
keywords = {Image enhancement, Variational method, Retinex model, Non-uniform enhancement},
abstract = {Image enhancement is a fundamental low-level task of significant importance that can directly affect high-level image processing tasks. Although various methods have been proposed to enhance images, the effectiveness of current methods deteriorates significantly under non-uniform lighting. Since the brightness may vary dramatically in different regions of real-world photos, current methods hardly achieve a good balance between enhancing low-light regions and retaining normal-light regions in the same image. Consequently, either the low-light regions are under-enhanced or the normal-light regions are over-enhanced, while at the same time, color distortion and artifacts are frequently found. To overcome this shortcoming, we propose a robust Retinex-based model with reflectance map re-weighting that can improve the brightness level of the low-light image and re-balance the brightness concurrently. We introduce an alternating scheme to solve our proposed model, in which the illumination map, reflectance map, and weighting map are updated iteratively. By utilizing the regularization terms, the noise is well-suppressed during the process. An initialization scheme for the weighting map is also proposed to make our model adaptable to a wide range of light conditions. To the best of our knowledge, we are the first to propose a variational model with an explicitly constructed re-weighting prior and the associated weighing map concept for the reflectance map. It can estimate the reflectance map, suppress noise, and re-balance the brightness simultaneously. A series of experimental results on a variety of popular datasets demonstrate the efficacy of our method and its superiority in enhancing real low-light images when compared to other state-of-the-art methods.}
}
@article{YANG2023109847,
title = {AdvMask: A sparse adversarial attack-based data augmentation method for image classification},
journal = {Pattern Recognition},
volume = {144},
pages = {109847},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109847},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005459},
author = {Suorong Yang and Jinqiao Li and Tianyue Zhang and Jian Zhao and Furao Shen},
keywords = {Data augmentation, Image classification, Sparse adversarial attack, Generalization},
abstract = {Data augmentation has been an essential technique for improving the generalization ability of deep neural networks in image classification tasks. However, intensive changes in appearance and different degrees of occlusion in images are the key factors that severely affect the generalization ability of image classification models. Therefore, in order to enhance the generalization performance and robustness of deep models, data augmentation approaches by providing models with more diverse training data in various scenarios are widely applied. Although many existing data augmentation methods simulate occlusion in the augmented images to enhance the generalization of models, these methods randomly delete some areas in images without considering the semantic information of images. In this work, we propose a novel data augmentation method named AdvMask for image classification based on sparse adversarial attack techniques. AdvMask first identifies the key points that have the greatest influence on the classification results via a proposed end-to-end sparse adversarial attack module. During the data augmentation process, AdvMask efficiently generates diverse augmented data with structured occlusions based on the key points. By doing so, AdvMask can force deep models to seek other relevant content while the most discriminative content is hidden. Extensive experimental results on various benchmark datasets and deep models demonstrate that our proposed method can effectively improve the generalization performance of deep models and significantly outperforms previous data augmentation methods. Code for reproducing our results is available at https://github.com/Jackbrocp/AdvMask.}
}
@article{KANG2023109840,
title = {Structure-preserving image translation for multi-source medical image domain adaptation},
journal = {Pattern Recognition},
volume = {144},
pages = {109840},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109840},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005381},
author = {Myeongkyun Kang and Philip Chikontwe and Dongkyu Won and Miguel Luna and Sang Hyun Park},
keywords = {Domain adaptation, Data augmentation, Mutual information, Segmentation, Unpaired image translation},
abstract = {Domain adaptation is an important task for medical image analysis to improve generalization on datasets collected from diverse institutes using different scanners and protocols. For images with visible domain shift, using image translation models is an intuitive and effective way to perform domain adaptation, but the structure of the generated image may often be distorted when large content discrepancies between domains exist; resulting in poor downstream task performance. To address this, we propose a novel image translation model that disentangles structure and texture to only transfer the latter by using mutual information and texture co-occurrence losses. We translate source domain images to the target domain and employ the generated results as augmented samples for domain adaptation segmentation training. We evaluate our method on three public segmentation datasets: MMWHS, Fundus, and Prostate datasets acquired from diverse institutes. Experimental results show that a segmentation model trained using the augmented images from our approach outperforms state-of-the-art domain adaptation, image translation, and domain generalization methods.}
}
@article{MOHAMUD2023109848,
title = {Encoder–decoder cycle for visual question answering based on perception-action cycle},
journal = {Pattern Recognition},
volume = {144},
pages = {109848},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109848},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005460},
author = {Safaa Abdullahi Moallim Mohamud and Amin Jalali and Minho Lee},
keywords = {Visual question answering, Vision language tasks, Multi-modality fusion, Attention, Bilinear fusion, Brain-inspired frameworks},
abstract = {In this study, we propose a novel encoder–decoder cycle (EDC) framework inspired by the human learning process called the perception-action cycle to tackle challenging problems such as visual question answering (VQA) and visual relationship detection (VRD). EDC considers the understanding of the visual features of an image as perception and the act of answering the question regarding that image as an action. In the perception-action cycle, information is primarily collected from the environment and then passed to sensory structures in the brain to form an understanding of the environment. Acquired knowledge is then passed to motor structures to perform an action on the environment. Next, sensory structures perceive the altered environment and improve their understanding of the surrounding world. This process of understanding the environment, performing an action correspondingly, and then re-evaluating the initial understanding occurs cyclically in human life. EDC initially mimics this mechanism of introspection by comprehending and refining visual features to acquire the proper knowledge for answering the question. Subsequently, it decodes visual and language features into answer features, feeding them back cyclically to the encoder. In the VRD task, EDC decodes visual features to generate predicate features. We evaluate the proposed framework on the TDIUC, VQA 2.0, and VRD datasets, which outperforms the state-of-the-art models on the TDIUC and VRD datasets.}
}
@article{PAN2023109832,
title = {Hyperspectral image destriping and denoising from a task decomposition view},
journal = {Pattern Recognition},
volume = {144},
pages = {109832},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109832},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005307},
author = {Erting Pan and Yong Ma and Xiaoguang Mei and Jun Huang and Qihai Chen and Jiayi Ma},
keywords = {Image restoration, Hyperspectral images, Denoising, Destriping, Multi-task learning},
abstract = {The generalized mathematical model for HSI denoising or destriping lacks stability and uniqueness properties, failing to accurately portray the distribution and effects of stripes. Solutions following such a model would inevitably result in excessive destriping of strip-free areas, leading to the loss of texture detail. To remedy the above deficiencies, we reformulate the destriping task and introduce a novel solution from the task decomposition view. It is broken down into auxiliary sub-tasks involving stripe mask detection, stripe intensity estimation, and HSI restoration, which greatly reduces the difficulty of solving such an ill-posed problem. Based on this, we adopt a sequential multi-task learning framework and propose a stripes location-dependent restoration network, termed SLDR, which integrates the distribution and intensity features of stripes to achieve accurate destriping and high-fidelity restoration. Furthermore, we design a stripe attribute-aware estimator and a weighted total variation loss function to capture the unique properties of stripes and adaptively adjust the restoration weights of striped and non-striped regions. Extensive evaluation and comprehensive ablation studies on synthetic and practical scenes show the effectiveness and superiority of our model and architecture.}
}
@article{IGARCIA2023109887,
title = {A Gaussian kernel for Kendall’s space of m-D shapes},
journal = {Pattern Recognition},
volume = {144},
pages = {109887},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109887},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300585X},
author = {Vicent Gimeno {i Garcia} and Ximo Gual-Arnau and M. Victoria Ibáñez and Amelia Simó},
keywords = {Riemannian manifold, Kendall shape space, Embedding, Reproducible Kernel Hilbert Space},
abstract = {In this paper, we develop an approach to exploit kernel methods with data lying on the m-D Kendall shape space. When data arise in a finite-dimensional curved Riemannian manifold, as in this case, the usual Euclidean computer vision and machine learning algorithms must be treated carefully. A good approach is to use positive definite kernels on manifolds to embed the manifold with its corresponding metric in a high-dimensional reproducing kernel Hilbert space, where it is possible to utilize algorithms developed for linear spaces. Different Gaussian kernels can be found in the literature on the 2-D Kendall shape space to perform this embedding. The main novelty of this work is to provide a Gaussian kernel for the m-D Kendall shape space. This new Kernel coincides in the case m=2 with the Gaussian kernels most widely used in the Kendall planar shape space and allows to define an embedding of the m-D Kendall shape space into a reproducible kernel Hilbert space. As far as we know, the complexity of the m-D Kendall shape space has meant that this embedding has not been addressed in the literature until now. This methodology will be tested on a machine learning problem with a simulated and a real data set.}
}
@article{ZHANG2023109892,
title = {Expansion window local alignment weighted network for fine-grained sketch-based image retrieval},
journal = {Pattern Recognition},
volume = {144},
pages = {109892},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109892},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005903},
author = {Zi-Chao Zhang and Zhen-Yu Xie and Zhen-Duo Chen and Yu-Wei Zhan and Xin Luo and Xin-Shun Xu},
keywords = {Fine-grained sketch-based image retrieval, Local feature, Expansion window, Attention mechanism},
abstract = {Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) is a worthwhile task, which can be useful in many scenarios like recommendation systems, receiving a great deal of attention. In this study, we analyze challenges faced in FG-SBIR and propose a novel Expansion Window Local Alignment Weighted Network (EWLAW-Net). Specifically, it contains two main components: the Expansion Window Local Alignment module (EWLA) and the Local Weighted Fusion module (LWF). The EWLA module adopts an expansion window mechanism to align local features extracted from the backbone with the same semantic meaning between photos and sketches. The LWF module assigns weights to each local feature of the sketch after evaluating their importance and fuses them to calculate the similarity between the sketch and photos for retrieval. Experiments are conducted on five datasets and the results demonstrate the effectiveness of the proposed method.}
}
@article{MAO2023109864,
title = {A novel method of human identification based on dental impression image},
journal = {Pattern Recognition},
volume = {144},
pages = {109864},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109864},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005629},
author = {Jiafa Mao and Lixin Wang and Ning Wang and Yahong Hu and Weigou Sheng},
keywords = {Dental impression image, Tooth print detection, Multi-scale feature extraction, Feature aggregation, Human identification},
abstract = {In large-scale natural disasters and special criminal cases, surface features of bodies, such as faces and fingerprints, are easily destroyed. Teeth possess strong high-temperature resistance, corrosion resistance, and high hardness, which can compensate for the shortcomings of the aforementioned situations. This paper proposes an identification method based on the aggregated features of multi-scale dental impression images. Firstly, a method exploiting the adaptive object detection method based on YOLOv8 is proposed to segment toothprints. Next, a novel geometric feature named calibrated offset distance is extracted, combined with the SIFT feature method, to extract multi-scale and multi-dimensional features from the global toothprint, local toothprints, and single-tooth prints. Finally, all features are aggregated to enhance the descriptive ability and robustness. Experimental results indicate that the method proposed in this paper demonstrates good identification performance.}
}
@article{XI2023109872,
title = {Online portfolio selection with predictive instantaneous risk assessment},
journal = {Pattern Recognition},
volume = {144},
pages = {109872},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109872},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005708},
author = {Wenzhi Xi and Zhanfeng Li and Xinyuan Song and Hanwen Ning},
keywords = {Portfolio optimization, Online learning, High-dimensional covariance matrix, Ensemble learning, High-dimensional short-term data},
abstract = {Online portfolio selection (OPS) has received increasing attention from machine learning and quantitative finance communities. Despite their effectiveness, the pioneering OPS methods have several key limitations. First, price predictions are usually based on predetermined trends, which is inadequate for a fast-changing market patterns. Second, each asset is treated individually, ignoring the pervading relevance among the assets. Third, the risk terms are usually missing or inappropriate in optimizations. This paper proposes a novel OPS method, namely, the online low-dimension ensemble method, to overcome the limitations. Motivated by the stylized facts for the co-movements of assets, the financial market is regarded as a high-dimensional dynamical system (HDS), and a large number of low-dimensional subsystems (LDSs) are randomly generated from the HDS to extract the correlation information among the assets. The assets’ price predictions are first made using these LDSs and then aggregated to formulate the final prediction using ensemble learning techniques. Thanks to the particular merits brought by our predicting scheme, we also develop a novel high-dimensional covariance matrix estimation/prediction method for short-term data, efficiently assessing the instantaneous risk of the projected portfolios. Compared with state-of-the-art methods, our approach obtains more accurate predictions as the correlation information is fully exploited. With the predictive instantaneous risk assessment, a more appropriate optimization problem is proposed, substantially improving the OPS setting and leading to significantly better investment performance. Therefore, this study develops a flexible and promising approach to learning fast-changing market patterns and demonstrates that the high-dimensional feature of the market is a crucial information source for financial modeling with short-term data rather than a barrier in the conventional sense. Extensive experiments on real-world datasets are conducted to illustrate our method further.}
}
@article{MA2023109880,
title = {Federated adaptive reweighting for medical image classification},
journal = {Pattern Recognition},
volume = {144},
pages = {109880},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109880},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005782},
author = {Benteng Ma and Yu Feng and Geng Chen and Changyang Li and Yong Xia},
keywords = {Medical image classification, Federated learning, Deep learning},
abstract = {Medical data sharing across institutes is crucial to large-scale multi-center studies and the development of real-world AI applications but suffers from serious privacy issues. A promising solution to address this challenge is federated learning, which typically aggregates a global model from heterogeneous data spread across numerous clients without exchanging data. However, the traditional federated learning algorithm (i.e., FedAvg) merely aggregates the locally distributed models according to the amount of data on each client and lacks the consideration of data heterogeneity. In this paper, we propose a novel Federated Adaptive Reweighting (FedAR) algorithm for medical image classification. FedAR employs a flexible re-weighting scheme that can balance adaptively the contributions of the amount of data and the performance of the local model on each client to the weight of that client. Specifically, we allow the amount of local data to contribute more to the weight of each client in the early training stage and let the performance of the local model play a more important role in the late stage. We have evaluated the proposed FedAR algorithm against the locally trained model, globally trained baseline, and two existing federated learning algorithms on the ISIC2018 dataset and Chest X-ray14 dataset under the settings with a variable number of clients. Our results suggest that FedAR is an effective federated learning algorithm that substantially outperforms existing federated learning approaches.}
}
@article{XIAO2023109828,
title = {Revisiting the transferability of adversarial examples via source-agnostic adversarial feature inducing method},
journal = {Pattern Recognition},
volume = {144},
pages = {109828},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109828},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005265},
author = {Yatie Xiao and Jizhe Zhou and Kongyang Chen and Zhenbang Liu},
keywords = {Adversarial attack, Transferability, Feature inducing, Diversity},
abstract = {Though deep neural networks (DNNs) have revealed their extraordinary performance in the fields of computer vision, it is evident that the vulnerability of DNNs to adversarial attacks with crafted human-imperceptible perturbations. Most existing adversarial attacks draw their attention to invading target deep task models by enhancing input-diagnostic features via image rotation, warp, or transformation to improve adversarial transferability. Such manners pay close concentration to operation on original inputs regardless of the properties from different source information. Research has inspired us to consider utilizing source-agnostic information and integrating generated features with raw inputs to enrich adversarial properties. For such needs, we propose a simple and flexible adversarial attack method with source-agnostic Feature Inducing Method (FIM) for improving the transferability of adversarial examples (AEs). FIM first focuses on generating perturbed features by imitating diverse patterns from multi-domain sources. Instead of exploiting the original inputs’ diversity, such proposed work gains the various properties by random feature imitation referring to different source distributions. By optimizing the generated features with norm bounds, FIM then integrates original inputs with imitative features. Such manner can diverse row positive class-general features, which reduce the capability of class-specific patterns on cross-model transferability. Based on the crafted property, FIM employs the adaptive gradient-based strategy on such information to generate perturbations, which helps to decrease probability dropping into local optimal when searching for the decision boundary of source and target models. We conduct detailed experiments to evaluate the performance of our proposed approach with existing baselines on three public datasets. The experimental results reveal the better performance of the proposed works on fooling source and target task models leading to a considerable margin in most adversarial scenarios. We further investigate adversarial attacks on adversarial defense models (with adversarial training and trades). Such a proposed attack strategy achieves better attack quality by a margin over 3.00% on CIFAR10 and reduces the robust accuracy of adversarially trained models by a large margin near 9.00% on MNIST. Furthermore, we exploit the performance of the proposed attack strategy applied to feature-level adversarial domains and conduct evaluations to demonstrate its adversarial feasibility in integrating with various attack mechanisms, which gains better adversarial effectiveness over 20.00% than the base attacks on studied deep task models.}
}
@article{ZHAO2023109836,
title = {Deep multi-view spectral clustering via ensemble},
journal = {Pattern Recognition},
volume = {144},
pages = {109836},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109836},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005344},
author = {Mingyu Zhao and Weidong Yang and Feiping Nie},
keywords = {Spectral embedding, Multi-view clustering, Ensemble clustering, Graph reconstruction},
abstract = {Graph-based methods have achieved great success in multi-view clustering. However, existing graph-based models generally utilize shallow and linear embedding functions to obtain the common spectral embedding for clustering assignments. In addition, the fusion similarity graphs from multiple views are generally obtained by a simple weighted-sum rule. To this end, we propose a novel deep multi-view spectral clustering via ensemble model (DMCE), which applies ensemble clustering to fuse the similarity graphs from different views. On this basis, we employ the graph auto-encoder to learn the common spectral embedding, which can be regarded as the indicator matrix directly. Moreover, a unified optimization framework is designed to update the variables in the proposed DMCE, which consists of graph reconstruction loss, orthogonal loss, and graph contrastive learning loss. Extensive experiments on six real-world benchmark datasets have demonstrated the effectiveness of our model compared with the state-of-the-art multi-view clustering methods.}
}
@article{YU2023109860,
title = {Multi-view clustering via efficient representation learning with anchors},
journal = {Pattern Recognition},
volume = {144},
pages = {109860},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109860},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005587},
author = {Xiao Yu and Hui Liu and Yan Zhang and Shanbao Sun and Caiming Zhang},
keywords = {Multi-view clustering, Large-scale, Anchor, Representation learning},
abstract = {Multi-view spectral clustering has gained considerable attention due to its potential to enhance clustering performance. Although many methods have shown promising results, they often suffer from high time complexity and are not suitable for large-scale datasets. On the other hand, anchor-based methods are well-known for their efficiency. These methods typically learn the similarity relationship between instances and anchors and then convert it into the similarity relationship between instances, involving a considerable number of calculations. To address this issue, we propose a novel method called Multi-view clustering via Efficient Representation LearnIng with aNchors (MERLIN) in this paper. Instead of learning the instance–instance relationship, MERLIN approaches the clustering problem from the perspective of representation learning. Specifically, MERLIN selects the same anchors for different views and utilizes these anchors to learn a consensus representation that integrates information from all views. Additionally, MERLIN adaptively learns weights for different views to fully exploit the complementary information among multiple views. In comparison with seven state-of-the-art baseline methods across five datasets, MERLIN demonstrates both efficiency and effectiveness in handling multi-view datasets and is suitable for handling large-scale datasets.}
}
@article{CARICHON2023109839,
title = {Unsupervised update summarization of news events},
journal = {Pattern Recognition},
volume = {144},
pages = {109839},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109839},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300537X},
author = {Florian Carichon and Florent Fettu and Gilles Caporossi},
keywords = {Natural language processing, Neural network, Automatic document summarization, Unsupervised approach, Update sentence compression, Information novelty},
abstract = {A long-running event represents a continuous stream of information on a given topic, such as natural disasters, stock market updates, or even ongoing customer relationship. These news stories include hundreds of individual, time-dependent texts. Simultaneously, new technologies have profoundly transformed the way we consume information. The need to obtain quick, relevant, and digest updates continuously has become a crucial issue and creates new challenges for the task of automatic document summarization. To that end, we introduce an innovative unsupervised method based on two competing sequence-to-sequence models to produce short updated summaries. The proposed architecture relies on several parameters to balance the outputs from the two autoencoders. This relation enables the overall model to correlate generated summaries with relevant information coming from both current and previous news iterations. Depending on the model configuration, we are then able to control the novelty or the consistency of terms included in generated summaries. We evaluate our method on a modified version of the TREC 2013, 2014, and 2015 datasets to track continuous events from a single source. We not only achieve state-of-the-art performance similar to other more complex unsupervised sentence compression approaches, but also influence the information included in the model in the summaries.}
}
@article{HUA2023109844,
title = {Deep fidelity in DNN watermarking: A study of backdoor watermarking for classification models},
journal = {Pattern Recognition},
volume = {144},
pages = {109844},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109844},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005423},
author = {Guang Hua and Andrew Beng Jin Teoh},
keywords = {Deep fidelity, Backdoor watermarking, Backdoor fidelity, Deep learning security, Neural network watermarking, Intellectual property protection, Ownership verification},
abstract = {Backdoor watermarking is a promising paradigm to protect the copyright of deep neural network (DNN) models. In the existing works on this subject, researchers have intensively focused on watermarking robustness, while the concept of fidelity, which is concerned with the preservation of the model’s original functionality, has received less attention. In this paper, focusing on deep image classification models, we show that the existing shared notion of the sole measurement of learning accuracy is inadequate to characterize backdoor fidelity. Meanwhile, we show that the analogous concept of embedding distortion in multimedia watermarking, interpreted as the total weight loss (TWL) in DNN backdoor watermarking, is also problematic for fidelity measurement. To address this challenge, we propose the concept of deep fidelity, which states that the backdoor watermarked DNN model should preserve both the feature representation and decision boundary of the unwatermarked host model. To achieve deep fidelity, we propose two loss functions termed penultimate feature loss (PFL) and softmax probability-distribution loss (SPL) to preserve feature representation, while the decision boundary is preserved by the proposed fix last layer (FixLL) treatment, inspired by the recent discovery that deep learning with a fixed classifier causes no loss of learning accuracy. With the above designs, both embedding from scratch and fine-tuning strategies are implemented to evaluate the deep fidelity of backdoor embedding, whose advantages over the existing methods are verified via experiments using ResNet18 for MNIST and CIFAR-10 classifications, and wide residual network (i.e., WRN28_10) for CIFAR-100 task. PyTorch codes are available at https://github.com/ghua-ac/dnn_watermark.}
}
@article{ABU2023109868,
title = {Underwater object classification combining SAS and transferred optical-to-SAS Imagery},
journal = {Pattern Recognition},
volume = {144},
pages = {109868},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109868},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005666},
author = {Avi Abu and Roee Diamant},
keywords = {Feature extraction, Shape descriptors, Self-similarity, Fourier descriptor, Region-based feature, Contour-based features},
abstract = {Combining synthetic aperture sonar (SAS) imagery with optical images for underwater object classification has the potential to overcome challenges such as water clarity, the stability of the optical image analysis platform, and strong reflections from the seabed for sonar-based classification. In this work, we propose this type of multi-modal combination to discriminate between man-made targets and objects such as rocks or litter. We offer a novel classification algorithm that overcomes the problem of intensity and object formation differences between the two modalities. To this end, we develop a novel set of geometrical shape descriptors that takes into account the geometrical relation between the object’s shadow and highlight. Results from 7,052 pairs of SAS and optical images collected during several sea experiments show improved classification performance compared to the state-of-the-art for better discrimination between different types of underwater objects. For reproducability, we share our database.}
}
@article{JI2023109816,
title = {Paired contrastive feature for highly reliable offline signature verification},
journal = {Pattern Recognition},
volume = {144},
pages = {109816},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109816},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005149},
author = {Xiaotong ji and Daiki Suehiro and Seiichi Uchida},
keywords = {Writer-independent signature verification, Skilled forgery, Offline signature verification, Paired contrastive feature, Learning with rejection, Top-rank learning},
abstract = {Signature verification requires high reliability. Especially in the writer-independent scenario with the skilled-forgery-only condition, achieving high reliability is challenging but very important. In this paper, we propose to apply two machine learning frameworks, learning with rejection and top-rank learning, to this task because they can suppress ambiguous results and thus give only reliable verification results. Since those frameworks accept a single input, we transform a pair of genuine and query signatures into a single feature vector, called Paired Contrastive Feature (PCF). PCF internally represents similarity (or discrepancy) between the two paired signatures; thus, reliable machine learning frameworks can make reliable decisions using PCF. Through experiments on three public signature datasets in the offline skilled-forgery-only writer-independent scenario, we evaluate and validate the effectiveness and reliability of the proposed models by comparing their performance with a state-of-the-art model.}
}
@article{ZHAO2023109876,
title = {Patch-guided point matching for point cloud registration with low overlap},
journal = {Pattern Recognition},
volume = {144},
pages = {109876},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109876},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005745},
author = {Tianming Zhao and Linfeng Li and Tian Tian and Jiayi Ma and Jinwen Tian},
keywords = {Point cloud registration, Low overlap, Matching pyramid, Cross-level fusion},
abstract = {Point cloud registration is a classic and fundamental problem. Existing point cloud registration methods obtain correspondence point pairs by calculating the correlation between point features. However, the instability of point features makes the outlier rate of corresponding point pairs high, resulting in poor matching results, especially when facing low overlap point clouds. An obvious fact is that patch matching has higher reliability than point matching. To this end, we propose a patch-guided point cloud registration network. Specifically, we perform fusion on patches and points at both the feature and result levels to achieve the guidance of patch to point matching and improve the accuracy of predicted point pairs. At the feature level, we propose a Matching Pyramid Network (MPN) for multi-level patch/point matching. The core of the MPN is an attention-based cross-layer context aggregation (CCA) module, which is used for the fusion of matching features between upper and lower layers. At the result level, we design a matching consistency judgment module to ensure that the point pairs are consistent in the matching of each layer, which greatly reduces the outlier ratio. Based on the above design, the corresponding point pairs predicted by our network have a high inlier ratio, which makes our method perform well in the face of low overlapping point clouds. Extensive experimental results show that our method outperforms other existing methods for indoor and outdoor datasets.}
}
@article{ZHAO2023109824,
title = {A balanced random learning strategy for CNN based Landsat image segmentation under imbalanced and noisy labels},
journal = {Pattern Recognition},
volume = {144},
pages = {109824},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109824},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005228},
author = {Xuemei Zhao and Yong Cheng and Luo Liang and Haijian Wang and Xingyu Gao and Jun Wu},
keywords = {Landsat image segmentation, Noisy labels, Confidence interval, Random learning, Multi-layer features},
abstract = {Landsat image segmentation is important for obtaining large-scale land cover maps. The accuracy of CNN-based Landsat image segmentation highly depends on the quantity and quality of the training samples. However, enough accurate labels for Landsat images are difficult to access. Fortunately, traditional classifier induced segmentation results can be considered as an alternative, although they are noisy and unbalanced to a certain extent. To resist noisy labels and alleviate the impact of imbalanced samples, this paper proposes a confidence interval based balanced random learning strategy. Firstly, a confidence interval-based mask is employed to control the random learning rate of the network from the entire noisy training set. Then, the multi-layer feature maps of CNN are fully utilized to compensate for the information loss in random learning, in which down-sampled labels are used to decrease the uncertainty brought by up-sampling CNN feature maps. In addition, considering the corruption of noisy labels on different classes, a balanced random learning with different confidence levels is performed on each class to further improve the learning ability of CNN. Experimental results on two widely used backbones, namely VGGNet and ResNet, demonstrate that the proposed balanced random learning strategy can effectively improve the performance of CNN under imbalanced and noisy labels, which can be improved by 3.41%.}
}
@article{WANG2023109817,
title = {Robust table structure recognition with dynamic queries enhanced detection transformer},
journal = {Pattern Recognition},
volume = {144},
pages = {109817},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109817},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005150},
author = {Jiawei Wang and Weihong Lin and Chixiang Ma and Mingze Li and Zheng Sun and Lei Sun and Qiang Huo},
keywords = {Table structure recognition, Separation line regression, Two-stage DETR, Dynamic query},
abstract = {We present a new table structure recognition (TSR) approach, called TSRFormer, to robustly recognize the structures of complex tables with geometrical distortions from various table images. Unlike previous methods, we formulate table separation line prediction as a line regression problem instead of an image segmentation problem and propose a new two-stage dynamic queries enhanced DETR based separation line regression approach, named DQ-DETR, to predict separation lines from table images directly. Compared to Vallina DETR, we propose three improvements in DQ-DETR to make the two-stage DETR framework work efficiently and effectively for the separation line prediction task: 1) A new query design, named Dynamic Query, to decouple single line query into separable point queries which could intuitively improve the localization accuracy for regression tasks; 2) A dynamic queries based progressive line regression approach to progressively regressing points on the line which further enhances localization accuracy for distorted tables; 3) A prior-enhanced matching strategy to solve the slow convergence issue of DETR. After separation line prediction, a simple relation network based cell merging module is used to recover spanning cells. With these new techniques, our TSRFormer achieves state-of-the-art performance on several benchmark datasets, including SciTSR, PubTabNet, WTW, FinTabNet, and cTDaR TrackB2-Modern. Furthermore, we have validated the robustness and high localization accuracy of our approach to tables with complex structures, borderless cells, large blank spaces, empty or spanning cells as well as distorted or even curved shapes on a more challenging real-world in-house dataset.}
}
@article{2023109930,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {144},
pages = {109930},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(23)00628-3},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006283}
}
@article{LI2023109841,
title = {A multi-grained unsupervised domain adaptation approach for semantic segmentation},
journal = {Pattern Recognition},
volume = {144},
pages = {109841},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109841},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005393},
author = {Luyang Li and Tai Ma and Yue Lu and Qingli Li and Lianghua He and Ying Wen},
keywords = {Domain adaptation, Unsupervised semantic segmentation, Neural network},
abstract = {When transferring knowledge between different datasets, domain mismatch greatly hinders model’s performance. So domain adaption has been brought up to tackle the problem. Traditional methods focusing either on global or local alignment play a limited role in improving model’s performance. In this paper, we propose a multi-grained unsupervised domain adaptation approach (Muda) for semantic segmentation. Muda aims to enforce multi-grained semantic consistency between domains by aligning domains at both global and category level. Specifically, coarse-grained adaptation uses global adversarial learning on an image translation model and a main segmentation model, which respectively attempts to eliminate appearance differences and to get similar segmentation maps from two domains. While fine-grained adaptation employs an auxiliary model to adapt category information to refine pseudo labels of target data. Experiments and ablation studies are conducted on two synthetic-to-real benchmarks: GTA5 → Cityscapes and SYNTHIA → Cityscapes, which show that our model outperforms the state-of-the-art methods.}
}
@article{LI2023109849,
title = {Few pixels attacks with generative model},
journal = {Pattern Recognition},
volume = {144},
pages = {109849},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109849},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005472},
author = {Yang Li and Quan Pan and Zhaowen Feng and Erik Cambria},
keywords = {Neural network vulnerability, Adversarial attack, Few pixels attacks, Generative attack},
abstract = {Adversarial attacks have attracted much attention in recent years, and a number of works have demonstrated the effectiveness of attacks on the entire image at perturbation generation. However, in practice, specially designed perturbation of the entire image is impractical. Some work has crafted adversarial samples with a few scrambled pixels by advanced search, e.g., SparseFool, OnePixel, etc., but they take more time to find such pixels that can be perturbed. Therefore, to construct the adversarial samples with few pixels perturbed in an end-to-end way, we propose a new framework, in which a dual-decoder VAE for perturbations finding is designed. To make adversarial learning more effective, we proposed a new version of the adversarial loss by considering the generalization. To evaluate the sophistication of the proposed framework, we compared it with more than a dozen existing related attack methods. The effectiveness and efficiency of the proposed framework are verified from the extensive experimental results. The validity of the model structure is also validated by the ablation study.}
}
@article{XIE2023109821,
title = {ANAS: Asymptotic NAS for large-scale proxyless search and multi-task transfer learning},
journal = {Pattern Recognition},
volume = {144},
pages = {109821},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109821},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005198},
author = {Bangquan Xie and Zongming Yang and Liang Yang and Ruifa Luo and Jun Lu and Ailin Wei and Xiaoxiong Weng and Bing Li},
keywords = {Neural architecture search, Memory consumption, Proxyless search, Multi-task transfer, Classification and segmentation},
abstract = {Neural Architecture Search (NAS) is an emerging solution to design a lightweight network for researchers to obtain a trade-off between accuracy and speed, releasing researchers from tedious repeated trials. However, the main shortcoming of NAS is its high and unstable memory consumption of the search work, especially for large-scale tasks. In this study, the proposed Asymptotic Neural Architecture Search network (ANAS) achieved a proxyless search for large-scale tasks with economic and stable memory consumption. Instead of proxy search like other NAS algorithms, ANAS achieved the large-scale proxyless search that directly learns deep neural network architecture for target task. ANAS reduced the peak value of memory consumption by an asymptotic method, and kept the memory consumption stable by the linkage change of a series of key indexes. A pruning operation and efficient candidate operations decreased the total memory consumption. Finally, ANAS achieved a good trade-off between accuracy and speed for classification tasks on CIFAR-10, CIFAR-100, and ImageNet datasets. Besides, except for the classification task, it achieved excellent multi-task transfer learning ability for implementing the segmentation task on CamVid and Cityscapes. ANAS reached 22.8% test errs with 5 M parameter on ImageNet, and 72.9 mIoU (mean Intersection over Union) with 119.9 FPS (Frames Per Second) on Cityscapes dataset.}
}
@article{ZHANG2023109885,
title = {ARAI-MVSNet: A multi-view stereo depth estimation network with adaptive depth range and depth interval},
journal = {Pattern Recognition},
volume = {144},
pages = {109885},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109885},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005836},
author = {Song Zhang and Wenjia Xu and Zhiwei Wei and Lili Zhang and Yang Wang and Junyi Liu},
keywords = {Multi-view stereo, Depth estimation, Adaptive range, Adaptive interval},
abstract = {Multi-View Stereo (MVS) is a fundamental problem in geometric computer vision which aims to reconstruct a scene using multi-view images with known camera parameters. However, the mainstream approaches represent the scene with a fixed all-pixel depth range and equal depth interval partition, which will result in inadequate utilization of depth planes and imprecise depth estimation. In this paper, we present a novel multi-stage coarse-to-fine framework to achieve adaptive all-pixel depth range and depth interval. We predict a coarse depth map in the first stage, then an Adaptive Depth Range Prediction module is proposed in the second stage to zoom in the scene by leveraging the reference image and the obtained depth map in the first stage and predict a more accurate all-pixel depth range for the following stages. In the third and fourth stages, we propose an Adaptive Depth Interval Adjustment module to achieve adaptive variable interval partition for pixel-wise depth range. The depth interval distribution in this module is normalized by Z-score, which can allocate dense depth hypothesis planes around the potential ground truth depth value and vice versa to achieve more accurate depth estimation. Extensive experiments on four widely used benchmark datasets (DTU, TnT, BlendedMVS, ETH 3D) demonstrate that our model achieves state-of-the-art performance and yields competitive generalization ability. Particularly, our method achieves the highest Acc and Overall on the DTU dataset, while attaining the highest Recall and F1-score on the Tanks and Temples intermediate and advanced dataset. Moreover, our method also achieves the lowest e1 and e3 on the BlendedMVS dataset and the highest Acc and F1-score on the ETH 3D dataset, surpassing all listed methods. Project website: https://github.com/zs670980918/ARAI-MVSNet}
}
@article{DING2023109833,
title = {Graph clustering network with structure embedding enhanced},
journal = {Pattern Recognition},
volume = {144},
pages = {109833},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109833},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005319},
author = {Shifei Ding and Benyu Wu and Xiao Xu and Lili Guo and Ling Ding},
keywords = {Graph machine learning, Graph Neural Network, Deep clustering, Self-supervised learning},
abstract = {Recently, deep clustering utilizing Graph Neural Networks has shown good performance in the graph clustering. However, the structure information of graph was underused in existing deep clustering methods. Particularly, the lack of concern on mining different types structure information simultaneously. To tackle with the problem, this paper proposes a Graph Clustering Network with Structure Embedding Enhanced (GC-SEE) which extracts nodes importance-based and attributes importance-based structure information via a feature attention fusion graph convolution module and a graph attention encoder module respectively. Additionally, it captures different orders-based structure information through multi-scale feature fusion. Finally, a self-supervised learning module has been designed to integrate different types structure information and guide the updates of the GC-SEE. The comprehensive experiments on benchmark datasets commonly used demonstrate the superiority of the GC-SEE. The results showcase the effectiveness of the GC-SEE in exploiting multiple types of structure for deep clustering.}
}
@article{LI2023109829,
title = {Compositional clustering: Applications to multi-label object recognition and speaker identification},
journal = {Pattern Recognition},
volume = {144},
pages = {109829},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109829},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005277},
author = {Zeqian Li and Xinlu He and Jacob Whitehill},
keywords = {Clustering algorithms, Compositional learning, Few-shot learning, Embedding models, Speaker diarization, Affinity propagation},
abstract = {We consider a novel clustering task in which clusters can have compositional relationships, e.g., one cluster contains images of rectangles, one contains images of circles, and a third (compositional) cluster contains images with both objects. In contrast to hierarchical clustering in which a parent cluster represents the intersection of properties of the child clusters, our problem is about finding compositional clusters that represent the union of the properties of the constituent clusters. This task is motivated by recently developed few-shot learning and embedding models (Alfassy et al., 2019; Li et al., 2021) that can distinguish the label sets, not just the individual labels, assigned to the examples. We propose three new algorithms – Compositional Affinity Propagation (CAP), Compositional k-means (CKM), and Greedy Compositional Reassignment (GCR) – that can partition examples into coherent groups and infer the compositional structure among them. We show promising results, compared to popular algorithms such as Gaussian mixtures, Fuzzy c-means, and Agglomerative Clustering, on the OmniGlot and LibriSpeech datasets. Our work has applications to open-world multi-label object recognition and speaker identification & diarization with simultaneous speech from multiple speakers.}
}
@article{ZHAO2023109808,
title = {A fast stereo matching network based on temporal attention and 2D convolution},
journal = {Pattern Recognition},
volume = {144},
pages = {109808},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109808},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300506X},
author = {Youchen Zhao and Hua Zhong and Boyuan Jia and Haixiong Li},
keywords = {Stereo matching, Temporal attention, 2D Convolution, Edge cues, Pyramid cost volume},
abstract = {We propose a fast stereo matching network based on temporal attention and 2D convolution (TANet). Due to the high similarity of the disparity between consecutive frames in an image sequence, we propose a temporal attention (TA) module that uses the disparity map of the previous frame to guide the disparity search range in the current frame, thus significantly improving the efficiency of disparity calculation in the cost volume module. Additionally, we propose a hierarchical cost construction and 2D convolution aggregation module that constructs a pyramid cost volume by fusing edge cues to establish detail constraints. This overcomes the problem of difficult convergence caused by information loss when replacing 3D convolution with 2D convolution. Experimental results show that the TA module effectively optimizes the cost volume and, together with 2D convolution, improves the computational speed. Compared with state-of-the-art algorithms, TANet achieves a speedup of nearly 4x, with a running time of 0.061s, and reduces the parameter count by nearly half while decreasing accuracy by 1.1%. Code is available at https://github.com/Y0uchenZ/TANet.}
}
@article{WU2023109865,
title = {Audio-driven talking face generation with diverse yet realistic facial animations},
journal = {Pattern Recognition},
volume = {144},
pages = {109865},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109865},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005630},
author = {Rongliang Wu and Yingchen Yu and Fangneng Zhan and Jiahui Zhang and Xiaoqin Zhang and Shijian Lu},
keywords = {Audio-driven talking face generation, Face, Face animation, Audio-to-visual mapping, Image synthesis},
abstract = {Audio-driven talking face generation, which aims to synthesize talking faces with realistic facial animations (including accurate lip movements, vivid facial expression details and natural head poses) corresponding to the audio, has achieved rapid progress in recent years. However, most existing work focuses on generating lip movements only without handling the closely correlated facial expressions, which degrades the realism of the generated faces greatly. This paper presents DIRFA, a novel method that can generate talking faces with diverse yet realistic facial animations from the same driving audio. To accommodate fair variation of plausible facial animations for the same audio, we design a transformer-based probabilistic mapping network that can model the variational facial animation distribution conditioned upon the input audio and autoregressively convert the audio signals into a facial animation sequence. In addition, we introduce a temporally-biased mask into the mapping network, which allows to model the temporal dependency of facial animations and produce temporally smooth facial animation sequence. With the generated facial animation sequence and a source image, photo-realistic talking faces can be synthesized with a generic generation network. Extensive experiments show that DIRFA can generate talking faces with realistic facial animations effectively.}
}
@article{HUANG2023109837,
title = {Learning consistent region features for lifelong person re-identification},
journal = {Pattern Recognition},
volume = {144},
pages = {109837},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109837},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005356},
author = {Jinze Huang and Xiaohan Yu and Dong An and Yaoguang Wei and Xiao Bai and Jin Zheng and Chen Wang and Jun Zhou},
keywords = {Person re-identification, Lifelong learning, Feature sharing},
abstract = {The lifelong person re-identification (LRe-ID) model retrieves a person across multiple cameras in continuous data streams and learns new coming datasets incrementally. However, there are two well-known challenges: catastrophic forgetting and generalization loss, which arise due to parameter updates and domain shifts. While there has been encouraging progress in balancing these challenges, few existing methods have addressed them from a unified feature perspective. Inspired by the Complementary Learning Systems theory, an effective framework is proposed to share consistent features and extract discriminative features for each sample. Specifically, this framework consists of Property Region Features, Feature Adaption and Feature Perspicacity. Property Region Features are the parametric representation of consistent region features, Feature Adaption and Feature Perspicacity are responsible for diversity features generation and discriminative features extraction, respectively. Moreover, a cascade knowledge distillation structure is introduced to guarantee Property Feature consistency, and correspondingly, a weighted distillation loss function is designed to prevent generalization loss on current domains caused by overlapping historical knowledge. Extensive experiments conducted on twelve Re-ID datasets, including both rehearsal and no-rehearsal settings, clearly validate the superiority of our method over state-of-the-art competitors, with significantly improved performance. The source code will be released on https://github.com/whisperH/ConRFL/.}
}
@article{LU2023109861,
title = {A weakly supervised inpainting-based learning method for lung CT image segmentation},
journal = {Pattern Recognition},
volume = {144},
pages = {109861},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109861},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005599},
author = {Fangfang Lu and Zhihao Zhang and Tianxiang Liu and Chi Tang and Hualin Bai and Guangtao Zhai and Jingjing Chen and Xiaoxin Wu},
keywords = {COVID-19, Weakly supervised, Lesion segmentation, Image inpainting},
abstract = {Recently, various fully supervised learning methods are successfully applied for lung CT image segmentation. However, pixel-wise annotations are extremely expert-demanding and labor-intensive, but the performance of unsupervised learning methods are failed to meet the demands of practical applications. To achieve a reasonable trade-off between the performance and label dependency, a novel weakly supervised inpainting-based learning method is introduced, in which only bounding box labels are required for accurate segmentation. Specifically, lesion regions are first detected by an object detection network, then we crop them out of the input image and recover the missing holes to normal regions using a progressive CT inpainting network (PCIN). Finally, a post-processing method is designed to get the accurate segmentation mask from the difference image of input and recovered images. In addition, real information (i.e., number, location and size) of the bounding boxes of lesions from the dataset guides us to make the training dataset for PCIN. We apply a multi-scale supervised strategy to train PCIN for a progressive and stable inpainting. Moreover, to remove the visual artifacts resulted from the invalid features of missing holes, an initial patch generation network (IPGN) is proposed for holes initialization with generated pseudo healthy image patches. Experiments on the public COVID-19 dataset demonstrate that PCIN is outstanding in lung CT images inpainting, and the performance of our proposed weakly supervised method is comparable to fully supervised methods.}
}
@article{LI2023109845,
title = {Self-imitation guided goal-conditioned reinforcement learning},
journal = {Pattern Recognition},
volume = {144},
pages = {109845},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109845},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005435},
author = {Yao Li and YuHui Wang and XiaoYang Tan},
keywords = {Goal-conditioned reinforcement learning, Self-imitation learning, Deterministic policy gradient, Behavior cloning},
abstract = {Goal-conditioned reinforcement learning (GCRL) aims to control agents to reach desired goals, which poses a significant challenge due to task-specific variations in configurations. However, current GCRL methods suffer from limitations in sample efficiency and the need for substantial training data. While existing self-imitation-based GCRL approaches can improve sample efficiency, their scalability to large-scale tasks is limited. In this paper, we propose integrating self-imitation learning with goal-conditioned RL methods into a compatible and reasonable framework. Specifically, we introduce a novel target action value function to aggregate self-imitation learning and goal-conditioned reinforcement learning. The designed target value effectively combines these two policy training mechanisms to accomplish specific tasks. Moreover, we theoretically demonstrate that our approach can learn a superior policy compared to both self-imitation learning and goal-conditioned reinforcement learning. Additionally, experimental results showcase the stability and effectiveness of our method compared to existing approaches in various challenging robotic control tasks.}
}
@article{ZHOU2023109869,
title = {Attribute subspaces for zero-shot learning},
journal = {Pattern Recognition},
volume = {144},
pages = {109869},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109869},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005678},
author = {Lei Zhou and Yang Liu and Xiao Bai and Na Li and Xiaohan Yu and Jun Zhou and Edwin R. Hancock},
keywords = {Zero-shot learning, Attribute localization, Subspace representation, Attribute subspaces, Self-supervised learning},
abstract = {Zero-shot learning (ZSL) aims to recognize unseen categories without corresponding training samples, which is a practical yet challenging task in computer vision and pattern recognition community. Current state-of-the-art locality-based ZSL methods aim to learn the explicit locality of discriminative attributes, which may suffer from insufficient class-level attribute supervision. In this paper, we introduce an Attribute Subspace learning method for ZSL (AS-ZSL) to learn implicit attribute composition, which is more general than attribute localization with only class-level attribute supervision. AS-ZSL exploits subspace representations that can effectively capture the intrinsic composition of high-dimensional image features and the diversity within attribute appearance. Furthermore, we develop a subspace distance based triplet loss to improve the distinguishability of the attribute subspace representation. Attribute subspace learning module is only needed for the training phase to jointly learn discriminative global features. This leads to a compact inference phase. Furthermore, the proposed AS-ZSL can be naturally extended to adapt to the transductive ZSL setting using a novel self-supervised training strategy. Extensive experimental results on several widely used ZSL datasets, i.e., CUB, AwA2, and SUN, demonstrate the advantage of AS-ZSL compared with the state-of-the-art under different ZSL settings.}
}
@article{LU2023109818,
title = {Neighborhood overlap-aware heterogeneous hypergraph neural network for link prediction},
journal = {Pattern Recognition},
volume = {144},
pages = {109818},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109818},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005162},
author = {Yifan Lu and Mengzhou Gao and Huan Liu and Zehao Liu and Wei Yu and Xiaoming Li and Pengfei Jiao},
keywords = {Heterogeneous graph, Structural information learning, Complex semantics, Link prediction},
abstract = {In real world, a large number of networks are heterogeneous, containing different types of semantics and connections. Existing studies typically only consider lower-order pairwise relations rather than higher-order group interactions. Furthermore, they tend to focus more on node attributes rather than graph structural information. This results models failing to maintain graph topology effectively, which reduces the effectiveness on link prediction. To address these limitations, we propose Neighborhood Overlap-aware Heterogeneous hypergraph neural network (NOH) that learns useful structural information from the heterogeneous graph and estimates overlapped neighborhood for link prediction. Our model fuses the heterogeneity of graphs with structural information so that the model maintains both lower-order pairwise relations and higher-order complex semantics. Our extensive experiments on four real-world datasets show that NOH consistently achieves state-of-the-art performance on link prediction.}
}
@article{TANG2023109826,
title = {AC2AS: Activation Consistency Coupled ANN-SNN framework for fast and memory-efficient SNN training},
journal = {Pattern Recognition},
volume = {144},
pages = {109826},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109826},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005241},
author = {Jianxiong Tang and Jian-Huang Lai and Xiaohua Xie and Lingxiao Yang and Wei-Shi Zheng},
keywords = {Spiking neural networks, Deep learning, Supervised learning, Image classification},
abstract = {Spiking neural networks are efficient computation models for low-power environments. Spike-based BP algorithms and ANN-to-SNN (ANN2SNN) conversions are successful techniques for SNN training. Nevertheless, the spike-base BP training is slow and requires large memory costs, while ANN2SNN needs many inference steps to obtain good performance. In this paper, we propose an Activation Consistency Coupled ANN-SNN (AC2AS) framework to train the SNN in a fast and memory-efficient way. The AC2AS consists of two components: (a) a weight-shared architecture between ANN and SNN and (b) spiking mapping units. Firstly, the architecture trains the weight-shared parameters on the ANN branch, resulting in fast training and low memory costs for SNN. Secondly, the spiking mapping units are designed to ensure that the activation values of the ANN are the spiking features. As a result, the activation consistency is guaranteed, and the classification error of the SNN can be optimized by training the ANN branch. Besides, we design an adaptive threshold adjustment (ATA) algorithm to decrease the firing of noisy spikes. Experiment results show that our AC2AS-based models perform well on the benchmark datasets (CIFAR10, CIFAR100, and Tiny-ImageNet). Moreover, the AC2AS achieves comparable accuracy under 0.625× time steps, 0.377× training time, 0.27× GPU memory costs, and 0.33× spike activities of the Spike-based BP model. The code is available at https://github.com/TJXTT/AC2ASNN.}
}
@article{MESHUWELDE2023109850,
title = {Counting-based visual question answering with serial cascaded attention deep learning},
journal = {Pattern Recognition},
volume = {144},
pages = {109850},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109850},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005484},
author = {Tesfayee MeshuWelde and Lejian Liao},
keywords = {Counting-based visual question answering, Visual geometry group16, Text convolutional neural network, Optimal weighted fused features, Improved tuna swarm optimization, Serial cascaded recurrent neural network with attention mechanism-based long short-term memory},
abstract = {The counting-based questions play a major part in Visual Question Answering (VQA), the most challenging factor is counting the different objects present in the images. Recently more attention is paid to design a model of count-aided VQA. Based on the questions, the VQA system responds with appropriate answers. Yet, the complex questions are necessitating in the system with answers. The earlier models are still facing the challenging problems of counting the various objects within the images as the models become futile to select the features and lack fine-grained representation. In order to sustain the image representation, this paper proposes a new model for VQA using the heuristic approach of serial cascaded deep learning methods. Initially, the standard data regarding images and text data are gathered and fed to the pre-processing process. Consequently, the feature extraction is done on both the image and the text data. Here, the deep features from images are taken using Visual Geometry Group 16 (VGG16) and the text features are extracted using Text Convolutional Neural Network (TCNN). Then, the optimal weighted fused features are obtained, where the weights used for getting the necessary features are tuned via the Improved Tuna Swarm Optimization (ITSO) algorithm. Finally, the counting answers are retrieved based on the given queries, which is carried out via Serial Cascaded Recurrent Neural Network with Attention Mechanism-based Long Short-Term Memory (SCRAM-LSTM). The performance is examined with divergent metrics compared with conventional models. Hence, the findings reveal that it offers superior performance in estimating the appropriate answers. Therefore, the proposed work is widely used for such potential applications as helping blind or visually impaired people to get information, integrating with image retrieval systems, and also for search engines. Especially, it is utilized for the vision and language systems.}
}
@article{RU2023109862,
title = {Cross-Modal Transformer for RGB-D semantic segmentation of production workshop objects},
journal = {Pattern Recognition},
volume = {144},
pages = {109862},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109862},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005605},
author = {Qingjun Ru and Guangzhu Chen and Tingyu Zuo and Xiaojuan Liao},
keywords = {Cross-Modal, Production workshop object, RGB-D, Semantic segmentation, Transformer},
abstract = {Scene understanding in a production workshop is an important technology to improve its intelligence level, semantic segmentation of production workshop objects is an effective method for realizing scene understanding. Since the varieties of information of production workshop, making full use of the complementary information of RGB image and depth image can effectively improve the semantic segmentation accuracy of production workshop objects. Aiming at solving the multi-scale and real-time problems of segmenting the production workshop objects, this paper proposes Cross-Modal Transformer (CMFormer), a Transformer-based cross-modal semantic segmentation model. Its key feature correction and feature fusion parts are composed of the Multi-Scale Channel Attention Correction(MS-CAC) module and the Global Feature Aggregation(GFA) module. By improving Multi-Head Self-Attention(MHSA) in Transformer, we design Cross-Modal Multi-Head Self-Attention(CM-MHSA) to build long-range interaction between RGB image and depth image, and further design the MS-CAC module and the GFA module on the basis of the CM-MHSA module, to achieve cross-modal information interaction in the channel and spatial dimensions. Among them, the MS-CAC module enriches the multi-scale features of each channel and achieve more accurate channel attention correction between the two modals; the GFA module interacts with RGB feature and depth feature in the spatial dimension and fuses global and local features at the same time. In the experiments on the NYU Depth v2 dataset, the CMFormer reached 68.00% MPA(Mean Pixel Accuracy) and 55.75% mIoU(Mean Intersection over Union), achieves the state-of-the-art results. While in the experiments on the Scene Objects for Production workshop dataset(SOP), the CMFormer achieves 96.74% MPA, 92.98% mIoU and 43 FPS(Frames Per Second), which has high precision and good real-time performance. Code is available at: https://github.com/FutureIAI/CMFormer}
}
@article{GUAN2023109873,
title = {Sparse kernel k-means for high-dimensional data},
journal = {Pattern Recognition},
volume = {144},
pages = {109873},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109873},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300571X},
author = {Xin Guan and Yoshikazu Terada},
keywords = {Clustering, Feature selection, Kernel method},
abstract = {The kernel k-means method usually loses its power when clustering high-dimensional data, due to a large number of irrelevant features. We propose a novel sparse kernel k-means clustering (SKKM) to extend the advantages of kernel k-means to the high-dimensional cases. We assign each feature a 0-1 indicator and optimize an equivalent kernel k-means loss function while penalizing the sum of the indicators. An alternating minimization algorithm is proposed to estimate both the class labels and the feature indicators. We prove the consistency of both clustering and feature selection of the proposed method. In addition, we apply the proposed framework to the normalized cut. In the numerical experiments, we demonstrate that the proposed method provides better/comparable performance compared to the existing high-dimensional clustering methods.}
}
@article{WANG2023109784,
title = {Improving point cloud classification and segmentation via parametric veronese mapping},
journal = {Pattern Recognition},
volume = {144},
pages = {109784},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109784},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300482X},
author = {Ruibin Wang and Xianghua Ying and Bowei Xing and Xin Tong and Taiyan Chen and Jinfa Yang and Yongjie Shi},
keywords = {3D Point cloud, Deep learning, Non-linear space mapping, Shape classification, Part segmentation, Semantic segmentation},
abstract = {Deep learning based 3D point cloud classification and segmentation has achieved remarkable success. Existing methods are usually implemented in the original space with 3D coordinates as inputs. However, we find that point networks taking only information of first-order coordinates hardly learn geometric features of higher order, such as point cloud normals or poses. In this study, we propose to map the input point clouds into a non-linear space to facilitate networks learning and leveraging high-order features. Firstly, we design the Parametric Veronese Mapping (PVM) function which automatically learns to map point clouds into a non-linear space. As a result, the mapped point clouds are enriched with high-order elements and maintain the basic point set properties as in the original 3D space. We can then exploit existing networks to learn high-order features from mapped point clouds. Secondly, we contribute a two-stage transformation learning module that modifies the previous one-stage module to better leverage high-order features for aligning point clouds in the projective space. Finally, an interaction module is designed to learn more discriminative features by aggregating information from both the original and projective space. Extensive experiments demonstrate that our method successfully improves the ability of most existing networks to learn high-order features and thus contributing to more accurate classification and segmentation. Moreover, the resulting models show stronger robustness to affine transformations and real-world perturbations.}
}
@article{TITO2023109834,
title = {Hierarchical multimodal transformers for Multipage DocVQA},
journal = {Pattern Recognition},
volume = {144},
pages = {109834},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109834},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005320},
author = {Rubèn Tito and Dimosthenis Karatzas and Ernest Valveny},
keywords = {Multipage document Visual Question Answering, Document Visual Question Answering, Multipage documents, Document Intelligence},
abstract = {Existing work on DocVQA only considers single-page documents. However, in real applications documents are mostly composed of multiple pages that should be processed altogether. In this work, we propose a new multimodal hierarchical method Hi-VT5, that overcomes the limitations of current methods to process long multipage documents. In contrast to previous hierarchical methods that focus on different semantic granularity (He et al., 2021) or different subtasks (Zhou et al., 2022) used in image classification. Our method is a hierarchical transformer architecture where the encoder learns to summarize the most relevant information of every page and then, the decoder uses this summarized representation to generate the final answer, following a bottom-up approach. Moreover, due to the lack of multipage DocVQA datasets, we also introduce MP-DocVQA, an extension of SP-DocVQA where questions are posed over multipage documents instead of single pages. Through extensive experimentation, we demonstrate that Hi-VT5 is able, in a single stage, to answer the questions and provide the page that contains the answer, which can be used as a kind of explainability measure.}
}
@article{WANG2023109830,
title = {CrowdMLP: Weakly-supervised crowd counting via multi-granularity MLP},
journal = {Pattern Recognition},
volume = {144},
pages = {109830},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109830},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005289},
author = {Mingjie Wang and Jun Zhou and Hao Cai and Minglun Gong},
keywords = {Weakly-supervised learning, Crowd counting, Multi-granularity MLP, Self-supervised proxy task},
abstract = {Currently, state-of-the-art crowd counting algorithms rely excessively on location-level annotations, which are burdensome to acquire. When only weak supervisory signals at the count level are available, it is arduous and error-prone to regress total counts due to the lack of explicit spatial constraints. To address this issue, we propose a novel and efficient counter, CrowdMLP, which explores the modelling of global dependencies of embeddings and regresses total counts by designing a multi-granularity MLP regressor. Specifically, a locally-focused pre-trained frontend is used to extract crude feature maps with intrinsic spatial cues, preventing the model from collapsing into trivial outcomes. The crude embeddings, along with the raw crowd scenes, are tokenized at different granularity levels. Next, the multi-granularity MLP mixes tokens at the dimensions of cardinality, channel, and spatial for mining global information. We also propose an effective proxy task called Split-Counting to overcome the limited samples and the lack of spatial hints in a self-supervised manner. Extensive experiments demonstrate that CrowdMLP significantly outperforms existing weakly-supervised counting algorithms and performs better than state-of-the-art location-level supervised approaches.}
}
@article{UMIRZAKOVA2023109866,
title = {Deep learning-driven diagnosis: A multi-task approach for segmenting stroke and Bell's palsy},
journal = {Pattern Recognition},
volume = {144},
pages = {109866},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109866},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005642},
author = {Sabina Umirzakova and Shabir Ahmad and Sevara Mardieva and Shakhnoza Muksimova and Taeg Keun Whangbo},
keywords = {Segmentation, Face parsing, Early stroke detection, Bell's palsy detection},
abstract = {Strong efforts have been undertaken to enhance the diagnosis and identification of diseases that cause facial paralysis, such as Bell's palsy and stroke, because of their detrimental social effects. Stroke is one of the most serious and potentially fatal conditions among the major cardiovascular disorders. We are introducing a deep-learning-based method for early diagnosis of facial paralysis diseases such as stroke and Bell's palsy. Recognizing the costs associated with traditional diagnostic techniques like magnetic resonance tomography (MRI) and computed tomography (CT) scan images, our model employs a multi-task network, integrating face parsing, facial asymmetry parsing, and category enhancement. Spatial inconsistencies are addressed via a depth-map estimation module that leverages an instance-specific kernel approach. To clarify the boundaries of facial components, we use category edge detection with a foreground attention module, generating generic geometric structures and detailed semantic cues. Our model is trained on two datasets, comprising individuals with regular smiles and those with one-sided facial weakness. This cost-effective, easily accessible solution can streamline the diagnostic process, minimizing data gaps, and reducing needless rescreening and intervention costs.}
}
@article{WANG2023109890,
title = {Crisis event summary generative model based on hierarchical multimodal fusion},
journal = {Pattern Recognition},
volume = {144},
pages = {109890},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109890},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005885},
author = {Jing Wang and Shuo Yang and Hui Zhao},
keywords = {Multimodal summary generation, Unimodal bias, Crisis event, Hierarchy, Dynamic selection},
abstract = {How to quickly obtain information about crisis events on social media such as Twitter and Weibo is crucial for follow-up rescue work and the promotion of postdisaster reconstruction. Therefore, it is very important to obtain useful information through multimodal summary generation technology. The current technology for generating crisis event summaries is mainly affected by unimodal bias and disregards the diversity of information in text and images. To solve these problems, this paper proposes a hierarchical multimodal crisis event summary generation model based on the modal alignment premise and hierarchical thinking. First, the visual context vector and text context vector are obtained, and then the hierarchical multimodal pointer model is employed to generate the text summary. Thus, the modal deviation is solved. Second, to select high-quality images, this paper proposes a dynamic selection strategy, which to some extent considers the requirements of the high correlation between text and images and the diversity of crisis information. Last, the experimental results based on the crisis event data in the MSMO dataset show that the proposed model achieves good performance in the summary generation and image selection of crisis events.}
}
@article{HUANG2023109838,
title = {EEG-based classification combining Bayesian convolutional neural networks with recurrence plot for motor movement/imagery},
journal = {Pattern Recognition},
volume = {144},
pages = {109838},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109838},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005368},
author = {Wenqie Huang and Guanghui Yan and Wenwen Chang and Yuchan Zhang and Yueting Yuan},
keywords = {Electroencephalogram (EEG), Real execution, Motor imagery (MI), Deep learning (DL), Bayesian convolutional neural networks (BCNNs), Recurrence plots (RPs), Feature extraction, Classification},
abstract = {Electroencephalogram (EEG)-based Motor imagery (MI) is a key topic in the brain-computer interface (BCI). The EEG-based real execution and motor imagery multi-class classification tasks are also crucial, but only a few kinds of literature research it. In addition, classification accuracy still has room for improvement, and the inter-individual variability problems in BCI applications need to be solved. To address these issues, we developed a novel model (RP-BCNNs) that combines the recurrence plot (RP) and Bayesian Convolutional Neural Networks (BCNNs). First, we employ an RP computation for preprocessed EEG signals of each channel and merge all RPs of all channels into one based on the weighted average method. Then, we feed the RP features into BCNNs to classify 2-class, 3-class, 4-class, and 5-class on real/imaginary movements classification tasks. The results show that the RP-BCNNs model outperforms the state-of-the-art methods, achieving average accuracies of 92.86%, 94.12%, 91.37%, 92.61% for real movements and 94.07%, 93.77%, 90.54%, 91.85% for imaginary movements. Our findings suggest that combining complex network methods with deep learning can improve the classification performance of EEG-based BCI systems (e.g., motor imagery, emotion recognition, and epileptic seizure classification).}
}
@article{LI2023109874,
title = {BLoG: Bootstrapped graph representation learning with local and global regularization for recommendation},
journal = {Pattern Recognition},
volume = {144},
pages = {109874},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109874},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005721},
author = {Ming Li and Lin Zhang and Lixin Cui and Lu Bai and Zhao Li and Xindong Wu},
keywords = {Graph neural networks (GNN), Graph representation learning, Graph contrastive learning, GNN-based recommender systems},
abstract = {With the explosive growth of online information, the significant application value of recommender systems has received considerable attention. Since user–item interactions can naturally fit into graph structure data, graph neural networks (GNNs), by virtue of their strong ability in graph representation learning, have become the new state-of-the-art approach to recommender systems. Recently, GNN-based contrastive self-supervised learning (SSL) methods have received careful attention due to their superiority over graph-based recommendation under the typical supervised learning paradigm. However, to achieve state-of-the-art performance, GNN-based recommendation with SSL often needs a huge amount of negative examples and the model’s performance is heavily dependent on complex data augmentations. Also, the information interaction among various augmented views is often performed under a single perspective (e.g., structure/feature space or node/graph level). In this paper, we propose a novel bootstrapped graph representation learning with local and global regularization for recommendation, i.e., BLoG, which constructs positive/negative pairs based on the aggregated node features by referring to two alternate views of the original user–item graph structure. In particular, BLoG learns user–item representations by encoding two augmented versions of a user–item bipartite graph using two separate encoders: an online encoder and a target encoder. To facilitate the information interaction between these two distinct graph encoders, we introduce local and global regularization for recommendation, where a graph structural contrastive loss and a node-level semantic loss are defined for local regularization while a graph-level contrastive loss is used for global regularization. An alternative optimization approach is used to train the online encoder and the target encoder. Experimental studies on three benchmark datasets demonstrate that BLoG achieves better recommendation accuracy than several existing baselines.}
}
@article{LIU2023109822,
title = {Unpaired image super-resolution using a lightweight invertible neural network},
journal = {Pattern Recognition},
volume = {144},
pages = {109822},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109822},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005204},
author = {Huan Liu and Mingwen Shao and Yuanjian Qiao and Yecong Wan and Deyu Meng},
keywords = {Image super-resolution, Unpaired SR, Image degradation, Invertible neural network, Generative adversarial network},
abstract = {Unpaired image super-resolution (SR) has recently attracted considerable attention in the unsupervised SR community. In contrast to supervised SR, existing unpaired SR methods inevitably resort to the generative adversarial network (GAN) to explore data distribution on the given HR and unpaired LR dataset. Nevertheless, predominant strategies often strive for sophisticated network structures or training pipelines, making them intractable to apply in real-world scenarios. In this work, a lightweight invertible neural network (INN) is proposed for unpaired SR to alleviate this limitation. Specifically, we regard image degradation and SR as a pair of mutually-inverse tasks and replace the two generators in one-stage GAN with INN. Due to the information lossless nature of INN, it is impossible to generate noise in vain during image degradation. We thus design a simple noise injection network to induce realistic noise, thereby simulating real LR images. To further maintain the stability and realism of the noise, we propose to extract the noise prior from the real-world LR image. With extracted noise prior as input, our noise injection network can narrow the gap between the generated noise and the real one, thereby encouraging the degraded images to match the real-world LR domain. Extensive experiments demonstrate that our method achieves comparable performance with other SOTA methods in quantitative and qualitative evaluations while enjoying faster speed and much smaller parameters.}
}
@article{JIAO2023109846,
title = {DTEC: Decision tree-based evidential clustering for interpretable partition of uncertain data},
journal = {Pattern Recognition},
volume = {144},
pages = {109846},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109846},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005447},
author = {Lianmeng Jiao and Haoyu Yang and Feng Wang and Zhun-ga Liu and Quan Pan},
keywords = {Evidential clustering, Interpretable clustering, Unsupervised decision tree, Belief function theory},
abstract = {Recently, the evidential clustering has been developed as a promising clustering framework for uncertain data, which generalizes those hard, fuzzy, possibilistic and rough clustering. However, the resulting cluster assignments are less interpretable in terms of human cognition, which limits its applications in those security, privacy or ethic related fields. In this study, the unsupervised decision tree model is introduced into the evidential clustering framework to improve the interpretability of the evidential partition. A Decision Tree-based Evidential Clustering (DTEC) algorithm is developed to build an unsupervised evidential decision tree, which uses the paths from the root node to leaf nodes to achieve the interpretability of each cluster. The proposed algorithm is composed of three procedures, i.e., cutting-point selection, node evidential splitting, and cluster adjustment, in which the first two procedures are carried out iteratively to build a preliminary unsupervised decision tree and the last procedure is designed to adjust the preliminary decision tree if the number of clusters is available. Both synthetic and real datasets are used to evaluate the performance of the proposed algorithm, and the experimental results demonstrate the good performance of the proposal compared with some representative fuzzy, evidential or decision tree-based clustering algorithms.}
}
@article{ZHENG2023109825,
title = {Adaptive local adversarial attacks on 3D point clouds},
journal = {Pattern Recognition},
volume = {144},
pages = {109825},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109825},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300523X},
author = {Shijun Zheng and Weiquan Liu and Siqi Shen and Yu Zang and Chenglu Wen and Ming Cheng and Cheng Wang},
keywords = {Point clouds, Adversarial attack, Salient regions, Adversarial examples},
abstract = {Modern artificial intelligence systems rely heavily on deep learning techniques. However, deep neural networks are easily disturbed by adversarial objects. Adversarial examples are beneficial to improve the robustness of the 3D neural network model and enhance the stability of the artificial intelligence system. At present, most 3D adversarial attack methods perturb the entire point cloud to generate adversarial examples, which results in high perturbation costs and low operability in the physical world. In this paper, we propose an adaptive local adversarial attack method (AL-Adv) on 3D point clouds to generate adversarial point clouds. First, we analyze the vulnerability of the 3D network model and extract the salient regions of the input point cloud, namely the vulnerable regions. Second, we propose an adaptive gradient attack algorithm that targets salient regions. The proposed attack algorithm adaptively assigns different disturbances in different directions of the three-dimensional coordinates of the point cloud. Experimental results show that our proposed AL-Adv method achieves a higher attack success rate than the global attack method. Specifically, the adversarial examples generated by AL-Adv demonstrate good imperceptibility and small generation costs.}
}