@article{ZHANG2022108709,
title = {FocusNet: Classifying better by focusing on confusing classes},
journal = {Pattern Recognition},
volume = {129},
pages = {108709},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108709},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200190X},
author = {Xue Zhang and Zehua Sheng and Hui-Liang Shen},
keywords = {Image classification, Inter-class correlations, Confusing classes},
abstract = {Nowadays, most classification networks use one-hot encoding to represent categorical data because of its simplicity. However, one-hot encoding may affect the generalization ability as it neglects inter-class correlations. We observe that, even when a neural network trained with one-hot labels produces incorrect predictions, it still pays attention to the target image region and reveals which classes confuse the network. Inspired by this observation, we propose a confusion-focusing mechanism to address the class-confusion issue. Our confusion-focusing mechanism is implemented by a two-branch network architecture. Its baseline branch generates confusing classes, and its FocusNet branch, whose architecture is flexible, discriminates correct labels from these confusing classes. We also introduce a novel focus-picking loss function to improve classification accuracy by encouraging FocusNet to focus on the most confusing classes. The experimental results validate that our FocusNet is effective for image classification on common datasets, and that our focus-picking loss function can also benefit the current neural networks in improving their classification accuracy.}
}
@article{SUN2022108728,
title = {Query-efficient decision-based attack via sampling distribution reshaping},
journal = {Pattern Recognition},
volume = {129},
pages = {108728},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108728},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002096},
author = {Xuxiang Sun and Gong Cheng and Lei Pei and Junwei Han},
keywords = {Adversarial examples, Decision-based attack, Image classification, Normal vector estimation, Distribution reshaping},
abstract = {With a limited query budget and only the final decision of a target model, how to find adversarial examples with low-magnitude distortion has attracted great attention among researchers. Recent solutions to this issue made use of the estimated normal vector at a boundary data point to search for adversarial examples. However, since the sampling independence between two sampling epochs, they still suffer from a prohibitively high query budget, which will get worse when the dimensionality of the attacked samples get increased. To push for further development, in this paper, we pay attention to a query-efficient method to estimate the normal vector for decision-based attack in high-dimensional space. Specifically, we propose a simple yet effective normal vector estimation framework for high-dimension decision-based attack via Sampling Distribution Reshaping, dubbed SDR. Next, SDR is incorporated into general geometric attack framework. Briefly, SDR leverages all the historically sampled noise to build a guiding vector, which will be used to reshape the next sampling distribution. Besides, we also extend SDR to different ℓp norms for p={2,∞} and deploy low-frequency constraint to enhance the performance of SDR. Compared to peer decision-based attacks, SDR can reach the competitive ℓp norms for p={2,∞}, according to extensive experimental evaluations against both defended and undefended classifiers. Since the simplicity and effectiveness of SDR, we think that reshaping the sampling distribution deserves further research in future works.}
}
@article{HUANG2022108736,
title = {AVPL: Augmented visual perception learning for person Re-identification and beyond},
journal = {Pattern Recognition},
volume = {129},
pages = {108736},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108736},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002175},
author = {Yewen Huang and Sicheng Lian and Haifeng Hu},
keywords = {Person Re-identification, Augmented visual perception learning, Batch attention, Two-stream hypothesis},
abstract = {In this work, we propose an Augmented Visual Perception Learning (AVPL) method for Person Re-identification (ReID) which is inspired by the two-stream hypothesis theory of Human Visual System (HVS). Deep learning methods dominate ReID and many state-of-the-art performances are achieved from the perspective of optimizing the model of ’what’ visual pathway. It does not blend ’what’ and ’where’ well. Our AVPL method uses the essential mechanism of the ventro-dorsal stream of the ’where’ visual pathway to expand the perception field of the model, and integrates with the ’what’ to complete the information of the visually salient regions. A novel Batch Attention (BA), the key component of our Augmented Visual Perception (AVP) module, is proposed to apply fusion and augmentation into all input feature maps within each batch. Through AVP module, the improved attention-based model attaches more importance to enhancement of salient features, therefore acquiring better perceptual ability of salient regions which provide the most distinguishably distinctions for ReID. Extensive experiments have been carried out on four main stream ReID datasets and two recognition datasets. In terms of ReID, our method achieves rank-1 accuracy of 95.2% on CUHK03-NP, 98.7% on Market-1501, 96.0% on DukeMTMC-reID and 88.8% on MSMT17-V1, outperforming the state-of-the-art methods by a large margin. Besides, it has been experimentally proven to be applicable and effective in other recognition tasks including facial expression recognition and action recognition with an improved accuracy.}
}
@article{LEE2022108720,
title = {Variational cycle-consistent imputation adversarial networks for general missing patterns},
journal = {Pattern Recognition},
volume = {129},
pages = {108720},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108720},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002011},
author = {Woojin Lee and Sungyoon Lee and Junyoung Byun and Hoki Kim and Jaewook Lee},
keywords = {Imputation, Missing data, Cycle-consistent},
abstract = {Imputation of missing data is an important but challenging issue because we do not know the underlying distribution of the missing data. Previous imputation models have addressed this problem by assuming specific kinds of missing distributions. However, in practice, the mechanism of the missing data is unknown, so the most general case of missing pattern needs to be considered for successful imputation. In this paper, we present cycle-consistent imputation adversarial networks to discover the underlying distribution of missing patterns closely under some relaxations. Using adversarial training, our model successfully learns the most general case of missing patterns. Therefore our method can be applied to a wide variety of imputation problems. We empirically evaluated the proposed method with numerical and image data. The result shows that our method yields the state-of-the-art performance quantitatively and qualitatively on standard datasets.}
}
@article{HU2022108744,
title = {Representation learning using deep random vector functional link networks for clustering},
journal = {Pattern Recognition},
volume = {129},
pages = {108744},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108744},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002254},
author = {Minghui Hu and P.N. Suganthan},
keywords = {Random vector functional link, Unsupervised learning, Consensus clustering, Manifold regularization},
abstract = {Random Vector Functional Link (RVFL) Networks have received a lot of attention due to the fast training speed as the non-iterative solution characteristic. Currently, the main research direction of RVFLs has supervised learning, including semi-supervised and multi-label. There are hardly any unsupervised research results for RVFLs. In this paper, we propose the unsupervised RVFL (usRVFL), and the unsupervised framework is generic that can be used with other RVFL variants, thus we extend it to an ensemble deep variant, unsupervised deep RVFL (usdRVFL). The unsupervised method is based on the manifold regularization while the deep variant is related to the consensus clustering method, which can increase the capability and diversity of RVFLs. Our unsupervised approaches also benefit from fast training speed, even the deep variant offers a very competitive computation efficiency. Empirical experiments on several benchmark datasets demonstrated the effectiveness of the proposed algorithms.}
}
@article{CAO2022108768,
title = {Unsupervised discriminative feature learning via finding a clustering-friendly embedding space},
journal = {Pattern Recognition},
volume = {129},
pages = {108768},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108768},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002497},
author = {Wenming Cao and Zhongfan Zhang and Cheng Liu and Rui Li and Qianfen Jiao and Zhiwen Yu and Hau-San Wong},
keywords = {Deep clustering, Unsupervised learning, Generative adversarial networks, Siamese network},
abstract = {In this paper, we propose an enhanced deep clustering network (EDCN), which is composed of a Feature Extractor, a Conditional Generator, a Discriminator and a Siamese Network. Specifically, we will utilize two kinds of generated data based on adversarial training, as well as the original data, to train the Feature Extractor for learning effective latent representations. In addition, we adopt the Siamese network to find an embedding space, where a better affinity similarity matrix is obtained as the key to success of spectral clustering in providing reliable pseudo-labels. Particularly, the obtained pseudo-labels will be used to generate realistic data by the Generator. Finally, the discriminator is used to model the real joint distribution of data and corresponding latent representations for Feature Extractor enhancement. To evaluate our proposed EDCN, we conduct extensive experiments on multiple data sets including MNIST, USPS, FRGC, CIFAR-10, STL-10, and Fashion-MNIST by comparing our method with a number of state-of-the-art deep clustering methods, and experimental results demonstrate its effectiveness and superiority.}
}
@article{BARISIN2022108747,
title = {Methods for segmenting cracks in 3d images of concrete: A comparison based on semi-synthetic images},
journal = {Pattern Recognition},
volume = {129},
pages = {108747},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108747},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200228X},
author = {Tin Barisin and Christian Jung and Franziska Müsebeck and Claudia Redenbach and Katja Schladitz},
keywords = {Computed tomography, Fractional Brownian surface, 3d segmentation, Crack detection, Machine learning, Deep learning},
abstract = {Concrete is the standard construction material for buildings, bridges, and roads. As safety plays a central role in the design, monitoring, and maintenance of such constructions, it is important to understand the cracking behavior of concrete. Computed tomography captures the microstructure of building materials and allows to study crack initiation and propagation. Manual segmentation of crack surfaces in large 3d images is not feasible. In this paper, automatic crack segmentation methods for 3d images are reviewed and compared. Classical image processing methods (edge detection filters, template matching, minimal path and region growing algorithms) and learning methods (convolutional neural networks, random forests) are considered and tested on semi-synthetic 3d images. Their performance strongly depends on parameter selection which should be adapted to the grayvalue distribution of the images and the geometric properties of the concrete. In general, the learning methods perform best, in particular for thin cracks and low grayvalue contrast.}
}
@article{ULICNY2022108707,
title = {Harmonic convolutional networks based on discrete cosine transform},
journal = {Pattern Recognition},
volume = {129},
pages = {108707},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108707},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001881},
author = {Matej Ulicny and Vladimir A. Krylov and Rozenn Dahyot},
keywords = {Harmonic network, Convolutional neural network, Discrete cosine transform, Image classification, Object detection, Semantic segmentation},
abstract = {Convolutional neural networks (CNNs) learn filters in order to capture local correlation patterns in feature space. We propose to learn these filters as combinations of preset spectral filters defined by the Discrete Cosine Transform (DCT). Our proposed DCT-based harmonic blocks replace conventional convolutional layers to produce partially or fully harmonic versions of new or existing CNN architectures. Using DCT energy compaction properties, we demonstrate how the harmonic networks can be efficiently compressed by truncating high-frequency information in harmonic blocks thanks to the redundancies in the spectral domain. We report extensive experimental validation demonstrating benefits of the introduction of harmonic blocks into state-of-the-art CNN models in image classification, object detection and semantic segmentation applications.}
}
@article{GAMMELLI2022108752,
title = {Recurrent flow networks: A recurrent latent variable model for density estimation of urban mobility},
journal = {Pattern Recognition},
volume = {129},
pages = {108752},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108752},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002333},
author = {Daniele Gammelli and Filipe Rodrigues},
keywords = {Urban mobility, Latent variable models, Normalizing flows, Variational inference},
abstract = {Mobility-on-demand (MoD) systems represent a rapidly developing mode of transportation wherein travel requests are dynamically handled by a coordinated fleet of vehicles. Crucially, the efficiency of an MoD system highly depends on how well supply and demand distributions are aligned in spatio-temporal space (i.e., to satisfy user demand, cars have to be available in the correct place and at the desired time). To do so, we argue that predictive models should aim to explicitly disentangle between temporal and spatial variability in the evolution of urban mobility demand. However, current approaches typically ignore this distinction by either treating both sources of variability jointly, or completely ignoring their presence in the first place. In this paper, we propose recurrent flow networks11Code available at https://www.github.com/DanieleGammelli/recurrent-flow-nets (RFN), where we explore the inclusion of (i) latent random variables in the hidden state of recurrent neural networks to model temporal variability, and (ii) normalizing flows to model the spatial distribution of mobility demand. We demonstrate how predictive models explicitly disentangling between spatial and temporal variability exhibit several desirable properties, and empirically show how this enables the generation of distributions matching potentially complex urban topologies.}
}
@article{NAKAMURA2022108776,
title = {Stochastic batch size for adaptive regularization in deep network optimization},
journal = {Pattern Recognition},
volume = {129},
pages = {108776},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108776},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002576},
author = {Kensuke Nakamura and Stefano Soatto and Byung-Woo Hong},
keywords = {Deep network optimization, Adaptive regularization, Stochastic gradient descent, Adaptive mini-batch size},
abstract = {We propose a first-order stochastic optimization algorithm incorporating adaptive regularization for pattern recognition problems in deep learning framework. The adaptive regularization is imposed by stochastic process in determining batch size for each model parameter at each optimization iteration. The stochastic batch size is determined by the update probability of each parameter following a distribution of gradient norms in consideration of their local and global properties in the neural network architecture where the range of gradient norms may vary within and across layers. We empirically demonstrate the effectiveness of our algorithm using an image classification task based on conventional network models applied to commonly used benchmark datasets. The quantitative evaluation indicates that our algorithm outperforms the state-of-the-art optimization algorithms in generalization while providing less sensitivity to the selection of batch size which often plays a critical role in optimization, thus achieving more robustness to the selection of regularity.}
}
@article{ZHENG2022108724,
title = {High-resolution rectified gradient-based visual explanations for weakly supervised segmentation},
journal = {Pattern Recognition},
volume = {129},
pages = {108724},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108724},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002059},
author = {Tianyou Zheng and Qiang Wang and Yue Shen and Xiang Ma and Xiaotian Lin},
abstract = {Visual explanations for convolutional neural networks (CNNs) act as the backbone for weakly supervised segmentation with image-level labels. This paper proposes a high-resolution rectified gradient-based class activation mapping with bounding box annotations (bbox) to improve the initial seed for weakly supervised segmentation (WSS) tasks. HRCAM extends Grad-CAM by separating the gradient maps from the class activation maps from the shallow layer for higher resolution. Gradient rectified methods are proposed to improve the visualization and WSS score. Experiments and evaluations are conducted to verify the performance of HRCAM-BB on Pascal VOC 2012 and COCO datasets. On Pascal VOC 2012 set, our method achieves outstanding performance with a mean intersection over union (mIOU) of 71.6 with image-level labels and 78.2 with bbox on WSSS, and increases the WSIS mIOU (AP50) to 52.1 with image-level labels, and 61.9 with bbox. our method surpasses the previous SOTA approach in the same condition.}
}
@article{GUO2022108715,
title = {Direction of arrival estimation for indoor environments based on acoustic composition model with a single microphone},
journal = {Pattern Recognition},
volume = {129},
pages = {108715},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108715},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001960},
author = {Xingchen Guo and Xuexin Xu and Xunquan Chen and Jinhui Chen and Rong Jia and Zhihong Zhang and Tetsuya Takiguchi and Edwin R. Hancock},
keywords = {Gaussian mixture model (GMM), Acoustic transfer function (ATF), Talker localization},
abstract = {This paper presents an effective method for multiple talker localisation using only a single microphone in a room. One of the main challenge here is obtaining a model that can be used for estimating the localization parameter. This model must be sensitive to all possible speaker locations and correctly discriminate their positions. The reverberant speech signal in a room environment can be composited by the clean speech and the acoustic transfer function (ATF). The ATF is a useful tool to describe changes of the speech source, and the approaches based on ATF can thus be used to identify talker localizations with a single microphone. This paper presents two methods, referred to as Composite Reverberant Speech (CRS) model and Direct Training Reverberant Speech (DTRS) model, and uses these methods for obtaining the ATF of a room. The approaches based on proposed methods can successfully and accurately process multi-talker localization task with single microphone. Experiments also demonstrate the effectiveness of the proposed methods.}
}
@article{BERENGUELBAETA2022108740,
title = {Atlanta scaled layouts from non-central panoramas},
journal = {Pattern Recognition},
volume = {129},
pages = {108740},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108740},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002217},
author = {Bruno Berenguel-Baeta and Jesus Bermudez-Cameo and Jose J. Guerrero},
keywords = {Omnidirectional vision, 3D vision, Non-central cameras, Layout recovery, Scene understanding},
abstract = {In this work we present a novel approach for 3D layout recovery of indoor environments using a non-central acquisition system. From a single non-central panorama, full and scaled 3D lines can be independently recovered by geometry reasoning without additional nor scale assumptions. However, their sensitivity to noise and complex geometric modeling has led these panoramas and required algorithms being little investigated. Our new pipeline aims to extract the boundaries of the structural lines of an indoor environment with a neural network and exploit the properties of non-central projection systems in a new geometrical processing to recover scaled 3D layouts. The results of our experiments show that we improve state-of-the-art methods for layout recovery and line extraction in non-central projection systems. We completely solve the problem both in Manhattan and Atlanta environments, handling occlusions and retrieving the metric scale of the room without extra measurements. As far as the authors’ knowledge goes, our approach is the first work using deep learning on non-central panoramas and recovering scaled layouts from single panoramas.}
}
@article{SULTANA2022108719,
title = {Unsupervised moving object segmentation using background subtraction and optimal adversarial noise sample search},
journal = {Pattern Recognition},
volume = {129},
pages = {108719},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108719},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200200X},
author = {Maryam Sultana and Arif Mahmood and Soon Ki Jung},
keywords = {Moving objects segmentation, Generative adversarial network, Background subtraction},
abstract = {Moving Objects Segmentation (MOS) is a fundamental task in many computer vision applications such as human activity analysis, visual object tracking, content based video search, traffic monitoring, surveillance, and security. MOS becomes challenging due to abrupt illumination variations, dynamic backgrounds, camouflage and scenes with bootstrapping. To address these challenges we propose a MOS algorithm exploiting multiple adversarial regularizations including conventional as well as least squares losses. More specifically, our model is trained on scene background images with the help of cross-entropy loss, least squares adversarial loss and ℓ1 loss in image space working jointly to learn the dynamic background changes. During testing, our proposed method aims to generate test image background scenes by searching optimal noise samples using joint minimization of ℓ1 loss in image space, ℓ1 loss in feature space, and discriminator least squares loss. These loss functions force the generator to synthesize dynamic backgrounds similar to the test sequences which upon subtraction results in moving objects segmentation. Experimental evaluations on five benchmark datasets have shown excellent performance of the proposed algorithm compared to the twenty one existing state-of-the-art methods.}
}
@article{CHO2022108703,
title = {Unsupervised video anomaly detection via normalizing flows with implicit latent features},
journal = {Pattern Recognition},
volume = {129},
pages = {108703},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108703},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001844},
author = {MyeongAh Cho and Taeoh Kim and Woo Jin Kim and Suhwan Cho and Sangyoun Lee},
keywords = {Video anomaly detection, Surveillance system, AutoEncoder, Normalizing flow},
abstract = {In contemporary society, surveillance anomaly detection, i.e., spotting anomalous events such as crimes or accidents in surveillance videos, is a critical task. As anomalies occur rarely, most training data consists of unlabeled videos without anomalous events, which makes the task challenging. Most existing methods use an autoencoder (AE) to learn to reconstruct normal videos; they then detect anomalies based on their failure to reconstruct the appearance of abnormal scenes. However, because anomalies are distinguished by appearance as well as motion, many previous approaches have explicitly separated appearance and motion informationfor example, using a pre-trained optical flow model. This explicit separation restricts reciprocal representation capabilities between two types of information. In contrast, we propose an implicit two-path AE (ITAE), a structure in which two encoders implicitly model appearance and motion features, along with a single decoder that combines them to learn normal video patterns. For the complex distribution of normal scenes, we suggest normal density estimation of ITAE features through normalizing flow (NF)-based generative models to learn the tractable likelihoods and identify anomalies using out-of-distribution detection. NF models intensify ITAE performance by learning normality through implicitly learned features. Finally, we demonstrate the effectiveness of ITAE and its feature distribution modeling on six benchmarks, including databases that contain various anomalies in real-world scenarios.}
}
@article{LIANG2022108706,
title = {Uncertainty-aware twin support vector machines},
journal = {Pattern Recognition},
volume = {129},
pages = {108706},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108706},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200187X},
author = {Zhizheng Liang and Lei Zhang},
keywords = {Uncertain data, Twin support vector machines, Halfspaces, Kernel functions, Data classification},
abstract = {There exist uncertain data in the real world due to some factors such as imprecise measurements and noise. Unlike deterministic data, the features of samples in uncertain data are often described by interval numbers or random vectors with probability density functions. In this paper we propose novel twin support vector machines (TSVMs) to handle uncertain data. In the proposed models which are referred to as uncertainty-aware TSVMs, each uncertain sample is modeled as a random vector with Gaussian distributions. To deal with the multi-dimensional integrals in the original models, we derive an interesting and important theorem which helps us transform the original models into the model involving one-dimensional integrals. The simplification of models makes the optimization problem tractable and the simplified models are solved by using the quasi-Newton optimization algorithm. The proposed decision rule allows us to classify uncertain samples with means and covariance matrices. In addition, we extend the proposed models to their kernel versions to capture the nonlinear structure of uncertain data. Experiments on a series of data sets have been performed to demonstrate that the proposed models gain better classification performance than some existing algorithms, especially for representing uncertain cross-plane problems.}
}
@article{FU2022108711,
title = {Fovea localization by blood vessel vector in abnormal fundus images},
journal = {Pattern Recognition},
volume = {129},
pages = {108711},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108711},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001923},
author = {Yinghua Fu and Ge Zhang and Jiang Li and Dongyan Pan and Yongxiong Wang and Dawei Zhang},
keywords = {Blood vessel vector (BVV), Fovea localization, Retinal raphe, Probability bubble, Region search},
abstract = {In human eyes, macula is responsible for sharp central vision with fovea in the center. The location of fovea becomes an important landmark in diagnosing the retinal diseases. As macula doesn’t have the clear boundary and obvious shape, deep learning techniques to locate the fovea often fail in complicated lesions and insufficient training samples, and the unsupervised method is incapable for illumination variations. In this paper, a new unsupervised fovea localization method using the retinal raphe and region searching is proposed, and the blood vessel vector (BVV) model is developed. After detecting blood vessels and OD by U-net and probability bubbles, the BVVs are conceived and the retinal raphe is obtained by summating all the BVVs, then the fovea is estimated through the local region searching. Compared with the parabola model, the BVV model does not involve the coordinate transformation and reduces the complexity to the linear time cost O(N). Two other unsupervised techniques the parabola model and intensity searching and five supervised techniques cGAN, U-net, DRNet, MedTnet and EANet are included and compared. The global feature of retinal vessels is utilized which makes the proposed method more robust to the lesions than the other localization methods. The experiments on public datasets Kaggle, MESSIDOR and IDRiD validate the effectiveness of the proposed method by the student’s t-test, and our method obtains the least average Euclidean distance to the groundtruth on Kaggle and almost least on Base 33 of MESSIDOR.}
}
@article{HADI2022108780,
title = {A new distance between multivariate clusters of varying locations, elliptical shapes, and directions},
journal = {Pattern Recognition},
volume = {129},
pages = {108780},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108780},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002618},
author = {Ali S. Hadi},
keywords = {Clustering methods, Complete linkage, Elliptical distance, Euclidean distance, Hamming distance, Hierarchical clustering, Iris data, -Means clustering, Manhattan distance, Single linkage, Robust estimation, Ward method},
abstract = {Clustering methods are based on the computations of both the distances between every pair of the n observations in a multivariate dataset as well as the distances between every pair of clusters in the dataset. The clusters can have different locations and varying elliptical shapes and directions. Numerous methods have been proposed in the literature for computing both of these two types of distances. The contributions of this paper are two folds. First, we propose a new elliptical distance between pairs of clusters in a dataset with different cluster centers and elliptical shapes and directions, Second, we proved analytically that the Ward distance and the Euclidean distance are equivalent. We propose a new classical method for computing the distance between a pair of clusters in the dataset. It is the only distance that does not assume spherical clusters. The proposed classical distances could also be made robust by replacing estimates of location and scale by their respective robust estimators. The proposed distance has a number of advantages including simplicity, interpretability, computational efficiency as well as the ability to accurately capture both the variability of the cluster centers as well as the variability of shapes and directions of their respective covariance matrices. The method is also illustrated by several motivating examples that demonstrate the need of the new proposed distance. The superiority of the proposed method is also demonstrated by application to real-life as well as challenging synthetic data.}
}
@article{HUO2022108727,
title = {Attention regularized semi-supervised learning with class-ambiguous data for image classification},
journal = {Pattern Recognition},
volume = {129},
pages = {108727},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108727},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002084},
author = {Xiaoyang Huo and Xiangping Zeng and Si Wu and Hau-San Wong},
keywords = {Semi-supervised learning, Image classification, Attention regularization, Class-ambiguous data},
abstract = {Data augmentation via randomly combining training instances and interpolating the corresponding labels has shown impressive gains in image classification. However, model attention regions are not necessarily meaningful in class semantics, especially for the case of limited supervision. In this paper, we present a semi-supervised classification model based on Class-Ambiguous Data with Attention Regularization, which is referred to as CADAR. Specifically, we adopt a Random Regional Interpolation (RRI) module to construct complex and effective class-ambiguous data, such that the model behavior can be regularized around decision boundaries. By aggregating the parameters of a classification network over training epochs to produce more reliable predictions on unlabeled data, RRI can also be applied to them as well as labeled data. Further, the classifier is enforced to apply consistent attention on the original and constructed data. This is important for inducing the model to learn discriminative features from the class-related regions. The experiment results demonstrate that CADAR significantly benefits from the constructed data and attention regularization, and thus achieves superior performance across multiple standard benchmarks and different amounts of labeled data.}
}
@article{COOPER2022108743,
title = {Believe the HiPe: Hierarchical perturbation for fast, robust, and model-agnostic saliency mapping},
journal = {Pattern Recognition},
volume = {129},
pages = {108743},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108743},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002242},
author = {Jessica Cooper and Ognjen Arandjelović and David J Harrison},
keywords = {XAI, AI safety, Saliency mapping, Deep learning explanation, Interpretability, Prediction attribution},
abstract = {Understanding the predictions made by Artificial Intelligence (AI) systems is becoming more and more important as deep learning models are used for increasingly complex and high-stakes tasks. Saliency mapping – a popular visual attribution method – is one important tool for this, but existing formulations are limited by either computational cost or architectural constraints. We therefore propose Hierarchical Perturbation, a very fast and completely model-agnostic method for interpreting model predictions with robust saliency maps. Using standard benchmarks and datasets, we show that our saliency maps are of competitive or superior quality to those generated by existing model-agnostic methods – and are over 20× faster to compute.}
}
@article{LIU2022108767,
title = {SELF-LLP: Self-supervised learning from label proportions with self-ensemble},
journal = {Pattern Recognition},
volume = {129},
pages = {108767},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108767},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002485},
author = {Jiabin Liu and Zhiquan Qi and Bo Wang and YingJie Tian and Yong Shi},
keywords = {Learning from label proportion, Self-supervised learning, Self-ensemble strategy, Multi-task learning},
abstract = {In this paper, we tackle the problem called learning from label proportions (LLP), where the training data is arranged into various bags, with only the proportions of different categories in each bag available. Existing efforts mainly focus on training a model with only the limited proportion information in a weakly supervised manner, thus result in apparent performance gap to supervised learning, as well as computational inefficiency. In this work, we propose a multi-task pipeline called SELF-LLP to make full use of the information contained in the data and model themselves. Specifically, to intensively learn representation from the data, we leverage the self-supervised learning as a plug-in auxiliary task to learn better transferable visual representation. The main insight is to benefit from the self-supervised representation learning with deep model, as well as improving classification performance by a large margin. Meanwhile, in order to better leverage the implicit benefits from the model itself, we incorporate the self-ensemble strategy to guide the training process with an auxiliary supervision information, which is constructed by aggregating multiple previous network predictions. Furthermore, a ramp-up mechanism is further employed to stabilize the training process. In the extensive experiments, our method demonstrates compelling advantages in both accuracy and efficiency over several state-of-the-art LLP approaches.}
}
@article{GAMMELLI2022108751,
title = {Generalized multi-output Gaussian process censored regression},
journal = {Pattern Recognition},
volume = {129},
pages = {108751},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108751},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002321},
author = {Daniele Gammelli and Kasper Pryds Rolsted and Dario Pacino and Filipe Rodrigues},
keywords = {Censored data, Gaussian processes, Variational inference},
abstract = {When modelling censored observations (i.e. data in which the value of a measurement or observation is un-observable beyond a given threshold), a typical approach in current regression methods is to use a censored-Gaussian (i.e. Tobit) model to describe the conditional output distribution. In this paper, as in the case of missing data, we argue that exploiting correlations between multiple outputs can enable models to better address the bias introduced by censored data. To do so, we introduce a heteroscedastic multi-output Gaussian process model which combines the non-parametric flexibility of GPs with the ability to leverage information from correlated outputs under input-dependent noise conditions. To address the resulting inference intractability, we further devise a variational bound to the marginal log-likelihood suitable for stochastic optimization. We empirically evaluate our model against other generative models for censored data on both synthetic and real world tasks and further show how it can be generalized to deal with arbitrary likelihood functions. Results show how the added flexibility allows our model to better estimate the underlying non-censored (i.e. true) process under potentially complex censoring dynamics.}
}
@article{ZHOU2022108723,
title = {Three-dimensional affinity learning based multi-branch ensemble network for breast tumor segmentation in MRI},
journal = {Pattern Recognition},
volume = {129},
pages = {108723},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108723},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002047},
author = {Lei Zhou and Shuai Wang and Kun Sun and Tao Zhou and Fuhua Yan and Dinggang Shen},
keywords = {Breast tumor segmentation, Three-dimensional affinity learning based refinement, Multi-branch ensemble network},
abstract = {Accurate and automatic breast tumor segmentation based on dynamic contrast-enhancement magnetic resonance imaging (DCE-MRI) plays an important role in breast cancer analysis. However, the background parenchymal enhancement and large variations in tumor size, shape or appearance make the task very challenging, and also the segmentation performance is still not satisfactory, especially for non-mass enhancement (NME) and small size tumors (≤2 cm). To address these challenges, we propose a novel 3D affinity learning based multi-branch ensemble network for accurate breast tumor segmentation. Specifically, two different types of subnetworks are built to form a multi-branch network. The two subnetworks are equipped with effective operation components, i.e., residual connection and channel-wise attention or making use of dense connectivity patterns, which can process the input images in parallel. Second, we propose an end-to-end trainable 3D affinity learning based refinement module by calculating the similarities between features of voxels, which is useful in discovering more pixels belonging to breast tumors. Third, two local affinity matrices are constructed by 3D affinity learning, which are used to refine the segmentation outputs of two subnetworks, respectively. Finally, a novel ensemble module is proposed to combine the information derived from the subnetworks, which can hierarchically merge the local and global affinity matrices derived from subnetworks. A large-scale breast DCE-MR images dataset with 420 subjects are built for evaluation, and comprehensive experiments have been conducted to demonstrate that our proposed method achieves superior performance over state-of-the-art medical image segmentation methods.}
}
@article{CHANG2022108778,
title = {Self-weighted learning framework for adaptive locality discriminant analysis},
journal = {Pattern Recognition},
volume = {129},
pages = {108778},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108778},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200259X},
author = {Wei Chang and Feiping Nie and Zheng Wang and Rong Wang and Xuelong Li},
keywords = {Supervised dimensionality reduction, Linear discriminant analysis, Re-weighted method},
abstract = {Linear discriminant analysis (LDA) is one of the most important dimensionality reduction techniques and applied in many areas. However, traditional LDA algorithms aim to capture the global structure from data and ignore the local information. That may lead to the failure of LDA in some real-world datasets which have a complex geometry distribution. Although there are many previous works that focus on preserving the local information, they are all stuck in the same problem that the neighbor relationships of pairwise data points obtained from the original space may not be reliable, especially in the case of heavy noise. Therefore, we proposed a novel self-weighted learning framework, named Self-Weighted Adaptive Locality Discriminant Analysis (SALDA), for locality-aware based dimensionality reduction. The proposed framework can adaptively learn an intrinsic low-dimensional subspace, so that we can explore the better neighbor relationships for samples under the ideal subspace. In addition, our model can automatically learn to assign the weights to data pairwise points within the same class and takes no extra parameters compared to other classical locality-aware methods. At last, the experimental results on both synthetic and real-world benchmark datasets demonstrate the effectiveness and superiority of the proposed algorithm.}
}
@article{HUUSARI2022108759,
title = {Cross-View kernel transfer},
journal = {Pattern Recognition},
volume = {129},
pages = {108759},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108759},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002400},
author = {Riikka Huusari and Cécile Capponi and Paul Villoutreix and Hachem Kadri},
keywords = {Multi-view learning, Cross-view transfer, Kernel completion, Kernel learning},
abstract = {We consider the kernel completion problem with the presence of multiple views in the data. In this context the data samples can be fully missing in some views, creating missing columns and rows to the kernel matrices that are calculated individually for each view. We propose to solve the problem of completing the kernel matrices with Cross-View Kernel Transfer (CVKT) procedure, in which the features of the other views are transformed to represent the view under consideration. The transformations are learned with kernel alignment to the known part of the kernel matrix, allowing for finding generalizable structures in the kernel matrix under completion. Its missing values can then be predicted with the data available in other views. We illustrate the benefits of our approach with simulated data, multivariate digits dataset and multi-view dataset on gesture classification, as well as with real biological datasets from studies of pattern formation in early Drosophila melanogaster embryogenesis.}
}
@article{CHENG2022108718,
title = {Entropy guided attention network for weakly-supervised action localization},
journal = {Pattern Recognition},
volume = {129},
pages = {108718},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108718},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001996},
author = {Yi Cheng and Ying Sun and Hehe Fan and Tao Zhuo and Joo-Hwee Lim and Mohan Kankanhalli},
keywords = {Temporal action localization, Weakly-supervised learning, Entropy guided loss, Global similarity loss},
abstract = {One major challenge of Weakly-supervised Temporal Action Localization (WTAL) is to handle diverse backgrounds in videos. To model background frames, most existing methods treat them as an additional action class. However, because background frames usually do not share common semantics, squeezing all the different background frames into a single class hinders network optimization. Moreover, the network would be confused and tends to fail when tested on videos with unseen background frames. To address this problem, we propose an Entropy Guided Attention Network (EGA-Net) to treat background frames as out-of-domain samples. Specifically, we design a two-branch module, where a domain branch detects whether a frame is an action by learning a class-agnostic attention map, and an action branch recognizes the action category of the frame by learning a class-specific attention map. By aggregating the two attention maps to model the joint domain-class distribution of frames, our EGA-Net can handle varying backgrounds. To train the class-agnostic attention map with only the video-level class labels, we propose an Entropy Guided Loss (EGL), which employs entropy as the supervision signal to distinguish action and background. Moreover, we propose a Global Similarity Loss (GSL) to enhance the action-specific attention map via action class center. Extensive experiments on THUMOS14, ActivityNet1.2 and ActivityNet1.3 datasets demonstrate the effectiveness of our EGA-Net.}
}
@article{LI2022108763,
title = {The devil in the tail: Cluster consolidation plus cluster adaptive balancing loss for unsupervised person re-identification},
journal = {Pattern Recognition},
volume = {129},
pages = {108763},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108763},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002448},
author = {Mingkun Li and He Sun and Chaoqun Lin and Chun-Guang Li and Jun Guo},
keywords = {Unsupervised person re-identification, Cluster consolidation, Cluster adaptive balancing loss, Long-tail problem},
abstract = {Unsupervised person re-identification (Re-ID) is to retrieve pedestrians from different camera views without supervision information. State-of-the-art methods are usually built upon training a convolution neural network with pseudo labels generated by clustering. Unfortunately, the pseudo labels are highly unbalanced and heavily noisy, carrying ineffective or even erroneous supervision information. To address these deficiencies, we present an effective clustering and reorganization approach, called Cluster Consolidation, which aims to separate a small proportion of unreliable data points from each cluster. This approach benefits to improve the quality of the pseudo labels, but also yields more tiny clusters. Thus, we further propose a Cluster Adaptive Balancing (CAB) loss to effectively train the network with the imbalance pseudo labels, where our CAB loss is able to automatically balance the importance of each cluster. We conduct extensive experiments on widely used person Re-ID benchmark datasets and demonstrate the effectiveness of our proposals.}
}
@article{COSKUN2022108702,
title = {An adaptive estimation method with exploration and exploitation modes for non-stationary environments},
journal = {Pattern Recognition},
volume = {129},
pages = {108702},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108702},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001832},
author = {Kutalmış Coşkun and Borahan Tümer},
keywords = {Stochastic learning, Concept drift, Change detection, Parameter estimation, Dynamic learning rate},
abstract = {Dynamic systems are highly complex and hard to deal with due to their subject- and time-varying nature. The fact that most of the real world systems/events are of dynamic character makes modeling and analysis of such systems inevitable and charmingly useful. One promising estimation method that is capable of unlearning past information to deal with non-stationarity is Stochastic Learning Weak Estimator (SLWE) by Oommen and Rueda (2006). However, due to using a constant learning rate, it faces a trade-off between plasticity and stability. In this paper, we model SLWE as a random walk and provide rigorous theoretical analysis of asymptotic behavior of estimates to obtain a statistical model. Utilizing this model, we detect changes in stationarity to switch between exploratory and exploitative learning modes. Experimental evaluations on both synthetic and real world data show that the proposed method outperforms related algorithms in different types of drifts.}
}
@article{NGUYENMEIDINE2022108771,
title = {Incremental multi-target domain adaptation for object detection with efficient domain transfer},
journal = {Pattern Recognition},
volume = {129},
pages = {108771},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108771},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002527},
author = {Le Thanh Nguyen-Meidine and Madhu Kiran and Marco Pedersoli and Jose Dolz and Louis-Antoine Blais-Morin and Eric Granger},
keywords = {Deep learning, Convolutional NNs, Object detection, Unsupervised domain adaptation, Multi-Target domain adaptation, Incremental learning},
abstract = {Recent advances in unsupervised domain adaptation have significantly improved the recognition accuracy of CNNs by alleviating the domain shift between (labeled) source and (unlabeled) target data distributions. While the problem of single-target domain adaptation (STDA) for object detection has recently received much attention, multi-target domain adaptation (MTDA) remains largely unexplored, despite its practical relevance in several real-world applications, such as multi-camera video surveillance. Compared to the STDA problem that may involve large domain shifts between complex source and target distributions, MTDA faces additional challenges, most notably the computational requirements and catastrophic forgetting of previously-learned targets, which can depend on the order of target adaptations. STDA for detection can be applied to MTDA by adapting one model per target, or one common model with a mixture of data from target domains. However, these approaches are either costly or inaccurate. The only state-of-art MTDA method specialized for detection learns targets incrementally, one target at a time, and mitigates the loss of knowledge by using a duplicated detection model for knowledge distillation, which is computationally expensive and does not scale well to many domains. In this paper, we introduce an efficient approach for incremental learning that generalizes well to multiple target domains. Our MTDA approach is more suitable for real-world applications since it allows updating the detection model incrementally, without storing data from previous-learned target domains, nor retraining when a new target domain becomes available. Our approach leverages domain discriminators to train a novel Domain Transfer Module (DTM), which only incurs a modest overhead. The DTM transforms source images according to diverse target domains, allowing the model to access a joint representation of previously-learned target domains, and to effectively limit catastrophic forgetting. Our proposed method – called MTDA with DTM (MTDA-DTM) – is compared against state-of-the-art approaches on several MTDA detection benchmarks and Wildtrack, a benchmark for multi-camera pedestrian detection. Results indicate that MTDA-DTM achieves the highest level of detection accuracy across multiple target domains, yet requires significantly fewer computational resources. Our code is available.11https://github.com/Natlem/M-HTCN.}
}
@article{DONG2022108750,
title = {Negational symmetry of quantum neural networks for binary pattern classification},
journal = {Pattern Recognition},
volume = {129},
pages = {108750},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108750},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200231X},
author = {Nanqing Dong and Michael Kampffmeyer and Irina Voiculescu and Eric Xing},
keywords = {Deep learning, Quantum machine learning, Binary pattern classification, Representation learning, Symmetry},
abstract = {Although quantum neural networks (QNNs) have shown promising results in solving simple machine learning tasks recently, the behavior of QNNs in binary pattern classification is still underexplored. In this work, we find that QNNs have an Achilles’ heel in binary pattern classification. To illustrate this point, we provide a theoretical insight into the properties of QNNs by presenting and analyzing a new form of symmetry embedded in a family of QNNs with full entanglement, which we term negational symmetry. Due to negational symmetry, QNNs can not differentiate between a quantum binary signal and its negational counterpart. We empirically evaluate the negational symmetry of QNNs in binary pattern classification tasks using Google’s quantum computing framework. Both theoretical and experimental results suggest that negational symmetry is a fundamental property of QNNs, which is not shared by classical models. Our findings also imply that negational symmetry is a double-edged sword in practical quantum applications.}
}
@article{YIN2022108710,
title = {MPCCL: Multiview predictive coding with contrastive learning for person re-identification},
journal = {Pattern Recognition},
volume = {129},
pages = {108710},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108710},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001911},
author = {Junhui Yin and Jiyang Xie and Zhanyu Ma and Jun Guo},
keywords = {Person re-identification, Kernel density estimation, Representation construction, Contrastive learning},
abstract = {In this paper, we investigate a new representation learning approach, termed as Multiview Predictive Coding with Contrastive Learning (MPCCL), for person re-identification (re-ID). Different from the conventional re-ID approaches that focus on learning representations from semantic label, our approach learns the identification of invariant information via representation reconstruction, which explores more fine-grained semantic information in representation space. Specifically, given a chosen identity, the learned representation of its single view can be reconstructed by those of other views. Therefore, kernel density estimation (KDE) is firstly introduced for the adaptive reconstruction of the representation. Then, contrastive learning is adopted to increase the distance between the representations of the same views with different identities. Finally, representation reconstruction and contrastive learning jointly supervise the representation learning process, thus obtaining fine-grained semantic information and appearance-free representations. Extensive experiments on several re-ID datasets demonstrate that the proposed approach yields state-of-the-art results.}
}
@article{YAN2022108779,
title = {Robust distance metric optimization driven GEPSVM classifier for pattern classification},
journal = {Pattern Recognition},
volume = {129},
pages = {108779},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108779},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002606},
author = {He Yan and Liyong Fu and Tian'an Zhang and Jun Hu and Qiaolin Ye and Yong Qi and Dong-Jun Yu},
keywords = {Classification problem, Distance metric learning, Outliers and noises, Robust L-GEPSVM method, Squared L-norm distance},
abstract = {Proximal support vector machine via generalized eigenvalues (GEPSVM) is one of the most successful methods for classification problems. However, GEPSVM is vulnerable to outliers since it learns classifiers based on the squared L2-norm distance without a specific strategy to deal with the outliers. Motivated by existing studies that improve the robustness of GEPSVM via the L1-norm distance or not-squared L2-norm distance formulation, a novel GEPSVM formulation that minimizes the p-order of L2-norm distance is proposed, namely, L2,p-GEPSVM. This formulation weakens the negative effects of both light and heavy outliers in the data. An iterative algorithm is designed to solve the general L2,p-norm distance minimization problems and rigorously prove its convergence. In addition, we adjust the parameters of L2,p-GEPSVM to balance the accuracy and training time. This is especially useful for larger datasets. Extensive results indicate that the L2,p-GEPSVM improves the classification performance and robustness in various experimental settings.}
}
@article{DALCOL2022108690,
title = {Graph regularization multidimensional projection},
journal = {Pattern Recognition},
volume = {129},
pages = {108690},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108690},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001716},
author = {Alcebiades {Dal Col} and Fabiano Petronetto},
keywords = {Mapping of patterns, Bidimensional mapping, Visualization, Multidimensional projection, Graph signal processing, Data analysis},
abstract = {This paper introduces a novel multidimensional projection method of datasets. Our method called Graph Regularization Multidimensional Projection (GRMP) is based on a technique from the graph signal processing theory, the graph regularization. Initially, a similarity graph is built on the high-dimensional space where the dataset lies. A two-dimensional distribution of points is then created in the visual space using a phyllotactic distribution. The similarity graph is copied properly over the phyllotactic distribution and the graph regularization is applied to their coordinates, which are interpreted as graph signals. The graph regularization reorganizes the phyllotactic distribution by bringing together points that represent similar data in the high-dimensional space. We employ synthetic and real datasets to demonstrate the effectiveness of our method. Furthermore, since the solution of the graph regularization can still be approximated using a fast approximation mechanism based on the Chebyshev polynomials, our method is computationally efficient even for large graphs.}
}
@article{HE2022108714,
title = {Multi-manifold discriminant local spline embedding},
journal = {Pattern Recognition},
volume = {129},
pages = {108714},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108714},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001959},
author = {Ping He and Xiaohua Xu and Xincheng Chang and Jie Ding and Suquan Chen},
keywords = {Manifold learning, Dimension reduction, Classification, Thin plate spline, Multiple manifolds},
abstract = {Manifold learning reveals the intrinsic low-dimensional manifold structure of high-dimensional data and has achieved great success in a wide spectrum of applications. However, traditional manifold learning methods assume that all the data lie on a common manifold, hence fail to capture the complicated geometry structure of the real-world data lying on multiple manifolds. This paper proposes a novel Multi-manifold Discriminant Local Spline Embedding (MDLSE) algorithm for high-dimensional classification, which considers a more realistic scenario where data of the same class lies on the same manifold. On the basis of this assumption, MDLSE seeks to reconstruct multiple manifolds for different classes of data in the embedding and separate them as apart as possible. In order to preserve the geometry structure of all the manifolds, MDLSE employs thin plate splines to align the local patches within each manifold compatibly in the global embedding. Meanwhile, to separate the different manifolds, MDLSE utilizes discriminative information to ensure the neighboring data from different manifolds to be mapped far from each other. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of MDLSE over the other representative manifold learning algorithms. The advantage of MDLSE is often more obvious on smaller size of training data and in lower embedding dimensions.}
}
@article{XU2022108700,
title = {Cycle-reconstructive subspace learning with class discriminability for unsupervised domain adaptation},
journal = {Pattern Recognition},
volume = {129},
pages = {108700},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108700},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001819},
author = {Yayun Xu and Hua Yan},
keywords = {Domain adaptation, Subspace learning, Transfer learning, Knowledge transfer},
abstract = {Unsupervised domain adaptation is used to effectively learn a classifier for data of the unlabeled target domain by utilizing the data of the source domain with sufficient labels but different distributions. In general, a transformation matrix is employed to acquire a common subspace where the distributions of the two domains are aligned, which is easy to lose lots of unique information of the two domains. To better preserve useful information during the transformation process, we propose a novel Cycle-Reconstructive Subspace Learning with Class Discriminability (CRSL) approach that uses two reconstructive matrixes through an iterative strategy to cycle-reconstruct data matrixes and update the common subspace. In this way, we learn the invariant features in the common subspace while better preserving global and local structures of the two original domains. Finally, we implement additional discriminative constraints such as intra-class aggregation and inter-class diffusion on the transformed features to ensure the class discriminability of data of the two domains. Extensive experiment results show that our conventional method outperforms state-of-the-art conventional methods and is comparable with advanced deep methods on four current domain adaptation datasets.}
}
@article{WANG2022108726,
title = {Pose error analysis method based on a single circular feature},
journal = {Pattern Recognition},
volume = {129},
pages = {108726},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108726},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002072},
author = {Zepeng Wang and Derong Chen and Jiulu Gong},
keywords = {Pose measurement, Monocular vision, Geometrical analysis, Outlier analysis, Optimisation},
abstract = {The measurement accuracy of pose parameters based on a single circular feature depends not only on the accuracy of camera calibration and feature extraction but also on the relative pose of the feature and camera—different poses correspond to different error transmission coefficients. To obtain the relationship between measurement errors and pose parameters, we propose an error analysis method based on geometric interpretation. The method characterises measurement error by the sensitivity the imaging feature has to the variation of pose parameters. In addition, the method can be extended to the error analysis work of other coplanar features' pose measurement algorithms. We conducted simulations on measurement errors of pose parameters under different poses, and the results show that the error distribution of pose parameters is in good agreement with the theoretical analysis. Moreover, we propose a method for judging and optimising outliers, and experimental results show the feasibility of this method.}
}
@article{YIN2022108758,
title = {Multilevel wavelet-based hierarchical networks for image compressed sensing},
journal = {Pattern Recognition},
volume = {129},
pages = {108758},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108758},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002394},
author = {Zhu Yin and WuZhen Shi and Zhongcheng Wu and Jun Zhang},
keywords = {Compressed sensing, Hierarchical reconstruction, Sparse signal, Multilevel wavelet transform},
abstract = {Recently, deep learning-based compressed sensing (CS) algorithms have been reported, which remarkably achieve pleasing reconstruction quality with low computational complexity. However, the sampling process of the common deep learning-based CS methods and the conventional ones cannot sufficiently exploit the structured sparsity within image sequences, especially in preserving finer texture details. In this paper, we propose a novel multilevel wavelet-based hierarchical networks for image compressed sensing (dubbed MWHCS-Net). In particular, MWHCS-Net consists of three modules: a sampling module based on a multilevel wavelet transform, a hierarchical initial reconstruction module and a lightweight deep reconstruction module. Motivated by the fact that a sparser signal is easier to reconstruct accurately, we present the sampling module based on multilevel wavelet transform with hierarchical subspace learning for progressive acquisition of measurements to further optimize sampling efficiency and stability. To enhance the finer texture details, the hierarchical initial reconstruction module is designed as a basic initial reconstruction network plus an enhanced initial reconstruction network, which corresponding to the dominant structure component and the texture detail component of the reconstructed image, respectively. At the same time, we also further explore the impact of the hierarchical initial reconstruction module and prove that the texture detail component branch plays an important role in improving the reconstruction quality. Experimental results demonstrate that the proposed MWHCS-Net achieves the state-of-the-art performance while maintaining an efficient running speed. Furthermore, MWHCS-Net outperforms the existing image CS methods based on deep learning in terms of anti-noise performance in most cases.}
}
@article{ZHU2022108742,
title = {Representation learning with deep sparse auto-encoder for multi-task learning},
journal = {Pattern Recognition},
volume = {129},
pages = {108742},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108742},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002230},
author = {Yi Zhu and Xindong Wu and Jipeng Qiang and Xuegang Hu and Yuhong Zhang and Peipei Li},
keywords = {Deep sparse auto-encoder, Multi-task learning, RICA, Labeled and unlabeled data},
abstract = {We demonstrate an effective framework to achieve a better performance based on Deep Sparse auto-encoder for Multi-task Learning, called DSML for short. To learn the reconstructed and higher-level features on cross-domain instances for multiple tasks, we combine the labeled and unlabeled data from all tasks to reconstruct the feature representations. Furthermore, we propose the model of Stacked Reconstruction Independence Component Analysis (SRICA for short) for the optimization of feature representations with a large amount of unlabeled data, which can effectively address the redundancy of image data. Our proposed SRICA model is developed from RICA and is based on deep sparse auto-encoder. In addition, we adopt a Semi-Supervised Learning method (SSL for short) based on model parameter regularization to build a unified model for multi-task learning. There are several advantages in our proposed framework as follows: 1) The proposed SRICA makes full use of a large amount of unlabeled data from all tasks. It is used to pursue an optimal sparsity feature representation, which can overcome the over-fitting problem effectively. 2) The deep architecture used in our SRICA model is applied for higher-level and better representation learning, which is designed to train on patches for sphering the input data. 3) Training parameters in our proposed framework has lower computational cost compared to other common deep learning methods such as stacked denoising auto-encoders. Extensive experiments on several real image datasets demonstrate our proposed framework outperforms the state-of-the-art methods.}
}
@article{KANG2022108766,
title = {Pay attention to what you read: Non-recurrent handwritten text-Line recognition},
journal = {Pattern Recognition},
volume = {129},
pages = {108766},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108766},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002473},
author = {Lei Kang and Pau Riba and Marçal Rusiñol and Alicia Fornés and Mauricio Villegas},
keywords = {Handwriting text recognition, Transformers, Self-Attention, Implicit language model},
abstract = {The advent of recurrent neural networks for handwriting recognition marked an important milestone reaching impressive recognition accuracies despite the great variability that we observe across different writing styles. Sequential architectures are a perfect fit to model text lines, not only because of the inherent temporal aspect of text, but also to learn probability distributions over sequences of characters and words. However, using such recurrent paradigms comes at a cost at training stage, since their sequential pipelines prevent parallelization. In this work, we introduce a novel method that bypasses any recurrence during the training process with the use of transformer models. By using multi-head self-attention layers both at the visual and textual stages, we are able to tackle character recognition as well as to learn language-related dependencies of the character sequences to be decoded. Our model is unconstrained to any predefined vocabulary, being able to recognize out-of-vocabulary words, i.e. words that do not appear in the training vocabulary. We significantly advance over prior art and demonstrate that satisfactory recognition accuracies are yielded even in few-shot learning scenarios.}
}
@article{LI2022108738,
title = {An end-to-end identity association network based on geometry refinement for multi-object tracking},
journal = {Pattern Recognition},
volume = {129},
pages = {108738},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108738},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002199},
author = {Rui Li and Baopeng Zhang and Zhu Teng and Jianping Fan},
keywords = {Multi-object tracking, Interactions, Occlusions, Data association, Identity verification},
abstract = {In multi-target tracking, object interactions and occlusions are two significant factors that affect tracking performance. To settle this, we propose an identity association network (IANet) that integrates the geometry refinement network (GRNet) and the identity verification (IV) module to perform data association and reason the mapping between the detections and tracklets. In our data association process, the object drifts caused by object interactions are suppressed effectively by encoding the direction and velocity of objects to refine the geometric position of tracklets. The tracklets with refined geometric information are further utilized in the IV module to achieve a sufficient encoding of multivariate spatial cues including both appearance and geometry information, which defeats the misleading impacts of interactions and occlusions dramatically in multi-object tracking. The extensive experiments and comparative evaluations have demonstrated that our proposed method can significantly outperform many state-of-the-art methods on benchmarks of 2D MOT2015, MOT16, MOT17, MOT20, and KITTI by using public detection and online settings.}
}
@article{LIU2022108774,
title = {VFMVAC: View-filtering-based multi-view aggregating convolution for 3D shape recognition and retrieval},
journal = {Pattern Recognition},
volume = {129},
pages = {108774},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108774},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002552},
author = {Zehua Liu and Yuhe Zhang and Jian Gao and Shurui Wang},
keywords = {Multi-view, Channel shuffle, Convolution, Recognition, Retrieval},
abstract = {Multi-view based 3D shape recognition methods have achieved state-of-the-art performance in 3D shape recognition and retrieval. The main focus of multi-view based approaches is determining how to fuse multi-view features into a compact, descriptive, and robust 3D shape descriptor that can then be utilized for 3D shape recognition and retrieval. This paper proposes a novel multi-view aggregating framework, view-filtering-based multi-view aggregating convolution (VFMVAC) to learn global shape descriptors for 3D shape recognition. The proposed VFMVAC applies a voting-based view filtering strategy to select representative views, also introduces a novel multi-view aggregating module to integrate multi-view features; this substantially improves the descriptiveness of the descriptors, and therefore improves the performance of 3D shape recognition and retrieval. Specifically, all views are fed into a voting-based view filtering module to select the top-k representative views. Subsequently, the features of the top-k views are fed into the multi-view aggregating module, which first conducts cross-view channel shuffle for achieving cross-view information flowing, and the resulted reshaped features are then fed into the aggregating convolution module for feature fusion. Experiments on benchmark datasets demonstrate that the proposed VFMVAC is effective and outperforms several recent techniques with respect to the classification and retrieval performance, robustness and efficiency.}
}
@article{LI2022108722,
title = {Paying attention for adjacent areas: Learning discriminative features for large-scale 3D scene segmentation},
journal = {Pattern Recognition},
volume = {129},
pages = {108722},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108722},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002035},
author = {Mengtian Li and Yuan Xie and Lizhuang Ma},
keywords = {Large-scale 3D point clouds, Attention, Long-tailed distribution, Segmentation},
abstract = {Despite recent improvements in analyzing large-scale 3D point clouds, several problems still exist: (a) segmentation models suffer from intra-class inconsistency and inter-class indistinction; (b) the existing methods ignore the inherent long-tailed class distribution of real-world 3D data. These problems result in unsatisfactory semantic segmentation predictions, especially in object adjacent areas. To handle these problems, this paper proposes a novel Adjacent areas Refinement Network (ARNet). Specifically, an Adjacent areas Refinement (AR) module is designed, which consists of two parallel attention blocks. Besides, our proposed attention blocks can process a large number of points (N∼105) with a slight increase in the computational complexity and time cost. Additionally, to deal with the inherent long-tailed class distribution in real-world 3D data, imbalance adjustment loss and occupancy regression loss are introduced. Based on this, the proposed network can handle the classification of both majority and minority classes, which is essential in distinguishing the ambiguous parts in large-scale 3D scenes. The proposed AR module and the loss functions can be easily integrated into the cutting-edge backbone networks, contributing to better performance in modeling semantic inter-dependencies and significantly improving the accuracy of the state-of-the-art semantic segmentation methods on indoor and outdoor scenes.}
}
@article{WANG2022108782,
title = {Salient object detection with image-level binary supervision},
journal = {Pattern Recognition},
volume = {129},
pages = {108782},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108782},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002631},
author = {Pengjie Wang and Yuxuan Liu and Ying Cao and Xin Yang and Yu Luo and Huchuan Lu and Zijian Liang and Rynson W.H. Lau},
keywords = {Weak supervision, Salient object detection, Binary labels},
abstract = {Recent deep learning based salient object detection (SOD) methods have achieved impressive performance. However, while fully-supervised methods require a large amount of labeled data, weakly-supervised methods still require a considerable human effort. To address this problem, we propose a novel weakly-supervised method for salient object detection based on only binary image tags, which are much cheaper to collect. Our basic idea is to construct a dataset of images that are labeled as either salient (with salient objects) or non-salient (without salient objects), and leverage such binary labels as supervision to learn a salient object detector based on existing unsupervised methods. In particular, we propose a target saliency map hallucinator, which can synthesize pseudo ground truth saliency maps for the salient images in the training data solely from binary labels. We can then use the pseudo ground truth labels to train a salient object detector. Experimental results show that our method performs comparably to the state-of-the-art weakly-supervised methods, but requires considerably less human supervision.}
}
@article{RIBERO2022108746,
title = {Federating recommendations using differentially private prototypes},
journal = {Pattern Recognition},
volume = {129},
pages = {108746},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108746},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002278},
author = {Mónica Ribero and Jette Henderson and Sinead Williamson and Haris Vikalo},
keywords = {Recommender systems, Differential Privacy, Federated Learning, Cross-Silo Federated Learning, Matrix Factorization},
abstract = {Machine learning methods exploit similarities in users’ activity patterns to provide recommendations in applications across a wide range of fields including entertainment, dating, and commerce. However, in domains that demand protection of personally sensitive data, such as medicine or banking, how can we learn recommendation models without accessing the sensitive data and without inadvertently leaking private information? Many situations in the medical field prohibit centralizing the data from different hospitals and thus require learning from information kept in separate databases. We propose a new federated approach to learning global and local private models for recommendation without collecting raw data, user statistics, or information about personal preferences. Our method produces a set of locally learned prototypes that allow us to infer global behavioral patterns while providing differential privacy guarantees for users in any database of the system. By requiring only two rounds of communication, we both reduce the communication costs and avoid excessive privacy loss associated with typical federated learning iterative procedures. We test our framework on synthetic data, real federated medical data, and a federated version of Movielens ratings. We show that local adaptation of the global model allows the proposed method to outperform centralized matrix-factorization-based recommender system models, both in terms of the accuracy of matrix reconstruction and in terms of the relevance of recommendations, while maintaining provable privacy guarantees. We also show that our method is more robust and has smaller variance than individual models learned by independent entities.}
}
@article{ZENG2022108754,
title = {Realistic frontal face reconstruction using coupled complementarity of far-near-sighted face images},
journal = {Pattern Recognition},
volume = {129},
pages = {108754},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108754},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002357},
author = {Kangli Zeng and Zhongyuan Wang and Tao Lu and Jianyu Chen and Baojin Huang and Zhen Han and Xin Tian},
keywords = {Face frontalization, Super-resolution, Information compensation, Far-near faces},
abstract = {There is still a huge gap in the accuracy of face recognition in public video surveillance scenarios. The far-sighted low-resolution (LR) frontal faces have holistic facial profiles but lack sufficient clearness, while the near-sighted high-resolution (HR) tilted faces show rich facial details yet incomplete facial structure suffering from the overhead self-occlusion of the head blocking the face. Following this observation, this paper proposes a dual-branch HR frontal face reconstruction network to explicitly exploit such coupled complementarity hidden in the far-near face images of the same subject, where one branch performs super-resolution (SR) of the LR frontal face and the other branch performs detail fusion and holistic compensation between multiple HR tilted faces as well as the super-resolved frontal result. In particular, we propose a secondary relevance attention mechanism to enhance the embedding of key features, which sequentially performs rough and precise feature matching and embedding, thus enabling coarse-to-fine progressive compensation. Further, scale-entangled densely connected blocks (SEDCB) are used to gradually integrate the relevance information at different scales (due to the different sighting distances) to promote the information interaction between the features of tilted faces. Besides, we also propose a ternary coupled sample pair (LR far-sighted frontal face, HR near-sighted tilted face, normal ground truth clear face) training scheme to supervise the network optimization. Extensive experimental results on two real-world tilt-view face datasets show that our method can not only reconstruct more realistic HR frontal faces but also facilitate the down-stream face identification task compared with the competing counterparts.}
}
@article{ZHENG2022108717,
title = {HFA-Net: High frequency attention siamese network for building change detection in VHR remote sensing images},
journal = {Pattern Recognition},
volume = {129},
pages = {108717},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108717},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001984},
author = {Hanhong Zheng and Maoguo Gong and Tongfei Liu and Fenlong Jiang and Tao Zhan and Di Lu and Mingyang Zhang},
keywords = {Building change detection, High frequency enhancement, Spatial-wise attention, Convolutional neural network},
abstract = {Building change detection (BCD) recently can be handled well under the booming of deep-learning based computer vision techniques. However, segmentation and recognition for objects with sharper boundaries still suffer from the poorly acquired high frequency information, which can result in the deteriorated annotation of building boundaries in BCD. To better obtain the high frequency pattern under the deep learning pipeline, we propose a high frequency attention-guided Siamese network (HFA-Net) in which a novel built-in high frequency attention block (HFAB) is applied. HFA-Net is designed to enhance high frequency information of buildings via HFAB which is composed of two main stages, i.e., the spatial-wise attention (SA) and the high frequency enhancement (HF). The SA firstly guides the model to search and focus on buildings, and HF is employed afterwards to highlight the high frequency information of the input feature maps. With high frequency information of buildings enhanced by HFAB, HFA-Net is able to better detect the edges of changed buildings, so as to improve the performance of BCD. Our proposed method is evaluated on three widely-used public datasets, i.e., WHU-CD, LEVIR-CD, and Google dataset. Remarkable experimental results on these datasets indicate that our proposed method can better detect edges of changed buildings and shows a better performance. The source code will be released at: https://github.com/HaiXing-1998/HFANet.}
}
@article{AVOLA2022108762,
title = {3D hand pose and shape estimation from RGB images for keypoint-based hand gesture recognition},
journal = {Pattern Recognition},
volume = {129},
pages = {108762},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108762},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002436},
author = {Danilo Avola and Luigi Cinque and Alessio Fagioli and Gian Luca Foresti and Adriano Fragomeni and Daniele Pannone},
keywords = {Hand pose estimation, Hand shape estimation, Deep learning, Hand gesture recognition},
abstract = {Estimating the 3D pose of a hand from a 2D image is a well-studied problem and a requirement for several real-life applications such as virtual reality, augmented reality, and hand gesture recognition. Currently, reasonable estimations can be computed from single RGB images, especially when a multi-task learning approach is used to force the system to consider the shape of the hand when its pose is determined. However, depending on the method used to represent the hand, the performance can drop considerably in real-life tasks, suggesting that stable descriptions are required to achieve satisfactory results. In this paper, we present a keypoint-based end-to-end framework for 3D hand and pose estimation and successfully apply it to the task of hand gesture recognition as a study case. Specifically, after a pre-processing step in which the images are normalized, the proposed pipeline uses a multi-task semantic feature extractor generating 2D heatmaps and hand silhouettes from RGB images, a viewpoint encoder to predict the hand and camera view parameters, a stable hand estimator to produce the 3D hand pose and shape, and a loss function to guide all of the components jointly during the learning phase. Tests were performed on a 3D pose and shape estimation benchmark dataset to assess the proposed framework, which obtained state-of-the-art performance. Our system was also evaluated on two hand-gesture recognition benchmark datasets and significantly outperformed other keypoint-based approaches, indicating that it is an effective solution that is able to generate stable 3D estimates for hand pose and shape.}
}
@article{ALAVI2022108770,
title = {A bi-level formulation for multiple kernel learning via self-paced training},
journal = {Pattern Recognition},
volume = {129},
pages = {108770},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108770},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002515},
author = {Fatemeh Alavi and Sattar Hashemi},
keywords = {Multiple kernel learning, Self-paced learning, Bi-level optimization, Local kernel alignment, Global kernel alignment},
abstract = {Multiple kernel learning (MKL) is a crucial issue which has been widely researched over the last two decades. Although existing MKL algorithms have achieved satisfactory performance in a broad range of applications, these methods do not adequately consider the adverse effects of unreliable or less reliable instances. To handle this shortcoming, we formulate multiple kernel learning in a bi-level learning paradigm consisting of the kernel combination weight learning (KWL) stage and the self-paced learning (SPL) stage, which alternatively negotiate with each other. The KWL stage dynamically absorbs reliable instances into model learning to accurately capture neighborhood relationships and obtains kernel coefficients via maximizing both global and local kernel alignment in a common schema. The SPL stage automatically evaluates the reliability of training samples via self-paced training. The extensive experiments indicate the robustness and superiority of the presented approach in comparison with existing MKL methods.}
}
@article{WAN2022108705,
title = {Multi-level graph learning network for hyperspectral image classification},
journal = {Pattern Recognition},
volume = {129},
pages = {108705},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108705},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001868},
author = {Sheng Wan and Shirui Pan and Shengwei Zhong and Jie Yang and Jian Yang and Yibing Zhan and Chen Gong},
keywords = {Graph convolutional network, Graph-based machine learning, Hyperspectral image classification, Remote sensing, Graph structural learning},
abstract = {Graph Convolutional Network (GCN) has emerged as a new technique for hyperspectral image (HSI) classification. However, in current GCN-based methods, the graphs are usually constructed with manual effort and thus is separate from the classification task, which could limit the representation power of GCN. Moreover, the employed graphs often fail to encode the global contextual information in HSI. Hence, we propose a Multi-level Graph Learning Network (MGLN) for HSI classification, where the graph structural information at both local and global levels can be learned in an end-to-end fashion. First, MGLN employs attention mechanism to adaptively characterize the spatial relevance among image regions. Then localized feature representations can be produced and further used to encode the global contextual information. Finally, prediction can be acquired with the help of both local and global contextual information. Experiments on three real-world hyperspectral datasets reveal the superiority of our MGLN when compared with the state-of-the-art methods.}
}
@article{LUO2022108713,
title = {ECDNet: A bilateral lightweight cloud detection network for remote sensing images},
journal = {Pattern Recognition},
volume = {129},
pages = {108713},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108713},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001947},
author = {Chen Luo and Shanshan Feng and Xutao Li and Yunming Ye and Baoquan Zhang and Zhihao Chen and YingLing Quan},
keywords = {Lightweight network, Efficient cloud detection, Dual-branch architecture},
abstract = {Cloud detection is one of the critical tasks in remote sensing image pre-processing and it has attracted extensive research interest. In recent years, deep neural networks based cloud detection methods have surpassed the traditional methods (threshold-based methods and conventional machine learning-based methods). However, current approaches mainly focus on improving detection accuracy. The computation complexity and large model size are ignored. To tackle this problem, we propose a lightweight deep learning cloud detection model: Efficient Cloud Detection Network (ECDNet). This model is based on the encoder-decoder structure. In the encoder, a two-path architecture is proposed to extract the spatial and semantic information concurrently. One pathway is the detail branch. It is designed to capture low-level detail spatial features with only a few parameters. The other pathway is the semantic branch, which is mainly for capturing context features. In the semantic branch, a proposed dense pyramid module (DPM) is designed for multi-scale contextual information extraction. The number of parameters and calculations in DPM is greatly reduced by features reusing. Besides, a FusionBlock is developed to merge these two kinds of information. Then the extreme lightweight decoder recovers the cloud mask to the same scale as the input image step by step. To improve performance, boost loss is introduced without inference cost increment. We evaluate the proposed method on two public datasets: LandSat8 and MODIS. Extensive experiments demonstrate that the proposed ECDNet achieves comparable accuracy as the state-of-art cloud detection methods, and meantime has a much smaller model size and less computation burden.}
}
@article{XIA2022108725,
title = {Dual relation network for temporal action localization},
journal = {Pattern Recognition},
volume = {129},
pages = {108725},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108725},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002060},
author = {Kun Xia and Le Wang and Sanping Zhou and Gang Hua and Wei Tang},
keywords = {Temporal action localization, Relation reasoning},
abstract = {Temporal action localization is a challenging task for video understanding. Most previous methods process each proposal independently and neglect the reasoning of proposal-proposal and proposal-context relations. We argue that the supplementary information obtained by exploiting these relations can enhance the proposal representation and further boost the action localization. To this end, we propose a dual relation network to model both proposal-proposal and proposal-context relations. Concretely, a proposal-proposal relation module is leveraged to learn discriminative supplementary information from relevant proposals, which allows the network to model their interaction based on appearance and geometric similarities. Meanwhile, a proposal-context relation module is employed to mine contextual clues by adaptively learning from the global context outside of region-based proposals. They effectively leverage the inherent correlation between actions and the long-term dependency with videos for high-quality proposal refinement. As a result, the proposed framework enables the model to distinguish similar action instances and locate temporal boundaries more precisely. Extensive experiments on the THUMOS14 dataset and ActivityNet v1.3 dataset demonstrate that the proposed method significantly outperforms recent state-of-the-art methods.}
}
@article{BAGCHI2022108757,
title = {EEG-ConvTransformer for single-trial EEG-based visual stimulus classification},
journal = {Pattern Recognition},
volume = {129},
pages = {108757},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108757},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002382},
author = {Subhranil Bagchi and Deepti R. Bathula},
keywords = {EEG, Visual stimulus classification, Deep learning, Transformer, Multi-head attention, Inter-region similarity, Temporal convolution, Inter-head diversity, Head representations},
abstract = {Different categories of visual stimuli evoke distinct activation patterns in the human brain. These patterns can be captured with EEG for utilization in application such as Brain-Computer Interface (BCI). However, accurate classification of these patterns acquired using single-trial data is challenging due to the low signal-to-noise ratio of EEG. Recently, deep learning-based transformer models with multi-head self-attention have shown great potential for analyzing variety of data. This work introduces an EEG-ConvTranformer network that is based on both multi-headed self-attention and temporal convolution. The novel architecture incorporates self-attention modules to capture inter-region interaction patterns and convolutional filters to learn temporal patterns in a single module. Experimental results demonstrate that EEG-ConvTransformer achieves improved classification accuracy over state-of-the-art techniques across five different visual stimulus classification tasks. Finally, quantitative analysis of inter-head diversity also shows low similarity in representational space, emphasizing the implicit diversity of multi-head attention.}
}
@article{TRAN2022108765,
title = {Security and privacy enhanced smartphone-based gait authentication with random representation learning and digital lockers},
journal = {Pattern Recognition},
volume = {129},
pages = {108765},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108765},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002461},
author = {Lam Tran and Thuc Nguyen and Hyunil Kim and Deokjai Choi},
keywords = {Gait authentication, Biometric template protection, Biometric cryptosystems, Gait recognition, Key binding scheme, Biometric key generation},
abstract = {Gait data captured by inertial sensors of smartphone have demonstrated promising results on user authentication. However, most existing models stored the enrolled gait pattern in plaintext for matching with the pattern being validated, thus, posed critical security and privacy issues. In this study, we present a gait cryptosystem that generates from gait data captured by smartphone sensors the random keys for user authentication, meanwhile, secures the gait pattern. First, we propose a revocable and random binary string extraction method using deep neural network followed by feature-wise binarization. A novel loss function for network optimization is also designed, to tackle not only the intra-user stability but also the inter-user randomness. Second, we propose a new biometric key generation scheme, namely Irreversible Error Correct and Obfuscate (IECO), improved from the Error Correct and Obfuscate (ECO) scheme, to securely generate from the binary string a random and irreversible key. The model was evaluated with two benchmark datasets as OU-ISIR and whuGAIT. The evaluation showed that our model could generate the key of 139 bits from 5-second data sequence with zero False Acceptance Rate (FAR) and False Rejection Rate (FRR) smaller than 5.441%. In addition, the security and user privacy analyses showed that our model was secure against existing attacks on biometric template protection, and fulfilled the irreversibility and unlinkability requirements.}
}
@article{ZHANG2022108737,
title = {Improving the Facial Expression Recognition and Its Interpretability via Generating Expression Pattern-map},
journal = {Pattern Recognition},
volume = {129},
pages = {108737},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108737},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002187},
author = {Jing Zhang and Huimin Yu},
keywords = {Facial expression recognition, Facial expression visualization, Expression pattern-map generator, Deep neural networks},
abstract = {Facial expression recognition focuses on extracting expression-related features on a face. In this paper, a novel method is proposed for facial expression modeling based on the following two aspects: seeking expression-related regions more accurately, and enhancing expression features more discriminating. To this end, we design a model containing three submodules: the Expression Feature Extractor (EFE), the Expression Mask Refiner (EMR), and the Expression Pattern-Map Generator (EPMG). The EFE module is the backbone that extracts expression features and generates a coarse attention mask which roughly indicates expression-related regions. The EMR module refines the mask to be more precise by modeling the relationship among expression-related regions, and generates the masked features. The EPMG module utilizes the masked features to further model the fusion and extraction process which obtains a compact and discriminating expression-salient embedding for recognition, and generates an expression pattern-map. We propose the concept of the expression pattern-map, which provides a unified visualization of expression features and improves the interpretability of facial expression recognition. Our model is evaluated on four public datasets (CK+, Oulu-CASIA, RAF-DB, AffectNet), and achieves the competitive performance compared with the state-of-the-art.}
}
@article{PAGESZAMORA2022108721,
title = {Unsupervised ensemble learning for genome sequencing},
journal = {Pattern Recognition},
volume = {129},
pages = {108721},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108721},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002023},
author = {Alba Pagès-Zamora and Idoia Ochoa and Gonzalo Ruiz Cavero and Pol Villalvilla-Ornat},
keywords = {Expectation maximization algorithm, Variant calling, Genome sequencing, Unsupervised multi-class ensemble classifier, GATK},
abstract = {Unsupervised ensemble learning refers to methods devised for a particular task that combine data provided by decision learners taking into account their reliability, which is usually inferred from the data. Here, the variant calling step of the next generation sequencing technologies is formulated as an unsupervised ensemble classification problem. A variant calling algorithm based on the expectation-maximization algorithm is further proposed that estimates the maximum-a-posteriori decision among a number of classes larger than the number of different labels provided by the learners. Experimental results with real human DNA sequencing data show that the proposed algorithm is competitive compared to state-of-the-art variant callers as GATK, HTSLIB, and Platypus.}
}
@article{QV2022108745,
title = {Clustering by centroid drift and boundary shrinkage},
journal = {Pattern Recognition},
volume = {129},
pages = {108745},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108745},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002266},
author = {Hui Qv and Tao Ma and Xinyi Tong and Xuhui Huang and Zhe Ma and Jiehong Feng},
keywords = {Clustering, Centroid drift, Boundary detection},
abstract = {Locating the centers before assigning clustering labels is a traditional routine of clustering methods, which also limits the development of new clustering ideas. In this paper, we achieve the clustering task by firstly identifying the boundary points in the feature space, and then we shrink the boundary points to allocate the un-clustered points. Concretely, we propose a Centroid Drift (CD) metric and a Boundary Shrinkage (BS) strategy to detect boundary points in the feature space and allocate labels for un-clustered points, respectively. Both the CD and BS are closely related to the pre-computed k-nearest neighbor matrix, contributing to the decrease of algorithm parameters. Moreover, the common problems of noise points and non-uniform density distribution of data points in clustering task can also be alleviated with our proposed large value suppression and normalization of k-nearest neighbor distance techniques. The experiments on synthetic datasets, real-world face image datasets and hyperspectral images demonstrate the superiorities of our proposed clustering framework.}
}
@article{WANG2022108729,
title = {Versatile, full‐spectrum, and swift network sampling for model generation},
journal = {Pattern Recognition},
volume = {129},
pages = {108729},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108729},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002102},
author = {Huanyu Wang and Yongshun Zhang and Jianxin Wu},
keywords = {Model generation, Convolutional neural networks, Structured pruning, Model compression},
abstract = {Given one task, it is difficult to generate CNN models for many different hardware platforms with extremely diverse computing power for this task. Repeating network pruning or architecture search for each platform is very time-consuming. In this paper, we propose properties that are required for this model generation problem: versatile (fits diverse applications and network structures), full-spectrum (generates models for devices with tiny to gigantic computing power), and swift (total training time for all platforms is short, and generated models have low latency). We show that existing methods do not satisfy these requirements and propose a VFS method (the V/F/S represents Versatile/Full-spectrum/Swift, respectively). VFS uses importance sampling to sample many submodels with versatile structures and with different input image resolutions. We propose new fine-tuning strategies that only need to fine-tune a best candidate submodel for few epochs for each platform. VFS satisfies all three requirements. It generates versatile models with low latency for diverse applications, is suitable for devices with a wide range of computing power differences, and the models which are generated by VFS achieve state-of-the-art accuracy.}
}
@article{YAO2022108708,
title = {A sparse graph wavelet convolution neural network for video-based person re-identification},
journal = {Pattern Recognition},
volume = {129},
pages = {108708},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108708},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001893},
author = {Yingmao Yao and Xiaoyan Jiang and Hamido Fujita and Zhijun Fang},
keywords = {Video-based person re-identification, Weighted sparse graph, Graph wavelet convolution neural network},
abstract = {Video-based person re-identification (Re-ID) aims to match identical person sequences captured across non-overlapping surveillance areas. It is an essential yet challenging task to effectively embed spatial and temporal information into the video feature representation. For one thing, we observe that different frames in the video can provide complementary information for each other. Also, local features which is lost due to target occlusion or visual ambiguity in one frame can be supplemented by the same pedestrian part in other frames. For another thing, graph neural network enables the contextual interactions between relevant regional features. Therefore, we propose a novel sparse graph wavelet convolution neural network (SGWCNN) for video-based person Re-ID. Distinct from previous graph-based Re-ID methods, we exploit the weighted sparse graph to model the semantic relation among the local patches of pedestrians in the video. Each local patch in one frame can extract supplementary information from highly related patches in other frames. Moreover, to effectively solve the problems of short time occlusion and pedestrian misalignment, the graph wavelet convolution neural network is adopted for feature propagation to refine regional features iteratively. Experiments and evaluation on three challenging benchmarks, that is, MARS, DukeMTMC-VideoReID, and iLIDS-VID, show that the proposed SGWCNN effectively improves the performance of video-based person re-identification.}
}
@article{CHEN2022108753,
title = {Orthogonal channel attention-based multi-task learning for multi-view facial expression recognition},
journal = {Pattern Recognition},
volume = {129},
pages = {108753},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108753},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002345},
author = {Jingying Chen and Lei Yang and Lei Tan and Ruyi Xu},
keywords = {Multi-view facial expression recognition, Orthogonal channel attention, Multi-task learning, Siamese convolutional neural network, Separated channel attention module},
abstract = {Multi-view facial expression recognition (FER) is a challenging computer vision task due to the large intra-class difference caused by viewpoint variations. This paper presents a novel orthogonal channel attention-based multi-task learning (OCA-MTL) approach for FER. The proposed OCA-MTL approach adopts a Siamese convolutional neural network (CNN) to force the multi-view expression recognition model to learn the same features as the frontal expression recognition model. To further enhance the recognition accuracy of non-frontal expression, the multi-view expression model adopts a multi-task learning framework that regards head pose estimation (HPE) as an auxiliary task. A separated channel attention (SCA) module is embedded in the multi-task learning framework to generate individual attention for FER and HPE. Furthermore, orthogonal channel attention loss is presented to force the model to employ different feature channels to represent the facial expression and head pose, thereby decoupling them. The proposed approach is performed on two public facial expression datasets to evaluate its effectiveness and achieves an average recognition accuracy rate of 88.41% under 13 viewpoints on Multi-PIE and 89.04% under 5 viewpoints on KDEF, outperforming state-of-the-art methods.}
}
@article{GU2022108716,
title = {Example-based color transfer with Gaussian mixture modeling},
journal = {Pattern Recognition},
volume = {129},
pages = {108716},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108716},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001972},
author = {Chunzhi Gu and Xuequan Lu and Chao Zhang},
keywords = {Color transfer, Gaussian mixture model, EM optimization},
abstract = {Color transfer, which plays a key role in image editing, has attracted noticeable attention recently. It has remained a challenge to date due to various issues such as time-consuming manual adjustments and prior segmentation issues. In this paper, we propose to model color transfer under a probability framework and cast it as a parameter estimation problem. In particular, we relate the transferred image with the example image under the Gaussian Mixture Model (GMM) and regard the transferred image color as the GMM centroids. We employ the Expectation-Maximization (EM) algorithm (E-step and M-step) for optimization. To better preserve gradient information, we introduce a Laplacian based regularization term to the objective function at the M-step which is solved by deriving a gradient descent algorithm. Given the input of a source image and an example image, our method is able to generate multiple color transfer results with increasing EM iterations. Extensive experiments show that our approach generally outperforms other competitive color transfer methods, both visually and quantitatively.}
}
@article{CHANDALIYA2022108761,
title = {ChildGAN: Face aging and rejuvenation to find missing children},
journal = {Pattern Recognition},
volume = {129},
pages = {108761},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108761},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002424},
author = {Praveen Kumar Chandaliya and Neeta Nain},
keywords = {Child face aging and rejuvenation, Child datasets, Face recognition, Age estimation, Gender preservation, Child trafficking},
abstract = {Child-face aging and rejuvenation have amassed considerable active research interest, owing to their immense impact on a broad range of social and security applications, e.g., digital entertainment, fashion and wellness, and searching for long-lost children using childhood photos. All current face aging approaches based on generative adversarial networks (GANs) focus on adult images or long-term aging. We present a new large-scale longitudinal Indian child (ICD) benchmark dataset to facilitate face age progression and regression, cross-age face recognition, age estimation, gender prediction, and kinship face recognition to alleviate these issues. Furthermore, we propose an automatic child-face age progression and regression model, namely, ChildGAN, that generates visually realistic images for enhanced face-identification accuracy while preserving the identity. Consequently, we have trained state-of-the-art (SOTA) face aging models on ICD for comprehensive qualitative and quantitative evaluations. We also present a multi-racial experiments dataset named Multi-Racial Child Dataset (MRCD) containing 64,965 child face images. The images are selected from publicly available datasets and web crawling. Finally, we investigate the generalization of ChildGAN by experimenting with White, Black, Asian, and Indian races. The experimental results suggest that the proposed ChildGAN and SOTA models can aid in reconnecting young children, who were lost at a young age as victims of child trafficking or abduction, with their families. The model and the MRCD web crawled images are available at https://github.com/praveenkumarchandaliya/ChildGAN_Tamp1/.}
}
@article{PARK2022108764,
title = {Maximization and restoration: Action segmentation through dilation passing and temporal reconstruction},
journal = {Pattern Recognition},
volume = {129},
pages = {108764},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108764},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200245X},
author = {Junyong Park and Daekyum Kim and Sejoon Huh and Sungho Jo},
keywords = {Action segmentation, Temporal segmentation, Video understanding},
abstract = {Action segmentation aims to split videos into segments of different actions. Recent work focuses on dealing with long-range dependencies of long, untrimmed videos, but still suffers from over-segmentation and performance saturation due to increased model complexity. This paper addresses the aforementioned issues through a divide-and-conquer strategy that first maximizes the frame-wise classification accuracy of the model and then reduces the over-segmentation errors. This strategy is implemented with the Dilation Passing and Reconstruction Network, composed of the Dilation Passing Network, which primarily aims to increase accuracy by propagating information of different dilations, and the Temporal Reconstruction Network, which reduces over-segmentation errors by temporally encoding and decoding the output features from the Dilation Passing Network. We also propose a weighted temporal mean squared error loss that further reduces over-segmentation. Through evaluations on the 50Salads, GTEA, and Breakfast datasets, we show that our model achieves significant results compared to existing state-of-the-art models.}
}
@article{LI2022108785,
title = {HAM: Hybrid attention module in deep convolutional neural networks for image classification},
journal = {Pattern Recognition},
volume = {129},
pages = {108785},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108785},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002667},
author = {Guoqiang Li and Qi Fang and Linlin Zha and Xin Gao and Nenggan Zheng},
keywords = {Hybrid attention module, Channel attention map, Spatial feature descriptor, HAM-integrated networks},
abstract = {Recently, many researches have demonstrated that the attention mechanism has great potential in improving the performance of deep convolutional neural networks (CNNs). However, the existing methods either ignore the importance of using channel attention and spatial attention mechanisms simultaneously or bring much additional model complexity. In order to achieve a balance between performance and model complexity, we propose the Hybrid Attention Module (HAM), a really lightweight yet efficient attention module. Given an intermediate feature map as the input feature, HAM firstly produces one channel attention map and one channel refined feature through the channel submodule, and then based on the channel attention map, the spatial submodule divides the channel refined feature into two groups along the channel axis to generate a pair of spatial attention descriptors. By applying saptial attention descriptors, the spatial submodule generates the final refined feature which can adaptively emphasize the important regions. Besides, HAM is a simple and general module, it can be embedded into various mainstream deep CNN architectures seamlessly and can be trained with base CNNs in the end-to-end way. We evaluate HAM through abundant of experiments on CIFAR-10, CIFAR-100 and STL-10 datasets. The experimental results show that HAM-integrated networks achieve accuracy improvements and further reduce the negative impact of less training data on deeper networks performance than its counterparts, which proves the effectiveness of HAM.}
}
@article{CHEN2022108769,
title = {Symmetric Binary Tree Based Co-occurrence Texture Pattern Mining for Fine-grained Plant Leaf Image Retrieval},
journal = {Pattern Recognition},
volume = {129},
pages = {108769},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108769},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002503},
author = {Xin Chen and Bin Wang and Yongsheng Gao},
keywords = {Leaf image pattern, Species recognition, Fine-grained image recognition, Feature fusion, Image retrieval},
abstract = {Leaf image patterns have been actively researched for plant species recognition. However, as a very challenging fine-grained pattern identification issue, cultivar recognition in which the leaf image patterns usually have very subtle difference among cultivars has not yet received considerable attention in computer vision and pattern recognition community. In this paper, a novel symmetric geometric configuration, named Symmetric Binary Tree (SBT) which has multiple symmetric branch pairs and can change in size, is designed to mine the multiple scale co-occurrence texture patterns. The resulting SBT descriptors encode both shape and texture features which make them more informative than the existing individual descriptors and co-occurrence features. A novel feature fusion scheme, named K-NN Based Handcrafted and Deep Features Fusion (KNN-HDFF) that encodes the neighbouring information of distance measure, is proposed for further boosting the retrieval performance. Extensive experiments conducted on the challenging soybean cultivar leaf image dataset and peanut cultivar leaf image dataset consistently indicate the superiority of the proposed method over the state-of-the-art methods on fine-grained leaf image retrieval. We also conduct extensive experiments of feature fusions using the proposed KNN-HDFF on the benchmark datasets and the experimental results prove its potential for improving the performance of cultivar identification which also indicates that fusing handcrafted and deep features may be the direction to address the challenging fine-grained image recognition problem.}
}
@article{ZHAO2022108741,
title = {Progressive privileged knowledge distillation for online action detection},
journal = {Pattern Recognition},
volume = {129},
pages = {108741},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108741},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002229},
author = {Peisen Zhao and Lingxi Xie and Jiajie Wang and Ya Zhang and Qi Tian},
keywords = {Online action detection, Knowledge distillation, Privileged information, Curriculum learning},
abstract = {Online Action Detection (OAD) in videos addresses the problem of real-time analysis for streaming videos, i.e., only the observed historical video frames are available at prediction time. Considering the future frames observable only at the training stage as a form of privileged information, this paper adopts the Learning Using Privileged Information (LUPI) paradigm. Knowledge distillation (KD) is employed to transfer the privileged information from the offline teacher to the online student. Note that this setting is different from conventional KD because the difference between the teacher and student models mostly lies in the input data rather than the network architecture. To relieves the input information gap for the LUPI, we propose a simple but effective Privileged Knowledge Distillation (PKD) method that enforce KD loss to partial hidden features of the student model. Moreover, we also schedules a curriculum learning procedure to gradually distill the privileged information. This approach is named as Progressive Privileged Knowledge Distillation (PPKD). Compared to some OAD methods that explicitly predict future frames or feature, our approach avoids predicting stage and achieves state-of-the-art accuracy on two popular OAD benchmarks, TVSeries and THUMOS14.}
}
@article{YUAN2022108704,
title = {A novel forget-update module for few-shot domain generalization},
journal = {Pattern Recognition},
volume = {129},
pages = {108704},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108704},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001856},
author = {Minglei Yuan and Chunhao Cai and Tong Lu and Yirui Wu and Qian Xu and Shijie Zhou},
keywords = {Few-shot classification, Domain adaptation, Few-shot domain generalization},
abstract = {Existing Few-Shot Learning (FSL) methods learn and recognize new classes with the help of prior knowledge. However, they cannot handle this task well in a cross-domain scenario when training and testing sets are from different domains, since the fact that prior knowledge in different domains often varies greatly. To solve this problem, in this paper, we propose a few-shot domain generalization method, which is designed to extract relationship embeddings using Forget-Update Modules named FUM. The relationship embedding considers valuable relational information between samples in a specific task, and the forget-update module takes into account differences between domains and adjusts the distribution of relational embeddings through forgetting and updating mechanisms based on specific tasks. To evaluate the few-shot domain generalization ability of FUM, extensive experiments on eight cross-domain scenarios and six same-domain scenarios are conducted, and the results show that FUM achieves superior performances compared to recent few-shot learning methods. Visualization results also show that the distribution of the relationship embeddings extracted by FUM has stronger few-shot domain generalization ability than the feature embeddings used in the existing FSL methods.}
}
@article{KORYCKI2022108749,
title = {Instance exploitation for learning temporary concepts from sparsely labeled drifting data streams},
journal = {Pattern Recognition},
volume = {129},
pages = {108749},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108749},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002308},
author = {Łukasz Korycki and Bartosz Krawczyk},
keywords = {Machine learning, Data stream mining, Concept drift, Sparse labeling, Active learning},
abstract = {Continual learning from streaming data sources becomes more and more popular due to the increasing number of online tools and systems. Dealing with dynamic and everlasting problems poses new challenges for which traditional batch-based offline algorithms turn out to be insufficient in terms of computational time and predictive performance. One of the most crucial limitations is that we cannot assume having an access to a finite and complete data set – we always have to be ready for new data that may complement our model. This poses a critical problem of providing labels for potentially unbounded streams. In real world, we are forced to deal with very strict budget limitations, therefore, we will most likely face the scarcity of annotated instances, which are essential in supervised learning. In our work, we emphasize this problem and propose a novel instance exploitation technique. We show that when: (i) data is characterized by temporary non-stationary concepts, and (ii) there are very few labels spanned across a long time horizon, it is actually better to risk overfitting and adapt models more aggressively by exploiting the only labeled instances we have, instead of sticking to a standard learning mode and suffering from severe underfitting. We present different strategies and configurations for our methods, as well as an ensemble algorithm that attempts to maintain a sweet spot between risky and normal adaptation. Finally, we conduct a complex in-depth comparative analysis of our methods, using state-of-the-art streaming algorithms relevant for the given problem.}
}
@article{OZER2022108712,
title = {SiameseFuse: A computationally efficient and a not-so-deep network to fuse visible and infrared images},
journal = {Pattern Recognition},
volume = {129},
pages = {108712},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108712},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001935},
author = {Sedat Özer and Mert Ege and Mehmet Akif Özkanoglu},
keywords = {Multi-temporal fusion, Efficient learning, Multi-modal fusion},
abstract = {Recent developments in pattern analysis have motivated many researchers to focus on developing deep learning based solutions in various image processing applications. Fusing multi-modal images has been one such application area where the interest is combining different information coming from different modalities in a more visually meaningful and informative way. For that purpose, it is important to first extract salient features from each modality and then fuse them as efficiently and informatively as possible. Recent literature on fusing multi-modal images reports multiple deep solutions that combine both visible (RGB) and infra-red (IR) images. In this paper, we study the performance of various deep solutions available in the literature while seeking an answer to the question: “Do we really need deeper networks to fuse multi-modal images?” To have an answer for that question, we introduce a novel architecture based on Siamese networks to fuse RGB (visible) images with infrared (IR) images and report the state-of-the-art results. We present an extensive analysis on increasing the layer numbers in the architecture with the above-mentioned question in mind to see if using deeper networks (or adding additional layers) adds significant performance in our proposed solution. We report the state-of-the-art results on visually fusing given visible and IR image pairs in multiple performance metrics, while requiring the least number of trainable parameters. Our experimental results suggest that shallow networks (as in our proposed solutions in this paper) can fuse both visible and IR images as well as the deep networks that were previously proposed in the literature (we were able to reduce the total number of trainable parameters up to 96.5%, compare 2,625 trainable parameters to the 74,193 trainable parameters).}
}
@article{2022108801,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {129},
pages = {108801},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(22)00282-5},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002825}
}