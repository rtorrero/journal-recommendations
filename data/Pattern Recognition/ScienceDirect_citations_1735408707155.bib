@article{CHEN2023109307,
title = {A local tangent plane distance-based approach to 3D point cloud segmentation via clustering},
journal = {Pattern Recognition},
volume = {137},
pages = {109307},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109307},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000080},
author = {Hui Chen and Tingting Xie and Man Liang and Wanquan Liu and Peter Xiaoping Liu},
keywords = {3D point cloud, Plane segmentation, Tangent distance, Adaptive clustering},
abstract = {This paper proposes an effective measure for the planar segmentation problem based on the clustering method. It uses the distance from a point to the local plane as a metric to characterize the relationship between data. As a result, the data points of the coplanar have a high similarity to distinguish each plane. A dissimilarity matrix of the input point cloud can be evaluated, and multidimensional scaling analysis is performed to reconstruct the correlation information between data points in the 3D Euclidean space. The obtained reconstructed point cloud shows the separation between different planes. An adaptive DBSCAN clustering method based on density stratification is developed to perform cluster segmentation on the reconstructed point cloud. Experimental results show that the proposed method can effectively solve the over-segmentation problem, and at the same time provide high segmentation accuracy.}
}
@article{ZHANG2023109279,
title = {Weakly supervised foreground learning for weakly supervised localization and detection},
journal = {Pattern Recognition},
volume = {137},
pages = {109279},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109279},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007580},
author = {Chen-Lin Zhang and Yin Li and Jianxin Wu},
keywords = {Weakly supervised object localization, Weakly supervised object detection, Foreground learning},
abstract = {Modern deep learning models require large amounts of accurately annotated data, which is often difficult to satisfy. Hence, weakly supervised tasks, including weakly supervised object localization (WSOL) and detection (WSOD), have recently received attention in the computer vision community. In this paper, we motivate and propose the weakly supervised foreground learning (WSFL) task by showing that both WSOL and WSOD can be greatly improved if groundtruth foreground masks are available. More importantly, we propose a complete WSFL pipeline with low computational cost, which generates pseudo boxes, learns foreground masks, and does not need any localization annotations. With the help of foreground masks predicted by our WSFL model, we achieve 74.37% correct localization accuracy on CUB for WSOL, and 55.7% mean average precision on VOC07 for WSOD, thereby establish new state-of-the-art for both tasks. Our WSFL model also shows excellent transfer ability.}
}
@article{CHENG2023109270,
title = {Hybrid routing transformer for zero-shot learning},
journal = {Pattern Recognition},
volume = {137},
pages = {109270},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109270},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200749X},
author = {De Cheng and Gerong Wang and Bo Wang and Qiang Zhang and Jungong Han and Dingwen Zhang},
keywords = {Zero-shot learning, Hybrid routing, Transformer, Attention},
abstract = {Zero-shot learning (ZSL) aims to learn models that can recognize unseen image semantics based on the training of data with seen semantics. Recent studies either leverage the global image features or mine discriminative local patch features to associate the extracted visual features to the semantic attributes. However, due to the lack of the necessary top-down guidance and semantic alignment for ensuring the model attend to the real attribute-correlation regions, these methods still encounter a significant semantic gap between the visual modality and the attribute modality, which makes their prediction on unseen semantics unreliable. To solve this problem, this paper establishes a novel transformer encoder-decoder model, called hybrid routing transformer (HRT). In HRT encoder, we embed an active attention, which is constructed by both the bottom-up and the top-down dynamic routing pathways to generate the attribute-aligned visual feature. While in HRT decoder, we use static routing to calculate the correlation among the attribute-aligned visual features, the corresponding attribute semantics, and the class attribute vectors to generate the final class label predictions. This design makes the presented transformer model a hybrid of 1) top-down and bottom-up attention pathways and 2) dynamic and static routing pathways. Comprehensive experiments on three widely-used benchmark datasets, namely CUB, SUN, and AWA2, are conducted. The obtained experimental results demonstrate the effectiveness of the proposed method. Our code is released in https://github.com/KORIYN/HRT.}
}
@article{DENG2023109251,
title = {RGB-D salient object ranking based on depth stack and truth stack for complex indoor scenes},
journal = {Pattern Recognition},
volume = {137},
pages = {109251},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109251},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007300},
author = {Jingzheng Deng and Jinxia Zhang and Zewen Hu and Liantao Wang and Jiacheng Jiang and Xinchao Zhu and Xinyi Chen and Yin Yuan and Chao Wang},
keywords = {Complex scenes, RGB-D, Salient object ranking, Indoor, Depth},
abstract = {RGB-D salient object detection has achieved a great development in recent years due to its extensive applications. Previous studies mainly focus on simple scene images with one single object. These models usually become overwhelmed by complex scenes with multiple objects. Moreover, these methods model salient object detection as a binary segmentation problem. However, psychology studies show that humans shift their visual attention from one object to another and rank salient objects, especially in complex indoor scenes. Following the psychological studies, we propose to rank salient objects in RGB-D images of complex indoor scenes. Due to the lack of such data, we first construct a RGB-D salient object ranking dataset containing complex indoor scenes with multiple objects. The saliency ranking of different objects is defined based on the order that an observer notices these objects. The final salient object ranking result is an average across the saliency rankings of 13 observers. This RGB-D salient object ranking dataset is also analyzed with current mainstream RGB-D salient object detection dataset for comparison. Since location information provided by depth images can help to determine the saliency ranking of objects, we further propose an end-to-end network exploiting depth stack and ground truth stack to predict the order of salient objects in complex scenes. The quantitative and qualitative comparisons demonstrate the effectiveness of the proposed method.}
}
@article{WANG2023109243,
title = {Diverse image inpainting with disentangled uncertainty},
journal = {Pattern Recognition},
volume = {137},
pages = {109243},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109243},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007221},
author = {Wentao Wang and Lu He and Li Niu and Jianfu Zhang and Yue Liu and Haoyu Ling and Liqing Zhang},
keywords = {Image inpainting, Diverse image inpainting, Disentangled representation},
abstract = {Most existing inpainting methods repair a corrupted image to a single output, which gives people no choice to select the most satisfactory result. However, image inpainting is essentially a multi-modal problem because the inpainted results could have multiple possibilities. To generate both diverse and realistic inpainted results, we propose a diverse image inpainting framework with disentangled uncertainty. We disentangle the uncertainty of the missing region into two aspects: structure and appearance. Correspondingly, we divide the process of diverse image inpainting into two stages: diverse structure inpainting and diverse appearance inpainting. In the first stage, we restore the structure of the missing region, producing diverse complete edge maps. In the second stage, using a complete edge map as the guidance, we fill in diverse appearance information of the missing region. We also design a light-weighted disentangling subnetwork to disentangle structure information and appearance information. Besides, we propose a novel style-based masked residual block to better deal with the uncertainty. Experiments on CelebA-HQ, Paris Street View, and Places2 demonstrate that our method can repair the corrupted image with higher fidelity and diversity than other existing methods.}
}
@article{WANG2023109319,
title = {Reducing bi-level feature redundancy for unsupervised domain adaptation},
journal = {Pattern Recognition},
volume = {137},
pages = {109319},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109319},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000201},
author = {Mengzhu Wang and Shanshan Wang and Wei Wang and Li Shen and Xiang Zhang and Long Lan and Zhigang Luo},
keywords = {Feature redundancy, Unsupervised domain adaptation, Whitening, Orthogonality},
abstract = {Unsupervised domain adaptation (UDA) deals with the problem of transferring knowledge from a labeled source domain to an unlabeled target domain when the two domains have distinct data distributions. Therefore, the purpose of domain adaptation is to mitigate the distribution divergence between the two domains. Many existing UDA methods only use the traditional batch normalization layer, but this may lead to a large number of feature redundancy and lead performance degradation. In this paper, we introduce a novel deep learning paradigm called feature redundancy in UDA to enhance adaptation ability. Specifically, we first show that feature redundancy also exists on unsupervised domain adaptation (UDA), which has been ignored by most previous efforts. We utilize feature similarity as a metric to measure feature redundancy and then analyze the relationship between uniform feature spectrum and minimal feature similarity. Based on this relationship, we intend to reduce cross-domain feature redundancy for UDA by making the distribution of feature spectrum uniforms in a bi-level way. For the first level, we propose a cross-domain batch normalization with the whitening module (xBN) to ensure compact domain-specific features and learn domain-invariant features at the same time. With the domain-specific features from the first level that paves a way, on the second level, we suggest an alternative orthogonal regularizer (OR) that can make the distribution of the feature spectrum more uniform, thus domain-invariant feature redundancy is mitigated. Such a bi-level mechanism greatly reduces the feature redundancy for UDA. To evaluate the efficacy of the proposed bi-level mechanism, we plug those two novel modules (i.e., xBN and OR) into convolutional neural networks (CNNs) to form our UDA model and also conduct the corresponding empirical evaluations on five cross-domain object recognition benchmarks including both classical and large-scale image datasets. Experimental results show that the proposed UDA model could achieve state-of-the-art performance both in quantity and quality. Our source codes will be released after publication.}
}
@article{LIN2023109283,
title = {Rectified Euler k-means and beyond},
journal = {Pattern Recognition},
volume = {137},
pages = {109283},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109283},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007622},
author = {Yunxia Lin and Songcan Chen},
keywords = {Kernel -means, Euler kernel, Pseudo centroid, Rectified euler -means},
abstract = {Euler k-means (EulerK) first maps data onto the unit hyper-sphere surface of equi-dimensional space via a complex mapping which induces the robust Euler kernel and next employs the popular k-means. Consequently, besides enjoying the virtues of k-means such as simplicity and scalability to large data sets, EulerK is also robust to noises and outliers. Although so, the centroids captured by EulerK deviate from the unit hyper-sphere surface and thus in strict distributional sense, actually are outliers. This weird phenomenon also occurs in some generic kernel clustering methods. Intuitively, using such outlier-like centroids should not be quite reasonable but it is still seldom attended. To eliminate the deviation, we propose two Rectified Euler k-means methods, i.e., REK1 and REK2, which retain the merits of EulerK while acquiring real centroids residing on the mapped space to better characterize the data structures. Specifically, REK1 rectifies EulerK by imposing the constraint on the centroids while REK2 views each centroid as the mapped image from a pre-image in the original space and optimizes these pre-images in Euler kernel induced space. Undoubtedly, our proposed REKs can methodologically be extended to solve problems of such a category. Finally, the experiments validate the effectiveness of REK1 and REK2.}
}
@article{HU2023109299,
title = {Multi-modal unsupervised domain adaptation for semantic image segmentation},
journal = {Pattern Recognition},
volume = {137},
pages = {109299},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109299},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007786},
author = {Sijie Hu and Fabien Bonardi and Samia Bouchafa and Désiré Sidibé},
keywords = {Unsupervised domain adaptation, Multi-modal learning, Self-supervised learning, Knowledge transfer, Semantic segmentation},
abstract = {We propose a novel multi-modal-based Unsupervised Domain Adaptation (UDA) method for semantic segmentation. Recently, depth has proven to be a relevent property for providing geometric cues to enhance the RGB representation. However, existing UDA methods solely process RGB images or additionally cultivate depth-awareness with an auxiliary depth estimation task. We argue that geometric cues that are crucial to semantic segmentation, such as local shape and relative position, are challenging to recover from an auxiliary depth estimation task with mere color (RGB) information. In this paper, we propose a novel multi-modal UDA method named MMADT, which relies on both RGB and depth images as input. In particular, we design a Depth Fusion Block (DFB) to recalibrate depth information and leverage Depth Adversarial Training (DAT) to bridge the depth discrepancy between the source and target domain. Besides, we propose a self-supervised multi-modal depth estimation assistant network named Geo-Assistant (GA) to align the feature space of RGB and depth and shape the sensitivity of our MMADT to depth information. We experimentally observed significant performance improvement in multiple synthetic to real adaptation benchmarks, i.e., SYNTHIA-to-Cityscapes, GTA5-to-Cityscapes and SELMA-to-Cityscapes. Additionally, our multi-modal UDA scheme is easy to port to other UDA methods with a consistent performance boost.}
}
@article{LAI2023109311,
title = {Efficient sampling using feature matching and variable minimal structure size},
journal = {Pattern Recognition},
volume = {137},
pages = {109311},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109311},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000122},
author = {Taotao Lai and Alireza Sadri and Shuyuan Lin and Zuoyong Li and Riqing Chen and Hanzi Wang},
keywords = {Model fitting, Guided sampling, Feature matching, Multiple structure data},
abstract = {Greedy search-based guided sampling is a promising research field in model fitting to data with multiple structures in the presence of a large number of outliers. However, these greedy search-based guided sampling algorithms are sensitive to the fixed minimal (acceptable) structure size and the initial model hypothesis: when the fixed minimal structure size is too small, data subsets sampled by these algorithms are not representative. In contrast, when it is too large, data subsets might be contaminated by outliers. Furthermore, these algorithms may fail to obtain an accurate model hypothesis, if the initial model hypothesis is far from the true model. In this paper, we address the above-mentioned two issues by proposing two greedy search-based strategies: one aims to adaptively estimate minimal structure sizes and the other aims to generate effective initial model hypotheses. Specifically, on one hand, to avoid using the fixed minimal structure size, a strategy is proposed to adaptively estimate minimal structure sizes by using previously obtained ones. On the other hand, to reduce the impact of outliers, a strategy is proposed to filter out outliers to obtain a reduced data subset by using a feature matching algorithm. Then, this strategy generates promising initial model hypotheses by using a proximity sampling on the reduced data subset. Finally, an efficient sampling algorithm based on the two proposed greedy search-based strategies is applied to three vision tasks, i.e., fundamental matrix estimation, homography plane detection and 3D motion segmentation. Extensive experimental results demonstrate the effectiveness of the proposed sampling algorithm.}
}
@article{YANG2023109275,
title = {RESKM: A General Framework to Accelerate Large-Scale Spectral Clustering},
journal = {Pattern Recognition},
volume = {137},
pages = {109275},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109275},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007543},
author = {Geping Yang and Sucheng Deng and Xiang Chen and Can Chen and Yiyang Yang and Zhiguo Gong and Zhifeng Hao},
keywords = {Machine learning, Spectral clustering, Unsupervised learning, Large-scale},
abstract = {Spectral Clustering is an effective preprocessing method in communities for its excellent performance, but its scalability still is a challenge. Many efforts have been made to face this problem, and several solutions are proposed, including Nyström Approximation, Sparse Representation Approximation, etc. However, according to our survey, there is still a large room for improvement. This work thoroughly investigates the factors relevant to large-scale Spectral Clustering and proposes a general framework to accelerate Spectral Clustering by utilizing the Robust and Efficient Spectral k-Means (RESKM). The contributions of RESKM are three folds: (1) a unified framework is proposed for large-scale Spectral Clustering; (2) it consists of four phases, each phase is theoretically analyzed, and the corresponding acceleration is suggested; (3) the majority of the existing large-scale Spectral Clustering methods can be integrated into RESKM and therefore be accelerated. Experiments on datasets with different scalability demonstrate that the robustness and efficiency of RESKM.}
}
@article{CHEN2023109271,
title = {Riemannian representation learning for multi-source domain adaptation},
journal = {Pattern Recognition},
volume = {137},
pages = {109271},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109271},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007506},
author = {Sentao Chen and Lin Zheng and Hanrui Wu},
keywords = {Convex optimization, Hellinger distance, Multi-source domain adaptation, Representation learning, Riemannian manifold},
abstract = {Multi-Source Domain Adaptation (MSDA) aims at training a classification model that achieves small target error, by leveraging labeled data from multiple source domains and unlabeled data from a target domain. The source and target domains are described by related but different joint distributions, which lie on a Riemannian manifold named the statistical manifold. In this paper, we characterize the joint distribution difference by the Hellinger distance, which bears strong connection to the Riemannian metric defined on the statistical manifold. We show that the target error of a neural network classification model is upper bounded by the average source error of the model and the average Hellinger distance, i.e., the average of multiple Hellinger distances between the source and target joint distributions in the network representation space. Motivated by the error bound, we introduce Riemannian Representation Learning (RRL): An approach that trains the network model by minimizing (i) the average empirical Hellinger distance with respect to the representation function, and (ii) the average empirical source error with respect to the network model. Specifically, we derive the average empirical Hellinger distance by constructing and solving unconstrained convex optimization problems whose global optimal solutions are easy to find. With the network model trained, we expect it to achieve small error in the target domain. Our experimental results on several image datasets demonstrate that the proposed RRL approach is statistically better than the comparison methods.}
}
@article{LAZARO2023109303,
title = {Neural network for ordinal classification of imbalanced data by minimizing a Bayesian cost},
journal = {Pattern Recognition},
volume = {137},
pages = {109303},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109303},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000043},
author = {Marcelino Lázaro and Aníbal R. Figueiras-Vidal},
keywords = {Bayes cost, Parzen windows, Ordinal classification, Imbalanced},
abstract = {Ordinal classification of imbalanced data is a challenging problem that appears in many real world applications. The challenge is to simultaneously consider the order of the classes and the class imbalance, which can notably improve the performance metrics. The Bayesian formulation allows to deal with these two characteristics jointly: It takes into account the prior probability of each class and the decision costs, which can be used to include the imbalance and the ordinal information, respectively. We propose to use the Bayesian formulation to train neural networks, which have shown excellent results in many classification tasks. A loss function is proposed to train networks with a single neuron in the output layer and a threshold based decision rule. The loss is an estimate of the Bayesian classification cost, based on the Parzen windows estimator, which is fitted for a thresholded decision. Experiments with several real datasets show that the proposed method provides competitive results in different scenarios, due to its high flexibility to specify the relative importance of the errors in the classification of patterns of different classes, considering the order and independently of the probability of each class.}
}
@article{XU2023109347,
title = {A Comprehensive Survey of Image Augmentation Techniques for Deep Learning},
journal = {Pattern Recognition},
volume = {137},
pages = {109347},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109347},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000481},
author = {Mingle Xu and Sook Yoon and Alvaro Fuentes and Dong Sun Park},
keywords = {Image augmentation, Deep learning, Image variation, Vicinity distribution, Data augmentation, Computer vision},
abstract = {Although deep learning has achieved satisfactory performance in computer vision, a large volume of images is required. However, collecting images is often expensive and challenging. Many image augmentation algorithms have been proposed to alleviate this issue. Understanding existing algorithms is, therefore, essential for finding suitable and developing novel methods for a given task. In this study, we perform a comprehensive survey of image augmentation for deep learning using a novel informative taxonomy. To examine the basic objective of image augmentation, we introduce challenges in computer vision tasks and vicinity distribution. The algorithms are then classified among three categories: model-free, model-based, and optimizing policy-based. The model-free category employs the methods from image processing, whereas the model-based approach leverages image generation models to synthesize images. In contrast, the optimizing policy-based approach aims to find an optimal combination of operations. Based on this analysis, we believe that our survey enhances the understanding necessary for choosing suitable methods and designing novel algorithms.}
}
@article{MIN2023109291,
title = {Hybrid feature enhancement network for few-shot semantic segmentation},
journal = {Pattern Recognition},
volume = {137},
pages = {109291},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109291},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007701},
author = {Hai Min and Yemao Zhang and Yang Zhao and Wei Jia and Yingke Lei and Chunxiao Fan},
keywords = {Semantic segmentation, Few-shot segmentation, Few-shot learning},
abstract = {Although few-shot semantic segmentation methods have been widely studied in computer vision field, it still has room for improvement. In this work, we propose to enrich the feature representation with texture information and assign adaptive weights to losses. Specially, we incorporate the texture information obtained by texture enhance module with layer's features on ResNet, and then get a series of hybrid features. The incorporation of texture information enhances the similarity calculation to make the support set guidance more effective. Besides, the proposed adaptive loss makes the network optimize in a better direction. The experiments testify that the proposed method achieves the better results than that of previous methods on few-shot segmentation dataset such as PASCAL-5i, COCO-20i and FSS-1000.}
}
@article{TANG2023109295,
title = {TCCFusion: An infrared and visible image fusion method based on transformer and cross correlation},
journal = {Pattern Recognition},
volume = {137},
pages = {109295},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109295},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007749},
author = {Wei Tang and Fazhi He and Yu Liu},
keywords = {Image fusion, Transformer, Deep learning, Infrared image, Cross correlation},
abstract = {Infrared and visible image fusion aims to obtain a synthetic image that can simultaneously exhibit salient objects and provide abundant texture details. However, existing deep learning-based methods generally depend on convolutional operations, which indeed have good local feature extraction ability, but the restricted receptive field limits its capability in modeling long-range dependencies. To conquer this dilemma, we propose an infrared and visible image fusion method based on Transformer and cross correlation, named TCCFusion. Specifically, we design a local feature extraction branch (LFEB) to preserve local complementary information, in which a dense-shape network is introduced to reuse the information that may be lost during the convolutional operation. To avoid the limitation of the receptive field and to fully extract the global significant information, a global feature extraction branch (GFEB) is devised that consists of three Transformer blocks for long-range relationship construction. In addition, LFEB and GFEB are arranged in a parallel fashion to maintain local and global useful information in a more effective way. Furthermore, we design a cross correlation loss to train the proposed fusion model in an unsupervised manner, with which the fusion result can obtain adequate thermal radiation information in an infrared image and ample texture details in a visible image. Massive experiments on two mainstream datasets illustrate that our TCCFusion outperforms state-of-the-art algorithms not only on visual quality but also on quantitative assessments. Ablation experiments on the network framework and objective function demonstrate the effectiveness of the proposed method.}
}
@article{YU2023109343,
title = {Low-rank tensor recovery via non-convex regularization, structured factorization and spatio-temporal characteristics},
journal = {Pattern Recognition},
volume = {137},
pages = {109343},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109343},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000444},
author = {Quan Yu and Ming Yang},
keywords = {Tensor completion, Tensor robust principle component analysis, Low-rank approximation, Dynamic background, Spatio-temporal characteristics},
abstract = {Recently, the convex low-rank 3rd-order tensor recovery has attracted considerable attention. However, there are some limitations to the convex relaxation approach, which may yield biased estimators. To overcome this disadvantage, we develop a novel non-convex tensor pseudo-norm to replace the weighted sum of the tensor nuclear norm as a tighter rank approximation. Then in tensor robust principle component analysis, we introduce the noise analysis to separate the spare foreground from the dynamic background more accurately. Furthermore, by introducing a spatio-temporal matrix, we can make better use of the inherent spatio-temporal characteristics of the low-rank static background and sparse foreground. Finally, we introduce an incoherent term to constrain the sparse foreground and the dynamic background to improve the separability. Some preliminary numerical examples of color image, video, and face image data sets are presented to illustrate the efficiency of our proposed methods.}
}
@article{ZHU2023109258,
title = {A Dual Self-Attention mechanism for vehicle re-Identification},
journal = {Pattern Recognition},
volume = {137},
pages = {109258},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109258},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007373},
author = {Wenqian Zhu and Zhongyuan Wang and Xiaochen Wang and Ruimin Hu and Huikai Liu and Cheng Liu and Chao Wang and Dengshi Li},
keywords = {Cross-region attention, Dual self-attention, Multi-attention network, Vehicle re-identification, Feature embedding},
abstract = {Vehicle re-identification has attracted tremendous attention from computer vision communities for its extensive applications in intelligent transportation and public security, while the high inter-class similarity and the large intra-class difference between vehicles bring out great challenges for re-identification (re-ID). To tackle these challenges, we learn from the self-attention mechanism in Natural Language Processing and propose a dual self-attention module to learn different regional dependencies: static self-attention for selectively refining semantic features and dynamic self-attention (called cross-region attention) for enhancing the spatial awareness of local feature. The static self-attention refines attended pixels within the entire image and salient regions, while the cross-region attention creatively captures the position-related regional dependencies for pixels within the windscreen area. These attention modules capture long-range dependencies and relative position information between different pixels or regions for vehicle feature learning globally and locally, realizing an efficient vehicle feature embedding by concatenating these augmented features for vehicle re-ID. Extensive experiments demonstrate the effectiveness and promising performance of our approach against the state-of-the-arts.}
}
@article{PAN2023109316,
title = {Theoretical guarantee for crowdsourcing learning with unsure option},
journal = {Pattern Recognition},
volume = {137},
pages = {109316},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109316},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000171},
author = {Yigong Pan and Ke Tang and Guangzhong Sun},
keywords = {Machine learning, Crowdsourcing learning, Labeling},
abstract = {Crowdsourcing learning, in which labels are collected from multiple workers through crowdsourcing platforms, has attracted much attention during the past decade. This learning paradigm would reduce the labeling cost since crowdsourcing workers may be non-expert and hence less costly. On the other hand, crowdsourcing learning algorithms also suffer from being misled by incorrect labels introduced by imperfect workers. To control such risks, recently, it has been suggested to provide workers an additional unsure option during the labeling process. Although the benefits of the unsure option have been empirically demonstrated, theoretical analysis is still limited. In this article, a theoretical analysis of crowdsourcing learning with the unsure option is presented. Specifically, an upper bound of minimally sufficient number of crowd labels required for learning a probably approximately correct (PAC) classification model with and without the unsure option are given respectively. Next, a condition under which providing (or not providing) an unsure option to workers is derived. Then, the theoretical results are extended to guide non-identical label options (with or without unsure options) to different workers. Last, several useful applications are proposed based on theoretical results.}
}
@article{GUO2023109308,
title = {A comprehensive evaluation framework for deep model robustness},
journal = {Pattern Recognition},
volume = {137},
pages = {109308},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109308},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000092},
author = {Jun Guo and Wei Bao and Jiakai Wang and Yuqing Ma and Xinghai Gao and Gang Xiao and Aishan Liu and Jian Dong and Xianglong Liu and Wenjun Wu},
keywords = {Adversarial examples, Evaluation metrics, Model robustness},
abstract = {Deep neural networks (DNNs) have achieved remarkable performance across a wide range of applications, while they are vulnerable to adversarial examples, which motivates the evaluation and benchmark of model robustness. However, current evaluations usually use simple metrics to study the performance of defenses, which are far from understanding the limitation and weaknesses of these defense methods. Thus, most proposed defenses are quickly shown to be attacked successfully, which results in the “arm race” phenomenon between attack and defense. To mitigate this problem, we establish a model robustness evaluation framework containing 23 comprehensive and rigorous metrics, which consider two key perspectives of adversarial learning (i.e., data and model). Through neuron coverage and data imperceptibility, we use data-oriented metrics to measure the integrity of test examples; by delving into model structure and behavior, we exploit model-oriented metrics to further evaluate robustness in the adversarial setting. To fully demonstrate the effectiveness of our framework, we conduct large-scale experiments on multiple datasets including CIFAR-10, SVHN, and ImageNet using different models and defenses with our open-source platform. Overall, our paper provides a comprehensive evaluation framework, where researchers could conduct comprehensive and fast evaluations using the open-source toolkit, and the analytical results could inspire deeper understanding and further improvement to the model robustness.}
}
@article{YIN2023109274,
title = {Hypergraph based semi-supervised symmetric nonnegative matrix factorization for image clustering},
journal = {Pattern Recognition},
volume = {137},
pages = {109274},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109274},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007531},
author = {Jingxing Yin and Siyuan Peng and Zhijing Yang and Badong Chen and Zhiping Lin},
keywords = {Symmetric nonnegative matrix factorization, Hypergraph learning, Semi-supervised learning, Clustering},
abstract = {Semi-supervised symmetric nonnegative matrix factorization (SNMF) has been shown to be a significant method for both linear and nonlinear data clustering applications. Nevertheless, existing SNMF-based methods only adopt a simple graph to construct the similarity matrix, and cannot fully use the limited supervised information for the construction of the similarity matrix. To overcome the drawbacks of previous SNMF-based methods, a new semi-supervised SNMF-based method called hypergraph based semi-supervised SNMF (HSSNMF), is proposed in this paper for image clustering. Specifically, HSSNMF adopts a predefined hypergraph to build a similarity matrix for capturing the high-order relationships of samples. By exploiting a new hypergraph based pairwise constraints propagation (HPCP) algorithm, HSSNMF propagates the pairwise constraints of the limited data points to the entire data points, which can make full use of the limited supervised information and construct a more informative similarity matrix. Using the multiplicative updating algorithm, a discriminative assignment matrix can then be obtained by solving the optimization problem of HSSNMF. Moreover, analyses of the convergence, supervisory information, and computational complexity of HSSNMF are presented. Finally, extensive clustering experiments have been conducted on six real-world image datasets, and the experimental results have demonstrated the superiority of HSSNMF while compared with several state-of-the-art methods.}
}
@article{EJIOGU2023109290,
title = {Real time iris segmentation quality evaluation using medoids},
journal = {Pattern Recognition},
volume = {137},
pages = {109290},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109290},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007695},
author = {Ugochi U.C. Ejiogu and Ogechukwu N. Iloanusi},
keywords = {Iris segmentation-quality estimation, Biometric vision and computing iris dataset, K-medoids clustering, Iris datasets, Iris ground-truth mask, Eccentricity, Annulus radii ratio, Euclidian distance, Iris mask, medoids},
abstract = {Integrating an efficient and robust iris segmentation-quality estimation module in iris biometric systems will undoubtedly enhance its performance and competitive advantage. It proffers a real time detection of segmentation errors to forestall their propagation to the subsequent modules. Hence, we propose a novel automatic iris segmentation-quality estimation model using medoids. The performance of the proposed model was empirically evaluated with reference to three published models, using three benchmarked iris datasets and our novel proprietary iris dataset – Biometrics Vision and Computing Iris dataset. The proposed medoids based model was experimentally demonstrated to be effective, robust and relatively efficient in estimating iris segmentation-quality. Specifically, the proposed model recorded the best classification accuracy rate of 95.27% on one of the datasets. Also, it consistently recorded the least classification error rate across several iris datasets with diverse segmentation-errors, which suggest that the medoids based model is relatively more robust than the examined counterparts.}
}
@article{ZHOU2023109312,
title = {Communication-efficient and Byzantine-robust distributed learning with statistical guarantee},
journal = {Pattern Recognition},
volume = {137},
pages = {109312},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109312},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000134},
author = {Xingcai Zhou and Le Chang and Pengfei Xu and Shaogao Lv},
keywords = {Distributed learning, Byzantine failure, Communication efficiency, Surrogate likelihood, Proximal algorithm},
abstract = {Communication efficiency and robustness are two major issues in modern distributed learning frameworks. This is due to the practical situations where some computing nodes may have limited communication power or may behave adversarial behaviors. To address the two issues simultaneously, this paper develops two communication-efficient and robust distributed learning algorithms for convex problems. Our motivation is based on surrogate likelihood framework and the median and trimmed mean operations. Particularly, the proposed algorithms are provably robust against Byzantine failures, and also achieve optimal statistical rates for strong convex losses and convex (non-smooth) penalties. For typical statistical models such as generalized linear models, our results show that statistical errors dominate optimization errors in finite iterations. Simulated and real data experiments are conducted to demonstrate the numerical performance of our algorithms.}
}
@article{SONG2023109278,
title = {Object detection based on cortex hierarchical activation in border sensitive mechanism and classification-GIou joint representation},
journal = {Pattern Recognition},
volume = {137},
pages = {109278},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109278},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007579},
author = {Yaoye Song and Peng Zhang and Wei Huang and Yufei Zha and Tao You and Yanning Zhang},
keywords = {Border sensitive mechanism, Cortex hierarchical activation, Object detection, Classification-GIoU joint representation},
abstract = {By imitating the brain neurons for object perception, the deep networks enable a comprehensive feature characterization in the task of object detection. Considering such a perceptual ability is usually bounded in a box area for feature extraction, the balance of dimension reduction and feature information retaining has been taken into account in more recent studies, especially for the information preservation in the border areas. Motivated by the mechanism of neuron cortex activation, in this work, a novel function based on cortex hierarchical activation is proposed to achieve more effective border sensitive mechanism by joint pooling in backbone networks. In order to avoid the parameter solidification, this strategy is also capable to benefit the feature extraction on the border without unnecessary model re-training. Furthermore, by replacing the square kernel with a designed band shape kernel, more adequate feature description can be obtained on the border via the combination of the strip hierarchical pooling and strip max pooling. With an extension of the proposed activation function on classification-GIoU joint representation, the overall detection accuracy has been further improved. Experimental evaluations on the COCO benchmark datasets have shown that the proposed work has a superior performance in comparison to other state-of-the-art detection approaches.}
}
@article{ROY2023109315,
title = {Multi-scale attention guided pose transfer},
journal = {Pattern Recognition},
volume = {137},
pages = {109315},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109315},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300016X},
author = {Prasun Roy and Saumik Bhattacharya and Subhankar Ghosh and Umapada Pal},
keywords = {Pose transfer, Attention, GAN, DeepFashion},
abstract = {Pose transfer refers to the probabilistic image generation of a person with a previously unseen novel pose from another image of that person having a different pose. Due to potential academic and commercial applications, pose transfer has been extensively studied in recent years. Among the various approaches to the problem, attention guided progressive generation is shown to produce state-of-the-art results in most cases. This paper presents an improved network architecture for pose transfer by introducing attention links at every resolution level of the encoder and decoder. By utilizing such dense multi-scale attention guided approach, we are able to achieve significant improvement over the existing methods both visually and analytically. We conclude our findings with extensive qualitative and quantitative comparisons against several existing methods on the DeepFashion dataset. We also show the generality of the proposed network architecture by extending it to multiple application domains, such as semantic reconstruction, virtual try-on and style manipulation.11For reproducibility, the code implementation and the pre-trained models are available at https://github.com/prasunroy/pose-transfer.}
}
@article{SUN2023109304,
title = {Detail enhancement-based vehicle re-identification with orientation-guided re-ranking},
journal = {Pattern Recognition},
volume = {137},
pages = {109304},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109304},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000055},
author = {Ziruo Sun and Xiushan Nie and Xiaopeng Bi and Shaohua Wang and Yilong Yin},
keywords = {Vehicle re-identification, Detail enhancement, Re-ranking},
abstract = {Vehicle re-identification (re-ID) has attracted extensive attention in the field of computer vision owing to the development of smart cities. Two issues remain in the task of vehicle re-ID: minor inter-class differences and extreme orientation variations. To address them, we propose a detailed enhancement-based vehicle re-ID method with orientation-guided re-ranking. In the proposed method, a novel enhancement module using stripe-based partition, is presented to avoid the negative influence of minor inter-class difference. The stripe-based partition subdivides the feature map of middle layers in the network into two stripes, and then the module explores more detailed information for vehicle re-ID. Furthermore, a multilayer feature fusion module is proposed to enhance the feature representation of a vehicle. To address the problem of extreme orientation variation in vehicle re-ID, we propose an orientation-guided re-ranking strategy to re-rank the retrieval result based on the orientation information and query expansion algorithm. This strategy can optimize the ranking of vehicles whose orientation is not similar to the query images in the post-processing stage. Extensive experiments on three public datasets confirm the effectiveness of our method.}
}
@article{CHAVOSHINEJAD2023109282,
title = {Self-supervised semi-supervised nonnegative matrix factorization for data clustering},
journal = {Pattern Recognition},
volume = {137},
pages = {109282},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109282},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007610},
author = {Jovan Chavoshinejad and Seyed Amjad Seyedi and Fardin {Akhlaghian Tab} and Navid Salahian},
keywords = {Nonnegative matrix factorization, Semi-supervised learning, Self-supervised learning, Ensemble clustering},
abstract = {Semi-supervised nonnegative matrix factorization exploits the strengths of matrix factorization in successfully learning part-based representation and is also able to achieve high learning performance when facing a scarcity of labeled data and a large amount of unlabeled data. Its major challenge lies in how to learn more discriminative representations from limited labeled data. Furthermore, self-supervised learning has been proven very effective at learning representations from unlabeled data in various learning tasks. Recent research works focus on utilizing the capacity of self-supervised learning to enhance semi-supervised learning. In this paper, we design an effective Self-Supervised Semi-Supervised Nonnegative Matrix Factorization (S4NMF) in a semi-supervised clustering setting. The S4NMF directly extracts a consensus result from ensembled NMFs with similarity and dissimilarity regularizations. In an iterative process, this self-supervisory information will be fed back to the proposed model to boost semi-supervised learning and form more distinct clusters. The proposed iterative algorithm is used to solve the given problem, which is defined as an optimization problem with a well-formulated objective function. In addition, the theoretical and empirical analyses investigate the convergence of the proposed optimization algorithm. To demonstrate the effectiveness of the proposed model in semi-supervised clustering, we conduct extensive experiments on standard benchmark datasets. The source code for reproducing our results can be found at https://github.com/ChavoshiNejad/S4NMF.}
}
@article{YUAN2023109298,
title = {A multi-strategy contrastive learning framework for weakly supervised semantic segmentation},
journal = {Pattern Recognition},
volume = {137},
pages = {109298},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109298},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007774},
author = {Kunhao Yuan and Gerald Schaefer and Yu-Kun Lai and Yifan Wang and Xiyao Liu and Lin Guan and Hui Fang},
keywords = {Weakly supervised learning, Representation learning, Contrastive learning, Semantic segmentation},
abstract = {Weakly supervised semantic segmentation (WSSS) has gained significant popularity as it relies only on weak labels such as image level annotations rather than the pixel level annotations required by supervised semantic segmentation (SSS) methods. Despite drastically reduced annotation costs, typical feature representations learned from WSSS are only representative of some salient parts of objects and less reliable compared to SSS due to the weak guidance during training. In this paper, we propose a novel Multi-Strategy Contrastive Learning (MuSCLe) framework to obtain enhanced feature representations and improve WSSS performance by exploiting similarity and dissimilarity of contrastive sample pairs at image, region, pixel and object boundary levels. Extensive experiments demonstrate the effectiveness of our method and show that MuSCLe outperforms current state-of-the-art methods on the widely used PASCAL VOC 2012 dataset.}
}
@article{KHATUN2023109246,
title = {Pose-driven attention-guided image generation for person re-Identification},
journal = {Pattern Recognition},
volume = {137},
pages = {109246},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109246},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007257},
author = {Amena Khatun and Simon Denman and Sridha Sridharan and Clinton Fookes},
keywords = {Person re-identification, Pose-transfer, Attention mechanism, Semantic-consistency loss},
abstract = {Person re-identification (re-ID) concerns the matching of subject images across different camera views in a multi camera surveillance system. One of the major challenges in person re-ID is pose variations across the camera network, which significantly affects the appearance of a person. Existing development data lack adequate pose variations to carry out effective training of person re-ID systems. To solve this issue, in this paper we propose an end-to-end pose-driven attention-guided generative adversarial network, to generate multiple poses of a person. We propose to attentively learn and transfer the subject pose through an attention mechanism. A semantic-consistency loss is proposed to preserve the semantic information of the person during pose transfer. To ensure fine image details are realistic after pose translation, an appearance discriminator is used while a pose discriminator is used to ensure the pose of the transferred images will exactly be the same as the target pose. We show that by incorporating the proposed approach in a person re-identification framework, realistic pose transferred images and state-of-the-art re-identification results can be achieved.}
}
@article{HSU2023109294,
title = {Recurrent wavelet structure-preserving residual network for single image deraining},
journal = {Pattern Recognition},
volume = {137},
pages = {109294},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109294},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007737},
author = {Wei-Yen Hsu and Wei-Chi Chang},
keywords = {Single image deraining, Recurrent wavelet residual network, Structure preservation, Image weighted blending},
abstract = {The combination of deep learning and image prior has been widely used in single image deraining since 2017. Recent studies have demonstrated an excellent deraining effect on the high-frequency part of rain images, but less attention was paid to the low-frequency part of rain images. The rain streaks remain in the low-frequency part of rain images, thus limiting the deraining effect. Since the rain streaks in rain images are often mixed with object edges and background scenes, it is challenging to separate rain from them by directly learning the deraining function in the image domain. To solve these problems, we propose a novel Recurrent Wavelet Structure-preserving Residual Network (RWSRNet), which mainly preserves and introduces the low-frequency sub-images of each level into the low-frequency rain removal sub-networks that are greatly different from the state-of-the-art approaches introducing wavelet transform. In addition, we also share the low-frequency structure information to the high-frequency sub-networks through block connection, which further enriches the detailed information, facilitates convergence, and strengthens the ability of our network to remove rain streaks in high frequency. Finally, we fuse the derained low-frequency sub-images of each level through the proposed image weighted blending module and finally reconstruct the low- and high-frequency sub-images into clean images through inverse wavelet transform recursively. The experimental results indicate that the proposed method achieves an excellent deraining effect on both low- and high-frequency parts of rain images and has better performance in low-frequency preservation and high-frequency enhancement in comparison with the state-of-the-art approaches on synthetic and real image datasets.}
}
@article{WANG2023109273,
title = {An adaptive mutual K-nearest neighbors clustering algorithm based on maximizing mutual information},
journal = {Pattern Recognition},
volume = {137},
pages = {109273},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109273},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200752X},
author = {Yizhang Wang and Wei Pang and Zhixiang Jiao},
keywords = {Mutual K-nearest neighbors, Adaptive clustering, Maximizing mutual information},
abstract = {Clustering based on Mutual K-nearest Neighbors (CMNN) is a classical method of grouping data into different clusters. However, it has two well-known limitations: (1) the clustering results are very much dependent on the parameter k; (2) CMNN assumes that noise points correspond to clusters of small sizes according to the Mutual K-nearest Neighbors (MKNN) criterion, but some data points in small size clusters are wrongly identified as noises. To address these two issues, we propose an adaptive improved CMNN algorithm (AVCMNN), which consists of two parts: (1) improved CMNN algorithm (abbreviated as VCMNN) and (2) adaptive VCMNN algorithm (abbreviated as AVCMNN). Specifically, the first part is VCMNN algorithm, we first reassign the data points in some small-size clusters by a novel voting strategy because some of them are wrongly identified as noise points, and the clustering results are improved. Then, the second part is AVCMNN, we use maximizing mutual information to construct an objective function to optimize the parameters of the proposed method and finally obtain the better parameters values and clustering results. We conduct extensive experiments on twenty datasets, including six synthetic datasets, ten UCI datasets, and four image datasets. The experimental results show that VCMNN and AVCMNN outperforms three classical algorithms (i.e., CMNN, DPC, and DBSCAN) and six state-of-the-art (SOTA) clustering algorithms in most cases.}
}
@article{KIM2023109286,
title = {Generating Transferable Adversarial Examples for Speech Classification},
journal = {Pattern Recognition},
volume = {137},
pages = {109286},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109286},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007658},
author = {Hoki Kim and Jinseong Park and Jaewook Lee},
keywords = {Speech classification, Adversarial attack, Transferability},
abstract = {Despite the success of deep neural networks, the existence of adversarial attacks has revealed the vulnerability of neural networks in terms of security. Adversarial attacks add subtle noise to the original example, resulting in a false prediction. Although adversarial attacks have been mainly studied in the image domain, a recent line of research has discovered that speech classification systems are also exposed to adversarial attacks. By adding inaudible noise, an adversary can deceive speech classification systems and cause fatal issues in various applications, such as speaker identification and command recognition tasks. However, research on the transferability of audio adversarial examples is still limited. Thus, in this study, we first investigate the transferability of audio adversarial examples with different structures and conditions. Through extensive experiments, we discover that the transferability of audio adversarial examples is related to their noise sensitivity. Based on the analyses, we present a new adversarial attack called noise injected attack that generates highly transferable audio adversarial examples by injecting additive noise during the gradient ascent process. Our experimental results demonstrate that the proposed method outperforms other adversarial attacks in terms of transferability.}
}
@article{QIU2023109300,
title = {Hierarchical nearest neighbor descent, in-tree, and clustering},
journal = {Pattern Recognition},
volume = {137},
pages = {109300},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109300},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000018},
author = {Teng Qiu and Yongjie Li},
keywords = {Clustering, In-tree, Hierarchical nearest neighbor descent, Mass cytometry},
abstract = {Recently, we have proposed a physically-inspired graph-theoretical method, called the Nearest Descent (ND), which is capable of organizing a dataset into an in-tree graph structure. Due to some beautiful and effective features, the constructed in-tree proves well-suited for data clustering. Although there exist some undesired edges (i.e., the inter-cluster edges) in this in-tree, those edges are usually very distinguishable, in sharp contrast to the cases in the famous Minimal Spanning Tree (MST). Here, we propose another graph-theoretical method, called the Hierarchical Nearest Neighbor Descent (HNND). Like ND, HNND also organizes a dataset into an in-tree, but in a more efficient way. Consequently, HNND-based clustering (HNND-C) is more efficient than ND-based clustering (ND-C) as well. This is well proved by the experimental results on five high-dimensional and large-size mass cytometry datasets. The experimental results also show that HNND-C achieves overall better performance than some state-of-the-art clustering methods.}
}
@article{TANG2023109320,
title = {Task-balanced distillation for object detection},
journal = {Pattern Recognition},
volume = {137},
pages = {109320},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109320},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000213},
author = {Ruining Tang and Zhenyu Liu and Yangguang Li and Yiguo Song and Hui Liu and Qide Wang and Jing Shao and Guifang Duan and Jianrong Tan},
keywords = {Object detection, Knowledge distillation, Computer vision},
abstract = {Mainstream object detectors are commonly constituted of two sub-tasks, including classification and regression tasks, implemented by two parallel heads. This classic design paradigm inevitably leads to inconsistent spatial distributions between classification score and localization quality (IOU). Therefore, this paper alleviates this misalignment in the view of knowledge distillation. First, we observe that the massive teacher achieves a higher proportion of harmonious predictions than the lightweight student. Based on this intriguing observation, a novel Harmony Score (HS) is devised to estimate the alignment of classification and regression qualities. HS models the relationship between two sub-tasks and is seen as prior knowledge to promote harmonious predictions for the student. Second, this spatial misalignment will result in inharmonious region selection when distilling features. To alleviate this problem, a novel Task-decoupled Feature Distillation (TFD) is proposed by flexibly balancing the contributions of classification and regression tasks. Eventually, HD and TFD constitute the proposed method, named Task-Balanced Distillation (TBD). Extensive experiments demonstrate the considerable potential and generalization of the proposed method. Notably, when equipped with TBD, the performances of RetinaNet-R18/RetinaNet-R50/Faster-RCNN-R18 can be boosted from 33.2/37.4/34.5 to 37.3/41.2/37.7, outperforming the recent KD-based methods like FRS, FGD, and MGD.}
}
@article{KHAN2023109344,
title = {A High Dynamic Range Imaging Method for Short Exposure Multiview Images},
journal = {Pattern Recognition},
volume = {137},
pages = {109344},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109344},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000456},
author = {Rizwan Khan and You Yang and Kejun Wu and Atif Mehmood and Zahid Hussain Qaisar and Zhonglong Zheng},
keywords = {Multiview camera, Image enhancement, High dynamic range},
abstract = {The restoration and enhancement of multiview low dynamic range (MVLDR) images captured in low lighting conditions is a great challenge. The disparity maps are hardly reliable in practical, real-world scenarios and suffers from holes and artifacts due to large baseline and angle deviation among multiple cameras in low lighting conditions. Furthermore, multiple images with some additional information (e.g., ISO/exposure time, etc.) are required for the radiance map and poses the additional challenges of deghosting to encounter motion artifacts. In this paper, we proposed a method to reconstruct multiview high dynamic range (MVHDR) images from MVLDR images without relying on disparity maps. We detect and accurately match the feature points among the involved input views and gather the brightness information from the neighboring viewpoints to optimize an image restoration function based on input exposure gain to finally generate MVHDR images. Our method is very reliable and suitable for a wide baseline among sparse cameras. The proposed method requires only one image per viewpoint without any additional information and outperforms others.}
}
@article{2023109392,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {137},
pages = {109392},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(23)00093-6},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000936}
}
@article{LI2023109297,
title = {Learning depth via leveraging semantics: Self-supervised monocular depth estimation with both implicit and explicit semantic guidance},
journal = {Pattern Recognition},
volume = {137},
pages = {109297},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109297},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007762},
author = {Rui Li and Danna Xue and Shaolin Su and Xiantuo He and Qing Mao and Yu Zhu and Jinqiu Sun and Yanning Zhang},
keywords = {Semantic-guided self-supervised depth estimation, Semantic-aware spatial feature modulation, Semantic-guided ranking loss, Robust point pair sampling, Uncertainty weighting},
abstract = {Self-supervised monocular depth estimation has shown great success in learning depth using only images for supervision. In this paper, we propose to enhance self-supervised depth estimation with semantics and propose a novel learning scheme, which incorporates both implicit and explicit semantic guidances. Specifically, we propose to relate depth distributions to the semantic category information by proposing a Semantic-aware Spatial Feature Modulation (SSFM) scheme, which implicitly modulates the semantic and depth features in a joint learning framework. The modulation parameters are generated from semantic labels to acquire category-level guidance. Meanwhile, a semantic-guided ranking loss is proposed to explicitly constrain the estimated depth borders using the corresponding segmentation labels. To avoid the impact brought by erroneous segmentation labels, both robust sampling strategy and prediction uncertainty weighting are proposed for the ranking loss. Extensive experimental results show that our method produces high-quality depth maps with semantically consistent depth distributions and accurate depth edges, outperforming the state-of-the-art methods by significant margins.}
}
@article{YUAN2023109289,
title = {A lightweight network for smoke semantic segmentation},
journal = {Pattern Recognition},
volume = {137},
pages = {109289},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109289},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007683},
author = {Feiniu Yuan and Kang Li and Chunmei Wang and Zhijun Fang},
keywords = {Smoke semantic segmentation, Deep learning, Attention mechanism, Lightweight network, Channel split and shuffle},
abstract = {To obtain real-time performance on computation limited devices, we propose a lightweight network for smoke segmentation. To enhance the ability of feature encoding, we first propose an Attention Encoding Module (AEM) by designing a Channel Split and Shuffle Attention Module (CSSAM), which can extract powerful features and reduce computations simultaneously. CSSAM adopts Channel split and shuffle to greatly reduce learnable parameters for improving computation speed, and uses attention mechanism to focus on salient objects to enhance the effectiveness of features. In addition, AEM repeatedly stacks CSSAM in different encoding stages to achieve scale invariance. For the middle-level features of encoding stages, we propose a Spatial Enhancement Module (SEM) to boost the representation ability of spatial details. SEM concatenates feature maps produced by average and maximum pooling to achieve dominant and global responses, which are then weighted by the activated output of global average pooling to generate attention features. In the highest level of encoding stages, we present a Channel Attention Module (CAM) to explicitly model interdependency between channels. By reshaping 2D features into 1D features, we use element-wise matrix multiplications to reduce computation complexity for extracting channel-related information. Finally, we design a Feature Fusion Module (FFM) and a Global Coefficient Path (GCP) to fuse the outputs of SEM and CAM in an attention way for further improving robustness of final features. Experiments show that our method is significantly superior to existing state-of-the-art algorithms in smoke datasets, and also obtains excellent results in both synthetic and real smoke datasets. However, our method has less than 1 M network parameters.}
}
@article{WANG2023109237,
title = {Learning a bi-directional discriminative representation for deep clustering},
journal = {Pattern Recognition},
volume = {137},
pages = {109237},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109237},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007166},
author = {Yiming Wang and Dongxia Chang and Zhiqiang Fu and Yao Zhao},
keywords = {Deep clustering, Representation learning, Manifold learning, Mutual information},
abstract = {Nowadays, deep clustering achieves superior performance by jointly performing representation learning and cluster assignment. Although numerous deep clustering algorithms have emerged, most of them have difficulty learning representations that fit the clustering distribution. To address this issue, we propose a bi-directional discriminative representation learning clustering (BDRC) framework in this paper. In our framework, a dual autoencoder network, a bi-directional mutual information maximization module and a self-supervised cluster prediction module are combined into a joint optimization framework. To learn more cluster-friendly representations, the bi-directional mutual information maximization module is executed on both samples and their nearest neighbors to explore the cluster relationships between samples. In order to improve the stability of the model, a self-supervised cluster prediction module is devised to predict clustering assignments to supervise the autoencoder using the KL-divergence. Moreover, the UMAP is used to find the manifold of the latent representations which can better preserve the global structure. Experiments on some benchmark datasets demonstrate the superiority of the proposed BDRC algorithm.}
}
@article{JIANG2023109265,
title = {Sparse norm regularized attribute selection for graph neural networks},
journal = {Pattern Recognition},
volume = {137},
pages = {109265},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109265},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007440},
author = {Bo Jiang and Beibei Wang and Bin Luo},
keywords = {Graph neural networks, Feature selection, Sparse regularization, Semi-supervised learning},
abstract = {Graph Neural Networks (GNNs) have been widely used for graph learning tasks. The main aspect of GNN’s layer-wise message passing is conducting attribute/feature propagation on graph. Most existing GNNs generally conduct feature propagation across all feature dimensions. However, in many real applications, attributes usually contain irrelevant and redundant noise. In this case, attribute/feature selection is desired to extract meaningful features and eliminate noisy ones for GNN’s layer-wise propagation. Based on this observation, in this paper, we combine ℓ2,1/ℓ1-norm regularized attribute selection and GNNs together and propose a novel Attribute selection guided GNNs (AsGNNs) for graph data representation. AsGNNs aim to adaptively select some desired meaningful features/attributes that best serve GNNs. Moreover, an effective optimization framework has also been derived to train the proposed AsGNNs. The proposed AsGNNs provide a general framework which can incorporate any GNNs to conduct feature selection for layer-wise propagation. In this paper, we implement AsGNNs on both graph convolutional network (GCN) and graph attention network (GAT) and develop AsGCN and AsGAT for graph learning. Experimental results on several benchmark datasets demonstrate the effectiveness of the proposed AsGNNs (AsGCN, AsGAT) on semi-supervised learning tasks.}
}
@article{MAHESHWARI2023109341,
title = {DCSNE: Density-based Clustering using Graph Shared Neighbors and Entropy},
journal = {Pattern Recognition},
volume = {137},
pages = {109341},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109341},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000420},
author = {Rashmi Maheshwari and Sraban Kumar Mohanty and Amaresh Chandra Mishra},
keywords = {Density-based clustering, Neighborhood information, Randomness, Shared neighbor, Disorderliness, Entropy},
abstract = {Density-based clustering techniques identify arbitrary shaped clusters in the presence of outliers by capturing the intrinsic distribution of data and separating high and low-density regions based on the neighborhood information. They use global parameters to compute the density distribution of data points, the optimization of which is quite a challenging and time intensive task. The similarity graphs constructed from the data points can easily capture the topology of the density regions without using any user-defined parameters. Moreover, the concept of entropy can be useful to capture the randomness and disorderliness present in highly complex data distributions. Our proposed algorithm makes use of entropy in conjunction with the graph local neighborhood information to compute density distribution of data points. Then, actual clusters are identified using the density distribution and the shared neighbor information between the regions. The experimental results show that the proposed technique outperforms other comparable methods in terms of cluster quality in the presence of noise on diversified gene expression and real datasets.}
}
@article{JIANG2023109277,
title = {Lightweight Semi-supervised Network for Single Image Rain Removal},
journal = {Pattern Recognition},
volume = {137},
pages = {109277},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109277},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007567},
author = {Nanfeng Jiang and Jiawei Luo and Junhong Lin and Weiling Chen and Tiesong Zhao},
keywords = {Rain Removal, Image Processing, Lightweight Network, Semi-supervised Learning},
abstract = {Deep learning technologies have shown their advantages in Single Image Rain Removal (SIRR) tasks. However, the derained results of most methods are limited to some challenges. First, due to the lack of real-world rainy/clean image pairs, many methods seriously rely on the labeled synthetic training images and will not effectively remove complex rain streaks in real-world scenarios. Second, most existing SIRR models require high computing power, which considerably limits their real-world applications. To address these issues, we propose a Lightweight Semi-supervised Network (LSNet) for SIRR. Our LSNet utilizes a compact semi-supervised framework to improve generalization ability in real-world rainy images removal. Meanwhile, in our semi-supervised framework, we also design a cascaded sub-network, which progressively removes complex rain streaks via a multi-stage manner. Specially, the multi-stage manner is based on a series of cascaded blocks, where we conduct recursive learning strategy to reduce model parameters. Extensive experimental results demonstrate that our method achieves comparable performance to the state-of-the-arts while has fewer parameters.}
}
@article{DONG2023109256,
title = {Control Distance IoU and Control Distance IoU Loss for Better Bounding Box Regression},
journal = {Pattern Recognition},
volume = {137},
pages = {109256},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109256},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200735X},
author = {Chen Dong and Miao Duoqian},
keywords = {Computer vision, Object detection, IoU, Loss function},
abstract = {Numerous improvements in feedback mechanisms have contributed to the great progress in object detection. In this paper, we first present an evaluation-feedback module, which consists of an evaluation system and feedback mechanism. Then we analyze and summarize traditional evaluation-feedback modules. We focus on both the evaluation system and the feedback mechanism, and propose Control Distance IoU and Control Distance IoU loss function (CDIoU and CDIoU loss) without increasing parameters in models, which make significant enhancements on several classical and emerging models. Finally, we propose Automatic Ground Truth Clustering (AGTC) and Floating Learning Rate Decay (FLRD) for faster regression in object detection. Experiments show that a coordinated evaluation-feedback module can effectively improve model performance. Both CNN and transformer-based detectors with CDIoU + CDIoU loss, AGTC, and FLRD achieve excellent performances. There are a maximum AP improvement of 2.9%, an average AP of 1.1% improvement on MS COCO, a maximum AP improvement of 8.2%, and an average AP improvement of 3.7% on Visdrone dataset.}
}
@article{HERRMANN2023109333,
title = {Amercing: An intuitive and effective constraint for dynamic time warping},
journal = {Pattern Recognition},
volume = {137},
pages = {109333},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109333},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000341},
author = {Matthieu Herrmann and Geoffrey I. Webb},
keywords = {Time series, Dynamic time warping, Elastic distance},
abstract = {Dynamic Time Warping (DTW) is a time series distance measure that allows non-linear alignments between series. Constraints on the alignments in the form of windows and weights have been introduced because unconstrained DTW is too permissive in its alignments. However, windowing introduces a crude step function, allowing unconstrained flexibility within the window, and none beyond it. While not entailing a step function, a multiplicative weight is relative to the distances between aligned points along a warped path, rather than being a direct function of the amount of warping that is introduced. In this paper, we introduce Amerced Dynamic Time Warping (ADTW), a new, intuitive, DTW variant that penalizes the act of warping by a fixed additive cost. Like windowing and weighting, ADTW constrains the amount of warping. However, it avoids both abrupt discontinuities in the amount of warping allowed and the limitations of a multiplicative penalty. We formally introduce ADTW, prove some of its properties, and discuss its parameterization. We show on a simple example how it can be parameterized to achieve an intuitive outcome, and demonstrate its usefulness on a standard time series classification benchmark. We provide a demonstration application in C++ Herrmann(2021)[1].}
}
@article{KE2023109305,
title = {Granularity-aware distillation and structure modeling region proposal network for fine-grained image classification},
journal = {Pattern Recognition},
volume = {137},
pages = {109305},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109305},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000067},
author = {Xiao Ke and Yuhang Cai and Baitao Chen and Hao Liu and Wenzhong Guo},
keywords = {Fine-grained visual classification, Multi-granularity feature learning, Knowledge distillation, Structure modeling},
abstract = {Fine-grained visual classification (FGVC) aims to identify objects belonging to multiple sub-categories of the same super-category. The key to solving fine-grained classification problems is to learn discriminative visual feature representation with only subtle differences. Although previous work based on refined feature learning has made great progress, however, high-level semantic features often lack key information for fine-grained visual object nuances. How to efficiently integrate semantic information of different granularities from classification networks is a critical. In this paper, we propose Granularity-aware Distillation and Structure Modeling region Proposal Network(GDSMP-Net). Our solution integrates multi-granularity hierarchical information through a multi-granularity fusion learning strategy to enhance feature representation. In view of the inherent challenges of large intra-class differences in FGVC, a cross-layer self-distillation regularization is proposed to to strengthen the connection between high-level semantics and low-level semantics for robust multi-granularity feature learning. On this basis, we use a weakly supervised method to generate local branches, and the collaborative learning of discriminative semantics and structural semantics based on local regions, facilitating model to perceive contextual information to capture structural interactions between local semantics. Comprehensive experiments show that our method achieves state-of-the-art performance on four widely-used challenging datasets.(CUB-200-2011, Stanford Cars, FGVC-Aircraft and NA-birds).}
}
@article{MUSSABAYEV2023109269,
title = {How to Use K-means for Big Data Clustering?},
journal = {Pattern Recognition},
volume = {137},
pages = {109269},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109269},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007488},
author = {Rustam Mussabayev and Nenad Mladenovic and Bassem Jarboui and Ravil Mussabayev},
keywords = {Big data, Clustering, Minimum sum-of-squares, Divide and conquer algorithm, Decomposition, K-means, K-means++, Global optimization, Unsupervised learning},
abstract = {K-means plays a vital role in data mining and is the simplest and most widely used algorithm under the Euclidean Minimum Sum-of-Squares Clustering (MSSC) model. However, its performance drastically drops when applied to vast amounts of data. Therefore, it is crucial to improve K-means by scaling it to big data using as few of the following computational resources as possible: data, time, and algorithmic ingredients. We propose a new parallel scheme of using K-means and K-means++ algorithms for big data clustering that satisfies the properties of a “true big data” algorithm and outperforms the classical and recent state-of-the-art MSSC approaches in terms of solution quality and runtime. The new approach naturally implements global search by decomposing the MSSC problem without using additional metaheuristics. This work shows that data decomposition is the basic approach to solve the big data clustering problem. The empirical success of the new algorithm allowed us to challenge the common belief that more data is required to obtain a good clustering solution. Moreover, the present work questions the established trend that more sophisticated hybrid approaches and algorithms are required to obtain a better clustering solution.}
}
@article{WU2023109293,
title = {Geometric-aware dense matching network for 6D pose estimation of objects from RGB-D images},
journal = {Pattern Recognition},
volume = {137},
pages = {109293},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109293},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007725},
author = {Chenrui Wu and Long Chen and Shenglong Wang and Han Yang and Junjie Jiang},
keywords = {6D pose estimation, Metric learning, Triplet loss, Dense correspondences, Geometric constraint},
abstract = {6D pose estimation for certain targets from RGB-D images is a fundamental problem in computer vision. Current methods emphasize learning the overall expression of the targets, which leads to poor performance under occlusion and truncation conditions. In this paper, we propose using a geometric-aware dense matching network to obtain visible dense correspondences between a RGB-D image and 3D model to address difficult predictions from unseen keypoints. Two geometrical structures are considered for dense matching. (1) The neighbor area of the correspondences is treated as suboptimal matches in addition to the correspondence to reduce the influence of the error caused by ground truth calibration. (2) The distance consistency of the correspondences is leveraged to eliminate the ambiguity from the symmetrical objects. Experiments on LM-O dataset (77.1% ADD(S)-0.1d) and YCB-V dataset (97.6% ADD(S)) show the effectiveness and advantages of our proposed method.11The source code will soon be available at https://github.com/Ray0089/geometric-aware-dense-matching.}
}
@article{GAN2023109317,
title = {Characters as graphs: Interpretable handwritten Chinese character recognition via Pyramid Graph Transformer},
journal = {Pattern Recognition},
volume = {137},
pages = {109317},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109317},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000183},
author = {Ji Gan and Yuyan Chen and Bo Hu and Jiaxu Leng and Weiqiang Wang and Xinbo Gao},
keywords = {Handwritten Chinese character Recognition, Transformer, Graph convolutional network, Pyramid graph},
abstract = {It is meaningful but challenging to teach machines to recognize handwritten Chinese characters. However, conventional approaches typically view handwritten Chinese characters as either static images or temporal trajectories, which may ignore the inherent geometric semantics of characters. Instead, here we first propose to represent handwritten characters as skeleton graphs, explicitly considering the natural characteristics of characters (i.e., characters as graphs). Furthermore, we propose a novel Pyramid Graph Transformer (PyGT) to specifically process the graph-structured characters, which fully integrates the advantages of Transformers and graph convolutional networks. Specifically, our PyGT can learn better graph features through (i) capturing the global information from all nodes with graph attention mechanism and (ii) modelling the explicit local adjacency structures of nodes with graph convolutions. Furthermore, the PyGT learns the multi-resolution features by constructing a progressive shrinking pyramid. Compared with existing approaches, it is more interpretable to recognize characters as geometric graphs. Moreover, the proposed method is generic for both online and offline handwritten Chinese character recognition (HCCR), and it also can be feasibly extended to handwritten text recognition. Extensive experiments empirically demonstrate the superiority of PyGT over the prevalent approaches including 2D-CNN, RNN/1D-CNN, and Vision Transformer (ViT) for HCCR. The code is available at https://github.com/ganji15/PyGT-HCCR.}
}
@article{WANG2023109309,
title = {ProbSAP: A comprehensive and high-performance system for student academic performance prediction},
journal = {Pattern Recognition},
volume = {137},
pages = {109309},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109309},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000109},
author = {Xinning Wang and Yuben Zhao and Chong Li and Peng Ren},
keywords = {Student academic performance, SAP prediction, Educational data mining (EDM), Imbalanced data management, XGBoost-Enhanced method},
abstract = {The student academic performance prediction is becoming an indispensable service in the computer supported intelligent education system. But conventional machine learning-based methods can only exploit the sparse discriminative features of student behaviors in imbalanced academic datasets to predict student academic performance (SAP). Furthermore, there is a lack of imbalanced data processing mechanisms that can efficiently capture student characteristics and achievement. Therefore, we propose a comprehensive and high-performance prediction framework to probe SAP characteristics (ProbSAP) on massive educational data, which can resolve imbalanced data issue and improve academic prediction performance for making course final mark prediction. It consists of three main components: collaborative data processing module for enhancing the data quality, scalable metadata clustering module for alleviating the imbalance of academic features, and XGBoost-enhanced SAP prediction module for academic performance forecasting. The collaborative data processing module integrates multi-dimensional academic data, which sustains a good supply for clustering and modeling in the ProbSAP framework. The comparative evaluation results demonstrate that ProbSAP delivers superior accuracy and efficiency improvement for the course final mark prediction of college students over other state-of-the-art methods such as CNN, SVR, RFR, XGBoost, Catboost-SHAP, and AS-SAN. On average, ProbSAP reduces the mean absolute error (MAE) by 84.76%, 72.11%, and 66.49% compared with XGBoost, Catboost-SHAP, and AS-SAN, respectively. It also leads to a better out-sample fit that minimizes prediction errors between 1% and 9% with over 98% of actual samples.}
}
@article{HOUFAR2023109281,
title = {Automatically weighted binary multi-view clustering via deep initialization (AW-BMVC)},
journal = {Pattern Recognition},
volume = {137},
pages = {109281},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109281},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007609},
author = {Khamis Houfar and Djamel Samai and Fadi Dornaika and Azeddine Benlamoudi and Khaled Bensid and Abdelmalik Taleb-Ahmed},
keywords = {Multi-view clustering, Large scale, Anchors, Discrete representation and BD-FFT},
abstract = {Clustering is inherently a process of exploratory data analysis. It has attracted more attention recently because much real-world data consists of multiple representations or views. However, it becomes increasingly problematic when dealing with large and heterogeneous data. It is worth noting that several approaches have been developed to increase computational efficiency, although most of them have some drawbacks: (1) Most existing techniques consider equal or static weights to quantify importance across different views and samples, so common and complementary features cannot be used. (2) The clustering task is performed by arbitrary initialization without caring about the rich structure of the joint discrete representation, and thus poorly executed. In this paper, we propose a novel approach called “Auto-Weighted Binary Multi-View Clustering Via Deep Initialization” for large-scale multi-view clustering based on two main scenarios. First, we consider the distinction between different views based on the importance of samples, and therefore apply a dynamic learning strategy for the automatic weighting of views and samples. Second, in the context of initializing binary clustering, we develop a new CNN feature and use a low-dimensional binary embedding by exploiting the efficient capabilities of Fourier mapping. Moreover, our approach simultaneously learns a joint discrete representation and performs direct clustering using a constrained binary matrix factorization; the optimization problem is perfectly solved in a unified learning model. Experimental results conducted on several challenging datasets demonstrate the effectiveness and superiority of the proposed approach over state-of-the-art methods in terms of accuracy, normalized mutual information, and purity.}
}
@article{ZHANG2023109345,
title = {Frequency learning attention networks based on deep learning for automatic modulation classification in wireless communication},
journal = {Pattern Recognition},
volume = {137},
pages = {109345},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109345},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000468},
author = {Duona Zhang and Yuanyao Lu and Yundong Li and Wenrui Ding and Baochang Zhang and Jing Xiao},
keywords = {Frequency learning, Attention mechanism, Automatic modulation classification, Wireless communication},
abstract = {Deep neural networks have been recently applied in automatic modulation classification task and achieved remarkable success. However, Existing neural networks mainly focus on the purely data-driven architecture design, and fail to explore the hand-crafted feature mechanisms which are particularly significant for radio signal presentation in wireless communication. Inspired by digital signal processing theories, we propose frequency learning attention networks (FLANs) to analyze the radio spectral bias from frequency perspective, based on a multi-spectral attention mechanism for learning-based frequency components selection. FLANs are the general case of classical global average pooling and leverage identical structures of the popular neural networks. Extensive experiments have been conducted to validate the superiority of FLANs for automatic modulation classification over a wide variety of state-of-the-art methods on RADIOML 2018.01A dataset.}
}
@article{CHEN2023109321,
title = {Non-residual unrestricted pruned ultra-faster line detection for edge devices},
journal = {Pattern Recognition},
volume = {137},
pages = {109321},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109321},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000225},
author = {Pengpeng Chen and Dongjingdian Liu and Shouwan Gao},
keywords = {Line detection, Network slimming, Edge computing, Network interpretability},
abstract = {Line detection with deep learning is a popular visual task that focuses mostly on lane detection. It requires quicker inference speed and lower consumption, especially for high-speed edge device applications. Based on the UFAST, we propose the Non-Residual Unrestricted Pruned Ultra-faster (NRUPU) line detection via a novel model compression method including non-interference structural reconstruction (NISR), shallow channel priority reservation (SCPR) pruning and non-residual equivalent transformation (NRET). NISR is a structure reconstruction scheme allocating residual branches into each layer to solve the cross-layer channel interference in ResNet-18. SCPR pruning directly uses the factors of BN layers to build channel importance evaluation for backbone and designs channel selection method for head based on data distribution consistency, reducing the parameters of each layer independently. Then NRET losslessly converts the multi-branch model to a single-branch one containing only convolution, linear, and relu, which reduces implementation complexity on edge devices. These designs follow the theoretical foundations: the data distribution transformation trend and effect of gradient back-propagation on model learning ability. Compared with previous pruning methods, our method optimizes not only the parameters of the model but also the structure of the model. We train NRUPU in RTX2080Ti and deploy tests on edge devices NVIDIA Jetson Xavier NX (NJXN) and Atlas 200 DK (A2DK). Extensive experiments are conducted on the dataset TuSimple, CULane and our belt dataset with 11,894 data. Results show that NRUPU achieves over 96% speed increase and over 66% parameter reduction on all datasets within 0.7% accuracy loss. The FPS can reach 749, 665 and 783 on RTX2080Ti, 133, 117 and 143 on NJNX, 178, 161 and 183 on A2DK respectively. The code is released at https://anonymous.4open.science/r/NRUPU-29B0.}
}
@article{MA2023109280,
title = {CrossRectify: Leveraging disagreement for semi-supervised object detection},
journal = {Pattern Recognition},
volume = {137},
pages = {109280},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109280},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007592},
author = {Chengcheng Ma and Xingjia Pan and Qixiang Ye and Fan Tang and Weiming Dong and Changsheng Xu},
keywords = {Object detection, Semi-supervised learning, 2D Semi-supervised object detection, 3D Semi-supervised object detection, Self-labeling},
abstract = {Semi-supervised object detection has recently achieved substantial progress. As a mainstream solution, the self-labeling-based methods train the detector on both labeled data and unlabeled data with pseudo labels predicted by the detector itself, but their performances are always limited. Through experimental analysis, we reveal the underlying reason is that the detector is misguided by the incorrect pseudo labels predicted by itself (dubbed self-errors). These self-errors can hurt performance even worse than random-errors, and can be neither discerned nor rectified during the self-labeling process. In this paper, we propose an effective detection framework named CrossRectify, to obtain accurate pseudo labels by simultaneously training two detectors with different initial parameters. Specifically, the proposed approach leverages the disagreements between detectors to discern the self-errors and refines the pseudo label quality by the proposed cross-rectifying mechanism. Extensive experiments show that CrossRectify achieves outperforming performances over various detector structures on 2D and 3D detection benchmarks.}
}
@article{SHIFATERABBI2023109268,
title = {Invariance encoding in sliced-Wasserstein space for image classification with limited training data},
journal = {Pattern Recognition},
volume = {137},
pages = {109268},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109268},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007476},
author = {Mohammad Shifat-E-Rabbi and Yan Zhuang and Shiying Li and Abu Hasnat Mohammad Rubaiyat and Xuwang Yin and Gustavo K. Rohde},
keywords = {R-CDT, Mathematical model, Generative model, Invariance learning},
abstract = {Deep convolutional neural networks (CNNs) are broadly considered to be state-of-the-art generic end-to-end image classification systems. However, they are known to underperform when training data are limited and thus require data augmentation strategies that render the method computationally expensive and not always effective. Rather than using a data augmentation strategy to encode invariances as typically done in machine learning, here we propose to mathematically augment a nearest subspace classification model in sliced-Wasserstein space by exploiting certain mathematical properties of the Radon Cumulative Distribution Transform (R-CDT), a recently introduced image transform. We demonstrate that for a particular type of learning problem, our mathematical solution has advantages over data augmentation with deep CNNs in terms of classification accuracy and computational complexity, and is particularly effective under a limited training data setting. The method is simple, effective, computationally efficient, non-iterative, and requires no parameters to be tuned. Python code implementing our method is available at https://github.com/rohdelab/mathematical_augmentation. Our method is integrated as a part of the software package PyTransKit, which is available at https://github.com/rohdelab/PyTransKit.}
}
@article{WANG2023109296,
title = {Toward a blind image quality evaluator in the wild by learning beyond human opinion scores},
journal = {Pattern Recognition},
volume = {137},
pages = {109296},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109296},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007750},
author = {Zhihua Wang and Zhi-Ri Tang and Jianguo Zhang and Yuming Fang},
keywords = {Blind image quality assessment, Opinion-free, Pseudo binary label, Unsupervised domain adaptation, gMAD competition},
abstract = {Nowadays, most existing blind image quality assessment (BIQA) models inthewild heavily rely on human ratings, which are extraordinarily labor-expensive to collect. Here, we propose an opinion−free BIQA method that learns from multiple annotators to assess the perceptual quality of images captured in the wild. Specifically, we first synthesize distorted images based on the pristine counterparts. We then randomly assemble a set of image pairs from the synthetic images, and use a group of IQA models to assign pseudo-binary labels for each pair indicating which image has higher quality as the supervisory signal. Based on the newly established pseudo-labeled dataset, we train a deep neural network (DNN)-based BIQA model to rank the perceptual quality, optimized for consistency with the binary rank labels. Since there exists domain shift, e.g., distortion shift and content shift, between the synthetic and in-the-wild images, we leverage two ways to alleviate this issue. First, the simulated distortions should be similar to authentic distortions as much as possible. Second, an unsupervised domain adaptation (UDA) module is further applied to encourage learning domain-invariant features between two domains. Extensive experiments demonstrate the effectiveness of our proposed opinion−free BIQA model, yielding SOTA performance in terms of correlation with human opinion scores, as well as gMAD competition. Our code is available at: https://github.com/wangzhihua520/OF_BIQA.}
}
@article{LABER2023109239,
title = {Shallow decision trees for explainable k-means clustering},
journal = {Pattern Recognition},
volume = {137},
pages = {109239},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109239},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200718X},
author = {Eduardo Laber and Lucas Murtinho and Felipe Oliveira},
keywords = {Clustering, Explainability, K-means, Decision trees},
abstract = {A number of recent works have employed decision trees for the construction of explainable partitions that aim to minimize the k-means cost function. These works, however, largely ignore metrics related to the depths of the leaves in the resulting tree, which is perhaps surprising considering how the explainability of a decision tree depends on these depths. To fill this gap in the literature, we propose an efficient algorithm with a penalty term in its loss function to favor the construction of shallow decision trees – i.e., trees whose leaves are not very deep, which translate to clusters that are defined by a small number of attributes and are therefore easier to explain. In experiments on 16 datasets, our algorithm yields better results than decision-tree clustering algorithms recently presented in the literature, typically achieving lower or equivalent costs with considerably shallower trees.}
}
@article{CELESTINO2023109288,
title = {2D Image head pose estimation via latent space regression under occlusion settings},
journal = {Pattern Recognition},
volume = {137},
pages = {109288},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109288},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007671},
author = {José Celestino and Manuel Marques and Jacinto C. Nascimento and João Paulo Costeira},
keywords = {Head pose estimation, Occlusion, Latent space, Euler angles},
abstract = {Head orientation is a challenging Computer Vision problem that has been extensively researched having a wide variety of applications. However, current state-of-the-art systems still underperform in the presence of occlusions and are unreliable for many task applications in such scenarios. This work proposes a novel deep learning approach for the problem of head pose estimation under occlusions. The strategy is based on latent space regression as a fundamental key to better structure the problem for occluded scenarios. Our model surpasses several state-of-the-art methodologies for occluded HPE, and achieves similar accuracy for non-occluded scenarios. We demonstrate the usefulness of the proposed approach with: (i) two synthetically occluded versions of the BIWI and AFLW2000 datasets, (ii) real-life occlusions of the Pandora dataset, and (iii) a real-life application to human-robot interaction scenarios where face occlusions often occur. Specifically, the autonomous feeding from a robotic arm.}
}
@article{LI2023109306,
title = {DeepSIR: Deep semantic iterative registration for LiDAR point clouds},
journal = {Pattern Recognition},
volume = {137},
pages = {109306},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109306},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000079},
author = {Qing Li and Cheng Wang and Chenglu Wen and Xin Li},
keywords = {Feature learning, 3D registration, LiDAR point clouds, Point score, Semantic segmentation},
abstract = {This paper proposes DeepSIR, a novel learning-based iterative registration framework for real-world 3D LiDAR point clouds. Specifically, a front-end semantic feature extraction (Semantic-feat) model is designed to fully explore semantic information in LiDAR data. To highlight the recognized objects of interest, we propose a novel point score that uses semantic and geometric information. To effectively integrate the extracted semantic features, geometric features, and point scores, we introduce an aggregation module to learn a hybrid feature on each point. Meanwhile, our method dynamically explores feature descriptions and optimizes poses through an iterative pipeline. Extensive experiments on outdoor driving datasets demonstrate that our DeepSIR achieves comparable performance to state-of-the-art methods and runs at a much faster speed. The source code will be made publicly available.11https://github.com/LeoQLi/DeepSIR}
}
@article{LIU2023109284,
title = {The Performance Index of Convolutional Neural Network-Based Classifiers in Class Imbalance Problem},
journal = {Pattern Recognition},
volume = {137},
pages = {109284},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109284},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007634},
author = {Yanchen Liu and King Wai Chiu Lai},
keywords = {Deep Learning, Convolutional Neural Network, Class Imbalance, Class Balance Index, Model Performance Index},
abstract = {Class imbalance is a common problem in many classification domains. This paper provides an evaluation index and one algorithm for this problem based on binary classification. The Model Performance Index (MPI) is proposed for assessing classifier performance as a new evaluation metric, considering class imbalance impacts. Based on MPI, we investigate algorithms to estimate ideal classifier performance with a fair distribution (1:1), referred to as the Ideal Model Performance Algorithm. Experimentally, compared with traditional metrics, MPI is more sensitive. Specifically, it can detect all types of changes in classifier performances, while others might remain at the same levels. Moreover, for the estimation of classifier performances, the algorithm reaches small differences between predictions and the values observed. Generally, for ideal performances, it achieved error rates of 0.060% - 1.3% for rare class in four experiments, showing a practical value on estimation and representation on the classifier performances.}
}
@article{FU2023109310,
title = {Knowledge aggregation networks for class incremental learning},
journal = {Pattern Recognition},
volume = {137},
pages = {109310},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109310},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000110},
author = {Zhiling Fu and Zhe Wang and Xinlei Xu and Dongdong Li and Hai Yang},
keywords = {Class incremental learning, Catastrophic forgetting, Dual-branch network, Knowledge aggregation, Model compression},
abstract = {Most existing class incremental learning methods rely on storing old exemplars to avoid catastrophic forgetting. However, these methods inevitably face the gradient conflict problem, the inherent conflict between new streaming knowledge and existing knowledge in the gradient direction. To alleviate gradient conflict, this paper reuses the previous knowledge and expands the branch to accommodate new concepts instead of fine-tuning the original models. Specifically, this paper designs a novel dual-branch network called Knowledge Aggregation Networks. The previously trained model is frozen as a branch to retain existing knowledge, and a consistent trainable network is constructed as the other branch to learn new concepts. An adaptive feature fusion module is adopted to dynamically balance the two branches’ information during training. Moreover, a model compression stage maintains the dual-branch structure. Extensive experiments on CIFAR-100, ImageNet-Sub, and ImageNet show that our method significantly outperforms the other methods and effectively balances stability and plasticity.}
}
@article{SONG2023109276,
title = {Deep continual hashing with gradient-aware memory for cross-modal retrieval},
journal = {Pattern Recognition},
volume = {137},
pages = {109276},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109276},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007555},
author = {Ge Song and Xiaoyang Tan and Ming Yang},
keywords = {Cross-modal retrieval, Deep hashing, Continual learning, Multi-label},
abstract = {Cross-modal hashing (CMH) has become widely used for large-scale multimedia retrieval. However, most current CMH methods focus on the closed retrieval scenario, not the real-world environments, i.e., complex and changing semantics. When data containing new class objects emerge, the current CMH has to retrain the model on all history training data, not the new data, to accommodate new semantics, but the never-stop upload of data on the Internet makes this impractical. In this paper, we devise a deep hashing method called Continual Cross-Modal Hashing with Gradient Aware Memory (CCMH-GAM) for learning binary codes of multi-label cross-modal data with increasing categories. CCMH-GAM is a two-step hashing architecture, one hashing network learns to hash the increasing semantics of data, i.e., label, into the semantic codes, and other modality-specific hashing networks learn to map data into the corresponding semantic codes. Specifically, to keep the encoding ability for old semantics, a regularization based on accumulating low-storage label-code pairs is designed for the former network. For the modality-specific networks, we propose a memory construction method via approximating the full episodic gradients of all data by some exemplars and derive its fast implementation with the upper bound of approximation error. Based on this memory, we propose a gradient projection method to theoretically improve the probability of old data’s code being unchanged after updating the model. Extensive experiments on three datasets demonstrate that CCMH-GAM can continually learn hash functions and yield state-of-the-art retrieval performance.}
}
@article{CAO2023109302,
title = {Poincaré Fréchet mean},
journal = {Pattern Recognition},
volume = {137},
pages = {109302},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109302},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000031},
author = {Xiaofeng Cao},
keywords = {Fréchet mean, Hyperbolic geometry, Poincaré model, Minimizing upper bound, ()-approximation},
abstract = {Generalizing the Fréchet mean from the Euclidean metric is not able to properly capture the geometric characteristics of many non-trivial operations, such as the non-dot inner product and non-Euclidean gradients defined on the manifold. One effective solution is to derive its hyperbolic representations in the Poincaré or hyperboloid model. Our goodness-of-fit testing shows that the Poincaré Fréchet mean achieves much lower χ2 power than that of the hyperboloid and typical non-linear kernels with regard to parameter perturbations. However, recent advanced optimization solvers, such as Riemannian gradient descent and minimizing upper bound, may result in imprecise convergences. This paper presents an (1−ϵ)-approximation approach to search a core-set on the Poincaré model, reducing deviations of the Poincaré Fréchet mean to its optimum. A hierarchical splitting algorithm that implicitly explores the hyperbolic representations for an arbitrary manifold is then presented. Experiments show that the (1−ϵ) Poincaré Fréchet mean adopted in hierarchical splitting, achieves better representations than Euclidean, kernel, and Lorentzian Fréchet means in graph and image data.}
}
@article{MEI2023109264,
title = {Multi-order similarity learning for multi-view spectral clustering},
journal = {Pattern Recognition},
volume = {137},
pages = {109264},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109264},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007439},
author = {Yanying Mei and Zhenwen Ren and Bin Wu and Tao Yang and Yanhua Shao},
keywords = {Spectral clustering, Multi-view clustering, Multi-order similarity, Graph learning, Tensor},
abstract = {This paper explores the problem of multi-view spectral clustering (MVSC) based on multi-order similarity learning. Unlike the existing methods that focus on direct similarity of pairwise data points without considering the hidden multi-order similarity among different data points, a novel multi-order similarity learning model for MVSC (MOSL) is proposed. Specifically, the first-order similarity (FOS) and second-order similarity (SOS) are learned to excavate the local structure relation and adjacent structure relation of pairwise data points. Afterwards, the third-order similarity (TOS) based on low-rank tensor is learned to excavate the view-specific information and consensus information from multiple views. Moreover, a trace constraint on each affinity graph from multiple views is learned to ensure the strict block diagonal structure of each affinity graph. Extensive experiments on six commonly benchmark datasets show that the proposed method outperforms state-of-the-art methods in most scenarios and is capable of revealing a reliable affinity graph structure concealed in different data points.}
}
@article{KIM2023109292,
title = {Uncertainty-aware semi-supervised few shot segmentation},
journal = {Pattern Recognition},
volume = {137},
pages = {109292},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109292},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007713},
author = {Soopil Kim and Philip Chikontwe and Sion An and Sang Hyun Park},
keywords = {Few shot segmentation, Meta learning, Uncertainty estimation, Semi-supervised learning, Prototype},
abstract = {Few shot segmentation (FSS) aims to learn pixel-level classification of a target object in a query image using only a few annotated support samples. This is challenging as it requires modeling appearance variations of target objects and the diverse visual cues between query and support images with limited information. To address this problem, we propose a semi-supervised FSS strategy that leverages additional prototypes from unlabeled images with uncertainty guided pseudo label refinement. To obtain reliable prototypes from unlabeled images, we meta-train a neural network to jointly predict segmentation and estimate the uncertainty of predictions. We employ the uncertainty estimates to exclude predictions with high degrees of uncertainty for pseudo label construction to obtain additional prototypes from the refined pseudo labels. During inference, query segmentation is predicted using prototypes from both support and unlabeled images including low-level features of the query images. Our approach can easily supplement existing approaches without the requirement of additional training when employing unlabeled samples. Extensive experiments on PASCAL-5i and COCO-20i demonstrate that our model can effectively remove unreliable predictions to refine pseudo labels and significantly improve upon baseline performance.}
}
@article{DUAN2023109318,
title = {Arbitrary Order Total Variation for Deformable Image Registration},
journal = {Pattern Recognition},
volume = {137},
pages = {109318},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109318},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000195},
author = {Jinming Duan and Xi Jia and Joseph Bartlett and Wenqi Lu and Zhaowen Qiu},
keywords = {Image Registration, Nonlinear Optimisation, ADMM, Total Variation, Arbitrary Order Derivatives},
abstract = {In this work, we investigate image registration in a variational framework and focus on regularization generality and solver efficiency. We first propose a variational model combining the state-of-the-art sum of absolute differences (SAD) and a new arbitrary order total variation regularization term. The main advantage is that this variational model preserves discontinuities in the resultant deformation while being robust to outlier noise. It is however non-trivial to optimize the model due to its non-convexity, non-differentiabilities, and generality in the derivative order. To tackle these, we propose to first apply linearization to the model to formulate a convex objective function and then break down the resultant convex optimization into several point-wise, closed-form subproblems using a fast, over-relaxed alternating direction method of multipliers (ADMM). With this proposed algorithm, we show that solving higher-order variational formulations is similar to solving their lower-order counterparts. Extensive experiments show that our ADMM is significantly more efficient than both the subgradient and primal-dual algorithms particularly when higher-order derivatives are used, and that our new models outperform state-of-the-art methods based on deep learning and free-form deformation. Our code implemented in both Matlab and Pytorch is publicly available at https://github.com/j-duan/AOTV.}
}
@article{RASOOL2023109287,
title = {Overcoming weaknesses of density peak clustering using a data-dependent similarity measure},
journal = {Pattern Recognition},
volume = {137},
pages = {109287},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109287},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200766X},
author = {Zafaryab Rasool and Sunil Aryal and Mohamed Reda Bouadjenek and Richard Dazeley},
keywords = {Clustering, Density peak clustering, Similarity measure, Data-dependent similarity},
abstract = {Density Peak Clustering (DPC) is a popular state-of-the-art clustering algorithm, which requires pairwise (dis)similarity of data objects to detect arbitrary shaped clusters. While it is shown to perform well for many applications, DPC remains: (i) not robust for datasets with clusters having different densities, and (ii) sensitive to the change in the units/scales used to represent data. These drawbacks are mainly due to the use of the data-independent similarity measure based on the Euclidean distance. In this paper, we address these issues by proposing an effective data-dependent similarity measure based on Probability Mass, which we call MP-Similarity, and by incorporating it in DPC to create MP-DPC, a data-dependent variant of DPC. We evaluate and compare MP-DPC against diverse baselines using several clustering metrics and datasets. Our experiments demonstrate that: (a) MP-DPC produces better clustering results than DPC using the Euclidean distance and existing data-dependent similarity measures; (b) MP-Similarity coupled with Shared-Nearest-Neighbor-based density metric in DPC further enhances the quality of clustering results; and (c) unlike DPC with existing data-independent and data-dependent similarity measures, MP-DPC is robust to the change in the units/scales used to represent data. Our findings suggest that MP-Similarity provides a more viable solution for DPC in datasets with unknown distribution or units/scales of features, which is often the case in many real-world applications.}
}
@article{GONG2023109272,
title = {Improving visual-semantic embeddings by learning semantically-enhanced hard negatives for cross-modal information retrieval},
journal = {Pattern Recognition},
volume = {137},
pages = {109272},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109272},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007518},
author = {Yan Gong and Georgina Cosma},
keywords = {Visual semantic embedding network, Cross-modal, Information retrieval, Hard negatives},
abstract = {Visual Semantic Embedding (VSE) networks aim to extract the semantics of images and their descriptions and embed them into the same latent space for cross-modal information retrieval. Most existing VSE networks are trained by adopting a hard negatives loss function which learns an objective margin between the similarity of relevant and irrelevant image–description embedding pairs. However, the objective margin in the hard negatives loss function is set as a fixed hyperparameter that ignores the semantic differences of the irrelevant image–description pairs. To address the challenge of measuring the optimal similarities between image–description pairs before obtaining the trained VSE networks, this paper presents a novel approach that comprises two main parts: (1) finds the underlying semantics of image descriptions; and (2) proposes a novel semantically-enhanced hard negatives loss function, where the learning objective is dynamically determined based on the optimal similarity scores between irrelevant image–description pairs. Extensive experiments were carried out by integrating the proposed methods into five state-of-the-art VSE networks that were applied to three benchmark datasets for cross-modal information retrieval tasks. The results revealed that the proposed methods achieved the best performance and can also be adopted by existing and future VSE networks.}
}
@article{LV2023109301,
title = {Semi-supervised node classification via fine-grained graph auxiliary augmentation learning},
journal = {Pattern Recognition},
volume = {137},
pages = {109301},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109301},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300002X},
author = {Jia Lv and Kaikai Song and Qiang Ye and Guangjian Tian},
keywords = {Graph neural network, Node classification, Data augmentation, Auxiliary learning},
abstract = {Node classification has become an important research topic in recent years. Since there are always a few training samples, researchers improve the performance by properly leveraging the predictions of unlabeled nodes during training. However, suffering from the model degradation resulted from the accumulative error of pseudo-labels, there is limited improvement. In this paper we present fine-grained Graph Auxiliary aUgmentation (GAU). It trains the primary task together with an automatically created auxiliary task which is a fine-grained node classification task. And an auxiliary augmentation strategy is designed to enlarge the labeled set for the auxiliary task by utilizing the pseudo-labels of the primary task. Comprehensive experiments show that GAU alleviates the sensitivity of the model to the pseudo-label quality, so more unlabeled nodes can participate in the training. From the perspective of co-training, the fine-grained auxiliary task which is trained by much more unlabeled nodes helps to learn better node representations from a different view, thereby boosting the final performance. Extensive experiments verify the superior performance of the GAU on different GNN architectures when compared with other state-of-the-art approaches.}
}
@article{CHOWDHURY2023109314,
title = {Feature weighting in DBSCAN using reverse nearest neighbours},
journal = {Pattern Recognition},
volume = {137},
pages = {109314},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109314},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000158},
author = {Stiphen Chowdhury and Na Helian and Renato {Cordeiro de Amorim}},
keywords = {Density-based clustering, Reverse nearest neighbour, DBSCAN},
abstract = {DBSCAN is arguably the most popular density-based clustering algorithm, and it is capable of recovering non-spherical clusters. One of its main weaknesses is that it treats all features equally. In this paper, we propose a density-based clustering algorithm capable of calculating feature weights representing the degree of relevance of each feature, which takes the density structure of the data into account. First, we improve DBSCAN and introduce a new algorithm called DBSCANR. DBSCANR reduces the number of parameters of DBSCAN to one. Then, a new step is introduced to the clustering process of DBSCANR to iteratively update feature weights based on the current partition of data. The feature weights produced by the weighted version of the new clustering algorithm, W-DBSCANR, measure the relevance of variables in a clustering and can be used in feature selection in data mining applications where large and complex real-world data are often involved. Experimental results on both artificial and real-world data have shown that the new algorithms outperformed various DBSCAN type algorithms in recovering clusters in data.}
}