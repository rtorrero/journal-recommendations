@article{LIU2023109048,
title = {Unauthorized AI cannot recognize me: Reversible adversarial example},
journal = {Pattern Recognition},
volume = {134},
pages = {109048},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109048},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005283},
author = {Jiayang Liu and Weiming Zhang and Kazuto Fukuchi and Youhei Akimoto and Jun Sakuma},
keywords = {Adversarial example, Reversible data hiding, AI security},
abstract = {In this study, we propose a new methodology to control how user’s data is recognized and used by AI via exploiting the properties of adversarial examples. For this purpose, we propose reversible adversarial example (RAE), a new type of adversarial example. A remarkable feature of RAE is that the image can be correctly recognized and used by the AI model specified by the user because the authorized AI can recover the original image from the RAE exactly by eliminating adversarial perturbation. On the other hand, other unauthorized AI models cannot recognize it correctly because it functions as an adversarial example. Moreover, RAE can be considered as one type of encryption to computer vision since reversibility guarantees the decryption. To realize RAE, we combine three technologies, adversarial example, reversible data hiding for exact recovery of adversarial perturbation, and encryption for selective control of AIs who can remove adversarial perturbation. Experimental results show that the proposed method can achieve comparable attack ability with the corresponding adversarial attack method and similar visual quality with the original image, including white-box attacks and black-box attacks.}
}
@article{LI2023109129,
title = {Dirichlet process mixture of Gaussian process functional regressions and its variational EM algorithm},
journal = {Pattern Recognition},
volume = {134},
pages = {109129},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109129},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006094},
author = {Tao Li and Jinwen Ma},
keywords = {Gaussian process, Dirichlet process, Functional data, Non-parametric Bayesian model, EM algorithm, Variational inference, Multi-modal data},
abstract = {Gaussian Process Functional Regression (GPFR) is a powerful tool in functional data analysis. In practical applications, functional data may be generated from different signal sources, and a single GPFR is not flexible enough to accurately model the data. To tackle the heterogeneity problem, a finite mixture of Gaussian Process Functional Regressions (mix-GPFR) was suggested. However, the number of components in mix-GPFR needs to be specified a priori, which is difficult to determine in practice. In this paper, we propose a Dirichlet Process Mixture of Gaussian Process Functional Regressions (DPM-GPFR), in which there are potentially infinite many GPFR components dominated by a Dirichlet process. Thus, DPM-GPFR is far more flexible than a single GPFR, and sidestep the model selection problem in mix-GPFR. We further develop a fully Bayesian treatment for learning DPM-GPFR based on the Variational Expectation-Maximization (VEM) algorithm. Experimental results on both synthetic datasets and real-world datasets demonstrate the effectiveness of our proposed method.}
}
@article{PINTOR2023109064,
title = {ImageNet-Patch: A dataset for benchmarking machine learning robustness against adversarial patches},
journal = {Pattern Recognition},
volume = {134},
pages = {109064},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109064},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005441},
author = {Maura Pintor and Daniele Angioni and Angelo Sotgiu and Luca Demetrio and Ambra Demontis and Battista Biggio and Fabio Roli},
keywords = {Adversarial machine learning, Adversarial patches, Neural networks, Defense, Detection},
abstract = {Adversarial patches are optimized contiguous pixel blocks in an input image that cause a machine-learning model to misclassify it. However, their optimization is computationally demanding, and requires careful hyperparameter tuning, potentially leading to suboptimal robustness evaluations. To overcome these issues, we propose ImageNet-Patch, a dataset to benchmark machine-learning models against adversarial patches. The dataset is built by first optimizing a set of adversarial patches against an ensemble of models, using a state-of-the-art attack that creates transferable patches. The corresponding patches are then randomly rotated and translated, and finally applied to the ImageNet data. We use ImageNet-Patch to benchmark the robustness of 127 models against patch attacks, and also validate the effectiveness of the given patches in the physical domain (i.e., by printing and applying them to real-world objects). We conclude by discussing how our dataset could be used as a benchmark for robustness, and how our methodology can be generalized to other domains. We open source our dataset and evaluation code at https://github.com/pralab/ImageNet-Patch.}
}
@article{ALALIMI2023109096,
title = {IDA: Improving distribution analysis for reducing data complexity and dimensionality in hyperspectral images},
journal = {Pattern Recognition},
volume = {134},
pages = {109096},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109096},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005763},
author = {Dalal AL-Alimi and Mohammed A.A. Al-qaness and Zhihua Cai and Eman Ahmed Alawamy},
keywords = {Feature reduction, Hyperspectral image, Classification, Feature fusion, Feature extraction, Dimensionality reduction},
abstract = {Hyperspectral images (HSIs) are known for their high dimensionality and wide spectral bands that increase redundant information and complicate classification. Outliers and mixed data are common problems in HSIs. Thus, preprocessing methods are essential in enhancing and reducing data complexity, redundant information, and the number of bands. This study introduces a novel feature reduction method (FRM) called improving distribution analysis (IDA). IDA works to increase the correlation between related data, decrease the distance between big and small data, and correct each value's location to be inside its group range. In IDA, the input data passes through three stages. Getting rid of outliers and improving data correlation is the first step. The second stage involves increasing the variance. The third is to simplify the data and normalize the distribution. IDA is compared with four popular FRMs in four available HSIs. It is also tested and evaluated in various classification models, including spatial, spectral, and spectral-spatial models. The experimental results demonstrate that IDA performs admirably in enhancing data distribution, reducing complexity, and accelerating performance.}
}
@article{XI2023109068,
title = {Learning comprehensive global features in person re-identification: Ensuring discriminativeness of more local regions},
journal = {Pattern Recognition},
volume = {134},
pages = {109068},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109068},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005489},
author = {Jiali Xi and Jianqiang Huang and Shibao Zheng and Qin Zhou and Bernt Schiele and Xian-Sheng Hua and Qianru Sun},
keywords = {Person re-identification, Baseline, Comprehensive},
abstract = {Person re-identification (Re-ID) aims to retrieve person images from a large gallery given a query image of a person of interest. Global information and fine-grained local features are both essential for the representation. However, global embedding learned by naive classification model tends to be trapped in the most discriminative local region, leading to poor evaluation performance. To address the issue, we propose a novel baseline network that learns strong global feature termed as Comprehensive Global Embedding (CGE), ensuring more local regions of global feature maps to be discriminative. In this work, two key modules are proposed including Non-parameterized Local Classifier (NLC) and Global Logits Revise (GLR). The NLC is designed to obtain a score vector of each local region on feature maps in a non-parametric manner. The GLR module directly revises the logits such that the subsequent cross entropy loss up-weights the loss assigned to samples with hard-to-learn local regions. The convergence of the deep model indicates more local regions (the number of local regions is manually defined) on the feature maps of each sample are discriminative. We implement these two modules on two strong baseline methods including the BagTricks (BOT) [1] and AGW [2]. The network achieves 65.9% mAP, 85.1% rank1 on MSMT17, 86.4% mAP, 87.4% rank1 on CUHK03 labeled, 84.2% mAP, 85.9% rank1 on CUHK03 detected, and 92.2% mAP, 96.3% rank1 on Market-1501. The results show that the proposed baseline achieves a new state-of-the-art when using only global embedding during inference without any re-ranking technique.}
}
@article{GE2023109088,
title = {Unsupervised Domain Adaptation via Deep Conditional Adaptation Network},
journal = {Pattern Recognition},
volume = {134},
pages = {109088},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109088},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005684},
author = {Pengfei Ge and Chuan-Xian Ren and Xiao-Lin Xu and Hong Yan},
keywords = {Deep learning, Domain adaptation, Feature extraction, Conditional maximum mean discrepancy, Kernel method},
abstract = {Unsupervised domain adaptation (UDA) aims to generalize the supervised model trained on a source domain to an unlabeled target domain. Previous works mainly rely on the marginal distribution alignment of feature spaces, which ignore the conditional dependence between features and labels, and may suffer from negative transfer. To address this problem, some UDA methods focus on aligning the conditional distributions of feature spaces. However, most of these methods rely on class-specific Maximum Mean Discrepancy or adversarial training, which may suffer from mode collapse and training instability. In this paper, we propose a Deep Conditional Adaptation Network (DCAN) that aligns the conditional distributions by minimizing Conditional Maximum Mean Discrepancy, and extracts discriminant information from the target domain by maximizing the mutual information between samples and the prediction labels. Conditional Maximum Mean Discrepancy measures the difference between conditional distributions directly through their conditional embedding in Reproducing Kernel Hilbert Space, thus DCAN can be trained stably and converge fast. Mutual information can be expressed as the difference between the entropy and conditional entropy of the predicted category variable, thus DCAN can extract the discriminant information of individual and overall distributions in the target domain, simultaneously. In addition, DCAN can be used to address a special scenario, Partial UDA, where the target domain category is a subset of the source domain category. Experiments on both UDA and Partial UDA show that DCAN achieves superior classification performance over state-of-the-art methods.}
}
@article{2023109162,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {134},
pages = {109162},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(22)00641-0},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006410}
}
@article{FAN2023109133,
title = {GraphDPI: Partial label disambiguation by graph representation learning via mutual information maximization},
journal = {Pattern Recognition},
volume = {134},
pages = {109133},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109133},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006136},
author = {Jinfu Fan and Yang Yu and Linqing Huang and Zhongjie Wang},
keywords = {Partial label learning, GraphDPI, Mutual Information, Triplet loss},
abstract = {Partial label learning (PLL) is a weakly supervised learning framework where each training instance is associated with more than one candidate label, and only one of them is the true label. Most of the existing PLL algorithms directly disambiguate the candidate labels according to the instance feature similarity, but fail to discover the latent semantic relationship over the entire dataset. In this paper, method GraphDPI, an innovative deep partial label disambiguation by graph representation via mutual information maximization, is proposed. This method can capture the semantic clusters with the most unique information in the latent space and automatically adapt to different feature distributions. Specifically, a new sampling method based on the graph is proposed to estimate mutual information, extending GCN to the field of weakly supervised learning. Therefore, the graph representation of the data can contain more distinguishing information to disambiguate candidate labels by maximizing the mutual information of the local graph representation and the global one. Furthermore, the triplet loss is introduced to fully exploit the relationship between instances and extract the latent embedding representation over the entire dataset. It thereby can make the model output as large as possible on the inter-class variation and as small as possible on the intra-class variation. Finally, the candidate labels can be disambiguated by the difference between semantic clusters. Experiments reveal the overwhelming performances of GraphDPI.}
}
@article{LI2023109072,
title = {SC-GAN: Subspace Clustering based GAN for Automatic Expression Manipulation},
journal = {Pattern Recognition},
volume = {134},
pages = {109072},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109072},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005520},
author = {Shuai Li and Liang Liu and Ji Liu and Wenfeng Song and Aimin Hao and Hong Qin},
keywords = {Facial attribute manipulation, GANs, Subspace clustering, SIFT K-means cluster},
abstract = {In recent years, the topics of facial attribute manipulation and decomposition have gained great popularity in computer vision and human computer interaction. Even though such methods have been preliminarily employed in some photo beautification applications, it still remains challenging due to the highly-versatile facial attributes and their drastic appearance changes subject to variation of deformation, illumination, pose, etc. The prevailing problems are especially severe when we are faced with group photos involving many faces. To overcome such critical limitations and discover more meaningful visual attributes and their possible decompositions, we develop a subspace clustering based generative adversarial network (SC-GAN) in this paper. Our SC-GAN can simultaneously decompose multiple subspaces and generate diverse samples correspondingly, thus the training of the generative models could be more effectively guided by facial attribute and its decomposition and manipulation in a natural and meaningful fashion. Our SC-GAN incorporates the SIFT K-means cluster, which could split the holistic semantic facial space into different subspaces without supervision, and help the new GAN generate more convincing results within specific subspaces. Extensive experiments and comprehensive evaluations confirm that, our method can greatly reduce the unexpected influences caused by portrait diversities and outperform the state-of-the-art facial attribute manipulation approaches.11https://github.com/buaaswf/SC-GAN/}
}
@article{LI2023109060,
title = {A framework based on local cores and synthetic examples generation for self-labeled semi-supervised classification},
journal = {Pattern Recognition},
volume = {134},
pages = {109060},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109060},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005404},
author = {Junnan Li and MingQiang Zhou and Qingsheng Zhu and Quanwang Wu},
keywords = {Semi-supervised learning, Semi-supervised classification, Self-labeled techniques, Examples generation, Representatives},
abstract = {Self-labeled techniques are semi-supervised classification models that overcome the shortage of labeled samples via an iterative process. Most relevant proposals are inspired by boosting schemes to iteratively enlarge labeled data, but these methods are constrained by the number and distribution of the initial labeled data. Up to the present, the only exceptions which can solve the above problem are SEG-SSC, k-means-SSC and LC-SSC. However, SEG-SSC relies on too many parameters. Besides, it is hard to improve the distribution of the initial labeled data when the initial labeled set can not roughly represent the distribution of the original data. k-means-SSC and LC-SSC fail to significantly improve the number of the initial labeled data by a limited number of representative points. To address the above issues, this paper proposes a framework based on local cores and synthetic examples generation for self-labeled semi-supervised classification (LCSEG-SSC). First, a new method for finding local cores on labeled and unlabeled data is proposed to improve the distribution of the initial labeled data. Second, STOPF or active labeling is used to predict found local cores. Third, a new example generation technique is proposed to create synthetic labeled samples, intending to improve the number of the initial labeled data. After that, any self-labeled with boosting schemes can be executed on the improved labeled data effectively. Intensive experiments prove that LCSEG-SSC outperforms state-of-the-art methods, especially in a relatively low ratio of labeled data.}
}
@article{YAO2023109084,
title = {Regularizing autoencoders with wavelet transform for sequence anomaly detection},
journal = {Pattern Recognition},
volume = {134},
pages = {109084},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109084},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005647},
author = {Yueyue Yao and Jianghong Ma and Yunming Ye},
keywords = {Sequence anomaly detection, Autoencoder, Discrete wavelet transform, Frequency domain regularization, Sample-adaptive regularization weight},
abstract = {Nowadays, systems or entities are usually monitored by devices, generating large amounts of time series. Detecting anomalies in them help prevent potential losses, thus arousing much research interest. Existing studies have adopted autoencoders to detect anomalies, where reconstruction errors are used to indicate outliers. However, sometimes autoencoders may also reconstruct anomalies well due to the learned general features in latent spaces. To solve the above problem, we propose to regularize autoencoders to grasp specific features of normal sequences. Specifically, spectral unique patterns are captured by statistical analysis on discrete wavelet transform (DWT) coefficients of input sequences, restricting latent spaces to reflect unique patterns of normal sequences in both time and frequency domains. Furthermore, a Weight Controller calculating sample-adaptive regularization weights is designed to fully utilize the regularization effect. Extensive experiments on three public benchmarks demonstrate the effectiveness and superiority of the proposed model compared with state-of-the-art algorithms.}
}
@article{CAI2023109063,
title = {Robust learning from noisy web data for fine-Grained recognition},
journal = {Pattern Recognition},
volume = {134},
pages = {109063},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109063},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200543X},
author = {Zhenhuang Cai and Guo-Sen Xie and Xingguo Huang and Dan Huang and Yazhou Yao and Zhenmin Tang},
keywords = {Fine-grained, Web-supervised, Noisy web data, Robust learning},
abstract = {Due to DNNs’ memorization effect, label noise lessens the performance of the web-supervised fine-grained visual categorization task. Previous literature primarily relies on small-loss instances for subsequent training. The current state-of-the-art approach JoCoR additionally employs explicit consistency constraints to make clean samples more confident. However, a joint loss designed for both sample selection criteria and parameter updating is not competent for training a robust model in the presence of web noise. Especially, false positives are assigned with larger weights, causing the model to pay more attention to misclassified noisy images. Besides, leveraging weight decay to forget discarded noisy instances is too slow and implicit to take effect. Therefore, we propose a simple yet effective approach named MS-DeJOR (Multi-Scale training with Decoupled Joint Optimization and Refurbishment). In contrast to JoCoR, we decouple sample selection from training procedure to handle the above problems. Specifically, a negative entropy term is applied to prevent false positives from being overemphasized. The model can explicitly forget those samples identified as noise by imposing such a regularization term on all training data. Furthermore, we use accumulated predictions to refurbish the noisy labels and re-weight training images to boost the model performance. A multi-scale feature enhancement module is adopted to extract discriminative and subtle feature representations. Extensive experiments show that MS-DeJOR yields state-of-the-art performances on three web-supervised fine-grained datasets, demonstrating the effectiveness of our approach. The data and source code have been available at https://github.com/msdejor/MS-DeJOR.}
}
@article{ZHANG2023109052,
title = {Density peaks clustering based on balance density and connectivity},
journal = {Pattern Recognition},
volume = {134},
pages = {109052},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109052},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005325},
author = {Qinghua Zhang and Yongyang Dai and Guoyin Wang},
keywords = {Clustering, Mutual nearest neighbor, Connectivity between data points, Fast search strategy},
abstract = {Density peaks clustering (DPC) algorithm regards the density peaks as the potential cluster centers, and assigns the non-center point into the cluster of its nearest higher-density neighbor. Although DPC can discover clusters with arbitrary shapes, it has some limitations. On the one hand, the density measure of DPC fails to eliminate the density difference among different clusters, which affects the accuracy of recognizing cluster center. On the other hand, the nearest higher-density point is determined without considering connectivity, which leads to continuously clustering errors. Therefore, DPC fails to obtain satisfactory clustering results on datasets with great density difference among clusters. In order to eliminate these limitations, a novel DPC algorithm based on balance density and connectivity (BC-DPC) is proposed. First, the balance density is proposed to eliminate the density difference among different clusters to accurately recognize cluster centers. Second, the connectivity between a data point and its nearest higher-density point is guaranteed by mutual nearest neighbor relationship to avoid continuously clustering errors. Finally, a fast search strategy is proposed to find the nearest higher-density point. The experimental results on synthetic, UCI, and image datasets demonstrate the efficiency and effectiveness of the proposed algorithm in this paper.}
}
@article{HAN2023109076,
title = {ML-DSVM+: A meta-learning based deep SVM+ for computer-aided diagnosis},
journal = {Pattern Recognition},
volume = {134},
pages = {109076},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109076},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005568},
author = {Xiangmin Han and Jun Wang and Shihui Ying and Jun Shi and Dinggang Shen},
keywords = {Deep neural network, Support vector machine plus, Learning using privileged information, Meta-learning},
abstract = {Transfer learning (TL) can improve the performance of a single-modal medical imaging-based computer-aided diagnosis (CAD) by transferring knowledge from related imaging modalities. Support vector machine plus (SVM+) is a supervised TL classifier specially designed for TL between the paired data in the source and target domains with shared labels. In this work, a novel deep neural network (DNN) based SVM+ (DSVM+) algorithm is proposed for single-modal imaging-based CAD. DSVM+ integrates the bi-channel DNNs and SVM+ classifier into a unified framework to improve the performance of both feature representation and classification. In particular, a new coupled hinge loss function is developed to conduct bidirectional TL between the source and target domains, which further promotes knowledge transfer together with the feature representation under the guidance of shared labels. To alleviate the overfitting caused by the increased parameters in DNNs for limited training samples, the meta-learning based DSVM+ (ML-DSVM+) is further developed, which designs randomly selecting samples from the training data instead of other CAD tasks for meta-tasks. This sampling strategy also can avoid the issue of class imbalance. ML-DSVM+ is evaluated on three medical imaging datasets. It achieves the best results of 88.26±1.40%, 90.45±5.00%, and 87.63±5.56% on accuracy, sensitivity and specificity, respectively, on the Bimodal Breast Ultrasound Image dataset, 90.00±1.05%, 72.55±3.87%, and 96.40±2.26% of the corresponding indices on the Alzheimer's Disease Neuroimaging Initiative dataset, and 85.76±3.12% of classification accuracy, 88.73±7.22% of sensitivity, and 82.60±1.56% of specificity for the Autism Brain Imaging Data Exchange dataset.}
}
@article{ROMEROMEDRANO2023109116,
title = {Multi-Source Change-Point Detection over Local Observation Models},
journal = {Pattern Recognition},
volume = {134},
pages = {109116},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109116},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005969},
author = {Lorena Romero-Medrano and Antonio Artés-Rodríguez},
keywords = {Change-point detection, Multi-source data, Heterogeneous data, Latent variable models},
abstract = {In this work, we address the problem of change-point detection (CPD) on high-dimensional, multi-source, and heterogeneous sequential data with missing values. We present a new CPD methodology based on local latent variable models and adaptive factorizations that enhances the fusion of multi-source observations with different statistical data-type and face the problem of high dimensionality. Our motivation comes from behavioral change detection in healthcare measured by smartphone monitored data and Electronic Health Records. Due to the high dimension of the observations and the differences in the relevance of each source information, other works fail in obtaining reliable estimates of the change-points location. This leads to methods that are not sensitive enough when dealing with interspersed changes of different intensity within the same sequence or partial missing components. Through the definition of local observation models (LOMs), we transfer the local CP information to homogeneous latent spaces and propose several factorizations that weight the contribution of each source to the global CPD. With the presented methods we demonstrate a reduction in both the detection delay and the number of not-detected CPs, together with robustness against the presence of missing values on a synthetic dataset. We illustrate its application on real-world data from a smartphone-based monitored study and add explainability on the degree of each source contributing to the detection.}
}
@article{WANG2023109100,
title = {Simultaneous Robust Matching Pursuit for Multi-view Learning},
journal = {Pattern Recognition},
volume = {134},
pages = {109100},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109100},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005805},
author = {Yulong Wang and Kit Ian Kou and Hong Chen and Yuan Yan Tang and Luoqing Li},
keywords = {Greedy algorithm, Multi-view learning, M-estimator, Sparse learning},
abstract = {Joint sparse representation (JSR) has attracted massive attention with many successful applications in pattern recognition recently. In this paper, we propose a novel robust multi-view JSR method referred to as Simultaneous Robust Matching Pursuit (SRMP) based on the outlier-resistant M-estimator originating from robust statistics. Because of the complexity of the objective function, we design an efficient optimization algorithm to implement SRMP based on the half-quadratic theory. In addition, we have also extended the proposed method for the problems of multi-view subspace clustering and multi-view pattern classification, respectively. The experimental results corroborate the efficacy and robustness of SRMP for multi-view data recovery, subspace clustering and classification.}
}
@article{GAO2023109111,
title = {A unified low-order information-theoretic feature selection framework for multi-label learning},
journal = {Pattern Recognition},
volume = {134},
pages = {109111},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109111},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200591X},
author = {Wanfu Gao and Pingting Hao and Yang Wu and Ping Zhang},
keywords = {Feature selection, Multi-label learning, Information theory, Low-order information-theoretic terms, Probability distribution assumption},
abstract = {The approximation of low-order information-theoretic terms for feature selection approaches has achieved success in addressing high-dimensional multi-label data. However, three critical issues exist in such type of approaches: (1) existing approaches are devised based on single heuristic variable correlation assumption, which biases towards some specific scene; (2) high-order variable correlations are ignored by cumulative summation low-order information-theoretic terms; (3) abundant approaches confuse researchers to devise and utilize appropriate approaches. To address these issues, two types of probability distribution assumption in terms of candidate features and labels are derived based on low-order variable correlations. Afterwords, clearing up all information-theoretic terms, we propose a unified feature selection framework including three low-order information-theoretic terms for multi-label learning named Selected Terms of Feature Selection (STFS). STFS contains high-order variable correlations in the form of low-order information-theoretic terms. Furthermore, many previous multi-label feature selection approaches can be reduced to special forms of STFS. Finally, extensive experiments conducted on twelve benchmark data sets in comparison to seven state-of-the-art approaches demonstrate the classification superiority of STFS.}
}
@article{SAHU2023109128,
title = {Egocentric video co-summarization using transfer learning and refined random walk on a constrained graph},
journal = {Pattern Recognition},
volume = {134},
pages = {109128},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109128},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006082},
author = {Abhimanyu Sahu and Ananda S. Chowdhury},
keywords = {Egocentric video, Transfer learning, Constrained graph, Random walks, Label refinement},
abstract = {In this paper, we address the problem of egocentric video co-summarization. We show how a shot level accurate summary can be obtained in a time-efficient manner using random walk on a constrained graph in transfer learned feature space with label refinement. While applying transfer learning, we propose a new loss function capturing egocentric characteristics in a pre-trained ResNet on the set of auxiliary egocentric videos. Transfer learning is used to generate i) an improved feature space and ii) a set of labels to be used as seeds for the test egocentric video. A complete weighted graph is created for a test video in the new transfer learned feature space with shots as the vertices. We derive two types of cluster label constraints in form of Must-Link (ML) and Cannot-link (CL) based on the similarity of the shots. ML constraints are used to prune the complete graph which is shown to result in substantial computational advantage, especially, for the long duration videos. We derive expressions for the number of vertices and edges for the ML-constrained graph and show that this graph remains connected. Random walk is applied to obtain labels of the unmarked shots in this new graph. CL constraints are applied to refine the cluster labels. Finally, shots closest to individual cluster centres are used to build the summary. Experiments on the short duration videos as in CoSum and TVSum datasets and long duration videos as in ADL and EPIC-Kitchens datasets clearly demonstrate the advantage of our solution over several state-of-the-art methods.}
}
@article{CHEN2023109124,
title = {An efficient point-set registration algorithm with dual terms based on total least squares},
journal = {Pattern Recognition},
volume = {134},
pages = {109124},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109124},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006045},
author = {Qing-Yan Chen and Da-Zheng Feng and Wei-Xing Zheng and Xiang-Wei Feng},
keywords = {Point-set registration, Total least squares, Dual terms, Errors-in-variables (EIV), Bilateral outliers},
abstract = {Point set registration (PSR) is competitive with related techniques because it purposefully captures the overall structure between two point-set patterns. Typically, the point set registration problem can be divided into two sub-problems: (1) search point set correspondence (PSC); (2) estimate spatial transformation matrix (STM). Searching for the best PSC is a classical combinatorial explosion problem, and estimating the STM is a continuous space optimization problem. Also, two feature point sets detected by two low-quality images include point-position errors and often involve bilateral outliers composed of such feature points that cannot form a correspondence relationship. To address the above problems, we propose an efficient PSR algorithm with dual (symmetrical) terms based on the total least squares (DT-TLS), which can correct errors-in-variables and suppress multiple outliers. Meanwhile, the framework of soft decision-making is presented, and a TLS-based criterion is constructed to efficiently exploit the probability and global structures of two-point sets. Such TLS-based criterion with single row-orthonormal STM includes two interesting dual (symmetrical) terms that can be conveniently exploited to suppress bilateral outliers. The experimental results show that DT-TLS achieves better performance than the state-of-the-art algorithms in some multi-view computer vision tasks, indicating that our proposed algorithm is suitable for solving PSR problems.}
}
@article{SUN2023109087,
title = {Learning isometry-invariant representations for point cloud analysis},
journal = {Pattern Recognition},
volume = {134},
pages = {109087},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109087},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005672},
author = {Xiao Sun and Yang Huang and Zhouhui Lian},
keywords = {3D Shape analysis, Isometry invariant, Non-rigid},
abstract = {3D shape analysis has drawn broad attention due to its increasing demands in various fields. Despite that impressive performance has been achieved on several databases, most researchers focus their efforts on improving the performance of shape classification, retrieval, segmentation, etc. They neglect the fact that the disturbances, such as orientation and deformation, may impact much on the perception, restricting the capacity of generalizing to real applications where the prior of orientation and pose is often unknown. In this paper, we conduct shape analysis on point clouds and propose the point projection feature, which is rotation-invariant. Specifically, a novel architecture is designed to mine features of different levels. We adopt a PointNet-based backbone to extract global feature for the point cloud, and the graph aggregation operation to perceive local pose variance in the Euclidean space or geodesic space. An efficient key point descriptor is designed to assign each point with different response and help recognize the overall geometry. Furthermore, a novel dataset, PKUnon-rigid, is built that is composed of non-rigid 3D objects, based on which we evaluate the capacity of several mainstream methods in terms of processing non-rigid shapes. Mathematical analyses and experimental results demonstrate that the proposed method can extract isometry-invariant representations for 3D shape analysis tasks without rotation augmentation, and outperforms other state-of-the-art methods. The proposed dataset is publicly available at https://github.com/tasx0823/PKUnon-rigid.}
}
@article{LIU2023109079,
title = {Curvilinear Structure Tracking Based on Dynamic Curvature-penalized Geodesics},
journal = {Pattern Recognition},
volume = {134},
pages = {109079},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109079},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005593},
author = {Li Liu and Mingzhu Wang and Shuwang Zhou and Minglei Shu and Laurent D. Cohen and Da Chen},
keywords = {Curvature-penalized geodesics, Local bending constraint, Coherence penalization, Curvilinear structures, Retinal vessels},
abstract = {Geodesic models are considered as a fundamental and powerful tool in the applications of curvilinear structure extraction, where the target structures are usually modeled as geodesic paths connecting prescribed points. Despite great advances in geodesic models, it still remains an unsolved problem of detecting weak curvilinear structures from complicated scenarios. In this paper, a dynamic high-order geodesic model for curvilinear structure extraction is introduced to alleviate the shortcuts or short branches combination problems suffered in the classical geodesic approaches. For that purpose, we take into account the nonlocal pattern of curvilinear structures and the local curvature of geodesic paths for the construction of geodesic metrics. Accordingly, the proposed model is able to blend the benefits from the on-the-fly nonlocal smoothness property, curvature regularization and appearance coherence penalization. The nonlocal smoothness property carried out via a local bending operator is constructed to provide a quantitative measure of geodesic advancing directions, meanwhile the coherence penalization is established to guarantee the consistency of the local appearance features extracted via a vessel detector. The experiment results on synthetic and real images illustrate that the proposed method obtains outperformance when compared to the classical geodesic-based tracing algorithms.}
}
@article{YAN2023109119,
title = {Towards deeper match for multi-view oriented multiple kernel learning},
journal = {Pattern Recognition},
volume = {134},
pages = {109119},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109119},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005994},
author = {Wenzhu Yan and Yanmeng Li and Ming Yang},
keywords = {Multi-view representation, Deep kernel, Feature fusion, Classification},
abstract = {Multi-view representation learning aims to exploit the complementary information underlying multiple view data to enhance the expressive power of data representation. Given that kernels in multiple kernel learning naturally correspond to different views, previous shallow similarity learning models cannot fully capture the complex hierarchical information. This work presents an effective deeper match model for multi-view oriented kernel (DMMV) learning which brings a deeper insight into the kernel match for similarity based multi-view representation fusion. Specifically, we propose local deep view-specific self-kernel (LDSvK) by mimicking the deep neural networks to faithfully characterize the local similarity between view-specific samples. Thus, the representation capacity of each view can be saliently analyzed. We build the global deep multi-view fusion kernel (GDMvK) by learning deep fusion of LDSvKs to learn a comprehensive measurement of the cross-view similarity. Notably, the proposed learning framework of the deeper local information extraction and global deep multiple kernel fusion provides a robust way in fitting multi-view data, and yields better learning performance. Experimental results on several multi-view benchmark datasets well demonstrate the effectiveness of our DMMV over other state-of-the-art methods.}
}
@article{CAI2023109067,
title = {High-order manifold regularized multi-view subspace clustering with robust affinity matrices and weighted TNN},
journal = {Pattern Recognition},
volume = {134},
pages = {109067},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109067},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005477},
author = {Bing Cai and Gui-Fu Lu and Liang Yao and Hua Li},
keywords = {High-order manifold regularization, Robust affinity matrices, Multi-view subspace clustering, Weighted TNN},
abstract = {Multi-view subspace clustering achieves impressive performance for high-dimensional data. However, many of these models do not sufficiently mine the intrinsic information among samples and consider the robustness problem of the affinity matrices, resulting in the degradation of clustering performance. To address these problems, we propose a novel high-order manifold regularized multi-view subspace clustering with robust affinity matrices and a weighted tensor nuclear norm (TNN) model (termed HMRMSC) to characterize real-world data. Specifically, all the similarity matrices of different views are first stacked into a third-order tensor. However, the constructed tensor may contain an additional inter-class representation since the data are usually noisy. Then, we use a technique similar to tensor principal component analysis (TPCA) to obtain a more robust similarity tensor, which is constrained by the so-called weighted TNN since the original TNN treats each singular value equally and usually considers no prior information of singular values. In addition, a high-order manifold regularized term is also added to utilize the manifold information of data. Finally, all the steps are unified into a framework, which is resolved by the augmented Lagrange multiplier (ALM) method. Experimental results on six representative datasets show that our model outperforms several state-of-the-art counterparts.}
}
@article{XIANG2023109046,
title = {Deep learning for image inpainting: A survey},
journal = {Pattern Recognition},
volume = {134},
pages = {109046},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109046},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200526X},
author = {Hanyu Xiang and Qin Zou and Muhammad Ali Nawaz and Xianfeng Huang and Fan Zhang and Hongkai Yu},
keywords = {Image inpainting, Image restoration, Generative adversarial network, Convolutional neural network},
abstract = {Image inpainting has been widely exploited in the field of computer vision and image processing. The main purpose of image inpainting is to produce visually plausible structure and texture for the missing regions of damaged images. In the past decade, the success of deep learning has brought new opportunities to many vision tasks, which promoted the development of a large number of deep learning-based image inpainting methods. Although these methods have many similarities, they also have their own characteristics due to the differences in data types, application scenarios, computing platforms, etc. It is necessary to classify and summarize these methods to provide a reference for the research community. In this survey, we present a comprehensive overview of recent advances in deep learning-based image inpainting. First, we categorize the deep learning-based techniques from multiple perspectives: inpainting strategies, network structures, and loss functions. Second, we summarize the open source codes and representative public datasets, and introduce the evaluation metrics for quantitative comparisons. Third, we summarize the real-world applications of image inpainting in different scenarios, and give a detailed analysis on the performance of different inpainting algorithms. At last, we conclude the survey and discuss about the future directions.}
}
@article{SEMENOGLOU2023109132,
title = {Data augmentation for univariate time series forecasting with neural networks},
journal = {Pattern Recognition},
volume = {134},
pages = {109132},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109132},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006124},
author = {Artemios-Anargyros Semenoglou and Evangelos Spiliotis and Vassilios Assimakopoulos},
keywords = {Time series, Forecasting, Data augmentation, Neural networks, M4 competition},
abstract = {Neural networks have been proven particularly accurate in univariate time series forecasting settings, requiring however a significant number of training samples to be effectively trained. In machine learning applications where available data are limited, data augmentation techniques have been successfully used to generate synthetic data that resemble and complement the original train set. Since the potential of data augmentation has been largely neglected in univariate time series forecasting, in this study we investigate nine data augmentation techniques, ranging from simple transformations and adjustments to sophisticated generative models and a novel upsampling approach. We empirically evaluate the impact of data augmentation on forecasting accuracy considering both shallow and deep feed-forward neural networks and time series data sets of different sizes from the M4 and the Tourism competitions. Our results suggest that certain data augmentation techniques that build on upsampling and time series combinations can improve forecasting performance, especially when deep networks are used. However, these improvements become less significant as the initial size of the train set increases.}
}
@article{LIU2023109059,
title = {Joint Graph Learning and Matching for Semantic Feature Correspondence},
journal = {Pattern Recognition},
volume = {134},
pages = {109059},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109059},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005398},
author = {He Liu and Tao Wang and Yidong Li and Congyan Lang and Yi Jin and Haibin Ling},
keywords = {Feature correspondence, Attention network, Graph matching, Graph learning},
abstract = {In recent years, powered by the learned discriminative representation via graph neural network (GNN) models, deep graph matching methods have made great progresses in the task of matching semantic features. However, these methods usually rely on heuristically generated graph patterns, which may introduce unreliable relationships to hurt the matching performance. In this paper, we propose a joint graph learning and matching network, named GLAM, to explore reliable graph structures for boosting graph matching. GLAM adopts a pure attention-based framework for both graph learning and graph matching. Specifically, it employs two types of attention mechanisms, self-attention and cross-attention for the task. The self-attention discovers the relationships between features to further update feature representations over the learnt structures; and the cross-attention computes cross-graph correlations between the two feature sets to be matched for feature reconstruction. Moreover, the final matching solution is directly derived from the output of the cross-attention layer, without employing a specific matching decision module. The proposed method is evaluated on three popular visual matching benchmarks (Pascal VOC, Willow Object and SPair-71k), and it outperforms previous state-of-the-art graph matching methods on all benchmarks. Furthermore, the graph patterns learnt by our model are validated to be able to remarkably enhance previous deep graph matching methods by replacing their handcrafted graph structures with the learnt ones.}
}
@article{TANG2023109135,
title = {Video representation learning for temporal action detection using global-local attention},
journal = {Pattern Recognition},
volume = {134},
pages = {109135},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109135},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200615X},
author = {Yiping Tang and Yang Zheng and Chen Wei and Kaitai Guo and Haihong Hu and Jimin Liang},
keywords = {Temporal action detection, Video representation, Untrimmed video analysis},
abstract = {Video representation is of significant importance for temporal action detection. The two sub-tasks of temporal action detection, i.e., action classification and action localization, have different requirements for video representation. Specifically, action classification requires video representations to be highly discriminative, so that action features and background features are as dissimilar as possible. For action localization, it is crucial to obtain information about the action itself and the surrounding context for accurate prediction of action boundaries. However, the previous methods failed to extract the optimal representations for the two sub-tasks, whose representations for both sub-tasks are obtained in a similar way. In this paper, a Global-Local Attention (GLA) mechanism is proposed to produce a more powerful video representation for temporal action detection without introducing additional parameters. The global attention mechanism predicts each action category by integrating features in the entire video that are similar to the action while suppressing other features, thus enhancing the discriminability of video representation during the training process. The local attention mechanism uses a Gaussian weighting function to integrate each action and its surrounding contextual information, thereby enabling precise localization of the action. The effectiveness of GLA is demonstrated on THUMOS’14 and ActivityNet-1.3 with a simple one-stage action detection network, achieving state-of-the-art performance among the methods using only RGB images as input. The inference speed of the proposed model reaches 1373 FPS on a single Nvidia Titan Xp GPU. The generalizability of GLA to other detection architectures is verified using R-C3D and Decouple-SSAD, both of which achieve consistent improvements. The experimental results demonstrate that designing representations with different properties for the two sub-tasks leads to better performance for temporal action detection compared to the representations obtained in a similar way.}
}
@article{RAHMAN2023109043,
title = {Tripartite sub-image histogram equalization for slightly low contrast gray-tone image enhancement},
journal = {Pattern Recognition},
volume = {134},
pages = {109043},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109043},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005234},
author = {Hafijur Rahman and Gour Chandra Paul},
keywords = {Contrast enhancement, Slightly low contrast image, Histogram equalization, Mean brightness sustainability, Image quality assessment, Bit-plane specific measure},
abstract = {In this paper, a neoteric tripartite sub-image histogram equalization method is proposed to enhance slightly low contrast gray-tone images, which is a less explored area in the literature. An image is decomposed into three sub-images to preserve its mean brightness, and the histograms of the sub-images are calculated. Then, the snipping procedure is applied to each histogram to constrain the pace of contrast enhancement. Subsequently, the equalization of the three histograms is performed independently, and finally, the three equalized sub-images are composed into a single image. The proposed method offers better outcomes as compared to several common and state-of-the-art histogram equalization-based methods regarding contrast improvement, blind/reference-less image spatial quality evaluator, mean brightness preservation, peak signal-to-noise ratio, mean structural similarity, gradient magnitude similarity deviation, feature similarity, bit-plane to bit-plane similarity, and visual image quality.}
}
@article{LI2023109083,
title = {Auto-weighted Tensor Schatten p-Norm for Robust Multi-view Graph Clustering},
journal = {Pattern Recognition},
volume = {134},
pages = {109083},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109083},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005635},
author = {Xingfeng Li and Zhenwen Ren and Quansen Sun and Zhi Xu},
keywords = {Multi-view clustering, Adaptive neighbors graph learning, Low-rank tensor learning, Noise estimation},
abstract = {Recently, tensor-singular value decomposition based tensor-nuclear norm (t-TNN) has achieved impressive performance for multi-view graph clustering. This primarily ascribes the superiority of t-TNN in exploring high-order structure information among views. However, 1) t-TNN cannot ideally approximate to the original rank minimization, which produces the suboptimal graph tensor; in addition, t-TNN treats different singular values equally, such that the larger singular values corresponding to certain significant feature information (i.e., prior information) has not been utilized fully; 2) the data of original high-dimensional space are often corrupted by noise and outliers, which always makes adaptive neighbors graph learning (ANGL) generate low-quality affinity graphs. To address these issues, we propose a novel multi-view graph clustering method termed auto-weighted tensor Schatten p-norm (t-ATSN) for robust multi-view graph clustering (t-ATSN-RMGC). Concretely, we first propose t-SVD based t-ATSN with 0<p<1 to make the learned graph tensor better approximate the target rank than t-TNN. Meanwhile, it can also automatically and appropriately shrink singular values for constructing a more refined graph tensor, so as to fully capture spatial structure in the graph tensor. Moreover, we introduce the Geman McClure loss function to enhance the robustness of ANGL for noise and outliers. Experimental results on benchmarks across different scenarios and sizes show that the proposed method consistently outperforms state-of-the-art methods.}
}
@article{LI2023109075,
title = {Frequency domain regularization for iterative adversarial attacks},
journal = {Pattern Recognition},
volume = {134},
pages = {109075},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109075},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005556},
author = {Tengjiao Li and Maosen Li and Yanhua Yang and Cheng Deng},
keywords = {Adversarial examples, Transfer-based attack, Black-box attack, Frequency-domain characteristics},
abstract = {Adversarial examples have attracted more and more attentions with the prosperity of convolutional neural networks. The transferability of adversarial examples is an important property that makes black-box attacks possible in real-world applications. On the other side, many adversarial defense methods have been proposed to improve the robustness, leading to the requirement for more transferable adversarial examples. Inspired by the regularization term for network parameters at training process, we treat adversarial attacks as training process of inputs and propose regularization constraint for inputs to prevent adversarial examples from overfitting the white-box networks and enhance the transferability. Specifically, we find a universal attribute that the outputs of convolutional neural networks have consistency to the low frequencies of inputs, and based on this, we construct a frequency domain regularization to inputs for iterative attacks. In this way, our method is compatible with existing iterative attack methods and can learn more transferable adversarial examples. Extensive experiments on ImageNet validate the superiority of our method, and compared with several attacks, we achieve attack success rate improvements of 8.0% and 11.5% on average to normal models and defense methods respectively.}
}
@article{LI2023109120,
title = {Robust sparse and low-redundancy multi-label feature selection with dynamic local and global structure preservation},
journal = {Pattern Recognition},
volume = {134},
pages = {109120},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109120},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006008},
author = {Yonghao Li and Liang Hu and Wanfu Gao},
keywords = {Feature selection, Multi-label learning, Sparse learning, Label correlations},
abstract = {Recent years, joint feature selection and multi-label learning have received extensive attention as an open problem. However, there exist three general issues in previous multi-label feature selection methods. First of all, existing methods either consider local label correlations or global label correlations when they design multi-label feature selection methods, in fact, both two types of label correlations are significant for feature selection; second, previous methods use the low-quality graph to excavate local label correlations so that the results of these methods are under-performing; third, feature redundancy is ignored by most of the sparse learning methods. To overcome these challenges, we preserve global label correlations and dynamic local label correlations by preserving graph structure. Additionally, the l2,1-norm and an inner product regularization term are imposed onto the objective function to preserve robust high row-sparsity and to select low redundant features. All the above terms are integrated into one learning framework, and then we utilize a simple yet effective scheme to optimize the framework. Experimental results demonstrate the classification superiority of the proposed method.}
}
@article{FANG2023109099,
title = {UDNet: Uncertainty-aware deep network for salient object detection},
journal = {Pattern Recognition},
volume = {134},
pages = {109099},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109099},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005799},
author = {Yuming Fang and Haiyan Zhang and Jiebin Yan and Wenhui Jiang and Yang Liu},
keywords = {Salient object detection, Contour uncertainty, Feature interaction},
abstract = {Most of the existing deep learning based salient object detection (SOD) models adopt multi-level feature fusion strategies, and have achieved remarkable progress. However, current SOD models still suffer from the uncertainty dilemma in predicting salient probabilities of the pixels surrounding the contour of salient objects. To solve this issue, we propose a novel uncertainty-aware SOD model, where multiple supervision signals, i.e., internal contour uncertainty map, saliency map and external contour uncertainty map, are used to guide the network to not only focus on the pixels in the salient object but also shift its partial attention to the pixels surrounding the contour of salient objects. Furthermore, we introduce a new feature interaction module to aggregate internal contour uncertainty features, saliency features and external contour uncertainty features in the decoding stage, aiming to enhance the model’s ability in dealing with the “uncertain” pixels. Extensive experiments on four public benchmark datasets demonstrate the superiority of the proposed method over the existing state-of-the-art SOD methods. Furthermore, the proposed method shows better attribute-based performance on the SOC dataset, suggesting that the proposed model can also handle challenging scenarios in SOD.}
}
@article{LU2023109127,
title = {Cross-domain structure learning for visual data recognition},
journal = {Pattern Recognition},
volume = {134},
pages = {109127},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109127},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006070},
author = {Yuwu Lu and Xingping Luo and Jiajun Wen and Zhihui Lai and Xuelong Li},
keywords = {Domain adaptation, Cross-domain, Classwise structure learning, Sample reweighting},
abstract = {Unsupervised domain adaptation methods are used to train an effect model by utilizing available knowledge from a labeled source domain for solving tasks in an unlabeled target domain. The most difficult challenge is determining methods to reduce distribution discrepancies and extract the largest number of domain-invariant features between the source and target domains to improve model performance. With the aim of minimizing the domain shift and maximizing domain-invariant feature extraction, we propose a cross-domain structure learning (CDSL) method for visual data recognition, which incorporates global distribution alignment and local discriminative structure preservation to capture the common, underlying features between domains. Specifically, we design a simple but effective classwise structure learning strategy with a specific compactness hierarchy to promote intraclass knowledge transfer and reduce the risk of negative transfer between domains. We also extend CDSL to different kinds of kernelization to address complex situations in the real world. Extensive experiments on several visual data benchmarks demonstrate the effectiveness of our proposed method.}
}
@article{CHEN2023109086,
title = {Domain Generalization by Joint-Product Distribution Alignment},
journal = {Pattern Recognition},
volume = {134},
pages = {109086},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109086},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005660},
author = {Sentao Chen and Lei Wang and Zijie Hong and Xiaowei Yang},
keywords = {Distribution alignment, Distribution divergence, Domain generalization, Feature transformation},
abstract = {In this work, we address the problem of domain generalization for classification, where the goal is to learn a classification model on a set of source domains and generalize it to a target domain. The source and target domains are different, which weakens the generalization ability of the learned model. To tackle the domain difference, we propose to align a joint distribution and a product distribution using a neural transformation, and minimize the Relative Chi-Square (RCS) divergence between the two distributions to learn that transformation. In this manner, we conveniently achieve the alignment of multiple domains in the neural transformation space. Specifically, we show that the RCS divergence can be explicitly estimated as the maximal value of a quadratic function, which allows us to perform joint-product distribution alignment by minimizing the divergence estimate. We demonstrate the effectiveness of our solution through comparison with the state-of-the-art methods on several image classification datasets.}
}
@article{MALDONADO2023109058,
title = {Mitigating the effect of dataset shift in clustering},
journal = {Pattern Recognition},
volume = {134},
pages = {109058},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109058},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005386},
author = {Sebastián Maldonado and Ramiro Saltos and Carla Vairetti and José Delpiano},
keywords = {Induced ordered weighted average, Kernel k-means, OWA operators, Dataset shift, Clustering},
abstract = {Dataset shift is a relevant topic in unsupervised learning since many applications face evolving environments, causing an important loss of generalization and performance. Most techniques that deal with this issue are designed for data stream clustering, whose goal is to process sequences of data efficiently under Big Data. In this study, we claim dataset shift is an issue for static clustering tasks in which data is collected over a long period. To mitigate it, we propose Time-weighted kernel k-means, a k-means variant that includes a time-dependent weighting process. We do this via the induced ordered weighted average (IOWA) operator. The weighting process acts as a gradual forgetting mechanism, prioritizing recent examples over outdated ones in the clustering algorithm. The computational experiments show the potential Time-weighted kernel k-means has in evolving environments.}
}
@article{YU2023109078,
title = {A Lie algebra representation for efficient 2D shape classification},
journal = {Pattern Recognition},
volume = {134},
pages = {109078},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109078},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005581},
author = {Xiaohan Yu and Yongsheng Gao and Mohammed Bennamoun and Shengwu Xiong},
keywords = {Lie algebra, 2D Shape classification, Covariance matrix, Lie group of SPD matrix},
abstract = {Riemannian manifold plays a vital role as a powerful mathematical tool in computer vision, with important applications in curved shape analysis and classification. Significant progress has recently been made by Riemannian framework based methods that achieved state-of-the-art classification accuracy and robustness. However, these Riemannian manifold and Lie group methods require a very high computational complexity and do not include a description of the shape regions. This paper presents a novel mathematical tool, called Block Diagonal Symmetric Positive Definite Matrix Lie Algebra (BDSPDMLA) to represent curves, which extends the existing Lie group representations to a compact yet informative Lie algebra representation. The proposed Lie algebra based method addresses the computational bottleneck problem of the Riemannian framework based methods. In addition, it allows the natural fusion of various regions information with curved shape features for a more discriminative shape description. Here the region information is represented by values of distance maps, local binary patterns (LBP) and image intensity. Extensive experiments on five publicly available databases demonstrate that the proposed Lie algebra based method can achieve a speed of over ten thousand times faster than the Riemannian manifold and Lie group based baseline methods, while obtaining comparable accuracies for 2D shape classification.}
}
@article{XING2023109123,
title = {Binary feature learning with local spectral context-aware attention for classification of hyperspectral images},
journal = {Pattern Recognition},
volume = {134},
pages = {109123},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109123},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006033},
author = {Changda Xing and Chaowei Duan and Zhisheng Wang and Meiling Wang},
keywords = {Classification of hyperspectral images, Local spectrum modules, Spectral context-awareness, Binary feature learning},
abstract = {The classification of hyperspectral images (HSIs) has achieved success in applications. For many approaches, features are directly extracted from whole spectral pixels, which can not well describe local characteristics. These methods are also susceptible to noise since each feature code is learned individually. Accordingly, a binary feature learning method with local spectral context-aware attention (BFLSC) is proposed for the classification. Specifically, for training samples, we first build the local spectrum models (LSMs) to describe local spectral properties, where each training sample is segmented into some parts and the difference between the central value and its neighborhoods is calculated in each part. Then, we construct the BFLSC model to learn a projection and binary features of training samples. In such model, the spectral context-awareness attention is established to collaboratively learn binary feature codes by enforcing one shift between 0/1 of each LSM, which enhances the robustness and stability of binary leaning. We also introduce the loss constraint, even distribution constraint, and variance constraint to reduce information loss and improve the quality of learned feature distribution. Additionally, an optimization scheme is designed to obtain the solution of the BFLSC model. Further, the learned binary features are added to train the support vector machine (SVM). For each testing sample, the LSMs are first extracted, and then mapped into binary features by the learned projection. The trained SVM is finally used for the mapped binary features to predict the label of the testing sample. Experimental results validate that our BFLSC realizes the better performance compared with some advanced approaches.}
}
@article{TIAN2023109050,
title = {Multi-stage image denoising with the wavelet transform},
journal = {Pattern Recognition},
volume = {134},
pages = {109050},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109050},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005301},
author = {Chunwei Tian and Menghua Zheng and Wangmeng Zuo and Bob Zhang and Yanning Zhang and David Zhang},
keywords = {Image denoising, CNN, Wavelet transform, Dynamic convolution, Signal processing},
abstract = {Deep convolutional neural networks (CNNs) are used for image denoising via automatically mining accurate structure information. However, most of existing CNNs depend on enlarging depth of designed networks to obtain better denoising performance, which may cause training difficulty. In this paper, we propose a multi-stage image denoising CNN with the wavelet transform (MWDCNN) via three stages, i.e., a dynamic convolutional block (DCB), two cascaded wavelet transform and enhancement blocks (WEBs) and a residual block (RB). DCB uses a dynamic convolution to dynamically adjust parameters of several convolutions for making a tradeoff between denoising performance and computational costs. WEB uses a combination of signal processing technique (i.e., wavelet transformation) and discriminative learning to suppress noise for recovering more detailed information in image denoising. To further remove redundant features, RB is used to refine obtained features for improving denoising effects and reconstruct clean images via improved residual dense architectures. Experimental results show that the proposed MWDCNN outperforms some popular denoising methods in terms of quantitative and qualitative analysis. Codes are available at https://github.com/hellloxiaotian/MWDCNN.}
}
@article{LI2023109074,
title = {Multi-label feature selection via robust flexible sparse regularization},
journal = {Pattern Recognition},
volume = {134},
pages = {109074},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109074},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005544},
author = {Yonghao Li and Liang Hu and Wanfu Gao},
keywords = {Multi-label learning, Feature selection, Sparse regularization, Classification},
abstract = {Multi-label feature selection is an efficient technique to deal with the high dimensional multi-label data by selecting the optimal feature subset. Existing researches have demonstrated that l1-norm and l2,1-norm are promising roles for multi-label feature selection. However, two important issues are ignored when existing l1-norm and l2,1-norm based methods select discriminative features for multi-label data. First, l1-norm can enforce sparsity on each feature across all instances while numerous selected features lack discrimination due to the generated zero weight values. Second, l2,1-norm not only neglects label-specific features but also ignores the redundancy among features. To this end, we design a Robust Flexible Sparse Regularization norm (RFSR), furthermore, proposing a global optimization framework named Robust Flexible Sparse regularized multi-label Feature Selection (RFSFS) based on RFSR. Finally, an efficient alternating multipliers based optimization scheme is developed to iteratively optimize RFSFS. Empirical studies on fifteen benchmark multi-label data sets demonstrate the effectiveness and efficiency of RFSFS.}
}
@article{ZHAO2023109118,
title = {Clean affinity matrix learning with rank equality constraint for multi-view subspace clustering},
journal = {Pattern Recognition},
volume = {134},
pages = {109118},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109118},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005982},
author = {Jinbiao Zhao and Gui-Fu Lu},
keywords = {Low-rank representation, Robust principal component analysis, Outliers value, Affinity matrix, Low-rank matrix decomposition},
abstract = {The existing multi-view subspace clustering (MVSC) algorithm still has certain limitations. First, the affinity matrix obtained by them is not clean and robust enough since the original multi-view data usually contain noise. Second, they also have defects in exploring the consistency between views. To compensate for these two shortcomings, we propose a novel MVSC, i.e., clean affinity matrix learning with rank equality constraint (CAMR) for MVSC. By borrowing the idea from robust principal component analysis (RPCA), the representation matrix of each view obtained by low-rank representation (LRR) is first cleaned up to obtain a cleaner and more robust affinity matrix. In addition, the rank constraint is utilized to explore the same clustering properties between different views. An objective function solution method based on an augmented Lagrange multiplier (ALM) is designed and tested on four widely employed datasets to verify that CAMR has better clustering performance than certain state-of-the-art methods. We provide the code of CAMR at https://github.com/zhaojinbiao/CAMR.}
}
@article{KORBAN2023109066,
title = {TAA-GCN: A temporally aware Adaptive Graph Convolutional Network for age estimation},
journal = {Pattern Recognition},
volume = {134},
pages = {109066},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109066},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005465},
author = {Matthew Korban and Peter Youngs and Scott T. Acton},
keywords = {Age estimation, Graph convolutional network, Facial graphs, Skeletal graphs},
abstract = {This paper proposes a novel age estimation algorithm, the Temporally-Aware Adaptive Graph Convolutional Network (TAA-GCN). Using a new representation based on graphs, the TAA-GCN utilizes skeletal, posture, clothing, and facial information to enrich the feature set associated with various ages. Such a novel graph representation has several advantages: First, reduced sensitivity to facial expression and other appearance variances; Second, robustness to partial occlusion and non-frontal-planar viewpoint, which is commonplace in real-world applications such as video surveillance. The TAA-GCN employs two novel components, (1) the Temporal Memory Module (TMM) to compute temporal dependencies in age; (2) Adaptive Graph Convolutional Layer (AGCL) to refine the graphs and accommodate the variance in appearance. The TAA-GCN outperforms the state-of-the-art methods on four public benchmarks, UTKFace, MORPHII, CACD, and FG-NET. Moreover, the TAA-GCN showed reliability in different camera viewpoints and reduced quality images.}
}
@article{LEI2023109106,
title = {Multi-scale enhanced graph convolutional network for mild cognitive impairment detection},
journal = {Pattern Recognition},
volume = {134},
pages = {109106},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109106},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005866},
author = {Baiying Lei and Yun Zhu and Shuangzhi Yu and Huoyou Hu and Yanwu Xu and Guanghui Yue and Tianfu Wang and Cheng Zhao and Shaobin Chen and Peng Yang and Xuegang Song and Xiaohua Xiao and Shuqiang Wang},
keywords = {Mild cognitive impairment detection, Multimodal brain connectivity networks, Multi-scale enhanced graph convolutional network},
abstract = {As an early stage of Alzheimer's disease (AD), mild cognitive impairment (MCI) is able to be detected by analyzing the brain connectivity networks. For this reason, we devise a new framework via multi-scale enhanced graph convolutional network (MSE-GCN) for MCI detection, which integrates the structural and functional information from the diffusion tensor imaging (DTI) and resting-state functional magnetic resonance imaging (R-fMRI), respectively. Specifically, both information in the brain connective networks is first integrated based on the local weighted clustering coefficients (LWCC), which is concatenated as the feature vector for representing a population graph's vertice. Simultaneously, the gender and age information in each subject are integrated with the structural and functional features to construct a sparse graph. Then, various parallel graph convolutional network (GCN) layers with multiple inputs are designed from the embedding from random walk embeddings in the GCN to identify the essential MCI graph information. Finally, all GCN layers’ outputs are concatenated via the fully connection layer to perform disease detection. The experimental results on the public Alzheimer's Disease Neuroimaging Initiative (ADNI) database show that our method is promising to detect MCI and superior to other competing algorithms, with a mean classification accuracy of 90.39% in the detection tasks.}
}
@article{YU2023109054,
title = {Improving adversarial robustness by learning shared information},
journal = {Pattern Recognition},
volume = {134},
pages = {109054},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109054},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005349},
author = {Xi Yu and Niklas Smedemark-Margulies and Shuchin Aeron and Toshiaki Koike-Akino and Pierre Moulin and Matthew Brand and Kieran Parsons and Ye Wang},
keywords = {Adversarial robustness, Information bottleneck, Multi-view learning, Shared information,},
abstract = {We consider the problem of improving the adversarial robustness of neural networks while retaining natural accuracy. Motivated by the multi-view information bottleneck formalism, we seek to learn a representation that captures the shared information between clean samples and their corresponding adversarial samples while discarding these samples’ view-specific information. We show that this approach leads to a novel multi-objective loss function, and we provide mathematical motivation for its components towards improving the robust vs. natural accuracy tradeoff. We demonstrate enhanced tradeoff compared to current state-of-the-art methods with extensive evaluation on various benchmark image datasets and architectures. Ablation studies indicate that learning shared representations is key to improving performance.}
}
@article{ZHANG2023109098,
title = {AugFCOS: Augmented fully convolutional one-stage object detection network},
journal = {Pattern Recognition},
volume = {134},
pages = {109098},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109098},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005787},
author = {Xiuwei Zhang and Wei Guo and Yinghui Xing and Wenna Wang and Hanlin Yin and Yanning Zhang},
keywords = {Feature pyramid network, Object detection, Sample selection, Attention module},
abstract = {As a pioneering work of introducing the idea of full convolutional network into the field of object detection, the fully convolutional one-stage object detection network (FCOS) has the advantage of excellent performance with low memory overhead. However, there are certain problems with FCOS that merit more research: the centerness quality assessment loss does not decrease during the late training stage, and its adaptive training sample selection (ATSS) relies heavily on the hyperparameter. To solve the aforementioned problems, we propose a novel object detection network, named augmented fully convolutional one-stage object detection network (AugFCOS). First of all, we propose an improved dynamic optimization loss (DOL) to mitigate the impact of the original centerness loss not decreasing. Then, a Robust Training Sample Selection (RTSS) is proposed to get rid of the dependence of hyper-parameter in ATSS of FCOS. Finally, a novel mixed attention feature pyramid network (MAFPN) is presented to enhance the multi-scale representation ability of feature pyramid network (FPN) and further improve the ability of multi-scale detection. The experimental results on MS COCO demonstrate the effectiveness of our proposed AugFCOS, where AugFCOS achieves approximate 2.0% to 2.9% increase when compared with ATSS and FCOS.}
}
@article{LI2023109077,
title = {Deep graph clustering with multi-level subspace fusion},
journal = {Pattern Recognition},
volume = {134},
pages = {109077},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109077},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200557X},
author = {Wang Li and Siwei Wang and Xifeng Guo and En Zhu},
keywords = {Graph clustering, Subspace, Self-expressive learning, Fusion},
abstract = {Attributed graph clustering combines both node attributes and graph structure information of data samples and has demonstrated satisfactory performance in various applications. However, how to choose the proper neighborhood for attributed graph clustering remains to be a challenge. A larger neighborhood may cause over-smoothed representations with less discrimination for clustering while the short-range ignore distant nodes and fails to capture the global information. In this paper, we propose a novel deep attributed graph clustering network with a multi-level subspace fusion module to address this issue. The first contribution of our work is to insert multiple self-expressive modules between low-level and high-level layers to promote more favorable features for clustering. The constraint of shared self-expressive matrix facilitates to preserve intrinsic structure without pre-defined neighborhoods as the previous methods do. Moreover, we introduce a novel loss function that leverages traditional reconstruction and the proposed structure fusion loss to effectively preserve multi-level clustering structures with both global and local discriminative features. Extensive experiments on public benchmark datasets validate the effectiveness of our proposed model compared with the state-of-the-art attribute graph clustering competitors by considerable margins.}
}
@article{TROMBINI2023109082,
title = {A goal-driven unsupervised image segmentation method combining graph-based processing and Markov random fields},
journal = {Pattern Recognition},
volume = {134},
pages = {109082},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109082},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005623},
author = {Marco Trombini and David Solarna and Gabriele Moser and Silvana Dellepiane},
keywords = {Graph signal processing, Segmentation, Markovian modeling, Parametric model estimation, Pattern recognition, Synthetic aperture radar, Magnetic resonance imagery},
abstract = {Image segmentation is the process of partitioning a digital image into a set of homogeneous regions (according to some homogeneity criterion) to facilitate a subsequent higher-level analysis. In this context, the present paper proposes an unsupervised and graph-based method of image segmentation, which is driven by an application goal, namely, the generation of image segments associated with a user-defined and application-specific goal. A graph, together with a random grid of source elements, is defined on top of the input image. From each source satisfying a goal-driven predicate, called seed, a propagation algorithm assigns a cost to each pixel on the basis of similarity and topological connectivity, measuring the degree of association with the reference seed. Then, the set of most significant regions is automatically extracted and used to estimate a statistical model for each region. Finally, the segmentation problem is expressed in a Bayesian framework in terms of probabilistic Markov random field (MRF) graphical modeling. An ad hoc energy function is defined based on parametric models, a seed-specific spatial feature, a background-specific potential, and local-contextual information. This energy function is minimized through graph cuts and, more specifically, the alpha-beta swap algorithm, yielding the final goal-driven segmentation based on the maximum a posteriori (MAP) decision rule. The proposed method does not require deep a priori knowledge (e.g., labelled datasets), as it only requires the choice of a goal-driven predicate and a suited parametric model for the data. In the experimental validation with both magnetic resonance (MR) and synthetic aperture radar (SAR) images, the method demonstrates robustness, versatility, and applicability to different domains, thus allowing for further analyses guided by the generated products.}
}
@article{ZHANG2023109070,
title = {Specialized re-ranking: A novel retrieval-verification framework for cloth changing person re-identification},
journal = {Pattern Recognition},
volume = {134},
pages = {109070},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109070},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005507},
author = {Renjie Zhang and Yu Fang and Huaxin Song and Fangbin Wan and Yanwei Fu and Hirokazu Kato and Yang Wu},
keywords = {Cloth changing person re-identification, Verification network, Re-Rank, Specialized features, Part-based comparison},
abstract = {Cloth changing person re-identification(Re-ID) can work under more complicated scenarios with higher security than normal Re-ID and biometric techniques and is therefore extremely valuable in applications. Meanwhile, the wide range of appearance flexibility results in more similar-looking, confusing images, which is the weakness of the widely used retrieval methods. In this work, we shed light on how to handle these similar images. Specifically, we propose a novel retrieval-verification framework. Given an image, the retrieval module will search for a shot list of similar images quickly. Our proposed verification network will then compare the probe image with these candidate images by contrasting local details for their similarity scores. An innovative ranking strategy is also introduced to achieve a good balance between retrieval and verification results. Comprehensive experiments are conducted to show the effectiveness of our framework and its capability in improving the state-of-the-art methods remarkably on both synthetic and realistic datasets.}
}
@article{WU2023109114,
title = {Semi-supervised adaptive kernel concept factorization},
journal = {Pattern Recognition},
volume = {134},
pages = {109114},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109114},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005945},
author = {Wenhui Wu and Junhui Hou and Shiqi Wang and Sam Kwong and Yu Zhou},
keywords = {Concept factorization, Semi-supervised learning, Clustering, Nonnegative matrix factorization, Kernel method},
abstract = {Kernelized concept factorization (KCF) has shown its advantage on handling data with nonlinear structures; however, the kernels involved in the existing KCF-based methods are empirically predefined, which may compromise the performance. In this paper, we propose semi-supervised adaptive kernel concept factorization (SAKCF), which integrates the data representation and kernel learning into a unified model to make the two learning processes adapt to each other. SAKCF extends traditional KCF in a semi-supervised manner, which encourages the high-dimensional representation to be consistent with both the limited supervisory and local geometric information. Besides, an alternating iterative algorithm is proposed to solve the resulting constrained optimization problem. Experimental results on six real-world data sets verify the effectiveness and advantages of our SAKCF over state-of-the-art methods when applied on the clustering task.}
}
@article{HOU2023109062,
title = {Towards Parameter-Free Clustering for Real-World Data},
journal = {Pattern Recognition},
volume = {134},
pages = {109062},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109062},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005428},
author = {Jian Hou and Huaqiang Yuan and Marcello Pelillo},
keywords = {Clustering, Real-world data, Dominant set, Density peak},
abstract = {While many clustering algorithms have been published, existing algorithms are often afflicted by some problems in processing real-world data. We present an algorithm to deal with two of these problems in this paper. First, the majority of clustering algorithms depend on one or more parameters. Second, some algorithms are not suitable for clusters of Gaussian distribution, whereas clusters of many real datasets are of Gaussian distribution approximately. Our algorithm generates clusters sequentially, and each cluster is obtained by expanding an initial cluster. The initial cluster is extracted with the dominant set algorithm, and we study the correlation between the pairwise data similarity matrix and clustering result to determine the involved scaling parameter adaptively. In expanding the initial cluster, we improve the density peak algorithm so that the expansion will not cross the boundary between two clusters, and the involved density parameter has little influence on clustering results. In our algorithm, the cluster expansion enables our algorithm to work well with clusters of Gaussian distribution, and two involved parameters can be fixed or determined adaptively. Our algorithm goes a step forward in parameter-free clustering for real-world data, and it is shown to perform better than or comparably to some commonly used algorithms with parameters in experiments with synthetic datasets composed of Gaussian clusters and real datasets.}
}
@article{BAI2023109110,
title = {Automatically detecting human-object interaction by an instance part-level attention deep framework},
journal = {Pattern Recognition},
volume = {134},
pages = {109110},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109110},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005908},
author = {Lin Bai and Fenglian Chen and Yang Tian},
keywords = {Human-object interaction, Instance part-level correlations, Self-attention-based model, Image context},
abstract = {Automatically detecting human-object interactions (HOIs) from an image is a very important but challenging task in computer vision. One of the significant problems in HOI detection is that similar human-object interactions are difficult to distinguish. Recently, many instance-centric HOI detection schemes, based on appearance features and coarse spatial information, have been proposed. These methods, however, lack the capacity of capturing and analyzing the fine-grained context between human poses and object parts, which plays a crucial role in HOI detection. To address these problems, we propose a novel instance part-level attention deep framework for HOI detection. Specifically, our approach consists of a human/object-part detection phase and an HOI detection phase. In the former phase, a part-level visual pattern estimation model is designed for capturing the fine-grained human body parts and object parts. In the latter phase, a self-attention-based deep network is proposed to learn the visual composite around the human-object pair that implicitly expresses the consistent spatial, scale, co-occurrence, and viewpoint relationships among human body parts and object parts across images, which are effective for predicting HOI. To the best of our knowledge, we are the first to propose a framework where the fine-grained part-level mutual context of a human-object pair is extracted to improve HOI detection. By comparing our approach with state-of-the-art HOI detection methods on benchmark datasets, we demonstrated that our proposed framework outperformed the existing HOI detection methods, such as significantly improving the performance of part-level visual pattern estimation, HOI detection, and the quality of the self-attention-based deep network structure.}
}
@article{DEHANDSCHUTTER2023109102,
title = {A consistent and flexible framework for deep matrix factorizations},
journal = {Pattern Recognition},
volume = {134},
pages = {109102},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109102},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005829},
author = {Pierre {De Handschutter} and Nicolas Gillis},
keywords = {Deep matrix factorization, Loss functions, Constrained optimization, First-order methods, Hyperspectral unmixing},
abstract = {Deep matrix factorizations (deep MFs) are recent unsupervised data mining techniques inspired by constrained low-rank approximations. They aim to extract complex hierarchies of features within high-dimensional datasets. Most of the loss functions proposed in the literature to evaluate the quality of deep MF models and the underlying optimization frameworks are not consistent because different losses are used at different layers. In this paper, we introduce two meaningful loss functions for deep MF and present a generic framework to solve the corresponding optimization problems. We illustrate the effectiveness of this approach through the integration of various constraints and regularizations, such as sparsity, nonnegativity and minimum-volume. The models are successfully applied on both synthetic and real data, namely for hyperspectral unmixing and extraction of facial features.}
}
@article{GAVINI2023109069,
title = {Thermal to Visual Person Re-Identification Using Collaborative Metric Learning Based on Maximum Margin Matrix Factorization},
journal = {Pattern Recognition},
volume = {134},
pages = {109069},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109069},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005490},
author = {Yaswanth Gavini and Arun Agarwal and B.M. Mehtre},
keywords = {Thermal to visual person re-identification, Cross-domain image retrieval, Collaborative metric learning, Matrix factorization},
abstract = {Thermal to visual person re-identification (T2V-ReID) is a cross-domain image retrieval problem. In this problem, the matching of a person’s image takes place, where the image is taken by different cameras (thermal and visual) at different times. This problem has numerous applications in night-time security surveillance. It is challenging due to the large intra-class variations and cross-domain discrepancies. Recently, deep metric learning methods are proposed for this problem. Still, there is a scope to improve the metric learning by generalizing the metric. In this paper, we have proposed the collaborative metric learning using Maximum Margin Matrix Factorization. It uses the group-wise similarities and collaboratively predicts the similarities. We can learn a more generalized metric by utilizing the maximized margin in this method. The proposed method is tested on the RegDB and RGB-D-T data sets, and the method outperforms the existing works in the few-shot learning settings.}
}
@article{PEIS2023109130,
title = {Unsupervised learning of global factors in deep generative models},
journal = {Pattern Recognition},
volume = {134},
pages = {109130},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109130},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006100},
author = {Ignacio Peis and Pablo M. Olmos and Antonio Artés-Rodríguez},
keywords = {VAE, Deep generative models, Global factors, Unsupervised learning, Disentanglement, Representation learning},
abstract = {We present a novel deep generative model based on non i.i.d. variational autoencoders that captures global dependencies among observations in a fully unsupervised fashion. In contrast to the recent semi-supervised alternatives for global modeling in deep generative models, our approach combines a mixture model in the local or data-dependent space and a global Gaussian latent variable, which lead us to obtain three particular insights. First, the induced latent global space captures interpretable disentangled representations with no user-defined regularization in the evidence lower bound (as in β-VAE and its generalizations). Second, we show that the model performs domain alignment to find correlations and interpolate between different databases. Finally, we study the ability of the global space to discriminate between groups of observations with non-trivial underlying structures, such as face images with shared attributes or defined sequences of digits images.}
}
@article{LIU2023109109,
title = {Towards open-set text recognition via label-to-prototype learning},
journal = {Pattern Recognition},
volume = {134},
pages = {109109},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109109},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005891},
author = {Chang Liu and Chun Yang and Hai-Bo Qin and Xiaobin Zhu and Cheng-Lin Liu and Xu-Cheng Yin},
keywords = {Open-set recognition, Scene text recognition, Low-shot recognition},
abstract = {Scene text recognition is a popular research topic which is also extensively utilized in the industry. Although many methods have achieved satisfactory performance for the close-set text recognition challenges, these methods lose feasibility in open-set scenarios, where collecting data or retraining models for novel characters could yield a high cost. For example, annotating samples for foreign languages can be expensive, whereas retraining the model each time when a “novel” character is discovered from historical documents costs both time and resources. In this paper, we introduce and formulate a new open-set text recognition task which demands the capability to spot and recognize novel characters without retraining. A label-to-prototype learning framework is also proposed as a baseline for the new task. Specifically, the framework introduces a generalizable label-to-prototype mapping function to build prototypes (class centers) for both seen and unseen classes. An open-set predictor is then utilized to recognize or reject samples according to the prototypes. The implementation of rejection capability over out-of-set characters allows automatic spotting of unknown characters in the incoming data stream. Extensive experiments show that our method achieves promising performance on a variety of zero-shot, close-set, and open-set text recognition datasets.}
}
@article{FANG2023109057,
title = {End-to-end kernel learning via generative random Fourier features},
journal = {Pattern Recognition},
volume = {134},
pages = {109057},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109057},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005374},
author = {Kun Fang and Fanghui Liu and Xiaolin Huang and Jie Yang},
keywords = {Generative random Fourier features, Kernel learning, End-to-end, One-stage, Generative network, Adversarial robustness},
abstract = {Random Fourier features (RFFs) provide a promising way for kernel learning in a spectral case. Current RFFs-based kernel learning methods usually work in a two-stage way. In the first-stage process, learning an optimal feature map is often formulated as a target alignment problem, which aims to align the learned kernel with a pre-defined target kernel (usually the ideal kernel). In the second-stage process, a linear learner is conducted with respect to the mapped random features. Nevertheless, the pre-defined kernel in target alignment is not necessarily optimal for the generalization of the linear learner. Instead, in this paper, we consider a one-stage process that incorporates the kernel learning and linear learner into a unifying framework. To be specific, a generative network via RFFs is devised to implicitly learn the kernel, followed by a linear classifier parameterized as a full-connected layer. Then the generative network and the classifier are jointly trained by solving an empirical risk minimization (ERM) problem to reach a one-stage solution. This end-to-end scheme naturally allows deeper features, in correspondence to a multi-layer structure, and shows superior generalization performance over the classical two-stage, RFFs-based methods in real-world classification tasks. Moreover, inspired by the randomized resampling mechanism of the proposed method, its enhanced adversarial robustness is investigated and experimentally verified.}
}
@article{LIU2023109122,
title = {A novel soft-coded error-correcting output codes algorithm},
journal = {Pattern Recognition},
volume = {134},
pages = {109122},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109122},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006021},
author = {Kun-Hong Liu and Jie Gao and Yong Xu and Kai-Jie Feng and Xiao-Na Ye and Sze-Teng Liong and Li-Yan Chen},
keywords = {Error-correcting output codes, Self-adaptive Strategy, Soft codes, Coverage measure, Subordination degree},
abstract = {Error-Correcting Output Codes (ECOC) algorithms enable multiclass classification by reassigning multiple classes to the positive/negative group with the class reassignment schemes being recorded as binary/ternary hard-coded (HC) codematrices. Different classes tend to get diverse subordination degrees to the positive/negative group, providing clues to correct potential errors. However, the HC codematrices are unable to provide the information in the subordination degrees. In this paper, a Soft-Coded ECOC (SC-ECOC) scheme, namely, the Sequential Forward Floating Selection algorithm, is proposed by filling codematrices with real values instead of hard codes to improve classification performance. This algorithm divides multiple classes into two groups by maximizing the ratio of inter-group distance to intra-group distance. Then a new measure coverage is designed to evaluate the subordination degrees of different classes to both groups, which are set as the elements to form a codematrix. Furthermore, a self-adaptive strategy adjusts the value of each element to fit learners better. Experiments are carried out to verify the performance of our algorithm on various data sets, and results confirm that our algorithm can achieve more balanced results compared with the traditional HC ECOC algorithms. Besides, the values of soft codes correlate with the difficulty level of various classes to improve the multiclass classification ability.}
}
@article{GAO2023109073,
title = {Video Object Segmentation using Point-based Memory Network},
journal = {Pattern Recognition},
volume = {134},
pages = {109073},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109073},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005532},
author = {Mingqi Gao and Jungong Han and Feng Zheng and James J.Q. Yu and Giovanni Montana},
keywords = {Video object segmentation, Point-based feature matching, Adaptive matching module},
abstract = {Recent years have witnessed the prevalence of memory-based methods for Semi-supervised Video Object Segmentation (SVOS) which utilise past frames efficiently for label propagation. When conducting feature matching, fine-grained multi-scale feature matching has typically been performed using all query points, which inevitably results in redundant computations and thus makes the fusion of multi-scale results ineffective. In this paper, we develop a new Point-based Memory Network, termed as PMNet, to perform fine-grained feature matching on hard samples only, assuming that easy samples can already obtain satisfactory matching results without the need for complicated multi-scale feature matching. Our approach first generates an uncertainty map from the initial decoding outputs. Next, the fine-grained features at uncertain locations are sampled to match the memory features on the same scale. Finally, the matching results are further decoded to provide a refined output. The point-based scheme works with the coarsest feature matching in a complementary and efficient manner. Furthermore, we propose an approach to adaptively perform global or regional matching based on the motion history of memory points, making our method more robust against ambiguous backgrounds. Experimental results on several benchmark datasets demonstrate the superiority of our proposed method over state-of-the-art methods.}
}
@article{MAO2023109097,
title = {Enhancing 3D-2D Representations for Convolution Occupancy Networks},
journal = {Pattern Recognition},
volume = {134},
pages = {109097},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109097},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005775},
author = {Qing Mao and Rui Li and Yu Zhu and Jinqiu Sun and Yanning Zhang},
keywords = {Implicit 3D representation, Multi-scale 3D position encoding, 3D Correlation-Guided Attentions},
abstract = {Convolutional Occupancy Networks (ConvONet) have gained popularity in object-level and scene-level reconstruction. However, how to better represent the 3D features for ConvONet remains an open question. In this paper, we propose to improve the representation for ConvONet by enhancing both 3D positional information and 3D-2D correlations. Considering that position information acts as the fundamental component of a 3D shape, we propose a Position-Aware Transformer (PAT) architecture that incorporates the Adaptive Multi-Scale Position Encoding (AMSPE) into the self-attention computation. By leveraging both global and local position aggregations in a multi-level manner, AMSPE enables better representations of both coarse and fine structures of the 3D shape. Meanwhile, since projecting 3D features to 2D planes for convolution inevitably introduces ambiguous or noisy representations, we propose a 3D Correlation-Guided Enhancement (CGE) network to bridge the gap between 3D and 2D shape representations. Specifically, we leverage the projected 3D correlations from PAT as the structural guidance, then compute the 3D Correlation-Guided Attentions (CGAs) to enhance the most representative features in the 2D space. In this way, the proposed architecture preserves the most informative structural representations while alleviating the impact of the mis-projected and noisy features. Experiments on ShapeNet and indoor scene dataset demonstrate the superiority of our method. Both quantitative and qualitative experiments show that our method achieves state-of-the-art performance for implicit-based 3D reconstruction.}
}
@article{NG2023109045,
title = {Fuzzy Superpixel-based Image Segmentation},
journal = {Pattern Recognition},
volume = {134},
pages = {109045},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109045},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005258},
author = {Tsz Ching Ng and Siu Kai Choy and Shu Yan Lam and Kwok Wai Yu},
keywords = {Fuzzy algorithm, Graph theory, Mean-shift, Segmentation, Superpixel},
abstract = {This article presents a multi-phase image segmentation methodology based on fuzzy superpixel decomposition, aggregation and merging. First, a collection of layers of dense fuzzy superpixels is generated by the variational fuzzy decomposition algorithm. Then a layer of refined superpixels is extracted by aggregating various layers of dense fuzzy superpixels using the hierarchical normalized cuts. Finally, the refined superpixels are projected into the low dimensional feature spaces by the multidimensional scaling and the segmentation result is obtained via the mean-shift-based merging approach with the spatial bandwidth adjustment strategy. Our algorithm utilizes the superimposition of fuzzy superpixels to impose more accurate spatial constraints on the final segmentation through the fuzzy superpixel aggregation. The fuzziness of superpixels also provides spatial features to measure affinities between fuzzy superpixels and refined superpixels, and guide the merging process. Comparative experiments with the existing approaches reveal a superior performance of the proposed method.}
}
@article{DUONG2023109126,
title = {Deep MinCut: Learning Node Embeddings by Detecting Communities},
journal = {Pattern Recognition},
volume = {134},
pages = {109126},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109126},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006069},
author = {Chi Thang Duong and Thanh Tam Nguyen and Trung-Dung Hoang and Hongzhi Yin and Matthias Weidlich and Quoc Viet Hung Nguyen},
keywords = {Node embedding, Graph representation learning, Community detection, Interpretable machine learning},
abstract = {We present Deep MinCut (DMC), an unsupervised approach to learn node embeddings for graph-structured data. It derives node representations based on their membership in communities. As such, the embeddings directly provide insights into the graph structure, so that a separate clustering step is no longer needed. DMC learns both, node embeddings and communities, simultaneously by minimizing the mincut loss, which captures the number of connections between communities. Striving for high scalability, we also propose a training process for DMC based on minibatches. We provide empirical evidence that the communities learned by DMC are meaningful and that the node embeddings are competitive in different node classification benchmarks.}
}
@article{ACENA2023109117,
title = {Support subsets estimation for support vector machines retraining},
journal = {Pattern Recognition},
volume = {134},
pages = {109117},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109117},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005970},
author = {Víctor Aceña and Isaac {Martín de Diego} and Rubén {R． Fernández} and Javier {M． Moguerza}},
keywords = {Support subset, , Incremental learning, Retraining, Alpha seeding},
abstract = {The availability of new data in previously trained Machine Learning (ML) models usually requires retraining and adjustment of the model. Support Vector Machines (SVMs) are widely used in ML because of their strong mathematical foundations and flexibility. However, SVM training is computationally expensive, both in time and memory. Hence, the training phase might be a limitation in problems where the model is updated regularly. As a solution, new methods for training and updating SVMs have been proposed in the past. In this paper, we introduce the concept of Support Subset and a new retraining methodology for SVMs. A Support Subset is a subset of the training set, such that retraining a ML model with this subset and the new data is equivalent to training with all the data. The performance of the proposal is evaluated in a variety of experiments on simulated and real datasets in terms of time, quality of the solution, resultant support vectors, and amount of employed data. The promising results provide a new research line for improving the effectiveness and adaptability of the proposed technique, including its generalization to other ML models.}
}
@article{YU2023109065,
title = {Meta-learning-based adversarial training for deep 3D face recognition on point clouds},
journal = {Pattern Recognition},
volume = {134},
pages = {109065},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109065},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005453},
author = {Cuican Yu and Zihui Zhang and Huibin Li and Jian Sun and Zongben Xu},
keywords = {Deep 3D face recognition, Point clouds, Adversarial samples, Meta-learning},
abstract = {Recently, deep face recognition using 2D face images has made great advances mainly due to the readily available large-scale face data. However, deep face recognition using 3D face scans, especially on point clouds, has been far from fully explored. In this paper, we propose a novel meta-learning-based adversarial training (MLAT) algorithm for deep 3D face recognition (3DFR) on point clouds. It consists of two alternate modules: adversarial sample generating for 3D face data augmentation and meta-learning-based deep network training. In the first module, adversarial samples of given 3D face scans are dynamically generated based on current deep 3DFR model. In the second module, a meta-learning framework is designed to avoid the performance decrease caused by the generated adversarial samples. Overall, MLAT algorithm combines the adversarial sample generating and meta-learning-based network training in a uniform framework, in which adversarial samples and network parameters are optimized alternately. Thus, it can continuously generate diverse and suitable adversarial samples, and then the meta-learning framework can further improve the accuracy of 3DFR model. Comprehensive experimental results show that the proposed approach consistently achieves competitive rank-one recognition accuracies on the BU-3DFE (100%), Bosphorus (99.78%), BU-4DFE (98.02%) and FRGC v2 (98.01%) database, and thereby substantiate its superiority.}
}
@article{STRAGAPEDE2023109089,
title = {BehavePassDB: Public Database for Mobile Behavioral Biometrics and Benchmark Evaluation},
journal = {Pattern Recognition},
volume = {134},
pages = {109089},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109089},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005696},
author = {Giuseppe Stragapede and Ruben Vera-Rodriguez and Ruben Tolosana and Aythami Morales},
keywords = {Mobile authentication, Continuous authentication, Behavioral biometrics, BehavePassDB, Device bias},
abstract = {Mobile behavioral biometrics have become a popular topic of research, reaching promising results in terms of authentication, exploiting a multimodal combination of touchscreen and background sensor data. However, there is no way of knowing whether state-of-the-art classifiers in the literature can distinguish between the notion of user and device. In this article, we present a new database, BehavePassDB, structured into separate acquisition sessions and tasks to mimic the most common aspects of mobile Human-Computer Interaction (HCI). BehavePassDB is acquired through a dedicated mobile app installed on the subjects devices, also including the case of different users on the same device for evaluation. We propose a standard experimental protocol and benchmark for the research community to perform a fair comparison of novel approaches with the state of the art11https://github.com/BiDAlab/MobileB2C_BehavePassDB/.. We propose and evaluate a system based on Long-Short Term Memory (LSTM) architecture with triplet loss and modality fusion at score level.}
}
@article{DAI2023109108,
title = {PFEMed: Few-shot medical image classification using prior guided feature enhancement},
journal = {Pattern Recognition},
volume = {134},
pages = {109108},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109108},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200588X},
author = {Zhiyong Dai and Jianjun Yi and Lei Yan and Qingwen Xu and Liang Hu and Qi Zhang and Jiahui Li and Guoqiang Wang},
keywords = {Deep learning, Domain adaption, Few-shot learning, Medical image classification, Variational autoencoder},
abstract = {Deep learning-based methods have recently demonstrated outstanding performance on general image classification tasks. As optimization of these methods is dependent on a large amount of labeled data, their application in medical image classification is limited. To address this issue, we propose PFEMed, a novel few-shot classification method for medical images. To extract general and specific features from medical images, this method employs a dual-encoder structure, that is, one encoder with fixed weights pre-trained on public image classification datasets and another encoder trained on the target medical dataset. In addition, we introduce a novel prior-guided Variational Autoencoder (VAE) module to enhance the robustness of the target feature, which is the concatenation of the general and specific features. Then, we match the target features extracted from both the support and query medical image samples and predict the category attribution of the query examples. Extensive experiments on several publicly available medical image datasets show that our method outperforms current state-of-the-art few-shot methods by a wide margin, particularly outperforming MetaMed on the Pap smear dataset by over 2.63%.}
}
@article{YU2023109113,
title = {Detecting group concept drift from multiple data streams},
journal = {Pattern Recognition},
volume = {134},
pages = {109113},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109113},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005933},
author = {Hang Yu and Weixu Liu and Jie Lu and Yimin Wen and Xiangfeng Luo and Guangquan Zhang},
keywords = {Concept drift, Data streams, Online learning, Hypothesis test},
abstract = {Concept drift may lead to a sharp downturn in the performance of streaming in data-based algorithms, caused by unforeseeable changes in the underlying distribution of data. In this paper, we are mainly concerned with concept drift across multiple data streams, and in situations where the drift of each data stream cannot be detected in time, due to slight underlying distribution drifts. We call this group concept drift. When compared to the detection of concept drift for a single data stream, the challenges of detecting group concept drift arise from three aspects: first, the training data become more complex; second, the underlying distribution becomes more complex; and third, the correlations between data streams become more complex. To address these challenges, the key idea of our method is to construct a distribution free test statistic, free from any underlying distribution in multiple data streams. Then, for streaming data, we design an online learning algorithm to obtain this test statistic, thereby determining the concept drift caused by the hypothesis test. The experiment evaluations with both synthetic and real-world datasets prove that our method can accurately detect concept drift from multiple data streams.}
}
@article{SACHDEVA2023109121,
title = {ScanMix: Learning from Severe Label Noise via Semantic Clustering and Semi-Supervised Learning},
journal = {Pattern Recognition},
volume = {134},
pages = {109121},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109121},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200601X},
author = {Ragav Sachdeva and Filipe Rolim Cordeiro and Vasileios Belagiannis and Ian Reid and Gustavo Carneiro},
keywords = {Noisy label learning, Semi-supervised learning, Semantic clustering, Self-supervised Learning, Expectation maximisation},
abstract = {We propose a new training algorithm, ScanMix, that explores semantic clustering and semi-supervised learning (SSL) to allow superior robustness to severe label noise and competitive robustness to non-severe label noise problems, in comparison to the state of the art (SOTA) methods. ScanMix is based on the expectation maximisation framework, where the E-step estimates the latent variable to cluster the training images based on their appearance and classification results, and the M-step optimises the SSL classification and learns effective feature representations via semantic clustering. We present a theoretical result that shows the correctness and convergence of ScanMix, and an empirical result that shows that ScanMix has SOTA results on CIFAR-10/-100 (with symmetric, asymmetric and semantic label noise), Red Mini-ImageNet (from the Controlled Noisy Web Labels), Clothing1M and WebVision. In all benchmarks with severe label noise, our results are competitive to the current SOTA.}
}
@article{ABHISHEK2023109081,
title = {Parzen Window Approximation on Riemannian Manifold},
journal = {Pattern Recognition},
volume = {134},
pages = {109081},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109081},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005611},
author = { Abhishek and Rakesh {Kumar Yadav} and Shekhar Verma},
keywords = {Parzen window, Data affinity, Graph Laplacian regularization, Manifold regularization},
abstract = {In graph motivated learning, label propagation largely depends on data affinity represented as edges between connected data points. The affinity assignment implicitly assumes even distribution of data on the manifold. This assumption may not hold and may lead to inaccurate metric assignment due to drift towards high-density regions. The drift affected heat kernel based affinity with a globally fixed Parzen window either discards genuine neighbors or forces distant data points to become a member of the neighborhood. This yields a biased affinity matrix. In this paper, the bias due to uneven data sampling on the Riemannian manifold is catered to by a variable Parzen window determined as a function of neighborhood size, ambient dimension, flatness range, etc. Additionally, affinity adjustment is used which offsets the effect of uneven sampling responsible for the bias. An affinity metric which takes into consideration the irregular sampling effect to yield accurate label propagation is proposed. Extensive experiments on synthetic and real-world data sets confirm that the proposed method increases the classification accuracy significantly and outperforms existing Parzen window estimators in graph Laplacian manifold regularization methods.}
}
@article{HELM2023109085,
title = {Distance-based positive and unlabeled learning for ranking},
journal = {Pattern Recognition},
volume = {134},
pages = {109085},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109085},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005659},
author = {Hayden S. Helm and Amitabh Basu and Avanti Athreya and Youngser Park and Joshua T. Vogelstein and Carey E. Priebe and Michael Winding and Marta Zlatic and Albert Cardona and Patrick Bourke and Jonathan Larson and Marah Abdin and Piali Choudhury and Weiwei Yang and Christopher W. White},
keywords = {Positive-and-unlabeled learning, ranking, network analysis},
abstract = {Learning to rank – producing a ranked list of items specific to a query and with respect to a set of supervisory items – is a problem of general interest. The setting we consider is one in which no analytic description of what constitutes a good ranking is available. Instead, we have a collection of representations and supervisory information consisting of a (target item, interesting items set) pair. We demonstrate analytically, in simulation, and in real data examples that learning to rank via combining representations using an integer linear program is effective when the supervision is as light as “these few items are similar to your item of interest.” While this nomination task is quite general, for specificity we present our methodology from the perspective of vertex nomination in graphs. The methodology described herein is model agnostic.}
}
@article{LIU2023109101,
title = {A dense light field reconstruction algorithm for four-dimensional optical flow constraint equation},
journal = {Pattern Recognition},
volume = {134},
pages = {109101},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109101},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005817},
author = {Jian Liu and Na Song and Zhengde Xia and Bin Liu and Jinxiao Pan and Abdul Ghaffar and Jianbin Ren and Ming Yang},
keywords = {Light field, Optical flow, A dense reconstruction},
abstract = {Dense light field sampling is an important basis for refocusing, depth estimation and 3-D imaging. It is difficult to obtain high resolution dense light field with a large-scale camera array and expensive equipment. At the same time, the current storage devices and transmission bandwidth also limit this technology's post-processing and application. In order to effectively reconstruct the angle domain of the light field based on the sparse light field data, this paper analyzes the correlation and constraint relationship between the optical flow field and the motion field of multi-view images in the same scene, extends the traditional optical flow constraint equation of two-dimensional imaging to the optical flow constraint equation of four-dimensional light field, and establishes an effective mathematical model. The coordinate position of the original pixel in the new angle image is determined by coordinate search, and its intensity is reconstructed. The experimental results of multi-scene dense reconstruction show that the proposed method can reconstruct the texture, shadow and color information in the light field of a long-baseline scene with high quality. The quantitative evaluation results show that the algorithm can be applied to dense light field reconstruction of complex scenes. The algorithm in this paper is only suitable for the case of optical flow constraint in a linear light field, and the follow-up research will focus on the case of nonlinear optical flow constraint.}
}
@article{SHI2023109080,
title = {Self-paced resistance learning against overfitting on noisy labels},
journal = {Pattern Recognition},
volume = {134},
pages = {109080},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109080},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200560X},
author = {Xiaoshuang Shi and Zhenhua Guo and Kang Li and Yun Liang and Xiaofeng Zhu},
keywords = {Convolutional neural networks, Self-paced resistance, Model overfitting, Noisy labels},
abstract = {Noisy labels composed of correct and corrupted ones are pervasive in practice. They might significantly deteriorate the performance of convolutional neural networks (CNNs), because CNNs are easily overfitted on corrupted labels. To address this issue, inspired by an observation, deep neural networks might first memorize the probably correct-label data and then corrupt-label samples, we propose a novel yet simple self-paced resistance framework to resist corrupted labels, without using any clean validation data. The proposed framework first utilizes the memorization effect of CNNs to learn a curriculum, which contains confident samples and provides meaningful supervision for other training samples. Then it adopts selected confident samples and a proposed resistance loss to update model parameters; the resistance loss tends to smooth model parameters’ update or attain equivalent prediction over each class, thereby resisting model overfitting on corrupted labels. Finally, we unify these two modules into a single loss function and optimize it in an alternative learning. Extensive experiments demonstrate the significantly superior performance of the proposed framework over recent state-of-the-art methods on noisy-label data. Source codes of the proposed method are available on https://github.com/xsshi2015/Self-paced-Resistance-Learning.}
}